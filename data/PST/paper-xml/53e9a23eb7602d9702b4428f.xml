<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Introducing Hierarchy-awareness in Replacement and Bypass Algorithms for Last-level Caches</title>
				<funder>
					<orgName type="full">Intel Israel</orgName>
				</funder>
				<funder>
					<orgName type="full">Intel Corporation</orgName>
				</funder>
				<funder>
					<orgName type="full">Intel India</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mainak</forename><surname>Chaudhuri</surname></persName>
							<email>mainakc@iitk.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<postCode>208016</postCode>
									<settlement>Kanpur</settlement>
									<country key="IN">INDIA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jayesh</forename><surname>Gaur</surname></persName>
							<email>jayesh.gaur@intel.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Intel Architecture Group</orgName>
								<address>
									<postCode>560103</postCode>
									<settlement>Bangalore</settlement>
									<country key="IN">INDIA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nithiyanandan</forename><surname>Bashyam</surname></persName>
							<email>nithiyanandan.bashyam@intel.com</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Intel Architecture Group</orgName>
								<address>
									<postCode>560103</postCode>
									<settlement>Bangalore</settlement>
									<country key="IN">INDIA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sreenivas</forename><surname>Subramoney</surname></persName>
							<email>sreenivas.subramoney@intel.com</email>
							<affiliation key="aff3">
								<orgName type="laboratory">Intel Architecture Group</orgName>
								<address>
									<postCode>560103</postCode>
									<settlement>Bangalore</settlement>
									<country key="IN">INDIA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joseph</forename><surname>Nuzman</surname></persName>
							<email>joseph.nuzman@intel.com</email>
							<affiliation key="aff4">
								<orgName type="laboratory">Intel Architecture Group Haifa 31015</orgName>
								<address>
									<country key="IL">ISRAEL</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Introducing Hierarchy-awareness in Replacement and Bypass Algorithms for Last-level Caches</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>B.3 [Memory Structures]: Design Styles Algorithms</term>
					<term>design</term>
					<term>measurement</term>
					<term>performance Last-level caches</term>
					<term>replacement policy</term>
					<term>bypass algorithm</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The replacement policies for the last-level caches (LLCs) are usually designed based on the access information available locally at the LLC. These policies are inherently sub-optimal due to lack of information about the activities in the innerlevels of the hierarchy. This paper introduces cache hierarchyaware replacement (CHAR) algorithms for inclusive LLCs (or L3 caches) and applies the same algorithms to implement efficient bypass techniques for exclusive LLCs in a three-level hierarchy. In a hierarchy with an inclusive LLC, these algorithms mine the L2 cache eviction stream and decide if a block evicted from the L2 cache should be made a victim candidate in the LLC based on the access pattern of the evicted block. Ours is the first proposal that explores the possibility of using a subset of L2 cache eviction hints to improve the replacement algorithms of an inclusive LLC. The CHAR algorithm classifies the blocks residing in the L2 cache based on their reuse patterns and dynamically estimates the reuse probability of each class of blocks to generate selective replacement hints to the LLC. Compared to the static re-reference interval prediction (SRRIP) policy, our proposal offers an average reduction of 10.9% in LLC misses and an average improvement of 3.8% in instructions retired per cycle (IPC) for twelve singlethreaded applications. The corresponding reduction in LLC misses for one hundred 4-way multi-programmed workloads is 6.8% leading to an average improvement of 3.9% in throughput. Finally, our proposal achieves an 11.1% reduction in LLC misses and a 4.2% reduction in parallel execution cycles for six 8-way threaded shared memory applications compared to the SRRIP policy.</p><p>In a cache hierarchy with an exclusive LLC, our CHAR proposal offers an effective algorithm for selecting the subset of blocks (clean or dirty) evicted from the L2 cache that need not be written to the LLC and can be bypassed. Compared to the TC-AGE policy (analogue of SRRIP for exclusive LLC), our best exclusive LLC proposal improves average throughput by 3.2% while saving an average of 66.6% of data transactions from the L2 cache to the on-die interconnect for one hundred 4-way multi-programmed workloads.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The replacement policy for a particular level of a cache hierarchy is usually designed based on the access information (frequency, recency, etc.) available only at that level of the hierarchy. Such a level-centric design methodology for the last-level cache (LLC) fails to incorporate two important pieces of information. First, the reuses taking place in the inner-levels of a hierarchy are not propagated to the LLC. Past research showed that propagating even a small fraction of these reuses to the LLC can significantly increase the traffic in the on-die interconnect <ref type="bibr">[7]</ref>. Second, the clean evictions from the inner-levels are usually not propagated to the LLC in an inclusive or a non-inclusive/non-exclusive hierarchy.</p><p>This paper, for the first time, studies the possibility of using an appropriately chosen subset of L2 cache evictions as hints for improving the replacement algorithms of an inclusive LLC (or L3 cache) in a three-level hierarchy. The central idea is that when the L2 cache residency of a block comes to an end, one can estimate its future liveness based on its reuse pattern observed during its residency in the L2 cache. Particularly, if we can deduce that the next reuse distance of such a block is significantly beyond the LLC reach, we can notify the LLC that this block should be marked a potential victim candidate in the LLC. An early eviction of such a block can help retain more blocks in the LLC with relatively shorter reuse distances. In a hierarchy with an exclusive LLC, this liveness information can be used to decide the subset of the L2 cache evictions that need not be allocated in the LLC.</p><p>To estimate the merit of making an inclusive LLC aware of the L2 cache evictions, we conduct an oracle-assisted experiment where the LLC runs the two-bit SRRIP policy <ref type="bibr" target="#b8">[8]</ref> (this is our baseline in this paper). The two-bit SRRIP policy fills a block into the LLC with a re-reference prediction value (RRPV) of two and promotes it to RRPV of zero on a hit. A block with RRPV three (i.e., large re-reference distance) is chosen as the victim in a set. If none exists, the RRPVs of all the blocks in the set are increased in steps of one until a block with RRPV three is found. Ties are broken by victimizing the block with the least physical way id. <ref type="foot" target="#foot_0">1</ref> In our oracle-assisted experiment, on an L2 cache eviction of a data block<ref type="foot" target="#foot_1">2</ref> , a next forward use distance oracle determines the relative order between the next forward use distances of the evicted block and the current SRRIP victim in the LLC set where the L2 cache victim belongs to. If the next forward use distance of the L2 cache victim is bigger, it is marked a potential victim by changing its RRPV to three in the LLC. The left bar in each group of Figure <ref type="figure" target="#fig_0">1</ref> shows the number of LLC misses of this oracle-assisted policy normalized to the baseline SRRIP. The bar on the right in each group shows the number of LLC misses in Belady's optimal algorithm <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b20">20]</ref> normalized to the baseline. These experiments are carried out on an offline cache hierarchy simulator that takes as input the entire L2 cache access trace of twelve single-threaded applications drawn from SPEC 2000 and SPEC 2006 suites. The L2 cache access trace of each application is collected for a representative set of one billion dynamic instructions chosen using the SimPoint toolset <ref type="bibr" target="#b22">[22]</ref>. The cache hierarchy consists of 32 KB 8-way L1 instruction and data caches, a 256 KB 8-way L2 cache, and a 2 MB 16-way inclusive LLC. The L1 and L2 caches implement LRU replacement policy. Overall, Belady's optimal policy saves 28.5% of the LLC misses (refer to the AVG group), while the oracle-assisted SRRIP policy with L2 cache eviction hints can save about 17% of the baseline LLC misses. As a result, this policy can bridge close to two-third of the gap between Belady's optimal and the baseline SRRIP. Of course, this potential can be realized only if we can accurately learn which L2 cache evictions should be used to update the RRPV rank in the target LLC set and the solution to this problem forms the crux of our proposal. To further justify the data in Figure <ref type="figure" target="#fig_0">1</ref>, we show in Figure <ref type="figure" target="#fig_1">2</ref> the percentage of blocks evicted from the L2 cache that are never recalled by the core from the time they are evicted from the L2 cache until they are evicted from the LLC. These data show that almost 60% of the blocks evicted from the L2 cache turn out to be dead in the LLC (refer to the AVG group). Such a block could be marked a potential LLC replacement candidate at the time it is evicted from the L2 cache provided we can separate it from the live blocks. Early replacement of such dead blocks can improve performance if the application has a good number of live blocks that can now stay longer in the LLC and enjoy additional reuses.</p><p>The fact that 60% of the L2 cache evictions are dead corresponds well with the already known fact that the blocks brought into the LLC have low use counts <ref type="bibr" target="#b21">[21]</ref>. For the data in Figure <ref type="figure" target="#fig_1">2</ref>, the average use count per LLC block is about 1.67 (reciprocal of dead percentage). In summary, the L2 cache eviction stream is rich in information regarding liveness of the cache blocks. Accurate separation of the live blocks from the dead ones in the L2 cache eviction stream can bridge a significant portion of the gap between the baseline and the optimal replacement policy for the LLC.</p><p>In Section 2, we present our cache hierarchy-aware replacement (CHAR) algorithms for inclusive LLCs. Figure <ref type="figure" target="#fig_2">3</ref> shows a high-level implementation of our CHAR algorithm for inclusive LLCs. The dead hint detector hardware is part of the L2 cache controller. It consumes the L2 cache eviction stream and identifies the eviction addresses that should be sent to the LLC as dead hints. As usual, it sends all dirty evictions to the LLC, some of which may be marked as dead hints. We note that the general framework of CHAR algorithms is not tied to any specific LLC replacement policy.</p><p>Section 2 also discusses how our CHAR proposal seamlessly applies to exclusive LLCs as well. In such designs, we use the dead hints to decide which blocks evicted from the L2 cache can be bypassed and need not be allocated in the exclusive LLC (blocks are allocated and written to an exclusive LLC when they are evicted from the L2 cache). This leads to bandwidth saving in the on-die interconnect and effective capacity allocation in the LLC. Further, we show that simple variants of our CHAR proposal can be used to dynamically decide if a block should be cached in exclusive mode or noninclusive/non-exclusive mode in the LLC. The former mode optimizes the effective on-die cache capacity, while the latter trades cache capacity for on-die interconnect bandwidth by tolerating controlled amount of duplication of contents between the LLC and the L2 cache. Our execution-driven simulation methodology and results for single-threaded, multi-programmed, and shared memory parallel workloads are discussed in Sections 3 and 4. For inclusive LLCs, CHAR achieves 10.9%, 6.8%, and 11.1% reductions in LLC misses respectively for these workload classes compared to the baseline SRRIP policy. This leads to an average IPC improvement of 3.8% for the single-threaded applications, an average throughput improvement of 3.9% for the multi-programmed workloads, and an average reduction of 4.2% in parallel execution cycles for the shared memory applications. For exclusive LLCs, the best CHAR proposal achieves an average throughput improvement of 3.2% compared to the two-bit TC-AGE baseline (analogue of SRRIP for exclusive LLC) <ref type="bibr" target="#b5">[5]</ref> while saving 66.6% data write transactions from the L2 cache to the on-die interconnect for the multi-programmed workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Update victim rank</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L2 cache Eviction detector</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dead hint interconnect</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L2-LLC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subset of L2 evictions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LLC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Work</head><p>A large body of research work exists in the domain of replacement policies for inclusive LLCs. However, almost all of these consider the LLC in isolation and design the algorithms based on information available locally at the LLC. Only a few published studies explore hierarchy-aware (sometimes called global) replacement policies for inclusive LLCs. The first such study explored a number of global replacement policies where different types of access hints from the inner-level are sent to the LLC <ref type="bibr" target="#b26">[26]</ref>. It showed that the advantage of such global schemes is limited to specific scenarios. A subsequent study further analyzes the limited utility of access hint-based global replacement schemes using a reuse distance argument <ref type="bibr" target="#b4">[4]</ref>.</p><p>A recent work shows that inner-level access hints can improve performance of an inclusive LLC significantly for a selected set of multi-programmed workloads if the on-die interconnect bandwidth is not a constraint <ref type="bibr">[7]</ref>. This study also proposes two techniques, namely, early core invalidation (ECI) and query-based selection (QBS) to infer the temporal locality of inner-level accesses without sending access hints to the LLC. At the time of an LLC eviction, ECI invalidates the next LLC victim block from the L1 and L2 caches so that the LLC can observe any short-term temporal locality of this block before it is evicted from the LLC. QBS probes the L1 and L2 caches when an eviction decision is taken in the LLC to infer the usefulness of the current LLC victim and accordingly modifies the selection of LLC victims. In all the inclusive LLC configurations used in this paper, we keep the inclusion overhead low by maintaining an 8:1 capacity ratio between the LLC and the L2 cache <ref type="bibr">[7]</ref>, thereby eliminating most of the negative effects of inclusion victims. A recent work <ref type="bibr" target="#b27">[27]</ref> explores an orthogonal dimension of the problem by proposing global cache management schemes to decide which level of the hierarchy in a two-level cache an incoming block should be placed in. Our proposal significantly differs from all these existing proposals. Our policy learns to identify a subset of blocks evicted from the L2 cache that can be made potential victim candidates in the LLC.</p><p>Our proposal shares some similarities with the dead block predictors. The existing dead block predictors predict the last access or the last burst of accesses to a block <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b18">18]</ref>. These predictors usually require partial or full program counters (PC) to construct the necessary correlations with liveness of cache blocks. A recent work constructs a PC-less dead-on-fill predictor for use in cache bypassing by dynamically segmenting the LLC between referenced and not referenced blocks <ref type="bibr" target="#b11">[11]</ref>. PC-less light-weight dead block predictors exploiting the fill order of LLC blocks have also been proposed <ref type="bibr" target="#b2">[2]</ref>. Our basic CHAR proposal infers the death of an LLC block at the time it is evicted from the L2 cache and does not rely on program counter information. We briefly explore how to extend this basic CHAR design to take into account PC-based correlations.</p><p>Our proposal relies on estimation of reuse distance patterns in the L2 cache eviction stream. L2 cache eviction patterns have been used to arrive at bypass decisions and assign insertion ages to the non-bypassed blocks in the context of exclusive L3 caches <ref type="bibr" target="#b5">[5]</ref>. LLC insertion and replacement policies based on static and dynamic re-reference interval prediction (SRRIP and DRRIP) have been explored <ref type="bibr" target="#b8">[8]</ref>. A recent proposal improves the re-reference interval prediction of RRIP by exploiting LLC fill PC signatures (SHiP-PC), memory region signatures (SHiP-Mem), and instruction trace signatures (SHiP-ISeq) <ref type="bibr" target="#b24">[24]</ref>. Another recent work shows how to extend RRIP to manage LLCs shared between CPU workloads and GPGPU workloads in a CPU-GPU heterogeneous environment <ref type="bibr" target="#b17">[17]</ref>. Further, the PACMan family of policies is shown to outperform the RRIP policy in the presence of hardware prefetching by judicious design of RRPV insertion and update algorithms for prefetch fills and prefetch hits <ref type="bibr" target="#b25">[25]</ref>. Prediction of reuse distances or next-use distances by correlating with program counters has also been studied <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b19">19]</ref>.</p><p>A recent proposal explores a set dueling-based solution to deliver performance close to an exclusive LLC while saving on-die interconnect bandwidth by dynamically switching the entire LLC between non-inclusive/non-exclusive and exclusive modes based on the outcome of the duel <ref type="bibr" target="#b23">[23]</ref>. We show that our best CHAR proposal for exclusive LLC can dynamically decide the caching modes of different classes of blocks in the LLC at a fine grain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">CHAR ALGORITHMS</head><p>This section details our proposal on cache hierarchy-aware replacement (CHAR). The high-level flow diagram of CHAR is shown in Figure <ref type="figure" target="#fig_2">3</ref>. Section 2.1 presents the design of the dead hint detector that identifies dead blocks in the L2 cache eviction stream in the context of an inclusive LLC. Section 2.2 explores the relationship of this design with PC-based dead block predictors. In Section 2.3, we discuss how the same dead hint detector design can be used in the context of an exclusive LLC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dead Hint Detector</head><p>The dead hint detector relies on the reuse behavior experienced by the blocks residing in the L2 cache to decide whether a block is likely to be recalled in future from the LLC after it is evicted from the L2 cache. To better learn a summary of this reuse behavior, we classify an L2 cache block (data block only) into one of five categories at the time of its eviction from the L2 cache. Such a classification is expected to separate the blocks with different liveness/death patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Classification of L2 Cache Blocks</head><p>The classification of the L2 cache blocks is based on an approximate estimation of reuse distances inferred from their L2 cache usage patterns. This classification is invoked when a block is evicted from the L2 cache. The following four attributes (A0, . . . , A3) are used to classify an L2 cache block.</p><p>? (A0) The type of the request that filled the block in the L2 cache (prefetch or demand).</p><p>? (A1) The request that filled the block in the L2 cache was a hit or a miss in the LLC.</p><p>? (A2) Number of demand uses (including the fill if it was a demand fill) enjoyed by the block during its residency in the L2 cache.</p><p>? (A3) The L2 state of the block when it is evicted from the L2 cache.</p><p>Table <ref type="table" target="#tab_0">1</ref> summarizes the class definitions, while Figure <ref type="figure" target="#fig_4">4</ref> shows how a block can transition from one class membership to another during its residency in the L2 cache. For example, a block belongs to C0 if a) it is filled into the L2 cache by a prefetch request that misses in the LLC, b) it fails to experience any demand hit during its residency in the L2 cache, and c) it is evicted from the L2 cache in a clean state (E or S in a MESI protocol). On the other hand, a block is categorized as a C2 block if a) it is filled into the L2 cache by a prefetch or a demand request that misses in the LLC, b) it enjoys exactly one demand use during its residency in the L2 cache (if it was filled by a demand request, it does not experience any demand hit), and c) it is evicted from the L2 cache in modified state. </p><formula xml:id="formula_0">Class A0 A1 A2 A3 C0 Prefetch Miss 0 E/S C1 X Miss 1 E/S C2 X Miss 1 M C3 X Miss ? 2 X C4 X Hit X X</formula><p>The C0 class is relevant only if a hardware prefetcher is turned on and this class separates the potential premature or incorrect prefetches from rest of the blocks. <ref type="foot" target="#foot_2">3</ref> The remaining four classes separate the L2 cache blocks into different reuse distance bins. Beyond the L1 cache hits, a C1 block is expected to have most of its natural reuse distances bigger than the reach of the L2 cache, since it fails to experience any demand hits while residing in the L2 cache. The C2 blocks are similar to the C1 blocks, except that the former class is modified and the latter is clean. We find that in several applications, separation of C1 blocks from the C2 blocks improves the identification of dead blocks in the L2 cache eviction stream.</p><p>The C3 blocks are likely to have a reuse cluster falling within the reach of the L2 cache. Finally, the C4 blocks are likely to have a reuse cluster within the LLC reach.  Our classification of the L2 cache blocks is inspired by the classification based on trip count and use count of cache blocks presented in an earlier study in the context of a cache hierarchy with an exclusive LLC <ref type="bibr" target="#b5">[5]</ref>. According to the terminology used in that study, the set C0 ? C1 ? C2 ? C3 contains the zero trip count blocks i.e., the blocks that are filled in the L2 cache for the first time during their residency in the hierarchy. The C4 blocks have positive trip counts because they are recalled at least once from the LLC. As a result, C4 is a subset of C0 ? C1 ? C2 ? C3.</p><p>Table <ref type="table">2</ref> shows how our five classes of cache blocks can be encoded in the L2 cache with just two extra state bits (S1, S0) per L2 cache block (as opposed to three bits per L2 cache block in <ref type="bibr" target="#b5">[5]</ref>). Figure <ref type="figure" target="#fig_4">4</ref>, through the class transitions, unambiguously defines the state transitions of these two bits on hits and writebacks to L2 cache blocks. 4   Table <ref type="table">2</ref>: Class encoding in the L2 cache</p><formula xml:id="formula_1">State M State S1 State S0 Class X 0 0 C0 0 0 1 C1 1 0 1 C2 X 1 0 C3 X 1 1 C4 0 0.2 0.4 0.6 0.8 1</formula><p>Fraction of dead evictions Figure <ref type="figure" target="#fig_5">5</ref> quantifies the benefit of implementing the aforementioned classification of the L2 cache blocks. It shows the 4 In our model, a block is filled into the L2 cache in either S or E state and it can transition to M state only if the L1 data cache writes the block back to the L2 cache. fraction of dead L2 cache evictions in each class for the nonprefetched baseline configuration on the single-threaded applications (32 KB 8-way LRU L1 caches, 256 KB 8-way LRU L2 cache, 2 MB 16-way SRRIP LLC). Naturally, the C0 class is non-existent in non-prefetched executions. Overall, the C1, C2, and C3 classes have very high fractions of dead blocks, while the C4 blocks are mostly live (see the average and median groups of bars). By looking at the average or median data, one may be tempted to merge the C1, C2, and C3 classes, given that they behave similarly. However, since C4 is a subset of C1 ? C2 ? C3, it is necessary to use a finer-grain partitioning of the set C1 ? C2 ? C3 to identify the blocks that eventually get promoted to C4. Also, there are applications where the C1, C2, and C3 classes behave very differently. For example, in 172.mgrid, the likelihood of finding a dead block in C1 evictions is below 0.5. The same is applicable to the C3 evictions of 401.bzip2. On the other hand, the C2 evictions coming out of the L2 cache in 179.art are primarily live. As a result, it is necessary to dynamically learn the reuse probability (same as live fraction) of each class of blocks evicted from the L2 cache and based on these probabilities we need to classify a block evicted from the L2 cache as dead or live. To save hardware resources necessary for this learning, our dead hint detector statically classifies all C4 evictions as live because dead blocks are usually a minority in the C4 class. The learning algorithm for the remaining four classes (C0, . . . , C3) is presented next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Learning Dead Evictions</head><p>The goal of the learning algorithm is to estimate the reuse probabilities of the L2 cache block classes. If the estimated probability of a class falls below an appropriate threshold, the blocks belonging to that class are identified as dead when they are evicted from the L2 cache and this hint is propagated to the LLC. To effectively learn these probabilities, each class C k (k ? {0, 1, 2, 3}) maintains two saturating counters, namely, the eviction counter (E k ) and the live counter (L k ).</p><p>An eviction counter E4 is also maintained for class C4. All these counters reside in the L2 cache controller. Further, sixteen LLC sample sets per 1024 LLC sets are dedicated for learning E k and L k . These LLC sample sets always execute the baseline SRRIP replacement policy. The L2 cache controller is made aware of the hash function for determining if an L2 cache fill or eviction address maps to an LLC sample set. E k keeps track of the number of L2 cache evictions belonging to class C k and mapping to the LLC sample sets. L k keeps track of which of these E k evictions are recalled from the LLC (these are the live evictions). The total number of L2 cache evictions mapping to the LLC sample sets is maintained in a saturating counter NE residing in the L2 cache controller.</p><p>Tables <ref type="table" target="#tab_1">3</ref> and<ref type="table" target="#tab_2">4</ref> show the actions of the L2 cache controller on an eviction and a fill, respectively. These actions are in addition to the usual ones such as sending a writeback to the LLC on evicting a dirty block from the L2 cache, etc.. The L2 cache eviction actions depend on whether the evicted address maps to an LLC sample set and the class of the evicted block. The L2 cache fill actions depend on whether the filled address maps to an LLC sample set and two attributes of the fill e.g., fill type (demand/prefetch) and hit/miss in the LLC. When a block mapping to one of the LLC sample sets is evicted from the L2 cache, its class id (k) and address (A) are sent to the LLC. The LLC stores the class id with the block. This storage is needed only for the sample sets in the LLC. The LLC, on a hit to a block in one of the sample sets, sends two pieces of additional information along with the fill message: the last stored class id (k) of the block and one bit signifying a hit in the LLC. This value of k is used to update L k , as shown in the first and third rows of Table <ref type="table" target="#tab_2">4</ref>.</p><p>From Tables <ref type="table" target="#tab_1">3</ref> and<ref type="table" target="#tab_2">4</ref>, we conclude that the estimated reuse probability of class C k is L k /E k , which is the collective hit  Finally, we present our dead block detection algorithm. This algorithm dynamically estimates an appropriate threshold t such that if L k /E k &lt; t or equivalently L k &lt; tE k , we identify a block belonging to class C k as dead when it is evicted from the L2 cache. One reasonable choice for t at any point in time during execution would be the baseline hit rate of the LLC. Such a dynamic choice would guarantee that if any class of blocks has hit rate lower than the current baseline LLC hit rate, the blocks in that class would be marked dead as and when they are evicted from the L2 cache. This algorithm would evict the less useful blocks from the LLC early and create more space in the LLC for the blocks that are contributing more heavily toward LLC hit rate. A highly accurate online estimate of the baseline LLC hit rate is E4/NE because E4 approximates the number of LLC hits to the sampled sets and NE approximates the total number of LLC accesses to the sampled sets.</p><p>To simplify the hardware, we approximate the E4/NE ratio such that t turns out to be a reciprocal of power of two. Therefore, we can synthesize L k &lt; tE k using a shifter and a comparator. we approximate it to half of the upper bound of the range. If it falls in any of the remaining ranges, we approximate it to the lower bound of the range, which is also half of the upper bound of the range. Finally, we determine t as ?4/NE where ?4 is the approximate value of E4. This leads to the following definition of t.</p><formula xml:id="formula_2">t = ? ? ? ? ? 1/16 if E4 ? NE/8 1/8 if NE/8 &lt; E4 ? NE/4 1/4 if NE/4 &lt; E4 ? NE/2 1/2 if E4 &gt; NE/2<label>(1)</label></formula><p>The computation of t may require further tuning. For certain workload classes, t = 1/2 may turn out to be very aggressive. In such situations, we recommend merging the last two ranges in Equation (1) i.e., t = 1/4 if E4 &gt; NE/4. Similarly, the last three ranges can be merged, if necessary. Also, a carefully chosen static value of t can offer reasonably good performance. A static value of 1/8 achieves excellent performance for our selection of workloads. In this study, we present results assuming Equation (1) for computing t dynamically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Few Implementation Details</head><p>The L2 cache, on evicting a data block, first queries the L1 data cache. If the query hits in the L1 data cache, the L1 data cache retains the block (our L2 cache is non-inclusive/nonexclusive of the L1 caches). Only those L2 cache data evictions that do not hit in the L1 data cache are passed on to the dead hint detector. The dead hint detector receives the address and the class of each such L2 cache eviction. It decides if the block is dead in the LLC by determining the current value of t and applying the dead hint detection algorithm (only for blocks not mapping to LLC sample sets). If the block is identified as dead, its address is sent to the LLC in a special dead hint message. The LLC, on receiving a dead hint message, sets the RRPV of the block to three. It also clears the bit position corresponding to the evicting core in the coherence bitvector of the block, since neither L1 cache nor L2 cache of the evicting core has the block. This saves a future back-invalidation message. <ref type="foot" target="#foot_3">5</ref> If the LLC receives an eviction message from the L2 cache for a block mapping to one of its sample sets, it stores the class id of the block found in the eviction message.</p><p>The combined hardware overhead of our CHAR proposal involves two extra state bits per L2 cache block, ten counters in the L2 cache controller (L0, . . . , L3, E0, . . ., E4, and NE), negligible logic overhead to implement the additional actions in the L2 cache controller on eviction and fill (Tables <ref type="table" target="#tab_1">3</ref> and<ref type="table" target="#tab_2">4</ref>), the logic of the dead hint detector in the L2 cache controller, three extra bits per LLC block for the sample sets to store class id, and handling of dead hints to non-sampled sets and eviction messages to sampled sets in the LLC. Note that our proposal does not involve any dynamic dueling between CHAR and baseline SRRIP policies, even though we use a small number of sampled sets in the LLC to dynamically learn the reuse behavior of the SRRIP policy. The L k , E k , and NE counters are halved periodically whenever the total number of L2 cache evictions mapping to the LLC sampled sets reaches a value of 127. We also experimented with halving intervals of 255, 1023, and 2047, but did not see any significant swing in performance. With a halving interval of 127, we need eight bits for each of the ten counters and seven bits for the counter that keeps track of the halving interval.</p><p>The CHAR algorithms seamlessly apply to multi-core scenarios without any change, since each core would have its own CHAR hardware attached to its private L2 cache controller. The sample sets in the shared LLC would be shared by all the threads.</p><p>For shared memory programs, we do not apply CHAR to the shared blocks, since the current proposal does not have the global view necessary to detect the death of shared blocks. We rely on the inclusive LLC for detecting the read-shared and read/write-shared blocks. When a request from a core hits in the LLC and the requested block is currently shared by another core (as indicated by the sharing bitvector directory), the LLC marks this block shared using an additional state bit per LLC block. Once marked, this bit remains set until the block is evicted from the LLC. Dead hints received for the blocks with this bit set are ignored by the LLC. Also, as soon as a block mapping to the LLC sample sets is identified as shared, it is upgraded to class C4 in the LLC so that it does not update the L k counters of any of the sharing cores. This policy will be referred to as CHAR-S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.4">Controlling Dead Hint Rate</head><p>Our dead hint generation algorithm tries to victimize the blocks belonging to the classes that have collective hit rates lower than the baseline LLC hit rate. However, there are situations where the overall baseline hit rate is reasonably high, but some of the classes are undergoing a phase transition and have low hit rates. The blocks belonging to these classes will be reused in near-future if we can retain them. CHAR cannot infer this by looking at the current hit rates of these classes and can hurt performance by sending premature dead hints to the LLC. The SPEC 2000 application 172.mgrid exhibits a few such phases. To address this problem, we incorporate a dead hint rate divider D with each core's L2 cache controller that sends out every D th dead hint to the LLC. The value of D is always a power of two, and ranges between one and 256, inclusive. In all simulations, we initialize D to one. In the following, we discuss how D is adjusted dynamically.</p><p>We ear-mark sixteen LLC sample sets (different from the sixteen baseline samples) per 1024 LLC sets to monitor the relative number of hits experienced by CHAR compared to the baseline samples. This is done using a saturating counter residing in the L2 cache controller. The counter is initialized to the midpoint M of the range of the counter e.g., M is 2 n-1 for an n-bit counter. The counter is incremented whenever the L2 cache receives a fill that hits in one of the CHAR sample sets of the LLC. The counter is decremented whenever the L2 cache receives a fill that hits in one of the baseline sample sets of the LLC.</p><p>After every eight halving intervals, we check the status of this counter. If the counter value is at least M +T gb , we halve D to double the dead hint rate because CHAR is performing better than the baseline. Note, however, that the minimum value of D is one. If the counter value is less than or equal to M -T ? gb , we quadruple D provided the LLC hit rate is at least 3/8 i.e., E4 is at least NE/4+NE /8; otherwise, D is left unchanged. The rationale is that we do not decrease the dead hint rate if the baseline LLC hit rate is anyway small. Note that this particular hit rate threshold (i.e., 3/8) may require tuning depending on the workload set under consideration.</p><p>The guard-band thresholds, T gb and T ? gb , are used to avoid potentially spurious adjustments in D and are set to 8 and 32 in this study. The dead hint rate division algorithm is conservative in the sense that it increases D much faster (though with a bigger guard-band) than it decreases D so that the performance penalty is low in the phases where CHAR becomes too aggressive. In multi-core configurations, the CHAR sample sets in the shared LLC are shared by all the threads and each private L2 cache controller maintains its own dead hint rate divider. To implement this algorithm, we use a 16-bit saturating counter for relative hit count monitoring (i.e., M is set to 2 15 ) and a 9-bit dead hint rate divider.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Code Space Correlation and CHAR-PC</head><p>Past research on dead block predictors has established a strong correlation between program counters (PCs) of the load/store instructions and the death patterns of the data blocks, as already discussed in Section 1.1. Figure <ref type="figure">6</ref>  caches, 256 KB 8-way LRU L2 cache, and 2 MB 16-way SR-RIP LLC) for the single-threaded programs with the hardware prefetcher disabled. We focus only on the classes C1, C2, and C3 that can source dead hints in a non-prefetched execution.</p><p>Let P C k be the set of program counters that cause the blocks, which eventually get classified as C k blocks, to be filled into the L2 cache. Figure <ref type="figure">6</ref> shows</p><formula xml:id="formula_3">|P C k ? P C k ? | as a fraction of |P C k ?P C k ? | for all ordered pairs (k, k ? ) with k, k ? ? {1, 2, 3}.</formula><p>It is very encouraging to find that the load/store instructions that bring C1 blocks into the L2 cache overlap very little with those that bring C2 and C3 blocks (see the average and median groups). The implication is that our PC-less classification of L2 cache blocks can effectively capture differing code space signatures for the pairs (C1, C2) and (C1, C3). However, the classes C2 and C3 share a sizable proportion of program counters. Further examination of this behavior revealed that the blocks that are written to and do not belong to C4 are consumed in either near-future (C3) or far-future (C2). The dirty C4 blocks have reuses in intermediate-future such that the reuse distance is within the LLC reach. Therefore, we conclude that our classification can successfully partition the dirty cache blocks based on their next use distance, which the code signature fails to do. While our classification correlates well with code space signatures, further improvements may be possible if we can split a class of blocks based on L2 cache fill PCs. For example, suppose the set P C1 has two program counters, say, P C11 and P C12. The blocks filled into the L2 cache by the instruction at P C11 may behave differently from those filled by the instruction at P C12. Clustering these both types of blocks into a single class would lower the overall prediction accuracy. To resolve this issue, we propose CHAR-PC, a PC-based extension to CHAR. CHAR-PC maintains, for each class C k with k ? {1, 2, 3}, an eight-entry fully associative table T k for storing the lower 14 bits of the PCs (after removing the lowest two bits) of the instructions that bring the C k blocks into the L2 cache. Each entry of T k has a valid bit, a 14-bit fill PC signature, and a three-bit saturating counter.</p><p>When a block is filled into the L2 cache, the 14-bit fill PC signature is stored with the L2 cache block. When a block mapping to an LLC sample set is evicted from the L2 cache, its class C k is determined and T k is looked up with its PC signature. If the signature is not found in T k , an entry is allocated by invoking the not-recently-used (NRU) replacement policy and the saturating counter for that entry is initialized to zero. If the entry is found in T k , the saturating counter for that entry is decremented by one. The fill PC signature and the class id of the evicted block are sent to the LLC. The LLC stores these along with the blocks mapping to the sample sets. When a block mapping to an LLC sample set is filled into the L2 cache as a result of an LLC hit, the last class id (k) and the last fill PC signature of the block are supplied by the LLC. At this point, T k is looked up with the last fill PC signature and if the entry is found, its saturating counter is incremented by one.</p><p>Finally, the dead block prediction takes place when a block not mapping to an LLC sample set and belonging to C k with k ? {1, 2, 3} is evicted from the L2 cache. T k is looked up with the fill PC signature of the block. If the entry is found and the saturating counter has a value zero, the block is predicted dead and a dead hint is propagated to the LLC. The signature length, the saturating counter size, and the prediction threshold of zero have been borrowed from the SHiP-PC proposal <ref type="bibr" target="#b24">[24]</ref>. In Section 4, we show that CHAR-PC, which exploits the cross product of our classification and L2 cache fill PC signature, improves performance beyond SHiP-PC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Application to Exclusive LLC</head><p>A block is allocated in an exclusive LLC when it is evicted from the L2 cache and it is de-allocated from the LLC on a subsequent hit or replacement. Since every L2 cache eviction (clean or dirty) must be sent to the LLC for allocation, an exclusive LLC design consumes much bigger on-die interconnect bandwidth compared to an inclusive LLC design. However, the dead hints of the CHAR algorithm can be used to identify the blocks that can be dropped by the L2 cache and need not be sent to the LLC for allocation. This is known as selective cache bypassing. In the following, we discuss the working of the CHAR algorithm in an exclusive LLC. Our baseline exclusive LLC allocates all L2 cache evictions and decides the insertion age of a block based on the two-bit TC-AGE policy <ref type="bibr" target="#b5">[5]</ref>. This policy is the analogue of SRRIP for exclusive LLCs. It inserts all C0, C1, C2, and C3 blocks at age two and the C4 blocks at age zero. Note that a block is in C4 class if and only if it has experienced at least one LLC hit. The LLC replacement policy is same as the inclusive SR-RIP replacement policy (i.e., it victimizes a block with age three). <ref type="foot" target="#foot_4">6</ref>In CHAR algorithm for exclusive LLC, every L2 cache eviction address is first sent to the LLC to update the coherence directory (the data is not sent at this point). The dead hint (one bit) for the block is sent to the LLC along with this message. If the block is not marked dead, the LLC requests the block from the L2 cache for allocation. If the block is marked dead, but there is an invalid way available in the target LLC set, a request for the block is sent by the LLC to the L2 cache. However, in this case, the block is filled at age three in the LLC. If the block is marked dead and there is no invalid way available in the target LLC set, the LLC sends a bypass command to the L2 cache. If the L2 cache receives a bypass command for a dirty block, it sends the block directly to the memory controller over the interconnect. On the other hand, if it receives a bypass command for a clean block, it drops the block and eliminates the data transaction. In summary, the CHAR algorithm allocates all live blocks in the LLC with an insertion age dictated by the TC-AGE policy. It also allocates a dead block at the highest possible age provided an invalid way is available in the target LLC set. All other dead blocks are either dropped (if clean) or sent to the memory controller (if dirty) by the L2 cache.</p><p>Since the CHAR algorithm classifies the C4 blocks as live, these blocks are always allocated in the LLC. A block moves to the C4 class as soon as it experiences a hit in the LLC and it remains in the C4 class during its residency in the cache hierarchy. As a result, a C4 block may get repeatedly allocated in and de-allocated from the LLC on its every trip between the L2 cache and the LLC. This leads to unnecessary wastage of on-die interconnect bandwidth.</p><p>We address this inefficiency by observing that a block allocated in the LLC should not be de-allocated on a hit because it is a live block and will have to be allocated again when it is evicted from the L2 cache. In other words, live blocks should be cached in non-inclusive/non-exclusive mode to save on-die interconnect bandwidth. However, if a C4 block is evicted from the LLC by the time it is evicted from the L2 cache, it must be re-allocated in the LLC at age zero so that the live blocks are retained in the cache hierarchy. Also, if such a block is evicted in the dirty state from the L2 cache, it must be sent to the LLC to update the LLC copy. In all other cases, the L2 cache can drop a C4 block. Finally, the age of a C4 block in the LLC is changed to zero when it is evicted from the L2 cache so that the block, being live, gets the highest level of protection in the LLC.</p><p>We explore three variations of this basic policy that eliminates the unnecessary data transactions involving the C4 blocks. While these policies do not de-allocate a block from the LLC on a hit, they update the age of the block in the LLC differently at the time of the hit. The first policy, CHAR-C4, leaves the age of the block unchanged at the time of hit.</p><p>The second policy, CHAR-C4-MAX, updates the age of the block to the maximum possible i.e., three. The third policy, CHAR-C4-MIN, updates the age of the block to the minimum possible i.e., zero. All three policies continue to fill cache blocks from memory directly into the L2 cache and a block is considered for allocation in the LLC only when it is evicted from the L2 cache and the dead hint detector identifies it as live. Among these three policies, CHAR-C4-MIN offers the best protection for C4 blocks and is expected to have the highest bypass rate (percentage of blocks dropped by the L2 cache) because it reduces the chance of a C4 block getting evicted from the LLC before it is evicted from the L2 cache. On the other hand, CHAR-C4-MAX is expected to have a bypass rate that is higher than CHAR but lower than CHAR-C4 and CHAR-C4-MIN. Since all these three policies reduce the effective on-die cache capacity, their relative performance depends on the sensitivity of the workloads toward cache capacity and on-die interconnect bandwidth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EVALUATION METHODOLOGY</head><p>We carry out our execution-driven simulations on a MIPS ISA simulator. We model an out-of-order issue core running at 4 GHz with 4-wide front-end and 8-wide commit. The core model has an 18-stage pipeline with 128-entry re-order buffer, 160-entry integer and floating-point register files, and a hybrid branch predictor similar to the Alpha 21264 tournament predictor. The front-end of the core has a 256-set 4-way branch target buffer, a 32-entry return address stack, and a 32-entry branch stack allowing 32 in-flight branch instructions. The issue unit of the core has a 32-entry integer queue, a 32-entry floating-point queue, and a 64-entry load/store queue. The execution unit of each core is equipped with six integer ALUs, three FPUs, and two load/store units.</p><p>Each core has private L1 and L2 caches. The L1 instruction and data caches are 32 KB 8-way with LRU replacement policy. In the inclusive LLC model, the capacity ratio between the LLC and the L2 cache is maintained at 8:1 in all our configurations to keep the inclusion overhead low <ref type="bibr">[7]</ref>. Our single-threaded applications are simulated on a 256 KB 8-way L2 cache with LRU replacement policy and a 2 MB 16-way LLC. The 4-way multi-programmed workloads are simulated on a 4-core model with each core having a 256 KB 8-way private L2 cache. The LLC is 8 MB 16-way and shared among all the cores. The 8-way threaded shared memory programs are simulated on an 8-core model with each core having a 128 KB 8-way private L2 cache. The LLC is 8 MB 16-way and shared among all the cores. In all the multi-core configurations, the LLC banks and the cores sit on a bidirectional ring that has a single-cycle hop time. Each hop has a core and an LLC bank. The L2 cache of a core connects to the router in each ring hop through a 32-entry outgoing message queue. The LLC in the single-threaded and multi-programmed models has banks of size 2 MB with access latency of eight cycles (tag+data). The model used to evaluate the shared memory programs has 1 MB LLC banks with seven-cycle access latency (tag+data). Each LLC bank can keep track of 16 outstanding misses. All the levels in the cache hierarchy have 64-byte block size. It is clear that this overhead is a small percentage (less than 0.1%) of the total storage devoted to the cache hierarchy. The CHAR-S implementation would need one extra shared bit per LLC block in the Shm configuration. This would increase the overhead of the Shm configuration to 20.86 KB, which is still less than 0.3% of the LLC capacity.</p><p>We model an aggressive memory system with four singlechannel memory controllers clocked at 2 GHz with a round robin mapping of consecutive cache blocks on the controllers. Each controller implements the FR-FCFS scheduling policy and connects to a DIMM (64-bit interface) built out of 8way banked DDR3-1600 DRAM chips. The 800 MHz DRAM part has burst length of eight and 9-9-9-27 (tCAS-tRCD-tRP-tRAS) access cycle parameters.</p><p>We model per-core multi-stream stride prefetchers that keep track of sixteen simultaneous streams and prefetch into the LLC and the L2 cache. The prefetcher attached to the L2 cache injects a prefetch request on an L2 cache miss as well as on a demand hit to a prefetched L2 cache block.</p><p>The exclusive LLC model uses the same configuration as the inclusive LLC model. Since there is no inclusion overhead in exclusive LLC models, the per-core private L2 cache capacity is increased to 512 KB (with a corresponding increase in access latency) to improve baseline performance.  <ref type="figure" target="#fig_0">1</ref>). The applications also represent a wide range of LLC misses per kilo instructions (MPKI) for the baseline SRRIP policy, as shown in Table <ref type="table" target="#tab_4">6</ref>. We simulate one billion dynamic instructions chosen using the SimPoint toolset <ref type="bibr" target="#b22">[22]</ref> from each application executed on the ref input set.</p><p>The single-threaded applications are mixed to prepare one hundred 4-way heterogeneous multi-programmed workloads. In each workload, each thread is executed for 500 million representative dynamic instructions. If a thread finishes executing its set of instructions early, it continues to run beyond that point so that the LLC contention can be modeled correctly. However, performance is reported based on the first 500 million instructions retired by each thread.</p><p>We pick six shared memory kernels and applications for preliminary evaluation in this paper. These are FFT (256K complex double points), Radix-sort (4M points, radix 32), and Ocean (514?514 grid) from SPLASH-2, Art (MinneSPEC input <ref type="bibr" target="#b15">[15]</ref>) and Equake (MinneSPEC input, ARCHduration 0.5) from SPEC OMP, and FFTW (4096?16?16 complex double points) <ref type="bibr" target="#b3">[3]</ref>. All the applications are executed in entirety on eight cores (one thread per core).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">SIMULATION RESULTS</head><p>We present the results for the single-threaded applications on the inclusive LLC model first with the hardware prefetcher disabled to understand how each application performs in isolation. Next we discuss the multi-core results for heterogeneous workload mixes as well as shared memory applications with and without the hardware prefetcher enabled. We conclude this section with a discussion of the results for the exclusive LLC model with the hardware prefetcher enabled. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Inclusive LLC Model</head><p>In the following, we discuss the performance of CHAR on the inclusive LLC model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Single-threaded Applications</head><p>Figure <ref type="figure" target="#fig_8">7</ref> compares the performance of CHAR, DRRIP <ref type="bibr" target="#b8">[8]</ref>, CHAR-PC, SDBP <ref type="bibr" target="#b12">[12]</ref>, and SHiP-PC <ref type="bibr" target="#b24">[24]</ref>. The last three policies require the program counter of the load/store instructions. SDBP is the state-of-the-art dead block prediction technique that correlates death of a cache block with the last-touch PC of the block. We do not exercise the LLC bypass component of SDBP so that strict inclusion is maintained between the LLC and the L2 cache. SHiP-PC is the state-of-the-art PC-correlated LLC insertion policy that improves the re-reference interval prediction of SRRIP and DR-RIP. The upper panel of Figure <ref type="figure" target="#fig_8">7</ref> compares the policies in terms of the number of LLC misses (lower is better) normalized to SRRIP, while the lower panel shows normalized IPC (higher is better). CHAR enjoys noticeable reductions in LLC misses in art, equake, bzip2, mcf, hmmer, libquantum, h264ref, and sphinx3. On average, CHAR reduces the number of LLC misses by 10.9%, while DRRIP exhibits a 5.8% reduction in LLC misses. The reduction in LLC misses achieved by CHAR-PC is 12.4%. Interestingly, CHAR achieves reductions in LLC misses similar to the SDBP and SHiP-PC policies without requiring any program counter information. This further confirms the finding of Section 2.2 that our L2 cache block classification captures the code space signatures quite well. Referring back to Figure <ref type="figure" target="#fig_0">1</ref>, we find that CHAR and CHAR-PC still leave significant room for improvement.</p><p>As shown in the lower panel of Figure <ref type="figure" target="#fig_8">7</ref>, the IPC gains correspond well with the LLC miss reductions. The IPC improvements achieved by CHAR and CHAR-PC are 3.8% and 4.4%, respectively. The corresponding improvements of DR-RIP, SDBP, and SHiP-PC are 2.7%, 3.5%, and 3.8%, respec-tively. Given the small benefit of CHAR-PC in comparison with CHAR and the added complexity of CHAR-PC, we do not pursue CHAR-PC any further. To further understand the sources of LLC hits in CHAR and DRRIP, Table <ref type="table" target="#tab_5">7</ref> shows the average distribution of LLC hits normalized to CHAR among the L2 cache block classes and instructions for the non-prefetched execution of the singlethreaded applications. To collect this data, every block evicted from the L2 cache is classified as C1, C2, C3, C4, or instruction and this is also recorded in the LLC. A subsequent LLC hit to such a block belonging to a category increments the hit count of the corresponding category. We show the average of this hit distribution for CHAR, DRRIP, SRRIP, oracleassisted SRRIP (SRRIP-Or), and Belady's optimal policy normalized to CHAR. The last two rows correspond to the two policies discussed in Figure <ref type="figure" target="#fig_0">1</ref>. Compared to SRRIP, both DRRIP and CHAR significantly improve the volume of hits enjoyed by the C4 blocks (recall that the C4 blocks are predominantly live). DRRIP achieves this while sacrificing some of the hits in the C1 class. The RRPV duel of DRRIP inserts all blocks in the LLC with RRPV of either two or three during an execution phase. Therefore, it is expected that during the phases dominated by a mix of C1, C2, and C3 blocks, DRRIP would probably insert all blocks with RRPV three, even though inserting some of these with RRPV two could have improved performance. CHAR can enjoy such selectivity because it learns to send dead hints based on the reuse probabilities of different classes of blocks. As a result, it improves the volume of hits across all the four classes. Overall, DRRIP and SRRIP experience 11% and 32% less LLC hits compared to CHAR. The last two rows of Table <ref type="table" target="#tab_5">7</ref> point out that further characterization of the C1, C2, and C3 classes is necessary to exploit the remaining performance potential. Before closing the discussion on the single-threaded applications, we examine the L2-LLC address traffic in the presence of the dead hints generated by CHAR. We divide the traffic into three parts: a) L2 cache miss requests and the L2 cache fills, b) L2 cache evictions (in SRRIP, these are only the dirty block evictions, while in CHAR, these also include the eviction messages to the LLC sample sets and the dead hints to the non-sample sets), c) back-invalidations and their acknowledgments exchanged at the time of an LLC eviction.</p><p>The upper panel of Figure <ref type="figure" target="#fig_9">8</ref> shows the address traffic in the L2-LLC interconnect normalized to SRRIP. Both SRRIP and CHAR have the same volume of traffic due to L2 cache misses and L2 cache fills. Although CHAR increases the L2 cache eviction traffic (as expected), it dramatically reduces the traffic due to back-invalidations. The reduction in backinvalidations in CHAR is observed for two reasons. First, due to less number of LLC misses in CHAR, LLC evictions are less in number leading to a lower volume of back-invalidations. Second, a dead hint message clears the coherence bitvector position of the core sending the dead hint. When such a block is evicted from the LLC, no back-invalidation is generated. On average, CHAR enjoys 10% less address traffic on the L2-LLC interconnect compared to SRRIP. The lower panel of Figure <ref type="figure" target="#fig_9">8</ref> further quantifies the fraction of L2 cache eviction addresses sent to the LLC by SRRIP and CHAR. On average, SRRIP sends 38.9% of all L2 cache evictions to the LLC (these are dirty cache block evictions) and CHAR sends 69.5% of all L2 cache evictions to the LLC. It is important to note that a dead hint is a dataless message and inflates only the address traffic, unlike the dirty block evictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Multi-core Workloads</head><p>For heterogeneous workload mixes, we present three different metrics to summarize the performance and fairness of several policies. Normalized average IPC (or throughput) i.e., i IP C N EW i i IP C SRRIP i summarizes the improvement in average throughput of the mix when a policy "NEW" replaces the baseline SRRIP, where IP Ci is the IPC of thread i (0 ? i ? 3). We use mini</p><formula xml:id="formula_4">IP C N EW i IP C SRRIP i</formula><p>as a conservative fairness metric that captures the minimum throughput improvement enjoyed by any thread in a mix. A policy "NEW" is considered at least as fair as the baseline SRRIP policy if this metric is at least 1.0 meaning that no thread has suffered from a slowdown due to introduction of the new policy. Finally, we evaluate the normalized LLC miss count as</p><formula xml:id="formula_5">i M N EW i i M SRRIP i</formula><p>, where Mi is the number of LLC misses experienced by thread i during the execution of its first 500 million instructions. For shared memory applications, we execute each application completely and compare the policies in terms of the execution time of the parallel computation.   <ref type="bibr" target="#b8">[8]</ref>, SDBP <ref type="bibr" target="#b12">[12]</ref>, SHiP-PC <ref type="bibr" target="#b24">[24]</ref>, ECI <ref type="bibr">[7]</ref>, and QBS <ref type="bibr">[7]</ref>. These data represent the average across one hundred heterogeneous mixes with the hardware prefetcher disabled. CHAR improves average throughput by 3.9% and reduces the LLC misses by 6.8%. Surprisingly, CHAR performs slightly better than SDBP and SHiP-PC even though the latter policies use the PCs for the load/store instructions that access the LLC and fill into the LLC, respectively. The SDBP and SHiP-PC policies significantly reduce the LLC miss counts of several non-memoryintensive mixes and as a result, these two policies save more LLC misses compared to CHAR, but they fail to convert these savings into throughput improvement. These results once again underscore the effectiveness of CHAR in capturing the code space signatures of LLC access patterns. CHAR does not have the complexity of carrying the PC information through the load-store pipeline, the L1 and L2 cache controllers, the on-die interconnect, and the LLC controller. Also, the SDBP and SHiP-PC policies need additional storage to maintain the PC signatures and the predictor tables.</p><p>Thread-aware DRRIP (TADRRIP) improves throughput by 3.1% and reduces the LLC miss count by 3.1%, on average. In Section 4.1.1, we have already explained how the fine-grain cache block classification of CHAR helps it enjoy more LLC hits compared to the DRRIP policy.</p><p>Finally, for completeness, we discuss the results of ECI and QBS (these policies were discussed in Section 1.1). For our workload mixes and simulated configurations, we do not expect ECI or QBS to deliver noticeable performance improvements. Only 2.5% of the back-invalidations sent at the time of LLC evictions in baseline SRRIP hit in the L1 or the L2 cache. As a result, at the time of an LLC eviction, the block has already been evicted from the core caches with a likelihood of 0.975. Hence, it is impossible for QBS to infer much about the temporal locality of the block by querying the core caches. Overall, QBS improves the average throughput by only 0.23% compared to the baseline SRRIP. For ECI, only 1.5% of early-invalidated blocks are recalled by the cores before they are evicted from the LLC leading to an average throughput improvement of 0.46% compared to the baseline SRRIP. The original study <ref type="bibr">[7]</ref> reported that QBS performs better than ECI and we believe that we observe the opposite trend due to selection of different workload mixes. Figure <ref type="figure" target="#fig_12">10</ref> shows the details of the normalized average and minimum throughput delivered by CHAR for each of the mixes. The mixes are arranged in the increasing order of throughput improvement. The average throughput profile ranges from a loss of 1.0% to an improvement of 9.5%. The minimum throughput profile also looks very encouraging as only 11 of the 100 mixes have minimum improvement below 1.0. The minimum throughput improvement of any thread ranges from 0.98 to 1.04, averaging at 1.01. Overall, CHAR remains at least as fair as SRRIP while offering a throughput improvement of 3.9% and an LLC miss saving of 6.8%. Figure <ref type="figure" target="#fig_13">11</ref> presents the parallel execution time (lower is better) of CHAR-S, DRRIP, TADRRIP, SDBP, and SHiP-PC normalized to SRRIP for the shared memory applications. CHAR-S achieves noticeable savings in FFTW, Radix-sort, Art, and Equake. On average, it saves 4.2% execution time compared to the baseline. This performance gain comes from an 11.1% reduction in LLC misses. The reductions in execution time achieved by DRRIP, TADRRIP, SDBP, and SHiP-PC are 1.8%, 0.4%, 0.4%, and 2.1%, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Interaction with Hardware Prefetching</head><p>All the results presented up to this point pertain to configurations with the hardware prefetcher disabled. In this section, we evaluate the performance of our policy proposal in the presence of a multi-stream hardware prefetcher. This would also validate the efficiency of our algorithm in identifying and managing the blocks belonging to the C0 class. Prefetching alone improves the average throughput of SR-RIP by 20.6% for the hundred heterogeneous mixes. Table <ref type="table" target="#tab_6">8</ref> further compares CHAR and PACMan-DYN-Global <ref type="bibr" target="#b25">[25]</ref> for these mixes in terms of average throughput improvement, the number of LLC demand misses saved, and the total volume of DRAM requests relative to SRRIP with the hardware prefetcher enabled. PACMan is a family of RRPV insertion and RRPV update policies that was shown to outperform DRRIP and SDBP in the presence of hardware prefetching <ref type="bibr" target="#b25">[25]</ref>. CHAR improves average throughput by 5.3% and significantly improves the effectiveness of the prefetcher by reducing the LLC demand miss count by 15.0% compared to the prefetched SRRIP. We find that compared to SRRIP, the CHAR algorithm evicts 11.3% less C0 blocks from LLC. This is primarily due to dynamic learning of the usefulness of C0 blocks in the LLC. The prefetched blocks that experience demand hits much later than they are prefetched into the hierarchy must be identified and retained in the LLC. These are a subset of the C0 blocks. CHAR dynamically monitors the collective reuse probability of the C0 blocks and learns to retain them if they are useful.</p><p>PACMan-DYN-Global delivers a throughput improvement of 2.8% compared to prefetched SRRIP. This policy dynamically selects one of the two algorithms, namely, PACMan-H and PACMan-HM. None of these policies upgrade the RRPV of a block on prefetch hits in the LLC. As a result, the blocks that can potentially enjoy multiple prefetch hits in the LLC followed by demand hits in the L2 cache get prematurely evicted from the LLC. Note that even though CHAR does not increment the live counter of C0 class if a C0 block experiences a prefetch hit in the LLC, it does increment the live counters of C1, C2, and C3 classes if a block belonging to any of these three experiences a prefetch hit in the LLC. Further, CHAR always upgrades the RRPV of a block in the LLC on a prefetch hit following the SRRIP policy. A common pattern experienced by a live block in CHAR is the following.</p><p>The block is prefetched into the L2 cache (filled as C0 in the L2 cache). It enjoys demand hit(s) in the L2 cache and is evicted as a C1, C2, or C3 block from the L2 cache. Later the block is again prefetched from the LLC and it enjoys further demand hits. While CHAR can retain such blocks in the LLC, PACMan-DYN-Global fails to do so because it does not upgrade the RRPVs on prefetch hits in the LLC.</p><p>Overall, we find that PACMan-DYN-Global is too aggressive in filtering prefetch-induced LLC pollution and in the process it often loses useful prefetched blocks early resulting in an eventual increase in the memory controller congestion because several prematurely evicted useful prefetches will have to be fetched/prefetched again. This fact is substantiated by the last row of Table <ref type="table" target="#tab_6">8</ref>. CHAR saves 8.8% DRAM requests compared to SRRIP, while PACMan-DYN-Global increases the memory traffic by 4.8%. The increased pressure on the memory controllers leads to loss in performance for several mixes when running with PACMan-DYN-Global. Figure <ref type="figure" target="#fig_14">12</ref> shows the details of throughput improvement and fairness of CHAR for the heterogeneous mixes relative to SR-RIP with the hardware prefetcher enabled. The mixes are arranged in the increasing order of throughput improvement. The throughput profile ranges from a loss of 1.2% to an improvement of 21.3%. The average throughput improvement achieved by CHAR is 5.3%. The minimum throughput improvement of any thread ranges from 0.96 to 1.13, averaging at 1.02. Only 16 mixes have minimum throughput improvement below 1.0. These data confirm that even in the presence of prefetching, CHAR continues to achieve significant performance improvement compared to SRRIP.</p><p>For the shared memory parallel programs, prefetching alone reduces the execution time of SRRIP by 15.6%. CHAR-S achieves a further 2.2% reduction in execution time relative to prefetched SRRIP. Thread-oblivious PACMan-DYN <ref type="bibr" target="#b25">[25]</ref> and thread-aware PACMan-DYN-Global respectively achieve 1.1% and 0.4% reduction in parallel execution time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Exclusive LLC Model</head><p>We summarize our results for the exclusive LLC model in Figure <ref type="figure" target="#fig_16">13</ref>. All the simulations are done with the hardware prefetcher enabled. The leftmost panel shows the average throughput of one hundred 4-way multi-programmed mixes for a non-inclusive (NI) LLC model <ref type="bibr">[7,</ref><ref type="bibr" target="#b23">23]</ref>  The middle panel of Figure <ref type="figure" target="#fig_16">13</ref> shows the bypass fractions for CHAR, CHAR-C4, CHAR-C4-MAX, and CHAR-C4-MIN. This fraction corresponds to the fraction of L2 cache evictions dropped by the L2 cache controller and not sent to the LLC or memory controllers in the exclusive LLC model. While CHAR bypasses 14.8% blocks on average, the other three policies that switch the C4 blocks to non-inclusive/non-exclusive mode enjoy more than 60% bypass rates. CHAR-C4 bypasses 66.6% of the L2 cache evictions.</p><p>The rightmost panel of Figure <ref type="figure" target="#fig_16">13</ref> quantifies the number of data write transactions from the L2 cache to the on-die interconnect in the exclusive LLC policies relative to the inclusive LLC model. For an exclusive LLC, these are the data transactions to LLC or memory controllers originating from L2 cache evictions that could not be bypassed. For an inclusive LLC, these are dirty writebacks from the L2 cache to the LLC. While TC-AGE generates 3.6 times data write transactions relative to the inclusive LLC model, the CHAR-C4 policy generates only 17% more writes to the LLC. As mentioned in Section 2.3, the TC-AGE policy does not exercise any bypass algorithm. Overall, CHAR-C4 is the best policy among the ones we have evaluated for an exclusive LLC. It improves the average throughput by 8.2% while generating only 17% more data writes to the LLC compared to an identically sized inclusive LLC hierarchy.  Figure <ref type="figure" target="#fig_18">14</ref> shows the details of throughput (upper panel) and bypass fraction (lower panel) of the CHAR-C4 policy for the heterogeneous mixes. The throughput is normalized to the TC-AGE policy. The mixes are arranged in the increasing order of normalized throughput. The throughput profile varies from a loss of 5.1% to a gain of 13.3%, while the bypass fraction is between 0.37 and 0.81. In other words, the CHAR-C4 policy can save 37% to 81% (average 66.6%) on-die data write traffic originating from the L2 cache evictions while delivering a throughput improvement of up to 13.3% (average 3.2%) relative to TC-AGE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">SUMMARY</head><p>This paper proposes hierarchy-aware replacement and allocation/bypass policies for LLCs in a three-level cache hierarchy. The proposal uses a carefully chosen subset of the L2 cache evictions as hints to improve the quality of replacement and allocation in inclusive and exclusive LLCs, respectively. When a data block is evicted from the L2 cache, it is passed on to a dead hint detector. The dead hint detector decides if the block should be marked a potential victim in an inclusive LLC, provided the block has already been evicted from the L1 data cache. A similar decision is used to identify L2 cache evictions that need not be allocated in an exclusive LLC and can be bypassed. Central to the dead hint detector logic is an approximate reuse distance-based classification of L2 cache blocks and estimation of reuse probability of each of these classes. If the reuse probability of a class falls below a threshold, any block belonging to that class is marked a potential victim/bypass candidate for inclusive/exclusive LLC when the block is evicted from the L2 cache.</p><p>We evaluate our policy proposal on single-threaded, multiprogrammed, and shared memory workloads. In an inclusive LLC, our cache hierarchy-aware algorithm offers an average throughput improvement of 5.3% for one hundred 4-way multi-programmed mixes compared to a baseline SRRIP policy with a well-tuned multi-stream hardware prefetcher enabled. Our best proposal for exclusive LLC improves the average throughput of one hundred 4-way multi-programmed mixes by 8.2% compared to an identical inclusive LLC hierarchy while bypassing 66.6% of the L2 cache evictions. As a result of this high bypass rate, this policy introduces only 17% more data write transactions into the on-die interconnect compared to an identical inclusive LLC hierarchy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Number of LLC misses with oracle-assisted LLC replacement policies normalized to baseline SRRIP in an inclusive LLC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: L2 cache evictions that are not recalled from the LLC in baseline SRRIP policy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Implementation of CHAR algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: State transitions among the classes of the L2 cache blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Fraction of dead evictions in each L2 cache block class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>The approximation is done by dividing the possible values of E4 into four ranges, namely, [0, NE/8], (NE/8, NE/4], (NE/4, NE/2], and (NE/2, NE]. On each L2 cache eviction, we first determine the range the current value of E4 falls in. If it falls in the lowermost range i.e., [0, NE/8],</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>|PC 1 ?|PC 1 ?Figure 6 :</head><label>116</label><figDesc>Figure 6: Relationship between L2 cache block classes and code space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Upper panel: Number of LLC misses normalized to SRRIP. Lower panel: IPC normalized to SRRIP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Upper panel: Address traffic in L2-LLC interconnect normalized to SRRIP. Lower panel: L2 cache eviction addresses sent to LLC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Left panel: Normalized average throughput comparison (higher is better). Right panel: Normalized average LLC miss count comparison (lower is better).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9</head><label>9</label><figDesc>Figure9summarizes the normalized average throughput and LLC miss counts delivered by CHAR, TADRRIP<ref type="bibr" target="#b8">[8]</ref>, SDBP<ref type="bibr" target="#b12">[12]</ref>, SHiP-PC<ref type="bibr" target="#b24">[24]</ref>, ECI[7], and QBS[7]. These data represent the average across one hundred heterogeneous mixes with the hardware prefetcher disabled. CHAR improves average throughput by 3.9% and reduces the LLC misses by 6.8%. Surprisingly, CHAR performs slightly better than SDBP and SHiP-PC even though the latter policies use the PCs for the load/store instructions that access the LLC and fill into the LLC, respectively. The SDBP and SHiP-PC policies significantly reduce the LLC miss counts of several non-memoryintensive mixes and as a result, these two policies save more</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Performance and fairness of CHAR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Comparison of parallel execution time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Performance and fairness of CHAR with prefetcher enabled.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>and TC-AGE, CHAR, CHAR-C4, CHAR-C4-MAX, and CHAR-C4-MIN policies implemented on the exclusive LLC model. The throughput results are normalized to an inclusive LLC model. The inclusive, non-inclusive, and exclusive LLC models have identically designed cache hierarchies (32 KB 8-way private L1 instruction and data caches, 512 KB 8-way private L2 cache, 8 MB 16-way shared LLC). The inclusive and the noninclusive LLCs implement the two-bit SRRIP policy. The NI LLC model is identical to the inclusive LLC model, except that on an LLC eviction the NI model does not invalidate the copy of the block in the L2 and L1 caches. The performance gain of the NI model over the inclusive model arises from elimination of back-invalidations, while the performance gap between the NI model and the TC-AGE policy running on the exclusive LLC model stems from the added effective capacity of the exclusive LLC. Overall, CHAR-C4 delivers the best performance improving the average throughput by 8.2% over the inclusive model and 3.2% over the TC-AGE exclusive model. The CHAR, CHAR-C4-MAX, and CHAR-C4-MIN policies deliver performance close to CHAR-C4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Throughput, bypass fraction, and L2 cache to interconnect data traffic in exclusive LLC model for multi-programmed workloads with prefetcher enabled.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Throughput normalized to TC-AGE and bypass fraction of CHAR-C4 policy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>L2 cache block classification</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>L2 cache block eviction flow (address A) Class Maps to LLC sample set Does not map to LLC sample set C k , k ? {0, 1, 2, 3} E k ++, NE++, send A and k to LLC Invoke dead block detection algorithm C k , k = 4 E4++, NE++, send A and k to LLC -</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>L2 cache block fill flow (last class id k, if hit in LLC sample set) 4}, fill in L2 cache as C4 Fill in L2 cache as C4 Prefetch miss in LLC Fill in L2 cache as C0 Fill in L2 cache as C0 rate of the blocks belonging to class C k , as learned from the LLC sample sets executing the baseline SRRIP policy. Notice, however, that a prefetch fill in the L2 cache does not update the live counter of C0, even if the filled block hits in an LLC sample set (third row of Table 4). This is because such a block was filled into the L2 cache last time by a prefetch request and got evicted as a C0 block without experiencing a demand</figDesc><table><row><cell>Fill attribute</cell><cell>Maps to LLC sample set</cell><cell>Does not map to LLC sample set</cell></row><row><cell>Demand hit in LLC</cell><cell>L k ++ if k = 4, fill in L2 cache as C4</cell><cell>Fill in L2 cache as C4</cell></row><row><cell cols="2">Demand miss in LLC Fill in L2 cache as C1</cell><cell>Fill in L2 cache as C1</cell></row><row><cell>Prefetch hit in LLC</cell><cell>L k ++ if k / ? {0,</cell><cell></cell></row></table><note><p>hit. If this block is again prefetched into the L2 cache, we speculate that this is likely to be a premature or incorrect prefetch and do not update the live counter of C0.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Storage overhead of CHAR</figDesc><table><row><cell>States</cell><cell>ST</cell><cell>Mprog</cell><cell>Shm</cell></row><row><cell>States in L2C</cell><cell cols="3">8192 bits 32768 bits 32768 bits</cell></row><row><cell cols="2">Counters in L2C 112 bits</cell><cell>448 bits</cell><cell>896 bits</cell></row><row><cell>LLC samples</cell><cell cols="2">1536 bits 6144 bits</cell><cell>6144 bits</cell></row><row><cell>TOTAL</cell><cell>1.20 KB</cell><cell>4.80 KB</cell><cell>4.86 KB</cell></row><row><cell cols="4">Table 5 summarizes the extra storage overhead of CHAR</cell></row><row><cell cols="4">(as discussed in Sections 2.1.3 and 2.1.4) for the three cache</cell></row><row><cell cols="4">configurations used to evaluate single-threaded (ST), multi-</cell></row><row><cell cols="4">programmed (Mprog), and shared memory (Shm) workloads.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Baseline inclusive LLC MPKI</figDesc><table><row><cell cols="2">mgrid applu</cell><cell>art</cell><cell>equake</cell><cell>gap</cell><cell>bzip2</cell></row><row><cell>2.92</cell><cell>4.62</cell><cell>6.51</cell><cell>13.93</cell><cell>3.60</cell><cell>1.18</cell></row><row><cell>gcc</cell><cell>mcf</cell><cell>hmmer</cell><cell>libq</cell><cell cols="2">h264ref sphinx3</cell></row><row><cell>5.35</cell><cell>28.84</cell><cell>0.34</cell><cell>11.38</cell><cell>0.31</cell><cell>7.43</cell></row><row><cell cols="6">We select twelve single-threaded applications (shown in Fig-</cell></row><row><cell cols="6">ure 1) from SPEC 2000 and SPEC 2006 suites. The selected</cell></row><row><cell cols="6">twelve applications represent a fair distribution of LLC miss</cell></row><row><cell cols="6">savings achievable by Belady's optimal algorithm. The sav-</cell></row><row><cell cols="6">ings range from 1.1% in gap to 55.9% in sphinx3 (see Fig-</cell></row><row><cell>ure</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>LLC hit distribution</figDesc><table><row><cell>Policy</cell><cell>C1</cell><cell>C2</cell><cell>C3</cell><cell>C4</cell><cell>Ins. Total</cell></row><row><cell>CHAR</cell><cell cols="5">0.11 0.08 0.04 0.76 0.01 1.00</cell></row><row><cell>DRRIP</cell><cell cols="5">0.07 0.05 0.01 0.75 0.01 0.89</cell></row><row><cell>SRRIP</cell><cell cols="5">0.09 0.05 0.01 0.52 0.01 0.68</cell></row><row><cell cols="6">SRRIP-Or 0.26 0.13 0.21 0.78 0.04 1.42</cell></row><row><cell>Belady</cell><cell cols="5">0.34 0.17 0.14 0.85 0.02 1.52</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 :</head><label>8</label><figDesc>Comparison between CHAR and PACMan-DYN-Global relative to SRRIP</figDesc><table><row><cell>Metric</cell><cell>CHAR</cell><cell>PACMan-DYN-Global</cell></row><row><cell>Throughput</cell><cell>5.3%</cell><cell>2.8%</cell></row><row><cell>improvement</cell><cell></cell><cell></cell></row><row><cell>LLC demand</cell><cell>15.0%</cell><cell>11.5%</cell></row><row><cell>misses saved</cell><cell></cell><cell></cell></row><row><cell cols="2">DRAM requests 8.8% less</cell><cell>4.8% more</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>SRRIP is known to outperform NRU and LRU<ref type="bibr" target="#b8">[8]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We do not apply our policy proposal to instruction blocks.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>In our simulation model, the prefetched blocks are brought into the LLC and the L2 cache, but not into the L1 cache.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>We simulate a bitvector-based directory coherence protocol. The L1 and L2 caches are private to each core and the LLC is shared. Each LLC tag is extended to maintain the coherence states and sharing bitvector in the inclusive LLC design.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>We use the term "age" instead of "RRPV" in this discussion to conform to the terminology used in the prior work on exclusive LLC management<ref type="bibr" target="#b5">[5]</ref>. Age can be considered synonymous to RRPV in this discussion.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6.">ACKNOWLEDGMENTS</head><p>This research effort is funded by <rs type="funder">Intel Corporation</rs>. The authors thank <rs type="person">Praveen Vishakantaiah</rs> from <rs type="funder">Intel India</rs> and <rs type="person">Koby Gottlieb</rs> from <rs type="funder">Intel Israel</rs> for their financial support and continued encouragement.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Study of Replacement Algorithms for a Virtual-storage Computer</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Belady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IBM Systems Journal</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="78" to="101" />
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pseudo-LIFO: The Foundation of a New Family of Replacement Policies for Last-level Caches</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chaudhuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International Symposium on Microarchitecture</title>
		<meeting>the 42nd International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2009-12">December 2009</date>
			<biblScope unit="page" from="401" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The Design and Implementation of FFTW3</title>
		<author>
			<persName><forename type="first">M</forename><surname>Frigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2005-02">February 2005</date>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="216" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deconstructing the Inefficacy of Global Cache Replacement Policies</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Garde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Subramaniam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th Annual Workshop on Duplicating, Deconstructing, and Debunking, held in conjunction with the 35th International Symposium on Computer Architecture</title>
		<imprint>
			<date type="published" when="2008-06">June 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bypass and Insertion Algorithms for Exclusive Last-level Caches</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Subramoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Symposium on Computer Architecture</title>
		<meeting>the 38th International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2011-06">June 2011</date>
			<biblScope unit="page" from="81" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Timekeeping in the Memory System: Predicting and Optimizing Memory Behavior</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaxiras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martonosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Symposium on Computer Architecture</title>
		<meeting>the 29th International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2002-05">May 2002</date>
			<biblScope unit="page" from="209" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Achieving Non-Inclusive Cache Performance with Inclusive Caches</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International Symposium on Microarchitecture</title>
		<meeting>the 43rd International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2010-12">December 2010</date>
			<biblScope unit="page" from="151" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">High Performance Cache Replacement using Re-reference Interval Prediction (RRIP)</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Symposium on Computer Architecture</title>
		<meeting>the 37th International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2010-06">June 2010</date>
			<biblScope unit="page" from="60" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive Insertion Policies for Managing Shared Caches</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Parallel Architecture and Compilation Techniques</title>
		<meeting>the 17th International Conference on Parallel Architecture and Compilation Techniques</meeting>
		<imprint>
			<date type="published" when="2008-10">October 2008</date>
			<biblScope unit="page" from="208" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cache Replacement Based on Reuse Distance Prediction</title>
		<author>
			<persName><forename type="first">G</forename><surname>Keramidas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Petoumenos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaxiras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Computer Design</title>
		<meeting>the 25th International Conference on Computer Design</meeting>
		<imprint>
			<date type="published" when="2007-10">October 2007</date>
			<biblScope unit="page" from="245" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Decoupled Dynamic Cache Segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Symposium on High Performance Computer Architecture</title>
		<meeting>the 18th International Symposium on High Performance Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2012-02">February 2012</date>
			<biblScope unit="page" from="235" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dead Block Replacement and Bypass with a Sampling Predictor</title>
		<author>
			<persName><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International Symposium on Microarchitecture</title>
		<meeting>the 43rd International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2010-12">December 2010</date>
			<biblScope unit="page" from="175" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Using Dead Blocks as a Virtual Victim Cache</title>
		<author>
			<persName><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Parallel Architectures and Compilation Techniques</title>
		<meeting>the 19th International Conference on Parallel Architectures and Compilation Techniques</meeting>
		<imprint>
			<date type="published" when="2010-09">September 2010</date>
			<biblScope unit="page" from="489" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Counter-based Cache Replacement and Bypassing Algorithms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kharbutli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Solihin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="433" to="447" />
			<date type="published" when="2008-04">April 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">MinneSPEC: A New SPEC Benchmark Workload for Simulation-Based Computer Architecture Research</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kleinosowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Lilja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Computer Architecture Letters</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2002-01">January 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dead-block Prediction &amp; Dead-block Correlating Prefetchers</title>
		<author>
			<persName><forename type="first">A-C</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Symposium on Computer Architecture</title>
		<meeting>the 28th International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2001-07">June/July 2001</date>
			<biblScope unit="page" from="144" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">TAP: A TLP-aware Cache Management Policy for a CPU-GPU Heterogeneous Architecture</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Symposium on High Performance Computer Architecture</title>
		<meeting>the 18th International Symposium on High Performance Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2012-02">February 2012</date>
			<biblScope unit="page" from="91" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cache Bursts: A New Approach for Eliminating Dead Blocks and Increasing Cache Efficiency</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st International Symposium on Microarchitecture</title>
		<meeting>the 41st International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2008-11">November 2008</date>
			<biblScope unit="page" from="222" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">NUcache: An Efficient Multicore Cache Organization Based on Next-Use Distance</title>
		<author>
			<persName><forename type="first">R</forename><surname>Manikantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Govindarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th IEEE International Symposium on High-performance Computer Architecture</title>
		<meeting>the 17th IEEE International Symposium on High-performance Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2011-02">February 2011</date>
			<biblScope unit="page" from="243" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Evaluation Techniques for Storage Hierarchies</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Mattson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IBM Systems Journal</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="78" to="117" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adaptive Insertion Policies for High Performance Caching</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Symposium on Computer Architecture</title>
		<meeting>the 34th International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2007-06">June 2007</date>
			<biblScope unit="page" from="381" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Automatically Large Scale Program Behavior</title>
		<author>
			<persName><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the 10th International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2002-10">October 2002</date>
			<biblScope unit="page" from="45" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">FLEXclusion: Balancing Cache Capacity and On-chip Bandwidth with Flexible Exclusion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th IEEE/ACM International Symposium on Computer Architecture</title>
		<meeting>the 39th IEEE/ACM International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2012-06">June 2012</date>
			<biblScope unit="page" from="321" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SHiP: Signature-Based Hit Predictor for High Performance Caching</title>
		<author>
			<persName><forename type="first">C-J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International Symposium on Microarchitecture</title>
		<meeting>the 44th International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2011-12">December 2011</date>
			<biblScope unit="page" from="430" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">PACMan: Prefetch-Aware Cache Management for High Performance Caching</title>
		<author>
			<persName><forename type="first">C-J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International Symposium on Microarchitecture</title>
		<meeting>the 44th International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2011-12">December 2011</date>
			<biblScope unit="page" from="442" to="453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cache Replacement Policy Revisited</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zahran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th Annual Workshop on Duplicating, Deconstructing, and Debunking, held in conjunction with the 34th International Symposium on Computer Architecture</title>
		<imprint>
			<date type="published" when="2007-06">June 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Global Management of Cache Hierarchies</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zahran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Mckee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Conference on Computing Frontiers</title>
		<meeting>the 7th Conference on Computing Frontiers</meeting>
		<imprint>
			<date type="published" when="2010-05">May 2010</date>
			<biblScope unit="page" from="131" to="140" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
