<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pincer-Search: An Efficient Algorithm for Discovering the Maximum Frequent Set</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dao-I</forename><surname>Lin</surname></persName>
						</author>
						<author role="corresp">
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Zvi</forename><forename type="middle">M</forename><surname>Kedem</surname></persName>
							<email>zvi.kedem@nyu.edu</email>
						</author>
						<author>
							<persName><forename type="first">-I</forename><forename type="middle">D</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName><forename type="middle">Z M</forename><surname>Kedem</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Redback Networks, Inc</orgName>
								<address>
									<addrLine>350 Holger Way</addrLine>
									<postCode>95134-1362</postCode>
									<settlement>San Jose</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">New York University</orgName>
								<address>
									<addrLine>251 Mercer St</addrLine>
									<postCode>10012-1185</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pincer-Search: An Efficient Algorithm for Discovering the Maximum Frequent Set</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AC8B79E1D714C2EF8E410B7C7DBBEC29</idno>
					<note type="submission">received 15 July 1999; revised 8 Jan 2001; accepted 19 Jan 2001; posted to Digital Library 7 Sept. 2001.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index TermsÐData mining</term>
					<term>knowledge discovery</term>
					<term>association rule</term>
					<term>maximum frequent set</term>
					<term>Pincer Search</term>
					<term>maximum frequent candidate set</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>AbstractÐDiscovering frequent itemsets is a key problem in important data mining applications, such as the discovery of association rules, strong rules, episodes, and minimal keys. Typical algorithms for solving this problem operate in a bottom-up, breadth-first search direction. The computation starts from frequent 1-itemsets (the minimum length frequent itemsets) and continues until all maximal (length) frequent itemsets are found. During the execution, every frequent itemset is explicitly considered. Such algorithms perform well when all maximal frequent itemsets are short. However, performance drastically deteriorates when some of the maximal frequent itemsets are long. We present a new algorithm which combines both the bottom-up and the top-down searches. The primary search direction is still bottom-up, but a restricted search is also conducted in the top-down direction. This search is used only for maintaining and updating a new data structure, the maximum frequent candidate set. It is used to prune early candidates that would be normally encountered in the bottom-up search. A very important characteristic of the algorithm is that it does not require explicit examination of every frequent itemset. Therefore, the algorithm performs well even when some maximal frequent itemsets are long. As its output, the algorithm produces the maximum frequent set, i.e., the set containing all maximal frequent itemsets, thus specifying immediately all frequent itemsets. We evaluate the performance of the algorithm using well-known synthetic benchmark databases, real-life census, and stock market databases. The improvement in performance can be up to several orders of magnitude, compared to the best previous algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>K NOWLEDGE discovery in databases (KDD) has received increasing attention and has been recognized as a promising new field of database research. It is defined by Fayyad et al. <ref type="bibr" target="#b8">[9]</ref> as ªthe nontrivial process of identifying valid, novel, potentially useful, and ultimately understandable patterns in data.º The key step in the knowledge discovery process is the data mining step, ªconsisting of applying data analysis and discovery algorithms that, under acceptable computational efficiency limitations, produce a particular enumeration of patterns over the dataº <ref type="bibr" target="#b8">[9]</ref>. This paper addresses a pattern discovery problem.</p><p>A key component of many data mining problems is formulated as follows: Given a large database of sets of items (representing market basket data, alarm signals, etc.), discover all frequent itemsets (sets of items), where a frequent itemset is one that occurs in at least some userdefined percentage (minimum support) of the database. Depending on the semantics attached to the input database, the frequent itemsets and the term ªoccurs,º we get the key components of different data mining problems, such as the discovery of association rules (e.g., <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b21">[22]</ref>), theories (e.g., <ref type="bibr" target="#b10">[11]</ref>), strong rule (e.g., <ref type="bibr" target="#b24">[25]</ref>), episodes (e.g., <ref type="bibr" target="#b19">[20]</ref>), and minimal keys (e.g., <ref type="bibr" target="#b10">[11]</ref>).</p><p>Typical algorithms for finding the frequent set, i.e., the set of all frequent itemsets, operate in a bottom-up, breadth-first fashion (e.g., <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b28">[29]</ref>). The computation starts from frequent 1-itemsets (the minimum length frequent itemsets) at the bottom and then extends one level up in every pass until all maximal (length) frequent itemsets are discovered. All frequent itemsets are explicitly examined and discovered by these algorithms. When all maximal frequent itemsets are short, these algorithms perform well. However, performance drastically decreases when any of the maximal frequent itemsets becomes longer because a maximal frequent itemset of size l implies the presence of 2 l À 2 additional frequent itemsets (its nontrivial subsets) as well, each of which is explicitly examined by such algorithms. In data mining applications where items are correlated, maximum frequent itemsets could be long <ref type="bibr" target="#b7">[8]</ref>.</p><p>Therefore, instead of examining and ªassemblingº all the frequent itemsets, an alternative approach might be to ªshortcutº the process and attempt to search for maximal frequent itemsets ªmore directly,º as they immediately specify all frequent itemsets. Furthermore, it suffices to know only the maximal frequent set in many data mining applications, such as the minimal key discovery and the theory extraction <ref type="bibr" target="#b10">[11]</ref>.</p><p>Finding the maximum frequent set (or MFS), the set of all maximal frequent itemsets, is essentially a search problem in a hypothesis search space (a lattice of subsets). The search for the maximum frequent set can proceed from the 1-itemsets to n-itemsets (bottom-up) or from the n-itemsets to 1-itemsets (top-down).</p><p>We present a novel Pincer-Search algorithm, which searches for the MFS from both bottom-up and top-down directions. It performs well even when the maximal frequent itemsets are long.</p><p>The bottom-up search is similar to Apriori <ref type="bibr" target="#b2">[3]</ref> and OCD <ref type="bibr" target="#b21">[22]</ref> algorithms. However, the top-down search is novel. It is implemented efficiently by introducing an auxiliary data structure, the maximum frequent candidate set (or MFCS), as explained later. By incorporating the computation of the MFCS in our algorithm, we are able to efficiently approach the MFS from both top-down and bottom-up directions. Unlike the bottom-up search that goes up one level in each pass, the MFCS can help the computation ªmove downº many levels in a single pass in the top-down direction.</p><p>In this paper, we apply the MFCS structure to the association rule mining. In fact, this structure can be applied to solving other data mining problems that have closure properties as discussed later. The monotone specialization relation discussed in <ref type="bibr" target="#b20">[21]</ref> and <ref type="bibr" target="#b17">[18]</ref> addressed the same closure properties.</p><p>Popular benchmark databases designed by Agrawal and Srikant <ref type="bibr" target="#b2">[3]</ref> have been used in <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b31">[32]</ref>. We use these same benchmarks to evaluate the performance of our algorithm. In most cases, our algorithm not only reduces the number of passes of reading the database but also reduces the number of candidates (for whom support is counted). In such cases, both I/O time and CPU time are reduced by eliminating the candidates that are subsets of maximal frequent itemsets found in the MFCS.</p><p>The organization of the rest of the paper is as follows: The problem of association rule mining and the importance of frequent itemsets are sketched in Section 2. The importance of maximum frequent set, structural properties for maximal frequent itemsets, and how they lead to algorithms for their discovery, together with an important representative algorithm are presented in Section 3. Our new Pincer-Search Algorithm is presented in Section 4. Performance evaluation is presented in Section 5. Related work is presented in Section 6. Concluding remarks are given in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">ASSOCIATION RULE MINING</head><p>This section briefly introduces the association rule mining problem, following as feasible the terminology of <ref type="bibr" target="#b1">[2]</ref>. Let I fi 1 ; i 2 ; . . . ; i m g be a set of m (distinct) items. We assume that the items are drawn from some totally ordered domain. For instance, if they are strings, they could be ordered lexicographically. So for convenience, an itemset will be stored as a sequence following the order of the domain. The itemsets could represent different items in a supermarket or different alarm signals in telecommunication networks <ref type="bibr" target="#b14">[15]</ref>. A transaction T is a set of items in I. A transaction could represent some customer purchases of some items from a supermarket or the set of alarm signals occurring within a time interval. A database D is just a set of transactions. A set of items is called an itemset. The number of items in an itemset is called the length of an itemset. Itemsets of some length k are referred to as k-itemsets.</p><p>A transaction T is said to support an itemset X I if and only if X T . The fraction of the transactions in D that support X is called the support of X, denoted as supportX. There is a user-defined minimum support threshold, which is a fraction, i.e., a number in (0,1]. An itemset is frequent iff its support is at least the minimum support. Otherwise, it is infrequent.</p><p>An association rule has the form R : X 3 Y , where X and Y are two nonempty and nonintersecting itemsets. The support for rule R is defined as supportX Y . A confidence factor for such a rule (customarily represented by percentage) is defined as 100 Á supportX Y =supportX (assume supportX &gt; 0) and is used to evaluate the strength of such an association rule. The confidence of a rule indicates how often it can be expected to apply, while its support indicates how trustworthy it is.</p><p>The goal of association rule mining is to discover that all their support and confidence rules consists of at least some user-defined minimum support and minimum confidence thresholds, respectively.</p><p>The normally followed scheme for mining association rules consists of two stages <ref type="bibr" target="#b2">[3]</ref>:</p><p>1. the discovery of frequent itemsets and 2. the generation of association rules. As the second step is rather straightforward and as the first step dominates the processing time, we explicitly focus the paper on the first step: the discovery of frequent itemsets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">FREQUENT ITEMSETS: STRUCTURAL PROPERTIES</head><p>AND BASIC DISCOVERY APPROACHES</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Maximum Frequent Set</head><p>Among all the frequent itemsets, some will be maximal frequent itemsets: They have no proper supersets that are themselves frequent. The maximum frequent set (or MFS) is the set of all the maximal frequent itemsets. The problem of discovering the frequent set can be reduced to the problem of discovering the MFS. The MFS immediately specifies frequent itemsets; these are precisely the nonempty subsets of its elements. The MFS forms a border between frequent and infrequent sets. Once the MFS is known, the supports of all the frequent itemsets can be computed by reading the database once.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Closure Properties</head><p>A typical frequent set discovery process follows a standard scheme. Throughout the execution, the set of all itemsets is partitioned, perhaps implicitly, into three sets:</p><p>1. frequent: This is the set of those itemsets that have so far been discovered as frequent. 2. infrequent: This is the set of those itemsets that so far have been discovered as infrequent. 3. unclassified: This is the set of all the other itemsets. Initially, the frequent and the infrequent sets are empty. Throughout the execution, they grow at the expense of the unclassified set. The execution terminates when the unclassified set becomes empty, and then, of course, all the maximal frequent itemsets are discovered.</p><p>Consider any process for classifying itemsets and some point in the execution where some itemsets have been classified as frequent, some as infrequent, and some are still unclassified. Two closure properties can be used to immediately classify some of the unclassified itemsets: Property 1. If an itemset is infrequent, all it supersets must be infrequent and they need not be examined further.</p><p>Property 2. If an itemset is frequent, all its subsets must be frequent and they need not be examined further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discovering Frequent Itemsets</head><p>In general, it is possible to search for the maximal frequent itemsets either bottom-up or top-down. If all maximal frequent itemsets are expected to be short (close to 1 in size), it seems efficient to search for them bottom-up. If all maximal frequent itemsets are expected to be long (close to n in size), it seems efficient to search for them top-down. We first sketch a realization of the most commonly used approach of discovering the MFS: a bottom-up approach. It consists of repeatedly applying a pass, itself consisting of two steps. At the end of pass k, all frequent itemsets of size k or less have been discovered.</p><p>As the first step of pass k 1, itemsets of size k 1, each having two frequent k-subsets with the same first k À 1 items are generated. Itemsets that are supersets of infrequent itemsets are pruned (and discarded), as of course they are infrequent (by Property 1). The remaining itemsets form the set of candidates for this pass.</p><p>As the second step, the support of the candidates is computed (by reading the database) and they are classified as either frequent or infrequent.</p><p>Example. Consider a database containing five distinct items 1, 2, 3, 4, and 5. There are four transactions in this database: {1, 2, 3, 4, 5}, {1, 3}, {1, 2}, and {1, 2, 3, 4}. The minimum support is set to 0.5. Fig. <ref type="figure" target="#fig_0">1</ref> shows an example of this bottom-up approach. All five 1-itemsets ({1}, {2}, {3}, {4}, and {5}) are candidates in the first pass. After the support counting phase, the 1-itemset {5} is determined to be infrequent. By Property 1, all the supersets of {5} need not be considered. So, the candidates for the second pass are {1,2}, {1,3}, {1,4}, {2,3}, {2,4}, and {3,4}. The same procedure is repeated until all the maximal frequent itemsets are obtained; in this example, only one: {1,2,3,4}.</p><p>In this bottom-up approach, every frequent itemset must have been a candidate at some pass and is therefore also explicitly considered. When at least one maximal frequent itemset happens to be long, this method will be inefficient. In such a case, it might be more efficient to search for the long maximal frequent itemsets using a top-down approach.</p><p>A top-down approach starts with the single n-itemset and decreases the size of the candidates by one in every pass. When a k-itemset is determined to be infrequent, all of its k À 1-subsets will be examined in the next pass. However, if a k-itemset is frequent, then all of its subsets must be frequent and need not be examined (by Property 2).</p><p>Example. See Fig. <ref type="figure" target="#fig_0">1</ref> and consider the same database as the previous example. The 5-itemset {1, 2, 3, 4, 5} is the only candidate in the first pass. After the support counting phase, it is infrequent. The candidates for the second pass are all the 4-subsets of itemset {1, 2, 3, 4, 5}. In this example, itemset {1, 2, 3, 4} is frequent and all the others are infrequent. By Property 2, all subsets of {1, 2, 3, 4} are frequent (but not maximal) and need not be examined. The same procedure is repeated until all maximal frequent itemsets are obtained (i.e., after all infrequent itemsets are visited).</p><p>In this top-down approach, every infrequent itemset is explicitly examined. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, every infrequent itemset (itemset {5} and its supersets) needs to be visited before the maximal frequent itemsets are obtained.</p><p>Note that, in a ªpureº bottom-up approach, only Property 1 above is used to prune candidates. This is the technique that many algorithms (e.g., <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b28">[29]</ref>) use to decrease the number of candidates. In a ªpureº top-down approach, only Property 2 is used to prune candidates. This is the technique used in <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">The Apriori Algorithm</head><p>The Apriori algorithm <ref type="bibr" target="#b2">[3]</ref> is a typical bottom-up approach algorithm. We describe it in some detail, as we will find it helpful to rely on this in presenting our results. The Apriori algorithm repeatedly uses the Apriori-gen algorithm to generate candidates and then count their supports by reading the entire database once. The algorithm is described in Fig. <ref type="figure">2</ref>.</p><p>Apriori-gen relies on Property 1 mentioned above. The candidate generation algorithm consists of a join procedure and a prune procedure.</p><p>The join procedure combines two frequent k-itemsets, which have the same k À 1-prefix to generate a k 1-itemset as a new preliminary candidate. Following the join procedure, the prune procedure is used to remove from the preliminary candidate set all itemsets c such that some k-subset of c is not a frequent itemset. See Fig. <ref type="figure">3</ref> for details.</p><p>The Apriori-gen algorithm has been very successful in reducing the number of candidates and has been used in many subsequent algorithms, such as DHP <ref type="bibr" target="#b25">[26]</ref>, Partition <ref type="bibr" target="#b28">[29]</ref>, Sampling <ref type="bibr" target="#b29">[30]</ref>, DIC <ref type="bibr" target="#b7">[8]</ref>, and Clique <ref type="bibr" target="#b31">[32]</ref>. As discussed in the last section, the bottom-up approach is good for the case when all maximal frequent itemsets are short and the top-down approach is good when all maximal frequent itemsets are long. If some maximal frequent itemsets are long and some are short, then both one-way search approaches will not be efficient.</p><p>To design an algorithm that can efficiently discover both long and short maximal frequent itemsets, one might think of simply running both bottom-up and top-down programs at the same time. However, this naive approach is not good enough. We can actually do much better than that.</p><p>Recall that the bottom-up approach described above uses only Property 1 to reduce the number of candidates and the top-down approach uses only Property 2 to reduce the number of candidates.</p><p>In our Pincer-Search approach (first presented in <ref type="bibr" target="#b16">[17]</ref>), we combine the top-down and the bottom-up searches and we synergistically rely on both properties to prune candidates. A key component of the approach is the use of information gathered in the search in one direction to prune more candidates during the search in the other direction.</p><p>If some maximal frequent itemset is found in the topdown direction, then this itemset can be used to eliminate (possibly many) candidates in the bottom-up direction. The subsets of this frequent itemset can be pruned because they are frequent (Property 2). Of course, if an infrequent itemset is found in the bottom-up direction, then it can be used to eliminate some candidates in the top-down direction (Property 1). This ªtwo-way search approachº can fully make use of both properties and, thus, speed up the search for the maximum frequent set.</p><p>Example. See Fig. <ref type="figure" target="#fig_3">4</ref>, which considers the database of Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>In the first pass, all five 1-itemsets are the candidates for the bottom-up search and the 5-itemset {1, 2, 3, 4, 5} is the candidate for the top-down search. After the support counting phase, infrequent itemset {5} is discovered by the bottom-up search and this information is shared with the top-down search. This infrequent itemset {5} not only allows the bottom-up search to eliminate its supersets as candidates but also allows the top-down search to eliminate its supersets as candidates in the second pass.</p><p>In the second pass, the candidates for the bottom-up search are {1, 2}, {1, 3}, {1, 4}, {2, 3}, {2, 4}, and {3, 4}. Itemsets {1, 5}, {2, 5}, {3, 5}, and {4, 5} are not candidates, since they are supersets of {5}. The only candidate for the top-down search in the second pass is {1, 2, 3, 4}, since all the other 4-subsets of {1, 2, 3, 4, 5} are supersets of {5}.</p><p>After the second support counting phase, {1, 2, 3, 4} is discovered to be frequent by the top-down search. This information is shared with the bottom-up search. All of its subsets are frequent and need not be examined. In this example, itemsets {1, 2, 3}, {1, 2, 4}, {1, 3, 4}, and {2, 3, 4} will not be candidates for our bottom-up or top-down searches. After that, the program can terminate, since there are no candidates for either bottom-up or top-down searches.</p><p>In this example, the number of candidates considered was smaller than required by either a bottom-up or topdown search. We also needed fewer reading passes the database than either bottom-up or top-down searches. The ªpureº bottom-up approach would have taken four passes and the ªpureº top-down approach would have taken five passes for this databaseÐwe needed only two. In fact, Pincer-Search will always use at most as many passes as the minimum of the passes used by the bottom-up approach and the top-down approach.</p><p>Reducing the number of candidates is of critical importance for the efficiency of the frequent set discovery process since the cost of the entire process comes from reading the database (I/O time) to generate the support of  Therefore, it is important that Pincer-Search reduces both the number of candidates and the number of passes. A realization of this two-way search algorithm is discussed next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Two-Way Search by Using the MFCS</head><p>We have designed a combined two-way search algorithm for discovering the maximum frequent set. It relies on a new data structure during its execution, the maximum frequent candidate set, or MFCS for short, which we define next. Definition 1. Consider some point during the execution of an algorithm for finding the MFS. Let FREQUENT be the set of the itemsets known to be frequent and let INFREQUENT be the set of the itemsets known to be infrequent. Then, the maximum frequent candidate set (MFCS) is the minimum cardinality set of itemsets satisfying the conditions.</p><formula xml:id="formula_0">FREQUENT f2 X j X P MFCSg INFREQUENT f2 X j X P MFCSg Y:</formula><p>In other words, MFCS is the set of all maximal itemsets that are not known to be infrequent at this state of the algorithm. Thus, obviously, at any point of the algorithm, MFCS is a superset of the MFS. When the algorithm terminates, the MFCS and the MFS are equal.</p><p>The computation of our algorithm follows the bottomup, breadth-first search approach. We base our presentation on the Apriori algorithm and, for greatest ease of exposition, we present our algorithm as a modification to that algorithm.</p><p>Briefly speaking, in each pass, in addition to counting supports of the candidates in the bottom-up direction, the algorithm also counts supports of the itemsets in the MFCS: This set is adapted for the top-down search. This will help in pruning candidates, but will also require changes in candidate generation, as explained later.</p><p>Consider a pass k, during which, in the bottom-up direction, itemsets of size k are to be classified. If, during the top-down direction, some itemset that is an element of the MFCS of cardinality greater than k is found to be frequent, then all its subsets of cardinality k can be pruned from the set of candidates considered in the bottom-up direction in this pass. They and their supersets will never be candidates throughout the rest of the execution, potentially improving performance. But of course, as the maximum frequent set is ultimately computed, they ªwill not be forgotten.º Similarly, when a new infrequent itemset is found in the bottom-up direction, the algorithm will use it to update the MFCS. The subsets of the MFCS must not contain this infrequent itemset.</p><p>Fig. <ref type="figure" target="#fig_4">5</ref> conceptually shows the combined two-way search. The MFCS is initialized to contain a single element, the itemset of cardinality n containing all the elements of the database. As an example of its utility, consider the first pass of the bottom-up search. If some m 1-itemsets are infrequent after the first pass (after reading the database once), the MFCS will have one element of cardinality n À m. This itemset is generated by removing the m infrequent items from the initial element of the MFCS. In this case, the top-down search goes down m levels in one pass. In general, unlike the search in the bottom-up direction, which goes up one level in one pass, the top-down search can go down many levels in one pass.</p><p>Notice that the bottom up and the top down searches do not proceed in a symmetrical fashion. The reason is that we assume that in general there are no extremely long frequent itemsets. If this assumption is not likely to hold, one can easily reverse the roles of the searches in the two directions.</p><p>By using the MFCS, we may be able to discover some maximal frequent itemsets in early passes. This early discovery of the maximal frequent itemsets can reduce the number of candidates and the passes of reading the database which in turn can reduce the CPU time and I/O time. This is especially significant when the maximal frequent itemsets discovered in the early passes are long.</p><p>For our approach to be effective, we need to address two issues. First, how to update the MFCS efficiently. Second, once the subsets of the maximal frequent itemsets found in the MFCS are removed, how to generate the correct candidate set for the subsequent passes in the bottom-up direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Updating the MFCS Efficiently</head><p>Consider some itemset Y that has been ªjustº classified as infrequent. By the definition of the MFCS, it will be a subset of one or more itemsets in the MFCS and we need to update the MFCS such that its subsets no longer contain Y . To update the MFCS, we will do the following process for every superset of Y that is in the MFCS. We replace every such itemset (say X) by jY j itemsets, each obtained by removing from X a single item (element) of Y . Such a newly  generated itemset is added to the MFCS only when it is not already a subset of any itemset in the MFCS. Formally, we have the MFCS-gen algorithm as in Fig. <ref type="figure">6</ref> (shown here for pass k). Proof. The algorithm excludes all the infrequent itemsets, so the final set will not contain any infrequent itemsets as subsets of its elements.</p><p>Step 7 removes only one item from the itemset m: the longest subset of the itemset m that does not contain the infrequent itemset s. Since this algorithm always generates longest itemsets, the number of the itemsets will be minimum at the end. t u</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">New Candidate Generation Algorithms</head><p>Recall that as discussed in Section 3.4, a preliminary candidate set will be generated after the join procedure is called. In our algorithm, after a maximal frequent itemset is added to the MFS, all of its subsets in the frequent set (computed so far) will be removed. We show by example that if the original join procedure of the Apriori-gen algorithm is applied; some of the needed itemsets could be missing from the preliminary candidate set. Consider Fig. <ref type="figure" target="#fig_6">7</ref>. Suppose that the original frequent itemset L 3 is {{1, 2, 3}, {1, 2, 4}, {1, 2, 5}, {1, 3, 4}, {1, 3, 5}, {1, 4, 5}, {2, 3, 4}, {2, 3, 5}, {2, 4, 5}, {2, 4, 6}, {2, 5, 6}, {3, 4, 5}, and {4, 5, 6}}. Assume that itemset {1, 2, 3, 4, 5} in the MFCS is determined to be frequent. Then, all 3-itemsets of the original frequent set L 3 will be removed from it by our algorithm, except for {2, 4, 6}, {2, 5, 6}, and {4, 5, 6}. Since the Apriori-gen algorithm uses a k À 1-prefix test on the frequent set to generate new candidates, and no two itemsets in the current frequent set {{2, 4, 6}, {2, 5, 6}, and {4, 5, 6}} share a 2-prefix, no candidate will be generated by applying the join procedure on this frequent set. However, the correct preliminary candidate set should be {{2, 4, 5, 6}}. Based on the above observation, we need to recover some missing candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">New Preliminary Candidate Set</head><p>Generation Procedure</p><p>In our new preliminary candidate set generation procedure, the join procedure of the Apriori-gen algorithm is first called to generate a temporary candidate set, which might be incomplete. In such a case, a recovery procedure will be called to recover the missing candidates.</p><p>All missing candidates can be obtained by restoring some itemsets to the current frequent set. The restored itemsets are extracted from the MFS of the current pass, which implicitly maintains all frequent itemsets discovered so far.</p><p>The first group of itemsets that needs to be restored contains those k-itemsets that have the same k À 1-prefix as some itemset in the current frequent set. Consider then, in pass k, an itemset X in the MFS and an itemset Y in the current frequent set such that jXj &gt; k. Suppose that the first k À 1 items of Y are in X and the (k À 1)st item of Y is equal to the jth item of X. We obtain the k-subsets of X that have the same k À 1-prefix as Y by taking one item of X that has an index greater than j and combining it with the first k À 1 items of Y , thus getting one of these k-subsets. After these k-itemsets are found, we recover candidates by combining them with itemset Y as shown in Fig. <ref type="figure">8</ref>.  The second group of itemsets that need to be restored consists of those k-subsets of the MFS having the same k À 1-prefix but having no common superset in the MFS. A similar recovery procedure can be applied after they are restored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">New Prune Procedure</head><p>After the recovery stage, a preliminary candidate set will be generated. We can then proceed to the prune stage. Instead of checking to see if all k-subsets of an itemset X are in L k , we can simply check to see if X is a subset of an itemset in the current MFCS, as shown in Fig. <ref type="figure">9</ref>. In comparison with the prune procedure of Apriori-gen, we use one fewer loop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Correctness of the New Candidate Generation Algorithm</head><p>In summary, our candidate generation process contains three steps, as described in Fig. <ref type="figure" target="#fig_5">10</ref>.</p><p>Lemma 2. The new candidate generation algorithm generates the correct candidate set.</p><p>Proof. We will show that, even though we remove the subsets of the MFS from the current frequent set, the algorithm produces the correct result. Recall the candidate generation process as in Apriori-gen. There are four possible cases when we combine two frequent k-itemsets, say I and J, which have the same k À 1-prefix, to generate a k 1-itemset as a new preliminary candidate. Case 1. I and J are not subsets of any elements of MFS. Both itemsets are in the current frequent set. The join procedure will combine them and generate a preliminary candidate.</p><p>Case 2. I is a subset of some element of MFS but J is not a subset of any element of MFS. I is removed from the current frequent set. However, combining I and J will generate a candidate that needs to be examined. The recovery procedure discussed above will recover candidates in this case.</p><p>Case 3. I and J are subsets of some element of MFS. We do not combine them since the candidate that they generate will be a subset of X and must be frequent.</p><p>Case 4. I and J are both subsets of element MFS, but there is no single element of MFS of which they are both subsets. Both I and J are removed from the current frequent set. However, by combining them, a required candidate will be generated. A recovery procedure similar to the one above will recover missing candidates in this case.</p><p>Our preliminary candidate generation algorithm considers the same combinations of the frequent itemsets as does the Apriori-gen algorithm. This preliminary candidate generation algorithm will generate all the candidates as the Apriori-gen algorithm does, except for those that are subsets of the MFS. By the recovery procedure, some subsets of the MFS will be restored in subsequent passes as necessary.</p><p>Lemma 1 showed that MFCS will be maintained correctly in every pass. Therefore, our new prune procedure will make sure that no superset of infrequent itemsets is in the preliminary candidate set. Therefore, the new candidate generation algorithm is correct. t u</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">The Basic Pincer-Search Algorithm</head><p>We now present our complete algorithm (see Fig. <ref type="figure" target="#fig_9">11</ref>), The Pincer-Search Algorithm, which relies on the combined approach for determining the maximum frequent set. Lines 9 to 12 constitute our new candidate generation procedure.</p><p>The MFCS is initialized to contain one itemset, which consists of all the database items. The MFCS is updated whenever new infrequent itemsets are found (line 8). If an itemset in the MFCS is found to be frequent, then its subsets will not participate in the subsequent support counting and candidate set generation steps. Line 6 will exclude those itemsets that are subsets of any itemset in the current MFS, which contains the frequent itemsets found in the MFCS. If some itemsets in L k are removed, the algorithm will call the recovery procedure to recover missing candidates (line 11).</p><p>Theorem 1. The Pincer-Search algorithm generates all maximal frequent itemsets.</p><p>Proof. Lemma 2 showed that our candidate generation algorithm will generate the candidate set correctly. The Pincer-Search algorithm will explicitly or implicitly discover all frequent itemsets as follows: The frequent itemsets are explicitly discovered when they are discovered by the bottom-up search (i.e., they were in the L k set at some point). The frequent itemsets are implicitly discovered when the top-down search discovers their frequent supersets (which are maximal) before the bottom-up search reaches them. Furthermore, only maximal frequent itemsets will be added to the MFS in Line 5. Therefore, the Pincer-Search algorithm generates all maximal frequent itemsets. t u  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">The Adaptive Pincer-Search Algorithm</head><p>In general, one may not want to use the ªbasicº version of the Pincer-Search algorithm. For instance, in some cases, there may be too many infrequent 2-itemsets. In such cases, it may be too costly to maintain the MFCS. The algorithm we have implemented is in fact an adaptive version of the algorithm. This adaptive version does not maintain the MFCS, while it delays the maintenance of the MFCS until a later pass when the expected cost of calculating the MFCS is worthwhile to increase the overall efficiency. This is also the algorithm whose performance is being evaluated in Section 5. Thus, the very small overhead of deciding when to use the MFCS is accounted for in the performance evaluation of our adaptive Pincer-Search algorithm. Another adaptive approach is to generate all candidates as the Apriori algorithm, but not to count the support of the candidates that are subsets of any itemset in the current MFS. This approach simplifies the basic Pincer-Search algorithm in such a way that it need not go through the candidate recovery process described in Section 4.4. A flag, indicating whether a candidate should be counted or not, can be easily maintained. Based on how the recovery process is done, its cost can be estimated by the number of the current frequent set and the number of the current MFS. When the estimated cost exceeds some threshold, we can switch from the recovery procedure to the candidate generation procedure that generates all candidates. This way, we can still avoid the support counting phase, which is the most time-consuming process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">PERFORMANCE EVALUATION</head><p>For the Pincer-Search algorithm to be effective, the topdown search needs to reach the maximal frequent itemsets faster than the bottom-up search. A reasonable question can be informally stated: ªCan the search in the top-down direction proceed fast enough to reach a maximal frequent itemset faster than the search in the bottom-up direction?º There can be no categorical answer as this really depends on the distribution of the frequent and infrequent itemsets. Encouragingly, according to both <ref type="bibr" target="#b2">[3]</ref> and our experiments, a large fraction of the 2-itemsets will usually be infrequent. These infrequent itemsets will cause the MFCS to go down the levels very fast, allowing it to reach some maximal frequent itemsets after only a few passes. Indeed, in our experiments, we have found that, in most cases, many of the maximal frequent itemsets are found in the MFCS in very early passes. For instance, in the experiment on database T20.I15.D100K (Fig. <ref type="figure" target="#fig_11">13</ref>), all maximal frequent itemsets containing up to 17 items are found in 3 passes only!</p><p>The performance evaluation presented compares the adaptive Pincer-Search algorithm to the Apriori algorithm. Although, later in related work, many variants of the Apriori algorithm will be described, we limit ourselves to this performance comparison because it is sufficiently instructive enough to understand the characteristics of the new algorithm's performance. Furthermore, these algorithms, except A-Random-MFS <ref type="bibr" target="#b10">[11]</ref> and Max-Miner <ref type="bibr" target="#b6">[7]</ref>, discover the entire frequent set. Their improvement over Apriori is usually less than an order of magnitude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Preliminary Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Auxiliary Data Structures Used</head><p>The databases used in performance evaluation are the synthetic databases used in <ref type="bibr" target="#b2">[3]</ref>, the census databases similar to <ref type="bibr" target="#b7">[8]</ref>, and the stock transaction databases from New York Stock Exchange, Inc. <ref type="bibr" target="#b22">[23]</ref>. The experiments were performed on an Intel Pentium Pro 200 with 64 MB RAM running Linux.</p><p>Also, as done in <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b26">[27]</ref>, we used a one-dimensional array and a two-dimensional array to speed up the process of the first and the second pass correspondingly. The support counting phase runs very fast by using an array since no searching is needed. No candidate generation process for 2-itemsets is needed because we use a twodimensional array to store the support of all combinations of those frequent 1-itemsets. We start using the link-list data structure after the third pass. For a fair comparison, in all the cases, the number of candidates shown in the figures does not include the candidates in the first two passes. The number of the candidates in the Pincer-Search algorithm includes the candidates in the MFCS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Scattered and Concentrated Distributions</head><p>We first concentrated on the synthetic databases since they allow for experimenting with different distributions of the frequent itemsets. For the same number of frequent itemsets, their distribution can be concentrated or scattered. In concentrated distribution, frequent itemsets of the same length contain many common items: The frequent items tend to cluster. If the frequent itemsets do not have many common elements, the distribution is scattered. In other words, the distribution is concentrated if there are only a few long maximal frequent itemsets and is scattered if there are many short maximal frequent itemsets. By using the synthetic data generation program, as in <ref type="bibr" target="#b2">[3]</ref>, we can generate databases with different distributions by adjusting various parameters. We will present experiments for examining the impact of the distribution type on the performance of the two algorithms.</p><p>In the first set of experiments, the number of the maximal frequent itemsets jLj is set to 2,000, as in <ref type="bibr" target="#b2">[3]</ref>. The frequent itemsets found in this set of experiments are rather scattered. To produce databases having a concentrated distribution of the frequent itemsets, we decrease the parameter jLj to 50 in the second set of experiments. The minimum supports are set to higher values so that the execution time will not be too long.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Nonmonotone Property of the Maximum Frequent Set and Its Benefits</head><p>For a given database, both the number of candidates and the number of frequent itemsets increase as the minimum support decreases. However, this is not the case for the number of the maximal frequent itemsets. For example, when minimum support is 9 percent, the maximum frequent set may be {{1, 2}, {1, 3}, {2, 3}}. When the minimum support decreases to 6 percent, the maximum frequent set could become {{1, 2, 3}}. The number of the maximal frequent itemsets decreased from three to one.</p><p>This ªnonmonotonicityº does not help bottom-up, breadth-first search algorithms. They will have to discover the entire frequent itemsets before the maximum frequent set is discovered. Therefore, in those algorithms, the time, the number of candidates, and the number of passes will monotonically increase when the minimum support decreases.</p><p>However, when the minimum support decreases, the length of some maximal frequent itemsets may increase and our MFCS may reach them faster. Therefore, our algorithm does have the potential to benefit from this nonmonotonicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experiments</head><p>The test databases are generated synthetically by an algorithm designed by the IBM Quest project <ref type="bibr" target="#b0">[1]</ref>. The synthetic data generation procedure is described in detail in <ref type="bibr" target="#b2">[3]</ref>, whose parameter settings we follow. The number of items N is set to 1,000. jDj is the number of transactions. jT j is the average size of transactions. jIj is the average size of maximal frequent itemsets. Thus, e.g., T10.I4.D100K specifies that the average size of transactions is 10, the average size of maximal frequent itemsets is four, and the database contains 100,000 transactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Scattered Distributions</head><p>The results of the first set of experiments for scattered distributions are shown in Fig. <ref type="figure" target="#fig_10">12</ref>. In these experiments, Pincer-Search sometimes used more candidates than Apriori. That is because of the number of additional candidates used in the MFCS is more than the number of extra candidates pruned relying on the MFCS. The maximal frequent itemsets found in the MFCS are so short that not too many subsets can be pruned. However, the I/O time saved may compensate for the extra cost. Therefore, we may still get some improvement. In these experiments, the best case occurs when the minimum support is 0.33 percent and the database is T10.I4.D100K. In this case, Pincer-Search ran about 17 percent faster than the Apriori algorithm.</p><p>Depending on the distribution of the frequent itemsets, it is also possible that our algorithm might spend time counting the support of the candidates in the MFCS while still not finding enough maximal frequent itemsets from the MFCS to cover the extra costs. For instance, in the worst case of all the experiments, Pincer-Search ran 28 percent slower than the Apriori algorithm when the minimum support is 0.25 percent and the database is T20.I6.D100K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Concentrated Distributions</head><p>In the second set of experiments, we study the relative performance of the two algorithms on databases with concentrated distributions. The results are shown in Fig. <ref type="figure" target="#fig_11">13</ref>. In the first experiment, we use the same parameters as the T20.I6.D100K database in the first set of experiments, but the parameter jLj is set to 50. The improvements of Pincer-Search begin to increase. When the minimum support is 17 percent, our algorithm runs about 43 percent faster than the Apriori algorithm.</p><p>The nonmonotone property of the maximum frequent set, considered in Section 5.1.3, impacts this experiment. When the minimum support is 12 percent, both the Apriori algorithm and Pincer-Search algorithm took eight passes to discover the maximum frequent set. But, when the minimum support decreases to 11 percent, the maximal frequent itemsets become longer. This forced the Apriori algorithm to take more passes (nine passes) and consider more candidates to discover the maximum frequent set. In contrast, the MFCS allowed our algorithm to reach the maximal frequent itemsets faster. Pincer-Search took only four passes and considered fewer candidates to discover all maximal frequent itemsets. We further increased the average size of the frequent itemsets in the next two experiments. The average size of the maximal frequent itemsets was increased to 10 in the second experiment and database T20.I10.D100K was used. The best case, in this experiment, is when the minimum support is 6 percent. Pincer-Search ran approximately 24 times faster than the Apriori algorithm. This improvement mainly came from the early discovery of maximal frequent itemsets which contain up to 16 items. Their subsets were not generated and counted in our algorithm. As shown in this experiment, the reduction of the number of candidates can significantly decrease both I/O time and CPU time.</p><p>The last experiment ran on database T20.I15.D100K. Pincer-Search took as few as three passes to discover all maximal frequent itemsets which contain as many as 17 items. This experiment shows improvements of more than two orders of magnitude when the minimum supports are 6 percent and 7 percent. One can expect even greater improvements when the average size of the maximal frequent itemsets is further increased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Census Databases</head><p>A PUMS file, which contains the public use microdata samples, was provided to us by Roberto Bayardo from IBM. This file contains actual census entries which constitute a five percent sample of a state that the file represents. This database is similar to the database looked at by Brin et al. <ref type="bibr" target="#b7">[8]</ref>. As discussed in that paper, this PUMS database is quite hard ªto data mine.º That is because a number of items in the census database appear in a large fraction of the database and, therefore, very many frequent itemsets will be discovered. From the distribution point of view, this means that some maximal frequent itemsets are quite long and the distribution of the frequent itemsets is quite scattered.</p><p>In order to compare the performance of the two algorithms within a reasonable time, we used the same approach as they proposed in the paper: We remove all items that have 80 percent or more support from the database. The experimental results are shown in Fig. <ref type="figure" target="#fig_12">14</ref>.</p><p>In this real-life census database, the Pincer-Search algorithm also performs well. In all cases, the Pincer-Search algorithm used less time, fewer candidates, and fewer reading passes of the database. The overall improvement in performance ranges from 10 percent to 650 percent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Stock Market Databases</head><p>We also conducted some experiments on the stock market databases. We used the trading data of June 1997,  collected by the New York Stock Exchange, for our experiments. The trading data contains the symbol, price, volume, and time for every trade. Around 3,000 stocks were chosen for these experiments. We are interested in discovering the pattern of price changes. A simplified problem can be stated as the followings: ªFor which stocks does the prices go up and down together during two-thirds of the time intervals in one day?º</p><p>Whether the price of a stock goes up or down is determined by comparing the first and the last trading price of the stock within an interval of time. If there is no trade for a stock during this period of time, then assume that the price is unchanged. Collect the stocks whose prices go up during this period of time and treat them as a transaction in our association rule mining problem. Do the same for the stocks whose prices went down during this period of time and form another transaction. Now, we can run the frequent set discovery algorithm to discover the price-changing patterns.</p><p>We ran experiments for every trading day of June 1997. Here, we only show the experiments on those days that reflect extreme cases when either Pincer-Search or Apriori performed best. The experiments on June 3 and June 20 are shown in Fig. <ref type="figure" target="#fig_13">15</ref>. The minimum supports range from 40 percent to 73 percent. We cannot choose a fixed range of minimum support because the data in each day is quite different from each other. The interval was set to 60 minutes.</p><p>For the experiments on the June 3 data, Pincer-Search performed much better than the Apriori algorithm. For the experiments on June 20 data, even though the Pincer-Search algorithm used fewer passes and fewer candidates than the Apriori algorithm, the overhead of maintaining the MFCS costs too much. The Apriori algorithm performed better in this case. It is possible to adjust some of the parameters in the adaptive approach to reduce the differences.</p><p>Because the number of records in the database is 15, which is very small, we have only a few available values on setting the minimum support. As we can see from these figures, there is a big jump in execution time when we decrease the minimum support down to some threshold value. For instance, in the experiment of June 3 when the minimum support is decreased from 53 percent (8/15) to 46 percent (7/15), the execution time increases significantly.</p><p>When we decrease the minimum support further, neither algorithm can complete the execution due to the limited size of the main memory. However, we suspect that Pincer-Search will do better when the minimum support is decreased (maximal frequent itemsets will be longer). In many cases, Pincer-Search did find many very long maximal frequent itemsets in very early passes. However, since the MFS is not complete, we cannot draw any definite conclusion.</p><p>In many of these experiments, Pincer-Search did use fewer passes. However, as the database is so small, the property of reducing the passes of reading the database for the Pincer-Search algorithm did not contribute to the saving of the execution time. The improvement came purely from reducing the number of candidates.</p><p>In all the other experiments that are not shown here, both algorithms perform competitively, since the maximal frequent itemsets are all short. The totals of the execution time of all other experiments are 3,451,400 seconds and 3,176,103 seconds for Pincer-Search and Apriori, respectively. The totals of the candidates used are 163,328 and 148,376 and the total passes used are 726 and 886 for Pincer-Search and Apriori, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK 6.1 Typical Algorithms for Frequent Set Discovery</head><p>We briefly discuss selected previous frequent set discovery algorithms in a roughly chronological order.</p><p>AIS and SETM Algorithms. The problem of association rule mining was first introduced in <ref type="bibr" target="#b1">[2]</ref>. An algorithm, called AIS, was given for discovering the frequent set. The SETM algorithm <ref type="bibr" target="#b15">[16]</ref> was later designed to use only standard SQL commands to find the frequent set. The Apriori algorithm <ref type="bibr" target="#b2">[3]</ref>, described above, performs much better than AIS and SETM.</p><p>The OCD Algorithm. It is worth adding that concurrently with the Apriori algorithm, the OCD algorithm <ref type="bibr" target="#b21">[22]</ref> used the same closure property to eliminate candidates. One slight difference between OCD and Apriori is in the preliminary candidates generation. OCD might produce a superset of the preliminary candidates produced in the join procedure of Apriori algorithm. DHP and Partition Algorithm. DHP algorithm <ref type="bibr" target="#b25">[26]</ref> extended the Apriori algorithm by introducing a hash filter for counting the upper-bound of the support of candidates in the next pass. Some candidates can be pruned before reading the database in the next pass.</p><p>The Partition algorithm <ref type="bibr" target="#b28">[29]</ref> was proposed to divide the database into equal-sized partitions. Each partition is processed independently to produce a local frequent set for that partition. After all local frequent sets are discovered, their union, the global candidate set, forms a superset of the actual frequent set. The database is then read again to produce the actual support for the global candidate set. The entire process takes only two (read) passes.</p><p>Sampling Algorithm. The Sampling Algorithm <ref type="bibr" target="#b29">[30]</ref> was proposed to consider first (small) samples of the database and discover an approximate frequent set by using a standard bottom-up approach algorithm. The approximate frequent set is then verified against the entire database. False frequent itemsets need to be removed and missing frequent itemsets need to be recovered.</p><p>A-Random-MFS, DIC, Dualize-and-Advance, and Max-Clique Algorithms. A-Random-MFS algorithm <ref type="bibr" target="#b10">[11]</ref> is a randomized algorithm for discovering the maximum frequent set. A single run of the algorithm cannot guarantee correct results. The full algorithm requires repeatedly calling the randomized algorithm until no new maximal frequent itemset can be found.</p><p>The Dynamic itemset counting (DIC) algorithm <ref type="bibr" target="#b7">[8]</ref> combines candidates of different lengths into one pass. The database is divided into partitions of equal size. In each pass, after the first i partitions are read, some itemsets containing up to i 1 items may become candidates based on the database partitions read so far.</p><p>The Dualize-and-Advance <ref type="bibr" target="#b9">[10]</ref> algorithm uses some greedy procedure to find some maximal frequent itemsets. The negative border of these itemsets is then computed. Each itemset in the negative border is checked to see whether it is frequent. If some are frequent, these frequent itemsets will be extended to be maximal in a greedy manner and repeat the above process. If all are infrequent, then the algorithm stops.</p><p>MaxClique <ref type="bibr" target="#b31">[32]</ref> used a hybrid traversal which contains a look-ahead phase followed by a pure bottom-up phase. The look-ahead phase consists of extending the frequent 2-itemsets until the extended itemset becomes infrequent. After the look-ahead phase, an Apriori-like traversal is executed.</p><p>One of the most important differences between MaxClique and Pincer-Search is that MaxClique only looks ahead at some long candidate itemsets during the initialization stage (in the second pass). In contrast, the Pincer-Search algorithm repeatedly maintains the upperbound of the frequent itemsets (MFCS) throughout the entire process. The look-ahead candidate itemsets are dynamically adjusted based on all available information discovered so far. In fact, the MFCS is the most accurate approximation one can get while no additional knowledge of the data is available.</p><p>Another important difference is that MaxClique used a bottom-up approach to calculate the look-ahead candidate itemsets. Conceptually, it keeps applying Apriori-gen until no more candidates can be generated. In contrast, Pincer-Search uses a top-down approach. As will be discussed in the next section, this top-down approach has the advantage that it is suitable for incremental updates. It updates the MFCS only when a new infrequent itemset is discovered. Ignoring implementation details, MaxClique can be viewed as a special case of Pincer-Search.</p><p>Max-Miner. The Max-Miner algorithm <ref type="bibr" target="#b6">[7]</ref> was recently proposed to discover the maximum frequent set. This algorithm partitions the candidate set into groups with the same prefix. Like Pincer-Search, it looks ahead at some long candidate itemsets throughout the search. The main difference is the long candidate itemsets that it examines. Max-Miner looks ahead at longest itemsets that can be constructed from every group. A frequency heuristic is used to reorder the items such that the most frequent items appear in the most candidate groups. According to the experiments in the paper, this itemreordering heuristic improves the performance dramatically. We feel that Max-Miner and Pincer-Search could be complementary. One of the possibilities is to run Max-Miner in the first few passes and switch to Pincer-Search for the later passes.</p><p>Tree Projection and FP-Growth. Tree projection <ref type="bibr" target="#b4">[5]</ref> and FP-growth <ref type="bibr" target="#b12">[13]</ref> are two of the most recent algorithms proposing new data structures to store compressed information about frequent itemsets. Tree projection uses a hierarchical lexicographic tree to project transactions at each node of the tree and uses a matrix counting technique on the projected transactions for counting support of candidate itemsets. A frequent pattern tree (FP-tree) is a compressed data structure used in the FP-growth algorithm. FP-tree is constructed after two scans of the database, and a divideand-conquer method is then used to generate the complete frequent itemsets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Other Related Work</head><p>In addition to the algorithms discussed so far, there has been extensive research relating to the problem of association rule mining, such as <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b23">[24]</ref>. Similar candidate pruning techniques have been applied to discover sequential patterns (e.g., <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b27">[28]</ref>) and episodes (e.g., <ref type="bibr" target="#b19">[20]</ref>). Some other papers concentrate on designing parallel algorithms on share-nothing parallel environment (e.g., <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b13">[14]</ref>) and share-memory parallel environment (e.g., <ref type="bibr" target="#b30">[31]</ref>). The discovery of frequent set is a key process in solving these problems, thus speeding up this discover is important.</p><p>The complexity of (level-wise) bottom-up, breadth-first search style algorithms was analyzed in <ref type="bibr" target="#b20">[21]</ref>. As our algorithm does not fit in this model, their low bound complexity does not apply to it.</p><p>Our work was inspired by the notion of version space <ref type="bibr" target="#b17">[18]</ref>. We found that, if we treat a newly discovered frequent itemset as a new positive training instance, a newly discovered infrequent itemset as a new negative training instance, the candidate set as the maximally specific generalization (S), and the MFCS as the maximally general generalization (G), then we will be able to use a two-way approaching strategy to discover the maximum frequent set (generalization) efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUDING REMARKS 7.1 Summary</head><p>An efficient way for discovering the maximum frequent set can be very useful in various data mining problems, such as the discovery of the association rules, theories, strong rules, episodes, and minimal keys. The maximum frequent set provides a unique representation of all the frequent itemsets. In many situations, it suffices to discover the maximum frequent set and, once it is known, all the required frequent subsets can be easily generated.</p><p>In this paper, we presented a novel algorithm that can efficiently discover the maximum frequent set. Our Pincer-Search algorithm could reduce both the number of times the database is read and the number of candidates considered. Experiments show that the improvement of using this approach can be very significant, especially when some maximal frequent itemsets are long.</p><p>A popular assumption is that the maximal frequent itemsets are usually very short and, therefore, the computation of all (and not just maximal) frequent itemsets is feasible. Such assumption on maximal frequent itemsets is not necessarily true for some important applications. Consider, for example, the problem of discovering patterns in the price changes of individual stocks in a stock market. Prices of individual stocks are frequently quite correlated with each other. Therefore, the discovered patterns may contain many items (stocks) and the frequent itemsets are long. We expect that our algorithm will useful in these applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Possible Extensions</head><p>The performance of the Pincer-Search algorithm in applications of discovering other price changing patterns in stock markets will be studied. The maximal frequent itemsets in many instances of such applications are likely to be long. Therefore, we expect the algorithm to provide dramatic performance improvements.</p><p>Many classification problems as discussed in <ref type="bibr" target="#b5">[6]</ref> tend to have long patterns. It is worthwhile to study the performance of the Pincer-Search algorithm on these problems.</p><p>In general, if some maximal frequent itemsets are long and the maximal frequent itemsets are distributed in a scattered manner, then the problem of discovering the MFS can be very hard. In this case, even Pincer-Search might not be able to solve it efficiently. Parallelizing the Pincer-Search algorithm might be a possible way to solve this hard problem. We propose dividing the candidate set in such a way that all the candidates that are subsets of an itemset in the MFCS are assigned to a same processor.</p><p>Although there might be some duplicate calculations, this way of partitioning the candidates can have the benefits that no synchronization or communication among processors is needed. Each processor can run totally independently. The issues to study would be the way to minimize the duplicate calculations and to maximize the use of available processors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. One-way searches.</figDesc><graphic coords="3,29.14,69.17,245.21,172.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Fig. 2. Apriori algorithm.</figDesc><graphic coords="4,29.14,69.17,245.21,124.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>candidates (CPU time) and the generation of new candidates (CPU time). The support counting of the candidates is the most expensive part. Therefore, the number of candidates dominates the entire processing time. Reducing the number of candidates not only can reduce the I/O time but also can reduce the CPU time since fewer candidates need to be counted and generated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Two-way search.</figDesc><graphic coords="5,29.14,69.17,245.21,86.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The search space of Pincer-Search.</figDesc><graphic coords="5,292.14,69.17,245.21,106.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Lemma 1 .</head><label>1</label><figDesc>Example. See Fig. 7. Suppose {{1, 2, 3, 4, 5, 6}} is the current (ªoldº) value of the MFCS and two new infrequent itemsets {1, 6} and {3, 6} are discovered. Consider first the infrequent itemset {1, 6}. Since the itemset {1, 2, 3, 4, 5, 6} (element of the MFCS) contains items 1 and 6, one of its subsets will be {1, 6}. By removing item 1 from itemset {1, 2, 3, 4, 5, 6}, we get {2, 3, 4, 5, 6}, and by removing item 6 from itemset {1, 2, 3, 4, 5, 6}, we get {1, 2, 3, 4, 5}. After considering itemset {1, 6}, the MFCS becomes {{1, 2, 3, 4, 5}, {2, 3, 4, 5, 6}}. Itemset {3, 6} is then used to update this MFCS. Since {3,6} is a subset of {2, 3, 4, 5, 6}, two itemsets {2, 3, 4, 5} and {2, 4, 5, 6} are generated to replace {2, 3, 4, 5, 6}. Itemset {2, 3, 4, 5} is a subset of itemset {1,2,3,4,5} in the new MFCS and it will not be added to the MFCS. Therefore, the MFCS becomes {{1, 2, 3, 4, 5}, {2, 4, 5, 6}}. The top-down arrows in Fig. 7 show the updates of the MFCS. The algorithm MFCS-gen correctly updates the MFCS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Example. See Fig. 7 .</head><label>7</label><figDesc>The MFS is {{1, 2, 3, 4, 5}} and the current frequent set is {{2, 4, 6}, {2, 5, 6}, {4, 5, 6}}. The only 3-subset of {{1, 2, 3, 4, 5}} that needs to be restored for itemset {2, 4, 6} to generate a new candidate is {2, 4, 5}. This is because it is the only subset of {{1, 2, 3, 4, 5}} that has the same length and the same 2-prefix as itemset {2, 4, 6}. By combining {2, 4, 5} and {2, 4, 6}, we recover the missing candidate {2, 4, 5, 6}. No itemsets need to be restored for itemsets {2,5,6} and {4,5,6}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .Fig. 8 .</head><label>68</label><figDesc>Fig. 6. MFCS-gen algorithm. Fig. 7. Pincer-Search.</figDesc><graphic coords="6,292.14,69.17,245.21,98.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .Fig. 10 .</head><label>910</label><figDesc>Fig. 9. New prune algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. The Pincer-Search algorithm.</figDesc><graphic coords="7,292.14,69.17,245.21,151.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Scattered distribution.</figDesc><graphic coords="9,141.90,69.17,282.68,256.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Concentrated distribution.</figDesc><graphic coords="10,144.28,350.19,277.87,89.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Census database.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. NYSE database (60-minute's interval).</figDesc><graphic coords="11,144.06,69.17,278.32,170.14" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This research was partially supported by the US National Science Foundation under grant number CCR-94-11590, by Intel Corporation and by Microsoft. The authors thank Rakesh Agrawal, Roberto J. Bayardo, and Ramakrishnan Srikant for kindly providing us with the experimental data. The authors thank Thomas Anantharaman and Sridhar Ramaswamy for their very valuable comments and suggestions. The authors would also like to thank Sridhar Ramaswamy for his proposal for the term Pincer-Search.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Arning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bollinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shafer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
		<title level="m">ªThe Quest Data Mining System,º Proc. Second ACM SIGKDD Int&apos;l Conf. Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="1996-08">Aug. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ªMining Association Rules between Sets of Items in Large Databases</title>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Imielinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. ACM Special Interest Group on Management of Data (SIGMOD)</title>
		<imprint>
			<date type="published" when="1993-05">May 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
		<title level="m">ªFast Algorithms for Mining Association Rules in Large Databases,º Proc. 20th Very Large Data Base Conf</title>
		<imprint>
			<date type="published" when="1994-09">Sept. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ªParallel Mining of Association Rules</title>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shafer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowledge and Data Eng</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="1996-12">Dec. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ªA Tree Projection Algorithm for Generation of Frequent Itemsets</title>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">V V</forename><surname>Prasad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Parallel and Distributed Computing</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Bayardo</surname></persName>
		</author>
		<title level="m">ªBrute-Force Mining of High-Confidence Classification Rules,º Proc. Third ACM SIGKKD Int&apos;l Conf. Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="1997-08">Aug. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Bayardo</surname></persName>
		</author>
		<title level="m">ªEfficient Mining Long Patterns from Databases,º Proc. ACM Special Interest Group on Management of Data (SIGMOD)</title>
		<imprint>
			<date type="published" when="1998-06">June 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ªDynamic Itemset Counting and Implication Rules for Market Basket Data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tsur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. ACM Special Interest Group on Management of Data (SIGMOD)</title>
		<imprint>
			<date type="published" when="1997-05">May 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">U</forename><surname>Fayyad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Piatetsky-Shapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Uthrusamy</surname></persName>
		</author>
		<title level="m">Advances in Knowledge Discovery and Data Mining</title>
		<meeting><address><addrLine>Menlo Park, Calif</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Gunopulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Khardon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mannila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Toivonen</surname></persName>
		</author>
		<title level="m">ªData Mining, Hypergraph Traversals, and Machine Learning,º Proc. 16th ACM SIGACT-SIGMOD-SIGART Symp. Principles of Database Systems (PODS)</title>
		<imprint>
			<date type="published" when="1997-05">May 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ªDiscovering All Most Specific Sentences by Randomized Algorithm</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gunopulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mannila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saluja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13th Int&apos;l Conf. database Theory (ICDT)</title>
		<meeting>13th Int&apos;l Conf. database Theory (ICDT)</meeting>
		<imprint>
			<date type="published" when="1997-01">Jan. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ªDiscovery of Multiple-Level Association Rules from Large Databases</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 21st Very Large Data Base Conf</title>
		<meeting>21st Very Large Data Base Conf</meeting>
		<imprint>
			<date type="published" when="1995-09">Sept. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ªMining Frequent Patterns Without Candidate Generation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. ACM Special Interest Group on Management of Data (SIGMOD)</title>
		<imprint>
			<date type="published" when="2000-05">May 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ªScalable Parallel Data Mining for Association Rules</title>
		<author>
			<persName><forename type="first">E</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. ACM Special Interest Group on Management of Data (SIGMOD)</title>
		<imprint>
			<date type="published" when="1997-05">May 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ªKnowledge Discovery from Telecommunication Network Alarm Databases</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ha Èto È Nen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Klemettinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mannila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ronkainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Toivonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th IEEE Int&apos;l Conf. Data Eng. (ICDE)</title>
		<meeting>12th IEEE Int&apos;l Conf. Data Eng. (ICDE)</meeting>
		<imprint>
			<date type="published" when="1996-02">Feb. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ªSet-Oriented Mining of Association Rules</title>
		<author>
			<persName><forename type="first">M</forename><surname>Houtsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Research Report RJ</title>
		<imprint>
			<biblScope unit="volume">9567</biblScope>
			<date type="published" when="1993-10">Oct. 1993</date>
		</imprint>
		<respStmt>
			<orgName>IBM Almaden Research Center</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kedem</surname></persName>
		</author>
		<title level="m">ªPincer-Search: A New Algorithm for Discovering the Maximum Frequent Set,º Proc. Sixth Extending Database Technology Conf</title>
		<imprint>
			<date type="published" when="1998-03">Mar. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
	<note>ªGeneralization as Search</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Mueller</surname></persName>
		</author>
		<idno>No. CS-TR- 3515</idno>
		<title level="m">ªFast Sequential and Parallel Algorithms for Association Rule Mining: A Comparison</title>
		<imprint>
			<date type="published" when="1995-08">Aug. 1995</date>
		</imprint>
		<respStmt>
			<orgName>Computer Science Dept., Univ. of Maryland-College Park</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Mannila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Toivonen</surname></persName>
		</author>
		<title level="m">ªDiscovering Frequent Episodes in Sequences,º Proc. First ACM SIGKDD Int&apos;l Conf. Knowledge Discovery and Data Mining (KDD)</title>
		<imprint>
			<date type="published" when="1995-08">Aug. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Mannila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Toivonen</surname></persName>
		</author>
		<idno>TR C-1997-8</idno>
		<title level="m">ªLevelwise Search and Borders of Theories in Knowledge Discovery</title>
		<imprint>
			<date type="published" when="1997-01">Jan. 1997</date>
		</imprint>
		<respStmt>
			<orgName>Dept. of Computer Science, Univ. of Helsinki</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Mannila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Toivonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Verkamo</surname></persName>
		</author>
		<title level="m">ªImproved Methods for Finding Association Rules,º Proc. AAAI Workshop Knowledge Discovery</title>
		<imprint>
			<date type="published" when="1994-07">July 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The TAQ Database Release 1.0 in CD-ROM</title>
		<imprint>
			<date type="published" when="1997-06">June 1997</date>
			<publisher>New York Stock Exchange, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>O È Zden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Silberschatz</surname></persName>
		</author>
		<title level="m">ªCyclic Association Rules,º Proc. 14th IEEE Int&apos;l Conf. Data Eng</title>
		<imprint>
			<date type="published" when="1998-02">Feb. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Piatetsky-Shapiro</surname></persName>
		</author>
		<title level="m">ªDiscovery, Analysis, and Presentation of Strong Rules,º Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">ªAn Effective Hash-Based Algorithm for Mining Association Rules</title>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. ACM Special Interest Group on Management of Data (SIGMOD)</title>
		<imprint>
			<date type="published" when="1995-05">May 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
		<title level="m">ªMining Generalized Association Rules,º Proc. 21st Very Larage Data Base Conf</title>
		<imprint>
			<date type="published" when="1995-09">Sept. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">ªMining Sequential Patterns: Generalizations and Performance Improvements</title>
		<author>
			<persName><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Fifth Extending Database Technology Conf. (EDBT)</title>
		<meeting>Fifth Extending Database Technology Conf. (EDBT)</meeting>
		<imprint>
			<date type="published" when="1996-03">Mar. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">ªAn Efficient Algorithm for Mining Association Rules in Large Databases</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sarasere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Omiecinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Navathe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 21st Very Large Data Base Conf</title>
		<meeting>21st Very Large Data Base Conf</meeting>
		<imprint>
			<date type="published" when="1995-09">Sept. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">ªSampling Large Databases for Association Rules</title>
		<author>
			<persName><forename type="first">H</forename><surname>Toivonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd Very Large Data Base Conf</title>
		<meeting>22nd Very Large Data Base Conf</meeting>
		<imprint>
			<date type="published" when="1996-09">Sept. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Zaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parthasarathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ogihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<idno>618</idno>
		<title level="m">ªParallel Data Mining for Association Rules on Shared-Memory Multi-Processors</title>
		<imprint>
			<date type="published" when="1996-05">May 1996</date>
		</imprint>
		<respStmt>
			<orgName>Dept. Computer Science, Univ. of Rochester</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Zaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parthasarathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ogihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<title level="m">ªNew Algorithms for Fast Discovery of Association Rules,º Proc. Third ACM SIGKDD Int&apos;l Conf. Knowledge Discovery and Data Mining (KDD)</title>
		<imprint>
			<date type="published" when="1997-08">Aug. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Zvi Kedem received the BSc, MSc, and DSc in mathematics (respectively, in 1967, 1971, and 1974) from the Technion±Israel Institute of Technology. He is a professor of computer science at New York University&apos;s Courant Institute of Mathematical Sciences. His research interests include computer graphics, database systems, parallel and distributed computing, and theoretical computer science. He is a senior member of the IEEE and a fellow of the ACM</title>
		<author>
			<persName><forename type="first">-I</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><surname>Lin</surname></persName>
		</author>
		<ptr target="http://computer.org/publications/dlib" />
		<imprint>
			<date type="published" when="1987">1987. 1998</date>
			<pubPlace>Taiwan; San Jose, California</pubPlace>
		</imprint>
		<respStmt>
			<orgName>MS degree in computer science from the National Taiwan University</orgName>
		</respStmt>
	</monogr>
	<note>Previously, he was a research scientist at Telcordia Technologies, Inc., Morristown, New Jersey, and is currently a technical lead at Redback Networks, Inc. For more information on this or any computing topic, please visit our Digital Library</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
