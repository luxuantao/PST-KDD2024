<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Graph Transformer with Adaptive Node Sampling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zaixi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">State Key Laboratory of Cognitive Intelligence</orgName>
								<address>
									<settlement>Hefei</settlement>
									<region>Anhui</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">State Key Laboratory of Cognitive Intelligence</orgName>
								<address>
									<settlement>Hefei</settlement>
									<region>Anhui</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Chee-Kong</forename><surname>Lee</surname></persName>
							<email>cheekonglee@tencent.com</email>
							<affiliation key="aff3">
								<address>
									<country>Tencent America</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="laboratory">Anhui Province Key Lab of Big Data Analysis and Application</orgName>
								<orgName type="institution">University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Graph Transformer with Adaptive Node Sampling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Transformer architecture has achieved remarkable success in a number of domains including natural language processing and computer vision. However, when it comes to graph-structured data, transformers have not achieved competitive performance, especially on large graphs. In this paper, we identify the main deficiencies of current graph transformers: (1) Existing node sampling strategies in Graph Transformers are agnostic to the graph characteristics and the training process. (2) Most sampling strategies only focus on local neighbors and neglect the long-range dependencies in the graph. We conduct experimental investigations on synthetic datasets to show that existing sampling strategies are sub-optimal. To tackle the aforementioned problems, we formulate the optimization strategies of node sampling in Graph Transformer as an adversary bandit problem, where the rewards are related to the attention weights and can vary in the training procedure. Meanwhile, we propose a hierarchical attention scheme with graph coarsening to capture the long-range interactions while reducing computational complexity. Finally, we conduct extensive experiments on real-world datasets to demonstrate the superiority of our method over existing graph transformers and popular GNNs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, the Transformer architecture <ref type="bibr" target="#b32">[33]</ref> and its variants (e.g., Bert <ref type="bibr" target="#b6">[7]</ref> and ViT <ref type="bibr" target="#b7">[8]</ref>) have achieved unprecedented successes in natural language processing (NLP) and computer vision (CV). In light of the superior performance of Transformer, some recent works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b37">38]</ref> attempt to generalize Transformer for graph data by treating each node as a token and designing dedicated positional encoding. However, most of these works only focus on small graphs such as molecular graphs with tens of atoms <ref type="bibr" target="#b37">[38]</ref>. For instance, Graphormer <ref type="bibr" target="#b37">[38]</ref> achieves state-of-the-art performance on molecular property prediction tasks. When it comes to large graphs, the quadratic computational and storage complexity of the vanilla Transformer with the number of nodes inhibits the practical application. Although some Sparse Transformer methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr">2,</ref><ref type="bibr" target="#b18">19]</ref> can improve the efficiency of the vanilla Transformer, they have not exploited the unique characteristics of graph data and require a quadratic or at least sub-quadratic space complexity, which is still unaffordable in most practical cases. Moreover, the full-attention mechanism potentially introduces noise from numerous irrelevant nodes in the full graph.</p><p>To generalize Transformer to large graphs, existing Transformer-based methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b6">7]</ref> on graphs explicitly or implicitly restrict each node's receptive field to reduce the computational and storage complexity. For example, Graph-Bert <ref type="bibr" target="#b40">[41]</ref> restricts the receptive field of each node to the nodes with top-k intimacy scores such as Personalized PageRank (PPR). GT-Sparse <ref type="bibr" target="#b8">[9]</ref> only considers 1-hop neighboring nodes. We argue that existing Graph Transformers have the following deficiencies: <ref type="bibr">(1)</ref> The fixed node sampling strategies in existing Graph Transformers are ignorant of the graph properties, which may sample uninformative nodes for attention. Therefore, an adaptive node sampling strategy aware of the graph properties is needed. We conduct case studies in Section 4 to support our arguments. <ref type="bibr">(2)</ref> Though the sampling method enables scalability, most node sampling strategies focus on local neighbors and neglect the long-range dependencies and global contexts of graphs. Hence, incorporating complementary global information is necessary for Graph Transformer.</p><p>To solve the challenge <ref type="bibr">(1)</ref>, we propose Adaptive Node Sampling for Graph Transformer (ANS-GT) and formulate the optimization strategy of node sampling in Graph Transformer as an adversary bandit problem. Specifically in ANS-GT, we modify Exp4.P method <ref type="bibr">[3]</ref> to adaptively assign weights to several chosen sampling heuristics (e.g., 1-hop neighbors, 2-hop neighbors, PPR) and combine these sampling strategies to sample informative nodes. The reward is proportional to the attention weights and the sampling probabilities of nodes, i.e. the reward to a certain sampling heuristic is higher if the sampling probability distribution and the node attention weights distribution are more similar. Then in the training process of Graph Transformer, the node sampling strategy is updated simultaneously to sample more informative nodes. With more informative nodes input into the self-attention module, ANS-GT can achieve better performance.</p><p>To tackle the challenge (2), we propose a hierarchical attention scheme for Graph Transformer to encode both local and global information for each node. The hierarchical attention scheme consists of fine-grained local attention and coarse-grained global attention. In the local attention, we use the aforementioned adaptive node sampling strategy to select informative local nodes for attention. As for global attention, we first use graph coarsening algorithms <ref type="bibr" target="#b25">[26]</ref> to pre-process the input graph and generate a coarse graph. Such algorithms mimic a down-sampling of the original graph via grouping the nodes into super-nodes while preserving global graph information as much as possible. The center nodes then interact with the sampled super-nodes. Such coarse-grained global attention helps each node capture long-distance dependencies while reducing the computational complexity of the vanilla Graph Transformers.</p><p>We conduct extensive experiments on real-world datasets to show the effectiveness of ANS-GT. Our method outperforms all the existing Graph Transformer architectures and obtains state-of-the-art results on 6 benchmark datasets. Detailed analysis and ablation studies further show the superiority of the adaptive node sampling module and the hierarchical attention scheme.</p><p>In summary, we make the following contributions:</p><p>• We propose Adaptive Node Sampling for Graph Transformer (ANS-GT), which modifies a multi-armed bandit algorithm to adaptively sample nodes for attention.</p><p>• In the hierarchical attention scheme, we introduce coarse-grained global attention with graph coarsening, which helps graph transformer capture long-range dependencies while increasing efficiency.</p><p>• We empirically evaluate our method on six benchmark datasets to show the advantage over existing Graph Transformers and popular GNNs.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Transformers for Graph</head><p>Recently, Transformer <ref type="bibr" target="#b32">[33]</ref> has shown its superiority in an increasing number of domains <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b39">40]</ref>, e.g. Bert <ref type="bibr" target="#b6">[7]</ref> in NLP and ViT <ref type="bibr" target="#b7">[8]</ref> in CV. Existing works attempting to generalize Transformer to graph data mainly focus on two problems: (1) How to design dedicated positional encoding for the nodes;</p><p>(2) How to alleviate the quadratic computational complexity of the vanilla Transformer and scale the Graph Transformer to large graphs. As for the positional encoding, GT <ref type="bibr" target="#b8">[9]</ref> firstly uses Laplacian eigenvectors to enhance node features. Graph-Bert <ref type="bibr" target="#b40">[41]</ref> studies employing Weisfeiler-Lehman code to encode structural information. Graphormer <ref type="bibr" target="#b37">[38]</ref> utilizes centrality encoding to enhance node features while incorporating edge information with spatial (SPD-indexed attention bias) and edge encoding. SAN <ref type="bibr" target="#b20">[21]</ref> further replaces the static Laplacian eigenvectors with learnable positional encodings and designs an attention mechanism that distinguishes local connectivity. For the scalability issue, one immediate idea is to restrict the number of attending nodes. For example, GAT <ref type="bibr" target="#b33">[34]</ref> and GT-Sparse <ref type="bibr" target="#b8">[9]</ref> only consider the 1-hop neighboring nodes; Gophormer <ref type="bibr" target="#b45">[46]</ref> uses GraphSAGE <ref type="bibr" target="#b10">[11]</ref> sampling to uniformly sample ego-graphs with pre-defined maximum depth; Graph-Bert <ref type="bibr" target="#b40">[41]</ref> restricts the receptive field of each node to the nodes with top-k intimacy scores (e.g., Katz and PPR). However, these fixed node sampling strategies sacrifice the advantage of the Transformer architecture. SAC <ref type="bibr" target="#b21">[22]</ref> tries to use an LSTM edge predictor to predict edges for self-attention operations. However, the fact that LSTM can hardly be parallelized reduces the computational efficiency of the Transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Sparse Transformers</head><p>In parallel, many efforts have been devoted to reducing the computational complexity of the Transformer in the field of NLP <ref type="bibr" target="#b22">[23]</ref> and CV <ref type="bibr" target="#b31">[32]</ref>. In the domain of NLP, Longformer <ref type="bibr">[2]</ref> applies block-wise or strode patterns while only fixing on fixed neighbors. Reformer <ref type="bibr" target="#b18">[19]</ref> replaces dot-product attention by using approximate attention computation based on locality-sensitive hashing. Routing Transformer <ref type="bibr" target="#b29">[30]</ref> employs online k-means clustering on the tokens. Linformer <ref type="bibr" target="#b34">[35]</ref> demonstrates that the self-attention mechanism can be approximated by a low-rank matrix and reduces the complexity from O(n 2 ) to O(n). As for vision transformers, Swin Transformer <ref type="bibr" target="#b23">[24]</ref> proposes the shifted windowing scheme which brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. Focal Transformer <ref type="bibr" target="#b36">[37]</ref> presents a new mechanism incorporating both fine-grained local and coarse-grained global attention to capture short-and long-range visual dependencies efficiently. However, these sparse transformers do not take the unique graph properties into consideration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Graph Neural Networks and Node Sampling</head><p>Graph neural networks (GNNs) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b41">42]</ref> follow a message-passing schema that iteratively updates the representation of a node by aggregating representations from neighboring nodes. When generalizing to large graphs, Graph Neural Networks face a similar scalability issue. This is mainly due to the uncontrollable neighborhood expansion in the aggregation stage of GNN. Several node sampling algorithms have been proposed to limit the neighborhood expansion, which mainly falls into node-wise sampling methods and layer-wise sampling methods. In node-wise sampling, each node samples k neighbors from its sampling distribution, then the total number of nodes in the l-th layer becomes O(k l ). GraphSage <ref type="bibr" target="#b10">[11]</ref> is one of the most well-known node-wise sampling methods with the uniform sampling distribution. GCN-BS <ref type="bibr" target="#b24">[25]</ref> introduces a variance reduced sampler based on multi-armed bandits. To alleviate the exponential neighbor expansion O(k l ) of the node-wise samplers, layer-wise samplers define the sampling distribution as a probability of sampling nodes given a set of nodes in the upper layer <ref type="bibr">[4,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b48">49]</ref>. From another perspective, these sampling methods can also be categorized into fixed sampling strategies <ref type="bibr" target="#b10">[11,</ref><ref type="bibr">4,</ref><ref type="bibr" target="#b48">49]</ref> and adaptive strategies <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b38">39]</ref>. However, none of the above sampling methods in GNNs can be directly applied in Graph Transformer as Graph Transformer does not follow the message passing schema.</p><p>3 Preliminaries</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition</head><p>Let G = (A, X) denote the unweighted graph where A ∈ R n×n represents the symmetric adjacency matrix with n nodes, and X ∈ R n×p is the attribute matrix of p attributes per node. The element A ij in the adjacency matrix equals to 1 if there exists an edge between node v i and node v j , otherwise A ij = 0. The label of node v i is y i . In the node classification problem, the classifier has the knowledge of the labels of a subset of nodes V L . The goal of semi-supervised node classification is to infer the labels of nodes in V \V L by learning a classification function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Transformer Architecture</head><p>The Transformer architecture consists of a series of Transformer layers <ref type="bibr" target="#b32">[33]</ref>. Each Transformer layer has two parts: a multi-head self-attention (MHA) module and a position-wise feed-forward network</p><formula xml:id="formula_0">(FFN). Let H = [h 1 , • • • , h m ]</formula><p>⊤ ∈ R m×d denote the input to the self-attention module where d is the hidden dimension, h i ∈ R d×1 is the hidden representation at position i, and m is the number of positions. The MHA module firstly projects the input H to query-, key-, value-spaces, denoted as Q, K, V, using three matrices</p><formula xml:id="formula_1">W Q ∈ R d×d K , W K ∈ R d×d K and W V ∈ R d×d V : Q = HW Q , K = HW K , V = HW V .<label>(1)</label></formula><p>Then, in each head i ∈ {1, 2, . . . , B} (B is the total number of heads), the scaled dot-product attention mechanism is applied to the corresponding {Q i , K i , V i } :</p><formula xml:id="formula_2">head i = Softmax Q i K T i √ d K V i .<label>(2)</label></formula><p>Finally, the outputs from different heads are further concatenated and transformed to obtain the final output of MHA:</p><formula xml:id="formula_3">MHA(H) = Concat ( head 1 , . . . , head B ) W O ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_4">W O ∈ R d×d . In this work, we employ d K = d V = d/B.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Graph Coarsening</head><p>The goal of Graph Coarsening <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b16">17]</ref> is to reduce the number of nodes in a graph by clustering them into super-nodes while preserving the global information of the graph as much as possible.</p><p>Given a graph G = (V, E) (V is the node set and E is the edge set), the coarse graph is a smaller weighted graph  We conduct experiments on the Newman artificial networks <ref type="bibr" target="#b9">[10]</ref> since it enable us to obtain networks with different properties easily. More detailed settings can be found in Appendix A. Here, we consider the property of homophily/heterophily as one example. Following previous works <ref type="bibr" target="#b46">[47]</ref>, the degree of homophily α can be defined as the fraction of edges in a network connecting nodes with the same class label, i.e. α ≜</p><formula xml:id="formula_5">G ′ = (V ′ , E ′ ). G ′ is obtained from the original graph by first computing a partition {C 1 , C 2 , • • • , C |V ′ | } of V , i.e., the clusters C 1 • • • C |V ′ | are disjoint</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>|(vi,vj )∈E∧yi=yj | |E|</head><p>. Graphs with α closer to 1 tend to have more edges connecting nodes within the same class, i.e. strong homophily; whereas networks with α closer to 0 have more edges connecting nodes in different classes, i.e. strong heterophily.</p><p>As shown in Figure <ref type="figure" target="#fig_0">1</ref>, there is no sampling strategy that dominates other strategies across the spectrum of α, which supports our claim that an adaptive sampling method is needed for graphs with different properties. For graphs with strong homophily (e.g., α = 1.0), it is easy to obtain high accuracy by sampling 1-hop neighbors or nodes with top PPR scores. On the other hand, for graphs with strong heterophily (e.g., α = 0.05), the accuracy of using 2-hop neighbors as neighborhoods (i.e., 73.1%) is much higher than that of using 1-hop neighbors and PPR. The probable reason is that the homophily ratio of 2-hop neighbors may rise with the increase of inter-class edges. Finally, Graph Transformer with KNN node sampling can get the most consistent results since KNN only calculates the similarity of attributes. Thus, KNN achieves the best performance (i.e., 77.2% accuracy) when all the nodes are connected randomly, i.e. α = 0.25 for the Newman network.</p><p>Hence, considering different graph properties (e.g., homophily/heterophily), Graph Transformer should adaptively sample the most informative nodes for attention. These observations motivate us to design the proposed hierarchical Graph Transformer with adaptive node sampling in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">The Proposed Method</head><p>In light of the limitations of existing Graph Transformers for large graphs and the motivating observation in the previous section, we propose two effective methods for Graph Transformer to adaptively sample informative nodes and capture the long-range coarse-grained dependencies in this section. We also show that our method has a computational complexity of O(n). The overview of the model framework is shown in Figure <ref type="figure" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Adaptive Node Sampling</head><p>Our Adaptive Node Sampling module aims to adaptively choose the batch of most informative nodes by a multi-armed bandit mechanism. In our setting, it is intuitive that the contributions of nodes to the learning performance can be time-sensitive. Meanwhile, the rewards which are associated with the model training process are not independent random variables across the iterations. The above situation can satisfy the adversarial setting in the multi-armed bandit problem <ref type="bibr">[1]</ref>. To adaptively choose the most informative nodes with the designed sampling strategies, we adjust the method ALBL proposed in <ref type="bibr" target="#b13">[14]</ref>, which modifies the EXP4.P method <ref type="bibr">[3]</ref>. EXP4.P possesses a strong theoretical guarantee for the adversarial setting.</p><p>Formally, let w t = (w t 1 , • • • , w t K ) be the adaptive weight vector in iteration t, where the k-th nonnegative element w t k is the weight corresponding to the k-th node sampling strategy. The weight vector w t is then scaled to a probability vector p t = (p t 1 , • • • , p t K ) where p t k ∈ [p min , 1] with p min &gt; 0. Our ANS-GT adaptively sample nodes based on the probability vector and then obtains the reward of the action.</p><p>For each center node, we consider the sampling probability matrix Q t ∈ R K×n , where K is the number of sampling heuristics and n is the number of nodes in the graph. Specifically, Q t k,j denotes the k-th sampling strategy's preference on selecting node j in iteration t and Q t is normalized to satisfy n j=1 Q t k,j = 1. Note that our ANS-GT is a general framework and is not restricted to a certain set of sampling heuristics. In our work, we adopt four representative sampling heuristics:</p><p>1-/2-hop neighbors: We adopt the normalized adjacency matrix A = D− 1 2 Â D− 1 2 for 1-hop neighbors and A 2 for 2-hop neighbors, where Â = A + I is the adjacency matrix of the graph G with self connections added and D is a diagonal matrix with Dii = j Âij .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KNN:</head><p>We adopts the cosine similarity of node attributes to measure the similarities of nodes. Mathematically, the similarity score S ij between node i and j is calculated as</p><formula xml:id="formula_6">S ij = x i •x j /(|x i |•|x j |)</formula><p>where x i is the feature vector of node v i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PPR:</head><p>The Personalized PageRank <ref type="bibr" target="#b26">[27]</ref> matrix S is calculated as: S = c(I − (1 − c)A) −1 , where factor c ∈ [0, 1] (set to 0.15 in our experiments). A denotes the column-normalized adjacency matrix. Finally, given the probability vector p t and the node sampling matrices Q t , the final node sampling probability is:</p><formula xml:id="formula_7">ψ t i = K k=1 p t k Q t ki .<label>(4)</label></formula><p>We introduce a novel reward scheme based on the attention weights which is intrinsic in Transformer. Formally, given an attention matrix A = Softmax(QK T )/ √ d, we use the first row of the attention matrix, i.e., A 1,i multiplying the magnitude of corresponding value V i to represent the significance of each node to the center node:</p><formula xml:id="formula_8">s i = A 1,i × ∥V i ∥.</formula><p>In the situation of multiple heads and layers in the Transformer, we average the significance scores in multiple attention matrices. The reward to the k-th sampling strategy is:</p><formula xml:id="formula_9">r k = N i=1 siQ t ki ψ t i</formula><p>, where N is the number of sampled nodes for each center node. r k can be interpreted as the dot product between the significance score vector and the normalized sampling probability vector. The intuition behind the reward design is that the reward to a certain sampling heuristic is higher if the sampling probability distribution and the node significance score distribution are closer. Thus, exploiting such a sampling heuristic can help graph transformer sample more informative nodes. Finally, we update w t with the reward. In experiments, for the efficiency and stability of training, we update the sampling weights and resample nodes every T epochs. The pseudo-code of ANS-GT is listed in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Hierarchical Graph Attention</head><p>We argue that most node sampling strategies (e.g., 1-hop neighbors) focus on local information and neglect long-range dependencies or global contexts. Therefore, to efficiently capture both the local and global information in the graph, we propose a novel hierarchical graph attention scheme including fine-grained attention and coarse-grained attention. Specifically, we use the proposed adaptive node sampling for local fined-grained attention. On the other hand, we adopt the graph Train Graph Transformer with the sampled node sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>if t%T = 0. then 6:</p><p>Obtains the attention matrices and calculate the significance scores: end if 11: end for coarsening algorithm <ref type="bibr" target="#b25">[26]</ref> to generate the coarsened graph G ′ . The sampled n s super-nodes from G ′ are used to capture long-range dependencies. Similar to <ref type="bibr" target="#b45">[46]</ref>, we also use n g global nodes with learnable features to store global context information. Finally, the hierarchical nodes are concatenated with the center nodes as the input sequences.</p><formula xml:id="formula_10">s i = A 1,i × ∥V i ∥. 7: Set W t = K k=1 w t k ,</formula><p>For the positional encoding, we use the proximity encoding in <ref type="bibr" target="#b45">[46]</ref></p><formula xml:id="formula_11">: Φ m (v i , v j ) = A m [i, j], m ∈ {0, • • • , M − 1},</formula><p>where A denotes the normalized adjacency matrices with self-loop. Note that our framework is agnostic to the positional encoding scheme and we left other positional encoding methods such as Laplacian eigenvectors <ref type="bibr" target="#b8">[9]</ref> for future exploration. We follow the Graphormer framework to obtain the output of the l-th transformer layer, H (l) :</p><formula xml:id="formula_12">Ĥ(l−1) = MHA(LN(H (l−1) )) + H (l−1)<label>(5)</label></formula><formula xml:id="formula_13">H (l) = FFN(LN( Ĥ(l−1) )) + Ĥ(l−1) .<label>(6)</label></formula><p>We apply the layer normalization (LN) before the multi-head self-attention (MHA) and the feedforward network (FFN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Optimization and Inference</head><p>In the training and inference, we sample S input sequences for each center node and use the center node representation from the final Transformer layer z (s) c for prediction. Note that the computational complexity is controllable by choosing suitable number of sampled nodes. A MLP (Multi-Layer Perceptron) is used to predict the node class:</p><formula xml:id="formula_14">y (s) = f M LP z (s) c ,<label>(7)</label></formula><p>where y c ∈ R C×1 stands for the classification result, C stands for the number of classes. In the training process, we optimize the average cross entropy loss of labeled training nodes V L :</p><formula xml:id="formula_15">L = − 1 S vi∈V L S s=1 y T i log y (s) i ,<label>(8)</label></formula><p>where y i ∈ R C×1 is the ground truth label of center node v i . In the inference stage, we take a bagging aggregation to improve accuracy and reduce variance: </p><formula xml:id="formula_16">y i = 1 S S s=1 y (s) i .<label>(9)</label></formula><formula xml:id="formula_17">+ n g ) 2 )</formula><p>where the number of sampled nodes N , the number of sampled super-nodes n s , the number of global nodes n g , and the number of augmentations S are specified constants. Finally, the cost of updating sampling weights is linear to n, which is mainly attributed to the computation of rewards. Empirical efficiency analyses of ANS-GT are shown in the Appendix C.</p><p>6 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Setup</head><p>Datasets. To comprehensively evaluate the effectiveness of ANS-GT, we conduct experiments on the six benchmark datasets including citation graphs Cora, CiteSeer, and PubMed <ref type="bibr" target="#b17">[18]</ref>; Wikipedia graphs Chameleon, Squirrel; the Actor co-occurrence graph <ref type="bibr">[5]</ref>; and WebKB datasets <ref type="bibr" target="#b27">[28]</ref> including Cornell, Texas, and Wisconsin. We set the train-validation-test split as 60%/20%/20%. The statistics of datasets are shown in the Appendix C.</p><p>Baselines. To evaluate the effectiveness of ANS-GT on graph representation learning, we compare it with 12 baseline methods, including 8 popular GNN methods, i.e. GCN <ref type="bibr" target="#b17">[18]</ref>, GAT <ref type="bibr" target="#b33">[34]</ref>, GraphSAGE <ref type="bibr" target="#b10">[11]</ref>, JKNet <ref type="bibr" target="#b35">[36]</ref>, APPNP <ref type="bibr" target="#b19">[20]</ref>, Geom-GCN <ref type="bibr" target="#b27">[28]</ref>, H 2 GCN <ref type="bibr" target="#b47">[48]</ref>, and GPRGNN <ref type="bibr" target="#b5">[6]</ref> along with four state-of-the-art Graph Transformers, i.e. GT <ref type="bibr" target="#b8">[9]</ref>, SAN <ref type="bibr" target="#b20">[21]</ref>, Graphormer <ref type="bibr" target="#b37">[38]</ref>, and Gophormer <ref type="bibr" target="#b45">[46]</ref>. We use node classification accuracy as the evaluation metric.</p><p>Implementation Details. We adopt AdamW as the optimizer and set the hyper-parameter ϵ to 1e-8 and (β1, β2) to (0.99,0.999). The peak learning rate is set to 2e-4 with a 100 epochs warm-up stage followed by a linear decay learning rate scheduler. We adopt the Variational Neighborhoods <ref type="bibr" target="#b25">[26]</ref> with a coarsening rate of 0.01 as the default coarsening method. All models were trained on one NVIDIA Tesla V100 GPU.</p><p>Parameter Settings. In the default setting, the dropout rate is set to 0.  Effectiveness of Adaptive Node Sampling. Our proposed adaptive node sampling module can adjust the weights for sampling based on the rewards as the training progresses. To evaluate its effectiveness and give more insights into the ANS module, we show the normalized sampling weights as a function of training epochs on four datasets in Figure <ref type="figure">3</ref>. Generally, we observe that the sampling weights of different sampling strategies are time-sensitive and gradually stabilize with the increase of the number of epochs. Interestingly, we find PPR and 1-hop neighbors achieves high weights on Cora while 2-hop neighbors dominate other sampling strategies on Squirrel. This may be explained by the fact that Cora and Squirrel are strong homophily/heterophily dataset respectively. For Citeseer and Actor, the weights of KNN firstly goes up and gradually decreases. This is probably due to the reason that nodes with similar attributes are most useful for the training at the beginning stage; local neighbors such as nodes with high PPR scores are more useful at the fine-tuning stage. Furthermore, Figure <ref type="figure" target="#fig_2">4</ref> shows the ablation studies of the adaptive node sampling module. We can observe that ANS-GT has a large advantage compared to its variant without the adaptive sampling module denoted as HGT (e.g., On Chameleon, ANS-GT achieves 65.42% while HGT only has 57.20%). Graph Coarsening Methods. In ANS-GT, we apply a hierarchical attention mechanism where the interactions between the center node and supernodes generated by graph coarsening are considered. Here, we evaluate ANS-GT with different coarsening algorithms and different coarsening rates. Specifically, the considered coarsening algorithms include Variation Neighborhoods (VN) <ref type="bibr" target="#b25">[26]</ref>, Variation Edges (VE) <ref type="bibr" target="#b25">[26]</ref>, and Algebraic (JC) <ref type="bibr" target="#b28">[29]</ref>. We vary the coarsening rate from 0.01 to 0.50. In Table <ref type="table" target="#tab_5">2</ref>, we can observe that there is no significant difference between different coars-ening algorithms, indicating the robustness of ANS-GT w.r.t. them. As for the coarsening rate, the results indicate that the coarsening rate of 0.01 to 0.10 has the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>Motivated by the obstacles to generalize Transformer to large graphs, we propose Adaptive Node Sampling for Graph Transformer (ANS-GT), which modifies a multi-armed bandit algorithm to adaptively sample nodes for attention in this paper. To incorporate long-range dependencies and global contexts, we further design a hierarchical graph attention scheme in which coarse-grained attention is achieved with graph coarsening. We empirically evaluate our method on six benchmark datasets to show the advantage over existing Graph Transformers and popular GNNs. The detailed analysis demonstrates that the adaptive node sampling module could effectively adjust the sampling strategies according to graph properties. Finally, We hope our work can help Transformer generalize to the graph domain and encourage the unified modeling of multi-modal data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Performance of Graph Transformer using different node sampling mechanisms: 1-hop, 2-hop, PPR and kNN respectively on Newman networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Model framework of our proposed method: (a) An example input graph. The center node for sampling is colored red. (b) We consider four sampling strategies in this work, i.e. 1-hop neighbors, 2-hop neighbors, PPR, and KNN. (c) The proximity encoding module. (d) Graph coarsening to cluster nodes into super-nodes. (e) The adaptive node sampling module. (f) The self-attention module in Graph Transformer. The output node embeddings are used for node classification. (g) In the sampled input node sequences, the gray nodes are the fine-grained nodes; the white nodes are the coarse-grained nodes from graph coarsening; the green nodes denote the global nodes. (h) We use the first row of the attention matrix, i.e., A 1,i multiplying the magnitude of the corresponding value V i to represent the significance of each node. Then we calculate the reward for each sampling strategy and update the weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Ablation studies to show the effectiveness of the adaptive node sampling module. HGT refers to the Hierarchical Graph Transformer without adaptive node sampling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The feature matrix and weighted adjacency matrix of G ′ are defined by X ′ ≜ P T X and A ′ ≜ P T AP . After Graph Coarsening, the number of nodes/edges in G ′ is significantly smaller than that of G. The coarsening rate can be defined as c = |V ′ | |V | .</figDesc><table><row><cell>4 Motivating Observations</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>$FFXUDF\</cell><cell>0.6 0.7 0.5</cell><cell>KRS</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.4 0.3</cell><cell>0.2 KRS .11 335</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1.0</cell></row></table><note>and cover all the nodes in V . Each cluster C i corresponds to a super-node in G ′ The partition can also be characterized by a matrix P ∈ {0, 1} |V |×|V ′ | , with Pij = 1 if and only if node v i in G belongs to cluster C j . Its normalized version can be defined byP ≜ P D − 1 2 , where D is a |V ′ | × |V ′ | diagonal matrix with |C i | as its i-th diagonal entry.To generalize Transformer to large graphs, existing Graph Transformer models typically choose to sample a batch of nodes for attention. However, real-world graph datasets exhibit different properties, which makes a fixed node sampling strategy unsuitable for all kinds of graphs. Here, we present a simple yet intuitive case study to illustrate how the performance of Graph Transformer changes with different node sampling strategies. The main idea is to use four popular node sampling strategies for node classification: 1-hop neighbors, 2-hop neighbors, PPR, and KNN. Then, we will check the performance of Graph Transformer on graphs with different properties. If the performance drops sharply when the property varies, it will indicate that graphs with different properties may require different node sampling strategies.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Algorithm 1 ANS-GT Input: Total training epochs E; p min ; update period T ; the number of sampled nodes N . Output: Trained Graph Transformer model, optimized w t . 1: Set w 1 k = 1 for k = 1, • • • , K. 2: Calculate the sampling probability matrix Q t . 3: for t = 1, 2, • • • , E do</figDesc><table><row><cell>4:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>and set p t k = (1 − Kp min )</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>K</cell><cell>w t j</cell></row><row><cell cols="9">j=1 W 8: Calculate ψ t i in Equ. 4 and sample N nodes. 9: Set r k = N i=1 siQ t ki ψ t i and update the weight vector w t+1 k using</cell></row><row><cell>w t+1 k</cell><cell>= w t k e</cell><cell>(</cell><cell>p min 2</cell><cell>)(r k + 1 P t k</cell><cell>)</cell><cell>ln(N/0.1) KT</cell><cell>.</cell></row><row><cell>10:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>t + p min for k = 1, • • • , K.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Node classification performance (mean±std%, the best results are bolded). 33±0.38 79.43±0.26 84.86±0.19 60.96±0.75 31.39±0.23 43.15±0.18 75.16±0.95 66.74±1.39 64.31±2.16 GAT 86.29±0.53 80.13±0.62 84.40±0.05 63.90±0.46 36.05±0.35 42.72±0.39 78.76±0.87 76.04±1.35 66.01±3.48 GraphSAGE 86.90±0.84 79.23±0.53 86.19±0.18 62.15±0.42 38.55±0.46 41.26±0.26 79.03±1.44 72.54±1.50 79.41±3.60 APPNP 87.15±0.43 79.33±0.35 87.04±0.17 51.91±0.56 38.80±0.25 37.76±0.45 91.18±0.75 91.75±0.72 82.56±3.57 Compared with existing Graph Transformers, ANS-GT requires extra computational costs on graph coarsening and sampling weights update. Here we want to show that the overall computational complexity of ANS-GT is linear with the number of nodes n. Hence, ANS-GT is scalable to large graphs. First, the computational complexity of graph coarsening is linear with n [26] and we only need to do it once before training. Second, the computational cost of self-attention calculation in one epoch is O(nS(N + n s</figDesc><table><row><cell>Model</cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell><cell>Chameleon</cell><cell>Actor</cell><cell>Squirrel</cell><cell>Texas</cell><cell>Cornell</cell><cell>Wisconsin</cell></row><row><cell cols="10">GCN 87.JKNet 87.70±0.65 78.43±0.31 87.64±0.26 62.92±0.49 33.42±0.28 42.60±0.50 77.51±1.72 64.26±1.16 81.20±1.96</cell></row><row><cell>H2GCN</cell><cell cols="9">87.92±0.82 77.60±0.76 89.55±0.14 61.20±0.95 36.22±0.33 38.51±0.20 86.37±2.67 84.93±1.89 87.73±1.57</cell></row><row><cell>GPRGNN</cell><cell cols="9">88.27±0.40 78.46±0.88 89.38±0.43 64.56±0.59 39.27±0.21 46.34±0.77 91.84±1.25 90.25±1.93 86.58±2.58</cell></row><row><cell>GT</cell><cell cols="9">71.84±0.62 67.38±0.76 82.11±0.39 57.86±1.20 37.94±0.26 25.68±0.22 66.70±1.13 60.39±1.66 65.08±4.37</cell></row><row><cell>SAN</cell><cell cols="9">74.02±1.01 70.64±0.97 86.22±0.43 55.62±0.43 38.24±0.53 25.56±0.23 70.10±1.82 61.20±1.17 65.30±3.80</cell></row><row><cell cols="10">Graphormer 72.85±0.76 66.21±0.83 82.76±0.24 36.81±1.96 37.85±0.29 25.45±0.12 68.56±1.74 59.41±1.21 67.53±3.38</cell></row><row><cell>Gophormer</cell><cell cols="9">87.65±0.20 76.43±0.78 88.33±0.44 57.40±0.14 37.50±0.42 37.85±0.36 88.25±1.96 89.46±1.51 85.09±2.60</cell></row><row><cell>ANS-GT</cell><cell cols="9">88.60±0.45 80.25±0.39 89.56±0.55 65.42±0.71 40.10±1.12 45.88±0.34 93.24±1.85 92.10±1.78 88.62±2.24</cell></row><row><cell cols="4">5.4 Computational Complexity</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>5, the end learning rate is set to 1e-9, the hidden dimension d is set to 128, the number of training epochs is set to 1,000, update The node classification results are shown in Table1. We apply 3 independent runs on random data splitting and report the means and standard deviations. We have the following obervations: (1) Generally, we observe that ANS-GT overperforms all Graph Transformer baselines and achieves state-of-the-art results on nearly all datasets, which demonstrates the effectiveness of our proposed model.(2)  We note that some Graph Transformer baselines achieve poor performance on node classification (e.g., GT only obtains 25.68% on Squirrel) compared with graph neural network models. This is probably due to the full graph attention mechanisms or the fixed node sampling schemes of existing Graph Transformers. For instance, ANS-GT achieves an accuracy of 45.88% on Squirrel while the best baseline has 43.15%.</figDesc><table><row><cell>6DPSOLQJ:HLJKW</cell><cell>KRS KRS .11 335</cell><cell>6DPSOLQJ:HLJKW</cell><cell>KRS KRS .11 335</cell><cell>6DPSOLQJ:HLJKW</cell><cell>KRS KRS .11 335</cell><cell>6DPSOLQJ:HLJKW</cell><cell>KRS KRS .11 335</cell></row><row><cell>(SRFK</cell><cell></cell><cell>(SRFK</cell><cell></cell><cell>(SRFK</cell><cell></cell><cell>(SRFK</cell><cell></cell></row><row><cell>(a) Cora</cell><cell></cell><cell>(b) Citeseer</cell><cell></cell><cell>(c) Actor</cell><cell></cell><cell cols="2">(d) Squirrel</cell></row><row><cell cols="8">Figure 3: The normalized sampling weights as a function of training epochs on four datasets.</cell></row><row><cell cols="3">6.2 Effectiveness of ANS-GT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">'DWDVHW Node Classification Performance. &amp;LWHVHHU &amp;KDPHOHRQ $FWRU $FFXUDF\</cell><cell>6TXLUUHO $16*7 +*7</cell></row></table><note>period T is set to 100, N is set to 20, M is set to 10, and the number of attention head H is set as 8. We tune other hyper-parameters on each dataset based on by grid search. The searching space of batch size, number of data augmentation S, the number of layers L, number of sampled nodes, number of sampled super-nodes, number global nodes are {8, 16, 32}, {4, 8, 16, 32}, {2, 3, 4, 5, 6}, {10, 15, 20, 25}, {0, 3, 6, 9}, {1, 2, 3} respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Sensitivity analysis of coarsening algorithms and coarsening rate. .13 88.30 87.32 87.22 JC 88.49 88.20 87.46 87.36 87.28 Actor VN 39.72 39.45 40.10 38.83 39.08 VE 39.20 39.66 39.51 38.94 39.06 JC 39.15 39.85 39.92 39.16 39.09</figDesc><table><row><cell cols="3">Dataset Method c=0.01 c=0.05 c=0.10 c=0.50 c=1.00</cell></row><row><cell></cell><cell>VN</cell><cell>88.60 88.55 88.14 87.85 87.26</cell></row><row><cell>Cora</cell><cell>VE</cell><cell>87.95 88</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgments</head><p>This research was partially supported by grants from the National Natural Science Foundation of China (Grant No.s 61922073 and U20A20229).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The nonstochastic multiarmed bandit problem</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolo</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on computing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="48" to="77" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Contextual bandit algorithms with supervised learning guarantees</title>
		<author>
			<persName><forename type="first">Alina</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lev</forename><surname>Reyzin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
				<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="19" to="26" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Fastgcn: fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Adaptive universal generalized pagerank graph neural network. ICLR</title>
		<author>
			<persName><forename type="first">Eli</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olgica</forename><surname>Milenkovic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive universal generalized pagerank graph neural network</title>
		<author>
			<persName><forename type="first">Eli</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olgica</forename><surname>Milenkovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A generalization of transformer networks to graphs</title>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dwivedi</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09699</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Community structure in social and biological networks</title>
		<author>
			<persName><forename type="first">Michelle</forename><surname>Girvan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">Ej</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
				<meeting>the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="7821" to="7826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adaptive transfer learning on graph neural networks</title>
		<author>
			<persName><forename type="first">Xueting</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhuan</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="565" to="574" />
		</imprint>
	</monogr>
	<note>Bang An, and Jing Bai</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Joint identification of network communities and semantics via integrative modeling of network topologies and node contents</title>
		<author>
			<persName><forename type="first">Dongxiao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weixiong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Active learning by learning</title>
		<author>
			<persName><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsuan-Tien</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adaptive sampling towards fast graph representation learning</title>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Scaling up graph neural networks via graph coarsening</title>
		<author>
			<persName><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengzhong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>SIGKDD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Reformer: The efficient transformer. ICLR</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Rethinking graph transformers with spectral attention</title>
		<author>
			<persName><forename type="first">Devin</forename><surname>Kreuzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prudencio</forename><surname>Létourneau</surname></persName>
		</author>
		<author>
			<persName><surname>Tossou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03893</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Sac: Accelerating and structuring self-attention via sparse adaptive connection</title>
		<author>
			<persName><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Transformer with memory replay</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barzan</forename><surname>Mozafari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Bandit samplers for training graph neural networks</title>
		<author>
			<persName><forename type="first">Ziqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Qi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Graph reduction with spectral and cut guarantees</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">116</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The pagerank citation ranking: Bringing order to the web</title>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terry</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<pubPlace>Stanford InfoLab</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Geom-gcn: Geometric graph convolutional networks</title>
		<author>
			<persName><forename type="first">Hongbin</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingzhe</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Chen-Chuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Relaxation-based coarsening and multiscale graph organization</title>
		<author>
			<persName><forename type="first">Dorit</forename><surname>Ron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Safro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Achi</forename><surname>Brandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multiscale Modeling &amp; Simulation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="407" to="423" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient content-based sparse attention with routing transformers</title>
		<author>
			<persName><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="53" to="68" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Towards an optimal asymmetric graph structure for robust semi-supervised node classification</title>
		<author>
			<persName><forename type="first">Zixing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1656" to="1665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Efficient transformers: A survey</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06732</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Belinda</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5453" to="5462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Focal self-attention for local-global interactions in vision transformers</title>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Do transformers really perform bad for graph representation?</title>
		<author>
			<persName><forename type="first">Chengxuan</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianle</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Performanceadaptive sampling strategy towards fast and accurate graph neural networks</title>
		<author>
			<persName><forename type="first">Minji</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Théophile</forename><surname>Gervet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoxu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sufeng</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewon</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A transformer-based framework for multivariate time series representation learning</title>
		<author>
			<persName><forename type="first">George</forename><surname>Zerveas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srideepika</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhaval</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anuradha</forename><surname>Bhamidipaty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carsten</forename><surname>Eickhoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2114" to="2124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Graph-bert: Only attention is needed for learning graph representations</title>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congying</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.05140</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Model inversion attacks against graph neural networks</title>
		<author>
			<persName><forename type="first">Zaixi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenya</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chee-Kong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>TKDE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Motif-based graph self-supervised learning for molecular property prediction</title>
		<author>
			<persName><forename type="first">Zaixi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chee-Kong</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="15870" to="15882" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Protgnn: Towards self-explaining graph neural networks</title>
		<author>
			<persName><forename type="first">Zaixi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheekong</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="9127" to="9135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Graph self-supervised learning for optoelectronic properties of organic semiconductors</title>
		<author>
			<persName><forename type="first">Zaixi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang-Yu</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chee-Kong</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML AI4Science Workshop</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Gophormer: Ego-graph transformer for node classification</title>
		<author>
			<persName><forename type="first">Jianan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaozhuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianlong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfang</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.13094</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Beyond homophily in graph neural networks: Current limitations and effective designs</title>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Beyond homophily in graph neural networks: Current limitations and effective designs</title>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Layerdependent importance sampling for training deep and large graph convolutional networks</title>
		<author>
			<persName><forename type="first">Difan</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yewen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Checklist 1. Do the main claims made in the abstract and introduction accurately reflect the paper&apos;s contributions and scope? [Yes</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Did you describe the limitations of your work</title>
		<imprint/>
	</monogr>
	<note>Please see Appendix</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Did you discuss any potential negative social impacts of your work</title>
		<imprint/>
	</monogr>
	<note>Please see Appendix</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Have you read the ethics review guidelines and ensured that your paper conforms to them</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Did you state the full set of assumptions of all theoretical results</title>
		<imprint/>
	</monogr>
	<note>N/A</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Did you include complete proofs of all theoretical results</title>
		<imprint/>
	</monogr>
	<note>N/A</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen</title>
		<imprint/>
	</monogr>
	<note>Yes</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Did you report error bars (e.g., with respect to the random seed after running experiments multiple times</title>
		<imprint/>
	</monogr>
	<note>Yes</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Did you include the total amount of computing and the type of resources used (e.g., type of GPUs, internal cluster</title>
		<imprint/>
	</monogr>
	<note>or cloud provider)? [Yes</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">If your work uses existing assets</title>
		<imprint/>
	</monogr>
	<note>did you cite the creators? [Yes</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Did you discuss whether and how consent was obtained from people whose data you&apos;re using</title>
		<imprint/>
	</monogr>
	<note>N/A</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content</title>
		<imprint/>
	</monogr>
	<note>N/A</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Did you include the full text of instructions given to participants and screenshots</title>
		<imprint/>
	</monogr>
	<note>N/A</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals</title>
		<imprint/>
	</monogr>
	<note>if applicable? [N/A</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation</title>
		<imprint/>
	</monogr>
	<note>N/A</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
