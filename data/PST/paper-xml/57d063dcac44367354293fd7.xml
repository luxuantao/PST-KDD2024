<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Methodology for Formalizing Model-Inversion Attacks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xi</forename><surname>Wu</surname></persName>
							<email>xiwu@cs.wisc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Wisconsin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matthew</forename><surname>Fredrikson</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Somesh</forename><surname>Jha</surname></persName>
							<email>jha@cs.wisc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Wisconsin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jeffrey</forename><forename type="middle">F</forename><surname>Naughton</surname></persName>
							<email>naughton@cs.wisc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Wisconsin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">-</forename><surname>Madison</surname></persName>
						</author>
						<title level="a" type="main">A Methodology for Formalizing Model-Inversion Attacks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">92CC20FB44E56D38E771E73C5E5BF7BD</idno>
					<idno type="DOI">10.1109/CSF.2016.32</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>confidentiality of training data induced by releasing machine-learning models, and has recently received increasing attention. Motivated by existing MI attacks and other previous attacks that turn out to be MI "in disguise," this paper initiates a formal study of MI attacks by presenting a game-based methodology. Our methodology uncovers a number of subtle issues, and devising a rigorous game-based definition, analogous to those in cryptography, is an interesting avenue for future work. We describe methodologies for two types of attacks. The first is for black-box attacks, which consider an adversary who infers sensitive values with only oracle access to a model. The second methodology targets the white-box scenario where an adversary has some additional knowledge about the structure of a model. For the restricted class of Boolean models and black-box attacks, we characterize model invertibility using the concept of influence from Boolean analysis in the noiseless case, and connect model invertibility with stable influence in the noisy case. Interestingly, we also discovered an intriguing phenomenon, which we call "invertibility interference," where a highly invertible model quickly becomes highly non-invertible by adding little noise. For the white-box case, we consider a common phenomenon in machine-learning models where the model is a sequential composition of several submodels. We show, quantitatively, that even very restricted communication between layers could leak a significant amount of information. Perhaps more importantly, our study also unveils unexpected computational power of these restricted communication channels, which, to the best of our knowledge, were not previously known.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Privacy concerns surrounding the release of statistical information have received considerable attention in the past decade. The goal of statistical data privacy research is to enable accurate extraction of valuable data patterns, while preserving an individual's privacy in the underlying dataset against data privacy attacks. In general, there have been two flavors of data privacy attacks in the literature. The first is against a specific privacy notion, such as differential privacy <ref type="bibr" target="#b0">[1]</ref>. Investigations of such attacks have led to lower bounds (e.g. <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>). The second kind of attack is against attribute privacy, which is a general concept where one studies how much distortion is needed in order to prevent an adversary from inferring sensitive attributes from non-sensitive ones (for concreteness, see for example reconstruction attacks <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>). In particular, attribute privacy attacks are widely considered in the applied data privacy literature, where scenarios such as the release of medical information are in focus.</p><p>This paper focuses on a specific class of attacks falling under the second type. The class of attacks we consider relate to inferring sensitive attributes from a released model (e.g. a machine-learning model), or model inversion (MI) attacks. Several of these attacks have appeared in the literature. Recently, Fredrikson et al. <ref type="bibr" target="#b5">[6]</ref> explored MI attacks in the context of personalized medicine. Specifically, Fredrikson et al. <ref type="bibr" target="#b5">[6]</ref> "invert" a publicly-released linear regression model in order to infer sensitive genetic markers, based on the model output (Warfarin dosage) plus several other non-sensitive attributes (e.g., height, age, weight). Interestingly, they demonstrate that knowledge of the model output (Warfarin dosage here), or even a reasonable approximation of it, leads to a statistically-significant increase in leakage of the sensitive attribute. This leads to natural questions of how widely such effective and efficient inversion attacks exist for statistical models, as well as how to quantify the additional leakage due to accessing the model.</p><p>Recently, more instances of effective MI attacks have been discovered, further stimulating interest in this class of attack. For example, <ref type="bibr" target="#b6">[7]</ref> considered a white-box MI attack on models used to classify images. They demonstrated that by exploiting the additional confidence information provided by such models, one can significantly improve both the effectiveness and efficiency of an MI attack. Interestingly, we note that these attacks are reminiscent of privacy attacks discussed in the context of inverting highly compressed image features, which were explored previously <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. It is our belief, however, that if we are to develop countermeasures against all these attacks, or even a precise explanation of the dangers they pose, we will need to go beyond example-based definitions and require a methodology to capture this phenomenon. We consider this paper to be an initial step and much work remains to be done.</p><p>In this paper we take a first step toward providing a formal treatment of MI attacks. Our contributions are summarized as follows:</p><p>• We present two methodologies, which are both inspired by the "two world" games common in cryptographic definitions. A methodology for blackbox attacks, where the adversary has oracle access to the model, and a methodology for white-box attacks, where the adversary has information about the model structure. Our methodologies provide a "blue print" for making these definitions precise for specific cases. Extending this to a precise general definition (such as the real and the ideal world definition used in the SMC literature) will be an interesting direction to pursue. Our methodology focuses on machine-learning (ML) models because they have been the target of existing MI attacks. One shortcoming of our methodology is that we do not take into account the specific structure of the ML model or the learning task. Again, connecting our methodology to various notions in the ML literature (such as stability) provides an attractive avenue for future work. • We then specialize our methodology to important special cases, in order to isolate important factors that affect model invertibility (i.e. how successfully one can invert the model). Identifying these factors is important for at least two applications. First, as a decision procedure prior to publishing a model, estimating invertibility can help one gauge the leakage of sensitive attributes, and thus help in deciding which part of a model is publishable. The second is to help in preventing MI attacks: if invertibility is low, then little noise may be used to effectively prevent MI attacks without sacrificing too much utility. • For the case of models that are Boolean functions (e.g., decision trees with attributes having finite domains), we have some concrete results. In this case, we can leverage powerful tools from Boolean analysis. Specifically for black-box MI attacks where the adversary knows the model output and precisely all other features, and there is no noise, we show that model invertibility is characterized by influence from Boolean analysis. Unfortunately, it becomes significantly more complicated if there is noise in the prior knowledge of the adversary. Neverthe-less, we show that the invertibility is related to stable influence in Boolean analysis. Interestingly, our exploration in the noisy situation also unveils a phenomenon where a highly invertible model quickly becomes highly non-invertible by adding a little noise. We study such phenomenon under the name "invertibility interference." • For white-box MI attacks, we study a common phenomenon where the computation of a machine learning model is a sequential composition of several layers or models. Exploiting the intermediate information communicated between these layers, even when it is highly compressed, can give a significant advantage to the adversary. In fact, the white-box attack described in <ref type="bibr" target="#b6">[7]</ref> exploits exactly such information where the confidence information is the likelihood probabilities computed at an intermediate layer of the model. We thus study how these restricted communication channels could leak information. Interestingly, our results show, quantitatively, that even with 1 bit of communication there could be a significant leakage. Our results also unveil unexpected computational power of these restricted channels, which, to the best of our knowledge, were previously unknown. The rest of the paper is organized as follows: Section II describes our methodologies for black-box and whitebox MI attacks. Then in Section III we give some technical background that is necessary for our development later. Section IV and Section V, specializes our general formulation to important special cases. Finally, we conclude the paper in Section VI by discussing connections of our formulation with other cryptographic notions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. A METHODOLOGY FOR FORMALIZING MI</head><p>ATTACKS An essential goal of studying MI attacks is to quantify the strength of the correlation between sensitive attributes and the output of the model. While this goal is very intuitive, formalizing these attacks poses a challenge due to the diversity of such attacks. Moreover, as we mentioned earlier, many different attacks can be viewed as "MI attacks." This suggests that it can be difficult to give a "unified" definition of MI attacks without risking over generalization (i.e., even a lot of benign cases with "weak correlation" will be classified as attacks). As a first attempt, our goal is thus to abstract out important factors from existing attacks, and present a methodology. Guided by these methodologies, later in this paper we identify special cases of MI attacks that lead to theoretical insights. This section is organized as follows: We start by discussing concepts from machine learning, which provides the background for our methodology. Then we discuss MI attacks in an intuitive manner. In Section II-A and II-B we present methodologies for black-box MI and white-box MI attacks, respectively. Along the way, we discuss how our methodology captures existing attacks and can be used to model other interesting scenarios that have not been addressed before.</p><p>Background. We formalize MI attacks in the generalized learning setting. In the generalized learning setting, a machine learning task is represented as a triple (Z, H, ), where Z is a sample space, H is a hypothesis space, and : H × Z → R is a loss function. Given a data generating distribution D, the goal of learning is to solve the following stochastic optimization problem:</p><formula xml:id="formula_0">min h∈H E x∼D [ (h, z)].</formula><p>In machine learning however, D is unknown and one must find an approximate solution using a dataset S of i.i.d. samples from D.</p><p>Recall that in the supervised learning setting, Z is of the form X ×Y where X is called a feature space and Y an output space. Further, a hypothesis h ∈ H has to take the form X → Y . On the other hand, the generalized learning setting, as formulated above, also incorporates unsupervised learning. For example in clustering, one maps z ∈ Z, a collection of points, to a set of clusters. Towards this goal, one thus needs to formulate a reasonable adversary model to capture how an adversary may exploit the model. We have the following simple observations: <ref type="bibr" target="#b0">(1)</ref> We are interested in MI attacks in the test phase of machine learning, where a model h has already been trained. <ref type="bibr" target="#b1">(2)</ref> It is necessary to have some objective for an attack, which can be captured by some function τ that maps a sample z ∈ Z to some range. (3) The quantification is carried over the training dataset, since the main concern is for participants in the dataset. <ref type="bibr" target="#b3">(4)</ref> The quantification is supposed to compare "two worlds", one where the adversary has access to the model, and the other where the adversary does not. This is to capture the fact that we want to quantify the additional risk of releasing a model.</p><p>Limitations: Next we discuss some limitations of our methodology, and addressing these limitations provides interesting avenue for future work. Our methodology focusses on one organization, so for example, our model does not cover the following scenario: a different organization can collect data S similar to S and build a model h (may be using the same learning algorithm), which can then be used to infer sensitive information about participants. Moreover, the results need to be interpreted on a case-by-case basis. For example, assume that our definition with parameterization for a specific context yields advantage of 1  N , where N is the size of the training set S. Should we consider this an attack? This depends on the context. We admit that our methodology does not exploit structure of the ML task and model (e.g., perhaps looking at the loss function l). In general, we believe that what is considered a privacy breach is highly dependent on the context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Black-Box MI</head><p>We now present a methodology for formalizing blackbox MI attacks where the adversary has oracle access to a model. Along the way, we introduce notation that will be used later.</p><p>Measuring Effectiveness of An Attack. It is attractive to consider the success of an attack on a single sample point. While this might be sensible in some specific scenarios (for example, the adversary wants to get genetic information about a specific individual), it does not seem to be a good formal measure. This is because a machine learning model, in contrast to an encryption, is supposed to communicate some information about the sample, so in the worst case it is always possible to extract some information about a specific individual.</p><p>On the other hand, one may attempt to measure an attack over the data-generating distribution D, which is in the definition of a machine-learning task. However, this leads to a complication as D is unknown in general and so one has to impose assumptions on its structure. We choose to measure an attack over the dataset used to train the model. This thus provides a privacy loss measure for participants in the dataset. Moreover, this allows us to carry out the quantification without an additional parameter D.</p><p>Adversaries and Their Power. We first note that a model at the test phase is fixed, so there is no asymptotic behavior since there is no infinite family of models. We thus model an adversary as a probabilistic algorithm without limiting its computational complexity. In other words, the adversary is all powerful. We note that other data privacy formulations, such as differential privacy <ref type="bibr" target="#b0">[1]</ref>, also make such an assumption on the adversarial power.</p><p>We now present a methodology for formulating black-box MI attacks with the goal of measuring the effectiveness of these attacks. To use this methodology as a template to generate precise definitions for specific scenarios, one has to instantiate auxiliary information generators gen and sgen in the methodology for attacks and simulated attacks, respectively. Having two different generators in the two worlds allows us additional flexibility (e.g., in the Warfarin attack the attacker in the MI-Attack world knows some "approximation" of the Warfarin dosage.) Note that this information cannot be computed by using the oracle because the adversary does not know all the feature values. In some cases, gen and sgen will be the same.</p><p>Methodology 1. The starting point of an MI attack is a machine learning problem, specified as a triple (Z, H, ). We use the following notations: 3) τ : The objective function computed by the adversary. For now, one can view it simply as some function that maps Z to {0, 1} * . 4) gen, sgen: Auxiliary information generators. They map a pair (S, z) to an advice string in {0, 1} * . As we noted before there are two worlds in our methodology (the MI-attack world) and (the simulated attack world). The MI-attack world: This world is described by a tuple (A, gen, τ, S, D S , Γ), where the adversary (A) is a probabilistic oracle machine (recall that gen generates an advice string for the adversary from (S, z)). Now the following game is played between the Nature and the adversary A.</p><p>(1) Nature draws a sample z from D S .</p><p>(2) Nature presents ν = gen(S, z) to the adversary.</p><p>(3) Adversary outputs A Γ(S) (ν). The gain of the game is evaluated as</p><formula xml:id="formula_1">gain(A, gen, τ, S, D S , Γ) = Pr[A Γ(S) (gen(S, z)) = τ (z)]</formula><p>where the probability is taken over the randomness of z ∼ D S , the randomness of gen, and the randomness of A. In other words, the gain is the probability that the adversary A with oracle access to the model Γ(S) and given the advice string generated by sgen is able to "guess" τ (z). The simulated world: is described by a tuple (A * , sgen, τ, S, D S ), where the adversary (A ) is a nonoracle machine and sgen is the second auxiliary information generator. The game between the Nature and A is exactly the same as in the MI-attack world, but A does not have oracle access to the learned model Γ(S). Similarly, the gain is defined as:</p><formula xml:id="formula_2">sgain(A * , sgen, τ, S, D S ) = Pr[A * (sgen(S, z)) = τ (z)]</formula><p>where the probability is taken over the randomness of z ∼ D S , the randomness of sgen, and the randomness of A * . Advantage: For (τ, S, Γ), the advantage of (gen, A) over (sgen, A * ) is computed as</p><formula xml:id="formula_3">adv (gen,A) (sgen,A * ) = | gain(A, gen, τ, S, D S , Γ) -sgain(A * , sgen, τ, S, D S )|.</formula><p>Leakage: We say that Γ(S) has ε-leakage for (τ, D S ) with respect to (gen, sgen, A) if there exists an adversary A * such that adv</p><formula xml:id="formula_4">(gen,A) (sgen,A * ) ≤ ε.</formula><p>Finally, Γ(S) has εleakage for (τ, D S ) with respect to (gen, sgen) if for any probabilistic adversary A, there exists an adversary A * such that adv</p><formula xml:id="formula_5">(gen,A) (sgen,A * ) ≤ ε.</formula><p>We remark that an interesting special case is to evaluate the gain against a uniform distribution over the training set. This case is interesting because a uniform distribution over the training set gives an approximation of the underlying data generating distribution D, as S is i.i.d. drawn from D. As a result, the gain against the uniform distribution over the training set also approximately measures the strength of the correlation for the data generating distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The MI-Attack World</head><p>The Simulated World Oracle access to Γ (a linear-regression model).</p><p>No access to the oracle. τ : τ (z) = x i , the VKORC1 genetic marker.</p><p>τ : τ (z) = x i . gen: gen(S, z) = (x -i , y, marginals of S).</p><p>sgen: sgen(S, z) = (x -i , marginals of S). A: An estimator w.r.t. h, x -i , y, marginals of S.</p><p>A * : An estimator w.r.t. h, x -i , marginals of S.</p><p>TABLE I: Warfarin-dosage Attack of Fredrikson et al. <ref type="bibr" target="#b5">[6]</ref>. We describe how to set up various parameters in order to put the attack of <ref type="bibr" target="#b5">[6]</ref> in our methodology. Note that in this formulation z takes the form of (x, y) and we feed y to the adversary (not h(x)). This is important because in  <ref type="table">I</ref>.</p><p>Note that in this formulation z takes the form of (x, y) and we feed y to the adversary <ref type="bibr">(</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>not h(x)). This is important because in Fredrikson et al.'s case, for the patients participating in the dataset, y does not come from model output, but rather is determined by medical doctors. Thus h(x) is only an approximation of y.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example 2 (Inferring Participation). A common privacy attack is to infer whether an individual is in a dataset. For example, differential privacy addresses such attacks, and uses noise to hide the participation of any individual (so, with/without a specific individual, the adversary draws the same conclusion with high probability.)</head><p>We note that participation attacks fit naturally into our methodology. In particular, consider the following goal function τ , for z ∈ Z,</p><formula xml:id="formula_6">τ (z) = 1 z ∈ S, 0 otherwise.</formula><p>That is, given z ∈ Z, the goal of the adversary is to decide whether z is in the training set or not. One may think that differential privacy is precisely the countermeasure for this attack. However, in principle it is not, although applying differential privacy may have certainly effect the outcome. This is because the design of differential privacy allows learning correlations, subject only to that any individual participation will not be able to change the correlation significantly. Therefore, once the correlation is found, one may still be able to use this correlation to infer participation of a population with certain accuracy.</p><p>Nonetheless differential privacy ensures that localized to any particular individual, his or her participation will not significantly change the results of such inferences (so the guarantee here is a form of "plausible deniability" for that particular individual). We remark that it would be interesting to carry out this attack empirically in a real-world setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. White-Box MI</head><p>We now move on to consider white-box MI attacks. We will now assume that the adversary has some additional knowledge about the model structure. The question is, however, how to model this knowledge about the structure?</p><p>We observe that machine learning models typically adopt a sequential composition of computations. For example, in the simplest case of linear models, one first computes a linear representation of the features, and then applies, for example, a logistic function to make a prediction (representing a probability in this case). As another example, in "one-vs-all" multiclass logistic regression, one trains multiple binary logistic regression models, each encoding the "likelihood" of a particular class, and then makes a final prediction based on these confidence information. As observed in <ref type="bibr" target="#b6">[7]</ref>, being able to observe such intermediate information, even though they might be highly compressed compared to the original information, can give the adversary a significant advantage in deducing sensitive values.</p><p>We are thus motivated to consider white-box MI attacks in the particular case of sequential composition. We note that this is in sharp contrast with attacks in cryptographic settings where the protocols typically have a significantly more complicated composition structure (compared to sequential composition). We start by defining machine learning models with k layers.</p><p>Definition 1 (k-Layer Model). Let X be a feature space and Y 1 , . . . , Y k be k output spaces. A k-layer model M is a model where its computation can be represented as a composition of k functions h 1 , . . . , h k , where</p><formula xml:id="formula_7">h 1 : X → Y 1 and h i : Y i-1 → Y i (2 ≤ i ≤ k).</formula><p>The output of h k is the output of the entire model.</p><p>We can now define white-box MI attacks. Compared to the black-box case, the only thing changes now is that: (i) the adversary is aware of the composition structure, and (ii) he might be able to observe intermediate information passing between the layers. We have the following definition.</p><p>Methodology 2 (k-layer White-Box MI Attack). The methodology of white-box MI attack is the same as the black-box one, except for two differences:</p><p>• Instead of letting the adversary A have oracle access to the model, we feed the k-layer representation of the machine-learning model as input to the adversary. • The auxiliary information generators gen and sgen take an additional parameter Γ (the learning algorithm). This allows the auxiliary information generators to generate information that might depend on the learning algorithm Γ (see <ref type="bibr">Example 4)</ref>. An important point to note is that in the simulated world the adversary still does not have access to the model Γ(S).</p><p>Modeling Examples. As in the black-box case, we now express existing attacks using our methodology.</p><p>Example 3 (Decision Tree <ref type="bibr" target="#b6">[7]</ref>). In <ref type="bibr" target="#b6">[7]</ref> the authors studied the following attack against decision trees. Not only does the adversary know the model structure, but also he knows, for each path of the tree, how many instances in the training set correspond to that path. In other words, the adversary knows both the exact decision tree, as well as the confidence information of each path (intuitively, one wants to follow the path that more training instances follow). They show that with such confidence information one can significantly improve attack accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Such a scenario can be captured by our methodology. The decision tree model is directly fed as input to the adversary. For the confidence information, the adversary can compute on its own by simulating the model on every instance of S. The adversary can do so because he is all-powerful and has white-box access to the model.</head><p>Example 4 (Neural Network <ref type="bibr" target="#b6">[7]</ref>). As we mentioned before, <ref type="bibr" target="#b6">[7]</ref> also studied another attack where for a neural network with a softmax layer (this layer encodes the probabilities corresponding to each class), the adversary can query for probability in that layer. Again, accessing this piece of information significantly improves attack accuracy.</p><p>This attack can also be easily captured by our methodology. One potential subtlety is the softmax probabilities, which cannot be computed by the adversary directly, though he has white-box access to the model. This is because he only knows partially the original input. Nevertheless, this can be generated by the auxiliary information generator by simulating Γ(S) on z and encode the output of the softmax layer in the auxiliary information.</p><p>Interestingly, we observe that several privacy attacks <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> for recovering image features, which appeared before the work of Fredrikson et al. <ref type="bibr" target="#b5">[6]</ref>, can also be captured using white-box MI attacks in a similar way. Due to lack of space, we defer a detailed description to the full version of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PRELIMINARIES</head><p>We now present technical preliminaries to facilitate our later development. In the first part of this section we give some background on Boolean analysis, which will be needed for our development of black-box MI attacks.</p><p>In the second part we present some assumptions we put on the machine learning models.</p><p>Boolean Analysis. We need some elementary concepts from Boolean analysis. More details regarding these concepts can be found in O'Donnell <ref type="bibr" target="#b10">[11]</ref>. In Boolean analysis, a Boolean function f : {-1, 1} n → {-1, 1} is viewed as a 2 n -dimensional real vector, and we consider an inner product space of these vectors, where the inner product is defined as</p><formula xml:id="formula_8">f, g = E x∼{-1,1} n [f (x)g(x)].</formula><p>A central concept of Boolean analysis is its Fourier expansion, where the Fourier basis is the set of all parity functions Ω = {χ S : S ⊆ [n]} where χ S (x) = i∈S x i is the parity function of bits in S. Any function f can be represented as</p><formula xml:id="formula_9">f = S⊆[n] f (S)χ S where f (S) is called the Fourier coefficient of f at S. Definition 2 (Influence). Let f : {-1, 1} n → {-1, 1} and i ∈ [n]. The influence of i-th coordinate of f is Inf i (f ) = Pr x∼{-1,1} n [f (x) = f (x ⊕i )] where x ⊕i means to flip the i-th bit of x.</formula><p>Influence is related to the difference operator D i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 3. D i is a linear operator applied to a Boolean function such that</head><formula xml:id="formula_10">(D i f )(x) = f (x i→1 )-f (x i→-1 )</formula><p>2</p><p>. Here x i→1 means we set the i-th bit of x to 1.</p><formula xml:id="formula_11">Definition 4. Let b ∈ {-1, 1} and -1 ≤ ρ ≤ 1. A random bit b is ρ-correlated with b if b = b w.p. 1 2 + ρ 2 -b w.p. 1 2 -ρ 2</formula><p>We write it as b ∼ N ρ (b). As ρ tends to 1, b is more likely to be b.</p><p>We say that z and x are ρ-correlated if each z i is drawn independently from N ρ (x i ), for i ∈ [n]. In such a case, we write it as z ∼ N ρ (x).</p><formula xml:id="formula_12">Definition 5 (Noise Stability). Let -1 ≤ ρ ≤ 1. The ρ-noise stability of f , denoted as Stab ρ [f ], is defined to be Stab ρ [f ] = E x∼{-1,1} n y∼Nρ(x) [f (x)f (y)] . Definition 6 (Stable Influence). Let 0 ≤ ρ ≤ 1. The ρ-stable influence of f at i, denoted as Inf (ρ) i [f ], is defined to be Inf (ρ) i [f ] = Stab ρ [D i f ] = E x∼{-1,1} n y∼Nρ(x) [D i f (x) D i f (y)] . Note that when ρ = 1, this reduces to Inf i [f ]. Definition 7 (Noise Operator). Let -1 ≤ ρ ≤ 1. The noise operator T ρ is defined as T ρ f (x) = E y∼Nρ(x) [f (y)].</formula><p>The following lemma gives some elementary properties of the noise operator and stable influence.</p><p>Lemma 1 (O'Donnell <ref type="bibr" target="#b10">[11]</ref>). We have the following</p><formula xml:id="formula_13">• T ρ is a linear operator. • T ρ f = S⊆[n] ρ |S| f (S)χ S . • Stab ρ [f ] = f, T ρ f = S⊆[n] ρ |S| f (S) 2 .</formula><p>Model Inversion. Because our functions are models learned from collected data, we will make the following assumption on the models: Definition 8 (No Trivial Feature Assumption). Let f : {-1, 1} n → R be a model learned from data. The no trivial feature assumption states that every feature has nontrivial influence. That is, for any</p><formula xml:id="formula_14">i ∈ [n], Inf i [f ] &gt; 0.</formula><p>The following simple proposition shows that if Inf i [f ] = 0, then one can obtain an "equivalent" function over the Boolean cube without x i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 2. Consider any</head><formula xml:id="formula_15">f : {-1, 1} n → {-1, 1}.</formula><p>Suppose that Inf i [f ] = 0, then there exists another function g which maps x 1 , . . . , x i-1 , x i+1 , . . . , x n to {-1, 1}, such that Inf j [g] = Inf j [f ] for any j = i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. BLACK-BOX MI ATTACKS</head><p>In this section we study black-box MI attacks. Due to lack of space, we will focus on the simplest possible models -binary classification, where all the features are binary as well. In the full version of this paper, we also extend the results to binary classification over generalized but finite domains.</p><p>Recall that our main technical goal is to isolate important factors that can affect model invertibility. Unfortunately, our formulation in Section II is quite complex so many factors may play a role. For example, intuitively the training process may have an impact -if we know that a model is trained using linear regression, would this give us an advantage? On the other hand, if one thinks about MI attacks at the application phase, where a model is fixed anyway, then it suggests that invertibility should be independent of the training.</p><p>To gain more understanding, we choose to start with simple scenarios where we can characterize model invertibility exactly. Interestingly, even these very abstract and seemingly oversimplified scenarios provide insights to our main question. Perhaps more importantly, they also give rise to intriguing and natural questions that provide ample scope for future investigations.</p><p>Specifically, in this section we specialize Methodology 1 in the following ways:</p><formula xml:id="formula_16">• We consider a Boolean model h : {-1, 1} n → {-1, 1}</formula><p>in the test phase. • We assume that the model invertibility is evaluated over the uniform distribution. That is, we assume that a feature vector is drawn uniformly from U {-1,1} n . • We consider two simple auxiliary information generators. In the first, noiseless generator gen 1 , gen 1 (S, (x, y)) = (x -i , y).</p><p>In the second independent perturbation generator,</p><formula xml:id="formula_17">gen ρ (S, (x, y)) = (z -i , y)</formula><p>where each bit of z -i equals that of x -i with probability<ref type="foot" target="#foot_0">1</ref> 2 + ρ 2 , and is flipped otherwise, or z -i ∼ N ρ (x -i ). Note that for ρ = 1 it degenerates to our noiseless generator. Under these specializations our main results are summarized as follows 1 .</p><p>1) In the noiseless case, we characterize model invertibility using the influence of a Boolean function. Interestingly, it turns out in this case, model invertibility is independent of the training. These results are presented in Section IV-A. 2) In the noisy case, we show that model invertibility is related to the stable influence of a Boolean function, though stable influence does not exactly capture the invertibility. These results are presented in Section IV-B. 3) Interestingly, we find that under noise, there is an interesting phenomenon where a highly invertible model quickly becomes highly non-invertible with only a little noise. We study this phenomenon under the name "invertibility interference." The results are presented in Section IV-C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Model Invertibility with No Noise</head><p>We now specialize our methodology for black-box MI attacks to the noiseless scenario. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>iii. Adversary outputs A Γ(S) (x -i , h(x)). The gain of this game is Pr</head><formula xml:id="formula_18">[A Γ(S) (x -i , h(x)) = x i ],</formula><p>where the probability is over samples x 1 , . . . , x n , and the randomness (if any) of the adversary. The Simulated World: In this case sgen 1 is defined as sgen 1 (S, (x, y)) = x -i . Because x i is independently and uniformly drawn from {-1, 1}, so for any simulated attack</p><formula xml:id="formula_19">A * , Pr[A * (x -i ) = x i ] = 1/2.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Therefore, the advantage of the game is defined to be</head><formula xml:id="formula_20">Pr[A Γ(S) (x -i , h(x)) = x i ] -1/2.</formula><p>For this type of MI attack, we consider the following deterministic algorithm for the adversary A.</p><p>We have the following simple lemma.</p><formula xml:id="formula_21">Lemma 3. Let A 1 denote Algorithm 1. Then gain(A 1 , gen 1 , x i , D U , S, Γ) = 1 2 + Inf i [Γ(S)] 2 ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>and so the advantage is</head><formula xml:id="formula_22">Inf i [Γ(S)] 2</formula><p>. Further this gain is optimal.</p><p>While this lemma is trivial to prove, an interesting observation regarding it is that the invertibility is independent of the training. That is, no matter what Γ and S are (and what distribution S is drawn from), the invertibility is characterized by influence, which is an intrinsic property of the model itself.</p><p>Input: x 1 , . . . , x i-1 , x i+1 , . . . , x n . y ∈ {-1, 1}.</p><p>Oracle access to f .</p><formula xml:id="formula_23">Output: b ∈ {-1, 1}. 1 Compute y 1 = f (x 1 , . . . , x i-1 , -1, x i+1 , . . . , x n ),</formula><p>and 2 y 2 = f (x 1 , . . . , x i-1 , +1, x i+1 , . . . , x n ).</p><p>3 If y 1 = y 2 , then if y 1 = y, output -1, otherwise output +1. 4 Otherwise, output the constant 1. Algorithm 1: A Deterministic Algorithm for Noiseless Uniform Flat MI Attack.</p><p>For noiseless MI attacks, it is also easy to characterize the most and least invertible functions. In the following recall that we assume the nontrivial feature assumption.</p><p>Most Invertible Functions. With the no trivial feature assumption, the most invertible function is χ [n] (x) = n i=1 x i , where every coordinate has influence 1, and so the advantage is 1/2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Least Invertible Functions. What functions are least invertible if we measure the invertibility by the maximum influence MaxInf i [h]?</head><p>In this direction, a natural candidate is the majority function. Indeed, using Stirling's formula (see Exercise 2.22 <ref type="bibr" target="#b10">[11]</ref>.), one can estimate that</p><formula xml:id="formula_24">Inf i [MAJ n ] ≈ O(1/ √ n) for every i ∈ [n].</formula><p>There are functions with much smaller influence. For example,</p><formula xml:id="formula_25">OR n (x) = 1 x 1 = x 2 = • • • = x n = 1. -1 otherwise.</formula><p>Then it is easy to check that Inf i [OR n ] = 2 1-n for every i ∈ [n]. We can also characterize the structure of the least invertible functions. Under the no trivial feature assumption, these functions are those that are "constant except at one point."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 4. Consider any</head><formula xml:id="formula_26">h : {-1, 1} n → {-1, 1}. If Inf i [h] &gt; 0, then Inf i [h] ≥ 2 1-n . Lemma 5. Let h : {-1, 1} n → {-1, 1} be a Boolean function. If Inf i [h] = 2 1-n for some i ∈ [n], then Inf i [h] &gt; 0 for every i ∈ [n]. Theorem 1. Let h : {-1, 1} n → {-1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>, 1} be a Boolean function. Then h satisfies the property that for every i ∈</head><formula xml:id="formula_27">[n], Inf i [h] = 2 1-n if</formula><p>and only if h is constant except at a unique point x 0 . In other words, there exist</p><formula xml:id="formula_28">x 0 ∈ {-1, 1} n and b ∈ {-1, 1} such that h(x) = b if x = x 0 , -b otherwise.</formula><p>Recall that a Boolean-valued function is unanimous if h(1, . . . , 1) = 1 and h(-1, . . . , -1) = -1. Therefore we have the following two corollaries, Corollary 1. OR n and AND n are the only unanimous Boolean functions where maximum influence is 2 1-n . Corollary 2. OR n and AND n are the only monotone Boolean functions where maximum influence is 2 1-n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Model Invertibility with Independent Noise</head><p>We now move on to the independent perturbation case.</p><p>Definition 10 (ρ-Independent Perturbation Uniform Black-Box MI Attack). Let (Z, H, ) be a learning problem where H consists of hypotheses of the form {-1, 1} n → {-1, 1}. Let Γ be a learning algorithm and S be a training set. For simplicity we denote Γ(S) as h. ρ-Independent Perturbation Uniform Black-Box MI attack for coordinate i is the following game.</p><p>The MI-Attack world: Let A be a probabilistic algorithm with binary output, then i. Nature draws (x, y) from D U . That is, nature draws x ∼ {-1, 1} n , and set y = h(x). ii. Nature presents gen ρ (S, (x, y)) = (z -i , y) to the adversary. iii. Adversary outputs A Γ(S) (z -i , y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The gain of this game is Pr</head><formula xml:id="formula_29">[A Γ(S) (z -i , y) = x i ],</formula><p>where the probability is over samples x 1 , . . . , x n , the randomness of gen ρ , and the randomness (if any) of the adversary.</p><p>The simulated world: For simulation, sgen ρ is defined as sgen ρ (S, x -i , y) = z -i . Because x i is independently and uniformly drawn from {-1, 1}, so for any simulated attack</p><formula xml:id="formula_30">A * , Pr[A * (z -i ) = x i ] = 1/2. Therefore, the advantage is defined as Pr[A Γ(S) (x -i , y) = x i ] -1/2.</formula><p>We now consider the following algorithm corresponding to the adversary A. The algorithm is the same as Algorithm 1, we repeat it here and note that now the input to the algorithm is z -i , instead of x -i . The gain of this algorithm is exactly the so called stable influence. Intuitively this is clear: Recall from Definition 6 that stable influence is defined as</p><formula xml:id="formula_31">E x∼{-1,1} n ,z∼Nρ(x) [D i f (x) D i f (z)]. Thus if D i f (x)</formula><p>and D i f (z) are of the same sign then Algorithm 2 guessed correctly. If the signs are different, then the guess is incorrect. Otherwise, one can show that the gain is 1/2. Formally, we have the following theorem.</p><formula xml:id="formula_32">Input: z 1 , . . . , z i-1 , z i+1 , . . . , z n . y ∈ {-1, 1}.</formula><p>Oracle access to f .</p><formula xml:id="formula_33">Output: b ∈ {-1, 1}. 1 Compute y 1 = f (z 1 , . . . , z i-1 , -1, z i+1 , . . . , z n ), and 2 y 2 = f (z 1 , . . . , z i-1 , +1, z i+1 , . . . , z n ). 3 If y 1 = y 2 , then if y 1 = y, output -1, otherwise output +1. 4 Otherwise, output the constant 1. Algorithm 2: A Deterministic Algorithm for ρ- Perturbation Uniform Singleton Flat MI Attack. Theorem 2. Let ρ ∈ [0, 1]. Let A ρ denote Algorithm 2. Then gain(A ρ , gen ρ , x i , D U , S, Γ) = 1 2 + Inf (ρ) i [h] 2</formula><p>where</p><formula xml:id="formula_34">Inf (ρ) i [h] is the ρ-stable influence of h (See Definition 6) at i-th coordinate. For ρ = 1, Inf 1 i [h] = E[D i f (x) D i f (z)] = E[D i f (x) 2 ] = Inf i [h].</formula><p>We thus get back the influence in the noiseless model of Lemma 3.</p><p>Remark 1 (On Optimality). Unfortunately, we note that for the ρ-independent perturbation model, Algorithm 2 no longer achieves the maximum possible gain. That is, there exists some model h : {-1, 1} n → {-1, 1} and some inversion algorithm A , such that the gain of A is larger than (1+Inf</p><formula xml:id="formula_35">(ρ) i [h])/2.</formula><p>The intuition is that, since the adversary knows h(x) exactly, so it can leverage the function table of h to "de-noise", For example, consider OR n . As long as we see that the model output is 1, we know that all input bits are 1. Therefore, the advantage we can achieve, even in the independent perturbation model, is Inf i [OR n ]/2 = 2 -n for any 0 ≤ ρ ≤ 1. On the other hand, the advantage of Algorithm 2 is ρ n-1 2 -n . We pose the following question that we would like to investigate in the future. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question 1. Consider ρ-independent perturbation model. Let A ρ denote Algorithm 2. Is it the case that for any</head><formula xml:id="formula_36">h : {-1, 1} n → {-1, 1},</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Invertibility Interference</head><p>Intuitively it is clear that noise will negatively the gain of the adversary. Theorem 2 quantifies this intuition using stable influence. For example, as we saw in the above, the gain of a natural algorithm (Algorithm 2) on OR n goes from Inf i [OR n ] to ρ n-1 Inf i [OR n ], which is exponentially small in the influence. However, this example is not very interesting in the sense that the influence of OR n is already very small (2 1-n ) in the noiseless case.</p><p>A more interesting phenomenon regarding noise is that highly invertible models in the noiseless case quickly becomes highly non-invertible due to a little noise. The reason behind is that multiple influential coordinates interfere with each other under noise. Let us see an example. In the noiseless model the most invertible function is the parity function χ [n] = i∈ <ref type="bibr">[n]</ref> x i . In this case, Inf i [f ] = 1 for every i. On the other hand, under independent noise, the invertibility of χ n becomes Inf (ρ)   n</p><formula xml:id="formula_37">χ [n] = Stab ρ D n χ [n] = χ [n-1] , T ρ χ [n-1] = ρ n-1 .</formula><p>Therefore the invertibility decays exponentially fast in n.</p><p>We term this phenomenon "invertibility interference:" When noise presents, if one does not know one of these influential coordinates exactly, then he cannot effectively invert the model to deduce the target feature. What is the stable influence if we have t influential coordinates? In this direction, we have the following simple result:</p><formula xml:id="formula_38">Theorem 3. Suppose that h : {-1, 1} n → {-1, 1} has t coordinates with influence 1. Let 0 &lt; ρ ≤ 1, then for any i ∈ [n], Inf (ρ) i [h] ≤ ρ t-1 Inf i [h]. Question 2.</formula><p>If, instead of having coordinates of influence 1, we are only guaranteed that individual influence is lower bounded by 1δ for some δ &gt; 0, how fast will the stable influence decay with respect to δ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. WHITE-BOX MI ATTACKS</head><p>We now move on to study white-box MI attacks. As discussed before, we assume that the computation of the models follows a sequential composition. This thus gives a natural view of MI attacks as communication games: One can think of each layer of the model as a player who sends a message to the next player, and the adversary as another player who observes the model output and has some additional information. Together, the goal of this game is to compute some function τ . This view gives a natural question:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>"How would knowing the communication structure and (possibly) observing some intermediate information in the communication help MI attacks?"</head><p>Empirically, the answer is that it helps a lot. As mentioned earlier, it is essential for white-box attacks as studied in <ref type="bibr" target="#b6">[7]</ref> to have access to the auxiliary confidence information, which makes the inversion algorithm much more effective.</p><p>The main purpose of this section is to give theoretical justifications for these empirical observations. At a high level, our results are summarized as follows:</p><p>1) Similar to our study of black-box MI attacks, we choose to specialize our methodology so as to obtain theoretical insights. To do so, we focus on white-box MI attacks on decision trees. An advantage of studying attacks on decision trees is its simplicity: The communication channel is very restricted, not only it is a sequential composition, but also in each iteration a player only reads a single bit of the input, and decides a binary output. 2) We show how to interpret white-box MI attacks on decision trees as alternating (communication) games. Specifically, these are communication games where the communication channel is oneway, unicast (following the sequential composition), and players alternatively hold two inputs. We give examples showing that these communication games are very restricted. 3) We show that, however, even when restricting these communication games to have 1 bit of communication between two neighboring players, it is still the case that for any goal function τ , there exists a game with enough players (corresponding to a machine learning model with enough many layers), such that there is an adversary who can compute τ correctly everywhere. This result illustrates the unexpected computational power of a restricted communication game, and in particular, that the leakage can be significant even in a very restricted white-box case. We now give more details in the rest of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Decision Trees, MI Attacks, and Alternating Games</head><p>From now on we consider oblivious decision trees, which are decision trees in which the same feature is examined at each level (of the tree). This restricts the machine learning models we consider (it is even a subclass of decision trees). Note that, however, the more we restrict the model (and its communication), the stronger our conclusion (regarding leakage in the white-box case) is if we can show significant information leakage.</p><p>We have mentioned that for a model with sequential composition, the communication channel is restricted: it is one-way and unicast. That is, each player only sends one message to the next player, in a fixed order. This is in sharp contrast with communication games studied in typical communication complexity literature <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, where the channel is either bidirectional or the messages are broadcasted.</p><p>We note that for oblivious decision trees, such communication games are further restricted. Consider an adversary who knows part of the input to the decision tree, then the communication game alternates between input he knows and input he does not know. Specifically, suppose that the input to the decision tree is z ∈ {0, 1} n , and assume that the adversary can see the bits at positions K ⊆ [n] (K stands for "known" positions), then, without loss of generality, the communication game can be viewed as: the first player examines several variables at positions in K, then sends a bit to the next player, who then examines several variables in [n] \ K, and so on. The final player, which is the adversary (who knows bits in K), determines an output.</p><p>The following definition captures our discussion so far mathematically:</p><p>Definition 11 (Alternating MI Attacks (AMI Attacks)). Let n, be natural numbers. Let k ≥ 3 be also a natural number. In Alternating MI Attack there are</p><formula xml:id="formula_39">(k -1) ≥ 2 functions: h 1 , . . . , h k-1 in the form of h 1 : {0, 1} n → {0, 1} and h i : {0, 1} n × {0, 1} → {0, 1} (i = 1, . . . , k -1)</formula><p>. let h (1) , . . . , h (k-1) : {0, 1} n × {0, 1} n → {0, 1} be the following sequence:</p><formula xml:id="formula_40">h (1) (x, y) = h 1 (x) h (i) (x, y) = h i (y, h (i-1) (x, y)) i = 2, 4, . . . h (i) (x, y) = h i (x, h (i-1) (x, y)) i = 3, 5, . . .</formula><p>Let A be a probabilistic algorithm that is an "adversary." Let τ : {0, 1} n × {0, 1} n → {0, 1} be a Boolean function on 2n bits. The alternating MI attack proceeds as follows: i. Nature samples x, y uniformly random from {0, 1} n . ii. If k is odd, then nature presents x to A, but not y.</p><p>Otherwise, nature presents y to A, but not x. iii. Nature also presents the output of h (k-1) , that is the output of the "outermost model", to A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>For odd k, the gain of the alternating MI attack is measured by</head><formula xml:id="formula_41">Pr[A(x, h (k-1) (x, y)) = τ (x, y)].</formula><p>Similarly for even k, the gain is defined as Pr[A y, h (k-1) (x, y)) = τ (x, y) . Both probabilities are taken over all the randomness: the randomness of sampling x, y (uniformly), the private randomness of h 1 , . . . , h k , and the randomness of A.</p><p>Note that the adversary in this formulation can only see the output of the outer model (h (k-1) ), beyond knowing part of the input. However, we also want to capture the intuition that the adversary may "inspect" some messages passed between layers in a machine learning model. We thus consider the following modification of the definition: Definition 12 (Alternating MI Attacks with Early Inspection). In alternating MI attacks with early inspection, the only difference is that instead of feeding h (k-1) to the adversary, the adversary can choose once to inspect the output of h (i) (1 ≤ i ≤ k -1), and based on that to compute the output.</p><p>Note that we restrict the adversary to be only able to inspect once -this is, again, to pose restriction on the communication, which gives stronger implication on the risk of leakage.</p><p>In the above definition of alternating MI attacks, we still need to distinguish between "layers" in a machine model, and the adversary. Towards our main result, which states that for any goal function τ there exists a model (with enough layers) that can allow an adversary to compute τ everywhere, we find that it is more convenient to work with a definition where we do not distinguish between functions inside a model and the function computed by the adversary. This leads to the following definition.</p><p>Definition 13 (Alternating One-Way Unicast Communication Games (AOWU)). Let n, k, be natural numbers. In Alternating One-Way Unicast Communication Games we have: <ref type="bibr" target="#b0">(1)</ref> The goal of the communication is to compute some function τ : {0, 1} n × {0, 1} n → {0, 1}. (2) There are k players, P 1 , . . . , P k . These players are allowed to use private randomness. (3) The players communicate in the way of one-way unicast. Namely, they play in the fixed order of P 1 , . . . , P k , and player P i is only allowed to send one message, a bit string of length , to player P i+1 , for i = 1, . . . , k -1. (4) P k is required to output a single bit, which is viewed as the output of the protocol. Similar to alternating MI attack, one can define the sequence of composed function P (1) , . . . , P (k) , where P (i) is the function computed by the first i players on {0, 1} n × {0, 1} n : P (1) (x, y) = P 1 (x) P (i) (x, y) = P i (y, P (i-1) (x, y)) i = 2, 4, . . . P (i) (x, y) = P i (x, P (i-1) (x, y)) i = 3, 5, . . . For AMI Attacks with Early Inspection, we have the following definition, Definition 14 (AOWU * ). An AOWU game with early stopping, or called an AOWU * game, is an AOWU game where any player P i , i ∈ [k], can stop the protocol, and claim his or her output as the output of the protocol.</p><p>From our discussion so far it follows that Lemma 6. (n, k, )-alternating MI attack and (n, k, )-AOWU games are equivalent. Further, (n, k,alternating MI attack with early inspection and (n, k, )-AOWU * games are equivalent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. On the Power of AMI Attacks with Early Inspection</head><p>We now study the power of alternating MI attacks with early inspection. Clearly, we can equivalently study AOWU games with early stop. We show that, even when restricting to 1 bit of communication, it is still surprisingly powerful.</p><p>To motivate this result, let us fist give an example, which illustrates "how restricted" these games are.</p><p>Example 5. Let IP(x, y) be the inner product of x and y, that is for x, y ∈ {0, 1} n , IP(x, y) = n i=1 (x i ∧ y i ). Consider one-way unicast alternating games that try to compute IP(x, y). The communicated messages are restricted to be of 1-bit long (that is = 1).</p><p>Let us consider the simple case that n = 2. That is, we want to compute the inner product of two length-2 bit strings. Note that in the traditional two-player communication model where Alice holds x and Bob holds y, then there is a trivial protocol where Alice sends to Bob 2 bit messages, one x 1 and one x 2 , so that Bob then has complete knowledge of x and can compute any function on x, y.</p><p>However, with AOWU games, there is a now a difficulty. Suppose that P 1 sends x 1 to P 2 , and P 2 computes x 1 y 1 . Then what will P 2 send to P 3 ? If P 2 sends y 2 to P 3 , then the progress that P 2 has made is essentially lost. However, if he sends x 1 y 1 , P 3 still does not know any information about y 2 . Therefore, at least with 2 bits communication they cannot solve the problem. What is the "right" lower bound on the number of players that are needed in order to compute inner product with 1 bit of communication?</p><p>We now construct a "universal" protocol that can compute any τ : {0, 1} n × {0, 1} n using an AOWU * protocol with 1 bit of communication. Note that odd-numbered players hold x, and evennumbered players hold y. • Player 2i -1 sends the following bit to player 2i:</p><p>He sends 1 if x is the lexicographically the i-th smallest string of all binary strings of length n. For example, player 1 sends 1 to player 2 if x = 0, and sends a bit 0 otherwise. • For player 2i, if she receives a bit 1, then she can be certain about the value of x. Because she also knows y, she can compute τ (x, y), stops the protocol early by asserting the special stopping bit, and claims the output. Otherwise, she keeps the special stop bit as 0 to indicate player 2i + 1 to continue the protocol.</p><p>Note that early stopping is essential here, otherwise player 2i + 1 cannot distinguish between a "value" that is for computing τ and a "signal" which indicates that the computation is already done. Following the construction we immediately have the following theorem, Theorem 4. Universal-1 protocols compute any τ with 1-bit message and 2 n+1 players.</p><p>Thus we have obtained the claimed main result regarding alternating MI attacks with early inspection. Theorem 5. For any goal function τ : {0, 1} n × {0, 1} n → {0, 1}, there exists a machine learning model with O(2 n ) layers, and an alternating MI attack with 1-bit communication, that computes τ correctly everywhere.</p><p>We close this section with two open questions. Question 3. For 1-bit communication, is universal-1 protocol essentially optimal in the sense that there is a function τ : {0, 1} n × {0, 1} n where any protocol computing τ requires Ω(2 n ) rounds of communications? Question 4. For 1-bit communication, is there a universal AOWU protocol (instead of AOWU * ) that computes every function τ : {0, 1} n × {0, 1} n → {0, 1}?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONNECTIONS WITH OTHER CRYPTOGRAPHIC NOTIONS</head><p>In this section we compare MI attack with two classic cryptographic primitives: Hard-Core Predicate and Secure Multiparty Computation. We assume that the readers have some basic familiarity with cryptographic terminologies. </p><formula xml:id="formula_42">[A (f (x)) = b(x)] ≤ 1 2 + μ(n).</formula><p>By viewing f as a "model", one can then simulate this definition by a black-box MI attack. Specifically, let out(x) = b(x), which is to compute a single bit. The joint distribution is J U = (U n , f(U n )). In the real world, given x ∼ U n , the auxiliary information generator gen gives the advice string gen(d f , x, f(x)) = f (x) to the adversary A . One can then observe that the gain of A in the real world, gain J U ,f,b (gen, A ), is exactly</p><formula xml:id="formula_43">Pr (x,y)∼J U [A (y) = b(x)] = Pr[A (f (U n )) = b(U n )].</formula><p>Therefore the goal of hard-core predicate is to find a "hard" predicate b so that for any adversary A , any negligible function μ(•), and all sufficiently large n's, the gain gain</p><formula xml:id="formula_44">J U ,f,b (gen, A ) ≤ 1/2 + μ(n).</formula><p>Following this simulation one can also observe two notable differences: First, in an MI attack an adversary typically has more auxiliary information than the case of hard-core predicates (which only observes the function output). For example, in a black-box MI attack as defined in Definition 9, an adversary has information about all except one feature. Second, we note that the goal of MI attacks is not to construct hard predicates. Rather, its goal is somewhat its "dual": The purpose is to study the leakage of certain model with respect to computing some output function. For statistical output, understanding this leakage may help us decide what information can be safely published.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Connection with Secure Multiparty Computation (SMC).</head><p>A more interesting notion to compare with is the Secure Multiparty Computation (SMC). To this end, we recall first the definition of m-party secure protocols Definition 16 (m-party secure protocols -sketch <ref type="bibr" target="#b19">[20]</ref>). Let f be an m-ary functionality and Π be an m-party protocol operating in the real model.</p><p>• For a real-model adversary A, controlling some minority of the parties (and tapping all communication channels), and an m-sequence x, we denote by REAL Π,A (x) the sequence of m outputs resulting from the execution of Π on input x under attack of the adversary A.</p><p>• For an ideal-model adversary A , controlling some minority of the parties, and an m sequence x, we denote by IDEAL Π,A (x) the sequence of m outputs resulting from the ideal process (that they together send input to a trusted third party, which then gives back the output) on input x under attack of the adversary A . We say that Π securely implements f with honest majority if for every feasible real model adversary A, controlling some minority of the parties, there exists a feasible ideal model, controlling the same parties, so that the probability ensembles {REAL Π,A (x)} x and {IDEAL Π,A (x)} x are computationally indistinguishable.</p><p>We observe that in this definition the privacy concerns of the output is not considered. Specifically, it can happen that in the ideal case, upon receiving the output from the trusted third party, one can infer partial information about the input of the other party. Yet such concerns will not factor in the distinguishability as they are contained in the ideal world. Put in another way, the privacy concerns considered in a secure protocol is whether the communication among parties in the real world leaks additional information compared to the ideal case.</p><p>Black-box MI Attacks and SMC. By contrast, for black-box MI attack, one considers precisely the privacy concerns of the output. That is how much sensitive information an adversary can recover from the output. To this end, one may argue that it is questionable why should one be bothered with the concern of the output, since this is the purpose of the computation.</p><p>On one hand, we feel that a fundamental difference here is what information constitutes the output. In the setting of SMC, the output is precisely defined (for example, whether two inputs are equal). However, in the setting of MI attack, the output is statistical and "noisy." For example, a model may carry too much information of some individuals if the learning procedure over-fits. Publishing such models may thus induce effective MI attacks and unwanted disclosure. Studying MI attacks can help us identify and quantify such leakage.</p><p>On the other hand, we note that, frequently, additional information is intentionally revealed by an SMC protocol due to performance considerations (for example, revealing the centroid in privacy-preserving clustering or revealing a bit of a honest party's input in the dualexecution SMC protocol <ref type="bibr" target="#b20">[21]</ref>). However, the ramifications of leaking this additional information are unclear. Perhaps our framework can be used to address such problems.</p><p>More concretely, let us consider a simple example where our current results for MI attacks (though studied in the setting of machine learning) can be applied to SMC. Consider two parties Alice and Bob who jointly compute a Boolean function f (x, y), where x is Alice's input and y is Bob's input. Suppose that the inputs are drawn uniformly from two sets X, Y . Now if Bob is malicious, and can see insensitive information x -i of Alice for x ∼ X, then with access to f how much better can he guess x i over X, compared to random guessing? Let f y (x) = f (x, y) (i.e. f y is the specialization of f where the second input is y). The answer to this question becomes exactly an MI problem against uniform distribution over X, where the adversary Bob has auxiliary information x -i . Our results in Section IV tells that the advantage is the influence of coordinate i of the function f y (•)</p><p>Therefore, what remains is to estimate the influence of coordinate i of f y . In this direction, we note that recent years there has been interesting progress on the algorithmic side of the influence theory. For example, a recent work by Ron, Rubinfeld, Safra, Samorodnistsky and Weinstein <ref type="bibr" target="#b21">[22]</ref> proves lower bounds in approximating influence and gives a better upper bound for monotone Boolean functions. By invoking their influence estimation algorithms (for example, their Algorithm 1), one can thus obtain a quantitative understanding of the risk of outputting the function f . White-box MI Attacks and SMC. This situation changes when we consider white-box MI attack. Intuitively, in composed MI attack, if one view sub-models as "parties", then we can ask how much information do these compositions (or communication) leak. Therefore, even if one modulo the concerns of the output of the outermost model, one could still investigate the additional leakage caused by the composition (or communication). This is closer to the goal of SMC.</p><p>Unlike SMC, however, is that the communication pattern of the white-box MI attacks is usually much more restricted. For example, composition of models in the usual sense gives "one-way unicast communication", rather than broadcasts, or arbitrary point-to-point communication (so it is not one-way). Indeed, as we saw in the paper, this way of communication induces intriguing and somewhat unexpected connections with communication complexity that does not seem to have been studied before. only depends on x -i , and is independent of x i , so the above is</p><formula xml:id="formula_45">Pr[b = x i | E 4 ] = 1 2 Pr[A(z -i , f(x i→1 )) = 1 | E 4 ] + Pr[A(z -i , f(x i→-1 )) = -1 | E 4 ]</formula><p>Note that conditioned on E 4 ,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pr</head><p>x-i∼{-1,1} n-1 z-i∼Nρ(x-i)</p><formula xml:id="formula_46">[A(z -i , f(x i→-1 )) = -1 | E 4 ] = Pr x-i∼{-1,1} n-1 z-i∼Nρ(x-i) [A(z -i , f(x i→1 )) = -1 | E 4 ]</formula><p>This gives that</p><formula xml:id="formula_47">Pr x-i∼{-1,1} n-1 z-i∼Nρ(x-i) [A(z -i , f(x i→1 )) = 1 | E 4 ]</formula><p>+ Pr</p><p>x-i∼{-1,1} n-1 z-i∼Nρ(x-i)</p><formula xml:id="formula_48">[A(z -i , f(x i→-1 )) = -1 | E 4 ] =1</formula><p>Therefore, the probability of guessing correctly is  The proof is complete.</p><formula xml:id="formula_49">Pr[E 1 ] + Pr[E3]+Pr[E4]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Lemma 6</head><p>Proof. By viewing the first k players P 1 , . . . , P k as models, the last player P k+1 as the adversary A , and message strings in {0, 1} as the model output in the model composition, the proof is complete.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>MI Attacks: Scenarios and Observations. Intuitively, MI attacks are designed to capture privacy concerns about participants in a training set, which arise from the following scenario: An organization trains a model over some dataset collected from a large set of individuals. After restricted access (say under some strict access control) to the model within the organization, now they want to release the model to the public for general use (e.g. say by a medical-clinic that specializes in providing personalized medicine.) We envision two mechanisms for releasing a model: release the model as a black box so public can use it freely, or release the model as a white box with some information about its architecture and parameters published. The concern is that certain correlation encoded in the model may be too strong such that a potential adversary can leverage the publicly published model, plus additional knowledge about individuals in the training set, to recover participants' sensitive information. The essential goal of studying MI attacks is to quantify the strength of such correlations so that one can have a better understanding to what degree such concerns matter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 )</head><label>1</label><figDesc>Γ: A training algorithm of the learning problem, which outputs a hypothesis Γ(S) ∈ H on an input training set S. 2) D S : A distribution over the training set S.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>and any probabilistic algorithm A , gain(A , gen ρ , x i , D U , S, Γ) ≤ gain(A ρ , gen ρ , x i , D U , S, Γ) + o n (1) ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Construction 1 (</head><label>1</label><figDesc>Universal-1 Protocol). Let τ : {0, 1} n × {0, 1} n → {0, 1} beany function. Consider the following protocol:• There are 2 • 2 n = 2 n+1 players, split into pairs,(1, 2),<ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b3">4)</ref>, (2i -1, 2i), . . . , (2 n+1 -1, 2 n+1 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2 .</head><label>2</label><figDesc>Observe that Pr[E 1 ] -Pr[E 2 ] is exactly the ρ-stable influence (ρ ∈ [0, 1]), Pr[E 1 ] -Pr[E 2 ] = E[D i f (x) D i f (z)] = Stab ρ [D i f ] = Inf (ρ) i [f ]. Combining with Pr[E 1 ]+Pr[E 2 ]+Pr[E 3 ]+Pr[E 4 ] = 1, we have that 2 Pr[E 1 ] + Pr[E 3 ] + Pr[E 4 ] = 1 + Inf (ρ) i [f ]Dividing by 2 on both sides gives the desired gain.Proof of Theorem 3Proof. We know that Inf(ρ) i [f ] = S i ρ |S|-1 f (S) 2 . Consider any S ⊆ [n] such that |S| &lt; t. We show that f (S) = 0. For this let A = {x : f (x) = χ S (x)}. We show that |A| = |A c | where A c is the complement of A. Indeed, consider any position i * / ∈ S such that Inf i * [f ] = 1. Such i * mustexist by our assumption. Thus the mapping x → x ⊕i * is a mapping from A to A c that is one-to-one and onto. Therefore,Inf (ρ) i [f ] = S i:|S|≥t ρ |S|-1 f (S) 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>2 ≤ρ t- 1</head><label>21</label><figDesc>Inf i [f ].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Fredrikson et al.'s case, for the patients participating in the dataset, y does not come from model output, but rather is determined by medical doctors. Thus h(x) is only an approximation of y.</figDesc><table><row><cell>Modeling Examples. We now discuss several examples</cell></row><row><cell>of applying our methodology.</cell></row></table><note><p><p><p><p><p>Example 1 (Warfarin Attack</p><ref type="bibr" target="#b5">[6]</ref></p>). Our first example is the Warfarin-dosage attack in the original work of Fredrikson et al.</p><ref type="bibr" target="#b5">[6]</ref></p>. The Warfarin-dosage attack is a black-box MI attack in the supervised learning setting. Thus Z = X × Y and X = n i=1 X i where X i 's are binary encoding of features, such as genotypes, race, etc. The attack, put in our formalization, is summarized in Table</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Definition 9 (Noiseless Uniform Black-Box attack). Let (Z, H, ) be a learning problem where H consists of hypotheses of the form {-1, 1} n → {-1, 1}. Let Γ be a learning algorithm and S be a training set. For simplicity we denote Γ(S) as h. Noiseless Uniform Black-Box MI attack for coordinate i is the following game.</figDesc><table /><note><p>The MI-attack world: Let A be a probabilistic algorithm with binary output, then i. Nature draws (x, y) from D U . That is, nature draws x ∼ {-1, 1} n , and set y = h(x). ii. Nature presents gen 1 (S, (x, y)) = (x -i , y) to the adversary.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Connection with Hard-Core Predicate. Let us first recall the definition of hard-core predicate Definition 15 (Hard-Core Predicate [19]). Let U n be a uniform distribution over {0, 1} n , and f : {0, 1} * → {0, 1} * . A polynomial time computable predicate b : {0, 1} * → {0, 1} is called a hard-core of f if for every probabilistic polynomial time algorithm A , there exists a negligible function μ(•) such that for all sufficiently large n's</figDesc><table><row><cell>Pr x∼Un</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>For sake of exposition, the proofs are put in the appendix.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We would like to thank Mike Hicks our shepherd and the reviewers for their helpful comments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>Proof of <ref type="bibr">Lemma 2</ref> Proof. Because Inf i [f ] = S:i∈S f (S) 2 , so if Inf i [f ] = 0, this means f (S) = 0 for any i ∈ S. In particular, we can set now g to be the following function:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Lemma 3</head><p>Proof. For any fixed x 1 , . . . , x i-1 , x i+1 , . . . , x n , if f (x) = f (x ⊕i ), then we guess x i right with probability 1. Otherwise, conditioned on f (x) = y, x i is uniformly and independently distributed over {-1, 1}. In this case, by constantly guessing 1 the correct probability is 1/2. This gives the desired gain of Algorithm 1, as well as its optimality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Lemma 4</head><p>Proof. If for any input x, f (x) = f (x ⊕i ), then there are at least two inputs, namely y = x, x ⊕i such that f (y) = f (y ⊕i ), this shows the probability</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Lemma 5</head><p>Proof. Without loss of generality, assume for contradiction that x 1 has influence 0. Then by Lemma 2, there is a function</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Theorem 1</head><p>Proof. (⇐) (⇐) (⇐) If f is constant except at a unique point x 0 , then for any i, the only inputs x on which f (x) = f (x ⊕i ) are x = x 0 and x = x ⊕i 0 . This proves that</p><p>The induction hypothesis is the following: Let g be a Boolean function on k ≤ (n -1) variables. If every coordinate of g has influence 2 1-k , then g is constant except at one point. Now let f be a function on n variables that Inf i [f ] = 2 1-n for every i ∈ [n]. Consider two Boolean functions on n-1 variables, g(x 2 , . . . , x n ) = f (1, x 2 , . . . , x n ) and h(x 2 , . . . , x n ) = f (-1, x 2 , . . . , x n ). We claim that the influence of x 2 is 2 2-n in one of g and h, and 0 in the other. Indeed, by the assumption that Inf 2 [f ] = 2 1-n , there must be a unique setting</p><p>). Note that z 1 is fixed to be 1 or -1, thus the influence of x 2 is 2 2-n in g or h, and 0 in the other.</p><p>Without loss of generality, suppose Inf 2 [g] = 2 2-n . Note that g is defined over n -1 variables, so by Lemma 5, Inf j [g] &gt; 0 for every 2 ≤ j ≤ n. On the other hand, clearly Inf j [g] ≤ 2 2-n for every j = 2, . . . , n. Thus we can apply the induction hypothesis to conclude that g is constant except one point. Moreover, h is constant. Finally, suppose g(y</p><p>This shows that f is constant except at one point. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Calibrating noise to sensitivity in private data analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mcsherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nissim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TCC</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="265" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust traceability from trace amounts</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Steinke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Vadhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 56th Annual Symposium on Foundations of Computer Science, FOCS 2015</title>
		<meeting><address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-10-20">17-20 October, 2015, 2015</date>
			<biblScope unit="page" from="650" to="669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The limits of two-party differential privacy</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mcgregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mironov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Reingold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Vadhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Colloquium on Computational Complexity (ECCC)</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">106</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Revealing information while preserving privacy</title>
		<author>
			<persName><forename type="first">I</forename><surname>Dinur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nissim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Second ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems</title>
		<meeting>the Twenty-Second ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">June 9-12, 2003. 2003</date>
			<biblScope unit="page" from="202" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The power of linear reconstruction attacks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Kasiviswanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rudelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2013</title>
		<meeting>the Twenty-Fourth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2013<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">January 6-8, 2013. 2013</date>
			<biblScope unit="page" from="1415" to="1433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Privacy in pharmacogenetics: An end-to-end case study of personalized warfarin dosing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lantz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ristenpart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd USENIX Security Symposium</title>
		<meeting>the 23rd USENIX Security Symposium<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">August 20-22, 2014. 2014</date>
			<biblScope unit="page" from="17" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Model inversion attacks that exploit confidence information and basic countermeasures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ristenpart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM conference on Computer and communications security</title>
		<meeting>the 22nd ACM conference on Computer and communications security</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image reconstruction based on local feature descriptors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Daneshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dept. Elect. Eng., Stanford Univ</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<pubPlace>Stanford, CA, USA, Tech. Rep</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">From bits to images: Inversion of local binary descriptors</title>
		<author>
			<persName><forename type="first">E</forename><surname>Angelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jacques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<idno>abs/1211.1265</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image reconstruction from bag-ofvisual-words</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2014</title>
		<meeting><address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">June 23-28, 2014, 2014</date>
			<biblScope unit="page" from="955" to="962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Analysis of Boolean Functions</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Donnell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Some bounds on multiparty communication complexity of pointer jumping</title>
		<author>
			<persName><forename type="first">C</forename><surname>Damm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jukna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sgall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Complexity</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="127" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Lower bounds for multi-player pointer jumping</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chakrabarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">22nd Annual IEEE Conference on Computational Complexity (CCC 2007)</title>
		<meeting><address><addrLine>San Diego, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06">June 2007. 2007</date>
			<biblScope unit="page" from="33" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tight bounds for set disjointness in the message passing model</title>
		<author>
			<persName><forename type="first">M</forename><surname>Braverman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ellen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Oshman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vaikuntanathan</surname></persName>
		</author>
		<idno>abs/1305.4696</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Impossibility of distributed consensus with one faulty process</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="374" to="382" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Randomized rumor spreading</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Karp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schindelhauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Vöcking</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">41st Annual Symposium on Foundations of Computer Science, FOCS 2000</title>
		<meeting><address><addrLine>Redondo Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-11-14">12-14 November 2000. 2000</date>
			<biblScope unit="page" from="565" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gossip-based computation of aggregate information</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kempe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dobra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">44th Symposium on Foundations of Computer Science (FOCS 2003)</title>
		<meeting><address><addrLine>Cambridge, MA, USA, Proceedings</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-10">October 2003. 2003</date>
			<biblScope unit="page" from="482" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Space pseudorandom generators by communication complexity lower bounds</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ganor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, APPROX/RANDOM 2014</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">September 4-6, 2014. 2014</date>
			<biblScope unit="page" from="692" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The Foundations of Cryptography</title>
		<author>
			<persName><forename type="first">O</forename><surname>Goldreich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Basic Techniques</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m">The Foundations of Cryptography</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Basic Applications</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Quid-pro-quo-tocols: Strengthening semi-honest protocols with dual execution</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security and Privacy</title>
		<meeting><address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-05-23">2012, 21-23 May 2012. 2012</date>
			<biblScope unit="page" from="272" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Approximating the influence of monotone boolean functions in o( √ n) query complexity</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rubinfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Safra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Samorodnitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Weinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOCT</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
