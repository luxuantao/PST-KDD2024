<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning with Pseudo-Ensembles</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Philip</forename><surname>Bachman</surname></persName>
							<email>phil.bachman@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University Montreal</orgName>
								<address>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ouais</forename><surname>Alsharif</surname></persName>
							<email>ouais.alsharif@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">McGill University Montreal</orgName>
								<address>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Doina</forename><surname>Precup</surname></persName>
							<email>dprecup@cs.mcgill.ca</email>
							<affiliation key="aff2">
								<orgName type="institution">McGill University Montreal</orgName>
								<address>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning with Pseudo-Ensembles</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9A73226D3170CD4377367910EAFDE0BE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We formalize the notion of a pseudo-ensemble, a (possibly infinite) collection of child models spawned from a parent model by perturbing it according to some noise process. E.g., dropout [9] in a deep neural network trains a pseudo-ensemble of child subnetworks generated by randomly masking nodes in the parent network. We examine the relationship of pseudo-ensembles, which involve perturbation in model-space, to standard ensemble methods and existing notions of robustness, which focus on perturbation in observation-space. We present a novel regularizer based on making the behavior of a pseudo-ensemble robust with respect to the noise process generating it. In the fully-supervised setting, our regularizer matches the performance of dropout. But, unlike dropout, our regularizer naturally extends to the semi-supervised setting, where it produces state-of-the-art results. We provide a case study in which we transform the Recursive Neural Tensor Network of [19] into a pseudo-ensemble, which significantly improves its performance on a real-world sentiment analysis benchmark.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Ensembles of models have long been used as a way to obtain robust performance in the presence of noise. Ensembles typically work by training several classifiers on perturbed input distributions, e.g. bagging randomly elides parts of the distribution for each trained model and boosting re-weights the distribution before training and adding each model to the ensemble. In the last few years, dropout methods have achieved great empirical success in training deep models, by leveraging a noise process that perturbs the model structure itself. However, there has not yet been much analysis relating this approach to classic ensemble methods or other approaches to learning robust models.</p><p>In this paper, we formalize the notion of a pseudo-ensemble, which is a collection of child models spawned from a parent model by perturbing it with some noise process. Sec. 2 defines pseudoensembles, after which Sec. 3 discusses the relationships between pseudo-ensembles and standard ensemble methods, as well as existing notions of robustness. Once the pseudo-ensemble framework is defined, it can be leveraged to create new algorithms. In Sec. 4, we develop a novel regularizer that minimizes variation in the output of a model when it is subject to noise on its inputs and its internal state (or structure). We also discuss the relationship of this regularizer to standard dropout methods. In Sec. <ref type="bibr" target="#b4">5</ref> we show that our regularizer can reproduce the performance of dropout in a fullysupervised setting, while also naturally extending to the semi-supervised setting, where it produces state-of-the-art performance on some real-world datasets. Sec. 6 presents a case study in which we extend the Recursive Neural Tensor Network from <ref type="bibr" target="#b18">[19]</ref> by converting it into a pseudo-ensemble. We generate the pseudo-ensemble using a noise process based on Gaussian parameter fuzzing and latent subspace sampling, and empirically show that both types of perturbation contribute to significant performance improvements beyond that of the original model. We conclude in Sec. 7.</p><p>2 What is a pseudo-ensemble?</p><p>Consider a data distribution p xy which we want to approximate using a parametric parent model f θ . A pseudo-ensemble is a collection of ξ-perturbed child models f θ (x; ξ), where ξ comes from a noise process p ξ . Dropout <ref type="bibr" target="#b8">[9]</ref> provides the clearest existing example of a pseudo-ensemble. Dropout samples subnetworks from a source network by randomly masking the activity of subsets of its input/hidden layer nodes. The parameters shared by the subnetworks, through their common source network, are learned to minimize the expected loss of the individual subnetworks. In pseudoensemble terms, the source network is the parent model, each sampled subnetwork is a child model, and the noise process consists of sampling a node mask and using it to extract a subnetwork.</p><p>The noise process used to generate a pseudo-ensemble can take fairly arbitrary forms. The only requirement is that sampling a noise realization ξ, and then imposing it on the parent model f θ , be computationally tractable. This generality allows deriving a variety of pseudo-ensemble methods from existing models. For example, for a Gaussian Mixture Model, one could perturb the means of the mixture components with, e.g., Gaussian noise and their covariances with, e.g., Wishart noise.</p><p>The goal of learning with pseudo-ensembles is to produce models robust to perturbation. To formalize this, the general pseudo-ensemble objective for supervised learning can be written as follows <ref type="foot" target="#foot_0">1</ref> :</p><formula xml:id="formula_0">minimize θ E (x,y)∼pxy E ξ∼p ξ L(f θ (x; ξ), y),<label>(1)</label></formula><p>where (x, y) ∼ p xy is an (observation, label) pair drawn from the data distribution, ξ ∼ p ξ is a noise realization, f θ (x; ξ) represents the output of a child model spawned from the parent model f θ via ξ-perturbation, y is the true label for x, and L(ŷ, y) is the loss for predicting ŷ instead of y.</p><p>The generality of the pseudo-ensemble approach comes from broad freedom in describing the noise process p ξ and the mechanism by which ξ perturbs the parent model f θ . Many useful methods could be developed by exploring novel noise processes for generating perturbations beyond the independent masking noise that has been considered for neural networks and the feature noise that has been considered in the context of linear models. For example, <ref type="bibr" target="#b16">[17]</ref> develops a method for learning "ordered representations" by applying dropout/masking noise in a deep autoencoder while enforcing a particular "nested" structure among the random masking variables in ξ, and <ref type="bibr" target="#b1">[2]</ref> relies heavily on random perturbations when training Generative Stochastic Networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related work</head><p>Pseudo-ensembles are closely related to traditional ensemble methods as well as to methods for learning models robust to input uncertainty. By optimizing the expected loss of individual ensemble members' outputs, rather than the expected loss of the joint ensemble output, pseudo-ensembles differ from boosting, which iteratively augments an ensemble to minimize the loss of the joint output <ref type="bibr" target="#b7">[8]</ref>. Meanwhile, the child models in a pseudo-ensemble share parameters and structure through their parent model, which will tend to correlate their behavior. This distinguishes pseudo-ensembles from traditional "independent member" ensemble methods, like bagging and random forests, which typically prefer diversity in the behavior of their members, as this provides bias and variance reduction when the outputs of their members are averaged <ref type="bibr" target="#b7">[8]</ref>. In fact, the regularizers we introduce in Sec. 4 explicitly minimize diversity in the behavior of their pseudo-ensemble members.</p><p>The definition and use of pseudo-ensembles are strongly motivated by the intuition that models trained to be robust to noise should generalize better than models that are (overly) sensitive to small perturbations. Previous work on robust learning has overwhelmingly concentrated on perturbations affecting the inputs to a model. For example, the optimization community has produced a large body of theoretical and empirical work addressing "stochastic programming" <ref type="bibr" target="#b17">[18]</ref> and "robust optimization" <ref type="bibr" target="#b3">[4]</ref>. Stochastic programming seeks to produce a solution to a, e.g., linear program that performs well on average, with respect to a known distribution over perturbations of parameters in the problem definition <ref type="foot" target="#foot_1">2</ref> . Robust optimization generally seeks to produce a solution to a, e.g., linear program with optimal worst case performance over a given set of possible perturbations of parameters in the problem definition. Several well-known machine learning methods have been shown equivalent to certain robust optimization problems. For example, <ref type="bibr" target="#b23">[24]</ref> shows that using Lasso (i.e. 1 regularization) in a linear regression model is equivalent to a robust optimization problem. <ref type="bibr" target="#b24">[25]</ref> shows that learning a standard SVM (i.e. hinge loss with 2 regularization in the corresponding RKHS) is also equivalent to a robust optimization problem. Supporting the notion that noise-robustness improves generalization, <ref type="bibr" target="#b24">[25]</ref> prove many of the statistical guarantees that make SVMs so appealing directly from properties of their robust optimization equivalents, rather than using more complicated proofs involving, e.g., VC-dimension. </p><formula xml:id="formula_1">Layer i-1 Layer i Layer i+1 (1)<label>(2) (3) (4)</label></formula><formula xml:id="formula_2">output f i θ : (1) compute ξ-perturbed output f i-1 θ of layers &lt; i, (2) compute f i θ from f i-1 θ , (3) ξ-perturb f i θ to get f i θ , (4) repeat up through the layers &gt; i.</formula><p>More closely related to pseudo-ensembles are recent works that consider approaches to learning linear models with inputs perturbed by different sorts of noise. <ref type="bibr" target="#b4">[5]</ref> shows how to efficiently learn a linear model that (globally) optimizes expected performance w.r.t. certain types of noise (e.g. Gaussian, zero-masking, Poisson) on its inputs, by marginalizing over the noise. Particularly relevant to our work is <ref type="bibr" target="#b20">[21]</ref>, which studies dropout (applied to linear models) closely, and shows how its effects are well-approximated by a Tikhonov (i.e. quadratic/ridge) regularization term that can be estimated from both labeled and unlabeled data. The authors of <ref type="bibr" target="#b20">[21]</ref> leveraged this label-agnosticism to achieve state-of-the-art performance on several sentiment analysis tasks. While all the work described above considers noise on the input-space, pseudo-ensembles involve noise in the model-space. This can actually be seen as a superset of input-space noise, as a model can always be extended with an initial "identity layer" that copies the noise-free input. Noise on the input-space can then be reproduced by noise on the initial layer, which is now part of the model-space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The Pseudo-Ensemble Agreement regularizer</head><p>We now present Pseudo-Ensemble Agreement (PEA) regularization, which can be used in a fairly general class of computation graphs. For concreteness, we present it in the case of deep, layered neural networks. PEA regularization operates by controlling distributional properties of the random vectors {f 2 θ (x; ξ), ..., f d θ (x; ξ)}, where f i θ (x; ξ) gives the activities of the i th layer of f θ in response to x when layers &lt; i are perturbed by ξ while layer i is left unperturbed. Fig. <ref type="figure" target="#fig_0">1</ref> illustrates the construction of these random vectors. We will assume that layer d is the output layer, i.e.f d θ (x) gives the output of the unperturbed parent model in response to x and f d θ (x; ξ) = f θ (x; ξ) gives the response of the child model generated by ξ-perturbing f θ . Given the random vectors f i θ (x; ξ), PEA regularization is defined as follows:</p><formula xml:id="formula_3">R(f θ , p x , p ξ ) = E x∼px E ξ∼p ξ d i=2 λ i V i (f i θ (x), f i θ (x; ξ)) ,<label>(2)</label></formula><p>where f θ is the parent model to regularize, x ∼ p x is an unlabeled observation, V i (•, •) is the "variance" penalty imposed on the distribution of activities in the i th layer of the pseudo-ensemble spawned from f θ , and λ i controls the relative importance of V i . Note that for Eq. 2 to act on the "variance" of the f i θ (x; ξ), we should have f i θ (x) ≈ E ξ f i θ (x; ξ). This approximation holds reasonably well for many useful neural network architectures <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b21">22]</ref>. In our experiments we actually compute the penalties V i between independently-sampled pairs of child models. We consider several different measures of variance to penalize, which we will introduce as needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The effect of PEA regularization on feature co-adaptation</head><p>One of the original motivations for dropout was that it helps prevent "feature co-adaptation" <ref type="bibr" target="#b8">[9]</ref>. That is, dropout encourages individual features (i.e. hidden node activities) to remain helpful, or at least not become harmful, when other features are removed from their local context. We provide some support for that claim by examining the following optimization objective <ref type="foot" target="#foot_2">3</ref> :</p><formula xml:id="formula_4">minimize θ E (x,y)∼pxy [L(f θ (x), y)] + E x∼px E ξ∼p ξ d i=2 λ i V i (f i θ (x), f i θ (x; ξ)) ,<label>(3)</label></formula><p>in which the supervised loss L depends only on the parent model f θ and the pseudo-ensemble only appears in the PEA regularization term. For simplicity, let</p><formula xml:id="formula_5">λ i = 0 for i &lt; d, λ d = 1, and V d (v 1 , v 2 ) = D KL (softmax(v 1 )|| softmax(v 2 ))</formula><p>, where softmax is the standard softmax and</p><formula xml:id="formula_6">D KL (p 1 ||p 2 )</formula><p>is the KL-divergence between p 1 and p 2 (we indicate this penalty by V k ). We use xent(softmax(f θ (x)), y) for the loss L(f θ (x), y), where xent(ŷ, y) is the cross-entropy between the predicted distribution ŷ and the true distribution y. Eq. 3 never explicitly passes label information through a ξ-perturbed network, so ξ only acts through its effects on the distribution of the parent model's predictions when subjected to ξ-perturbation. In this case, (3) trades off accuracy against feature co-adaptation, as measured by the degree to which the feature activity distribution at layer i is affected by perturbation of the feature activity distributions for layers &lt; i.</p><p>We test this regularizer empirically in Sec. 5.1. The observed ability of this regularizer to reproduce the performance benefits of standard dropout supports the notion that discouraging "co-adaptation" plays an important role in dropout's empirical success. Also, by acting strictly to make the output of the parent model more robust to ξ-perturbation, the performance of this regularizer rebuts the claim in <ref type="bibr" target="#b21">[22]</ref> that noise-robustness plays only a minor role in the success of standard dropout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Relating PEA regularization to standard dropout</head><p>The authors of <ref type="bibr" target="#b20">[21]</ref> show that, assuming a noise process ξ such that E ξ [f (x; ξ)] = f (x), logistic regression under the influence of dropout optimizes the following objective:</p><formula xml:id="formula_7">n i=1 E ξ [ (f θ (x i ; ξ), y i )] = n i=1 (f θ (x i ), y i )) + R(f θ ),<label>(4)</label></formula><p>where f θ (x i ) = θx i , (f θ (x i ), y i ) is the logistic regression loss, and the regularization term is:</p><formula xml:id="formula_8">R(f θ ) ≡ n i=1 E ξ [A(f θ (x i ; ξ)) -A(f θ (x i ))] ,<label>(5)</label></formula><p>where A(•) indicates the log partition function for logistic regression.</p><p>Using only a KL-d penalty at the output layer, PEA-regularized logistic regression minimizes:</p><formula xml:id="formula_9">n i=1 (f θ (x i ), y i ) + E ξ [D KL (softmax(f θ (x i )) || softmax(f θ (x i ; ξ)))] .<label>(6)</label></formula><p>Defining distribution p θ (x) as softmax(f θ (x)), we can re-write the PEA part of Eq. 6 to get:</p><formula xml:id="formula_10">E ξ [D KL (p θ (x) || p θ (x; ξ))] = E ξ c∈C p c θ (x) log p c θ (x) p c θ (x; ξ)<label>(7)</label></formula><formula xml:id="formula_11">= c∈C E ξ p c θ (x) log exp f c θ (x) c ∈C exp f c θ (x; ξ) exp f c θ (x; ξ) c ∈C exp f c θ (x)<label>(8)</label></formula><formula xml:id="formula_12">= c∈C E ξ [p c θ (x)(f c θ (x) -f c θ (x; ξ)) + p c θ (x)(A(f θ (x; ξ)) -A(f θ (x)))]<label>(9)</label></formula><formula xml:id="formula_13">= E ξ c∈C p c θ (x)(A(f θ (x; ξ)) -A(f θ (x))) = E ξ [A(f θ (x; ξ)) -A(f θ (x))]<label>(10)</label></formula><p>which brings us to the regularizer in Eq. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">PEA regularization for semi-supervised learning</head><p>PEA regularization works as-is in a semi-supervised setting, as the penalties V i do not require label information. We train networks for semi-supervised learning in two ways, both of which apply the objective in Eq. 1 on labeled examples and PEA regularization on the unlabeled examples. The first way applies a tanh-variance penalty V t and the second way applies a xent-variance penalty V x , which we define as follows:</p><formula xml:id="formula_14">V t (ȳ, ỹ) = || tanh(ȳ) -tanh(ỹ)|| 2 2 , V x (ȳ, ỹ) = xent(softmax(ȳ), softmax(ỹ)),<label>(11)</label></formula><p>where ȳ and ỹ represent the outputs of a pair of independently sampled child models, and tanh operates element-wise. The xent-variance penalty can be further expanded as:</p><formula xml:id="formula_15">V x (ȳ, ỹ) = D KL (softmax(ȳ)|| softmax(ỹ)) + ent(softmax(ȳ)),<label>(12)</label></formula><p>where ent(•) denotes the entropy. Thus, V x combines the KL-divergence penalty with an entropy penalty, which has been shown to perform well in a semi-supervised setting <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14]</ref>. Recall that at non-output layers we regularize with the "direction" penalty V c . Before the masking noise, we also apply zero-mean Gaussian noise to the input and to the biases of all nodes. In the experiments, we chose between the two output-layer penalties V t /V x based on observed performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Testing PEA regularization</head><p>We tested PEA regularization in three scenarios: supervised learning on MNIST digits, semi-supervised learning on MNIST digits, and semi-supervised transfer learning on a dataset from the NIPS 2011 Workshop on Challenges in Learning Hierarchical Models <ref type="bibr" target="#b12">[13]</ref>.</p><p>Full implementations of our methods, written with THEANO <ref type="bibr" target="#b2">[3]</ref>, and scripts/instructions for reproducing all of the results in this section are available online at: http://github.com/Philip-Bachman/Pseudo-Ensembles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Fully-supervised MNIST</head><p>The MNIST dataset comprises 60k 28x28 grayscale hand-written digit images for training and 10k images for testing. For the supervised tests we used SGD hyperparameters roughly following those in <ref type="bibr" target="#b8">[9]</ref>. We trained networks with two hidden layers of 800 nodes each, using rectified-linear activations and an 2 -norm constraint of 3.5 on incoming weights for each node. For both standard dropout (SDE) and PEA, we used softmax → xent loss at the output layer. We initialized hidden layer biases to 0.1, output layer biases to 0, and inter-layer weights to zero-mean Gaussian noise with σ = 0.01. We trained all networks for 1000 epochs with no early-stopping (i.e. performance was measured for the final network state). SDE obtained 1.05% error averaged over five random initializations. Using PEA penalty V k at the output layer and computing classification loss/gradient only for the unperturbed parent network, we obtained 1.08% averaged error. The ξ-perturbation involved node masking but not bias noise. Thus, training the same network as used for dropout while ignoring the effects of masking noise on the classification loss, but encouraging the network to be robust to masking noise (as measured by V k ), matched the performance of dropout. This result supports the equivalence between dropout and this particular form of PEA regularization, which we derived in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Semi-supervised MNIST</head><p>We tested semi-supervised learning on MNIST following the protocol described in <ref type="bibr" target="#b22">[23]</ref>. These tests split MNIST's 60k training samples into labeled/unlabeled subsets, with the labeled sets containing n l ∈ {100, 600, 1000, 3000} samples. For labeled sets of size 600, 1000, and 3000, the full training data was randomly split 10 times into labeled/unlabeled sets and results were averaged over the splits. For labeled sets of size 100, we averaged over 50 random splits. The labeled sets had the same number of examples for each class. We tested PEA regularization with and without denoising autoencoder pre-training <ref type="bibr" target="#b19">[20]</ref> <ref type="foot" target="#foot_3">4</ref> . Pre-trained networks were always PEA-regularized with penalty V x on the output layer and V c on the hidden layers. Non-pre-trained networks used V t on the output layer, except when the labeled set was of size 100, for which V x was used. In the latter case, we gradually increased the λ i over the course of training, as suggested by <ref type="bibr" target="#b6">[7]</ref>. We generated the pseudoensembles for these tests using masking noise and Gaussian input+bias noise with σ = 0.1. Each network had two hidden layers with 800 nodes. Weight norm constraints and SGD hyperparameters were set as for supervised learning.  <ref type="bibr" target="#b22">[23]</ref>) uses a nearest-neighbors-based graph Laplacian regularizer to make predictions "smooth" with respect to the manifold underlying the data distribution p x . MTC+ (the Manifold Tangent Classifier from <ref type="bibr" target="#b15">[16]</ref>) regularizes predictions to be smooth with respect to the data manifold by penalizing gradients in a learned approximation of the tangent space of the data manifold. PL+ (the Pseudo-Label method from <ref type="bibr" target="#b13">[14]</ref>) uses the joint-ensemble predictions on unlabeled data as "pseudo-labels", and treats them like "true" labels. The classification losses on true labels and pseudo-labels are balanced by a scaling factor which is carefully modulated over the course of training. PEA regularization (without pre-training) outperforms all previous methods in every setting except 100 labeled samples, where PL+ performs better, but with the benefit of pre-training. By adding pretraining (i.e. PEA+), we achieve a two-fold reduction in error when using only 100 labeled samples.  <ref type="bibr" target="#b22">[23]</ref>, Manifold Tangent Classifier <ref type="bibr" target="#b15">[16]</ref>, Pseudo-Label <ref type="bibr" target="#b13">[14]</ref>, standard dropout plus fuzzing <ref type="bibr" target="#b8">[9]</ref>, dropout plus fuzzing with pre-training, PEA, and PEA with pre-training. Methods with a "+" used contractive or denoising autoencoder pre-training <ref type="bibr" target="#b19">[20]</ref>. The testing protocol and the results left of MTC+ were presented in <ref type="bibr" target="#b22">[23]</ref>. The MTC+ and PL+ results are from their respective papers and the remaining results are our own. We trained SDE(+) using the same network/SGD hyperparameters as for PEA. The only difference was that the former did not regularize for pseudo-ensemble agreement on the unlabeled examples. We measured performance on the standard 10k test samples for MNIST, and all of the 60k training samples not included in a given labeled training set were made available without labels. The best result for each training size is in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Transfer learning challenge (NIPS 2011)</head><p>The organizers of the NIPS 2011 Workshop on Challenges in Learning Hierarchical Models <ref type="bibr" target="#b12">[13]</ref> proposed a challenge to improve performance on a target domain by using labeled and unlabeled data from two related source domains. The labeled data source was CIFAR-100 <ref type="bibr" target="#b10">[11]</ref>, which contains 50k 32x32 color images in 100 classes. The unlabeled data source was a collection of 100k 32x32 color images taken from Tiny Images <ref type="bibr" target="#b10">[11]</ref>. The target domain comprised 120 32x32 color images divided unevenly among 10 classes. Neither the classes nor the images in the target domain appeared in either of the source domains. The winner of this challenge used convolutional Spike and Slab Sparse Coding, followed by max pooling and a linear SVM on the pooled features <ref type="bibr" target="#b5">[6]</ref>. Labels on the source data were ignored and the source data was used to pre-train a large set of convolutional features. After applying the pre-trained feature extractor to the 120 training images, this method achieved an accuracy of 48.6% on the target domain, the best published result on this dataset.</p><p>We applied semi-supervised PEA regularization by first using the CIFAR-100 data to train a deep network comprising three max-pooled convolutional layers followed by a fully-connected hidden layer which fed into a softmax → xent output layer. Afterwards, we removed the hidden and output layers, replaced them with a pair of fully-connected hidden layers feeding into an 2 -hinge-loss output layer <ref type="foot" target="#foot_4">5</ref> , and then trained the non-convolutional part of the network on the 120 training images from the target domain. For this final training phase, which involved three layers, we tried standard dropout and dropout with PEA regularization on the source data. Standard dropout achieved 55.5% accuracy, which improved to 57.4% when we added PEA regularization on the source data. While most of the over the previous state-of-the-art (i.e. 48.6%) was due to dropout and an improved training strategy (i.e. supervised pre-training vs. unsupervised pre-training), controlling the feature activity and output distributions of the pseudo-ensemble on unlabeled data allowed significant further improvement.</p><p>6 Improved sentiment analysis using pseudo-ensembles</p><p>We now show how the Recursive Neural Tensor Network (RNTN) from <ref type="bibr" target="#b18">[19]</ref> can be adapted using pseudo-ensembles, and evaluate it on the Stanford Sentiment Treebank (STB) task. The STB task involves predicting the sentiment of short phrases extracted from movie reviews on RottenTomatoes.com. Ground-truth labels for the phrases, and the "sub-phrases" produced by processing them with a standard parser, were generated using Amazon Mechanical Turk. In addition to pseudoensembles, we used a more "compact" bilinear form in the function f : R n × R n → R n that the RNTN applies recursively as shown in Figure <ref type="figure" target="#fig_3">3</ref>. The computation for the i th dimension of the original f (for v i ∈ R n×1 ) is:</p><formula xml:id="formula_16">f i (v 1 , v 2 ) = tanh([v 1 ; v 2 ] T i [v 1 ; v 2 ] + M i [v 1 ; v 2 ; 1]</formula><p>), whereas we use:</p><formula xml:id="formula_17">f i (v 1 , v 2 ) = tanh(v 1 T i v 2 + M i [v 1 ; v 2 ; 1]),</formula><p>in which T i indicates a matrix slice of tensor T and M i indicates a vector row of matrix M . In the original RNTN, T is 2n × 2n × n and in ours it is n × n × n. The other parameters in the RNTNs are a transform matrix M ∈ R n×2n+1 and a classification matrix C ∈ R c×n+1 ; each RNTN outputs c class probabilities for vector v using softmax(C[v; 1]). A ";" indicates vertical vector stacking.</p><p>We initialized the model with pre-trained word vectors. The pre-training used word2vec on the training and dev set, with three modifications: dropout/fuzzing was applied during pre-training (to match the conditions in the full model), the vector norms were constrained so the pre-trained vectors had standard deviation 0.5, and tanh was applied during word2vec (again, to match conditions in the full model). All code required for these experiments is publicly available online.</p><p>We generated pseudo-ensembles from a parent RNTN using two types of perturbation: subspace sampling and weight fuzzing. We performed subspace sampling by keeping only n 2 randomly sampled latent dimensions out of the n in the parent model when processing a given phrase tree. Using the same sampled dimensions for a full phrase tree reduced computation time significantly, as the parameter matrices/tensor could be "sliced" to include only the relevant dimensions <ref type="foot" target="#foot_5">6</ref> . During training we sampled a new subspace each time a phrase tree was processed and computed testtime outputs for each phrase tree by averaging over 50 randomly sampled subspaces. We performed weight fuzzing during training by perturbing parameters with zero-mean Gaussian noise before processing each phrase tree and then applying gradients w.r.t. the perturbed parameters to the unperturbed parameters. We did not fuzz during testing. Weight fuzzing has an interesting interpretation as an implicit convolution of the objective function (defined w.r.t. the model parameters) with an isotropic Gaussian distribution. In the case of recursive/recurrent neural networks this may prove quite useful, as convolving the objective with a Gaussian reduces its curvature, thereby mitigating some problems stemming from ill-conditioned Hessians <ref type="bibr" target="#b14">[15]</ref>. For further description of the model and training/testing process, see the supplementary material and the code from http://github.com/Philip-Bachman/Pseudo-Ensembles. RNTN is the original "full" model presented in <ref type="bibr" target="#b18">[19]</ref>. CTN is our "compact" tensor network model. +F/S indicates augmenting our base model with weight fuzzing/subspace sampling. PV is the Paragraph Vector model in <ref type="bibr" target="#b11">[12]</ref> and DCNN is the Dynamic Convolutional Neural Network model in <ref type="bibr" target="#b9">[10]</ref>.  Following the protocol suggested by <ref type="bibr" target="#b18">[19]</ref>, we measured root-level (i.e. whole-phrase) prediction accuracy on two tasks: fine-grained sentiment prediction and binary sentiment prediction. The fine-grained task involves predicting classes from 1-5, with 1 indicating strongly negative sentiment and 5 indicating strongly positive sentiment. The binary task is similar, but ignores "neutral" phrases (those in class 3) and considers only whether a phrase is generally negative (classes 1/2) or positive (classes 4/5). Table <ref type="table" target="#tab_2">2</ref> shows the performance of our compact RNTN in four forms that include none, one, or both of subspace sampling and weight fuzzing. Using only 2 regularization on its parameters, our compact RNTN approached the performance of the full RNTN, roughly matching the performance of the second best method tested in <ref type="bibr" target="#b18">[19]</ref>. Adding weight fuzzing improved performance past that of the full RNTN. Adding subspace sampling improved performance further and adding both noise types pushed our RNTN well past the full RNTN, resulting in state-ofthe-art performance on the binary task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RNTN PV DCNN CTN CTN+F CTN+S CTN+F+S</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>We proposed the notion of a pseudo-ensemble, which captures methods such as dropout <ref type="bibr" target="#b8">[9]</ref> and feature noising in linear models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21]</ref> that have recently drawn significant attention. Using the conceptual framework provided by pseudo-ensembles, we developed and applied a regularizer that performs well empirically and provides insight into the mechanisms behind dropout's success. We also showed how pseudo-ensembles can be used to improve the performance of an already powerful model on a competitive real-world sentiment analysis benchmark. We anticipate that this idea, which unifies several rapidly evolving lines of research, can be used to develop several other novel and successful algorithms, especially for semi-supervised learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: How to compute partial noisy</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Performance of PEA regularization for semi-supervised learning using the MNIST dataset. The top row of filter blocks in (a) were the result of training a fixed network architecture on 600 labeled samples using: weight norm constraints only (RAW), standard dropout (SDE), standard dropout with PEA regularization on unlabeled data (PEA), and PEA preceded by pre-training as a denoising autoencoder [20] (PEA+PT). The bottom filter block in (a) was the result of training with PEA on 100 labeled samples. (b) shows test error over the course of training for RAW/SDE/PEA, averaged over 10 random training sets of size 600/1000.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: How to feedforward through theRecursive Neural Tensor Network. First, the tree structure is generated by parsing the input sentence. Then, the vector for each node is computed by look-up at the leaves (i.e. words/tokens) and by a tensor-based transform of the node's children's vectors otherwise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table /><note><p>compares the performance of PEA regularization with previous results. Aside from CNN, all methods in the table are "general", i.e. do not use convolutions or other image-specific techniques to improve performance. The main comparisons of interest are between PEA(+) and other methods for semi-supervised learning with neural networks, i.e. E-NN, MTC+, and PL+. E-NN (EmbedNN from</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance of semi-supervised learning methods on MNIST with varying numbers of labeled samples. From left-to-right the methods are Transductive SVM , neural net, convolutional neural net, EmbedNN</figDesc><table><row><cell></cell><cell>TSVM</cell><cell>NN</cell><cell cols="4">CNN E-NN MTC+ PL+</cell><cell cols="4">SDE SDE+ PEA PEA+</cell></row><row><cell cols="5">100 16.81 25.81 22.98 16.86</cell><cell cols="6">12.03 10.49 22.89 13.54 10.79 5.21</cell></row><row><cell>600</cell><cell>6.16</cell><cell cols="2">11.44 7.68</cell><cell>5.97</cell><cell>5.13</cell><cell>4.01</cell><cell>7.59</cell><cell>5.68</cell><cell>2.44</cell><cell>2.87</cell></row><row><cell>1000</cell><cell>5.38</cell><cell cols="2">10.70 6.45</cell><cell>5.73</cell><cell>3.64</cell><cell>3.46</cell><cell>5.80</cell><cell>4.71</cell><cell>2.23</cell><cell>2.64</cell></row><row><cell>3000</cell><cell>3.45</cell><cell>6.04</cell><cell>3.35</cell><cell>3.59</cell><cell>2.57</cell><cell>2.69</cell><cell>3.60</cell><cell>3.00</cell><cell>1.91</cell><cell>2.30</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Fine-grained and binary root-level prediction performance for the Stanford Sentiment Treebank task.</figDesc><table><row><cell>Fine-grained</cell><cell>45.7</cell><cell>48.7</cell><cell>48.5</cell><cell>43.1</cell><cell>46.1</cell><cell>47.5</cell><cell>48.4</cell></row><row><cell>Binary</cell><cell>85.4</cell><cell>87.8</cell><cell>86.8</cell><cell>83.4</cell><cell>85.3</cell><cell>87.8</cell><cell>88.9</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>It is easy to formulate analogous objectives for unsupervised learning, maximum likelihood, etc.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Note that "parameters" in a linear program are analogous to inputs in standard machine learning terminology, as they are observed quantities (rather than quantities optimized over).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>While dropout is well-supported empirically, its mode-of-action is not well-understood outside the limited context of linear models.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>See our code for a perfectly complete description of our pre-training.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>We found that 2-hinge-loss performed better than softmax → xent in this setting. Switching to softmax → xent degrades the dropout and PEA results but does not change their ranking.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>This allowed us to train significantly larger models before over-fitting offset increased model capacity. But, training these larger models would have been tedious without the parameter slicing permitted by subspace sampling, as feedforward for the RNTN is O(n 3 ).</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Understanding dropout</title>
		<author>
			<persName><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sadowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">É</forename><surname>Thibodeau-Laufer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.1091v5[cs.LG]</idno>
		<title level="m">Deep generative stochastic networks trainable by backprop</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Theano: A cpu and gpu math expression compiler</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Python for Scientific Computing Conference (SciPy)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Theory and applications of robust optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bertsimas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Caramanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning with marginalized corrupted features</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large-scale feature learning with spike-and-slab sparse coding</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Semi-Supervised Learning, chapter Entropy Regularization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<title level="m">Elements of Statistical Learning II</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580v1[cs.NE]</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Workshop on challenges in learning hierarchical models: Transfer learning and optimization</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName><forename type="first">D.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On the difficulties of training recurrent neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pacanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The manifold tangent classifier</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rifai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning ordered representations with nested dropout</title>
		<author>
			<persName><forename type="first">O</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Gelbart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<author>
			<persName><forename type="first">A</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dentcheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ruszczynski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lectures on Stochastic Programming: Modeling and Theory</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dropout training as adaptive regularization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An empirical analysis of dropout in piecewise linear networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep learning via semi-supervised embedding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ratle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Robust regression and lasso</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Caramanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Robustness and regularization of support vector machines</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Caramanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
