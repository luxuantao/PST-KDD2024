<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Linformer: Self-Attention with Linear Complexity</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-06-14">14 Jun 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sinong</forename><surname>Wang</surname></persName>
							<email>sinongwang@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Belinda</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
							<email>belindali@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
							<email>mkhabsa@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Han</forename><surname>Fang</surname></persName>
							<email>hanfang@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
							<email>haom@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Linformer: Self-Attention with Linear Complexity</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-06-14">14 Jun 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2006.04768v3[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large transformer models have shown extraordinary success in achieving state-ofthe-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses O(n 2 ) time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from O(n 2 ) to O(n) in both time and space. The resulting linear transformer, the Linformer, performs on par with standard Transformer models, while being much more memory-and time-efficient.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transformer models <ref type="bibr" target="#b28">(Vaswani et al., 2017)</ref> have become ubiquitous for wide variety of problems in natural language processing (NLP), including translation <ref type="bibr" target="#b20">(Ott et al., 2018)</ref>, text classification, question answering, among others <ref type="bibr" target="#b24">(Raffel et al., 2019;</ref><ref type="bibr" target="#b19">Mohamed et al., 2019)</ref>. Over the last couple of years, the number of parameters in state-of-the-art NLP transformers has grown drastically, from the original 340 million introduced in BERT-Large to 175 billion in <ref type="bibr">GPT-3 (Brown et al., 2020)</ref>. Although these large-scale models yield impressive results on wide variety of tasks, training and deploying such model are slow in practice. For example, the original BERT-Large model <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> takes four days to train on 16 Cloud TPUs, and the recent <ref type="bibr">GPT-3 (Brown et al., 2020)</ref> consumed orders of magnitude more petaflops / day to train compared to its predecessor, GPT-2 <ref type="bibr" target="#b23">(Radford et al., 2019)</ref>. Beyond training, deploying Transformer models to real world applications is also expensive, usually requiring extensive distillation <ref type="bibr" target="#b9">(Hinton et al., 2015)</ref> or compression.</p><p>The main efficiency bottleneck in Transformer models is its self-attention mechanism. Here, each token's representation is updated by attending to all other tokens in the previous layer. This operation is key for retaining long-term information, giving Transformers the edge over recurrent models on long sequences. However, attending to all tokens at each layer incurs a complexity of O(n 2 ) with respect to sequence length. Thus, in this paper, we seek to answer the question: can Transformer models be optimized to avoid this quadratic operation, or is this operation required to maintain strong performance?</p><p>Prior work has proposed several techniques for improving the efficiency of self-attention. One popular technique is introducing sparsity into attention layers <ref type="bibr" target="#b5">(Child et al., 2019;</ref><ref type="bibr" target="#b22">Qiu et al., 2019;</ref><ref type="bibr" target="#b1">Beltagy et al., 2020)</ref> by having each token attend to only a subset of tokens in the whole sequence. This reduces the overall complexity of the attention mechanism to O(n ? n) <ref type="bibr" target="#b5">(Child et al., 2019)</ref>. However, as shown in <ref type="bibr" target="#b22">Qiu et al. (2019)</ref>, this approach suffers from a large performance drop with limited efficiency gains, i.e., a 2% drop with only 20% speed up. More recently, the Reformer <ref type="bibr" target="#b12">(Kitaev et al., 2020)</ref> used locally-sensitive hashing (LSH) to reduce the self-attention complexity to O(n log(n)). However, in practice, the Reformer's efficiency gains only appear on sequences with length &gt; 2048 (Figure <ref type="figure">5</ref> in <ref type="bibr" target="#b12">Kitaev et al. (2020)</ref>). Furthermore, the Reformer's multi-round hashing approach actually increases the number of sequential operations, which further undermines their final efficiency gains.</p><p>In this work, we introduce a novel approach for tackling the self-attention bottleneck in Transformers. Our approach is inspired by the key observation that self-attention is low rank. More precisely, we show both theoretically and empirically that the stochastic matrix formed by self-attention can be approximated by a low-rank matrix. Empowered by this observation, we introduce a novel mechanism that reduces self-attention to an O(n) operation in both space-and time-complexity: we decompose the original scaled dot-product attention into multiple smaller attentions through linear projections, such that the combination of these operations forms a low-rank factorization of the original attention. A summary of runtimes for various Transformer architectures, including ours, can be found in Table <ref type="table" target="#tab_0">1</ref>.</p><p>One predominant application of Transformers, that has seen the most gains, is using them as pretrained language models, whereby models are first pretrained with a language modeling objective on a large corpus, then finetuned on target tasks using supervised data <ref type="bibr" target="#b6">(Devlin et al., 2019;</ref><ref type="bibr" target="#b15">Liu et al., 2019;</ref><ref type="bibr" target="#b13">Lewis et al., 2019)</ref>. Following <ref type="bibr" target="#b6">Devlin et al. (2019)</ref>, we pretrain our model on BookCorpus <ref type="bibr" target="#b30">(Zhu et al., 2015)</ref> plus English Wikipedia using masked-language-modeling objective. We observe similar pretraining performance to the standard Transformer model. We then finetune our pretrained models on three tasks from GLUE <ref type="bibr" target="#b29">(Wang et al., 2018)</ref> and one sentiment analysis task, IMDB reviews <ref type="bibr" target="#b16">(Maas et al., 2011)</ref>. On these tasks, we find that our model performs comparably, or even slightly better, than the standard pretrained Transformer, while observing significant training and inference speedups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Architecture</head><p>Complexity per Layer Sequential Operation <ref type="bibr" target="#b28">(Vaswani et al., 2017)</ref> O(n 2 ) O(1) Sparse Tansformer, <ref type="bibr" target="#b5">(Child et al., 2019)</ref> O(n ? n) O(1) Reformer, <ref type="bibr" target="#b12">(Kitaev et al., 2020</ref>) 2 Backgrounds and Related works</p><formula xml:id="formula_0">Recurrent O(n) O(n) Transformer,</formula><formula xml:id="formula_1">O(n log(n)) O(log(n)) Linformer O(n) O(1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Transformer and Self-Attention</head><p>The Transformer is built upon the idea of Multi-Head Self-Attention (MHA), which allows the model to jointly attend to information at different positions from different representation subspaces. MHA is defined as</p><formula xml:id="formula_2">MultiHead(Q, K, V ) = Concat(head 1 , head 2 , . . . , head h )W O ,<label>(1</label></formula><p>) where Q, K, V ? R n?dm are input embedding matrices, n is sequence length, d m is the embedding dimension, and h is the number of heads. Each head is defined as:</p><formula xml:id="formula_3">head i = Attention(QW Q i , KW K i , V W V i ) = softmax QW Q i (KW K i ) T ? d k P V W V i ,<label>(2)</label></formula><p>where</p><formula xml:id="formula_4">W Q i , W K i ? R dm?d k , W V i ? R dm?dv , W O ? R hdv?dm</formula><p>are learned matrices and d k , d v are the hidden dimensions of the projection subspaces. For the rest of this paper, we will not differentiate between d k and d v and just use d.</p><p>The self-attention defined in (2) refers to a context mapping matrix P ? R n?n . The Transformer uses P to capture the input context for a given token, based on a combination of all tokens in the sequence. However, computing P is expensive. It requires multiplying two n ? d matrices, which is O(n 2 ) in time and space complexity. This quadratic dependency on the sequence length has become a bottleneck for Transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Related works</head><p>There has been much prior literature on improving the efficiency of Transformers, especially the self-attention bottleneck. The most common techniques for model efficiency that can be applied to Transformers (some specific to Transformers, others more general-purpose) include:</p><p>Mixed Precision <ref type="bibr" target="#b18">(Micikevicius et al., 2017)</ref>: Using half-precision or mixed-precision representations of floating points is popular in deep learning, and is also widely used in training Transformers <ref type="bibr" target="#b21">(Ott et al., 2019)</ref>. This technique can be further improved through Quantization Aware Training <ref type="bibr" target="#b11">(Jacob et al., 2018;</ref><ref type="bibr" target="#b8">Fan et al., 2020)</ref>, where the weights are quantized during training and the gradients are approximated with the Straight-Through Estimator. This line of work is orthogonal to our approach, and we use mixed-precision training by default.</p><p>Knowledge Distillation <ref type="bibr" target="#b9">(Hinton et al., 2015)</ref>: Knowledge distillation aims to transfer the "knowledge" from a large teacher model to a lightweight student model. The student model is then used during inference. However this approach has drawbacks: It does not address speeding up the teacher model during training, and moreover, student models usually suffer performance degradation compared to the teacher model. For example, when distilling a 12-layer BERT to a 6-layer BERT, the student model experiences an average 2.5% performance drop on several benchmark tasks <ref type="bibr" target="#b26">(Sanh et al., 2019)</ref>.</p><p>Sparse Attention <ref type="bibr" target="#b5">(Child et al., 2019)</ref>: This technique improves the efficiency of self-attention by adding sparsity in the context mapping matrix P . For example, the Sparse Transformer <ref type="bibr" target="#b5">(Child et al., 2019)</ref> only computes P ij around the diagonal of matrix P (instead of the all P ij ). Meanwhile, blockwise self-attention <ref type="bibr" target="#b22">(Qiu et al., 2019)</ref> divides P into multiple blocks and only computes P ij within the selected blocks. However, these techniques also suffer a large performance degradation, while having only limited additional speed-up, i.e., 2% drop with 20% speed up.</p><p>LSH Attention <ref type="bibr" target="#b12">(Kitaev et al., 2020)</ref>: Locally-sensitive hashing (LSH) attention utilizes a multi-round hashing scheme when computing dot-product attention, which in theory reduces the self-attention complexity to O(n log(n)). However, in practice, their complexity term has a large constant 128 2 and it is only more efficient than the vanilla transformer when sequence length is extremely long.</p><p>Improving Optimizer Efficiency: Microbatching <ref type="bibr" target="#b10">(Huang et al., 2019)</ref> splits a batch into small microbatches (which can be fit into memory), and then separately runs forward and backward passes on them with gradient accumulation. Gradient checkpointing <ref type="bibr" target="#b3">(Chen et al., 2016)</ref> saves memory by only caching activations of a subset of layers. The uncached activations are recomputed during backpropagation from the latest checkpoint. Both techniques trade off time for memory, and do not speed up inference.</p><p>As we've noted, most common techniques have limitations in reducing both the training and inference time/memory consumption, we investigate how to optimize the self-attention layers and introduce our approach next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Self-Attention is Low Rank</head><p>In this section, we demonstrate that the self-attention mechanism, i.e., the context mapping matrix P , is low-rank.</p><p>We first provide a spectrum analysis of the context mapping matrix P . We use two pretrained transformer models, RoBERTa-base (12-layer stacked transformer) and RoBERTa-large (24-layer stacked transformer) <ref type="bibr" target="#b15">(Liu et al., 2019)</ref> on two tasks: masked-language-modeling task on Wiki103 <ref type="bibr" target="#b17">(Merity et al., 2016)</ref> and classification task on IMDB <ref type="bibr" target="#b16">(Maas et al., 2011)</ref>. In Figure <ref type="figure" target="#fig_0">1</ref> (left), we apply singular value decomposition into P across different layers and different heads of the model, and plot the normalized cumulative singular value averaged over 10k sentences. The results exhibit a clear long-tail spectrum distribution across each layer, head and task. This implies that most of the information of matrix P can be recovered from the first few largest singular values. In Figure <ref type="figure" target="#fig_0">1</ref> (right), we plot a heatmap of the normalized cumulative singular value at the 128-th largest singular value (out of 512). We observe that the spectrum distribution in higher layers is more skewed than in lower layers, meaning that, in higher layers, more information is concentrated in the largest singular values and the rank of P is lower.</p><p>Below, we provide a theoretical analysis of the above spectrum results.</p><formula xml:id="formula_5">Theorem 1. (self-attention is low rank) For any Q, K, V ? R n?d and W Q i , W K i , W V i ? R d?d , for any column vector w ? R n of matrix V W V</formula><p>i , there exists a low-rank matrix P ? R n?n such that Pr( P w T -P w T &lt; P w T ) &gt; 1 -o(1) and rank( P ) = ?(log(n)),</p><p>(3) where the context mapping matrix P is defined in (2). Proof. Based on the definition of the context mapping matrix P , we can write</p><formula xml:id="formula_6">P = softmax QW Q i (KW K i ) T ? d A = exp (A) ? D -1 A ,<label>(4)</label></formula><p>where D A is an n ? n diagonal matrix. The main idea of this proof is based on the distributional Johnson-Lindenstrauss lemma (Lindenstrauss, 1984) (JL for short). We construct the approximate low rank matrix as</p><formula xml:id="formula_7">P = exp (A) ? D -1 A R T R</formula><p>, where R ? R k?n with i.i.d. entries from N (0, 1/k). We can then use the JL lemma to show that, for any column vector w ? R n of matrix V W V i , when k = 5 log(n)/( 2 -3 ), we have</p><formula xml:id="formula_8">Pr P R T Rw T -P w T ? P w T &gt; 1 -o(1).</formula><p>(5)</p><p>For more details, refer to the supplementary materials.</p><p>Given the low-rank property of the context mapping matrix P , one straightforward idea is to use singular value decomposition (SVD) to approximate P with a low-rank matrix P low , as follows</p><formula xml:id="formula_9">P ? P low = k i=1 ? i u i v T i = u 1 , ? ? ? , u k k diag{? 1 , ? ? ? , ? k } ? ? ? v 1 . . . v k ? ? ? ? ? ? ? ? k (6)</formula><p>where ? i , u i and v i are the i largest singular values and their corresponding singular vectors. Based on the results in Theorem 1 and the Eckart-Young-Mirsky Theorem <ref type="bibr" target="#b7">(Eckart &amp; Young, 1936)</ref>, one can use P low to approximate self-attention (2) with error and O(nk) time and space complexity. However, this approach requires performing an SVD decomposition in each self-attention matrix, which adds additional complexity. Therefore, we propose another approach for low-rank approximation that avoids this added complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model</head><p>In this section, we propose a new self-attention mechanism which allows us to compute the contextual mapping P ? V W V i in linear time and memory complexity with respect to sequence length. The main idea of our proposed linear self-attention (Figure <ref type="figure" target="#fig_1">2</ref>) is to add two linear projection matrices E i , F i ? R n?k when computing key and value. We first project the original</p><formula xml:id="formula_10">(n ? d)-dimensional Scaled Dot-Product Attention Linear Concat Scaled Dot-Product Attention Linear Linear V K Q Projection Projection Linear Linear Linear Linear n ? dm dm ? dk k ? n ? Linformer ? = k ? dk W ! (W " ) K (V)</formula><p>Sequence length / batch size key and value layers KW K i and V W V i into (k ? d)-dimensional projected key and value layers. We then compute an (n ? k)-dimensional context mapping matrix P using scaled dot-product attention.</p><formula xml:id="formula_11">head i = Attention(QW Q i , E i KW K i , F i V W V i ) = softmax QW Q i (E i KW K i ) T ? d k P :n?k ? F i V W V i k?d ,<label>(7)</label></formula><p>Finally, we compute context embeddings for each head i using P ? (F i V W V i ). Note the above operations only require O(nk) time and space complexity. Thus, if we can choose a very small projected dimension k, such that k n, then we can significantly reduce the memory and space consumption. The following theorem states that, when k = O(d/ 2 ) (independent of n), one can approximate P ? V W V i using linear self-attention (7) with error. Theorem 2. (Linear self-attention) For any</p><formula xml:id="formula_12">Q i , K i , V i ? R n?d and W Q i , W K i , W V i ? R d?d , if k = min{?(9d log(d)/ 2</formula><p>), 5?(log(n)/ 2 )}, then there exists matrices E i , F i ? R n?k such that, for any row vector w of matrix</p><formula xml:id="formula_13">QW Q i (KW K i ) T / ? d, we have Pr softmax(wE T i )F i V W V i -softmax(w)V W V i ? softmax(w) V W V i &gt; 1 -o(1) (8)</formula><p>Proof. The main idea of proof is based on the distributional Johnson-Lindenstrauss lemma <ref type="bibr" target="#b14">(Lindenstrauss, 1984)</ref>. We first prove that for any row vector x ? R n of matrix</p><formula xml:id="formula_14">QW Q i (KW K i ) T / ? d k and column vector y ? R n of matrix V W V i , Pr exp(xE T i )F i y T -exp(x)y T ? exp(x)y T &gt; 1 -2e -( 2 -3 )k/4 ,<label>(9)</label></formula><p>where E i = ?R and F i = e -? R, where R ? R k?n with i.i.d. entries from N (0, 1/k) and ? is a small constant. Applying the result in (9) to every row vector of matrix A and every column vector of matrix V , one can directly prove that, for any row vector A i of matrix A,</p><formula xml:id="formula_15">Pr exp(A i E T i )F i V -exp(A i )V ? exp(A i )V &gt; 1 -o(1),<label>(10)</label></formula><p>by setting k = 5 log(nd)/( 2 -3 ). This result does not utilize the low rank property of matrix A (rank(A)=d) and the resultant k has a dependency on sequence length n. We will further utlize the fact that rank(A)=d to prove the choice of k can be constant and independent of sequence length n.</p><p>For more details, refer to the supplementary materials.</p><p>In Figure <ref type="figure" target="#fig_1">2</ref> (top right), we plot the inference speed of Linformer and standard Transformer versus sequence length, while holding the total number of tokens fixed. We see that while standard Transformer becomes slower at longer sequence lengths, the Linformer speed remains relatively flat and is significantly faster at long sequences.</p><p>Additional Efficiency Techniques Several additional techniques can be introduced on top of Linformer to further optimize for both performance and efficiency:</p><p>Parameter sharing between projections: One can share parameters for the linear projection matrices E i , F i across layers and heads. In particular, we experimented with 3 levels of sharing:</p><p>? Headwise sharing: for each layer, we share two projection matrices E and F such that E i = E and F i = F across all heads i. ? Key-value sharing: we do headwise sharing, with the additional constraint of sharing the key and value projections. For each layer, we create a single projection matrix E such that E i = F i = E for each key-value projection matrix across all head i. ? Layerwise sharing: we use a single projection matrix E across all layers, for all heads, and for both key and value.</p><p>For example, in a 12-layer, 12-head stacked Transformer model, headwise sharing, key-value sharing and layerwise sharing will introduce 24, 12, and 1 distinct linear projection matrices, respectively.</p><p>Nonuniform projected dimension: One can choose a different projected dimension k for different heads and layers. As shown in Figure <ref type="figure" target="#fig_0">1</ref> (right), the contextual mapping matrices in different heads and layers have distinct spectrum distributions, and heads in higher layer tend towards a more skewed distributed spectrum (lower rank). This implies one can choose a smaller projected dimension k for higher layers.</p><p>General projections: One can also choose different kinds of low-dimensional projection methods instead of a simple linear projection. For example, one can choose mean/max pooling, or convolution where the kernel and stride is set to n/k. The convolutional functions contain parameters that require training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we present experimental results for the the techniques described above. We analyze the techniques one-by-one and explore how they impact performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Pretraining Perplexities</head><p>We first compare the pretraining performance of our proposed architecture against RoBERTa <ref type="bibr" target="#b15">(Liu et al., 2019)</ref>, which is based on the Transformer. Following <ref type="bibr" target="#b6">Devlin et al. (2019)</ref>, we use BookCorpus <ref type="bibr" target="#b30">(Zhu et al., 2015)</ref> plus English Wikipedia as our pretraining set (3300M words). All models are pretrained with the masked-language-modeling (MLM) objective, and the training for all experiments are parallelized across 64 Tesla V100 GPUs with 250k updates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of projected dimension:</head><p>We experiment with various values for the projected dimension k.</p><p>(We use the same k across all layers and heads of Linformer.) In the Figure <ref type="figure" target="#fig_2">3</ref>(a) and (b), we plot the validation perplexity curves for both the standard Transformer and the Linformer across different k, for maximum sequence lengths n = 512 and n = 1024. As expected, the Linformer performs better as projected dimension k increases. However, even at k = 128 for n = 512 and k = 256 for n = 1024, Linformer's performance is already nearly on par with the original Transformer. Effect of sharing projections: In Figure <ref type="figure" target="#fig_2">3</ref>(c), we plot the validation perplexity curves for the three parameter sharing strategies (headwise, key-value, and layerwise) with n = 512. Note that when we use just a single projection matrix (i.e. for layerwise sharing), the resulting Linformer model's validation perplexity almost matches that of the the non-shared model. This suggests that we can decrease the number of additional parameters in our model, and consequently, it's memory consumption, without much detriment to performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of longer sequences:</head><p>We evaluate the effect of sequence length during Linformer pretraining.</p><p>In the Figure <ref type="figure" target="#fig_2">3</ref>(d), we plot the validation perplexity for Linformer with n ? {512, 1024, 2048, 4096}, holding projected dimension k fixed at 256. Note that as sequence length increases, even though our projected dimension is fixed, the final perplexities after convergence remain about the same. This further empirically supports our assertion that the Linformer is linear-time. Table <ref type="table">2</ref>: Dev set results on benchmark natural language understanding tasks. The RoBERTa-base model here is pretrained with same corpus as BERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Downstream Results</head><p>Thus far, we have only examined the pretraining perplexities of our model. However, we wish to show that our conclusions hold after finetuning on downstream tasks. We finetune our Linformer on IMDB <ref type="bibr" target="#b16">(Maas et al., 2011)</ref> and SST-2 <ref type="bibr" target="#b27">(Socher et al., 2013)</ref> (sentiment classification), as well as QNLI (natural language inference) <ref type="bibr" target="#b25">(Rajpurkar et al., 2016)</ref>, and QQP (textual similarity) <ref type="bibr" target="#b4">(Chen et al., 2018)</ref> We do the same with RoBERTa, 12-layer BERT-base and 6-layer distilled BERT. All of our models, including the Transformer baselines, were pretrained with the same objective, pretraining corpus, and up to 250k updates (although our Linformer takes much less wall-clock time to get to 250k updates, and was consequently trained for less time). Results are listed in Table <ref type="table">2</ref>.</p><p>We observe that the Linformer model (n = 512, k = 128) has comparable downstream performance to the RoBERTa model, and in fact even slightly outperforms it at k = 256. Moreover, we note that although the Linformer's layerwise sharing strategy shares a single projection matrix across the entire model, it actually exhibits the best accuracy result of all three parameter sharing strategies. Furthermore, the Linformer pretrained with longer sequence length (n = 1024, k = 256) has similar results to the one pretrained with shorter length (n = 512, k = 256), this empirically supports the notion that the performance of Linformer model is mainly determined by the projected dimension k instead of the ratio n/k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Inference-time Efficiency Results</head><p>In Table <ref type="table" target="#tab_2">3</ref>, we report the inference efficiencies of Linformer (with layerwise sharing) against a standard Transformer. We benchmark both models' inference speed and memory on a 16GB Tesla V100 GPU card. We randomly generate data up to some sequence length n and perform a full forward pass on a multiple batches. We also choose batch size based on the maximum batch size that can fit in memory, and our memory savings are computed based on this number. From Table <ref type="table" target="#tab_2">3</ref>, we see that even with n = 512 and k = 128, Linformer has 1.5? faster inference time and allows for a 1.7? larger maximum batch size than the Transformer. As sequence length increases, the inference-time speed-up and memory savings are even more dramatic. We also plot inference times of both Linformer and Transformer on the 100 data samples in the top right of Figure <ref type="figure" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Transformer models are notoriously slow to train and deploy in practice since their self-attention operations have O(n 2 ) time and space complexity with respect to sequence length n. In this paper, we demonstrate, both theoretically and empirically, that the stochastic matrix formed by self-attention mechanism is low-rank. We further leverage this observation to propose a new, highly efficient selfattention mechanism. Through a combination of theoretical and empirical analysis, we demonstrate that our proposed approach is O(n) with respect to sequence length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>Our work focuses on making Transformers more efficient by introducing a mechanism that reduces self-attention to linear-time complexity. Potential positive impacts of efficient transformers include increasing the accessibility of our models, both for deployment on devices, as well as during training for research purposes. It also has potential impact on training transformer on images since we can support very long sequences. Furthermore, there are positive environmental benefits associated with decreasing the power consumption of models. As such, we see no immediate negative ethical or societal impacts of our work beyond what applies to other core building blocks of deep learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proof of Theorem 1</head><p>Proof. The main proof idea is based on the distributional Johnson-Lindenstrauss lemma (Lindenstrauss, 1984) (JL, for short), the following version is from <ref type="bibr" target="#b0">(Arriaga &amp; Vempala, 2006)</ref>.</p><p>Lemma 1. Let R be an k ? n matrix, 1 ? k ? n, with i.i.d. entries from N (0, 1/k). For any x, y ? R n , we have</p><formula xml:id="formula_17">Pr ( Rx ? (1 + ) x ) &gt; 1 -e -( 2 -3 )k/4 ,<label>(11)</label></formula><formula xml:id="formula_18">Pr xR T Ry T -xy T ? xy &gt; 1 -2e -( 2 -3 )k/4 .<label>(12)</label></formula><p>For simplicity, we will omit the subscript i for matrix</p><formula xml:id="formula_19">W K i , W Q i , W V i , E i and F i . We will regard Q as QW Q , K as KW K and V as V W V . Define A = QW Q i (KW K i ) T ? d<label>(13)</label></formula><p>Based on the definition of contextual mapping matrix P , we have</p><formula xml:id="formula_20">P = softmax QW Q i (KW K i ) T ? d = exp (A) ? D -1 A ,<label>(14)</label></formula><p>where D A is an n ? n diagonal matrix such that</p><formula xml:id="formula_21">(D A ) ii = n j=1 exp (A ji )<label>(15)</label></formula><p>Here we provide a constructive proof. Given any approximation error &gt; 0, define the following matrix.</p><formula xml:id="formula_22">P = exp (A) ? D -1 A R T R,<label>(16)</label></formula><p>where R be an k ? n matrix, 1 ? k ? n, with i.i.d. entries from N (0, 1/k). Clearly the rank of matrix P satisifies rank</p><formula xml:id="formula_23">( P ) ? rank(R) = k.<label>(17)</label></formula><p>We further show that, when k = log(n), we have that, for any column vector w ? R n ,</p><formula xml:id="formula_24">Pr P h -P h ? P h &gt; 1 -o(1).<label>(18)</label></formula><p>This concludes the theorem. For any row vector u ? R n of matrix P and any column vector w ? R n of matrix V W V , applying the JL Lemma, we can obtain</p><formula xml:id="formula_25">Pr uR t Rw T -uw T ? uw T &gt; 1 -2e -( 2 -3 )k/4 .<label>(19)</label></formula><p>Therefore, we have</p><formula xml:id="formula_26">Pr P w T -P w T ? P w T = Pr P R T Rw T -P w T ? P w T (a) ? 1 - x?P Pr xR T Rw T -xw T &gt; xw T (b) &gt;1 -2ne -( 2 -3 )k/4 .<label>(20)</label></formula><p>The above, step (a) is based on the union bound. The step (b) is utilizing the result of JL Lemma. Let k = 5 log(n)/( 2 -3 ), then theorem follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proof of Theorem 2</head><p>Proof. Define E = ?R and F = e -? R, where R ? R n?k with i.i.d. entries from N (0, 1/k), ? is a constant with ? = 1/2 n . We will first prove that for any row vector x ? R n of matrix QK T and column vector y ? R n of matrix V , Applying the result in (21) to every row vector of matrix A and every column vector of matrix V , one can directly prove that, for any row vector A i of matrix A,</p><formula xml:id="formula_27">Pr exp(A i E T )F V -exp(A i )V ? exp(A i ) V &gt; 1 -o(1),<label>(24)</label></formula><p>by setting k = 5 log(nd)/( 2 -3 ). This result does not utilize the low rank property of matrix A (rank(A)=d) and the resultant k has a dependency on sequence length n. We will further prove the choice of k can be constant and independent of sequence length n.</p><p>Based on the fact that rank(A)=d, we can find a row submatrix A s ? R 2d?d of matrix exp(AE T )F H such that rank(A s )=d. Applying the result in (21) to every row vector of matrix A s and every column vector of matrix V , and k = 9 log(d)/( 2 -3 ), we can obtain that, for any row vector A s i of matrix A s , Pr exp(A s i E T )F V -exp(A s i )V ? exp(A s i ) V &gt; 1 -o(1),</p><p>Furthermore, define the matrix ? ? R n?2d as</p><formula xml:id="formula_29">? = exp(AE T )F V exp(A)V ? exp(A s E T )F V exp(A s )V -1<label>(26)</label></formula><p>We have that, for any row vector A i of matrix A, 1 ? i ? n.</p><formula xml:id="formula_30">exp(A i E T )F V -exp(A i )V = ? i exp(A s E T )F V -? i exp(A s )V (a) ? [exp(A s E T )F V -exp(A s )V ] T 2 ? i (b) ??(d) exp(A s E T )F V -exp(A s )V F =?(d) 2d i=1 exp(A s i E T )F V -exp(A s i )V (c) ? ?(d) 2d i=1 exp(A s i ) V ? ?(d) exp(A s ) V</formula><p>The above, step (a) utilizes the inequality Ax ? A 2 ? x , where A 2 = ? max (A T A) (? max (?) is the largest eigenvalue) is the spectrum norm of a matrix A. The step (b) is based on matrix norm inequality A 2 ? A F , where A F = ( 1?i,j?n A 2 ij ) 1/2 is the Frobenius norm of matrix A. The step (c) is based on the results of (24).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Left two figures are spectrum analysis of the self-attention matrix in pretrained transformer model (Liu et al., 2019) with n = 512. The Y-axis is the normalized cumulative singular value of context mapping matrix P , and the X-axis the index of largest eigenvalue. The results are based on both RoBERTa-base and large model in two public datasets: Wiki103 and IMDB. The right figure plots the heatmap of normalized cumulative eigenvalue at the 128-th largest eigenvalue across different layers and heads in Wiki103 data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Left and bottom-right show architecture and example of our proposed multihead linear self-attention. Top right shows inference time vs. sequence length for various Linformer models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Pretraining validation perplexity versus number of updates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Pr exp(xE T )F y T -exp(x)y T ? exp(x)y T &gt; 1 -2e -( 2 -3 )k/4 .(21) Based on the triangle inequality, we haveexp(xE T )F y exp(x)y T ? exp(xE T )F y -exp(x)R T Ry + exp(x)R T Ry -exp(x)y T (a) ? (1 + ) y exp(xE T ) -exp(x)R T + exp(x)R T Ry -exp(x)y T (b) ? exp(x)R T Ry -exp(x)y T + o( exp(x) y ) (c) ? exp(x) y + o( exp(x) y )(22) The above, step (a) is based on the Cauchy inequality and JL Lemma in (11). The step (b) utilizes the fact that exponential function is Lipchitz continuous in a compact region. Then we can choose a small enough ?, i.e., ? = ?(1/n) such that exp(?xR) -exp(?x)R = o( exp(x) ) (23) The step (c) is based on the JL Lemma defined in (12).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note><p>Per-layer time complexity and minimum number of sequential operations as a function of sequence length (n) for various architectures.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Inference-time efficiency improvements of the Linformer over the Transformer, across various projected dimensions k and sequence lengths n. Left table shows time saved. Right table shows memory saved.</figDesc><table><row><cell>length n</cell><cell cols="3">projected dimensions k 128 256 512 1024 2048</cell><cell>length n</cell><cell cols="2">projected dimensions k 128 256 512 1024 2048</cell></row><row><cell cols="2">512 1.5x 1.3x -</cell><cell>-</cell><cell>-</cell><cell cols="2">512 1.7x 1.5x -</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">1024 1.7x 1.6x 1.3x -</cell><cell>-</cell><cell cols="3">1024 3.0x 2.9x 1.8x -</cell><cell>-</cell></row><row><cell cols="4">2048 2.6x 2.4x 2.1x 1.3x -</cell><cell cols="3">2048 6.1x 5.6x 3.6x 2.0x -</cell></row><row><cell cols="4">4096 3.4x 3.2x 2.8x 2.2x 1.3x</cell><cell cols="3">4096 14x 13x 8.3x 4.3x 2.3x</cell></row><row><cell cols="4">8192 5.5x 5.0x 4.4x 3.5x 2.1x</cell><cell cols="3">8192 28x 26x 17x 8.5x 4.5x</cell></row><row><cell cols="4">16384 8.6x 7.8x 7.0x 5.6x 3.3x</cell><cell cols="3">16384 56x 48x 32x 16x</cell><cell>8x</cell></row><row><cell cols="4">32768 13x 12x 11x 8.8x 5.0x</cell><cell cols="3">32768 56x 48x 36x 18x</cell><cell>16x</cell></row><row><cell cols="3">65536 20x 18x 16x 14x</cell><cell>7.9x</cell><cell cols="3">65536 60x 52x 40x 20x</cell><cell>18x</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An algorithmic theory of learning: Robust concepts and random projection</title>
		<author>
			<persName><forename type="first">I</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santosh</forename><surname>Arriaga</surname></persName>
		</author>
		<author>
			<persName><surname>Vempala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="161" to="182" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><surname>Longformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<title level="m">The long-document transformer</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Training deep nets with sublinear memory cost</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.06174</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoji</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leqi</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quora question pairs</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The approximation of one matrix by another of lower rank</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Eckart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gale</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="218" />
			<date type="published" when="1936">1936</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Training with quantization noise for extreme fixed-point compression</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.07320</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gpipe: Efficient training of giant neural networks using pipeline parallelism</title>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youlong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="103" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Quantization and training of neural networks for efficient integer-arithmetic-only inference</title>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Skirmantas</forename><surname>Kligys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2704" to="2713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ves</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>ACL</publisher>
			<pubPlace>Bart</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Extensions of lipschitz maps into a hilbert space</title>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><surname>Lindenstrauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Contemp. Math</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="189" to="206" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Andrew L Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies</title>
		<meeting>the 49th annual meeting of the association for computational linguistics: Human language technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07843</idno>
		<title level="m">Pointer sentinel mixture models</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Paulius</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonah</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksii</forename><surname>Kuchaiev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.03740</idno>
		<title level="m">Ganesh Venkatesh, et al. Mixed precision training</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmytro</forename><surname>Okhonko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.11660</idno>
		<title level="m">Transformers with convolutional context for asr</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Scaling neural machine translation</title>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Research Papers</title>
		<meeting>the Third Conference on Machine Translation: Research Papers</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="48" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Blockwise self-attention for long document understanding</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02972</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01108</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 conference on empirical methods in natural language processing</title>
		<meeting>the 2013 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno>CoRR, abs/1804.07461</idno>
		<ptr target="http://arxiv.org/abs/1804.07461" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
