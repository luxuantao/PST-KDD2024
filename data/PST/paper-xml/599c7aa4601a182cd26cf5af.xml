<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Intelligent fault diagnosis of rolling bearing using hierarchical convolutional network based health state classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chen</forename><surname>Lu</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Zhenya</forename><surname>Wang</surname></persName>
							<email>wangzy@buaa.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Bo</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Reliability and Systems Engineering</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<addrLine>Xueyuan Road</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Haidian District</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country>China Science</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Technology on Reliability &amp; Environmental Engineering Laboratory</orgName>
								<orgName type="institution">Haidian District</orgName>
								<address>
									<addrLine>Xueyuan Road</addrLine>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">School of Reliability and Systems Engineering</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<addrLine>Xueyuan Road</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Haidian District</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Intelligent fault diagnosis of rolling bearing using hierarchical convolutional network based health state classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">01D4FB508D7395248D1E31C336E094D1</idno>
					<idno type="DOI">10.1016/j.aei.2017.02.005</idno>
					<note type="submission">Received 19 May 2016 Received in revised form 29 August 2016 Accepted 19 February 2017</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Fault diagnosis Convolutional neural network Rolling bearing</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Rolling bearing tips are often the most susceptible to electro-mechanical system failure due to highspeed and complex working conditions, and recent studies on diagnosing bearing health using vibration data have developed an assortment of feature extraction and fault classification methods. Due to the strong non-linear and non-stationary characteristics, an effective and reliable deep learning method based on a convolutional neural network (CNN) is investigated in this paper making use of cognitive computing theory, which introduces the advantages of image recognition and visual perception to bearing fault diagnosis by simulating the cognition process of the cerebral cortex. The novel feature representation method for bearing data is first discussed using supervised deep learning with the goal of identifying more robust and salient feature representations to reduce information loss. Next, the deep hierarchical structure is trained in a robust manner that is established using a transmitting rule of greedy training layer by layer. Convolution computation, rectified linear units, and sub-sampling are applied for weight replication and reducing the number of parameters that need to be learned to improve the general feedforward back propagation training. The CNN model could thus reduce learning computation requirements in the temporal dimension, and an invariance level of working condition fluctuation and ambient noise is provided by identifying the elementary features of bearings. A top classifier followed by a back propagation process is used for fault classification. Contrast experiments and analyses have been undertaken to delineate the effectiveness of the CNN model for fault classification of rolling bearings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Fault diagnosis has been of greater importance as a performance assessment of system degradation by accurately classifying the current health state from monitored signal, in which bearing diagnostics have been a topic of research interest for over 20 years to meet customer demands regarding up-time, health management and maintenance <ref type="bibr" target="#b0">[1]</ref>. Considering that rolling bearings are one of the key components for rotating machinery such as machine tools, gearboxes, and turbo-machinery, the research interest has been widely developed for bearing signal processing and for fault detection and classification with the increasing demand driven mostly by economic, environmental, reliability, and safety incentives <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. A traditional isolated rolling bearing fault diagnosis method typically employs the following stages: (1) preprocessing of the original vibration signals; <ref type="bibr" target="#b1">(2)</ref> fault detection and (3) fault isolation based on data and model driven methods. For instance, empirical mode decomposition (EMD) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, wavelet packet decomposition (WPD) <ref type="bibr" target="#b5">[6]</ref> and short time Fourier transform (STFT) et al. can provide good support for fault feature extraction, and with the development of machine learning the classifiers such as support vector machine (SVM) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, random forest (RF) <ref type="bibr" target="#b8">[9]</ref> and probabilistic neural network (PNN) <ref type="bibr" target="#b9">[10]</ref> have well development for accurate fault pattern identification. Jinde Zheng utilized the partially ensemble empirical mode decomposition and variable predictive model-based class discrimination to diagnose the rolling bearing fault <ref type="bibr" target="#b10">[11]</ref>; Lu Ou et al. extracted the GFT impulse component as the features of vibration signals to diagnose the fault of rolling bearings <ref type="bibr" target="#b11">[12]</ref>; Muhammet Unal proposed the rolling bearing diagnosis method which analyzes the vibration signals by Hilbert Transform and extracts the features by FFT, after optimizing the features of extracted and classify them through artificial neural network <ref type="bibr" target="#b12">[13]</ref>. Such fault diagnosis methods aim at revealing intrinsic information for bearing health state representations and further taking advantage of the feature information for fault detection and classification.</p><p>Previous studies have shown that feature extraction appears to be an important prerequisite to achieve the expected diagnostic accuracies, which are hitherto dependent on the time-frequency analysis to a large extent. <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>. The feature extraction methods based on time-frequency analysis allow for detecting the indicators due to different types of bearing damage; then, the visual information and calculated parameters that are different between the impacts could be indexes to judge whether the bearing is in a healthy condition <ref type="bibr" target="#b16">[17]</ref>. The development of signal analysis methods suited to extracting time-varying features from nonstationary signals has become important for machinery fault diagnosis <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18]</ref>. Jay Lee discussed the application of prognosis and health management for bearings <ref type="bibr" target="#b18">[19]</ref> in which typical methods for bearing feature extraction, fault diagnosis, performance assessment, and degradation prediction are discussed as well as tools for selecting the most appropriate algorithms for specific applications. However, it was noted that ambient interference, hardware jamming signals, and working condition fluctuation typically have an non-negligible influence on the performance of artificial feature extraction; thus, novel methods with respect to such problems have been considered <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> aimed at improving the robustness and environmental suitability of the extracted features. Whereas time-frequency feature extraction has made tremendous progress, it appears that the relative methods where features are selected manually may bring about information loss due to insufficient data utilization, which, to some degree, limits better robustness and accuracy. Therefore, deep learning methods characterized by a hierarchical structure and greedy learning begin to attract more attention in recent years because high-level representations of robust and essential features can be achieved, which results in better performance.</p><p>As a novel theory of feature learning, deep learning was first proposed by Geoffrey Hinton et al. as an effective way to imitate the human brain learning process and shows a great superiority in pattern recognition and image processing <ref type="bibr" target="#b21">[22]</ref>. A number of studies have been conducted based on the experience of scientific knowledge in the area of biology and cognitive science and have produced some effective methods, such as deep belief networks, the restricted Boltzmann machine, and the stacked autoencoder <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>, which are used to guide the greedy training of multiple layers via unsupervised learning. Modified methods have also been analyzed to yield a new point for complex distributions to achieve better generalization performance on challenging recognition tasks <ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref>. These methods shed light on a novel aspect of feature extraction where high-level feature representation and strong self-learning ability are achieved for the sake of more effective classification. Therefore, the information important for classification with respect to diagnosis issues are learnt automatically, which could reduce the human labor or prior knowledge used in traditional shallow learning methods. However, a problem arises in unsupervised learning in that the calculating quantities and the computational resources may be considerable with large training samples when considering the lack of indicated labels. In this regard, the convolutional neural network (CNN) was proposed as a supervised learning method <ref type="bibr" target="#b27">[28]</ref> and brings some illumination to pattern classification: achieving essential and robust feature information using a greedy learning process while reducing the training parameters using local field sensing, weight replication, and sub-sampling.</p><p>With the advantages stated above, it should be noted that deep learning has been rarely applied for fault diagnosis of rotating machines. Prasanna Tamilselvan et al. presented a novel multisensor health diagnosis method using a deep belief network <ref type="bibr" target="#b28">[29]</ref> demonstrating the feasibility and preponderance of deep learning in fault pattern recognition and diagnosis. In this particular study, considering the complicated operating conditions of bearings, it is not always possible to determine the most suitable type of features to extract, thus deep learning method that improves robustness and computation efficiency is exploited based on a hierarchical convolutional network (CNN) to mine discriminant feature representations by greedy self-learning throughout the propagation steps, which could not only discover the robust relations of input samples due to strong anti-noise ability via convolution computation but also had fewer training parameters due to weight replication and sub-sampling. Gradient-based back propagation is conducted after the forward learning process, which results in a global optimization of the model parameters. A classifier based on a CNN appears to be a good approach to directly classify the raw signal and to integrate the signal processing functions within the discriminant steps <ref type="bibr" target="#b29">[30]</ref>. Based on this study, the fault diagnosis accuracy of the bearing fault pattern with different denoising proportions was tested and could be used to form a knowledge base on whether the approach is applicable for detecting and classifying faults with unexpected influence. Therefore, one motivation on the employment of the CNN model is the possibility of mining highlevel knowledge that is directly related to the bearing diagnosis problem, and one other interest is to learn fault features through a general-purpose learning architecture instead of hand labor or prior knowledge, thereby achieving better applications on fault pattern identification regardless of working condition fluctuations and ambient noise. This work aims to provide an alternative endto-end solution to fault diagnosis without any dedicated feature extraction or pre-processing technique, enabling potentially high performance.</p><p>The paper is divided as follows: in Section 2, a description of the high-level feature representation based on the CNN model is presented where the input normalization, establishment of a deep hierarchical structure and key processes in the learning process, such as convolution computation, global optimization based on back propagation are discussed; Section 3 introduces the basic learning procedure with respect to bearing diagnosis problems. In Section 4, comparative experiments are conducted to validate the effectiveness of the proposed deep learning method, and the results from different applied methodologies are discussed numerically; and Section 5 presents the paper's conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">General description of CNNs</head><p>This section details the proposed CNN-based diagnosis approach. Section 2.1 discusses the pre-training process with respect to input normalization and data regularization. Section 2.2 presents the general architecture. Section 2.3 overviews the basic forward learning process of the CNN, where the involved methodologies in greedy forward learning, such as convolution computation, rectified linear units, and sub-sampling are discussed. Section 2.4 presents the overall optimization stepwise procedure for the back propagation based fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Input normalization</head><p>In this study, the inputs are the three axis accelerations from the original vibration signals. Considering the characteristic of the CNN model, an image transformation method is utilized to map the original monitoring information to a series of feature maps, aiming to better fit the diagnosis model requirements in terms of multiple-layer based greedy learning. The detailed implementation rationale to establish a feature map (e.g. size 20 Â 20) is illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. First, monitoring data with time scale and amplitude scale normalized are acquired, and the datasets for each fault pattern are divided into a series of time sub-series by means of continuous interleved sampling, in which time sub-series from the first subset to the twentieth one are arranged in order to generate the first data matrix. A feature map is then established through a matrix reconstruction method based on slipping insertion as displayed on the right part in Fig. <ref type="figure" target="#fig_0">1</ref>. This process is repeated for subsequent time sub-series to generate desirable feature map sets used for training.</p><p>In the deep learning process, a feature map represents a layer entity, which means that each map of the prior hidden layer is a channel combination, while the following hidden layer subsamples and transforms the signal in the time domain <ref type="bibr" target="#b29">[30]</ref>. Compared with other data pre-processing methods, the input data using the feature map based permutation can fully retain the original information of vibration signals through lossless image transformation, as well as improving the learning stably and efficiency in terms of the notable topology of the proposed CNN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">CNN topology</head><p>Although greedy training is considered to be a suitable method to identify essential factors in deep learning, a problem arises in that a considerable amount of computations are required for adequate learning, which brings about high hardware and computation requirements in specific implementations of fault classification <ref type="bibr" target="#b30">[31]</ref>. In addition, previous studies showed that a number of fully connected hidden layers are difficult to pre-train when initialized randomly due to the number of weights <ref type="bibr" target="#b31">[32]</ref>. The CNN model is thus proposed as a partially connected deep learning framework that is motivated by minimal data preprocessing requirements <ref type="bibr" target="#b32">[33]</ref>.</p><p>Using the deep learning method given by Hinton et al. <ref type="bibr" target="#b21">[22]</ref>, the hierarchical structure of the CNN model in this study is established using greedy training as the deep learning method. However, one notable exception is that the structure is an integrated multilayer perceptron with a special topology and each sub-perception typically contains two types of hidden layers: convolutional layers and sub-sampling layers, which result in a different forward propagation process based on local connections of neurons between conjoint layers, as shown in Fig. <ref type="figure">2</ref>.</p><p>In Fig. <ref type="figure">2</ref>, it is observed that convolutional and sub-sampling layers are arranged alternately where the number of feature maps are determined by the number of convolution kernels and are gradually increased. The size of feature maps would be smaller if there was a deeper learning degree due to the feature reduction and optimization via local field sensing, weight replication, and subsampling <ref type="bibr" target="#b33">[34]</ref>. The units in the output layer are set to be fully connected to a top classifier, such as softmax regression, for pattern identification. Such topology is also demonstrated by the possibility of easily interpreting the trained convolution kernel, i.e., the receptive fields. The kernels are regularized to matrixes for fault pattern recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Greedy forward learning</head><p>As described in Section 2.2, the basic structure of the proposed CNN consists of multiple perceptions and a fully connected pattern recognition layer, while each perception is comprised of a convolutional layer and a followed sub-sample layer. This model exploits the greedy forward learning to mine salient fault information efficiently and reduce computational complexity. The greedy layerwise training consists of first training the first convolutional layer using input data to achieve corresponding parameters x l ij ; then, the following sub-sampling layer is trained using the obtained activations instead of using the initial input data <ref type="bibr" target="#b34">[35]</ref>. This process is repeated for subsequent layers: the output of each layer is used as the input for the next layer, and parameters of other layers remain unchanged while one hidden layer is trained, i.e., each layer of the model is trained individually, and a gradient-based back propagation is applied after the forward learning process for global optimization.</p><p>Taking the greedy learning process in one perception as an example, there are generally four steps defined as L 1 , L 2 , L 3 , and L 4 as shown in Fig. <ref type="figure">3</ref>. The description of the greedy training process in the CNN model is below.</p><p>For L 1 : Convolutional Layer Convolutional filtering is applied to obtain the salient features of the input data while information propagates through the different layers. In the image recognition field, normally each convolution kernel denotes a type of feature, such as oriented edges or corners; thus, salient features in the whole image would be identified using convolution computation <ref type="bibr" target="#b35">[36]</ref>. In this paper, similar studies are also conducted for bearing fault characteristic mining. Ref. <ref type="bibr" target="#b36">[37]</ref> gives the basic description of the convolution process as below.</p><p>Considering the filter is a type of linear time-invariant system where x½k is not relevant to the independent variable n, the output y½n is defined as the following:</p><formula xml:id="formula_0">y½n ¼ X 1 k¼À1 x½k Á h½n À k ¼ x½n Ã h½n ð<label>1Þ</label></formula><p>where x½n and h½n denote the input vector and the impulse responses, respectively, and x½n Ã h½n refers to the convolution process, which is essentially a multiplication process of reverse displacement sequences. Despite the mathematical meaning, convolution is applied for pattern recognition as well on the basis of neural network theory and demonstrates great results in audio and image classification, e.g., the CNN model. Impulse responses in such cases are defined as convolution kernels:</p><formula xml:id="formula_1">h½i; j ¼ h½N À i þ 1; N À j þ 1 ð<label>2Þ</label></formula><p>where h½Á and h½Á refer to the convolution and output functions, respectively. Note that the convolution kernels are initialized randomly in terms of the sets of connection weights in the feature maps and are rotated 180 degree for the subsequent processing. Therefore, in the greedy learning process, assuming that a neuron in the model is nðl; m; jÞ, where l; m; j are the layer, the map, and its position in the map, respectively, the current value y l m ðjÞ in one feature map in the layer is:</p><formula xml:id="formula_2">y l m ðjÞ ¼ f ðx l m ðjÞÞ<label>ð3Þ</label></formula><p>where x l m ðjÞ represents the scalar product between a set of input neurons and the weight connections between these neurons and the neuron number j in the map m in the layer l. f ðÁÞ referrers to the mapping function of the convolution process.</p><p>It is noted that an integrated computing method must be considered when multiple input feature maps exist in the convolution computation. In this regard, considering the local fields in the same location of each input feature map, the activation is calculated as the following:</p><formula xml:id="formula_3">y lþ1 i ðjÞ ¼ s X m i¼1 f ðx l i ðjÞ þ b k Þ !<label>ð4Þ</label></formula><p>where sðÁÞ denotes the activation function, and x l i is the weight of each feature map.</p><p>For L 2 : Rectification Layer</p><p>Stepped sigmoid units are normally set to be the activation units of transfer functions in traditional CNNs; however, a problem arises in that such units may make learning less stable and more time consuming as well as the well-known vanishing gradient problem <ref type="bibr" target="#b37">[38]</ref>. In this paper, a modification of rectified linear units (ReLUs) is used to make the learning process far more interesting in terms of being more similar to a model of real neurons and to also make the process more useful for practical applications. Compared to the saturating nonlinearities such as sigmoid and tangent functions, the employed non-saturating ReLUs are capable to deal with the gradient diffusion problem and show better fitting ability with respect to large-scale training datasets <ref type="bibr" target="#b38">[39]</ref>. The activation function of the convolution process in the CNNs is calculated as the following:</p><formula xml:id="formula_4">SðxÞ ¼ logð1 þ expðxÞÞ x &gt; 0 0 x 6 0<label>ð5Þ</label></formula><p>where x ¼ x l m ðjÞ. Normally the activation function SðxÞ could simplify to maxð0; x þ Nð0; rðxÞÞÞ, where Nð0; VÞ is Gaussian noise with zero mean and variance V. Previous studies have explored various rectified nonlinearities in the context of CNNs and have been proven to be beneficial to improve the discriminative performance in the forward convolution process due to the reasonable sparsity representation <ref type="bibr" target="#b39">[40]</ref>.</p><p>In view of the non-saturating property of ReLUs, in this study the weights of CNN are initialized based on Gaussian distribution, where random values with zero mean and small standard deviation are generated initially, then the standard deviation is increased layer by layer to obtain an asymmetrical weight set in the greedy forward learning process. This revision aims to ensure the learning reliability in consideration of the inhomogeneity of ReLUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>For L 3 : Local Contrast Normalization Layer</head><p>In the CNN model, advanced local field sensing and weights replication are adaptive solutions to generate desirable feature maps. Local field sensing <ref type="bibr" target="#b40">[41]</ref> means that a hidden unit is suggested to be connected to only the input units corresponding to a certain time span of the bearing data in the CNN, while weight replication <ref type="bibr" target="#b31">[32]</ref> enables all the units in one feature map share the same set of weights and biases. The advantage is that general and inherent statistical characteristics could be extracted by means of such methods resulting in a high robustness for data fluctuation caused by ambient noise and working conditions, as well as reducing the computation resource problem. On this basis, local contrast normalization is employed in this study to further investigate the possibility of eliminating higher order statistical characteristics based on local subtraction and division as described below.</p><p>Assuming that a feature map of size n Â n, for a local field unit h m ðm 2 ð1; n Â nÞÞ of which corresponding neighbourhood is defined as a vector b ¼ ½b 1 ; Á Á Á ; b l , the local contrast normalization of h m could be calculated as:</p><formula xml:id="formula_5">Lcn m ¼ h m À 1 l P l i¼1 ðx i b i Þ kxbk 2<label>ð6Þ</label></formula><p>where kbk 2 indicates the Euclidean norm of vector b. In this study, we also introduce a Gaussian window function to determine the weight coefficients x i to take into account the effect of spatial distance. Let d im ði 2 ð1; lÞ; m 2 ð1; n Â nÞÞ be the distance between unit b i and h m , d im is calculated as:</p><formula xml:id="formula_6">d im ¼ kðjjb i À h m jjÞ ¼ exp Àjjb i À h m jj 2 4r 2 !<label>ð7Þ</label></formula><p>where r is the width parameter of the radial basis function. The distance weight x i of unit b i could be thus defined as:</p><formula xml:id="formula_7">x i ¼ d im P l j¼1 d jm<label>ð8Þ</label></formula><p>The normalization process is repeated for subsequent units in the feature map. This layer aims at eliminating meaningless correlation and finding the best representation for the greedy learning and classification.</p><p>For L 4 : Sub-sampling Layer Although the high-level feature representations obtained using the convolution process appear to be more applicable than the initial data with respect to pattern classification, it is still computationally challenging with superabundant dimensions for a top classifier, such as softmax regression, which is prone to over fitting <ref type="bibr" target="#b32">[33]</ref>. However, it is noted that the approximate feature position relative to other features becomes relevant once a convolution process is applied whereas the exact location is less important resulting in a level of invariance for spatial and time change. Therefore, a natural approach, which is referred to as sub-sampling, is proposed to aggregate statistics, such as the mean and max values of the convolved features at various locations, by performing a local averaging and subsampling to reduce the dimensions and sensitivity of the output to ambient influence <ref type="bibr" target="#b31">[32]</ref>. The sub-sampling process is shown in Fig. <ref type="figure">3</ref>.</p><p>In this study, assuming a sub-sampling region of size m Â m, the convolved features are divided into disjoint regions, thus the subsampling process is:</p><formula xml:id="formula_8">a l jÀs ¼ f ðx l j downðM lÀ1 i Þ þ b l j Þ ð<label>9Þ</label></formula><p>where down (Á) denotes the sub-sampling computation where the maximum feature activations over these regions are calculated to obtain the stationary convolved features of a high level of invariance for pattern classification. The. M j , x l j , and b l j are the jth feature map, the weight matrix, and the bias in the lth layer, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Back propagation based fine-tuning</head><p>The proposed CNN model is composed of multiple perceptions generally based on the convolutional layers, rectification layers, local contrast normalization layers, sub-sampling layers and a top classifier, and the input information transmitted from layer to layer is finally best represented using high-level feature representations, which are beneficial for subsequent pattern classification. Typically, each layer is optimized individually, which is generally adjusted to maximize the overall performance of the complete system; however, a problem arises when such a method is time consuming and suboptimal when multiple layers must be trained. Therefore, an alternative is to train the entire hierarchical structure to achieve a global minimization error, such as in conjugate back propagation <ref type="bibr" target="#b41">[42]</ref>.</p><p>The procedure of the back propagation process is listed in Table <ref type="table">1</ref>.</p><p>In Table <ref type="table">1</ref>, it should be noted that the back propagation processes in Step 3 and Step 4 are of various calculation forms in the CNNs <ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b44">[45]</ref>. In this paper, the partial derivatives are calculated as follows.</p><p>Step 3: Sub-sampling Layer to Convolutional Layer Assuming the error sensitivity values are d l for the convolutional layer and d lþ1 for the subsequent sub-sampling layer, the relationship expression is, thus, stated as the following:</p><formula xml:id="formula_9">d l j ¼ x lþ1 j ðf 0 ðx l j Þ upðd lþ1 j ÞÞ<label>ð10Þ</label></formula><p>where upðÁÞ is an up-sampling process in contrast to sub-sampling and is used to regulate the sizes of the feature maps in the subsampling layer to be the same as the feature maps in the convolutional layer, and is the internal product.</p><p>The partial derivatives are calculated as the following:</p><formula xml:id="formula_10">@l @x l ij ¼ X u;v ðd l j Þ uv ðp lÀ1 i Þ uv<label>ð11Þ</label></formula><formula xml:id="formula_11">@l @b j ¼ X u;v ðd l j Þ u;v<label>ð12Þ</label></formula><p>where p i is the value of internal product, and ðu; vÞ is the corresponding location of the hidden unit in the feature map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>Back propagation learning procedure for the SDA model.</p><p>Step 1 Calculate the outputs of the layers in the forward direction from the first hidden layer to the output layer Step 2</p><p>Calculate the partial derivative of the output layer of the corresponding cost function <ref type="bibr">Step 3</ref> Calculate the residual error of each unit from the sub-sampling layer to the convolutional layer <ref type="bibr">Step 4</ref> Calculate the residual error of each unit from the convolutional layer to the sub-sampling layer Step 5</p><p>Repeat step 3 and step 4 until reaching the first layer Step 6</p><p>Update the initial weights and bias using gradient descent <ref type="bibr">Step 7</ref> Tune based on the conjugate gradient approach</p><p>Step 4: Convolutional Layer to Sub-sampling Layer Assuming there are M feature maps in the sub-sampling layer, the error sensitivity value of the ith unit corresponding to the jth convolution kernel is the following:</p><formula xml:id="formula_12">d l i ¼ X M j¼1 d lþ1 j Ã x ij<label>ð13Þ</label></formula><p>where Ã is the discrete convolution. The convolution is defined mathematically, which is different from the convolution computation in the forward learning. Therefore, the partial derivatives are calculated as the following:</p><formula xml:id="formula_13">@l @x l ¼ M l i Ã d l j<label>ð14Þ</label></formula><formula xml:id="formula_14">@l @b j ¼ X u;v ðd l j Þ u;v<label>ð15Þ</label></formula><p>where M l i denotes the ith feature map in the sub-sampling layer. Note that the back propagation must be slightly modified with respect to the weight replication, and a solution to effective training is that the partial derivatives are calculated regardless of this condition at the initial time; then, the connections that share the same parameter are added to form the derivative.</p><formula xml:id="formula_15">@l @x k ¼ X ði;jÞ2V k @l @u i;j<label>ð16Þ</label></formula><p>where @l @x k indicates the sum of partial derivatives of the connections that share the same weight x k . u i;j is the connection weight between units i and j, and V k denotes the parameter set. Detailed back propagation methodology is discussed in a previous study <ref type="bibr" target="#b44">[45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CNN-Based intelligent fault diagnosis</head><p>Based on CNNs, the main contribution of this study is the proposal of an intelligent fault diagnosis method that mines the bearing data characteristics and identifies the fault patterns accurately and efficiently: (1) Compared to traditional feature extraction and classification algorithms, the proposed deep learning method is able to realize high-level feature representation automatically so that it is less dependent on hand-engineered prior knowledge about signal processing techniques and diagnostic expertise. (2) Several improvements have been conducted in terms of the deep learning architecture and greedy learning, such as ReLUs and Gaussian window function based local contrast normalization that directly affect the performance of the trained CNN, thereby achieving better efficiency and further avoiding the disturbance of ambient noise and working fluctuations.</p><p>The diagnosis procedure is specifically displayed in Fig. <ref type="figure">4</ref>. The diagnosis process generally consists of four steps. First, the time and frequency features are extracted from the training set and merged to a matrix that is regarded as the input of the CNN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The format of the matrix is fx</head><formula xml:id="formula_16">i ; t i g M i¼1</formula><p>, where x i is the i th feature map with health state label t i . The feature dimensionality determines the size of the convolutional kernel used. Second, the CNN deep architecture is established layer by layer, as described in Section 2, where the convolution computation, the rectification unit mapping function, the local contrast normalization, and the sub-sampling are conducted to achieve salient characteristics. In this part, assuming m convolution kernels of size m Â n, the number of total parameters to be trained in the convolutional layer could be reduced to Num ¼ m Â ðn Â nÞ=4 via a local field, weight replication and sub-sampling aiming to improve the calculation efficiency during supervised deep learning. Third, a conjugate gradientbased back propagation is applied to fine-tune the parameters by minimizing the residual errors between the expected outputs and real activations. The completion of training is achieved when the differences are optimum. Finally, the trained CNN is used to diagnose faults of the rolling bearings.</p><p>In the proposed method, the forward learning steps help CNNs capture the main variations of the input data with relatively fewer parameters calculated, whereas the fine-tuning process enables the trained deep learning models to discover the discriminative information. Therefore, the CNN model is capable of fault characteristics mining and the intelligent diagnosis of rolling bearings with ambient noise and working condition fluctuations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Numerical examples</head><p>This section is devoted to show, by numerical example, the reliability and efficiency of the CNN model for fault diagnosis of rolling bearings. Different working conditions and experimental environments are selected to validate the effectiveness of the proposed method. Rolling bearing data collected at the bearing data center of Case Western Reserve University are used for testing and verifying the proposed method experimentally. The bearing test rig contained a 2 horsepower (hp) motor used as the prime mover to drive a shaft coupled with a bearing housing. The test rig also included both drive end (DE) and fan end (FE) 6205-2RS JEM SKF and NTN equivalent bearings to which single point faults were injected using electro-discharge machining with fault diameters of 7 mils, 14 mils, 21 mils, 28 mils, and 40 mils with the motor loads varied at 1, 2, and 3 hp. To quantify the stationary effect of the outer raceway faults, experiments were conducted for both FE and DE bearings with outer raceway faults located at 3 o'clock, 6 o'clock and 12 o'clock <ref type="bibr" target="#b45">[46]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data description</head><p>For data acquisition, vibration data were collected using accelerometers attached to the housing with magnetic bases and placed at the 12 o'clock position of the motor housing for both the DE and FE bearings. Digital data were collected at 12,000 samples per second, and data were also collected at 48,000 samples per second for the drive end bearing faults.</p><p>In this study, the DE bearing data for the normal (N), inner race fault (F1), outer race fault (F2), and the rolling element fault (F3) conditions were acquired for fault pattern classification where the fault diameters were selected to be 7 mils, 14 mils, and 21 mils. Dataset that contains 10 bearing fault states under loads of 1-3 hp is also employed to investigate the diagnosis ability in terms of working condition obfuscation, where the same fault state under different loads is labeled as one class. Ten working conditions are conducted to verify the effectiveness of the proposed method in consideration of multiple and mixed fault patterns. The fault information with respect to the test bearings is listed in Table <ref type="table" target="#tab_0">2</ref>.</p><p>Case 2: Qian-Peng test rig rolling bearing data In addition, an experiment based on other bearing data collected from the Qian-Peng test rig (QPZZ-II) in the lab was also applied for verification, as shown in Fig. <ref type="figure">5</ref>.</p><p>The test rig contained a drive motor, which drives a shaft as the moving output. The inner-ring fault, outer-ring fault and roller element fault are introduced by wire-electrode cutting a crevice on the surface of inner ring, outer ring and one of the roller elements as marked in Fig. <ref type="figure" target="#fig_3">6</ref>. The control cabinet, which adjusts the drive section, could simulate the fault characteristics at different speeds. The variable speed range of the test rig was 75-1450 rpm whereas the drive power was 0.75 kW. The maximum torque was 5.0 N ⁄ m. For data acquisition, the vibration signals were collected using a three-axis vibration sensor that was used at a sampling frequency of 5120 Hz. Note that in this experiment a relatively small amount of training data was used.</p><p>The test bearings used are cylindrical roller bearing (N205EM HRB CHINA), the detail structure information of which is listed in Table <ref type="table" target="#tab_1">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">CNN model establishment</head><p>Supervised deep learning was used to address the data based on the established CNN model in this particular section aiming to obtain salient fault characteristics and better robustness. In view of the calculation amount and the reconstruction efficiency, the CNN model was set to consist of four hidden layers with multiple feature maps where the convolutional layers and sub-sampling layers were arranged alternating structure: ''C1-S1-C2-S2". There would be fewer hidden units in the feature maps when the layer was deeper resulting in a parameter reduction process, e.g., assuming an input feature map of size a Â a, a convolutional kernel of size b Â b, and a sub-sampling region of size s Â s. The sizes of feature maps in the convolutional layer and the sub-sampling layer could be calculated as the following: </p><formula xml:id="formula_17">C1 ¼ c Â c ¼ ða À b þ 1Þ Â ða À b þ 1Þ ð 17Þ</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S1 ¼ ðc=sÞ Â ðc=sÞ ð 18Þ</head><p>The main parameters of the CNN model are listed in Table <ref type="table" target="#tab_2">4</ref>.</p><p>In the pre-training process, it should be noted that the entire training set was regulated to a series of data sets of 400 <ref type="bibr">(20 ⁄ 20)</ref> based on the experience performing figure recognition whereas the data interval was 2, and every 100 blocks were regarded as a training sample. Typically, it is suggested that the size of the convolutional kernel is rather important to achieve high performance with respect to the receptive field and weight replication <ref type="bibr" target="#b40">[41]</ref>; thus, an experiment was conducted to observe the changing trend of the training classification error based on different sizes of the convolutional kernels, as shown in Fig. <ref type="figure" target="#fig_4">7</ref>.</p><p>In Fig. <ref type="figure" target="#fig_4">7</ref>, the classification error fades when a convolution kernel with a larger size from 2 ⁄ 2 to 5 ⁄ 5 are used in the experiment showing best performance at 0.23%. However, the subsequent values increase gradually; then, the convolution kernel continues to be larger, which means that the oversized convolutional kernel may bring about unnecessary negative effects in the training process and the fault classification results will then meet a limitation.</p><p>Note also that in the forward propagation, the feature maps in sub-sampling 1 (S1) and convolutional layer 2 (C2) were locally connected based on the local connection criterion to achieve better feature representations, which is motivated by the visual local cognition mechanism <ref type="bibr" target="#b46">[47]</ref>. The corresponding connection relationship is shown in Table <ref type="table" target="#tab_3">5</ref>.</p><p>In Table <ref type="table" target="#tab_3">5</ref>, each column indicates which feature map in S1 was combined by the units in a particular feature map of C2. The first nine feature maps in C2 were obtained by convolving a different number of consecutive feature maps in S1, and the subsequent two feature maps were obtained based on the feature maps in odd and even number locations, whereas the last feature map was fully connected. With this method, the invariant-to-location shift in the bearing feature information will be learned more sufficiently in the convolution process, and greater robustness for ambient noise and working conditions are expected to be achieved as well.</p><p>Ten times cross validation was used in the experiment, i.e., the original data were divided into ten groups of which nine groups were set as the training data, whereas the rest group was for testing. The average value of the ten tests was considered to be the final result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparative analysis of fault diagnosis results with multiple working conditions</head><p>Contrast experiments for both rolling bearing data were conducted to validate the effectiveness of the CNN model with existing diagnosis algorithms <ref type="bibr" target="#b47">[48]</ref><ref type="bibr" target="#b48">[49]</ref><ref type="bibr" target="#b49">[50]</ref>. Basic architectures of the comparative diagnosis algorithms were as follows. The stacked denoising autoencoder (SAE) was established with four layers of which neurons were 50, 50, 20, 4, respectively. Note that the SAE is also a  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Size of convolution kernel</head><p>Training error (%) deep learning method where unsupervised self-learning is conducted during forward learning. A Gaussian kernel function was applied for training a one against all support vector machine (SVM). A shallow softmax regression classifier (SR) that was identical to the top classifier of the CNN model was also used to verify the ability of the proposed method in adaptively mining fault characteristics. This paper compares the classification results with the actual data, i.e., assuming the number of correct and total classification results were m i and n i in the ith iteration, respectively, the accuracy of the conducted experiment was calculated as the following:</p><formula xml:id="formula_18">Acc ¼ X N i¼1 m i n i , N<label>ð19Þ</label></formula><p>where N denotes the cross validation times, which is ten in this experiment.</p><p>The diagnosis results for each cross validation trial of the two experiments are illustrated in Figs. <ref type="figure" target="#fig_5">8</ref> and<ref type="figure" target="#fig_6">9</ref>, respectively.</p><p>As shown in Fig. <ref type="figure" target="#fig_5">8</ref>, the results of SVM and SR were not satisfied through comparison with the deep learning methods in Case 1 demonstrating the effectiveness of capturing nonlinear fault characteristics with deep architectures to facilitate the diagnosis. The SR model that was identical to the top classifier but not pretrained by greedy forward learning varied greatly. Taking the dataset in motor load 1-3 hp as an example, the proposed CNN model outperformed other methods, of which diagnosis accuracies ranged from 91.88% to 93.79% and the standard deviations below 1.17%, while the best results of the SAE, SVM, and SR were 87.94%, 73.63%, and 71.94%, respectively. It presents that the proposed method could effectively and stably identify not only bearing fault categories but also fault severer degrees. It is also worth mentioning that the employed dataset included massive samples for ten bearing health states under different loads, and its diagnosis accuracies demonstrated that the proposed method was capable to diagnose the bearing faults regardless of the working condition fluctuations.</p><p>In addition, although the CNN and SDA methods can both achieve an average accuracy of greater than 85 percent, the CNN model had less fluctuation, which means that the convolution computation based supervised deep learning may achieve better data steadiness with sufficient training samples and labels. Besides, compared with the SAE model, less computational time was consumed in the CNN model with respect to parameter reduction during the training process (in i5 CPU, MATLAB 2014b) demonstrating the effectiveness of the convolution computation, rectified linear units and sub-sampling, as described in Section 2.</p><p>An interesting phenomenon was also observed in the deep learning methods that unsupervised learning based model, e.g. SAE, achieved results comparable to the CNN in some trials. Taking the sixth cross validation in motor load 3 hp as an example, the diagnosis accuracies of the CNN and SAE were 97.91% and  From the diagnosis results, the classification rates for both algorithms clearly decrease with a reduction in the number of training samples. The proposed CNN method still produced greater diagnosis results in most of the trials mainly due to its capability of learning high complexity relationships between the fault characteristics and the health states through supervised deep learning. As a typical algorithm for unsupervised deep learning, the SAE performed well when the training samples were massive, e.g., the best result was nearly 100 percent when the training proportion was 70%; however, an obvious decline appeared in the subsequent experiments with fewer training data. This is mainly because in unsupervised deep learning it is difficult to learn sufficient regularity information and incidence relationships with limited input data.</p><formula xml:id="formula_19">Y Y Y Y Y 2 Y Y Y Y Y Y Y Y 3 Y Y Y Y Y Y Y Y Y Y 4 Y Y Y Y Y Y Y Y Y Y 5 Y Y Y Y Y Y Y Y 6 Y Y Y Y Y<label>2</label></formula><p>Case 1 demonstrates the effectiveness of the proposed method in massive data processing; however, in regard to Case 2 where the bearing data are one third of the data in Case 1, the classification accuracies of the deep learning methods are of little difference with other algorithms, as shown in Fig. <ref type="figure" target="#fig_7">10</ref>. Taking the average results as examples, the diagnostic accuracies of CNN, SAE, SVM, and SR were 95.78%, 92.85%, 92.91%, and 90.47%, respectively. That is to say, such deep learning methods are more suitable for dealing with a large amount of data when identifying essential feature representations. However, the CNN model is still of better robustness with respect to diagnosis stability obtaining a maximum fluctuation of 0.66%. The detailed diagnosis results including average accuracies and standard deviations of the multiple working conditions in the cross validation process are summarized in Table <ref type="table" target="#tab_4">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparative experiments with different degrees of ambient noise</head><p>Rolling bearings typically work in environments with unknown factors, such as working condition fluctuations and ambient noise; thus, the diagnosis methods used are typically requested to be effective when and have significant anti-noise properties to function in complex working conditions. As stated above, it appears that the CNN model is of high robustness and adaptability due to the convolution computation processes, such as local field sensing and weight replication. An application analysis of the CNN model with respect to different degrees of ambient noise was conducted to investigate the applicability of the proposed model in such situations.</p><p>In this particular experiment, the data form a DE bearing operating in 2 hp were used for model training where random noise was added in each feature map to obtain different signal-to-noise ratios (SNR) from 10 dB to 50 dB, as shown in Fig. <ref type="figure" target="#fig_8">11</ref>. Normally there would be less signal distortion with a greater SNR.</p><p>Similar architectures of existing diagnosis algorithms used for comparison are established, as described in Section 4.3. Note that a stacked denoising autoencoder (SDA) was used instead of the SAE where a denoising process is used to enhance the anti-noise ability in forward unsupervised learning <ref type="bibr" target="#b24">[25]</ref>. The basic objective of the experiments was to verify the diagnosis ability with strong ambient noise, i.e., if an algorithm has better denoising performance, the diagnosis results should be more stable and accurate  As shown in Fig. <ref type="figure" target="#fig_9">12</ref>, the shallow architecture based methods showed more obvious declines with a lower SNR obtaining a maximum difference of 14.34%, whereas the CNN and SDA models were rather insensitive to the ambient noise due to the strong anti-noise ability. For instance, relatively high classification results of 90.87% and 91.42% could be still achieved using the CNN and SDA models at a low SNR of 10 dB, respectively. However, it should be noted that, although the two deep learning methods are both of high robustness in fault diagnosis (the SDA model is slightly better with respect to stability), the CNN model has a quicker calculation speed and requires lower computational resources in terms of fewer training parameters, and this advantage would be more evident with a deeper hierarchical structure showing a better application when considering the relevant classification results. This demon-   strates that the convolution computation and the sub-sampling processes indeed form a shared and robust representation that has some invariance to the ambient noise. Detailed diagnosis results with different degrees of ambient noise are listed in Table <ref type="table" target="#tab_5">7</ref>.</p><p>By analyzing the number of classification errors of each fault mode in the diagnosis models, the classification errors were found to mainly appear in the normal data and were likely to be classified to the fault modes when a strong random noise existed. An explanation is that the strong noise may result in a data obfuscation that has a negative influence on the fluctuation trend and the data locations in the feature maps, as shown in Fig. <ref type="figure" target="#fig_8">11</ref>, which enabled the normal data sets to be relatively similar to the fault data. Despite the negative influence, the CNN model indeed had the advantage of capturing salient fault characteristics in view of external disturbances resulting in fewer fluctuations in the diagnostic accuracies, as listed in Table <ref type="table" target="#tab_5">7</ref>.</p><p>It should be noted that in this paper the architecture selection of the CNN is not discussed detailedly. When determining the parameters, the feature size of the next layer is smaller than that of the previous layer so that the mapping process of can be viewed as a data compression or feature extraction. However, a deeper network could be tried for more applications although the fivelayer CNN performs well for diagnosis of rolling bearings in this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and future work</head><p>A new version of the CNN-based deep learning method, which was originally introduced for image recognition, is presented in this paper to provide valuable insight for bearing fault diagnosis. As mentioned above, the feature extraction used by traditional shallow learning approaches has shown an unavoidable limitation in terms of greater accuracy and robustness. Therefore, in the CNN model, salient high-level feature representations are learned from the input samples directly using supervised deep learning where the convolutional and sub-sampling layers are arranged alternately for greedy learning. Rectified linear units, local contrast normalization and weight replication are used to represent elementary features during the convolution computation aiming to provide a level of invariance to environment interference and to reduce the computational requirements. Followed by a top classifier and global optimization via back propagation, the use of the CNN model demonstrated on a method to improve the fault pattern classification accuracy of rolling bearings with respect to ambient noise and working condition fluctuations. Comparison experiments were conducted to test the efficiency of the proposed method, and an accuracy of greater than 90% was achieved with fewer computational resource.</p><p>Future plans include conducting more experimental tests to further understand the limitations of the deep learning method with regard to a larger range of speed fluctuations and periodic variability of the bearing. In addition, as a supervised learning method, a mass of training samples and labels are required for sufficient learning, i.e., the classification accuracy is relatively more depen-dent on the data quality compared with other unsupervised deep learning methods, as shown in the experiment above, which may be a limitation in terms of the generalization ability. This aspect will be subject of future research. The architecture selection is still an open problem for neural networks and we will focus on this problem in future work </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Schematic diagram of time series permutation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 . 4 LFig. 3 .</head><label>243</label><figDesc>Fig. 2. Deep hierarchical structure of the CNN model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Case 1 :FineFig. 4 .</head><label>14</label><figDesc>Fig. 4. Flowchart of the proposed method.</figDesc><graphic coords="6,333.32,67.91,168.56,384.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Faults of inner ring, outer ring and roller element.</figDesc><graphic coords="8,92.24,67.92,397.59,115.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Changing trend of different convolution kernels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Diagnosis results of 10 cross validations of Case 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Diagnosis results of 10 cross validations of Case 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Classification results with different training samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Fluctuation of the bearing data at SNR of 10 dB and 50 dB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Contrast classification results with different noise levels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="7,153.07,515.34,297.69,223.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2</head><label>2</label><figDesc>Data declaration.</figDesc><table><row><cell>Motor load (hp)</cell><cell>Fault diameter (mils)</cell><cell>Fault type</cell><cell>Classification label</cell></row><row><cell>1/2/3/1-3</cell><cell>0</cell><cell>N</cell><cell>1</cell></row><row><cell>1/2/3/1-3</cell><cell>7</cell><cell>F1</cell><cell>2</cell></row><row><cell>1/2/3/1-3</cell><cell>14</cell><cell>F1</cell><cell>3</cell></row><row><cell>1/2/3/1-3</cell><cell>21</cell><cell>F1</cell><cell>4</cell></row><row><cell>1/2/3/1-3</cell><cell>7</cell><cell>F2</cell><cell>5</cell></row><row><cell>1/2/3/1-3</cell><cell>14</cell><cell>F2</cell><cell>6</cell></row><row><cell>1/2/3/1-3</cell><cell>21</cell><cell>F2</cell><cell>7</cell></row><row><cell>1/2/3/1-3</cell><cell>7</cell><cell>F3</cell><cell>8</cell></row><row><cell>1/2/3/1-3</cell><cell>14</cell><cell>F3</cell><cell>9</cell></row><row><cell>1/2/3/1-3</cell><cell>21</cell><cell>F3</cell><cell>10</cell></row></table><note><p>Fig. 5. Qian-Peng test rig.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3</head><label>3</label><figDesc>Bearing structure information of QPZZ-II.</figDesc><table><row><cell>Designation</cell><cell>Inside diameter</cell><cell>Outside diameter</cell><cell>Thickness</cell><cell>Element type</cell></row><row><cell>N205EM</cell><cell>25 mm</cell><cell>52 mm</cell><cell>15 mm</cell><cell>Cylindrical roller</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4</head><label>4</label><figDesc>Parameters of the CNN model.</figDesc><table><row><cell>Model parameters</cell><cell>Input layer</cell><cell>Convolutional layer 1</cell><cell>Sub-sampling layer 1</cell><cell>Convolutional layer 2</cell><cell>Sub-sampling layer 2</cell><cell>Output layer</cell></row><row><cell>Number of feature maps</cell><cell>1</cell><cell>6</cell><cell>6</cell><cell>12</cell><cell>12</cell><cell>4</cell></row><row><cell>Size of feature maps</cell><cell>20 * 20</cell><cell>16 * 16</cell><cell>8 * 8</cell><cell>4 * 4</cell><cell>2 * 2</cell><cell>1 * 1</cell></row><row><cell>Learning parameters</cell><cell>Convolutional Kernel</cell><cell>Data Interval</cell><cell>Sub-sampling Region</cell><cell>Learning Rate</cell><cell>Training Epochs</cell><cell>Iterations</cell></row><row><cell></cell><cell>5 * 5</cell><cell>2</cell><cell>2 * 2</cell><cell>1</cell><cell>100</cell><cell>30</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5</head><label>5</label><figDesc>Local connection relationship between S1 and C2.</figDesc><table><row><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>1 0</cell><cell>1 1</cell><cell>1 2</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6</head><label>6</label><figDesc>Fault classification accuracy.</figDesc><table><row><cell>Working condition</cell><cell>Classification accuracy</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>CNN</cell><cell>SAE</cell><cell>SVM</cell><cell>SR</cell></row><row><cell>Load/hp</cell><cell cols="2">Case 1: Data from Case Western Reserve University</cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>96.48% ± 0.76</cell><cell>93.12% ± 1.29</cell><cell>81.41% ± 2.30</cell><cell>81.96% ± 2.52</cell></row><row><cell>2</cell><cell>97.78% ± 0.49</cell><cell>95.24% ± 1.84</cell><cell>82.47% ± 2.46</cell><cell>78.63% ± 3.68</cell></row><row><cell>3</cell><cell>96.97% ± 0.88</cell><cell>93.16% ± 1.82</cell><cell>81.11% ± 2.26</cell><cell>77.34% ± 2.58</cell></row><row><cell>1-3</cell><cell>92.60% ± 1.17</cell><cell>81.57% ± 2.75</cell><cell>70.37% ± 3.26</cell><cell>69.92% ± 4.02</cell></row><row><cell>Average execution time (s)</cell><cell>217 ± 14</cell><cell>468 ± 11</cell><cell>56 ± 3</cell><cell>41 ± 3</cell></row><row><cell>Speed (rpm)</cell><cell cols="2">Case 2: Data from Qian-Peng test rig</cell><cell></cell><cell></cell></row><row><cell>750</cell><cell>95.78% ± 0.66</cell><cell>93.85% ± 0.79</cell><cell>92.91% ± 1.93</cell><cell>90.47% ± 2.73</cell></row><row><cell>Average execution time (s)</cell><cell>40 ± 5</cell><cell>98 ± 10</cell><cell>15 ± 2</cell><cell>19 ± 3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7</head><label>7</label><figDesc>Diagnosis results with different degrees of ambient noise.</figDesc><table><row><cell>SNR</cell><cell>Classification results</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>CNN</cell><cell>SDA</cell><cell>SVM</cell><cell>SR</cell></row><row><cell>10</cell><cell>90.87% ± 1.87</cell><cell>91.42% ± 2.01</cell><cell>64.71% ± 4.01</cell><cell>65.91% ± 5.34</cell></row><row><cell>20</cell><cell>96.69% ± 1.99</cell><cell>89.43% ± 1.94</cell><cell>73.57% ± 3.16</cell><cell>71.28% ± 3.94</cell></row><row><cell>30</cell><cell>93.47% ± 1.19</cell><cell>94.55% ± 0.96</cell><cell>76.63% ± 2.33</cell><cell>76.54% ± 1.82</cell></row><row><cell>40</cell><cell>96.16% ± 0.82</cell><cell>97.58% ± 0.61</cell><cell>79.29% ± 1.62</cell><cell>77.75% ± 1.59</cell></row><row><cell>50</cell><cell>97.71% ± 0.62</cell><cell>98.13% ± 0.59</cell><cell>82.19% ± 1.61</cell><cell>80.25% ± 1.32</cell></row><row><cell>CPU time (s)</cell><cell>126 ± 2</cell><cell>225 ± 3</cell><cell>20 ± 2</cell><cell>17 ± 2</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>C. Lu et al. / Advanced Engineering Informatics 32 (2017) 139-151</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by the National Natural Science Foundation of China (Grant Nos. 51605014, 51575021 and 51105019), the Technology Foundation Program of National Defense (Grant No. Z132013B002), as well as the Innovation Foundation of BUAA for PhD Graduates.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust performance degradation assessment methods for enhanced rolling element bearing prognostics</title>
		<author>
			<persName><forename type="first">H</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Eng. Inform</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="127" to="140" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Rotating machinery prognostics: state of the art, challenges and opportunities</title>
		<author>
			<persName><forename type="first">A</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mathew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mech. Syst. Signal Pr</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="724" to="739" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A research on intelligent fault diagnosis of wind turbines based on ontology and FMECA</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Eng. Inform</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="115" to="125" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Diagnosis of subharmonic faults of large rotating machinery based on EMD</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mech. Syst. Signal. Pr</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="467" to="475" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The fault feature extraction of rolling bearing based on EMD and difference spectrum of singular value</title>
		<author>
			<persName><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Shock. Vib</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fault diagnosis and prognosis using wavelet packet decomposition. Fourier transform and artificial neural network</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Intell. Manuf</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1213" to="1227" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fault diagnosis of monoblock centrifugal pump using SVM</title>
		<author>
			<persName><forename type="first">V</forename><surname>Muralidharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sugumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Indira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eng. Sci. Technol., Int. J</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="152" to="157" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic bearing fault diagnosis based on one-class m-SVM</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fernández-Francos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Martínez-Rego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Fontenla-Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alonso-Betanzos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Ind. Eng</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="357" to="365" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fault diagnosis in spur gears based on genetic algorithm and random forest</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cerrada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zurita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cabrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Artes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mech. Syst. Signal Pr</title>
		<imprint>
			<biblScope unit="page" from="87" to="103" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fault diagnosis based on dependent feature vector and probability neural network for rolling element bearings</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Math. Comput</title>
		<imprint>
			<biblScope unit="volume">247</biblScope>
			<biblScope unit="page" from="835" to="847" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rolling bearing fault diagnosis based on partially ensemble empirical mode decomposition and variable predictive model-based class discrimination</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arch. Civ. Mech. Eng</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="784" to="794" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A new rolling bearing fault diagnosis method based on GFT impulse component extraction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mech. Syst. Signal Pr</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="162" to="182" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fault diagnosis of rolling bearings using a genetic algorithm optimized neural network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Unal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Onat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Demetgul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kucuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Measurement</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="187" to="196" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multiwavelet transform and its applications in mechanical fault diagnosis -a review</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mech. Syst. Signal Pr</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recent advances in time-frequency analysis methods for machinery fault diagnosis: a review with application examples</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mech. Syst. Signal Pr</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="165" to="205" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Enhanced diagnostic certainty using information entropy theory</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="141" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Empirical mode decomposition, an adaptive approach for interpreting shaft vibratory signals of large rotating machinery</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Tavner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Sound. Vib</title>
		<imprint>
			<biblScope unit="volume">321</biblScope>
			<biblScope unit="page" from="1144" to="1170" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fault diagnosis of main engine journal bearing based on vibration analysis using Fisher linear discriminant. Knearest neighbor and support vector machine</title>
		<author>
			<persName><forename type="first">A</forename><surname>Moosavian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ahmadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tabatabaeefar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vibroeng</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="894" to="906" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Prognostics and health management design for rotary machinery systems-reviews, methodology and applications</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghaffari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Siegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mech. Syst. Signal Pr</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="314" to="334" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Novel method for rolling element bearing health assessment-A tachometer-less synchronously averaged envelope feature extraction technique</title>
		<author>
			<persName><forename type="first">D</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Al-Atat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shauche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mech. Syst. Signal Pr</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="362" to="376" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Using continuous wavelet transform of generalized flexibility matrix in damage identification</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Ashory</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Masoumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jamshidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Khalili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vibroeng</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="512" to="519" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural. Comput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L D P Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 19 (NIPS&apos;06</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L Y B P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-fifth International Conference on Machine Learning</title>
		<meeting>the Twenty-fifth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>O'neil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Recurrent Neural Networks for Noise Reduction in Robust ASR</title>
		<imprint>
			<biblScope unit="page" from="22" to="25" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F F G A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Face recognition: a convolutional neural-network approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Back</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="98" to="113" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Failure diagnosis using deep belief learning based health state classification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tamilselvan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reliab. Eng. Syst. Safe</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="124" to="135" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for P300 detection with application to brain-computer interfaces</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cecotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graeser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern. Anal</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="433" to="445" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yoshua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pascal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hugo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 19 (NIPS&apos;06)</title>
		<imprint>
			<publisher>Pub Place</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adaptive convolutional neural network and its application in face recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural. Process. Lett</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="389" to="399" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Understanding deep convolutional networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philos. Trans. R. Soc. A</title>
		<imprint>
			<biblScope unit="volume">374</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Efficient learning of sparse representations with an energybased model</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P S C M</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">06</biblScope>
			<biblScope unit="page" from="1137" to="1144" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">String representations and distances in deep Convolutional Neural Networks for image classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Barat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ducottet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern. Recogn</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="104" to="115" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Damelin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Miller</surname></persName>
		</author>
		<title level="m">The Mathematics of Signal Processing</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Improved automatic speech recognition system using sparse decomposition by basis pursuit with deep rectifier neural networks and compressed sensing recomposition of speech signals</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gavrilescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th International Conference on Communications (COMM)</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">DropSample: a new training method to enhance deep convolutional neural networks for large-scale unconstrained handwritten Chinese character recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern. Recogn</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="190" to="203" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">What is the best multi-stage architecture for object recognition?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jarrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int Conf. Comput. Vision</title>
		<imprint>
			<biblScope unit="page" from="2146" to="2153" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Contour and Grouping in Computer Vision</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Mundy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Di Gesú</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shape</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">An adaptive conjugate gradient algorithm for large-scale unconstrained optimization</title>
		<author>
			<persName><forename type="first">N</forename><surname>Andrei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Appl. Math</title>
		<imprint>
			<biblScope unit="volume">292</biblScope>
			<biblScope unit="page" from="83" to="91" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Scene classification via a gradient boosting random convolutional network framework</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE. Trans. Geosci. Remote</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="1793" to="1802" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Noise-enhanced convolutional neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Osoba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kosko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">B Y B</forename><surname>Yann Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Rolling element bearing diagnostics using the Case Western Reserve University data: a benchmark study</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Randall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mech. Syst. Signal Pr</title>
		<imprint>
			<biblScope unit="page" from="100" to="131" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A convolutional neural network approach for objective video quality assessment</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Le</forename><surname>Callet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Viard-Gaudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1316" to="1327" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Health assessment and fault diagnosis for centrifugal pumps using Softmax regression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vibroeng</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1464" to="1474" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A fault diagnosis approach for roller bearing based on IMF envelope spectrum and SVM</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Measurement</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="943" to="950" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Novel solutions for an old disease: diagnosis of acute appendicitis with random forest, support vector machines, and artificial neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Surgery</title>
		<imprint>
			<biblScope unit="volume">149</biblScope>
			<biblScope unit="page" from="87" to="93" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
