<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Predictive data mining in clinical medicine: Current issues and guidelines</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Riccardo</forename><surname>Bellazzi</surname></persName>
							<email>riccardo.bellazzi@unipv.it</email>
							<affiliation key="aff0">
								<orgName type="department">Dipartimento di Informatica e Sistemistica</orgName>
								<orgName type="institution">Universit Ã  di Pavia</orgName>
								<address>
									<addrLine>via Ferrata 1</addrLine>
									<postCode>27100</postCode>
									<settlement>Pavia</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Blaz</forename><surname>Zupan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Computer Science</orgName>
								<orgName type="institution">University of Ljubljana</orgName>
								<address>
									<country key="SI">Slovenia</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Human and Molecular Genetics</orgName>
								<orgName type="institution">Baylor College of Medicine</orgName>
								<address>
									<settlement>Houston</settlement>
									<region>TX</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Predictive data mining in clinical medicine: Current issues and guidelines</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AF5B0993FC40E4CDFF6282E2A381B67F</idno>
					<idno type="DOI">10.1016/j.ijmedinf.2006.11.006</idno>
					<note type="submission">Received 27 October 2006 Accepted 17 November 2006</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Data mining Predictive models Clinical medicine Data mining process Data analysis</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Background: The widespread availability of new computational methods and tools for data analysis and predictive modeling requires medical informatics researchers and practitioners to systematically select the most appropriate strategy to cope with clinical prediction problems. In particular, the collection of methods known as 'data mining' offers methodological and technical solutions to deal with the analysis of medical data and construction of prediction models. A large variety of these methods requires general and simple guidelines that may help practitioners in the appropriate selection of data mining tools, construction and validation of predictive models, along with the dissemination of predictive models within clinical environments.</p><p>Purpose: The goal of this review is to discuss the extent and role of the research area of predictive data mining and to propose a framework to cope with the problems of constructing, assessing and exploiting data mining models in clinical medicine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods:</head><p>We review the recent relevant work published in the area of predictive data mining in clinical medicine, highlighting critical issues and summarizing the approaches in a set of learned lessons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results:</head><p>The paper provides a comprehensive review of the state of the art of predictive data mining in clinical medicine and gives guidelines to carry out data mining studies in this field.</p><p>Conclusions: Predictive data mining is becoming an essential instrument for researchers and clinical practitioners in medicine. Understanding the main issues underlying these methods and the application of agreed and standardized procedures is mandatory for their deployment and the dissemination of results. Thanks to the integration of molecular and clinical data taking place within genomic medicine, the area has recently not only gained a fresh impulse but also a new set of complex problems it needs to address.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Over the last few years, the term 'data mining' has been increasingly used in the medical literature. In general, the term has not been anchored to any precise definition but to some sort of common understanding of its meaning: the use of (novel) methods and tools to analyze large amounts of data. Data mining has been applied with success to different fields of human endeavor, including marketing, banking, customer relationship management, engineering and various areas of science. However, its application to the analysis of medical data -despite high hopes -has until recently been relatively limited. This is particularly true of practical applications in clinical medicine which may benefit from specific data mining approaches that are able to perform predictive modeling, exploit the knowledge available in the clinical domain and explain proposed decisions once the models are used to support clinical decisions. The goal of predictive data mining in clinical medicine is to derive models that can use patientspecific information to predict the outcome of interest and to thereby support clinical decision-making. Predictive data mining methods may be applied to the construction of decision models for procedures such as prognosis, diagnosis and treatment planning, which -once evaluated and verified -may be embedded within clinical information systems.</p><p>In this paper, we give a methodological review of data mining, focusing on its data analysis process and highlighting some of the most relevant issues related to its application in clinical medicine. We limit the paper's scope to predictive data mining whose methods are methodologically ripe and often easily available and may be particularly suitable for the class of problems arising from clinical data analysis and decision support.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>Data mining is the process of selecting, exploring and modeling large amounts of data in order to discover unknown patterns or relationships which provide a clear and useful result to the data analyst <ref type="bibr" target="#b0">[1]</ref>. Coined in the mid-1990s, the term data mining has today become a synonym for 'Knowledge Discovery in Databases' which, as proposed by Fayyad et al. <ref type="bibr" target="#b1">[2]</ref>, emphasized the data analysis process rather than the use of specific analysis methods. Data mining problems are often solved by using a mosaic of different approaches drawn from computer science, including multi-dimensional databases, machine learning, soft computing and data visualization, and from statistics, including hypothesis testing, clustering, classification and regression techniques. The craft of data mining lies in the appropriate choice and combination of these techniques to efficiently and reliably solve a given problem. Data mining tasks can, in general, be classified to tasks of description and prediction. While description aims at finding human-interpretable patterns and associations, after considering the data as a whole and constructing a model prediction seeks to foretell some response of interest. Although the goals of description and prediction may overlap (the models generated by some prediction methods may point out some interesting patterns), the main distinction is that prediction requires the data to include a special response variable. The response may be categorical or numerical, thus further classifying predictive data mining as, respectively, classification and regression. In this review we address classification problems in particular: while the difference between the two lies mainly in the set of methods used, the data mining process applied to regression and classification problems is quite similar.</p><p>Before we go on, we use a simple example to introduce the basic concepts in predictive data mining and to show the application of two popular but quite different data classification techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Introductory example</head><p>For our example, consider a trauma surgeon specialized in hip arthroplasty who would like to know if and how she can predict a patient's long-term clinical status after the surgery.</p><p>The fictitious and purposely simplified data set (Fig. <ref type="figure" target="#fig_0">1A</ref>) we use in this example, whose structure was inspired by a real study <ref type="bibr" target="#b2">[3]</ref>, consists of 20 patient records each described by three attributes: 'health', giving the patient's overall health at the time of the operation, 'timing', which tells if the operation was on time or delayed, and 'complications', which reports on the degree of complications occurring during the operation.</p><p>The data includes the response variable called 'outcome' that reports if the treatment was considered successful as evaluated at the follow-up examination at least 2 years after the operation. The snapshots of visualization of data mining models and results presented in this section were obtained using the Orange data mining suite <ref type="bibr" target="#b3">[4]</ref>.</p><p>Our example shows the use of two different modeling techniques to induce predictive models from our data set. The first one, called a naÃ¯ve Bayesian classifier, is one of the simplest yet it is a useful and often a fairly accurate predictive data mining method <ref type="bibr" target="#b4">[5]</ref>. We build a naÃ¯ve Bayesian classifier by estimating various probabilities from the data. For instance, using a relative frequency estimate, an unconditional probability for a successful operation P(outcome = good) is 0.55, as there are 11 out of 20 patients in the data set labeled with this class. This is also the probability of a successful operation that we can predict for a patient in the absence of any other information. Prior probability gets updated when other attribute values are known. Following the naÃ¯ve Bayesian rule the probability of the outcome is proportional to the prior probability times the conditional probability of the attribute value given the outcome. For instance, if we know that the timing for our patient has been good we update the prior by multiplying it with P(timing = good|outcome = good) = 9/11 thus obtaining 0.846. Similarly, for the outcome = bad, the prior equals to 0.45, the update related to timing is P(timing = good|outcome = bad) = 5/9, thus obtaining 0.250. After normalization, the probability of a good outcome for this patient equals 0.643. Knowing the value of other attributes requires a further update of these probabilities. For instance, knowing that the patient had many complications during the surgery the probability of a good outcome decreases to about 0.5.</p><p>The naÃ¯ve Bayesian classifier is thus comprised of unconditional and conditional probabilities as estimated from the data set. The model can be nicely visualized with a nomogram <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> (Fig. <ref type="figure" target="#fig_0">1B</ref>), a graphical representation that may serve both for analysis of the model (how and to what extent do particular values of predictive factors influence the outcome) and for making predictions. Our nomogram shows, for instance, that complications = no is an attribute-value combination that is the most significant indicator of a good outcome. On the other side, bad timing reduces the probability of a successful treatment the most. The nomogram in Fig. <ref type="figure" target="#fig_0">1B</ref> also shows a classification of a patient with good timing and many complications. Nomograms have been frequently used to represent logistic regression models <ref type="bibr" target="#b7">[8]</ref> and a number of them are in regular clinical use <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>An explanation of classifications and model interpretability that allows the domain expert to inspect the innerworkings of the classifiers may both be very important in clinical medicine, where every decision should always be clearly motivated. Another popular data mining technique that addresses both aspects is the induction of decision trees. Decision trees are built through recursive data partitioning, where in each iteration the data is split according to the values of a selected attribute. The recursion stops at 'pure' data subsets which only include instances of the same class. Heuristics that include those for choosing the best split attributes and tree pruning aim at obtaining small but accurate trees by avoiding the overfitting of the data <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>Fig. <ref type="figure" target="#fig_0">1C</ref> shows a decision tree as induced from our data set. A simple pruning rule which does not allow data splitting if any resulting set contains fewer than two instances was used. The attribute timing is favored at a root node and splits 20 cases into a group of 14 (right branch, timing = good) and 6 (left branch, timing = bad). Notice that the latter leads to a leaf where the prevailing outcome is bad. Classification with a decision tree means following a path from the root node to the leaf, which also determines the outcome and its probability. For instance, for a patient with good health status, good timing and some complications we would reach the leftmost leaf at the bottom of the figure with a prediction outcome = good and associated probability of 0.75.</p><p>Decision trees are praised for their transparency, allowing the decision-maker to examine and understand the decision model and its workings. In addition, each path in the decision tree can be regarded as a decision rule. For instance, for the leftmost leaf at the bottom of the tree from Fig. <ref type="figure" target="#fig_0">1C</ref>, the inferred classification rule is: IF timing of the operation is good AND there were some complications during the operation AND patient's overall health at the time of the operation is good. THEN a good outcome of the operation is expected, P(outcome = good) = 0.75.</p><p>Similar classification rules can also be inferred directly, that is, without going through the construction of classification trees. A popular approach to such inference is a rule-covering algorithm where the conditional part of the rule is iteratively refined so as to cover mostly the examples from one prevailing class. Once such a rule is discovered, the examples it covers are removed from the training set, and rule discovery continues by running the algorithm on the remaining examples. The procedure terminates once all the examples have been covered. Popular implementations of this approach are the CN2 <ref type="bibr" target="#b12">[13]</ref> and AQ families of algorithms <ref type="bibr" target="#b13">[14]</ref>; the result of running the former on our example set is presented in Fig. <ref type="figure" target="#fig_1">2</ref>.</p><p>Fig. <ref type="figure" target="#fig_2">3</ref> shows how the naÃ¯ve Bayesian classifier and the decision tree classified three new cases. While the classifications are qualitatively similar, there are some differences in the predicted probabilities. To estimate how well each of our algorithms would perform on unseen cases, however, data mining most often uses a hold-out procedure that repeatedly builds classifiers for one and tests them on another data set. One such procedure that is useful when data sets are small is called 'leave-one-out'. For our 20 training instances, it in turn selects one case, induces the classifier on the 19 remaining cases and tests it on the selected case. We can then report, for instance, in how many of the 20 runs the classifiers predicted the correct outcome (classification accuracy), or report some other performance measures such as sensitivity or specificity. Fig. <ref type="figure" target="#fig_3">4</ref> shows an example of such a report and also points out that, while both classifiers did not perform well, the decision tree performed somewhat better. The poor performance on this data set, manifested in low classification accuracy as well as the low values of other statistics, can be attributed to the many conflicting cases with the same attribute values but of a different class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Predictive data mining (classification) methods</head><p>Predictive data mining methods originate from different research fields and often use very diverse modeling approaches. They come in various flavors and may be compared on the basis of:</p><p>â¢ their handling of missing data and noise;</p><p>â¢ their treatment of different types of attributes (categorical, ordinal, continuous); â¢ the presentation of classification models which may or may not allow the domain expert to examine it and understand the inner workings; â¢ the reduction of the number of tests <ref type="bibr" target="#b14">[15]</ref>, that is, the reduction of attributes needed to derive the conclusion; â¢ the computational cost for induction and the use of classification models; â¢ their ability to explain the decisions reached when models are used in decision-making; â¢ generalization, that is, the ability to perform well with unseen cases.  We list here some of the most commonly used predictive data mining methods and order them according to a recent ranking from the relevant pool at KDNuggets (http://www. kdnuggets.com/polls/2006/data mining methods.htm, April 2006) which asked data miners to name the techniques they most frequently use:</p><p>Decision trees use recursive data partitioning, induce transparent classifiers whose performance may suffer from data segmentation: the leaves in decision trees may include too few instances to obtain reliable predictions. The computational complexity of the induction algorithms is low due to powerful heuristics. Most current data mining suites include variants of C4.5 and its successor See5 and CART decision tree induction algorithms <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>Decision rules in the form of 'IF condition-based-onattribute-values THEN outcome-value' may be constructed from induced decision trees as in the C4.5rules <ref type="bibr" target="#b10">[11]</ref>, or can be derived directly from the data as is the case with AQ and CN2 algorithms <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. While in their performance these algorithms share most of their characteristics with decision trees, they may be computationally more expensive.</p><p>Logistic regression is a powerful and well-established method from statistics <ref type="bibr" target="#b15">[16]</ref>. It is an extension of ordinary regression and it can model a two-valued outcome which usually represents the occurrence or non-occurrence of some event. Like with the naÃ¯ve Bayesian classifier, the underlying model for probability is multiplicative <ref type="bibr" target="#b16">[17]</ref> but uses a more sophisticated method based on a maximum likelihood estimation to determine the coefficients in its probability formula. Handling of the missing attribute values is not straightforward. The model can be nicely represented through a nomogram <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>Artificial neural networks were up until recently the most popular artificial intelligence-based data modeling algorithm used in clinical medicine. This is probably due to their good predictive performance, albeit they may have a number of deficiencies <ref type="bibr" target="#b17">[18]</ref> that include high sensitivity to the parameters of the method-including those that determine the architecture of the network, high computational cost in training, and induction of the model that may -at best -be hard to interpret by domain experts. Neural networks may be able to model complex non-linear relationships, comprising an advantage over simpler modeling methods like the naÃ¯ve Bayesian classifier or logistic regression.</p><p>Support vector machines (SVM) are perhaps today's most powerful classification algorithm in terms of predictive accuracy <ref type="bibr" target="#b18">[19]</ref>. They are based on strong mathematical foundations and statistical learning theory <ref type="bibr" target="#b19">[20]</ref>. Central to the method is a procedure that finds a hyperplane that separates the examples of different outcomes (see Fig. <ref type="figure" target="#fig_4">5</ref>). Being primarily designed for two-class problems, SVMs find a hyperplane with a maximum distance to the closest point of the two classes; such a hyperplane is called the optimal hyperplane. A set of instances that is closest to the optimal hyperplane is called a support vector. Finding the optimal hyperplane provides a linear classifier. Besides such linear kernels, support vector machines are also frequently used with other, non-linear kernels which in essence transform the original attribute space to a new, higher dimensional space in which the linear classifier is inferred. Popular kernel functions are, for instance, polynomial, radial basis and sigmoid functions. The choice of the appropriate kernel should in principle depend on the properties of the data set and problem domain.</p><p>For real data sets, the hyperplane that would clearly separate the examples of different classes most often does not exist. To solve this problem, a soft margin method was proposed <ref type="bibr" target="#b20">[21]</ref> where the resulting hyperplane splits the data set into two sets that are as clean as possible, that is, where one class prevails to the highest possible degree.</p><p>Support vector machines are becoming increasingly popular in medicine and, in particular, in bioinformatics. With the exception of linear kernels, where the structure of the model can be easily revealed through the coefficients that define a linear hyperplane, support vector machines use a formalism that is often unsuitable for interpretation by human experts. As such, and if we are only interested in predictive accuracy, support vector machines are a serious contender to artificial neural networks, especially since their performance may be more robust and depend less on the specific selection of the method's parameters.</p><p>The naÃ¯ve Bayesian classifier is an approach we have already introduced. Despite its simplicity, its performance is often at least comparable with other more sophisticated approaches <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22]</ref>. Due to the fast induction of a classifier, it may be used as a baseline algorithm in comparative studies. When surpassed in predictive performance by other more sophisticated algorithms, this often indicates the presence of non-linear interactions between attributes.</p><p>Bayesian networks are probabilistic graphical models that are able to conveniently express a joint probability distribution over a number of variables through a set of conditional probability distributions. A Bayesian network is a directed acyclic graph where each node represents a stochastic variable and arcs represent a probabilistic dependency between a node and its parents. Each variable x i is assumed to be independent of its non-descendants given its set of parents, pa(x i ). Under this assumption, known as a Markov assumption, the joint probability distribution of all variables (x) can be written following the so-called chain rule:</p><formula xml:id="formula_0">p(x) = n i=1 p(x i |pa(x i ))</formula><p>The network is fully specified by a set of conditional probability distributions which quantifies the qualitative relationships between the variables expressed by the graph. Such probability distributions depend on a set of parameters Ã, such as the entries of the conditional probability tables for discrete variables or the mean and variance of the Gaussian distribution for continuous variables. Although Bayesian networks have been traditionally applied in medicine as an instrument to perform probabilistic reasoning <ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref>, several algorithms and tools are nowadays available to learn both the graph structure and the underlying probabilistic model from the data <ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref>. Bayesian networks can be easily applied in classification problems, where they can be seen as a generalization of the naÃ¯ve Bayesian classifier by modeling the interactions between the problem variables. They are now increasingly used in both predictive data mining and in bioinformatics <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>. The main drawbacks of this method lie in learning the graph structure, which may require a large data set, and in interpretation of the inferred causalities <ref type="bibr" target="#b27">[28]</ref>.</p><p>The algorithms for learning Bayesian networks from data are based on the framework of Bayesian model selection. The goal is to learn the structure S with the highest posterior probability distribution, given a data set x. Such a posterior probability distribution can be computed as:</p><formula xml:id="formula_1">p(S|x) = p(x|S)p(S) p(x) â p(x|S)p(S)</formula><p>The posterior is proportional to the product of two terms, namely the marginal likelihood p(x|S), that measures how likely the model is with respect to the data x, and the prior probability of the structure p(S). The marginal likelihood is obtained as the average of the likelihood over the values of the parameter set Ã. The marginal likelihood can be computed in close form only when the variables are discrete <ref type="bibr" target="#b28">[29]</ref> and when the model is conditionally Gaussian <ref type="bibr" target="#b32">[33]</ref>.</p><p>The comparison of the posterior distribution of the different structures requires the exploration of the space of all possible structures, a problem which turns out to be intractable with a brute-force approach. To cope with this problem, many heuristic algorithms have been proposed in the literature. The most widely applied is the greedy search algorithm K2, described in Ref. <ref type="bibr" target="#b28">[29]</ref>, but genetic algorithms <ref type="bibr" target="#b33">[34]</ref> and Monte Carlo Markov Chain techniques <ref type="bibr" target="#b34">[35]</ref> have also been successfully applied.</p><p>The k-nearest neighbors algorithm is inspired by the approach often taken by domain experts who make decisions based on previously seen similar cases <ref type="bibr" target="#b16">[17]</ref>. For a given data instance, the k-nearest neighbors classifier searches for the k most similar training instances and classifies based on their prevailing class. The search for the most similar instances may be slow and requires the retrieval of a complete training set at the time of classification.</p><p>The methods listed above are often an integral part of most modern data mining suites and, alone or in combination with pre-processing, often perform well and sufficiently fast. The biggest differences when treating clinical data may arise in the predictive performance and interpretability of results. Throughout this review, we advocate that both of these are important and if methods perform similarly with respect to accuracy those which offer an explanation and interpretable models should be preferred.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Standards</head><p>Standards in predictive data mining are sparse but emerging. Those recently gaining attention and wide acceptance are CRISP-DM, SEMMA and PMML. The first two are standards that define the process of data mining. CRISP-DM was crafted by the Cross-Industry Standard Process for the Data Mining Interest Group (www.crisp-dm.org), which in the late 1990s defined a so-called CRISP-DM Process Model <ref type="bibr" target="#b35">[36]</ref>. Related to predictive data mining are standards for data presentation and coding, like the Systemized Nomenclature of Human and Veterinary Medicine (SNOMED) and the Unified Medical Language System (UMLS). But as these standards are employed in clinical database management systems that data miners use to obtain their data, and as they help with the uniformity and consistency of the data sets <ref type="bibr" target="#b36">[37]</ref>, they have not (yet?) become an explicit part of any data mining system addressing the analysis of medical data. An exception to this but which lies beyond the scope of this review is medical text mining where, for instance, UMLS has been used to relate medical concepts and abstracts of papers cited in Medline <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Predictive data mining and statistics</head><p>In its brief history, data mining was at the start somewhat misleadingly associated solely with data analysis methods coming from fields other than statistics. The exposed characteristics of these methods were that they could address large quantities of data, make use of different data types (various types of attributes, text mining), were very flexible in modeling (e.g., inclusion of non-linearity) and could automate most of the analysis process. The initial success of the approaches in areas such as market basket analysis and text mining, along with the over-emphasis of machine-learning and pattern-recognition approaches in emerging data mining suites, provoked several statisticians to encourage their community to engage and contribute to the field <ref type="bibr" target="#b39">[40]</ref>. Since then, the field has matured substantially and its coming of age is also reflected in the way today's data mining relates to statistics. Data mining suites are becoming part of large statistical packages, major books on data mining have been written by statisticians <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b40">41]</ref>, while many recent developments in data mining have focused on bridging statistics, visualization and data analysis approaches from various fields of engineering.</p><p>A much-emphasized distinction between classical statistics and data mining involves the sheer size of data tackled by the latter <ref type="bibr" target="#b39">[40]</ref>. Data mining deals with secondary analysis. There, the data is not purposely collected to test some research hypothesis but is obtained from legacy databases or data warehouses where the volume of data may be much greater. In this paper, though, we argue that for applications in clinical data analysis other aspects of data mining may be just or even more relevant. Most importantly, these include making the knowledge discovered from the data explicit and communicable to domain experts, the provision of an explanation when deploying and using this knowledge with new cases, and the ability to encode and use the domain knowledge in the data analysis process.</p><p>Data sets -including those drawn from clinical medicineare often prone to different sources of noise, they may include various types of predictive features (e.g. nominal, real-valued, come from a time-series, etc.), may include a substantial number of missing feature values and may be governed by underlying non-linear processes. Modern data mining and statistical methods are often powerful enough to handle most of these cases, with the main difference being in the approach to the discovery of predictive models. Explorative data analysis in statistics most often involves a manual search and the modeling of, for instance, non-linearities and attribute interactions, whereas when using data mining one would first rely on automatic techniques such as constructive induction <ref type="bibr" target="#b41">[42]</ref>, attribute interaction discovery <ref type="bibr" target="#b42">[43]</ref> and approaches to non-linear modeling that systematically search through the data and attribute space. Existing knowledge in some problem domain would in statistics influence the composition of the data set to be collected, while -when appropriately encoded -it would help data mining to focus and report only on problem-relevant patterns found within secondary data analysis. For data mining algorithms, the data is only one source of information and others include any additional knowledge that can be obtained, encoded and made useful in the analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Predictive data mining and genomic medicine</head><p>In recent years, predictive data mining has received a strong impulse from research in molecular biology. Data mining methods such as hierarchical clustering <ref type="bibr" target="#b43">[44]</ref> or support vector machines <ref type="bibr" target="#b44">[45]</ref> are routinely applied in the analysis of high-throughput data coming from DNA microarrays or massspectrometry. Quite interestingly, over the last few years several papers have highlighted the potential of predictive data mining to infer clinically relevant models from molecular data and to therefore provide decision support in the novel field of genomic medicine <ref type="bibr" target="#b45">[46]</ref>. Nowadays, three different kinds of molecular data may be available to clinicians: (i) genotype data, often represented by a collection of single nucleotide polymorphisms (SNPs), DNA sequence variations that occur when a single nucleotide in the genome sequence is altered; since each individual has many SNPs, their sequence forms a unique DNA pattern for that person; (ii) gene expression data, which can be measured with DNA microarrays to obtain a snapshot of the activity of all genes in one tissue at a given time or with techniques that rely on a polymerase chain reaction (PCR) and real-time PCR when the expression of only a few genes needs to be measured with greater precision; (iii) protein expression data, which can include a complete set of protein profiles obtained with mass spectra technologies, or a few protein markers which can be measured with ad hoc essays. The majority of papers published in the area of predictive data mining for genomic medicine deals with the goal of analyzing gene expression data coming from DNA microarrays, consisting of thousands of genes for each patient, with the aim to diagnose (sub)types of diseases and to obtain a prognosis which may lead to individualized therapeutic decisions. The published papers are mainly related to oncology, where there is a strong need for defining individualized therapeutic strategies <ref type="bibr" target="#b46">[47]</ref>. A seminal paper from this area is that of Golub et al. <ref type="bibr" target="#b47">[48]</ref> and focuses on the problem of the early differential diagnosis of acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL). They were able to derive a classification model based on a weighted-voting approach relying on a list of about 50 genes. Today, there are many reports that show the potential usefulness of DNA microarray data for an outcome prediction in cancer treatment <ref type="bibr" target="#b48">[49]</ref><ref type="bibr" target="#b49">[50]</ref><ref type="bibr" target="#b50">[51]</ref>. To improve classification accuracy and the clinical relevance of the prognostic models, some authors have proposed an integration of clinical and gene expression data. Nevins et al. <ref type="bibr" target="#b51">[52]</ref> proposed a decision tree-based approach whereby the genes are first grouped into 'metagenes' and then used in a decision tree in conjunction with clinical data, such as lymphatic node status, to forecast a patient's survival. A different approach has been proposed by Futschik et al. <ref type="bibr" target="#b52">[53]</ref>, where clinical and microarray data are used to build two separate models for the outcome prediction of diffuse large B-cell lymphoma. The models employed are a Bayesian classifier and a Fuzzy Neural Network. The final prediction is obtained by means of an ensemble classifier whose parameters are also inferred from the training data. Clinical parameters and gene expressions have also been combined in Cox regression modeling in the risk stratification of Medulloblastomas <ref type="bibr" target="#b53">[54]</ref>.</p><p>In recent years some criticism has emerged against the gene-expression-based approaches to construct predictive models and derive lists of genes useful for outcome prediction <ref type="bibr" target="#b54">[55]</ref>. Although several groups have published lists of predictive genes with very good predictive performance, it has been observed that these may vary widely from study to study. Such variability may be related to a lack of robustness due to the small number of clinical cases with respect to the number of attributes. A recent commentary by Berrar et al. <ref type="bibr" target="#b55">[56]</ref> points out that many data mining papers in genomics and proteomics are affected by so-called selection bias since the feature selection is often (wrongly) performed on the entire data set prior to the cross-validation. This procedure adapts the classifier too much to the data set. The same problem, with reference to prominent early publications on the classification modeling of cancer gene expression data, was noted by Simon et al. <ref type="bibr" target="#b56">[57]</ref>. Ein-Dor et al. <ref type="bibr" target="#b57">[58]</ref> used a theoretical analysis to show that thousands of patients may be needed to obtain reliable gene lists. The integration of knowledge on the genes function and on the biomedical processes with clinical and gene expression data and the fusion of data coming from different studies <ref type="bibr" target="#b58">[59]</ref> are promising directions for improving the robustness and practical impact of those studies.</p><p>Thanks to the possibility of measuring mass spectra from serum, proteomic profiles drawn from mass spectrometry techniques <ref type="bibr" target="#b44">[45]</ref> have been analyzed to derive predictive models. In this case, the feature set is represented by a few hundreds or thousands of mass/charge ratios, in dependence on the resolution of the measurement technique. Predictive data mining approaches in this area have been applied to forecast patient outcomes in the case of prostate and ovarian cancer <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b60">61]</ref>. In those applications the pre-processing phase is crucial. For example, Yu et al. <ref type="bibr" target="#b44">[45]</ref> developed a strategy based on a combination of feature filtering with the Kolmogorov-Smirnov test, wavelet analysis and support vector machines to define the predictive model. The high number of features combined with the need for pre-processing the raw spectra makes the problem of learning robust models very hard. As proteomic data is characterized by many features and much fewer cases, the risk of overfitting is even higher than with microarray data sets. Several proposals for systematic procedures to extract predictive models from mass spectrometry data have been recently proposed to avoid these problems <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b62">63]</ref>.</p><p>Protein expression markers are also widely used for building prognostic models in cancer while recently there has been great interest in applying statistical modeling and data mining to the analysis of tissue microarray data, which are a new high-throughput tool for the study of protein expression patterns in tissue <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b64">65]</ref>. We can expect that this area will give rise to several data mining applications in the next few years.</p><p>Another area where predictive data mining has been applied is the analysis of data on single nucleotide polymorphisms (SNPs). Genome-wide association studies most often include several hundred patients and controls and consider several hundred thousands of SNPs, with the goal of identifying those for which the risk of the disease is increased. The ambitious goal is to use SPN information to find the genetic basis of so-called complex traits, i.e. those traits that do not strictly follow Mendelian inheritance. The definition of a multi-variate prognostic model is a typical data mining task which has been studied in several papers. For example, Sebastiani et al. <ref type="bibr" target="#b31">[32]</ref> use Bayesian networks to extract the relationships between SNPs and the risk of a stroke in patients suffering from Sickle cell anemia. They were able to extract a model with a small number of genes which were validated on a separate data set predicting the occurrence of a stroke in 114 individuals with 98.2% accuracy. Quite interestingly, in this paper the selection of genes and SNPs was performed by integrating prior knowledge in the data analysis process. The construction of models based on SNPs is not trivial since it has to face the same dimensionality problems in proteomics and genomics mentioned above. When it is important to model (or to discover) SNP interactions it is usually necessary to limit the analysis to several tens of SNPs due to the availability of data and complexity of the analysis <ref type="bibr" target="#b65">[66]</ref>. To improve scalability, progress in the field will depend on the use of interaction analysis, constructive induction, and visualization <ref type="bibr" target="#b66">[67]</ref>. Moreover, there is a need to integrate standard statistical analysis based on pedigrees and a linkage disequilibrium in order to reduce the number of SNPs.</p><p>With gene-gene interactions playing an important role in the susceptibility and progression of common diseases and response to treatment, and with the emerging case-control studies that collect genome-wide SNP data, the logical next step is a genome-wide, gene-gene interaction analysis. Yet, the data mining tools that could consider hundreds of thousands of SNPs and gene and protein expression profiles of thousands of patients do not exist yet. A major challenge to computer scientists is therefore to make these tools available and to design efficient heuristics to surpass the prohibitively complex exhaustive search for gene interactions. The challenge in designing such software is to provide an interactive, explorative analysis interface that provides users who are not computer scientists with seamless support in interaction discovery and the formation of new hypothesis to be then tested in a wet lab.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Contribution of data mining to predictive modeling in clinical medicine</head><p>Predictive models in clinical medicine are '. . . tools for helping decision making that combine two or more items of patient data to predict clinical outcomes' <ref type="bibr" target="#b67">[68]</ref>. Such models may be used in several clinical contexts by clinicians and may allow a prompt reaction to unfavorable situations <ref type="bibr" target="#b68">[69]</ref>. Data mining may effectively contribute to the development of clinically useful predictive models thanks to at least three inter-related aspects: (a) a comprehensive and purposive approach to data analysis that involves the application of methods and approaches drawn from different scientific areas; (b) the explanatory capability of such models; (c) the capability of using the domain (background) knowledge in the data analysis process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">A systematic and integrated process</head><p>As an engineering discipline, data mining relies on its associated process model which, being so important to the field, was recently regarded with much attention and for which several standards have been developed. The advantage but also the difficulty of data mining is that it is a framework that integrates various different approaches taken from diverse disciplines. Following the standard steps in studying a problem, data analysis and deployment can help researchers make systematic use of these various tools and appropriately choose from among the available techniques. Just like protocols in medicine, process standards in data mining help their users by guiding them through analysis process exposing those aspects that could otherwise be forgotten or neglected. Recently, a number of major data mining suites like that of the SPSS' Clementine (www.spss.com/spssbi/clementine) and the SAS' Enterprise Miner (www.sas.com/products/miner) have made the use of process standards explicit: there, the user chooses the phase he wants to address and is only shown a set of tools applicable to that phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Explanation</head><p>Data mining includes approaches that may play a double role: they may be used to derive a classification rule and to understand what information is contained in the available data. Inspired by early expert systems like Mycin <ref type="bibr" target="#b69">[70]</ref> and Internist <ref type="bibr" target="#b70">[71]</ref> that were quite rooted in medical applications, the explicit communication of knowledge discovered from the data and the subsequent explanation of decisions when this knowledge is used in the classification of new cases is what is emphasized by a number of data mining techniques. In the introductory example we have already demonstrated that classification trees can reveal interesting patterns in observational data.</p><p>Examples where such analysis has led to the discovery of new medical knowledge include studies of brain injury <ref type="bibr" target="#b71">[72]</ref>, geriatrics <ref type="bibr" target="#b72">[73]</ref> and trauma care <ref type="bibr" target="#b73">[74]</ref>.</p><p>Another formalism for representing classification models that allows for an easy explanation of the results are Bayesian networks. An interesting application of Bayesian network learning in the predictive data mining context has been recently published by Sierra and Larranaga <ref type="bibr" target="#b74">[75]</ref>. In their work they compare the accuracy of a naÃ¯ve Bayesian approach in forecasting the survival of malignant skin melanoma patients with that of the three different Bayesian networks induced from the data. Fig. <ref type="figure" target="#fig_5">6A</ref> shows an example of an induced Bayesian network as reproduced from the original paper. We note that, for example, the variable sex is not considered to be useful to classify the cases, while the variables 'number of positive nodes', 'thickness' and 'stage' are found to be dependent on each other. Fig. <ref type="figure" target="#fig_5">6B</ref> shows the naÃ¯ve Bayes model of the same problem. Since the network structure remains fixed after the learning phase, the graphical output only reflects the a priori assumptions on the variable relationships, while the learned knowledge is hidden in the probability tables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">The utility of domain knowledge</head><p>Together with the capability to explain, some data mining algorithms can take so-called 'background knowledge' into account. Literally, background knowledge is the 'information that is essential to understanding a situation or a problem' <ref type="bibr" target="#b75">[76]</ref>. In the process of building a predictive model, using background knowledge means being capable of taking into account information which is already known and should not be rediscovered from the data. This issue may be particularly important in the analysis of medical data <ref type="bibr" target="#b76">[77]</ref>. Background knowledge can be expressed in different formats: examples may be found in the areas of decision rules <ref type="bibr" target="#b77">[78]</ref>, Bayesian Models <ref type="bibr" target="#b78">[79]</ref>, fuzzy sets <ref type="bibr" target="#b79">[80]</ref> and concept hierarchies <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b80">81]</ref>. Among others, a method that may be particularly appropriate to deal and encode the background knowledge involves Bayesian networks. In Bayesian networks the background knowledge is exploited to define the network structure, i.e. number of variables, arcs and arc directions. Moreover, following the Bayesian paradigm prior probabilities on the conditional probability tables are given in order to take into account the background knowledge available on the relationships between the problem variables. Prior probabilities allow a model to be derived even when the information coming from the data is weak, and may help in avoiding overfitting where the derived model would reproduce the data too closely and fail to correctly classify the new and unseen cases <ref type="bibr" target="#b81">[82]</ref>. An example of this approach is a study on a Bayesian network designed to assess the prophylaxis of graft versus host disease after bone marrow transplantation in children <ref type="bibr" target="#b78">[79]</ref>. The network structure was assessed on the basis of the avail-able background knowledge, while the probabilities were first defined by experts and then updated on the basis of a data set of fifty patients <ref type="bibr" target="#b78">[79]</ref>. The use of background knowledge in building Bayesian networks and in eliciting their probabilities is an active area of research <ref type="bibr" target="#b81">[82]</ref><ref type="bibr" target="#b82">[83]</ref><ref type="bibr" target="#b83">[84]</ref> where data mining and knowledge engineering frequently combine their efforts and results.</p><p>Background knowledge can also be easily exploited in the construction of classification rules: for instance, an incomplete set of classification rules provided by the expert can be refined and augmented on the basis of the available data, while the rule search can be driven by a certain number of monotonicity constraints <ref type="bibr" target="#b84">[85,</ref><ref type="bibr" target="#b85">86]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Predictive data mining process: tasks and guidelines</head><p>Data mining is most often the application of a number of different techniques from various disciplines with the goal to discover interesting patterns from data. Given the large variety of techniques available and interdisciplinary fields, it is no surprise that data mining is often viewed as a craft that is hard to learn and even harder to master.</p><p>As we mentioned, several process models and standards have been proposed to introduce engineering principles, systemize the process and define typical data mining tasks. In the section on standards we introduced CRISP-DM, a data mining process standard that seems to be gaining the widest acceptance. While CRISP-DM enumerates a number of methods that may be used to accomplish data mining tasks, it is not meant to give precise guidelines on which techniques, evaluation schemes and statistics to use. Namely, these should all be specific to a problem domain, particular data mining tasks and the type of data under consideration. Predictive data mining in clinical medicine is an example of such a specific task and guidelines that in particular address different aspects of medical data analysis could be provided to accompany the CRISP-DM model and make it more useful in this In the following description of the predictive data mining process, we generally adhere to the CRISP-DM schema but we also try to be specific and list a number of problems, recommendations and guidelines that may apply to medical predictive mining and which have been proposed and evaluated by active researchers and developers in the field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Defining the problem, setting the goals</head><p>Predictive data mining is concerned with analyzing data sets that are composed of data instances (e.g., cases or list of observations), where each instance is characterized by a number of attributes (also referred to as predictors, features, factors, or explanatory variables). There is a special additional attribute called an outcome variable, also referred to as a class, dependent or response variable. In general, the task of predictive data mining is to find the best fitting model that relates attributes to the outcome. Unlike standard data mining data sets, medical data sets may be smaller: typically, the number of instances are from several tens to several thousands. The number of attributes may widely range from several tens (classical problems from clinical medicine) to thousands (proteomics, genomics). The goal of predictive data mining in clinical medicine is to construct a predictive model that is sound, makes reliable predictions and helps physicians improve their prognosis, diagnosis or treatment planning procedures. In terms of data analysis, there is a number of important questions that data mining may answer, including:</p><p>(a) Are the data and corresponding predictive features sufficient to construct a predictive model of acceptable performance? (b) Which of the attributes are the most predictive? Which are those that need to be included in the predictive model? (c) What is the relationship between the attribute and the outcome? (d) Can we find any interesting combination (or relationship) between the attributes? Can any intermediate factors be derived from original attributes that may boost the performance of the predictive model and indicate an interesting phenomenon?</p><p>To find the answer to (a), it is very useful, if not required, that the measures of success are defined at this stage of data mining. This may include the decision upon which statistics to use for evaluating the predictive models and what the acceptable ranges are for these. Defining the criteria of acceptability of the resulting model prior to the actual data mining may help in producing less biased and a more objective evaluation of data mining results.</p><p>Data mining is rich in methods that may help find the answers to the other three questions. Techniques such as feature ranking <ref type="bibr" target="#b86">[87]</ref>, feature subset selection <ref type="bibr" target="#b87">[88]</ref> and constructive induction <ref type="bibr" target="#b41">[42]</ref> may help find the most relevant features and construct new ones from a combination of features from the original set (questions b and d). As we previously discussed, many data mining methods such as classification trees <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> and rules <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> focus on the construction of interpretable predictive models expressed in a textual form that can be com-municated to and scrutinized by domain experts (questions c and d).</p><p>Data mining provides a large toolbox of techniques and so as to narrow the choice of which ones to use for a particular problem answering the following questions at the stage prior to actual data mining may help:</p><p>(1) Should the resulting model be 'transparent', i.e., defined through some language (like a set of rules) that the user may interpret? (2) When used in decision-making, should the predictive model support the explanation? (3) Should predictive models report the probabilities of outcomes? Should confidence intervals be reported? Knowing how the model derives its prediction and being able to use the model's logic to explain how the conclusion was reached may significantly increase a physician's confidence in the model and help increase its acceptance. The current practice, though, may be different: one of the most often used artificial intelligence-based data mining techniques in building predictive models from clinical data involves artificial neural networks, from which it is far from trivial to understand the mechanisms that govern computation of the outcome and may in this respect be considered a 'black box'. Often, such models are reliable in terms of prediction but it has also been shown that some much simpler techniques such as, for instance, the naÃ¯ve Bayes classifier, perform equally well <ref type="bibr" target="#b21">[22]</ref> and may additionally accommodate for explanation <ref type="bibr" target="#b4">[5]</ref> and model transparency <ref type="bibr" target="#b88">[89]</ref>. Somehow similar in terms of its simplicity of the model, predictive power and ability to explain, statisticians often recommend that data mining techniques should be compared to logistic regression <ref type="bibr" target="#b17">[18]</ref>. When performing clinical data mining, it may often be worthwhile to try the simple techniques first. Question 3 above is relatively rhetorical: yes, to be useful in clinical practice, predictive models should model probabilities and, wherever possible, should report on confidence intervals. During the 1990s, this would have been quite an exception since most predictive data mining methods provided only crisp classifications, that is, they only reported which of the outcomes was the most probable one without quantifying this probability <ref type="bibr" target="#b89">[90]</ref>. Only recently and through a relatively straightforward extension of existing algorithms most data mining methods do in fact allow the reporting of probabilities of outcomes. Rarely, however, do the data mining suites include implementations that are able to report the confidence intervals of predicted probabilities. Finally, another question arises that may highly influence the selection of data mining techniques: (4) Is there additional knowledge that domain experts can explicitly make available for the modeling methods? If so, how can this knowledge be encoded? Unfortunately, although several examples of the use of background knowledge in clinical data mining are available, as described in the previous section, no data mining standards are (yet) available on how to encode such knowledge. Techniques to allow the use of background knowledge are often crafted by specialized research groups and rarely, if at all, find their way into commer-cially available data mining suites. However, while the inclusion of background knowledge is far from trivial it may have most significant impact on both performance and comprehensibility <ref type="bibr" target="#b84">[85]</ref>. A substantial (but worthwhile!) effort is needed by the data mining community to standardize this area and make the existing academic tools available to the wider community of users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Data preparation</head><p>For data mining, clinical data most often come from dedicated databases (or even paper-based forms) that were purposely collected to study a particular clinical problem.</p><p>Although not yet widely available, another important source of clinical data is data warehouses. Currently, the most widely used data mining algorithms require data to be placed in a tabular form that includes predictive factors and outcomes and is constructed by querying single or several dedicated databases <ref type="bibr" target="#b90">[91]</ref>. An important rule in the construction and evaluation of predictive models is that they should never be built and tested on the same data set. For this, techniques like cross-validation are used (see the next section) but it may also be a good idea at this stage to split the data into two sets: the first one, often referred to as the learning set, is used to compare different data mining algorithms, estimate their performance using some statistical metrics, find the best set of parameters for feature ranking, selection and learning methods and, finally, to select the modeling technique that performs best. Using this technique, a final model is to be developed from a complete learning set and tested on a second data set, commonly referred to as a validation set. The data split may be arbitrary or based on time or source label of the data instances.</p><p>Separate learning and validation sets are necessary to objectively assess the predictive performance. Data mining models may be complex and in extreme cases may 'remember' each data instance that they learned from. Such models perform perfectly on data that was used for learning, but poorly with any new case that does not match some data instance from the learning data. It is said that such models generalize poorly due to overfitting. Most contemporary data mining techniques include efficient mechanisms to avoid overfitting, like pruning for decision trees, limiting the complexity for the neural network, and the selection of only the most significant rules for decision rule modeling, but it is only the evaluation of an independent data set that can guarantee that the good performance did not result from overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Modeling and evaluation</head><p>Once the data is split into a learning and validation set, it is now time to employ our modeling techniques and trim their parameters to the learning data set. The goal of this phase is to determine which data mining algorithm performs best so we can use it to generate our target predictive model. Predictive models can be evaluated on the basis of their predictive performance and comprehensibility. Of the two, predictive performance is easier to quantify and typical statistics include metrics such as sensitivity, specificity, classification accuracy <ref type="bibr" target="#b91">[92]</ref>, area under the ROC curve <ref type="bibr" target="#b92">[93]</ref> and Brier score <ref type="bibr" target="#b93">[94]</ref>. Comprehensibility is a subjective measure that is assessed by participating domain experts. While it may be prohibitively hard to quantify the comprehensibility, preferable models may be found by answering questions like: 'given the two models, which one is easier to understand? which one explains the decisions better? which one do the experts have greater confidence using?' If comprehensibility and explanation are at stake, the data mining algorithms can be ranked first using the chosen predictive performance statistics and then, of the few top ranking models, domain experts may select the final model based on its comprehensibility and ability to explain.</p><p>As mentioned in the previous section, to estimate those statistics that evaluate the predictive performance a desirable approach is to apply the so-called hold-out strategy: a subset of the learning set, the training set, is used to construct the model while another subset, the test set, is used to estimate the accuracy of the model. However, the holdout procedure makes quite inefficient use of the data: the typical strategy is to learn from two-thirds of the data and then to test on the remaining one-third of the sample. Such a strategy may not be applicable with a small number of data since the algorithms for learning the prognostic model may have problems due to the reduced data set for learning, while the test may be still insufficient to reach the desired confidence interval limits. A popular contemporary method to be use in solving the abovementioned problems is k-fold cross-validation. With cross-validation, the data are split into a number (k) of data subsets which contain approximately an equal number of data instances and approximately match the outcome distribution of the learning set (stratified cross-validation). Typically, the learning data set is split into ten data subsets (10-fold cross validation). Then, data from the nine subsets are used for modeling while the remaining subset is used to test the resulting model and assess statistics. The process of training and testing is repeated 10 times, each time using a different testing subset. Averaged statistics are then reported and characterize the modeling method. Besides cross-validation, other data splitting approaches may be used such as 'leaveone-out' cross-validation, random sampling, bootstrap, etc. <ref type="bibr" target="#b94">[95,</ref><ref type="bibr" target="#b95">96]</ref>.</p><p>Special attention should be paid to parameter estimation. Most data mining methods depend on a set of parameters that define the behavior of the learning algorithm and directly or indirectly influence the complexity of the resulting models. For instance, the degree of pruning may be set for decision tree induction, the number of units in a hidden layer may be set for feed-forward neural network models and the required level of statistical significance may be defined for decision rules. While the finding of the best set of parameters can be characterized as a search in parameter space that employs some state-of-the-art optimization technique, practitioners often define a set of most likely values of parameters and, again through cross validation, evaluate each set separately to find the winner. The evaluation of data mining methods then yields not only the ranking of data mining techniques but also identifies the appropriate parameter set to be used with. Note also that feature ranking, subset selection and construction may have their own parameters, which also require optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Construction of the target predictive model</head><p>The evaluation techniques described above provide the grounds for ranking data mining methods and identifies a suitable set of parameters. We can now use a complete evaluation data set and the best-ranked method to construct our predictive model. The resulting model is then evaluated on the validation data set and, if its predictive performance is acceptable, then this is now our target predictive model. Note that when reporting on the predictive qualities of the model, it is only the statistics obtained using the validation data set that have merit and should matter; reporting on results from learning data sets may be deceiving as they are prone to overfitting. If the task of data mining is to observe the relationships between the features and the features and the outcome, it is now time to scrutinize, analyze and visualize the resulting model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Deployment and dissemination</head><p>Once the predictive model has been constructed and evaluated, this is where most clinical data mining projects stop. This is quite unfortunate because clinical data mining should also be concerned with the deployment of resulting models and discovered relationships and with studying their potential impact on clinical environments. For instance, would finding an interesting relationship change the current medical practice? Can the constructed predictive model be used for day-to-day decision support? What would the value be of such a system? Would, once the model is deployed, the quality of health care increase or would some of the related costs decrease? Reports on the utility of models constructed by predictive data mining are at best rare and so is the deployment of predictive models in clinical environments.</p><p>Technically, one of the issues that may the prevent smooth dissemination of constructed predictive models within the clinical environment is related to bridging predictive data mining and decision support <ref type="bibr" target="#b96">[97,</ref><ref type="bibr" target="#b97">98]</ref>. Clinical decision support tools <ref type="bibr" target="#b98">[99]</ref> often include predictive models and have dedicated interfaces that ease the utility of the particular application for physicians or medical personnel. Data mining tools are often quite complex, may be very expensive and are intended for specialists. Data mining tools are optimized for model development and usually do not provide specific interfaces for when deploying the model. While model development is, as we have discussed in this paper, nontrivial and often a complex task, computing an outcome using the predictive model is usually straightforward and does not need much computational power. We should therefore not expect the data mining suites to be appropriate environments for decision support. A technical difficulty of bridging the two technologies is that data mining tools usually do not offer encoding and the saving of the developed models in a form that is compatible with some decision support systems.</p><p>There have been some recent advances that may help us tackle this problem and ease the bridging of data mining and decision support. In our review, we have already mentioned the PMML standard for encoding the prediction models in XML-based documents. If this or similar standards take ground, we may expect them to be supported by major data mining tools and, perhaps even more importantly, by decision support shells that would deploy a specific predictive model and provide an appropriate interface. As a demonstration of such technology, the decisions-at-hand schema by Zupan et al. <ref type="bibr" target="#b99">[100]</ref> allows for the saving of data mining models in the XML format and provides either web-or handheld-based decision support shells. For example, for the naÃ¯ve Bayesian classifier from Fig. <ref type="figure" target="#fig_0">1B</ref> a PocketPC interface that supports the data entry and reports the outcome is shown in Fig. <ref type="figure" target="#fig_6">7</ref>. The guiding idea of these and similar approaches is, on one side, to bridge data mining and decision support and, on the other side, to decouple the two technologies allowing users, in our case physicians and medical personnel, to use lightweight, inexpensive, computationally non-demanding and easy to use decision shells. Besides the utility per se, being able to save the predictive data mining model may also provide significant advantages in the communication of results in evaluation and testing phases of data mining projects <ref type="bibr" target="#b100">[101]</ref>.</p><p>Predictive data mining models can also be used as an instrument for assessing and comparing evidence-based medicine results with the outcomes collected in clinical practice. The availability of data collected in clinical institutions on their specific processes of disease management allows for the capability of integrating evidence-based strategies and hence of making prognostic reasoning, with local knowledge coming from the data collected in the clinical routine <ref type="bibr" target="#b101">[102]</ref>. The merging of evidence-based medicine and local experience may be seen as a particular kind of data mining problem where the background knowledge can be obtained from clinical studies. In this case, background knowledge is far more than a set of constraints; rather, it provides for a reference that can be complemented during the learning process <ref type="bibr" target="#b102">[103]</ref>. The goal of data analysis is therefore related to the better comprehension of the information that is contained in the data, thus highlighting cases that do not confirm well-established knowledge or problems in the data collection procedures. Moreover, its aim is to uncover the peculiarities of the specific clinical center in order to better tailor their prognostic guidelines. In this respect, data mining can be seen as part of a medical institution's information infrastructure, where it is used to 'procure' knowledge that is maintained and operationalized through a knowledge management system <ref type="bibr" target="#b67">[68]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>Compared to data mining in business, marketing and the economy, medical data mining applications have several distinguishing features <ref type="bibr" target="#b103">[104]</ref>. The most important one is that medicine is a safety critical context <ref type="bibr" target="#b104">[105]</ref> in which decisionmaking activities should always be supported by explanations. This means that the value of each datum may be higher than in other contexts: experiments can be costly due to the involvement of the personnel and use of expensive instrumentation and due to the potential discomfort of the patients involved.</p><p>In clinical mining, the data sets can be small and report nonreproducible situations. The data may be further affected by several sources of uncertainty, like those from measurement errors or missing data or from errors in coding the information buried in textual reports. Physicians and researchers deal with such difficulties by exploiting their knowledge of the domain. Similarly, data mining can cope with these problems by carefully applying variable and model selection, by correctly evaluating the resulting models and by explicitly encoding this knowledge and using it in data analysis <ref type="bibr" target="#b105">[106]</ref>. At present, data mining is a very diverse field with a number of techniques that may serve the same purpose and behave equally well. It may not be practical to explore all alternative methods when mining a particular data set, while the choice of which techniques to use is often guided by the instincts of expert data miners. While it is unlikely that with the current variety of approaches the community can come up with cookbooks and recipes, we have tried to provide some general task descriptions and a simple set of guidelines that may apply to the construction of clinical predictive models using data mining techniques. Overall, the ideas we have presented may be summarized in the following list:</p><p>â¢ Define the success criteria in advance. Set acceptable ranges of evaluation statistics prior to modeling. â¢ If possible and for reference compare the performance results with those obtained from classical statistical modeling. â¢ Model probabilities, not crisp class membership. Prefer methods that report confidence intervals. â¢ Avoid overfitting. Never test models on data that was used in their construction. In the case of small data sets, use cross-validation or similar techniques to obtain evaluation statistics. â¢ If possible, test the resulting model on an independent separate data set. â¢ Report performance scores with confidence intervals.</p><p>â¢ Prefer modeling techniques that expose relations and can present them in a readable form. If the discovery of relationships is a goal of data mining, avoid black-box models. â¢ If still of acceptable performance prefer simple modeling techniques, possibly those that derive models that can be reviewed and criticized by experts. â¢ Feature ranking, feature selection, constructive induction and so on, together with any parameter estimation, are all part of the modeling and should be tested within crossvalidation. Using them in pre-processing that takes place prior to cross-validation leads to overfitting. â¢ The project is not finished when a good model is found.</p><p>Think how to include your model within some clinical information or decision support system. If possible, perform a cost/benefit study. â¢ Explicitly assess the model's applicability and its potential for generalization. Here, in particular consider the type of data collection (retrospective, prospective, derived from a clinical trial or from clinical routine), the number of available data and performance of the model.</p><p>These guidelines relate to newly emerging issues in personalized and genomic medicine. Today, the construction of reliable predictive models may require the integration of data drawn from heterogeneous sources that include clinical, laboratory, genetic, genomic and proteomic data. The full availability of data repositories and warehouses able to concurrently provide such information about a single patient, and the methods to integrate it within a decision support system are issues which remain to be resolved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>At present, many ripe predictive data mining methods have been successfully applied to a variety of practical problems in clinical medicine. As suggested by Hand <ref type="bibr" target="#b39">[40]</ref>, data mining is particularly successful where data are in abundance.</p><p>For clinical medicine, this includes the analysis of clinical data warehouses, epidemiological studies and emerging studies in genomics and proteomics. Crucial to such data are those data mining approaches which allow the use of the background knowledge, discover interesting interpretable and non-trivial relationships, construct rule-based and other symbolic-type models that can be reviewed and scrutinized by experts, discover models that offer an explanation when used for prediction and, finally, bridge model discovery and decision support to deploy predictive models in daily clinical practice. With the promises offered by genomic medicine and upcoming needs to integrate molecular and clinical data, data mining and other knowledge-intensive computational approaches are becoming required for advancing the stateof-the-art of both research and real-life applications <ref type="bibr" target="#b106">[107,</ref><ref type="bibr" target="#b107">108]</ref>. Last but not least, clinical data mining deals with 'bed-side' problems, that is, with models that forecast the patient's outcome. Decision-making that uses a particular prediction model should therefore also take into account the issues of ethics and the cost of prediction while being concerned with the analysis of outcomes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 -</head><label>1</label><figDesc>Fig. 1 -Induction of prediction models. The figure shows an example of a training data set with three attributes, an outcome and 20 instances (A), a nomogram representing a naÃ¯ve Bayesian classifier (B), and a decision tree developed from the same data set (C). To use a nomogram for prediction, each attribute value relates to the number of points (the topmost scale), which after summation give the total number of points and corresponding probability (the two scales on the bottom of B).</figDesc><graphic coords="4,85.44,66.73,420.12,461.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 -</head><label>2</label><figDesc>Fig. 2 -Classification rules inferred by a CN2-like covering algorithm from the data set from Fig. 1A. While the first rule covers only those examples with a good outcome, the class distribution of the other two rules is mixed as the coverage includes one example from the minority (good outcome) class. Rule quality was assessed through a Laplace probability estimate.</figDesc><graphic coords="5,52.54,66.61,237.60,152.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 -</head><label>3</label><figDesc>Fig. 3 -Predictions of the naive Bayesian classifier (Fig. 1B) and decision tree (Fig. 1C) for three different cases. The question mark in the third case for the attribute Health signifies a missing (unknown) value. Probabilities by each classifier are given for both outcomes, 'good' and 'bad' (rightmost two columns, probabilities are separated by a column, the prevailing class label is also shown).</figDesc><graphic coords="5,52.54,554.37,237.60,108.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 -</head><label>4</label><figDesc>Fig. 4 -Evaluation results for a naive Bayesian classifier and decision tree inference algorithm on an example data set from Fig. 1A using a 'leave-one-out' test.</figDesc><graphic coords="5,311.57,66.79,234.00,183.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 -</head><label>5</label><figDesc>Fig. 5 -Scatterplot of a two-class data set with maximum-margin hyperplanes found by a support vector machine induction algorithm with a linear kernel. The data instances along the hyperplanes that define the margin (plotted in red) are called support vectors.</figDesc><graphic coords="6,64.43,66.37,204.48,200.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 -</head><label>6</label><figDesc>Fig. 6 -The output of the survival prediction problem in a malignant skin tumor, presented by Sierra et al. [75]. Subfigure (A) shows the Bayesian network as induced from the data, while (B) shows the naive Bayesian model. Model (A) better describes the relationships between the variables and the outcomes.</figDesc><graphic coords="10,85.44,66.16,419.76,173.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 -</head><label>7</label><figDesc>Fig. 7 -Snapshot of decisions-at-Hand software on a PocketPC that shows the nomogram reporting on the outcome. The prediction was made on the same case as shown in Fig. 1B.</figDesc><graphic coords="13,332.57,66.36,192.24,255.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>.org) is very relevant to the communication, sharing and deployment of predictive models. PMML is an emerging vendor-independent open standard for defining data mining models. It defines an XML-based markup language for the encoding of many predictive data mining models that include decision trees and rules, NaÃ¯ve Bayesian Classifiers and logistic regression models. The most recent version of popular data mining suites supports this standard by being able to export and import models encoded in complying XML files.</figDesc><table><row><cell>CRISP-DM</cell></row><row><cell>breaks data mining into several phases: business and data</cell></row><row><cell>understanding, data preparation, modeling, evaluation and</cell></row><row><cell>deployment. It defines the inputs, outputs and general strate-</cell></row><row><cell>gies to be applied in each phase. SEMMA (sample, explore,</cell></row><row><cell>modify, model, assess) is a data mining methodology proposed</cell></row><row><cell>by the SAS Institute and used within its powerful data mining</cell></row><row><cell>suite. While CRISP-DM provides for a comprehensive project</cell></row><row><cell>management template, SEMMA focuses mostly on the appli-</cell></row><row><cell>cation to exploratory statistical and visualization-based data</cell></row><row><cell>mining techniques.</cell></row><row><cell>Predictive Data Mining Markup Language (PMML,</cell></row><row><cell>www.dmg</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>i n t e r n a t i o n a l j o u r n a l o f m e d i c a l i n f o r m a t i c s 7 7 ( 2 0 0 8 ) 81-97</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to acknowledge the help given by the International Medical Informatics Association and its Working Group on Intelligent Data Analysis and Data Mining, which they are chairing. The work was supported by a Slovenian-Italian Bilateral Collaboration Project. RB is also supported by the Italian Ministry of University and Scientific Research through the PRIN Project 'Dynamic modeling of gene and protein expression profiles: clustering techniques and regulatory networks', and BZ by the Slovenian Research Agency's Program Grant. r e f e r e n c e s</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Giudici</surname></persName>
		</author>
		<title level="m">Applied Data Mining Statistical Methods for Business and Industry</title>
		<imprint>
			<publisher>Wiley &amp; Sons</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Data mining and knowledge discovery in databases</title>
		<author>
			<persName><forename type="first">U</forename><surname>Fayyad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Piatetsky-Shapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="24" to="26" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Predicting patient&apos;s long-term clinical status after hip arthroplasty using hierarchical decision modelling and data mining, Meth</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zupan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Demsar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Smrke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bozikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Stankovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Bratko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Beck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Med</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="25" to="31" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Orange: from experimental machine learning to interactive data mining</title>
		<author>
			<persName><forename type="first">J</forename><surname>Demsar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zupan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Leban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Curk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference of Machine Learning</title>
		<meeting><address><addrLine>Pisa, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="537" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Inductive and Bayesian learning in medical diagnosis</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kononenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Artif. Intelligen</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="317" to="337" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A practical device for the application of a diagnostic or prognostic function, Meth</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lubsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Van Der Does</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Med</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="127" to="129" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Nomograms for visualization of naive bayesian classifier</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mozina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Demsar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Kattan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zupan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Principles Practice of Knowledge Discovery in Databases (PKDD-04)</title>
		<meeting>the Principles Practice of Knowledge Discovery in Databases (PKDD-04)<address><addrLine>Pisa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="337" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E</forename><surname>Harrell</surname></persName>
		</author>
		<title level="m">Regression Modeling Strategies: With Applications to Linear Models, Logistic Regression, and Survival Analysis</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A preoperative nomogram for disease recurrence following radical prostatectomy for prostate cancer</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Kattan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Eastham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Stapleton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Wheeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Scardino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Natl. Cancer Inst</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="766" to="771" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">International validation of a preoperative nomogram for prostate cancer recurrence after radical prostatectomy</title>
		<author>
			<persName><forename type="first">M</forename><surname>Graefen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">I</forename><surname>Karakiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Cagiannos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Henshall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Grygiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Clin. Oncol</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="3206" to="3212" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
		<title level="m">C4.5: Programs for Machine Learning</title>
		<meeting><address><addrLine>San Mateo, Calif</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Classification and Regression Trees</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Chapman &amp; Hall</publisher>
			<pubPlace>New York, London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The CN2 Induction Algorithm</title>
		<author>
			<persName><forename type="first">P</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Niblett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="261" to="283" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning patterns in noisy data: the AQ approach</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kaufman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and its Applications</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Karkaletsis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Spyropoulos</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="22" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Intelligent data analysis for medical diagnosis: using machine learning and temporal abstraction</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lavrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kononenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Keravnou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kukar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zupan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Commun</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="191" to="218" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Applied Logistic Regression, 2nd ed</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Hosmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lemeshow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<title level="m">The Elements of Statistical Learning: Data Mining, Inference, and Prediction</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On the misuses of artificial neural networks for prognostic and diagnostic classification in oncology</title>
		<author>
			<persName><forename type="first">G</forename><surname>Schwarzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Vach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schumacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Med</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="541" to="561" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">An Introduction to Support Vector Machines and Other Kernel-Based Learning Methods</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge, UK, New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">Statistical Learning Theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Support-vectors networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Machine learning for medical diagnosis: history, state of the art and perspective</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kononenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Med</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="89" to="109" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Medical expert systems based on causal probabilistic networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Andreassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">V</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Olesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Biomed. Comput</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Montironi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Abmayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bibbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Bartels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Clinical applications of Bayesian belief networks in pathology</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="237" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">NasoNet, modeling the spread of nasopharyngeal cancer with networks of probabilistic events in discrete time</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Galan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Aguado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Diez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Med</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="247" to="264" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The role of Bayesian Networks in the diagnosis of pulmonary embolism</title>
		<author>
			<persName><forename type="first">D</forename><surname>Luciani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marchesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bertolini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Thromb. Haemost</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="698" to="707" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sequential updating of conditional probabilities on directed graphical structures</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Spiegelhalter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Lauritzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="579" to="605" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A guide to the literature on learning probabilistic networks from data</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Buntine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Know. Data Eng</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="195" to="210" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A Bayesian method for the induction of probabilistic networks from data</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Herskovits</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="309" to="347" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Robust learning with missing data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ramoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="147" to="170" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Application of a data-mining method based on Bayesian networks to lesion-deficit analysis</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Herskovits</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Gerring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1664" to="1673" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Genetic dissection and prognostic modeling of overt stroke in sickle cell anemia</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sebastiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Ramoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Steinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Genet</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="435" to="440" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning Gaussian networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hackerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>De Mantaras</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Poole</surname></persName>
		</editor>
		<meeting>the Tenth Conference on Uncertainty in Artificial Intelligence<address><addrLine>San Francisco, CA/Seattle, WA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="235" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning Bayesian networks by genetic algorithms: a case study in the prediction of survival in malignant skin melanoma</title>
		<author>
			<persName><forename type="first">P</forename><surname>Larra Ãaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sierra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Michelena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P J M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence in Medicine Europe</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Keravnou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Garbay</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Baud</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Wyatt</surname></persName>
		</editor>
		<meeting><address><addrLine>Grenoble, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="261" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Using prior knowledge to improve genetic network reconstruction from microarray data</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Le</forename><surname>Phillip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In. Silico. Biol</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="335" to="353" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kerber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Khabaza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Reinartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shearer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wirth</surname></persName>
		</author>
		<title level="m">CRISP-DM 1. 0: Step-by-Step Data Mining Guide: The CRISP-DM Consortium</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Anatomic pathology data mining</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Berman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Data Mining and Knowledge Discovery</title>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Cios</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin/Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="61" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Supporting discovery in medicine by association rule mining in Medline and UMLS</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hristovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Peterlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dzeroski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medinfo</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1344" to="1348" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Effective mapping of biomedical text to the UMLS Metathesaurus: the MetaMap program</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Aronson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AMIA Symp</title>
		<meeting>AMIA Symp</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="17" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Data mining: statistics and more?</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Hand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. Statist</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="112" to="118" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mannila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
		<title level="m">Principles of Data Mining</title>
		<meeting><address><addrLine>Cambridge, Mass</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Data-driven constructive induction</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bloedorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Michalski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intell. Syst</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Attribute interactions in medical data analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jakulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Bratko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Smrke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Demsar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zupan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Conference on Artificial Intelligence in Medicine in Europe (AIME 2003)</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Dojad</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Keravnou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Barahona</surname></persName>
		</editor>
		<meeting>the Ninth Conference on Artificial Intelligence in Medicine in Europe (AIME 2003)<address><addrLine>Protaras, Cyprus</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="229" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Cluster analysis and display of genome-wide expression patterns</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Eisen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Spellman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Botstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci. U. S. A</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="14863" to="14868" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Ovarian cancer identification based on dimensionality reduction for high-throughput mass spectrometry data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ongarello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fiedler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Toffolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cobelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Trajanoski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="2200" to="2209" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Tarczy-Hornoch, Data integration and genomic medicine</title>
		<author>
			<persName><forename type="first">B</forename><surname>Louie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Martin-Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Halevy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Biomed. Inform</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="5" to="16" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Using molecular information to guide brain tumor therapy</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Mischel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cloughesy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Clin. Pract. Neurol</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="232" to="233" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Molecular classification of cancer: class discovery and class prediction by gene expression monitoring</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Slonim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tamayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gaasenbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Mesirov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Coller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Loh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">286</biblScope>
			<biblScope unit="page" from="531" to="537" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Gene expression profiling predicts clinical outcome of breast cancer</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Van't Veer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Van De Vijver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Peterse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Van Der Kooy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">415</biblScope>
			<biblScope unit="page" from="530" to="536" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Prediction of central nervous system embryonal tumour outcome based on gene expression</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Pomeroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tamayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gaasenbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Sturla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Angelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Mclaughlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Goumnerova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">415</biblScope>
			<biblScope unit="page" from="436" to="442" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Diffuse large B-cell lymphoma outcome prediction by gene-expression profiling and supervised machine learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Shipp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tamayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Kutok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Aguiar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gaasenbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Angelo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Med</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="68" to="74" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Towards integrated clinico-genomic models for personalized medicine: combining gene expression signatures and clinical factors in breast cancer outcomes prediction</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Nevins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dressman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pittman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>West</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hum. Mol. Genet</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">Spec</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Prediction of clinical behaviour and treatment for cancers</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Futschik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Reeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kasabov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Bioinform</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="53" to="S58" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Combining gene expression profiles and clinical parameters for risk stratification in medulloblastomas</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fernandez-Teijeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Betensky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Sturla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tamayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Pomeroy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Clin. Oncol</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="994" to="998" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Molecular classification and molecular forecasting of breast cancer: ready for clinical application?</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Brenton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Carey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Caldas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Clin. Oncol</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="7350" to="7360" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Avoiding model selection bias in small-sample genomic datasets</title>
		<author>
			<persName><forename type="first">D</forename><surname>Berrar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dubitzky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1245" to="1250" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Pitfalls in the use of DNA microarray data for diagnostic and prognostic classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Radmacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dobbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Mcshane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Natl. Cancer Inst</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="14" to="18" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Thousands of samples are needed to generate a robust gene list for predicting outcome in cancer</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ein-Dor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Zuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Domany</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci. U. S. A</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="5923" to="5928" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">The molecular portraits of breast tumors are conserved across microarray platforms</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Marron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">F</forename><surname>Qaqish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Livasy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Carey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>BMC Genom</publisher>
			<biblScope unit="page">96</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Serum protein fingerprinting coupled with a pattern-matching algorithm distinguishes prostate cancer from benign prostate hyperplasia and healthy men</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Clements</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Cazares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">J</forename><surname>Semmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Schellhammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Res</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="3609" to="3614" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Use of proteomic patterns in serum to identify ovarian cancer</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Petricoin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Ardekani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Hitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Fusaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Steinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Mills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Simone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lancet</title>
		<imprint>
			<biblScope unit="volume">359</biblScope>
			<biblScope unit="page" from="572" to="577" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A new approach for the analysis of mass spectrometry data for biomarker discovery</title>
		<author>
			<persName><forename type="first">N</forename><surname>Barbarini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Magni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bellazzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AMIA Annu Symp. Proc</title>
		<imprint>
			<biblScope unit="page" from="26" to="30" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Class prediction and discovery using gene microarray and proteomics mass spectroscopy data: curses, caveats, cautions</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Somorjai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dolenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Baumgartner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1484" to="1491" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Statistical methods for analyzing tissue microarray data</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Minin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Seligson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Horvath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Biopharm. Stat</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="671" to="685" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Defining aggressive prostate cancer using a 12-gene model</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Bismar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Demichelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Riva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Varambally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kutok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Aster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neoplasia</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="59" to="68" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Machine learning for detecting gene-gene interactions: a review</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Mckinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Bioinform</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="77" to="88" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">A flexible computational framework for detecting, characterizing, and interpreting statistical patterns of epistasis in genetic studies of human disease susceptibility</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">T</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Holden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Barney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Theor. Biol</title>
		<imprint>
			<biblScope unit="volume">241</biblScope>
			<biblScope unit="page" from="252" to="261" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Prognostic models: clinically useful or quickly forgotten?</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Wyatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Altman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMJ</title>
		<imprint>
			<biblScope unit="page">311</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Pretreatment nomogram for predicting the outcome of three-dimensional conformal radiotherapy in prostate cancer</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Kattan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Zelefsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Kupelian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Scardino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fuks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Leibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Clin. Oncol</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="3352" to="3359" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Computer-based consultations in clinical therapeutics: explanation and rule acquisition capabilities of the MYCIN system</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Shortliffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Axline</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Buchanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biomed. Res</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="303" to="320" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Internist-1, an experimental computer-based diagnostic consultant for general internal medicine</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Pople</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Myers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">N. Engl. J. Med</title>
		<imprint>
			<biblScope unit="volume">307</biblScope>
			<biblScope unit="page" from="468" to="476" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Predicting recovery in patients suffering from traumatic brain injury by using admission variables and physiological data: a comparison between decision tree analysis and logistic regression</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Sleeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Statham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mcquatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Corruble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Howells</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Macmillan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurosurg</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="326" to="336" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">A classification tree for predicting recurrent falling in community-dwelling older persons</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Stel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Pluijm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Deeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Smit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Bouter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Geriatr. Soc</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="1356" to="1364" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Patients with hip fracture: subgroups and their outcomes</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Eastwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Magaziner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Silberzweig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Hannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Strauss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Siu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Geriatr. Soc</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="1240" to="1249" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Predicting survival in malignant skin melanoma using Bayesian networks automatically induced by genetic algorithms An empirical comparison between different approaches</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sierra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Larranaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Med</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="215" to="230" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fellbaums</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
		<title level="m">WordNet An Electronic Lexical Database</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Knowledge-based data analysis and interpretation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zupan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bellazzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Med</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="163" to="165" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">The utility of background knowledge in learning medical diagnostic rules</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lavrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dzeroski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pirnat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Krizman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Artif. Intelligen</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="273" to="293" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">An influence diagram for assessing GVHD prophylaxis after bone marrow transplantation in children</title>
		<author>
			<persName><forename type="first">S</forename><surname>Quaglini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bellazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Locatelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stefanelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Salvaneschi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Decis. Mak</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="223" to="235" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Knowledge-based and data-driven models in arrhythmia fuzzy classification, Meth</title>
		<author>
			<persName><forename type="first">R</forename><surname>Silipo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vergassola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Berthold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Med</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="397" to="402" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Two-stage machine learning model for guideline development</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Shankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Pazzani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Med</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="51" to="71" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Expert knowledge and its role in learning Bayesian Networks in medicine: an appraisal</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lucas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence in Medicine</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Quaglini</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Barahona</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Andreassen</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="156" to="166" />
			<date type="published" when="2001">2001</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Building probabilistic networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Druzdzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Van Der Gaag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transn. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="481" to="486" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note>Where do the numbers come from?</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Sensitivity analysis: an aid for belief-network quantification</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M H</forename><surname>Coupe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Van Der Gaag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D F</forename><surname>Habbema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Eng. Rev</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="215" to="232" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">The utility of background knowledge in inductive learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pazzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kibler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="57" to="94" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Acceptance of rules generated by machine learning among medical experts, Meth</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Pazzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Shankle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Med</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="380" to="385" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Estimating attributes: analysis and extensions of RELIEF</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kononenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Machine Learning (ECML)</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="171" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Wrappers for feature subset selection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="273" to="324" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Machine learning for survival analysis: a case study on recurrence of prostate cancer</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zupan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Demsar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Kattan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Bratko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Med</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="59" to="75" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Intelligent data analysis in medicine and pharmacology: a position statement</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bellazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zupan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Intelligent Data Analysis in Medicine and Pharmacology (IDAMAP)</title>
		<meeting><address><addrLine>Brighton, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="2" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Data preparation for data mining</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pyle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Morgan Kaufmann Publishers</publisher>
			<pubPlace>San Francisco, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Rule evaluation measures: a unifying view</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lavrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Flach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zupan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Inductive Logic Programming</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="174" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">The use of relative operating characteristic (ROC) curves in test performance evaluation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Shultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arch. Pathol. Lab. Med</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="13" to="20" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Verification of forecasts expressed in terms of probability</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Brier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Month. Weather Rev</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="1" to="3" />
			<date type="published" when="1950">1950</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<title level="m">Data Mining: Practical Machine Learning Tools and Techniques With Java Implementations</title>
		<meeting><address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Construction and assessment of classification rules</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Hand</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Chichester</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Integrating Decision Support and Data Mining by Hierarchical Multi-Attribute Decision Models, in: Intl. Workshop on Integration and Collaboration Aspects of Data Mining, Decision Support and Meta-Learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bohanec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zupan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="25" to="36" />
			<pubPlace>Helsinki, Finland</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Factors that predict outcome of intensive care treatment in very elderly patients: a review</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>De Rooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abu-Hanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>De Jonge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Crit. Care</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="307" to="R314" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H V</forename><surname>Bemmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Musen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Helder</surname></persName>
		</author>
		<title level="m">Handbook of Medical Informatics</title>
		<meeting><address><addrLine>Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Decisions at hand: a decision support system on handhelds</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zupan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Porenta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Vidmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Aoki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Bratko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Beck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medinfo</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="566" to="570" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">Orange and Decisions-at-Hand: bridging predictive data mining and decision support., in: Intlerationa Workshop on Integration and Collaboration Aspects of Data Mining, Decision Support and Meta-Learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zupan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Demsar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Kattan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ohori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Graefen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bohanec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Beck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="151" to="162" />
			<pubPlace>Helsinki, Finland</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Knowledge management in healthcare: towards &apos;knowledge-driven&apos; decision-support services</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Abidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Med. Inf</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="5" to="18" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Knowledge discovery from data?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pazzani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intell. Syst</title>
		<imprint>
			<biblScope unit="page" from="10" to="13" />
			<date type="published" when="2000-04">March-April 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Uniqueness of medical data mining</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Cios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Med</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title level="m" type="main">Safe and Sound: Artificial Intelligence In Hazardous Applications</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Das</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, Mass</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Intelligent data analysis, Meth</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bellazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zupan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Med</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="362" to="364" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Health care in the information society A prognosis for the year 2013</title>
		<author>
			<persName><forename type="first">R</forename><surname>Haux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ammenwerth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Herzog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Knaup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Med. Inf</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="3" to="21" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<ptr target="http://research.microsoft.com/towards2020science" />
		<title level="m">Towards 2020 Science</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
