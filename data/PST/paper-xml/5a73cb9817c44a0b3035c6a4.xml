<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Visual Tracking: Review and Experimental Comparison</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-11-06">November 6, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Peixia</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information and Communication Engineering</orgName>
								<orgName type="department" key="dep2">Faculty of Electronic Information and Electrical Engineering</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information and Communication Engineering</orgName>
								<orgName type="department" key="dep2">Faculty of Electronic Information and Electrical Engineering</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information and Communication Engineering</orgName>
								<orgName type="department" key="dep2">Faculty of Electronic Information and Electrical Engineering</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information and Communication Engineering</orgName>
								<orgName type="department" key="dep2">Faculty of Electronic Information and Electrical Engineering</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lijun</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information and Communication Engineering</orgName>
								<orgName type="department" key="dep2">Faculty of Electronic Information and Electrical Engineering</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information and Communication Engineering</orgName>
								<orgName type="department" key="dep2">Faculty of Electronic Information and Electrical Engineering</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information and Communication Engineering</orgName>
								<orgName type="department" key="dep2">Faculty of Electronic Information and Electrical Engineering</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information and Communication Engineering</orgName>
								<orgName type="department" key="dep2">Faculty of Electronic Information and Electrical Engineering</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Visual Tracking: Review and Experimental Comparison</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-11-06">November 6, 2017</date>
						</imprint>
					</monogr>
					<idno type="MD5">B80951595A73A2911A538CB339823E9B</idno>
					<idno type="DOI">10.1016/j.patcog.2017.11.007</idno>
					<note type="submission">Received date: 3 May 2017 Revised date: 19 October 2017 Accepted date: 5 November 2017 Preprint submitted to Pattern Recognition</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Pattern Recognition Visual Tracking</term>
					<term>Deep Learning</term>
					<term>CNN</term>
					<term>RNN</term>
					<term>Pre-training</term>
					<term>Online Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, deep learning has achieved great success in visual tracking. The goal of this paper is to review the state-of-the-art tracking methods based on deep learning. First, we introduce the background of deep visual tracking, including the fundamental concepts of visual tracking and related deep learning algorithms. Second, we categorize the existing deep-learningbased trackers into three classes according to network structure, network function and network training. For each categorize, we explain its analysis of the network perspective and analyze papers in different categories. Then, we conduct extensive experiments to compare the representative methods on the popular OTB-100, TC-128 and VOT2015 benchmarks. Based on our observations, we conclude that: (1) The usage of the convolutional neural network (CNN) model could significantly improve the tracking performance. (2) The trackers using the convolutional neural network (CNN) model to distinguish the tracked object from its surrounding background could get more accurate results, while using the CNN model for template matching is usually faster. (3) The trackers with deep features perform much better than those with low-level hand-crafted features. (4) Deep features from different convolutional layers have different characteristics and</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Online visual tracking is a very critical issue in computer vision and video processing, which has numerous realistic applications including navigation, surveillance, robotics, traffic control, augmented reality, to name a few. Many efforts have been done in last decades, however, it is still a challenging task to develop a robust and efficient tracker due to difficulties from partial occlusion, illumination variation, background clutter, motion blur, viewpoint change and so on.</p><p>Traditional tracking algorithms usually focus on developing robust appearance model from the perspectives of hand-crafted features, online learning algorithms, or both. Some milestones include IVT <ref type="bibr" target="#b0">[1]</ref>, MIL <ref type="bibr" target="#b1">[2]</ref>, TLD <ref type="bibr" target="#b2">[3]</ref>, APGL1 <ref type="bibr" target="#b3">[4]</ref>, SCM <ref type="bibr" target="#b4">[5]</ref>, ASLAS <ref type="bibr" target="#b5">[6]</ref>, STRUCK <ref type="bibr" target="#b6">[7]</ref>, and KCF <ref type="bibr" target="#b7">[8]</ref>. However, the reports on large-scale benchmark evaluations (both OTB-100 <ref type="bibr" target="#b8">[9]</ref>, TC128 <ref type="bibr" target="#b9">[10]</ref> and VOT2015 <ref type="bibr" target="#b10">[11]</ref>) suggest that the performance of these traditional algorithms is far from the requirement of realistic applications.</p><p>Over the last five years, deep learning <ref type="bibr" target="#b11">[12]</ref> have achieved an impressive</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T suite of results thanks to their success on automatic feature extraction via multi-layer nonlinear transformations, especially in computer vision <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">14]</ref>, speech recognition <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b16">16]</ref> and natural language processing <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b18">18]</ref>.</p><p>Motivated by these breakthroughs, several deep-learning-based trackers (e.g., FCNT <ref type="bibr" target="#b19">[19]</ref>, MDNet <ref type="bibr" target="#b20">[20]</ref>, STCT <ref type="bibr" target="#b21">[21]</ref>, SINT <ref type="bibr" target="#b22">[22]</ref>, SiameFC <ref type="bibr" target="#b24">[23]</ref>, C-COT <ref type="bibr" target="#b25">[24]</ref>, GOTURN <ref type="bibr" target="#b26">[25]</ref> , TCNN <ref type="bibr" target="#b27">[26]</ref>, ADNet <ref type="bibr" target="#b28">[27]</ref> and SANet <ref type="bibr" target="#b29">[28]</ref>) have demonstrated the potential advantages for significantly improving the tracking performance. The performance on the OTB-100 <ref type="bibr" target="#b8">[9]</ref> dataset is constantly refreshed by the tracking methods based on deep learning (such as Deep-SRDCF <ref type="bibr" target="#b30">[29]</ref>, HCFT <ref type="bibr" target="#b31">[30]</ref> and HDT <ref type="bibr" target="#b32">[31]</ref>). The MDNet <ref type="bibr" target="#b20">[20]</ref> tracker is the winner of VOT2015 competition. All the top-4 trackers in the VOT2016 competition, including C-COT <ref type="bibr" target="#b25">[24]</ref>, TCNN <ref type="bibr" target="#b27">[26]</ref>, SSAT <ref type="foot" target="#foot_0">1</ref> and MLDF <ref type="foot" target="#foot_1">2</ref> , are based on deep neural networks.</p><p>Although a variety of tracking methods based on deep learning have been presented, until now, there exists no work to make a detailed survey, comprehensive evaluation and insightful analysis on these deep trackers.</p><p>In this work, we review existing deep-learning-based tracking algorithms and evaluate them on recent benchmarks. Furthermore, we attempt to address several important issues: <ref type="bibr" target="#b0">(1)</ref>  (c) For the CNN model, the trackers using multiple convolutional layers usually perform better than those using a single convolutional layer. <ref type="bibr">(d)</ref> The end-to-end networks usually perform better than the feature extraction networks in designing a robust deep tracking method. (e) For visual tracking, the most suitable network training method is to per-train networks with video information and online fine-tune them with subsequent observations.</p><p>The rest of the paper is organized as follows. In Section 2, we briefly introduce some fundamental concepts on online visual tracking and some closely related deep learning algorithms. In Section 3, we review the existing trackers based on deep learning from three aspects: network structure, and provide some discussions and analysis on them. Finally, we summarize our paper and point out some further directions in Section 5. Here, we provide Figure <ref type="figure" target="#fig_2">1</ref> for a clear understand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background and Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Fundamental Concepts of Online Visual Tracking</head><p>Online visual tracking aims to track any object in realistic scenes. Broadly speaking, a visual tracking method consists of two main components: a motion model that describes the states of an object over time and predicts its likely state (e.g., Kalman filter <ref type="bibr" target="#b33">[32]</ref> and particle filter <ref type="bibr" target="#b34">[33,</ref><ref type="bibr" target="#b35">34]</ref>); and an observation model that depicts the appearance information of the tracked object and verifies predictions in each frame <ref type="bibr" target="#b36">[35]</ref>. Some researches have demonstrated that the observation model plays a more important role than the motion model <ref type="bibr" target="#b37">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>From the perspective of the observation model, the existing trackers usually can be categorized into either generative (e.g., <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b38">37,</ref><ref type="bibr" target="#b39">38,</ref><ref type="bibr" target="#b40">39,</ref><ref type="bibr" target="#b41">40]</ref>)</p><p>or discriminative (e.g., <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b42">41,</ref><ref type="bibr" target="#b43">42,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b44">43,</ref><ref type="bibr" target="#b46">44,</ref><ref type="bibr" target="#b47">45,</ref><ref type="bibr" target="#b48">46]</ref>) methods. Generative methods focus on searching for the regions that are the most similar to the tracked object, including template-based ( <ref type="bibr" target="#b33">[32,</ref><ref type="bibr" target="#b49">47,</ref><ref type="bibr" target="#b39">38]</ref>), subspace-based</p><formula xml:id="formula_0">([1]</formula><p>), sparse representation <ref type="bibr" target="#b50">[48,</ref><ref type="bibr" target="#b51">49]</ref>, to name a few. While discriminative trackers usually consider tracking as a classification problem that distinguishes the tracked objects from its local surrounding backgrounds. Several classic machine learning techniques have been attempted to solve the tracking problem, such as boosting <ref type="bibr" target="#b52">[50,</ref><ref type="bibr" target="#b53">51]</ref>, support vector machine <ref type="bibr" target="#b54">[52]</ref>,</p><p>naive bayes <ref type="bibr" target="#b43">[42]</ref>, random forest <ref type="bibr" target="#b55">[53]</ref>, multiple instance learning <ref type="bibr" target="#b1">[2]</ref>, metric learning <ref type="bibr" target="#b56">[54]</ref>, structured learning <ref type="bibr" target="#b6">[7]</ref>, latent variable learning <ref type="bibr" target="#b57">[55]</ref>, correlation filter <ref type="bibr" target="#b46">[44]</ref> and so on. In <ref type="bibr" target="#b37">[36]</ref> To the best of our knowledge, there exist some surveys and evaluations on traditional tracking algorithms <ref type="bibr" target="#b58">[56,</ref><ref type="bibr" target="#b59">57,</ref><ref type="bibr" target="#b60">58,</ref><ref type="bibr" target="#b36">35,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b61">59,</ref><ref type="bibr" target="#b10">11]</ref>, which provide useful insights and conclusions for visual tracking. However, until now, no work has been done to review the deep-learning-based trackers, which are all recently published (after 2013) and shown to achieve state-of-the-art</p><formula xml:id="formula_1">A C C E P T E D M A N U S C R I P T</formula><p>performance on the popular benchmarks. We note that our work is the first attempt to make a comprehensive review and exhaustive evaluation of existing deep visual trackers on large-scale benchmarks, which will facilitate the readers' understanding the benefits of deep learning for visual tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Related Deep Learning Algorithms</head><p>Deep learning has been intensively studied and demonstrated remarkable success in a wide range of computer vision areas, including image classification, object detection, semantic segmentation, image caption, pose estimation, saliency detection, edge detection, etc. This section mainly discusses recent progresses in image classification and object detection, which are highly correlated to visual tracking. Readers are referred to deep learning survey papers for more comprehensive reviews.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image classification is one of the earliest computer vision tasks where</head><p>DNNs have shown their strong learning capabilities. In <ref type="bibr" target="#b62">[60]</ref>, an 8-layer CNN model is trained in an end-to-end manner on the ImageNet classification data set <ref type="bibr" target="#b63">[61]</ref> and delivers record-breaking performance. It further indicates that the applications of dropout layers <ref type="bibr" target="#b64">[62]</ref>, Rectified Linear Units (ReLUs) <ref type="bibr" target="#b65">[63]</ref>, and data augmentation techniques can effectively facilitate easier network training and significantly reduces overfitting. Following <ref type="bibr" target="#b62">[60]</ref>, deeper networks with more sophisticated architectures <ref type="bibr" target="#b66">[64,</ref><ref type="bibr" target="#b67">65]</ref> have been designed, yielding higher classification accuracy.  <ref type="bibr" target="#b71">[69]</ref> improves the efficiency by developing the ROI-pooling layer to extract features from shared convolutional feature maps. In <ref type="bibr" target="#b72">[70]</ref>, the region proposal network is proposed and trained end-to-end with Fast R-CNN to generate high-quality region proposals. Instead of performing detection with CNN-based classifiers, <ref type="bibr" target="#b73">[71]</ref> formulates the prediction of bounding boxes and class probabilities as a regression problem with DNNs, which delivers superior performance at a significantly accelerated speed. Some recent methods also focus on addressing object detection at the instance-level <ref type="bibr" target="#b74">[72]</ref> or in video sequences <ref type="bibr" target="#b75">[73]</ref> with deep-learning-based approaches. It should be noted that object detection and visual tracking are essentially different in that object detection aims to distinguish objects of different categories, while visual tracking is designed to locate objects of interests in an class-agnostic manner. Nonetheless, they are also highly correlated. For instance, some recent visual tracking methods <ref type="bibr" target="#b76">[74]</ref> pre-train networks on object detection data sets. Others <ref type="bibr" target="#b78">[75,</ref><ref type="bibr" target="#b79">76]</ref> leverage object detection results or region proposals to facilitate more accurate online tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Deep Visual Tracking</head><p>In this work, we attempt to review and discuss the existing deep-learning-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">CNN</head><p>The CNN model <ref type="bibr" target="#b62">[60,</ref><ref type="bibr" target="#b66">64,</ref><ref type="bibr" target="#b67">65,</ref><ref type="bibr" target="#b69">67]</ref> is very suitable for developing robust and candidates for visual tracking, in which the optimal state can be determined based on the highest matching score. After that, Bertinetto et al. <ref type="bibr" target="#b24">[23]</ref> develop a fully connected siamese network to match the object template and current search region in a convolutional manner, shown as Figure <ref type="figure" target="#fig_11">4</ref>.</p><p>Besides, Chen et al. <ref type="bibr" target="#b93">[89]</ref> present a more generic approach utilizing a novel two-flow convolutional neural network to include two inputs (one is the object image patch, another is the search region patch) and produce a response map that predicts how likely the object appears in a specific location. <ref type="bibr" target="#b82">[79,</ref><ref type="bibr" target="#b98">94,</ref><ref type="bibr" target="#b105">100]</ref> also use two-path network to evaluate the object location.</p><p>Recently, some trackers apply the satisfactory performance of correlation filter to deep neural network. Valmadre et al. <ref type="bibr" target="#b92">[88]</ref> interpret correlation filter as a differentiable layer in a siamese network. A closed-form solution via the fourier domain ensures the network being trained end to end. Similar idea can also be found in <ref type="bibr" target="#b97">[93]</ref>.</p><formula xml:id="formula_2">A C C E P T E D M A N U S C R I P T 3.1.2. RNN</formula><p>The recurrent neural network (RNN) model is suitable for sequence modeling since its neuron's output can be directly applied to itself in the next time. Motivated by some research works on handwriting recognition <ref type="bibr" target="#b108">[103,</ref><ref type="bibr" target="#b109">104]</ref> or speech recognition <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b16">16]</ref>, some attempts have been done Zhuang et al. <ref type="bibr" target="#b96">[92]</ref> exploit the autoencoder network to construct deep feature representations and utilize the shallow subspace model to recovery partial occlusions or other unexpected noises. This method effectively combines both shallow and deep models and achieves better performance than the traditional DLT algorithm <ref type="bibr" target="#b83">[80]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network Function</head><p>For visual tracking, deep networks can be not only used for extracting effective features but also adopted for evaluating the candidates of the tracked object. From this perspective, the tracking algorithms based on deep learning can be roughly classified into two categories (see Figure <ref type="figure" target="#fig_4">2</ref>): </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Feature Extraction Network (FEN)</head><p>Motivated by the success of deep features on image classification, many researchers attempt to exploit deep networks for feature extraction in designing a tracking method. This kind of algorithms can be mainly divided into two aspects: FEN-SL (using deep features from a single layer); and FEN-ML (using deep features from multiple layers).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FEN-SL:</head><p>The DLT <ref type="bibr" target="#b83">[80]</ref>, Trans-DLT <ref type="bibr" target="#b86">[83]</ref> and CNN-SVM <ref type="bibr" target="#b100">[95]</ref> methods design deep networks for feature extraction and adopt the outputs of the networks as object features. The CNT <ref type="bibr" target="#b85">[82]</ref> method propagates an input image forward in a CNN network to extract weak features, and then learns a classifier for distinguishing these features into positive and negative ones. The DeepSRDCF <ref type="bibr" target="#b30">[29]</ref> method extracts features from the first layers of the VGG network, and combines deep features with the SRDCF framework to improve the tracking performance. The RPNT <ref type="bibr" target="#b79">[76]</ref> method utilizes a network similar with Faster R-CNN to draw proposals and extracts the image features using the outputs of last convolutional layers of Faster R-CNN. The RTT <ref type="bibr" target="#b80">[77]</ref> method applies RNN to get a saliency map of the search region, which helps the correlation filter to reduce the interference of background.  abstraction in the feature hierarchies. Features in earlier layers retain higher spatial resolution for precise localization with low-level visual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FEN-ML:</head><p>While features in latter layers capture more semantic information and less fine-grained spatial details. Thus, they extract features from three different layers and use a fix weight to combine the feature maps generated by those layers, shown as Figure <ref type="figure" target="#fig_5">3</ref>. The results have demonstrated that the features extracted from multiple layers perform better than the features from a single layer for developing a robust tracking method. After that, Ma et al. <ref type="bibr" target="#b102">[97]</ref> use a designed network to replace the Conv3-3 layer in <ref type="bibr" target="#b31">[30]</ref> and therefore improve the tracking performance. Qi et al. <ref type="bibr" target="#b32">[31]</ref> extract features from six convolutional layers and combine these layers using an adaptive weight scheme. The RTT <ref type="bibr" target="#b80">[77]</ref> method uses a combination of four output maps of the deep network trained from four different directions. In addition, the C-COT <ref type="bibr" target="#b25">[24]</ref> algorithm proposes a joint learning framework to fuse deep features from different spatial pyramids. Based on C-COT <ref type="bibr" target="#b25">[24]</ref>, Martin et al. <ref type="bibr" target="#b90">[86]</ref> aim to combine hand-crafted features with deep features and reduce</p><formula xml:id="formula_3">A C C E P T E D M A N U S C R I P T</formula><p>the algorithm redundancy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">End to End Network (EEN)</head><p>In contrast with the FEN-based methods, the EEN-based trackers train a network to conduct both feature extraction and candidate evaluation.</p><p>According to the differences of network outputs, we can roughly divide the EEN-based methods into three categories: EEN-S, EEN-M and EEN-B.</p><p>Their outputs are object score (S), confidence map (M) and bounding box (B), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EEN-S:</head><p>This kind of methods ( <ref type="bibr" target="#b83">[80,</ref><ref type="bibr" target="#b95">91,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b29">28,</ref><ref type="bibr" target="#b27">26,</ref><ref type="bibr" target="#b78">75,</ref><ref type="bibr" target="#b22">22]</ref>) generates a series of candidates using particle filter or sliding window schemes, and then produces the scores of these candidates for localizing the tracked object. The DeepTrack <ref type="bibr" target="#b95">[91]</ref> method samples a set of candidates with a sliding window, and evaluates the likelihoods of these candidates using a CNN model designed by multiple convolutional layers and a joint learned fully-connected layer. In <ref type="bibr" target="#b22">[22]</ref>, the SINT method generates a lot of particles and calculates their similarity scores using the siamese network. The optimal state can be determined by the particle with the highest score. In addition to outputting score, the ADNet <ref type="bibr" target="#b28">[27]</ref> uses a network to output an action by reinforcement learning, which will guide the bounding box to object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EEN-M:</head><p>This kind of trackers usually exploits deep networks to generate a confidence map (or probability map, response map, heat map) and then uses other methods to localize the tracked object. In <ref type="bibr" target="#b19">[19]</ref>, two deep networks are designed with the Conv4-3 and Conv5-3 layers of the VGG-16 model and then used to calculate the response maps. In <ref type="bibr" target="#b21">[21]</ref>, the last convolutional layers are jointly combined to produce the final heat map. The</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T SO-DLT <ref type="bibr" target="#b76">[74]</ref> method first pre-trains a CNN model to recognize what is an object and then generates a probability map instead of producing a simple class label. The SiameFC <ref type="bibr" target="#b24">[23]</ref> tracker utilizes a pre-trained fully convolutional siamese network, the inputs of which are the object template and current search region (Figure <ref type="figure" target="#fig_11">4</ref>). In each frame, this method generates a response map regarding the tracked object with convolution operation. The similar structure is also adopted in the YCNN <ref type="bibr" target="#b93">[89]</ref> method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EEN-B:</head><p>This kind of algorithms learns an end-to-end network to directly produce the bounding box (or position) of the tracked object in each frame (such as <ref type="bibr" target="#b26">[25,</ref><ref type="bibr" target="#b81">78,</ref><ref type="bibr" target="#b78">75,</ref><ref type="bibr" target="#b82">79]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Network Training</head><p>The network training is also a critical issue for developing a robust deep-learning-based tracker, which may be used to transfer visual prior, online learning, or both. According to the manners of pre-training and online learning, we can divide the existing deep visual trackers into five categories: NP-OL, IP-NOL, IP-OL, VP-NOL and VP-OL (shown in Figure <ref type="figure" target="#fig_4">2</ref>).</p><p>It should be noted that the 'OL' or 'NOL' here is only for the network rather than the whole tracker. The detailed explanations are presented in Table <ref type="table" target="#tab_2">1</ref>.  (the similar idea is presented in <ref type="bibr" target="#b21">[21]</ref>). In <ref type="bibr" target="#b27">[26]</ref>, Nam et al. effectively combine pre-trained convolutional layers and multiple fully-connected layers with a tree structure to achieve good tracking performance. The other algorithms offline train a CNN structure and online fine-tune it to meet the requirement of the specific tracking task, such as SO-DLT <ref type="bibr" target="#b76">[74]</ref>, DST <ref type="bibr" target="#b84">[81]</ref>,</p><p>CNN-Tracker <ref type="bibr" target="#b103">[98]</ref>, Trans-DLT <ref type="bibr" target="#b86">[83]</ref>, DNT <ref type="bibr" target="#b104">[99]</ref> and DRT <ref type="bibr" target="#b105">[100]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VP-NOL:</head><p>In essence, visual tracking is a sequential inference problem regarding video data. Thus, pre-training deep models with video information may facilitate dealing with the tracking problem. There have been some attempts to train deep networks with tracking videos. The SINT <ref type="bibr" target="#b22">[22]</ref> method exploits a siamense network offline trained by tracking videos to Then, the network is updated online. Different from <ref type="bibr" target="#b28">[27]</ref>, Janghoon et al. <ref type="bibr" target="#b98">[94]</ref> propose an agent for template choosing. The author applies policybased method to train the model, making it be able to choose the best one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment and Analysis</head><p>In this section, we conduct a comprehensive experimental evaluation on the recent OTB-100 <ref type="bibr" target="#b8">[9]</ref>, TC-128 <ref type="bibr" target="#b9">[10]</ref> and VOT2015 <ref type="bibr" target="#b10">[11]</ref> datasets, which explains the benefits of exploiting deep learning in visual tracking. In the following subsections, we first introduce the adopted benchmarks and the tracking methods to be compared, and then report the experimental comparisons on OTB-100 <ref type="bibr" target="#b8">[9]</ref>, TC-128 <ref type="bibr" target="#b9">[10]</ref> and VOT2015 <ref type="bibr" target="#b10">[11]</ref>. Finally, we conduct the detailed discussions on our experiments, and provide the useful insights and conclusions. It should be mentioned that all trackers are tested on PC with a 3.4GHz CPU and a GTX 1080 GPU with 8G memory. All compared trackers are implemented with matlab for fair speed comparison. The project of this paper can be found in http://ice.dlut.edu. cn/lu/publications.html.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation Benchmark</head><p>This subsection introduces the adopted dataset (OTB-100 <ref type="bibr" target="#b8">[9]</ref>, TC-128 <ref type="bibr" target="#b9">[10]</ref>,</p><p>VOT2015 <ref type="bibr" target="#b10">[11]</ref>) and deep-learning-based trackers to be evaluated in our  The DP value denotes the percentage that the centre location error (i.e., the Euclidean distance between the center of the tracked target and that of the ground truth) is smaller than a certain threshold in the sequence. The OS value is calculated by the ratio of successfully tracked frames. Usually, the overlap score between the tracking bounding box R T and the ground truth R G is larger than a pre-defined threshold (such as 0.5), the target is regarded as being tracked successfully. Figure <ref type="figure" target="#fig_14">6</ref> illustrates an overall comparison of all 16 deep trackers and 6 traditional top-ranked trackers; while Table <ref type="table" target="#tab_5">3</ref> and Table <ref type="table" target="#tab_6">4</ref> report the attribute-based performance of these trackers for highlighting the trackers' abilities in handling different challenges.</p><formula xml:id="formula_4">A C C E P T E D M A N U S C R I P T</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TC-128:</head><p>The TC-128 <ref type="bibr" target="#b9">[10]</ref>   <ref type="bibr" target="#b10">[11]</ref> and OTB-100 <ref type="bibr" target="#b8">[9]</ref> is that the VOT2015 challenge provides a re-initialization protocol (i.e., trackers are reset with ground-truths in the middle of evaluation if tracking failures are observed).</p><p>In this paper, we evaluate 15 deep-learning-based trackers and 7 baseline ones in terms of both accuracy and robustness (AR) rank and expected average overlap (EAO) metrics. It should be noted that the 15 trackers here are the same as trackers tested on OTB-100 and TC-128 except for MCPF <ref type="bibr" target="#b89">[85]</ref>, the code of which is packaged and cannot be changed. The AR rank is created by ranking the trackers over each sequence and averaging the rank lists according to the quantized accuracy and robustness. It does convert the accuracy and robustness to equal scales, thus, the averaged rank cannot be interpreted as a concrete tracking application result.  To address this issue, the EAO <ref type="bibr" target="#b10">[11]</ref> rule is also introduced to rank different Tracking Algorithm: In our experiment, we collect 16 deep visual trackers, the source codes or benchmark results of which have been publicly available already. These methods include ECO <ref type="bibr" target="#b90">[86]</ref>, CFNet <ref type="bibr" target="#b92">[88]</ref>, MCPF <ref type="bibr" target="#b89">[85]</ref>,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>DNT <ref type="bibr" target="#b104">[99]</ref>, DCFNet <ref type="bibr" target="#b97">[93]</ref>, MDNet <ref type="bibr" target="#b20">[20]</ref>, SANet <ref type="bibr" target="#b29">[28]</ref>, TCNN <ref type="bibr" target="#b27">[26]</ref>, C-COT <ref type="bibr" target="#b25">[24]</ref>,</p><p>STCT <ref type="bibr" target="#b21">[21]</ref>, FCNT <ref type="bibr" target="#b19">[19]</ref>, HCFT <ref type="bibr" target="#b31">[30]</ref>, HDT <ref type="bibr" target="#b32">[31]</ref>, SiameFC <ref type="bibr" target="#b24">[23]</ref>, CNT <ref type="bibr" target="#b85">[82]</ref> and DLT <ref type="bibr" target="#b83">[80]</ref> (the detailed information can be found in Table <ref type="table" target="#tab_4">2</ref>) <ref type="foot" target="#foot_2">3</ref> . In addition, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Discussions</head><p>Analysis of Network Structure: Different network structures have different characteristics. Before exploiting the popular CNN methods, some trackers have attempted to use an autoencoder or a simply designed network for constructing robust appearance models (such as DLT <ref type="bibr" target="#b83">[80]</ref> and CNT <ref type="bibr" target="#b85">[82]</ref>). However, these trackers just achieved comparable results with top-ranked traditional tracking methods (see Figure <ref type="figure" target="#fig_14">6</ref> and Figure <ref type="figure" target="#fig_18">9</ref>, which have not demonstrated the benefits of deep learning for visual tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Most existing deep trackers use deep convolutional neural networks (CNN)</head><p>to develop appearance models. Some of them use CNN to distinguish the object from the background (i.e., CNN-C), while others use CNN to match candidates with object model (i.e., CNN-M). Distinguishing positive sam-     </p><formula xml:id="formula_5">A C C E P T E D M A N U S C R I P T</formula><p>ples with negative ones is easier than matching two feature patches, so CNN-C trackers perform more accurate than CNN-M trackers (see Figure <ref type="figure" target="#fig_14">6</ref> and Figure <ref type="figure" target="#fig_18">9</ref>). While CNN-M trackers are usually faster (see Figure <ref type="figure" target="#fig_25">11</ref>  <ref type="table" target="#tab_5">3</ref>, Table <ref type="table" target="#tab_6">4</ref>, Table <ref type="table" target="#tab_7">5</ref>). This is mainly be- ROLO <ref type="bibr" target="#b78">[75]</ref> and RNNT <ref type="bibr" target="#b81">[78]</ref>. But their performance is not satisfactory and requires to be improved. Overall, for visual tracking, the usage of the RNN model is far from being effectively exploited and will be a future research direction. single layer (i.e., FEN-SL) while others use a combination of both lower and deeper convolutional layers (i.e., FEN-ML). Based on our empirical observations, trackers using features from multiple layers (e.g., HCFT <ref type="bibr" target="#b31">[30]</ref> and HDT <ref type="bibr" target="#b32">[31]</ref>) usually perform better than those using features from a single layer (such as DLT <ref type="bibr" target="#b83">[80]</ref>). Besides, we can also conclude the same result from self-comparison of HCFT <ref type="bibr" target="#b31">[30]</ref> (see Figure <ref type="figure" target="#fig_23">10</ref>). The second kind of deep trackers exploit deep networks to produce probability score (i.e., EEN-S), map (i.e., EEN-M) or even bounding box (i.e., EEN-B), according to which the tracked object can be easily located in the search region. Most EEN-S trackers (e.g., MDNet <ref type="bibr" target="#b20">[20]</ref>, SANet <ref type="bibr" target="#b29">[28]</ref> and TCNN <ref type="bibr" target="#b27">[26]</ref>) are designed</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T based on the particle filter framework. They have achieved amazing performance as shown in Table <ref type="table" target="#tab_5">3</ref>, however, it is time-consuming to pass every particle through the whole network. To address this issue, some EEN-M algorithms (such as STCT <ref type="bibr" target="#b21">[21]</ref>, FCNT <ref type="bibr" target="#b19">[19]</ref> and SO-DLT <ref type="bibr" target="#b76">[74]</ref>) adopt the search region as the input and the probability map as the output, which are able to exploit the convolution operation to speed up the trackers. In general, the EEN trackers perform better than the FEN ones, which demonstrates that deep networks are more useful than traditional ones to be a classifier or matcher.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Further Work</head><p>In </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>•</head><label></label><figDesc>The first comprehensive survey on deep-learning-based trackers • Review existing deep visual trackers from three different perspectives • Large-scale benchmark evaluations of deep visual trackers • Summarize cutting-edge research works and discuss future directions • Provide useful insights and conclusions for deep visual trackers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>What is the connection and difference between these deep-learning-based trackers? We present three classification rules to review existing deep visual trackers according to the structures of network, the functions of network and network training methods; (2) Why is deep learning suitable for visual tracking? In this paper, we conduct a comprehensive evaluation on existing open-source deep-learning-based A C C E P T E D M A N U S C R I P T trackers, and further explain the reason why deep feature is more robust than hand-crafted feature and why interaction between different parts of the network could improve tracking performance; (3) How to better leverage deep networks for visual tracking and what is the future direction of the development on deep visual tracking? By evaluating existing deep trackers and some top-ranked baseline methods, we make some useful conclusions and point out the possible research directions. In order to construct a large-scale and fair comparison, we test 16 deep trackers and 6 baseline methods on the popular OTB-100, TC-128 and VOT2015 benchmarks. The comparison results indicate that: (a) The trackers using the convolutional neural network (CNN) model to distinguish the tracked object from its local background could get more accurate results, while using CNN for template matching is usually faster. (b) The trackers with deep features usually perform much better than those with low-level hand-crafted features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The structure of this paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>,</head><label></label><figDesc>Wang et al.  demonstrate that feature extraction is also a very important issue in designing a robust tracker (such as the tracker with HOG features performs much better than that with Haar-like features). From the research experiences of the traditional tracking algorithms, we can conclude that any progress of feature extraction methods or machine learning techniques may facilitate developing the tracking method. Thus, we believe that the usage of deep learning could improve the tracking performance since the deep learning techniques have been shown powerful abilities on feature extraction and object classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Structure of three classification methods and algorithms in each category.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The structure of HCFT [30].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>3. 1 .</head><label>1</label><figDesc>Network Structure Networks with different structures focus on solving different tasks. The convolutional neural network (CNN) has been demonstrated to be effective for feature extraction and achieved great success on image classification. While the recurrent neural network (RNN) is able to remember previous states and establish temporal connection, which is suitable for sequence modeling. For visual tracking, from the perspective of network structure, we can roughly category the exiting trackers into three classes: CNN-based trackers, RNN-based trackers, and trackers based on other networks (Figure 2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>appearance model in the tracking task, due to its powerful ability on feature extraction and image classification. Similar with the traditional track-A C C E P T E D M A N U S C R I P T ers, the CNN-based tracking methods can also be either discriminative or generative. The discriminative method aims to conduct a binary classification with the CNN model for effectively distinguishing the tracked object from its surrounding backgrounds. The generative one focuses on learning a robust similarity function to accurately match the object template within a given search region. We note that the former one as CNN-C for highlighting the classification task and the later one as CNN-M for highlighting the matching process.CNN-C:To develop a CNN-C-based tracker, a convenient way is to replace hand-crafted features with deep features extracted from the CNN model in the traditional tracking framework, such as correlation filter, online SVM and so on. For the VGGNet<ref type="bibr" target="#b66">[64]</ref> model, Ma et al.<ref type="bibr" target="#b31">[30]</ref> find that the outputs of the last convolutional layer encode the semantic information and such representations are robust to significant appearance variations. However, their spatial resolution is too coarse to precisely localize the tracked objects. In contrast, earlier convolutional layers provide more precise localization but are less invariant to appearance changes. Thus, they construct three correlation-filter-based classifiers with three different convolutional layers (Conv3-4, Conv4-4 and Conv5-4), and combine their corresponding response maps to identify the tracked object. The network structure is shown as Figure3. The DeepSRDCF<ref type="bibr" target="#b30">[29]</ref> method combines activations from the convolutional layer of a CNN model with the SRDCF<ref type="bibr" target="#b106">[101]</ref> framework. Different with image classification, the results of the algorithm suggest that activations from the first layer could provide superior tracking performance compared to the deeper layers and the convolutional features provide improved results compared standard hand-crafted features. SomeA C C E P T E D MA N U S C R I P T similar ideas have been presented in<ref type="bibr" target="#b32">[31,</ref><ref type="bibr" target="#b102">97,</ref><ref type="bibr" target="#b25">24,</ref><ref type="bibr" target="#b90">86,</ref><ref type="bibr" target="#b28">27]</ref>. In<ref type="bibr" target="#b79">[76]</ref>,Zhu et al.    use the CNN model similar with Faster R-CNN to generate object proposals, and then the proposals are put into an online Sturctured SVM<ref type="bibr" target="#b107">[102]</ref> to obtain the object state. In<ref type="bibr" target="#b100">[95]</ref>, Hong et al. adopt the CNN model to produce discriminative saliency maps, and combine them with online SVM to learn a robust appearance model. In addition, the deep networks can be not only used for extracting visual features but also for conducting classification (such as<ref type="bibr" target="#b76">[74,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b104">99]</ref>).CNN-M:Different from the CNN-C-based trackers, the CNN-M ones use convolutional neural networks to learn effective matching functions. In [22], Tao et al. propose a siamese network model to match the object template</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>to exploit semantic information among spatial configurations and temporal association among frames in visual tracking. Cui et al.<ref type="bibr" target="#b80">[77]</ref> propose a RNN-based method to produce a confidence map and apply it to correlation filter. The RNN model is trained from four different directions, which makes the appearance model robust to partial occlusion. Fan et al.<ref type="bibr" target="#b29">[28]</ref> adopt a RNN model similar with the RTT<ref type="bibr" target="#b80">[77]</ref> method, in which the features produced by RNN are added into a CNN network to get a robust feature representation. The above-mentioned two papers mainly focus on establishing spatial relationship among different image parts using the RNN model. Besides, there exist some attempts to construct temporal correlation with RNN. Ning et al.<ref type="bibr" target="#b78">[75]</ref> investigate the regression capability of long short-term memory (LSTM) in the temporal domain, and propose to concatenate high-level visual features produced by convolutional networks with region information. Gan et al.<ref type="bibr" target="#b81">[78]</ref> feed the feature vector of the input frame into a recurrent neural network. The recurrent neural network effectively updates its internal memory vector based on the previous memory vector, previous location of an object and the current frame. Daniel et al.<ref type="bibr" target="#b82">[79]</ref> use a two-path network followed by two LSTM blocks to remember the object appearance and motion.A C C E P T E D M A N U S C R I P T3.1.3. OthersIn addition to the CNN-and RNN-based trackers, some researchers have attempted to develop robust tracking algorithms using other deep networks (especially the autoencoder network<ref type="bibr" target="#b110">[105,</ref><ref type="bibr" target="#b111">106,</ref><ref type="bibr" target="#b112">107]</ref>). Wang et al.<ref type="bibr" target="#b83">[80]</ref> propose the first deep-learning-based tracker that trains a stacked denoising autoencoder offline to learn generic image features. These features have been demonstrated to be robust against appearance variations.In<ref type="bibr" target="#b84">[81]</ref>, the proposed tracker learns complex-valued invariant representations from tracked sequential image patches via strong temporal slowness constraint and stacked convolutional autoencoders. The deep slow local representations are learned offline on unlabeled data and transferred to the observational model of their proposed tracker. Motivated by<ref type="bibr" target="#b83">[80]</ref>,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>( 1 )</head><label>1</label><figDesc>feature extraction network (FEN), which merely uses deep networks to extract deep features, and then adopts the traditional method to learn the appearance model and locate the target; (2) end to end network (EEN), which not only uses deep networks for feature extraction but also for candi-A C C E P T E D M A N U S C R I P T date evaluation. The outputs of the EEN methods can be in terms of probability map, heat map, candidate's score, object position or even bounding box directly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Different layers of a deep network could provide multi-level feature description. Thus, it is reasonable to exploit multiple layers for feature extraction in developing a robust tracker. Ma et al. [30] observe that different convolutional layers of a typical CNN model provide multiple levels of A C C E P T E D M A N U S C R I P T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The structure of SiameFC [23].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The structure of MDNet [20].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>S C R I P T learn a matching function, and integrates this matching function with the particle filter method to construct the entire tracking framework. After that, Bertinetto et al.<ref type="bibr" target="#b24">[23]</ref> use tracking videos to train a fully convolutional siamese network, which localizes the accurate positions of the tracked object under the convolution framework. Similarly, Held et al. [25] present a image-comparison tracking framework based on two-frame architectures with traditional convolutional layers and fully connected layers trained in an offline manner. The traditional convolutional layers aim to extract robust deep feature representations, and the fully connected layers focus on learning a complex feature comparison between the object template and the candidate samples. This method achieves very fast performance in tracking generic objects. VP-OL: Besides the VP-NOL-based trackers, there exist some attempts to combine video pre-training and online learning for developing an effective tracking method. By using a large set of tracking videos with ground truths, the MDNet [20] method pre-trains a CNN model with shared layers and multiple branches of domain-specific layers to obtain a generic object representation, shown as Figure 5. Domain-specific layers correspond to individual training sequences, where each branch is responsible for conducting a binary classification to identify the tracked object in each domain. Then, each domain is trained in the network iteratively to obtain a generic object representation in the shared layers. During the tracking process, the MDNet method constructs a new network to combine the pretrained shared layers and online updates this network to capture the appearance change of the tracked object. The SANet [28] method exploits the same idea with the MDNet tracker, and introduces an additional RNN-A C C E P T E D M A N U S C R I P T based structure to further enhance object representation. Recently, deep reinforcement learning have drawn more attention in visual tracking, which is suitable when we are lack of training labels or have delayed labels. Yun et al. [27] attempt to learn an agent which could evaluate the moving direction of bounding box through unsupervised learning in a frame. The part of whole model is pre-trained offline with videos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Distance precision plots (DP) and the area under curve (AUC) over OTB-100 benchmark sequences on 22 tested trackers using one-pass evaluation (OPE).</figDesc><graphic coords="25,126.39,194.30,156.98,137.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 7 :OTB- 100 :</head><label>7100</label><figDesc>Figure 7: Distance precision plots (DP) and the area under curve (AUC) over TC-128 benchmark sequences on 22 tested trackers using one-pass evaluation (OPE).</figDesc><graphic coords="25,126.39,460.50,156.98,137.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>dataset is presented by Liang et al. and focus on color information. This benchmark contains 128 color sequences with A C C E P T E D M A N U S C R I P T ground truth and challenge factor annotations, including Illumination Variation (IV), Scale Variation (SV), Occlusion (OCC), Deformation (DEF), Motion Blur (MB), Fast Motion (FM), In-Plane Rotation (IPR), Out-of-Plane Rotation (OPR), Out-of-View (OV), Background Clutters (BC), and Low Resolution (LR). The 11 challenge factors as well as the evaluation metrics here are the same with the OTB-100 [9] dataset. Figure 7 illustrates an overall comparison of all 16 deep trackers and 6 traditional top-ranked trackers; while Figure 8 reports the attribute-based performance of these trackers for highlighting the trackers' abilities in handling different challenges. Besides, we test the speeds of the 22 trackers on this benchmark and report the comparison results in Figure 11. VOT2015: The VOT2015 [11] dataset consists of 60 short sequences annotated with 6 different attributes including occlusion, illumination change, motion change, size change, camera motion and unassigned. The major difference between VOT2015</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Distance precision plots (DP) in different challenges over TC-128 benchmark sequences on 22 tested trackers using one-pass evaluation (OPE).</figDesc><graphic coords="28,175.11,512.41,99.26,110.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Accuracy-robust rank (AR) and expected accuracy overlap (EAO) over VOT2015.</figDesc><graphic coords="29,125.48,156.07,148.89,137.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>tracking algorithms. It estimates how accurate the estimated bounding box is after a certain number of frames are processed since initialization. Figure 9 reports both accuracy-robust rank (AR) and expected accuracy overlap (EAO) plots of different trackers and Table 5 provides accuracy scores and failure times of different trackers for each individual attribute on the VOT2015 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>we select 6</head><label>6</label><figDesc>baseline trackers including MEEM<ref type="bibr" target="#b44">[43]</ref> 4 , KCF<ref type="bibr" target="#b46">[44]</ref> 5 , TGPR<ref type="bibr" target="#b57">[55]</ref> 6 , SCM<ref type="bibr" target="#b4">[5]</ref> 7 , ASLA<ref type="bibr" target="#b5">[6]</ref> 8 , and SRDCF<ref type="bibr" target="#b106">[101]</ref> 9 . These methods have achieved top performance on the above-mentioned three benchmarks within the tracking methods with low-level hand-crafted features. In our experiments, we run the source codes (with same parameters) or use tracking results provided by the original authors to conduct experimental comparisons. Specially, all trackers are re-tested on TC-128<ref type="bibr" target="#b9">[10]</ref> for fair speed comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head></head><label></label><figDesc>), because most trackers use the siamese network to model prior information instead of online fine-tuning. Compared with traditional trackers, the CNN-based deep tracking algorithms perform significantly better than the traditional methods in terms of both overall performance (Figure 6 and Figure 9)and different challenges (Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head></head><label></label><figDesc>cause deep features have stronger representation ability than hand-crafted features. Hand-crafted feature can also be seen as the shallow features of deep network. They include less semantic information compared with deep features. Besides, end-to-end training makes the classifier or matcher fit well with deep features. The usage of the CNN model has achieved satisfactory performance in visual tracking, however, it cannot model sequential information. The RNN model can depict the temporal or spatial relationships among members. In addition, there are also some attempts to use for modeling temporal relationships among continuous frames, such as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Distance precision plots (DP) over OTB-100 benchmark sequences on HCFT &amp; KCF (left) and DeepSRDCF &amp; SRDCF (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head></head><label></label><figDesc>Training: For deep-learning-based trackers, the role of network training is to transferring visual prior via pre-training, capturing the appearance change (during the tracking process) via online learning, or both them. The deep visual tracker with image-based pre-training is able to effectively transfer visual prior within natural images and capture much powerful feature descriptions. These methods are usually integrated with either online updating traditional models (i.e., IP-NOL) or online fine-tuning deep networks (i.e., IP-OL) to depict the visual variations of the tracked object during the tracking processing. It can be seen from our experiment evaluation that these methods (e.g., TCNN, ECO, C-COT, STCT, FCNT, HCFT, HDT) achieve significant improvements in comparison with traditional trackers based on hand-crafted features. In addition, the trackers merely with video-based pre-training (such as SINT and SiameFC) achieve satisfactory performance. The deep models of these trackers are pre-trained before tracking and are not updated during the tracking process. Their most important advantage is that it does not require additional computation and storage costs. Thus, further improving A C C E P T E D M A N U S C R I P T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Speed comparison over TC-128 benchmark sequences.</figDesc><graphic coords="38,110.27,156.07,348.87,206.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head></head><label></label><figDesc>this work, we review the recently proposed visual trackers based on deep learning and conduct extensive experiments to evaluate the existing deep-learning-based tracking methods in comparison with some traditional baseline algorithms. The main contributions of this work are threefolds. First, we review the existing deep visual trackers in three aspects including network structure, network function and network training, and then discuss these trackers from each perspective. Second, we conduct extensive experiments to compare the representative methods on the popular OTB-100, TC-128 and VOT2015 benchmarks. This large-scale evalu-A C C E P T E D M A N U S C R I P T ation facilitates the readers' understanding the benefits of deep learning for visual tracking (especially in comparison with traditional trackers with hand-crafted features). Third, we analyze the results obtained by different deep trackers and obtain the following useful insights and conclusions, which will facilitate the researchers' designing their own trackers based on deep learning. Although deep learning has been used in visual tracking and achieved promising improvements compared with traditional trackers, there also exist many topics to be investigated. First, the deep features have much redundancy which limits both speed and accuracy improvement. It will be a promising direction to reduce the redundancy in deep visual tracking. Second, most trackers use VGG network. Developing more effective network structures should be noticed. Third, the lack of training data need more focus on unsupervised or weakly supervised learning. Reinforcement learning or exploiting generative adversarial networks to generate more training samples will improve the tracking performance. Besides, the transfer ability of model is pretty important in visual tracking.We may see some new directions such as one-shot learning appearing in tracking. In conclusion, improving tracking efficiency and solving the lack of training data will be new directions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Abbreviations in Network Training.</figDesc><table><row><cell>Abbreviation</cell><cell>Full Name</cell></row><row><cell>NP</cell><cell>No Pre-trained</cell></row><row><cell>IP</cell><cell>Image Pre-trained</cell></row><row><cell>VP</cell><cell>Video Pre-trained</cell></row><row><cell>OL</cell><cell>Online Learning</cell></row><row><cell>NOL</cell><cell>No Online Learning</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>4 http://cs-people.bu.edu/jmzhang/MEEM/MEEM.html 5 http://www.robots.ox.ac.uk/ ˜joao/circulant/index.html 6 http://www.dabi.temple.edu/ ˜hbling/code/TGPR.htm 7 http://ice.dlut.edu.cn/lu/Project/cvpr12_scm/cvpr12_scm.htm</figDesc><table><row><cell>M A N U S C R I P T</cell></row><row><cell>8 http://ice.dlut.edu.cn/lu/Project/cvpr12_jia_project/cvpr12_ jia_project.htm 9 https://www.cvl.isy.liu.se/research/objrec/visualtracking/ A C C E P T E D</cell></row><row><cell>regvistrack/</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Detailed information of deep visual trackers evaluated in our paper. Abb -Ab-</figDesc><table><row><cell cols="7">breviation, NS -Network Structure, NF -Network Function, C -Code (M -Matlab, m -</cell></row><row><cell cols="2">Matconvnet, c -Caffe).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Abb</cell><cell>Full Name</cell><cell>NS</cell><cell>NF</cell><cell>NT</cell><cell>C</cell><cell>Resource Link</cell></row><row><cell>ECO[86]</cell><cell>ECO: Efficient Convolution Operators for Tracking</cell><cell>CNN-C</cell><cell>FEN-ML</cell><cell>IP-NOL</cell><cell>M+m</cell><cell>http://www.cvl.isy.liu.se/ research/objrec/visualtracking/ ecotrack/index.html</cell></row><row><cell>CFNet[88]</cell><cell>End-to-end representation learning for Correlation Filter based tracking</cell><cell>CNN-M</cell><cell>EEN-M</cell><cell>VP-NOL</cell><cell>M+m</cell><cell>http://www.robots.ox.ac.uk/ ˜luca/cfnet.html</cell></row><row><cell>MCPF[85]</cell><cell>Multi-task Correlation Particle Filter for Robust Visual Tracking</cell><cell>CNN-C</cell><cell>FEN-ML</cell><cell>IP-NOL</cell><cell>M+m</cell><cell>http://nlpr-web.ia.ac.cn/mmc/ homepage/tzzhang/mcpf.html</cell></row><row><cell>DNT[99]</cell><cell>Dual Deep Network for Visual Tracking</cell><cell>CNN-C</cell><cell>EEN-M</cell><cell>IP-OL</cell><cell>M+c</cell><cell>http://ice.dlut.edu.cn/lu/ publications.html</cell></row><row><cell>DCFNet[93]</cell><cell>DCFNet: Discriminant Correlation Filters Network for Visual Tracking</cell><cell>CNN-M</cell><cell>EEN-M</cell><cell>VP-OL</cell><cell>M+m</cell><cell>https: //github.com/foolwood/DCFNet</cell></row><row><cell></cell><cell>Learning Multi-Domain</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MDNet[20]</cell><cell>Convolutional Neural Networks for Visual</cell><cell>CNN-C</cell><cell>EEN-S</cell><cell>VP-OL</cell><cell>M+m</cell><cell>https: //github.com/HyeonseobNam/MDNet</cell></row><row><cell></cell><cell>Tracking</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>STCT: Sequentially Training</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>STCT[21]</cell><cell>Convolutional Networks for</cell><cell>CNN-C</cell><cell>EEN-M</cell><cell>IP-OL</cell><cell>M+c</cell><cell>https://github.com/scott89/STCT</cell></row><row><cell></cell><cell>Visual Tracking</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FCNT[19]</cell><cell>Visual Tracking with Fully Convolutional Networks</cell><cell>CNN-C</cell><cell>EEN-M</cell><cell>IP-OL</cell><cell>M+c</cell><cell>http://scott89.github.io/FCNT/</cell></row><row><cell>HCFT[30]</cell><cell>Hierarchical Convolutional Features for Visual Tracking</cell><cell>CNN-C</cell><cell>FEN-ML</cell><cell>IP-NOL</cell><cell>M+m</cell><cell>https://sites.google.com/site/ jbhuang0604/publications/cf2</cell></row><row><cell>HDT[31]</cell><cell>Hedged Deep Tracking</cell><cell>CNN-C</cell><cell>FEN-ML</cell><cell>IP-NOL</cell><cell>M+m</cell><cell>https://sites.google.com/site/ yuankiqi/hdt/</cell></row><row><cell>SiameFC[23]</cell><cell>Fully-Convolutional Siamese Networks for Object Tracking</cell><cell>CNN-M</cell><cell>EEN-M</cell><cell>VP-NOL</cell><cell>M+m</cell><cell>http://www.robots.ox.ac.uk/ ˜luca/siamese-fc.html</cell></row><row><cell>C-COT[24]</cell><cell>Beyond Correlation Filters: Learning Continuous Convolution Operators for Visual Tracking</cell><cell>CNN-C</cell><cell>FEN-ML</cell><cell>IP-NOL</cell><cell>M+m</cell><cell>http://www.cvl.isy.liu.se/ research/objrec/visualtracking/ conttrack/index.html</cell></row><row><cell></cell><cell>Robust Visual Tracking via</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CNT[82]</cell><cell>Convolutional Networks</cell><cell>OTHERS</cell><cell>FEN-SL</cell><cell>NP-OL</cell><cell>M</cell><cell>http://kaihuazhang.net/</cell></row><row><cell></cell><cell>Without Training</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Learning A Deep Compact</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DLT[80]</cell><cell>Image Representation for</cell><cell>OTHERS</cell><cell>FEN-SL</cell><cell>IP-NOL</cell><cell>M</cell><cell>http://winsty.net/dlt.html</cell></row><row><cell></cell><cell>Visual Tracking</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>TCNN[26]</cell><cell>Modeling and Propagating CNNs in a Tree Structure for Visual Tracking</cell><cell>CNN-C</cell><cell>EEN-S</cell><cell>IP-OL</cell><cell>M+m</cell><cell>http://home.unist.ac.kr/ professor/bhhan/</cell></row><row><cell>SANet[28]</cell><cell>SANet: Structure-Aware Network for Visual Tracking</cell><cell>CNN-C, RNN</cell><cell>EEN-S</cell><cell>VP-OL</cell><cell>M+m</cell><cell>http: //www.dabi.temple.edu/ ˜hbling/ publication-selected.htm</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Average precision scores of different trackers for each individual attribute on the OTB-100 dataset.</figDesc><table><row><cell></cell><cell>IV</cell><cell>SV</cell><cell>OCC</cell><cell>DEF</cell><cell>MB</cell><cell>FM</cell><cell>IPR</cell><cell>OPR</cell><cell>OV</cell><cell>BC</cell><cell>LR</cell><cell>Overall</cell></row><row><cell>SANet[28]</cell><cell>0.926</cell><cell>0.891</cell><cell>0.866</cell><cell>0.899</cell><cell>0.858</cell><cell>0.853</cell><cell>0.903</cell><cell>0.906</cell><cell>0.790</cell><cell>0.931</cell><cell>0.882</cell><cell>0.913</cell></row><row><cell>ECO[86]</cell><cell>0.914</cell><cell>0.881</cell><cell>0.908</cell><cell>0.859</cell><cell>0.904</cell><cell>0.865</cell><cell>0.892</cell><cell>0.907</cell><cell>0.913</cell><cell>0.942</cell><cell>0.888</cell><cell>0.910</cell></row><row><cell>MDNet[20]</cell><cell>0.915</cell><cell>0.893</cell><cell>0.857</cell><cell>0.899</cell><cell>0.879</cell><cell>0.869</cell><cell>0.910</cell><cell>0.900</cell><cell>0.822</cell><cell>0.924</cell><cell>0.854</cell><cell>0.909</cell></row><row><cell>C-COT[24]</cell><cell>0.884</cell><cell>0.882</cell><cell>0.904</cell><cell>0.859</cell><cell>0.906</cell><cell>0.870</cell><cell>0.877</cell><cell>0.899</cell><cell>0.895</cell><cell>0.882</cell><cell>0.885</cell><cell>0.903</cell></row><row><cell>TCNN[26]</cell><cell>0.920</cell><cell>0.870</cell><cell>0.831</cell><cell>0.848</cell><cell>0.869</cell><cell>0.843</cell><cell>0.895</cell><cell>0.880</cell><cell>0.772</cell><cell>0.878</cell><cell>0.890</cell><cell>0.884</cell></row><row><cell>MCPF[85]</cell><cell>0.881</cell><cell>0.864</cell><cell>0.862</cell><cell>0.816</cell><cell>0.841</cell><cell>0.827</cell><cell>0.888</cell><cell>0.867</cell><cell>0.764</cell><cell>0.823</cell><cell>0.918</cell><cell>0.873</cell></row><row><cell>DNT[99]</cell><cell>0.871</cell><cell>0.824</cell><cell>0.792</cell><cell>0.855</cell><cell>0.775</cell><cell>0.781</cell><cell>0.855</cell><cell>0.864</cell><cell>0.779</cell><cell>0.829</cell><cell>0.827</cell><cell>0.857</cell></row><row><cell>HDT[31]</cell><cell>0.820</cell><cell>0.811</cell><cell>0.774</cell><cell>0.821</cell><cell>0.794</cell><cell>0.802</cell><cell>0.844</cell><cell>0.805</cell><cell>0.663</cell><cell>0.844</cell><cell>0.766</cell><cell>0.848</cell></row><row><cell>HCFT[30]</cell><cell>0.832</cell><cell>0.802</cell><cell>0.778</cell><cell>0.792</cell><cell>0.797</cell><cell>0.792</cell><cell>0.865</cell><cell>0.816</cell><cell>0.677</cell><cell>0.843</cell><cell>0.786</cell><cell>0.842</cell></row><row><cell>STCT[21]</cell><cell>0.829</cell><cell>0.819</cell><cell>0.785</cell><cell>0.823</cell><cell>0.779</cell><cell>0.770</cell><cell>0.815</cell><cell>0.819</cell><cell>0.694</cell><cell>0.842</cell><cell>0.784</cell><cell>0.841</cell></row><row><cell>FCNT[19]</cell><cell>0.777</cell><cell>0.763</cell><cell>0.731</cell><cell>0.760</cell><cell>0.726</cell><cell>0.708</cell><cell>0.830</cell><cell>0.800</cell><cell>0.629</cell><cell>0.750</cell><cell>0.755</cell><cell>0.798</cell></row><row><cell>SRDCF[101]</cell><cell>0.792</cell><cell>0.749</cell><cell>0.735</cell><cell>0.734</cell><cell>0.782</cell><cell>0.762</cell><cell>0.745</cell><cell>0.742</cell><cell>0.597</cell><cell>0.775</cell><cell>0.613</cell><cell>0.789</cell></row><row><cell>MEEM[43]</cell><cell>0.740</cell><cell>0.740</cell><cell>0.741</cell><cell>0.754</cell><cell>0.722</cell><cell>0.728</cell><cell>0.794</cell><cell>0.794</cell><cell>0.685</cell><cell>0.746</cell><cell>0.605</cell><cell>0.781</cell></row><row><cell>SiameFC[23]</cell><cell>0.736</cell><cell>0.739</cell><cell>0.722</cell><cell>0.690</cell><cell>0.724</cell><cell>0.741</cell><cell>0.742</cell><cell>0.756</cell><cell>0.669</cell><cell>0.690</cell><cell>0.815</cell><cell>0.771</cell></row><row><cell>DCFNet[93]</cell><cell>0.722</cell><cell>0.743</cell><cell>0.755</cell><cell>0.671</cell><cell>0.686</cell><cell>0.673</cell><cell>0.738</cell><cell>0.755</cell><cell>0.727</cell><cell>0.741</cell><cell>0.767</cell><cell>0.751</cell></row><row><cell>CFNet[88]</cell><cell>0.694</cell><cell>0.715</cell><cell>0.674</cell><cell>0.643</cell><cell>0.687</cell><cell>0.695</cell><cell>0.767</cell><cell>0.734</cell><cell>0.533</cell><cell>0.724</cell><cell>0.787</cell><cell>0.748</cell></row><row><cell>KCF[44]</cell><cell>0.719</cell><cell>0.639</cell><cell>0.630</cell><cell>0.617</cell><cell>0.618</cell><cell>0.620</cell><cell>0.701</cell><cell>0.677</cell><cell>0.501</cell><cell>0.713</cell><cell>0.546</cell><cell>0.696</cell></row><row><cell>TGPR[55]</cell><cell>0.615</cell><cell>0.569</cell><cell>0.581</cell><cell>0.584</cell><cell>0.522</cell><cell>0.503</cell><cell>0.653</cell><cell>0.633</cell><cell>0.402</cell><cell>0.596</cell><cell>0.539</cell><cell>0.627</cell></row><row><cell>CNT[82]</cell><cell>0.558</cell><cell>0.514</cell><cell>0.515</cell><cell>0.502</cell><cell>0.368</cell><cell>0.380</cell><cell>0.539</cell><cell>0.561</cell><cell>0.388</cell><cell>0.607</cell><cell>0.522</cell><cell>0.564</cell></row><row><cell>SCM[5]</cell><cell>0.605</cell><cell>0.550</cell><cell>0.541</cell><cell>0.512</cell><cell>0.322</cell><cell>0.320</cell><cell>0.533</cell><cell>0.554</cell><cell>0.409</cell><cell>0.579</cell><cell>0.484</cell><cell>0.559</cell></row><row><cell>DLT[80]</cell><cell>0.570</cell><cell>0.540</cell><cell>0.490</cell><cell>0.486</cell><cell>0.454</cell><cell>0.421</cell><cell>0.529</cell><cell>0.551</cell><cell>0.486</cell><cell>0.561</cell><cell>0.661</cell><cell>0.553</cell></row><row><cell>ALSA[6]</cell><cell>0.524</cell><cell>0.515</cell><cell>0.434</cell><cell>0.453</cell><cell>0.284</cell><cell>0.307</cell><cell>0.495</cell><cell>0.515</cell><cell>0.384</cell><cell>0.531</cell><cell>0.489</cell><cell>0.506</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Average success rate scores of different trackers for each individual attribute on the OTB-100 dataset.</figDesc><table><row><cell></cell><cell>IV</cell><cell>SV</cell><cell>OCC</cell><cell>DEF</cell><cell>MB</cell><cell>FM</cell><cell>IPR</cell><cell>OPR</cell><cell>OV</cell><cell>BC</cell><cell>LR</cell><cell>Overall</cell></row><row><cell>SANet[28]</cell><cell>0.677</cell><cell>0.640</cell><cell>0.635</cell><cell>0.630</cell><cell>0.663</cell><cell>0.642</cell><cell>0.639</cell><cell>0.649</cell><cell>0.600</cell><cell>0.669</cell><cell>0.592</cell><cell>0.661</cell></row><row><cell>ECO[86]</cell><cell>0.713</cell><cell>0.669</cell><cell>0.680</cell><cell>0.633</cell><cell>0.718</cell><cell>0.678</cell><cell>0.655</cell><cell>0.673</cell><cell>0.660</cell><cell>0.700</cell><cell>0.617</cell><cell>0.691</cell></row><row><cell>MDNet[20]</cell><cell>0.689</cell><cell>0.661</cell><cell>0.646</cell><cell>0.649</cell><cell>0.686</cell><cell>0.667</cell><cell>0.655</cell><cell>0.661</cell><cell>0.626</cell><cell>0.676</cell><cell>0.591</cell><cell>0.677</cell></row><row><cell>C-COT[24]</cell><cell>0.682</cell><cell>0.658</cell><cell>0.674</cell><cell>0.614</cell><cell>0.716</cell><cell>0.673</cell><cell>0.627</cell><cell>0.652</cell><cell>0.648</cell><cell>0.652</cell><cell>0.619</cell><cell>0.673</cell></row><row><cell>TCNN[26]</cell><cell>0.678</cell><cell>0.641</cell><cell>0.621</cell><cell>0.615</cell><cell>0.681</cell><cell>0.648</cell><cell>0.645</cell><cell>0.640</cell><cell>0.583</cell><cell>0.629</cell><cell>0.610</cell><cell>0.654</cell></row><row><cell>MCPF[85]</cell><cell>0.628</cell><cell>0.604</cell><cell>0.620</cell><cell>0.570</cell><cell>0.597</cell><cell>0.583</cell><cell>0.620</cell><cell>0.619</cell><cell>0.553</cell><cell>0.601</cell><cell>0.598</cell><cell>0.628</cell></row><row><cell>DNT[99]</cell><cell>0.637</cell><cell>0.598</cell><cell>0.597</cell><cell>0.620</cell><cell>0.609</cell><cell>0.600</cell><cell>0.615</cell><cell>0.630</cell><cell>0.591</cell><cell>0.603</cell><cell>0.543</cell><cell>0.631</cell></row><row><cell>HDT[31]</cell><cell>0.535</cell><cell>0.489</cell><cell>0.528</cell><cell>0.543</cell><cell>0.563</cell><cell>0.550</cell><cell>0.555</cell><cell>0.533</cell><cell>0.472</cell><cell>0.578</cell><cell>0.420</cell><cell>0.564</cell></row><row><cell>HCFT[30]</cell><cell>0.552</cell><cell>0.491</cell><cell>0.537</cell><cell>0.532</cell><cell>0.580</cell><cell>0.558</cell><cell>0.567</cell><cell>0.543</cell><cell>0.484</cell><cell>0.585</cell><cell>0.437</cell><cell>0.568</cell></row><row><cell>STCT[21]</cell><cell>0.644</cell><cell>0.594</cell><cell>0.590</cell><cell>0.603</cell><cell>0.625</cell><cell>0.607</cell><cell>0.567</cell><cell>0.582</cell><cell>0.530</cell><cell>0.635</cell><cell>0.527</cell><cell>0.624</cell></row><row><cell>FCNT[19]</cell><cell>0.543</cell><cell>0.506</cell><cell>0.515</cell><cell>0.529</cell><cell>0.555</cell><cell>0.558</cell><cell>0.558</cell><cell>0.549</cell><cell>0.470</cell><cell>0.529</cell><cell>0.442</cell><cell>0.555</cell></row><row><cell>SRDCF[101]</cell><cell>0.613</cell><cell>0.565</cell><cell>0.559</cell><cell>0.544</cell><cell>0.610</cell><cell>0.595</cell><cell>0.544</cell><cell>0.550</cell><cell>0.460</cell><cell>0.583</cell><cell>0.480</cell><cell>0.598</cell></row><row><cell>MEEM[43]</cell><cell>0.517</cell><cell>0.474</cell><cell>0.504</cell><cell>0.489</cell><cell>0.545</cell><cell>0.525</cell><cell>0.529</cell><cell>0.525</cell><cell>0.488</cell><cell>0.519</cell><cell>0.335</cell><cell>0.530</cell></row><row><cell>SiameFC[23]</cell><cell>0.568</cell><cell>0.557</cell><cell>0.543</cell><cell>0.506</cell><cell>0.568</cell><cell>0.569</cell><cell>0.557</cell><cell>0.558</cell><cell>0.506</cell><cell>0.523</cell><cell>0.573</cell><cell>0.582</cell></row><row><cell>DCFNet[93]</cell><cell>0.581</cell><cell>0.570</cell><cell>0.573</cell><cell>0.497</cell><cell>0.564</cell><cell>0.545</cell><cell>0.557</cell><cell>0.575</cell><cell>0.557</cell><cell>0.569</cell><cell>0.551</cell><cell>0.580</cell></row><row><cell>CFNet[88]</cell><cell>0.544</cell><cell>0.539</cell><cell>0.516</cell><cell>0.473</cell><cell>0.567</cell><cell>0.553</cell><cell>0.568</cell><cell>0.542</cell><cell>0.414</cell><cell>0.549</cell><cell>0.590</cell><cell>0.568</cell></row><row><cell>KCF[44]</cell><cell>0.479</cell><cell>0.399</cell><cell>0.443</cell><cell>0.436</cell><cell>0.456</cell><cell>0.448</cell><cell>0.469</cell><cell>0.453</cell><cell>0.393</cell><cell>0.498</cell><cell>0.307</cell><cell>0.477</cell></row><row><cell>TGPR[55]</cell><cell>0.445</cell><cell>0.386</cell><cell>0.423</cell><cell>0.426</cell><cell>0.418</cell><cell>0.402</cell><cell>0.459</cell><cell>0.451</cell><cell>0.326</cell><cell>0.424</cell><cell>0.338</cell><cell>0.449</cell></row><row><cell>CNT[82]</cell><cell>0.459</cell><cell>0.410</cell><cell>0.409</cell><cell>0.389</cell><cell>0.349</cell><cell>0.334</cell><cell>0.409</cell><cell>0.429</cell><cell>0.347</cell><cell>0.482</cell><cell>0.394</cell><cell>0.446</cell></row><row><cell>SCM[5]</cell><cell>0.486</cell><cell>0.432</cell><cell>0.420</cell><cell>0.381</cell><cell>0.320</cell><cell>0.311</cell><cell>0.412</cell><cell>0.421</cell><cell>0.324</cell><cell>0.457</cell><cell>0.347</cell><cell>0.441</cell></row><row><cell>DLT[80]</cell><cell>0.426</cell><cell>0.401</cell><cell>0.336</cell><cell>0.319</cell><cell>0.389</cell><cell>0.353</cell><cell>0.381</cell><cell>0.396</cell><cell>0.350</cell><cell>0.383</cell><cell>0.464</cell><cell>0.402</cell></row><row><cell>ALSA[6]</cell><cell>0.433</cell><cell>0.408</cell><cell>0.367</cell><cell>0.362</cell><cell>0.282</cell><cell>0.291</cell><cell>0.393</cell><cell>0.411</cell><cell>0.321</cell><cell>0.432</cell><cell>0.341</cell><cell>0.410</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Accuracy scores and failure times of different trackers for each individual attribute</figDesc><table><row><cell cols="22">on the VOT2015 dataset. ('A'-accuracy score, 'R'-failure times, 'CM'-camera motion, 'US'-</cell></row><row><cell cols="22">unsigned, 'ILL'-illum change, 'MC'-motion change, 'OCC'-occlusion, 'SC'-size change )</cell></row><row><cell>Name</cell><cell>A</cell><cell>CM</cell><cell>R</cell><cell>A</cell><cell>US</cell><cell>R</cell><cell>A</cell><cell>ILL</cell><cell>R</cell><cell>A</cell><cell>MC</cell><cell>R</cell><cell>A</cell><cell cols="2">OCC</cell><cell>R</cell><cell>A</cell><cell>SC</cell><cell>R</cell><cell>A</cell><cell>Pooled R</cell></row><row><cell>SANet</cell><cell>0.61</cell><cell></cell><cell>18.13</cell><cell>0.66</cell><cell></cell><cell>5.20</cell><cell>0.68</cell><cell></cell><cell>1.07</cell><cell>0.56</cell><cell></cell><cell>15.73</cell><cell cols="2">0.54</cell><cell cols="2">13.93</cell><cell>0.56</cell><cell></cell><cell>11.20</cell><cell cols="2">0.61</cell><cell>43.07</cell></row><row><cell>ECO</cell><cell>0.56</cell><cell></cell><cell>15.00</cell><cell>0.59</cell><cell></cell><cell>5.00</cell><cell>0.64</cell><cell></cell><cell>1.00</cell><cell>0.49</cell><cell></cell><cell>17.00</cell><cell cols="2">0.41</cell><cell cols="2">17.00</cell><cell>0.50</cell><cell></cell><cell>10.00</cell><cell cols="2">0.54</cell><cell>43.00</cell></row><row><cell>MDNet</cell><cell>0.61</cell><cell></cell><cell>20.00</cell><cell>0.65</cell><cell></cell><cell>6.20</cell><cell>0.68</cell><cell></cell><cell>1.07</cell><cell>0.56</cell><cell></cell><cell>15.73</cell><cell cols="2">0.54</cell><cell cols="2">13.93</cell><cell>0.56</cell><cell></cell><cell>11.20</cell><cell cols="2">0.60</cell><cell>45.93</cell></row><row><cell>C COT</cell><cell>0.56</cell><cell></cell><cell>17.00</cell><cell>0.56</cell><cell></cell><cell>10.00</cell><cell>0.66</cell><cell></cell><cell>1.00</cell><cell>0.49</cell><cell></cell><cell>19.00</cell><cell cols="2">0.51</cell><cell cols="2">18.00</cell><cell>0.51</cell><cell></cell><cell>12.00</cell><cell cols="2">0.54</cell><cell>52.00</cell></row><row><cell>TCNN</cell><cell>0.59</cell><cell></cell><cell>25.67</cell><cell>0.63</cell><cell></cell><cell>7.13</cell><cell>0.67</cell><cell></cell><cell>2.27</cell><cell>0.56</cell><cell></cell><cell>21.67</cell><cell cols="2">0.51</cell><cell cols="2">16.20</cell><cell>0.54</cell><cell></cell><cell>15.67</cell><cell cols="2">0.59</cell><cell>57.53</cell></row><row><cell>DNT</cell><cell>0.56</cell><cell></cell><cell>30.04</cell><cell>0.58</cell><cell></cell><cell>15.87</cell><cell>0.50</cell><cell></cell><cell>2.20</cell><cell>0.50</cell><cell></cell><cell>23.20</cell><cell cols="2">0.43</cell><cell cols="2">17.53</cell><cell>0.48</cell><cell></cell><cell>14.67</cell><cell cols="2">0.54</cell><cell>72.87</cell></row><row><cell>HDT</cell><cell>0.51</cell><cell></cell><cell>29.00</cell><cell>0.58</cell><cell></cell><cell>16.00</cell><cell>0.44</cell><cell></cell><cell>3.00</cell><cell>0.48</cell><cell></cell><cell>31.00</cell><cell cols="2">0.43</cell><cell cols="2">17.00</cell><cell>0.38</cell><cell></cell><cell>17.00</cell><cell cols="2">0.51</cell><cell>80.00</cell></row><row><cell>HCFT</cell><cell>0.52</cell><cell></cell><cell>24.00</cell><cell>0.56</cell><cell></cell><cell>17.00</cell><cell>0.45</cell><cell></cell><cell>3.00</cell><cell>0.48</cell><cell></cell><cell>30.00</cell><cell cols="2">0.43</cell><cell cols="2">16.00</cell><cell>0.37</cell><cell></cell><cell>16.00</cell><cell cols="2">0.50</cell><cell>73.00</cell></row><row><cell>STCT</cell><cell>0.58</cell><cell></cell><cell>44.40</cell><cell>0.61</cell><cell></cell><cell>13.27</cell><cell>0.65</cell><cell></cell><cell>3.87</cell><cell>0.50</cell><cell></cell><cell>26.60</cell><cell cols="2">0.44</cell><cell cols="2">31.87</cell><cell>0.50</cell><cell></cell><cell>16.80</cell><cell cols="2">0.56</cell><cell>94.33</cell></row><row><cell>FCNT</cell><cell>0.51</cell><cell></cell><cell>31.33</cell><cell>0.56</cell><cell></cell><cell>10.33</cell><cell>0.49</cell><cell></cell><cell>2.93</cell><cell>0.50</cell><cell></cell><cell>22.47</cell><cell cols="2">0.51</cell><cell cols="2">18.33</cell><cell>0.42</cell><cell></cell><cell>14.00</cell><cell cols="2">0.52</cell><cell>64.20</cell></row><row><cell>SRDCF</cell><cell>0.56</cell><cell></cell><cell>32.00</cell><cell>0.62</cell><cell></cell><cell>15.00</cell><cell>0.68</cell><cell></cell><cell>5.00</cell><cell>0.49</cell><cell></cell><cell>25.00</cell><cell cols="2">0.48</cell><cell cols="2">19.00</cell><cell>0.51</cell><cell></cell><cell>13.00</cell><cell cols="2">0.56</cell><cell>71.00</cell></row><row><cell>MEEM</cell><cell>0.49</cell><cell></cell><cell>42.00</cell><cell>0.58</cell><cell></cell><cell>24.00</cell><cell>0.48</cell><cell></cell><cell>5.00</cell><cell>0.47</cell><cell></cell><cell>38.00</cell><cell cols="2">0.47</cell><cell cols="2">26.00</cell><cell>0.36</cell><cell></cell><cell>30.00</cell><cell cols="2">0.50</cell><cell>107.00</cell></row><row><cell>SiameFC</cell><cell>0.56</cell><cell></cell><cell>32.00</cell><cell>0.60</cell><cell></cell><cell>15.00</cell><cell>0.67</cell><cell></cell><cell>2.00</cell><cell>0.51</cell><cell></cell><cell>30.00</cell><cell cols="2">0.47</cell><cell cols="2">20.00</cell><cell>0.51</cell><cell></cell><cell>20.00</cell><cell cols="2">0.55</cell><cell>84.00</cell></row><row><cell>DCFNet</cell><cell>0.55</cell><cell></cell><cell>32.00</cell><cell>0.60</cell><cell></cell><cell>23.00</cell><cell>0.59</cell><cell></cell><cell>4.00</cell><cell>0.49</cell><cell></cell><cell>30.00</cell><cell cols="2">0.48</cell><cell cols="2">17.00</cell><cell>0.49</cell><cell></cell><cell>15.00</cell><cell cols="2">0.55</cell><cell>92.00</cell></row><row><cell>CFNet</cell><cell>0.56</cell><cell></cell><cell>60.00</cell><cell>0.63</cell><cell></cell><cell>22.00</cell><cell>0.72</cell><cell></cell><cell>6.00</cell><cell>0.51</cell><cell></cell><cell>48.00</cell><cell cols="2">0.43</cell><cell cols="2">18.00</cell><cell>0.52</cell><cell></cell><cell>24.00</cell><cell cols="2">0.56</cell><cell>126.00</cell></row><row><cell>KCF</cell><cell>0.49</cell><cell></cell><cell>57.00</cell><cell>0.54</cell><cell></cell><cell>38.00</cell><cell>0.49</cell><cell></cell><cell>7.00</cell><cell>0.46</cell><cell></cell><cell>54.00</cell><cell cols="2">0.47</cell><cell cols="2">22.00</cell><cell>0.37</cell><cell></cell><cell>26.00</cell><cell cols="2">0.49</cell><cell>146.00</cell></row><row><cell>TGPR</cell><cell>0.46</cell><cell></cell><cell>58.87</cell><cell>0.55</cell><cell></cell><cell>29.53</cell><cell>0.45</cell><cell></cell><cell>7.20</cell><cell>0.45</cell><cell></cell><cell>51.07</cell><cell cols="2">0.44</cell><cell cols="2">26.80</cell><cell>0.36</cell><cell></cell><cell>34.53</cell><cell cols="2">0.48</cell><cell>134.60</cell></row><row><cell>CNT</cell><cell>0.48</cell><cell></cell><cell cols="2">111.67 0.57</cell><cell></cell><cell>46.00</cell><cell>0.59</cell><cell></cell><cell>8.67</cell><cell>0.42</cell><cell></cell><cell>91.88</cell><cell cols="2">0.34</cell><cell cols="2">34.67</cell><cell>0.41</cell><cell></cell><cell>42.33</cell><cell cols="2">0.48</cell><cell>230.88</cell></row><row><cell>SCM</cell><cell>0.25</cell><cell></cell><cell cols="2">364.33 0.24</cell><cell></cell><cell cols="2">331.53 0.26</cell><cell></cell><cell>38.60</cell><cell>0.22</cell><cell></cell><cell cols="3">241.60 0.24</cell><cell cols="3">105.93 0.21</cell><cell></cell><cell cols="3">202.07 0.23</cell><cell>978.20</cell></row><row><cell>DLT</cell><cell>0.17</cell><cell></cell><cell cols="2">132.00 0.16</cell><cell></cell><cell>48.00</cell><cell>0.17</cell><cell></cell><cell>15.00</cell><cell>0.17</cell><cell></cell><cell>94.00</cell><cell cols="2">0.17</cell><cell cols="2">51.00</cell><cell>0.17</cell><cell></cell><cell>66.00</cell><cell cols="2">0.17</cell><cell>274.00</cell></row><row><cell>ASLS</cell><cell>0.26</cell><cell></cell><cell cols="2">360.07 0.25</cell><cell></cell><cell cols="2">345.53 0.25</cell><cell></cell><cell>41.40</cell><cell>0.23</cell><cell></cell><cell cols="3">237.00 0.25</cell><cell cols="3">103.20 0.23</cell><cell></cell><cell cols="3">204.73 0.24</cell><cell>990.47</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>SSAT is an extended version of MDNet<ref type="bibr" target="#b20">[20]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>MLDF is designed based on FCNT<ref type="bibr" target="#b19">[19]</ref> and STCT<ref type="bibr" target="#b21">[21]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>We note that both SiameFC<ref type="bibr" target="#b24">[23]</ref>, CFNet<ref type="bibr" target="#b92">[88]</ref> methods have multiple different implementation versions. Here, SiameFC means SiameFC 3s and CFNet means CFNet Conv2.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Incremental learning for robust visual tracking</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="125" to="141" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust object tracking with online multiple instance learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1619" to="1632" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Tracking-learning-detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Kalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1409" to="1422" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Real time robust L1 tracker using accelerated proximal gradient approach</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1830" to="1837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Robust object tracking via sparse collaborative appearance model</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2356" to="2368" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Visual tracking via coarse and fine structural local sparse appearance models</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4555" to="4564" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Struck: Structured output tracking with kernels</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Golodetz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saffari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2096" to="2109" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Online object tracking: A benchmark</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2411" to="2418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Object tracking benchmark</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1834" to="1848" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Encoding color information for visual tracking: Algorithms and benchmark</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Blasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5630" to="5644" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The visual object tracking VOT2015 challenge results</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cehovin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vojír</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Häger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Nebehay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Pflugfelder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="564" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="436" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Region-based convolutional networks for accurate object detection and segmentation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="142" to="158" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><surname>A C C E P T E D M A N U S C R I P T</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Joint ctc-attention based end-to-end speech recognition using multi-task learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4835" to="4839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep neural networks employing multi-task learning and stacked bottleneck features for speech synthesis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Valentini-Botinhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Watts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4460" to="4464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2773" to="2781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>arXiv abs/1409.0473</idno>
	</analytic>
	<monogr>
		<title level="j">Clinical Orthopaedics and Related Research</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Visual tracking with fully convolutional networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3119" to="3127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning multi-domain convolutional neural networks for visual tracking</title>
		<author>
			<persName><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4293" to="4302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">STCT: sequentially training convolutional networks for visual tracking</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1373" to="1381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Siamese instance search for tracking</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1420" to="1429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName><surname>A C C E P T E D M A N U S C R I P T</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fullyconvolutional siamese networks for object tracking</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision Workshops</title>
		<meeting>the European Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="850" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Beyond correlation filters: Learning continuous convolution operators for visual tracking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="472" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to track at 100 FPS with deep regression networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="749" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Modeling and propagating cnns in a tree structure for visual tracking</title>
		<author>
			<persName><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<idno>arXiv abs/1608.07242</idno>
	</analytic>
	<monogr>
		<title level="j">Clinical Orthopaedics and Related Research</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Action-decision networks for visual tracking with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2711" to="2720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sanet: Structure-aware network for visual tracking</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning spatially regularized correlation filters for visual tracking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Häger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4310" to="4318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hierarchical convolutional features for visual tracking</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3074" to="3082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hedged deep tracking</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4303" to="4311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Kernel-based object tracking</title>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">R</forename><surname>Member</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="564" to="575" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Color-based probabilistic tracking</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vermaak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gangnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="661" to="675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Tracking in Low Frame Rate Video: A cascade particle filter with discriminative observers of different life spans</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kawade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1728" to="1740" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A survey of appearance models in visual object tracking</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">48</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Understanding and diagnosing visual tracking systems</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3101" to="3109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">PROST: Parallel robust online simple tracking</title>
		<author>
			<persName><forename type="first">J</forename><surname>Santner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saffari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="723" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Tracking by sampling and integratingmultiple trackers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1428" to="1441" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Visual tracking via weighted local cosine similarity</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1838" to="1850" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Real-time and robust object tracking in video via lowrank coherency analysis in feature space</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2885" to="2905" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Robust superpixel tracking</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1639" to="1651" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Fast compressive tracking</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2002" to="2015" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">MEEM: robust tracking via multiple experts using entropy minimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="188" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName><surname>A C C E P T E D M A N U S C R I P T</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">High-speed tracking with kernelized correlation filters</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="583" to="596" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Robust visual tracking via online multiple instance learning with fisher information</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3917" to="3926" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Robust visual tracking via co-trained kernelized correlation filters</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Suganthan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="82" to="93" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Robust fragments-based tracking using the integral histogram</title>
		<author>
			<persName><forename type="first">A</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shimshoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="798" to="805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Robust visual tracking and vehicle classification via sparse representation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2259" to="2272" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Online object tracking with sparse prototypes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="314" to="325" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">On-line boosting and vision</title>
		<author>
			<persName><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="260" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Ensemble tracking</title>
		<author>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="261" to="271" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Support vector tracking</title>
		<author>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1064" to="1072" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">On-line random forests</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saffari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Santner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Godec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1393" to="1400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning adaptive metric for robust visual tracking</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2288" to="2300" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Transfer learning based visual tracking with gaussian processes regression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="188" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A tutorial on particle filters for online nonlinear/non-gaussian bayesian tracking</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Arulampalam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maskell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Clapp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="174" to="188" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Object tracking: A survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Recent advances and trends in visual tracking: A review</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="3823" to="3831" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Visual tracking: An experimental survey</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1442" to="1468" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>arXiv abs/1207.0580</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>arXiv abs/1409.1556</idno>
	</analytic>
	<monogr>
		<title level="j">Clinical Orthopaedics and Related Research</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R-Cnn</forename><surname>Fast</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R-Cnn</forename><surname>Faster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">You only look once: Unified, realtime object detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R-Cnn</forename><surname>Mask</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Object detection from video tubelets with convolutional neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="817" to="825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Transferring rich feature hierarchies for robust visual tracking</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
		<idno>arXiv abs/1501.04587</idno>
	</analytic>
	<monogr>
		<title level="j">Clinical Orthopaedics and Related Research</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title/>
		<author>
			<persName><surname>A C C E P T E D M A N U S C R I P T</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Spatially supervised recurrent convolutional neural networks for visual object tracking</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<idno>arXiv abs/1607.05781</idno>
	</analytic>
	<monogr>
		<title level="j">Clinical Orthopaedics and Related Research</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Robust visual tracking with deep convolutional neural network based object proposals on pets</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1265" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Recurrently target-attending tracking</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1449" to="1458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Robust visual tracking with deep convolutional neural network based object proposals on PETS</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1265" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Re3 : Real-time recurrent regression networks for object tracking</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<idno>arXiv abs/1705.06368</idno>
	</analytic>
	<monogr>
		<title level="j">Clinical Orthopaedics and Related Research</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Learning a deep compact image representation for visual tracking</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="809" to="817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Self-taught learning of a deep invariant representation for visual tracking via temporal slowness principle</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2964" to="2982" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Robust visual tracking via convolutional networks without training</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1779" to="1792" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Video tracking using learned hierarchical features</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1424" to="1435" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Human tracking using convolutional neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1610" to="1623" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title/>
		<author>
			<persName><surname>A C C E P T E D M A N U S C R I P T</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Multi-task correlation particle filter for robust object tracking</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4335" to="4343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">ECO: efficient convolution operators for tracking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6638" to="6646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Learning a temporally invariant representation for visual tracking</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Image Processing</title>
		<meeting>the IEEE International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="857" to="861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">End-to-end representation learning for correlation filter based tracking</title>
		<author>
			<persName><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2805" to="2813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Once for all: a two-flow convolutional neural network for visual tracking</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tao</surname></persName>
		</author>
		<idno>arXiv abs/1604.07507</idno>
	</analytic>
	<monogr>
		<title level="j">Clinical Orthopaedics and Related Research</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Attentional correlation filter network for adaptive visual tracking</title>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Demiris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4807" to="4816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Learning discriminative feature representations by convolutional neural networks for visual tracking</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName><surname>Deeptrack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Visual tracking via shallow and deep collaborative model</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">218</biblScope>
			<biblScope unit="page" from="61" to="71" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Discriminant correlation filters network for visual tracking</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dcfnet</forename></persName>
		</author>
		<idno>arXiv abs/1704.04057</idno>
	</analytic>
	<monogr>
		<title level="j">Clinical Orthopaedics and Related Research</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Visual tracking by reinforced decision making</title>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<idno>arXiv abs/1702.06291</idno>
	</analytic>
	<monogr>
		<title level="j">Clinical Orthopaedics and Related Research</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title/>
		<author>
			<persName><surname>A C C E P T E D M A N U S C R I P T</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Online tracking by learning discriminative saliency map with convolutional neural network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="597" to="606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Deep tracking: Visual tracking using deep convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dehghan</surname></persName>
		</author>
		<idno>arXiv abs/1512.03993</idno>
	</analytic>
	<monogr>
		<title level="j">Clinical Orthopaedics and Related Research</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">When correlation filters meet convolutional neural networks for visual tracking</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1454" to="1458" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Online discriminative object tracking via deep convolutional neural network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Cnntracker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1088" to="1098" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Dual deep network for visual tracking</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2005" to="2015" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Deep relative tracking</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1845" to="1858" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Learning spatially regularized correlation filters for visual tracking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Häger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4310" to="4318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Learning to localize objects with structured output regression</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="2" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Convolutional neural network committees for handwritten character classification</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Document Analysis and Recognition</title>
		<meeting>the International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1135" to="1139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for offline handwritten chinese character classification</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Neural Networks</title>
		<meeting>the International Joint Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Coarse-to-fine auto-encoder networks (CFAN) for real-time face alignment</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
