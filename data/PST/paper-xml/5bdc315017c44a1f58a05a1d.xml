<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LEMNA: Explaining Deep Learning based Security Applications</title>
				<funder ref="#_VCkDwDp #_yV67vqa #_FwA5nTh">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wenbo</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Pennsylvania State University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">JD Security Research Center</orgName>
								<address>
									<addrLine>3 Virginia Tech</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dongliang</forename><surname>Mu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Pennsylvania State University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Nanjing University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Pennsylvania State University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Stevens Institute of Technolog</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Purui</forename><surname>Su</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gang</forename><surname>Wang</surname></persName>
							<email>gangwang@vt.edu</email>
						</author>
						<author>
							<persName><forename type="first">Xinyu</forename><surname>Xing</surname></persName>
							<email>xxing@ist.psu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The Pennsylvania State University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">JD Security Research Center</orgName>
								<address>
									<addrLine>3 Virginia Tech</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LEMNA: Explaining Deep Learning based Security Applications</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3243734.3243792</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Explainable AI</term>
					<term>Binary Analysis</term>
					<term>Deep Recurrent Neural Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While deep learning has shown a great potential in various domains, the lack of transparency has limited its application in security or safety-critical areas. Existing research has attempted to develop explanation techniques to provide interpretable explanations for each classification decision. Unfortunately, current methods are optimized for non-security tasks (e.g., image analysis). Their key assumptions are often violated in security applications, leading to a poor explanation fidelity.</p><p>In this paper, we propose LEMNA, a high-fidelity explanation method dedicated for security applications. Given an input data sample, LEMNA generates a small set of interpretable features to explain how the input sample is classified. The core idea is to approximate a local area of the complex deep learning decision boundary using a simple interpretable model. The local interpretable model is specially designed to (1) handle feature dependency to better work with security applications (e.g., binary code analysis); and</p><p>(2) handle nonlinear local boundaries to boost explanation fidelity. We evaluate our system using two popular deep learning applications in security (a malware classifier, and a function start detector for binary reverse-engineering). Extensive evaluations show that LEMNA's explanation has a much higher fidelity level compared to existing methods. In addition, we demonstrate practical use cases of LEMNA to help machine learning developers to validate model behavior, troubleshoot classification errors, and automatically patch the errors of the target models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In recent years, Deep Neural Networks have shown a great potential to build security applications. So far, researchers have successfully applied deep neural networks to train classifiers for malware classification <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b68">68]</ref>, binary reverse-engineering <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b71">71]</ref> and network intrusion detection <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b62">62]</ref>, which all achieved an exceptionally high accuracy.</p><p>While intrigued by the high-accuracy, security practitioners are concerned about the lack of transparency of the deep learning models and thus hesitated to widely adopt deep learning classifiers in security and safety-critical areas. More specifically, deep neural networks could easily contain hundreds of thousands or even millions of neurons. This network, once trained with massive datasets, can provide a high classification accuracy. However, the high complexity of the network also leads to a low "interpretability" of the model. It is very difficult to understand how deep neural networks make certain decisions. The lack of transparency creates key barriers to establishing trusts to the model or effectively troubleshooting classification errors.</p><p>To improve the transparency of deep neural networks, researchers start to work on explanation methods to interpret the classification results. Most existing works focus on non-security applications such as image analysis or natural language processing (NLP). Figure <ref type="figure" target="#fig_1">1a</ref> shows an example. Given an input image, the explanation method explains the classification result by pinpointing the most impactful features to the final decision. Common approaches involve running forward propagation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b76">76]</ref> or backward propagation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b53">53]</ref> in the network to infer important features. More advanced methods <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b45">45]</ref> produce explanations under a "blackbox" setting where no knowledge of classifier details is available. The basic idea is to approximate the local decision boundary using a linear model to infer the important features.</p><p>Unfortunately, existing explanation methods are not directly applicable to security applications. First, most existing methods are designed for image analysis, which prefers using Convolutional Neural Networks (CNN). However, CNN model is not very popular in security domains. Security applications such as binary reverseengineering and malware analysis either have a high-level feature dependency (e.g, binary code sequences), or require high scalability. As a result, Recurrent Neural Networks (RNN) or Multilayer Perceptron Model (MLP) are more widely used <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b68">68]</ref>. So far, there is no explanation method working well on RNN. Second, existing methods still suffer from a low explanation fidelity, as validated by our experiments in ?5. This might be acceptable for image analysis, but can cause serious troubles in security applications. For example, in Figure <ref type="figure" target="#fig_1">1a</ref>, the highlighted pixels are not entirely accurate (in particular at the edge areas) but are sufficient to provide an intuitive understanding. However, for security applications such as Session 2D: ML 2 CCS'18, October 15-19, 2018, Toronto, ON, Canada binary analysis, incorrectly highlighting one byte of code may lead to serious misunderstandings or interpretation errors.</p><p>Our Designs. In this paper, we seek to develop a novel, high-fidelity explanation method dedicated for security applications. Our method works under a black-box setting and introduces specialized designs to address the above challenges. Given an input data instance x and a classifier such as an RNN, our method aims to identify a small set of features that have key contributions to the classification of x. This is done by generating a local approximation of the target classifier's decision boundary near x. To significantly improve the fidelity of the approximation, our method no longer assumes the local detection boundary is linear, nor does it assume the features are independent. These are two key assumptions made by existing models <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b45">45]</ref> which are often violated in security applications, causing a poor explanation fidelity. Instead, we introduce a new approach to approximate the non-linear local boundaries based on a mixture regression model <ref type="bibr" target="#b26">[27]</ref> enhanced by fused lasso <ref type="bibr" target="#b64">[64]</ref>.</p><p>Our design is based on two key insights. First, a mixture regression model, in theory, can approximate both linear and non-linear decision boundaries given enough data <ref type="bibr" target="#b34">[35]</ref>. This gives us the flexibility to optimize the local approximation for a non-linear boundary and avoid big fitting errors. Second, "fused lasso" is a penalty term commonly used for capturing feature dependency. By adding fused lasso to the learning process, the mixture regression model can take features as a group and thus capture the dependency between adjacent features. In this way, our method produces high-fidelity explanation results by simultaneously preserving the local nonlinearity and feature dependency of the deep learning model. For convenience, we refer to our method as "Local Explanation Method using Nonlinear Approximation" or LEMNA.</p><p>Evaluations. To demonstrate the effectiveness of our explanation model, we apply LEMNA to two promising security applications: classifying PDF malware <ref type="bibr" target="#b55">[55]</ref>, and detecting the function start to reverse-engineer binary code <ref type="bibr" target="#b52">[52]</ref>. The classifiers are trained on 10,000 PDF files and 2,200 binaries respectively, and both achieve an accuracy of 98.6% or higher. We apply LEMNA to explain their classification results and develop a series of fidelity metrics to assess the correctness of the explanations. The fidelity metrics are computed either by directly comparing the approximated detection boundary with the real one, or running end-to-end feature tests. The results show that LEMNA significantly outperforms existing methods across all different classifiers and application settings.</p><p>Going beyond the effectiveness assessment, we demonstrate how security analysts and machine learning developers can benefit from the explanation results. First, we show that LEMNA could help to establish trusts by explaining how classifiers make the correct decisions. In particular, for both binary and malware analyses, we demonstrate the classifiers have successfully learned a number of well-known heuristics and "golden rules" in the respective domain. Second, we illustrate that LEMNA could extract "new knowledge" from classifiers. These new heuristics are difficult to be manually summarized in a direct way, but make intuitive sense to domain experts once they are extracted by LEMNA. Finally, with LEMNA's capability, an analyst could explain why the classifiers produce errors. This allows the analyst to automatically generate targeted patches  Contributions. Our paper makes three key contributions.</p><p>? We design and develop LEMNA, a specialized explanation method for deep learning based security applications. Using a mixture regression model enhanced by fused lasso, LEMNA generates high-fidelity explanation results for a range of deep learning models including RNN. ? We evaluate LEMNA using two popular security applications, including PDF malware classification and function start detection in binary reverse-engineering. We propose a series of "fidelity" metrics to quantify the accuracy of the explanation results. Our experiments show that LEMNA outperforms existing explanation methods by a significant margin. ? We demonstrate the practical applications of the explanation method. For both binary analysis and malware detection, LEMNA sheds lights on why the classifier makes correct and incorrect decisions. We present a simple method to automatically convert the insights into actionable steps to patch the targeted errors of the classifiers.</p><p>To the best our knowledge, this is the first explanation system specially customized for security applications and RNN. Our work is only the initial step towards improving the model transparency for more effective testing and debugging of deep learning models. By making the decision-making process interpretable, our efforts can make a positive contribution to building reliable deep learning systems for critical applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">EXPLAINABLE MACHINE LEARNING</head><p>In this section, we start with introducing the background of explainable machine learning, and then discuss existing explanation techniques. Following that, in Section ?3, we introduce key security applications using deep learning models and discuss why existing explanation techniques are not applicable to security applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Definition</head><p>Explainable machine learning seeks to provide interpretable explanations for the classification results. More specifically, given an input instance x and a classifier C, the classifier will assign a label y for x during the testing time. Explanation techniques then aim to illustrate why instance x is classified as y. This often involves identifying a set of important features that make key contributions to the classification process (or result). If the selected features are interpretable to human analysts, then these features can offer an "explanation". Figure <ref type="figure" target="#fig_1">1</ref> shows examples for image classification and sentiment analysis. The classifier decision can be explained by selected features (e.g., highlighted pixels and keywords).</p><p>In this paper, we focus on the deep neural networks to develop explanation methods for security applications. Up to the present, most existing explanation methods are designed for image analysis or NLP. We categorize them into "whitebox" and "blackbox" methods and describe how they work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Whitebox Explanation Methods</head><p>Most existing explanation techniques work under the whitebox setting where the model architecture, parameters, and training data are known. These techniques are also referred as Deep Explanation Methods and mainly designed for CNN. They leverage two major strategies to infer feature importance: (1) forward propagation based input or structure occlusion; and (2) gradient-based backpropagation. We discuss those techniques in the following.</p><p>Forward Propagation based Methods. Given an input sample, the key idea is to perturb the input (or hidden network layers) and observe the corresponding changes. The intuition behind is that perturbing important features is more likely to cause major changes to the network structure and the classification output. Existing methods either nullify a subset of features or removing intermediate parts of the network <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b74">74,</ref><ref type="bibr" target="#b76">76]</ref>. A recent work <ref type="bibr" target="#b18">[19]</ref> extends this idea to detecting adversarial examples (i.e., malicious inputs aiming to cause classification errors).</p><p>Backward Propagation based Methods. Back-propagation based methods leverage the gradients of the deep neural network to infer feature importance. The gradients can be the partial derivatives of classifier output with respect to the input or hidden layers. By propagating the output back to the input, these methods directly calculate the weights of input features. For image classifiers, the basic method is to compute a feature "saliency map" using the gradients of output with respect to the input pixels in images <ref type="bibr" target="#b54">[54,</ref><ref type="bibr" target="#b57">57]</ref> or video frames <ref type="bibr" target="#b17">[18]</ref>. Later works improve this idea by applying saliency map layer by layer <ref type="bibr" target="#b2">[3]</ref> or mapping groups of pixels <ref type="bibr" target="#b50">[50]</ref>.</p><p>Backward propagation based methods face the challenge of "zero gradient". Inside a neural network, the activation functions often have saturated parts, and the corresponding gradients will become zero. Zero gradients make it difficult (if not impossible) for the "saliency map" to back-track the important features. Recent works <ref type="bibr" target="#b53">[53,</ref><ref type="bibr" target="#b59">59]</ref> attempted to address this problem through approximation. However, this sacrifices the fidelity of the explanation <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Blackbox Explanation Methods</head><p>Blackbox explanation methods require no knowledge about the classifier internals such as network architecture and parameters. Instead, they treat the classifier as a "blackbox" and analyze it by sending inputs and observing the outputs (i.e., Model Induction Methods).</p><p>The most representative system in this category is LIME <ref type="bibr" target="#b45">[45]</ref>. Given an input x (e.g., an image), LIME systematically perturbs x to obtain a set of artificial images from the nearby areas of x in the feature space (see x ? and x ?? in Figure <ref type="figure" target="#fig_3">2</ref>). Then, LIME feeds  the artificial images to the target classifier f (x) to obtain labels, and uses the labeled data to fit a linear regression model ?(x). This ?(x) aims to approximate the small part of f (x) near the input image in the feature space. LIME assumes that the local area of the classification boundary near the input instance is linear, and thus it is reasonable to use a linear regression model to locally represent the classification decision made by f (x). Linear regression is selfexplanatory, and thus LIME can pinpoint important features based on the regression coefficients. A recent work SHAP <ref type="bibr" target="#b33">[34]</ref> tries to extend LIME by adding weights to the artificially generated data samples. Other works propose to use other linear models (e.g., decision tree <ref type="bibr" target="#b5">[6]</ref> and decision set <ref type="bibr" target="#b30">[31]</ref>) to incrementally approximate the target detection boundaries.</p><p>As a side note, we want to clarify that machine learning explanation is completely different from feature selection methods such as Principal Component Analysis (PCA) <ref type="bibr" target="#b25">[26]</ref>, Sparse Coding <ref type="bibr" target="#b38">[39]</ref> or Chi-square Statistics <ref type="bibr" target="#b49">[49]</ref>. Explanation methods aim to identify the key features of a specific input instance x to specifically explain how an instance x is classified. On the other hand, feature selection methods such as PCA are typically applied before training on the whole training data to reduce the feature dimension (to speed up the training or reduce overfitting), which cannot explain how a specific classification decision is made.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPLAINING SECURITY APPLICATIONS</head><p>While deep learning has shown a great potential to build security applications, the corresponding explanation methods are largely falling behind. As a result, the lack of transparency reduces the trust. First, security practitioners may not trust the deep learning model if they don't understand how critical decisions are made. Second, if security practitioners cannot troubleshoot classification errors (e.g., errors introduced by biased training data), the concern is that these errors may be amplified later in practice. In the following, we introduce two key security applications where deep learning has recently achieved success. Then we discuss why existing explanation methods are not applicable to the security applications.  <ref type="table">1</ref>: Design space of explainable machine learning for security applications ( =true; =false; =partially true).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Deep Learning in Security Applications</head><p>In this paper, we focus on two important classes of security applications: binary reverse engineering and malware classification.</p><p>Binary Reverse-Engineering. The applications of deep learning in binary analysis include identifying function boundaries <ref type="bibr" target="#b52">[52]</ref>, pinpointing the function type signatures <ref type="bibr" target="#b14">[15]</ref> and tracking down similar binary code <ref type="bibr" target="#b71">[71]</ref>. More specifically, using a bi-directional RNN, Shin et al. improve the function boundary identification and achieve a nearly perfect performance <ref type="bibr" target="#b52">[52]</ref>. Chua et al. also use RNN to accurately track down the arguments and types of functions in binaries <ref type="bibr" target="#b14">[15]</ref>. More recently, Xu et al. employ an MLP to encode a control flow graph to pinpoint vulnerable code fragments <ref type="bibr" target="#b71">[71]</ref>.</p><p>Malware Classification. Existing works mainly use MLP models for large-scale malware classifications. For example, researchers have trained MLP to detect malware at the binary code level <ref type="bibr" target="#b48">[48]</ref> and classify Android malware <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21]</ref>. More recently, Wang et al. <ref type="bibr" target="#b68">[68]</ref> propose an adversarial resistant neural network for detecting malware based on audit logs <ref type="bibr" target="#b6">[7]</ref>.</p><p>A key observation is that RNN and MLP are more widely adopted by these security applications compared to CNN. The reason is that RNN is designed to handle sequential data, which performs exceptionally well in processing the long sequences of binary code. Particularly, Bi-directional RNN can capture the bi-directional dependencies in the input sequences between each hex <ref type="bibr" target="#b52">[52]</ref>. For malware classification, MLP is widely used for its high efficiency. On the other hand, CNN performs well on images since it can take advantage of the grouping effect of features on the 2D images <ref type="bibr" target="#b29">[30]</ref>. These security applications do not have such "matrix-like" data structures to benefit from using CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Why Not Existing Explanation Methods</head><p>There are key challenges to directly apply existing explanation methods to the security applications. In Table <ref type="table">1</ref>, we summarize the desired properties, and why existing methods fail to deliver them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supporting RNN and MLP.</head><p>There is a clear mismatch between the model choices of the above security applications and existing explanation methods. Most existing explanation methods are designed for CNN to work with image classifiers. However, as mentioned in ?3.1, security applications of our interests primarily adopt RNN or MLP. Due to model mismatches, existing explanation methods are not quite applicable. For example, the back-propagation methods including "saliency map" <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b57">57]</ref> and activation difference propagation <ref type="bibr" target="#b53">[53]</ref> require special operations on the convolutional layers and pooling layers of CNN, which do not exist in RNN or MLP 1 .</p><p>1 <ref type="bibr" target="#b14">[15]</ref> presents some case studies using saliency map to explain RNN, but is forced to ignore the feature dependency of RNN, leading to a low explanation fidelity.  Blackbox methods such as LIME do not support RNN well either (validated by our experiments later). Methods like LIME assume features are independent, but this assumption is violated by RNN which explicitly models the dependencies of sequential data.</p><p>Supporting Locally Non-linear Decision Boundary. Most existing methods (e.g., LIME) assume the local linearity of the decision boundary. However, when the local decision boundary is non-linear, which is true for most complex networks, those explanation methods would produce serious errors. Figure <ref type="figure" target="#fig_5">3a</ref> shows an example where the decision boundary around x is highly non-linear. In other words, the linear part is heavily restricted to a very small region. The typical sampling methods can easily hit the artificial data points beyond the linear region, making it difficult for a linear model to approximate the decision boundary near x. Later in our experiments ( ? 5), we confirm that a simple linear approximation will significantly degrade the explanation fidelity.</p><p>Supporting Blackbox Setting. Although both whitebox and blackbox methods have their application scenarios, blackbox methods are still more desirable for security applications. Noticeably, it is not uncommon for people to use pre-trained models (e.g., "Bi-directional RNN" <ref type="bibr" target="#b52">[52]</ref>, "prefix tree" in Dyninst <ref type="bibr" target="#b4">[5]</ref>) where the detailed network architecture, parameters or training data are not all available. Even though a few forward propagation methods can be forced to work under a blackbox setting (by giving up the observations of intermediate layers), it would inevitably lead to performance degradation.</p><p>Summary. In this paper, we aim to bridge the gaps by developing dedicated explanation methods for security applications. Our method aims to work under a blackbox setting and efficiently support popular deep learning models such as RNN, MLP, and CNN. More importantly, the method need to achieve a much higher explanation fidelity to support security applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">OUR EXPLANATION METHOD</head><p>To achieve the above goals, we design and develop LEMNA. At the high-level, we treat a target deep learning classifier as a blackbox and derive explanation through model approximation. In order to provide a high fidelity explanation, LEMNA needs to take a very different design path from existing methods. First, we introduce fused lasso <ref type="bibr" target="#b64">[64]</ref> to handle the feature dependency problems that are often encountered in security applications and RNN (e.g., time series analysis, binary code sequence analysis). Then, we integrate fused lasso into a mixture regression model <ref type="bibr" target="#b27">[28]</ref> to approximate locally nonlinear decision boundaries to support complex security applications.</p><p>In the following, we first discuss the insights behind the design choices of using fused lasso and mixture regression model. Then, we describe the technical details to integrate them into a single model to handle feature dependencies and locally nonlinearity at the same time. Finally, we introduce additional steps to utilize LEMNA to derive high-fidelity explanations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Insights behind Our Designs</head><p>Fused Lasso. Fused lasso is a penalty term commonly used for capturing feature dependencies, and is useful to handle the dependent features in deep learning models such as RNN. At the high-level, "fused lasso" forces LEMNA to group relevant/adjacent features together to generate meaningful explanations. Below, we introduce the technical details of this intuition.</p><p>To learn a model from a set of data samples, a machine learning algorithm needs to minimize a loss function L(f (x), y) that defines the dissimilarity between the true label and the predicted label by the model. For example, to learn a linear regression model f (x) = ?x + ? from a data set with N samples, a learning algorithm needs to minimize the following equation with respect to the parameter ? using Maximum Likelihood Estimation (MLE) <ref type="bibr" target="#b37">[38]</ref>.</p><formula xml:id="formula_0">L(f (x), y) = N i=1 ??x i -y i ? .<label>(1)</label></formula><p>Here, x i is a training sample, represented by an M-dimensionality feature vector</p><formula xml:id="formula_1">(x 1 , x 2 , ? ? ? , x M ) T . The label of x i is denoted as y i . The vector ? = (? 1 , ? 2 , ? ? ? ? M )</formula><p>contains the coefficients of the linear model. ? ? ? is the L2-norm measuring the dissimilarity between the model prediction and the true label. Fused lasso is a penalty term that can be introduced into any loss functions used by a learning algorithm. Take linear regression for example. Fused lasso manifests as a constraint imposed upon coefficients, i.e.,</p><formula xml:id="formula_2">L(f (x), y) = N i=1 ??x i -y i ? , subject to M j=2 ?? j -? j-1 ? ? S .</formula><p>(2) Fused lasso restricts the dissimilarity of coefficients assigned to adjacent features within a small threshold S (i.e., a hyper-parameter) when a learning algorithm minimizes the loss function. As a result, the penalty term forces a learning algorithm to assign equal weights to the adjacent features. Intuitively, this can be interpreted as forcing a learning algorithm to take features as groups and then learn a target model based on feature groups.</p><p>Security applications, such as time series analysis and code sequence analysis, often need to explicitly model the feature dependency of sequential data using RNN. The resulting classifier makes a classification decision based on the co-occurrence of features. If we use a standard linear regression model (e.g., LIME) to derive an explanation, we cannot approximate a local decision boundary correctly. This is because a linear regression model cannot capture feature dependency and treat them independently.</p><p>By introducing fused lasso in the process of approximating local decision boundary, we expect the resulting linear model to have the following form:</p><formula xml:id="formula_3">f (x) = ? 1 x 1 + ? 2 (x 2 + x 3 ) + ? 3 (x 4 + x 5 ) + ? ? ? + ? k x M ,<label>(3)</label></formula><p>where features are grouped together and thus important features are likely to be selected as a group or multiple groups. Explicitly modeling this process in LEMNA helps to derive a more accurate explanation, particularly for the decision made by an RNN. We further explain this idea using an example of sentiment analysis in Figure <ref type="figure" target="#fig_1">1b</ref>. With the help of fused lasso, a regression model would collectively consider adjacent features (e.g., words next to each other in a sentence). When deriving the explanations, our model does not simply yield a single word "not"<ref type="foot" target="#foot_0">2</ref> , but can accurately capture the phrase "not worth the price" as the explanation for the sentiment analysis result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mixture Regression Model.</head><p>A mixture regression model allows us to approximate locally nonlinear decision boundaries more accurately. As shown in Figure <ref type="figure" target="#fig_5">3b</ref>, a mixture regression model is a combination of multiple linear regression models, which makes it more expressive to perform the approximation:</p><formula xml:id="formula_4">y = K k =1 ? k (? k x + ? k ) , (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>where K is a hyper-parameter indicating the total number of linear components combined in the mixture model; ? k indicates the weight assigned to that corresponding component. Given sufficient data samples, whether the classifier has a linear or non-linear decision boundary, the mixture regression model can nearly perfectly approximate the decision boundary (using a finite set of linear models) <ref type="bibr" target="#b34">[35]</ref>. As such, in the context of deep learning explanation, the mixture regression model can help avoid the aforementioned non-linearity issues and derive more accurate explanations.</p><p>To illustrate this idea, we use the example in Figure <ref type="figure" target="#fig_5">3</ref>. As shown in Figure <ref type="figure" target="#fig_5">3a</ref>, a standard linear approximation cannot guarantee the data sampled around the input x still remain in the locally linear region. This can easily lead to imprecise approximation and lowfidelity explanations. Our method in Figure <ref type="figure" target="#fig_5">3b</ref> approximates the local decision boundary with a polygon boundary, in which each blue line represents an independent linear regression model. The best linear model for producing the explanation should be the red line passing through the data point x. In this way, the approximation process can yield an optimal linear regression model for pinpointing important features as the explanation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Development</head><p>Next, we convert these design insights into a functional explanation system. We introduce the technical steps to integrate fused lasso in the learning process of a mixture regression model so that we can handle feature dependency and decision boundary non-linearity at the same time. Technically speaking, we need to derive a mixture regression model by minimizing the following equation</p><formula xml:id="formula_6">L(f (x), y) = N i=1 ? f (x i ) -y i ? , subject to M j=2 ?? k j -? k (j-1) ? ? S , k = 1, . . . , K .<label>(5)</label></formula><p>where f (?) represents the mixture regression model shown in Equation (4), and ? k j indicates the parameter in the k th linear regression model tied to its j th feature.</p><p>Different from a standard linear regression, our optimization objective is intractable and we cannot simply utilize MLE to perform minimization. To effectively estimate parameters for the mixture regression model, we utilize an alternative approach.</p><p>First, we represent the mixture regression model in the form of probability distributions</p><formula xml:id="formula_7">y i ? K k =1 ? k N (? k x i , ? 2 k ) .<label>(6)</label></formula><p>Then, we treat ? 1:K , ? 1:K and ? 2 1:K as parameters <ref type="foot" target="#foot_1">3</ref> . By taking a guess at these parameters, we initialize their values and thus perform parameter estimation by using Expectation Maximization (EM) <ref type="bibr" target="#b36">[37]</ref>, an algorithm which estimates parameters by repeatedly performing two steps -E-Step and M-Step. In the following, we briefly describe how this EM algorithm is used in our problem. More details can be found in Appendix-A.</p><p>In the Equation ( <ref type="formula" target="#formula_7">6</ref>), y i follows a distribution which combines K Gaussian distributions, and each of these distributions has the mean ? k x i and the variance ? 2 k . In the E-Step, we assign each of the data samples to one of the Gaussian distributions by following the standard procedure applied in learning an ordinary mixture regression model. Based on the data samples assigned in the previous E-Step, we then re-compute the parameters ? 1:K , ? 1:K and ? 2 1:K . For the parameters ? 1:K and ? 2 1:K , the re-computation still follows the standard procedure used by ordinary mixture model learning. But, for each parameter in ? 1:K , re-computation follows a customized procedure. That is to compute ? k by minimizing the following equation with respect to</p><formula xml:id="formula_8">? k L(x, y) = N k i=1 ?? k x i -y i ? , subject to M j=2 ?? k j -? k (j-1) ? ? S ,<label>(7)</label></formula><p>where N k refers to the number of samples assigned to the k th component. Here, the reason behind this re-computation customization is that fused lasso has to be imposed to parameters ? 1:K in order to grant a mixture regression model the ability to handle feature dependency. As we can observe, the equation above shares the same form with that shown in Equation <ref type="bibr" target="#b1">(2)</ref>. Therefore, we can minimize the equation through MLE and thus compute the values for parameters ? 1:K .</p><p>Following the standard procedure of EM algorithm, we repeatedly perform the E-step and M-Step. Until stability is reached (i.e., the Gaussian distributions do not vary much from the E-step to the M-step), we output the mixture regression model. Note that we convert ? 2  1:K into the model parameter ? 1:K by following the standard approach applied in ordinary mixture model learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Applying the Model for Explanation</head><p>With the enhanced mixture regression model, we now discuss how to derive high-fidelity explanations for deep learning classifiers.</p><p>Approximating Local Decision Boundary. Given an input instance x, the key to generate the explanation is to approximate the local decision boundary of the target classifier. The end product is an "interpretable" linear model that allows us to select a small set of top features as the explanation. To do so, we first synthesize a set of data samples locally (around x) following the approach described in <ref type="bibr" target="#b45">[45]</ref>. The idea is to randomly nullify a subset of features of x.</p><p>Using the corpus of synthesized data samples, we then approximate the local decision boundary. There are two possible schemes: one is to train a single mixture regression model to perform multiclass classification; the other scheme is to train multiple mixture regression models, each of which performs binary classification. For efficiency considerations, we choose the second scheme and put more rigorous analysis to the Appendix-B.</p><p>Deriving Explanations. Given the input data instance x, and its classification result y, we now can generate explanations as a small set of important features to x's classification. More specifically, we obtain a mixture regression model enhanced by fused lasso. From this mixture model, we then identify the linear component that has the best approximation of the local decision boundary. The weights (or coefficients) in the linear model can be used to rank features. A small set of top features is selected as the explanation result.</p><p>Note that LEMNA is designed to simultaneously handle non-linearity and feature dependency, but this does not mean that LEMNA cannot work on deep learning models using relatively independent features (e.g., MLP or CNN). In fact, the design of LEMNA provides the flexibility to adjust the explanation method according to the target deep learning model. For example, by increasing the hyper-parameter S (which is a threshold for fused lasso), we can relax the constraint imposed upon parameter ? 1:K and allow LEMNA to better handle less dependent features. In Section ?5, we demonstrate the level of generalizability by applying LEMNA to security applications built on both RNN and MLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATION</head><p>In this section, we evaluate the effectiveness of our explanation method on two security applications: malware classification and binary reverse engineering. This current section focuses evaluating  to LEMNA, our system explains the classification decision by color-coding the most important hex. Feature importance decreases from red to yellow. on the accuracy of the explanation through a series of fidelity metrics. In the next section ( ?6), we will present practical use cases of LEMNA to understand classifier behavior, troubleshoot classification errors, and patch the errors of the classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>We apply LEMNA to two security applications: detecting the "function start" for reverse-engineering binary code using RNN, and classifying PDF malware based on MLP. Below, we introduce details about the two security applications, the implementation of LEMNA, and the comparison baselines.</p><p>Binary Reverse-Engineering. Binary code reverse-engineering, which transfers binary code to assembly code, is a crucial step in (1) examining and detecting malware <ref type="bibr" target="#b51">[51]</ref>, (2) hardening the security of software <ref type="bibr" target="#b75">[75]</ref>, and (3) generating security patches <ref type="bibr" target="#b56">[56]</ref>. For years, binary analysis is primarily done manually by experienced security analysts. Recently, researchers show that well-trained RNN can help handle critical reverse-engineering steps such as detecting the function start <ref type="bibr" target="#b52">[52]</ref>, which can significantly save human efforts. Considering the importance of detecting function start (i.e., all binary code reverse-engineering requires knowing the function start), we choose this application to test LEMNA.</p><p>We follow <ref type="bibr" target="#b52">[52]</ref> to build a RNN based on a widely used dataset that contains 2200 binaries <ref type="bibr" target="#b4">[5]</ref>. We compile these binaries under x86 architecture and gcc compiler with four different optimization levels O0, O1, O2, and O3 respectively. This produces 4 training datasets, one for each optimization level. Like <ref type="bibr" target="#b52">[52]</ref>, we use the bi-directional RNN and train 4 different classifiers.</p><p>Each binary in the dataset is presented as a sequence of hex code. As shown in Figure <ref type="figure" target="#fig_6">4</ref>, we first transfer the hex code to their decimal values, and treat each element in the sequence as a feature.</p><p>For training, each element in the sequence has a label of either "a function start" or "not a function start". As shown in Figure <ref type="figure" target="#fig_6">4</ref>, suppose the original binary code is "90 90 90 83 ec 4c" and the function start is at "83", then the label vector is (0, 0, 0, 0, 1, 0, 0). We follow <ref type="bibr" target="#b52">[52]</ref> to truncate very long binary sequences and set the maximum length to 200. Then we feed the sequences into the RNN. We used Keras <ref type="bibr" target="#b13">[14]</ref> to train the model, with Theano [63] as a backend. We split the dataset randomly using 70% of the samples for training, and the rest 30% for testing.  As shown in Table <ref type="table" target="#tab_3">2</ref>, the detection accuracy is extremely high, with a 98.57% or higher precision and recall for all cases. The results are comparable to those reported in <ref type="bibr" target="#b52">[52]</ref>. The hyper-parameters of the RNNs can be found in the Appendix-C. PDF Malware Classifier. We follow <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b48">48]</ref> to construct a MLPbased malware classifier based on a widely used dataset (4999 malicious PDF files and 5000 benign files) <ref type="bibr" target="#b55">[55]</ref>. We follow <ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b58">58]</ref> to extract 135 features for each file. The features were manually crafted by researchers based on the meta-data and the structure of the PDF, such as number of object markers and number of javascript markers. The full feature list can be found in the Mimicus <ref type="bibr" target="#b0">[1]</ref>. We follow the standard method to transform the feature values into a binary representation <ref type="bibr" target="#b40">[41]</ref> (i.e., nonzero feature values are converted to 1), which helps avoid certain high-value features skewing the training process. Like before, we randomly select 70% of the datasets (malware and benign 1:1) as the training data, and use the remaining 30% as the testing data. As shown in Table <ref type="table" target="#tab_3">2</ref>, our precision and recall are both above 98.13%, which are similar to <ref type="bibr" target="#b55">[55]</ref>.</p><p>LEMNA Implementation. We treat the above RNN and MLP as the target classifiers to run LEMNA. Given an input instance, LEMNA approximates the target classifier and explain the classification result. "Explanations" are presented as the most important features for the given input. For the malware classifier, LEMNA outputs a small set of top features that explains why a file is (not) a malware. For the "function start" detector, an example is shown in Figure <ref type="figure" target="#fig_6">4</ref>. Given an input hex sequence and the detected function start (i.e., "83"), LEMNA marks out a small set of hex code in the sequence that has the biggest contribution. Here, "83" is the function start, and LEMNA points out that the hex code "90" before the function start is the most important reason of the detection.</p><p>LEMNA has 3 hyper-parameters that are configurable. First, to approximate the local decision boundary, we set to craft N data samples for the model fitting (see ?4). The second and third parameters are the number of mixture components K, and the threshold of the fused lasso S. For binary function start detection, we set parameters as: N =500, K=6, S=1e -4. For malware classification, we set parameters as: N =500, K=6, S=1e4. Note that the parameter S is set very differently because malware analysis features are relatively independent, while the binary analysis features have a high dependency level. We fix these parameters to run most of our experiments. Later, we have a dedicated section to perform sensitivity tests on the parameter settings (which shows LEMNA is not sensitive to these hyper-parameters).</p><p>LEMNA's Computational Costs. The computational costs of LEMNA are relatively low. For both security applications, the time to generate the explanation for a given instance is about 10 seconds. This computation task further benefits from parallelization. For example, using a server with Intel Xeon CPU E5-2630, one Nvidia Tesla K40c  We use an image classifier as an toy example to explain the fidelity test. Figure <ref type="figure" target="#fig_8">5a</ref> is the original input image ("sweater"). Figure <ref type="figure" target="#fig_8">5b</ref> is the explanation produced by LEMNA where important features (pixels) are highlighted in red. Figure <ref type="figure" target="#fig_8">5c</ref>-5e are three testing instances we generated to test the fidelity of the explanation.</p><p>GPU and 256G RAM, it takes about 2.5 hours to explain all 25, 040 binary testing sequences for O0 with 30 threads.</p><p>Comparison Baselines. We use two baselines for comparison. First, we use the state-of-the-art blackbox method LIME <ref type="bibr" target="#b45">[45]</ref> as our comparison baseline. LIME <ref type="bibr" target="#b45">[45]</ref> has been used to explain image classifiers and NLP applications. Its performance on security applications and RNN is not yet clear <ref type="foot" target="#foot_2">4</ref> . For a fair comparison, we also configure LIME with N =500 which is the number of artificial samples used to fit the linear regression model. Second, we use a random feature selection method as the baseline. Given an input, the Random method selects features randomly as the explanation for the classification result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Fidelity Evaluation</head><p>To validate the correctness (fidelity) of the explanation, we conduct a two-stage experiment. In the first stage, we directly examine the accuracy of our local approximation with respect to the original decision boundary. This is likely to give an initial estimation of the explanation accuracy. In the second stage, we perform end-toend evaluation on the explanation fidelity. We design three fidelity tests to show whether the selected features are indeed the main contributors to the classification results.</p><p>Evaluation 1: Local Approximation Accuracy. This metric is directly computed by comparing the approximated decision boundary and the original one. We measure Root Mean Square Error</p><formula xml:id="formula_9">(RMSE): RMSE = n i =1 (p i -pi ) n</formula><p>, where p i represents a single prediction obtained from a target deep learning classifier, pi denotes the approximated prediction obtained from the explanation method, and n is the total number of testing data samples. More specifically, we start from a given classifier and a set of testing data samples. For each testing data sample x i , we first obtain a prediction probability p i using the classifier. Then for x i , we follow Equation <ref type="bibr" target="#b5">(6)</ref> to generate a regression model, which can produce an estimated prediction probability pi . After running these steps for all n testing samples, we obtain a prediction vector P = (p 1 , p 2 , ..., p n ) and the corresponding approximation vector P = ( p1 , p2 , ..., pn ). Finally, we computer RMSE based on the two vectors. A lower RMSE means the approximated decision boundary ( P) is closer to the true boundary (P), indicating a higher fidelity of explanation.</p><p>Evaluation 2: End-to-end Fidelity Tests. To validate the correctness of the selected features, we design three end-to-end fidelity tests. To help readers to understand the testing process, we use "image classifier" as a toy example <ref type="foot" target="#foot_3">5</ref> . The procedure works in the same way for other classifiers. As shown in Figure <ref type="figure" target="#fig_8">5</ref>, the image classifier is trained to classify "shoe" from "sweater". Figure <ref type="figure" target="#fig_8">5a</ref> is the input image (x) with the label as "sweater". In Figure <ref type="figure" target="#fig_8">5b</ref>, the explanation method explains the reasons for the classification by highlighting important pixels (features) in red. We denote the selected features as F x . To test the fidelity of the explanation, we have three intuitions:</p><p>? If features F x are accurately selected, then removing F x from the input x will lead to classifying this image to a different label, i.e., "shoe" (Figure <ref type="figure" target="#fig_8">5c</ref>). ? If features F x are accurately selected, then adding the feature values of F x to an image of "shoe" is likely to lead to a misclassification, i.e., classifying it as a "sweater" (Figure <ref type="figure" target="#fig_8">5d</ref>). ? If features F x are accurately selected, we can craft a synthetic images that only contains the features in F x , and this synthetic image is likely to be classified as "sweater" (Figure <ref type="figure" target="#fig_8">5e</ref>). Using these intuitions, we construct 3 different fidelity tests to validate the selected features. More formally, given an input instance x and its classification label y, LEMNA identifies a small set of important features (F x ) as the "explanation". We then follow the steps below to generate 3 testing samples t(x) 1 , t(x) 2 and t(x) 3 for feature validation:</p><p>? Feature Deduction Test: we construct a sample t(x) 1 by nullifying the selected features F x from the instance x. ? Feature Augmentation Test: we first select one random instance r from the opposite class (i.e., as long as r 's label is not y). Then we construct t(x) 2 by replacing the feature values of the instance r with those of F x . ? Synthetic Test: we construct t(x) 3 as a synthetic instance.</p><p>We preserve the feature values of the selected features F x while randomly assigning values for the remaining features. The key variable in this experiment is the number of important features selected as the "explanation" (i.e., |F x |). Intuitively, a larger |F x | may yield a better explanation fidelity, but hurts the interpretability of results. We want to keep |F x | small so that human analysts are able to comprehend.</p><p>For each classifier, we run the fidelity tests on the testing dataset (30% of the whole data). Given an instance x in the testing dataset, we generate 3 samples, one for each fidelity test. We feed the 3 samples into the classifier, and examine the positive classification rate (PCR). PCR measures the ratio of the samples still classified as x's original label. Note that "positive" here does not mean "malware" or "function start". It simply means the new sample is still classified as the x's original label. If the feature selection is accurate, we expect the feature deduction samples return a low PCR, the feature augmentation samples return a high PCR, and the synthetic testing samples return a high PCR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experimental Results</head><p>Our experiments show that LEMNA outperforms LIME and the random baseline by a significant margin across all fidelity metrics.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Local Approximation Accuracy. As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PDF Malware</head><p>(a) Feature Deduction test. A lower PCR reflects a higher explanation fidelity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PDF Malware</head><p>(b) Feature Augmentation test. A higher PCR reflects a higher explanation fidelity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PDF Malware</head><p>(c) Synthetic test. A higher PCR reflects a higher explanation fidelity.</p><p>Figure <ref type="figure">6</ref>: Fidelity test results. y-axis denotes the positive classification rate PCR and y-axis denote the number of selected features NFeature by the explanation method. Due to the space limit, the results of Binary-O1 and O2 are shown in Appendix-D.</p><p>observation holds for both the malware classifier and the function start detection. The best performing result of LIME has a RMSE of 0.1532, which is still almost 10 times higher than the worse performing result of LEMNA ( 0.0196). This result confirms that our mixture regression model is able to build a much more accurate approximation than a simple linear model. Note that this metric is not applicable to the random baseline since the random baseline does not construct a decision boundary.   Figure <ref type="figure">6b</ref> shows the results of the feature augmentation test. Recall that feature augmentation is to add the selected features of input x to an instance of the opposite class, expecting the classifier to produce a label of x. A higher PCR indicates the selected features are more important to x. The results are relatively consistent with the previous test: (1) adding a small number of top features can flip the label of the instance in the opposite class; (2) our method outperforms both baselines by a big margin. Noticeably, for the PDF malware classifier, by replacing the top 5 features, 75% of the testing cases flip their labels.</p><p>Figure <ref type="figure">6c</ref> shows a similar trend for the synthetic test. Using our selected features from a given x, the synthetic instances are more likely to be labeled as x's label. Using only 5 top features, the synthetic instances have a 85%-90% of the chance to take x's label, indicating that the core patterns have been successfully captured.</p><p>Across all three tests, our LEMNA outperforms LIME and the random baseline by a big margin. Interestingly, for the malware classifier, LIME performs as poor as random feature selection. This is because the feature vectors are sparse, which hurts the "smoothness" of the decision boundary. LIME has a hard time to accurately approximate the non-smooth boundary, which again validates our design intuition. Our system is more suitable for security applications, considering that security applications require a much higher explanation precision compared to image analysis tasks.</p><p>Sensitivity of Hyper-parameters. Finally, we test how our results would change if the parameters are set differently. We tested a large number of parameter configurations, and find that our conclusions remain consistent. Due to the space limit, we summarize key results in Table <ref type="table" target="#tab_7">4</ref>. The three hyper-parameters are the "number of crafted data samples" for model fitting (N ), the "total number of mixture components" (K), and the "threshold for fused lasso" (S). Table <ref type="table" target="#tab_7">4</ref> presents the results of the binary function start detector on the O0 dataset. We show 4 groups of configurations where we change one parameter at a time. For the fidelity tests, we fix the number of selected features as 25 to calculate the PCR. The results confirm that changing the hyper-parameters do not significantly influence the performance of LEMNA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">APPLICATIONS OF ML EXPLANATION</head><p>So far, we have validated the fidelity of the explanation results. In this section, we present practical applications of LEMNA. We use case studies to show how the explanation results can help security analysts to 1) establish trusts to the trained classifiers, 2) troubleshoot classification errors, 3) and systematically patch the targeted errors. In the following, we primarily focus on the binary reverse-engineering application since this application domain of deep learning is relatively new and not well-understood. We have Table <ref type="table">5</ref>: Case study for the binary analysis (15 cases). Our explanation method ranks features and marks the most important features as red , followed by orange , gold , yellow . We also translate the hex code to assembling code for the ease of understanding. Note that the F. start refers to the function start detected by the deep learning classifier. The function start is also marked by a black square in the hex sequence. *For false negatives under R.F.N., we present the real function start that the classifier failed to detect, and explain why the function start is missed.</p><p>performed the same analysis for the PDF malware classifier, and the results are in Appendix-E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Understanding Classifier Behavior</head><p>The primary application of our explanation method is to assess the reliability of the classifiers and help to establish the "trust". We argue that classifier reliability and trusts do not necessarily come from a high classification accuracy on the training data. Often cases, the training data is not complete enough to capture all the possible variances. Instead, trusts are more likely to be established by understanding the model behavior. In this section, we examine two key directions to understand how classifier makes decisions:</p><p>(1) capturing and validating "golden rules" and well-established heuristics; and (2) discovering new knowledge.</p><p>Capturing Well-known Heuristics (C.W.H.). A reliable classifier should at least capture the well-known heuristics in the respective application domain. For example, in the area of binary reverse-engineering, security practitioners have accumulated a set of useful heuristics to identify the function start, some of which are even treated as "golden rules". Certain "golden rules" are derived from the specifications of the Application Binary Interface (ABI) standards <ref type="bibr" target="#b21">[22]</ref>. For example, the ABI requires a function to store the old frame pointer (ebp) at the start if this function maintains a new frame pointer. This leads to the most commonly seen prologue [push ebp; mov ebp, esp]. Another set of well-established rules come from mainstream compilers. For example, GNU GCC often inserts nop instructions before a function start, which aligns the function for architectural optimization <ref type="bibr" target="#b43">[43]</ref>. By analyzing the explanation results, we observed strong evidence that deep learning classifiers have successfully captured well-known heuristics. In Table <ref type="table">5</ref>, we show 4 most representative cases, one for each classifier (or optimization level). In Case-1, the classifier correctly detected the function start at "55". Then our LEMNA shows why 55 is marked as the function start by highlighting the importance of features (i.e., the hex code nearby). The result matches the well-known golden rule, namely [push ebp; mov ebp,esp]. This suggests the classifiers are making decisions in a reasonable way. Similarly, Case-2 captures the function start "53" right after a "c3". This corresponds to a popular heuristic introduced by compilers as compilers often make a function exit in the end through a "ret" instruction (particularly at the O0 and O1 level).</p><p>In Case-4, "83" is the function start and LEMNA highlighted the "90" in red. This indicates that the classifier follows the " nop right before a function start" rule, which is caused by compilers padding "nop"s prior to aligned functions. Similarly, in Case-3, LEMNA highlighted padding instruction [lea esi,[esi+eiz*1+0]], which is another pattern introduced by compilers. Overall, LEMNA shows that well-known heuristics are successfully captured by the classifiers.</p><p>During our analysis, we observe that well-known heuristics are widely applicable at the lower optimization levels (O0, O1), but do not cover as many binaries at the higher levels (O2, O3). For example, 95% of the functions at O0-level start with [55 89 E5], matching the heuristics of Case-1. 74% of the O1-optimized functions have ret as the ending instruction (Case-2). On the contrary, only 30% of the binary functions at the O2 or O3 level match the well-known heuristics, e.g., padding instructions at the function end ("[90 90 90 90]", "[8d b4 26 00 00 00 00]". This makes intuitive sense because the higher-level optimization would significantly diversify the code structure, making golden rules less effective.</p><p>Discovering New Knowledge (D.N.K.). In addition to matching well-known heuristics, we also examine if the classifiers have picked Session 2D: ML 2 CCS'18, October 15-19, 2018, Toronto, ON, Canada up new heuristics beyond existing knowledge. For security applications, we argue that the new heuristics need to be interpretable by domain experts. In the domain of binary analysis, many potentially useful heuristics are specific to individual functions, and it is hard to summarize all of them manually. For example, the utility functions inserted by the linker often have unique beginning code segments and those segments rarely appear elsewhere (e.g., the _start function always start with [xor ebp, ebp; pop esi]).</p><p>Manually organizing such rules are not practical. However, these rules, once derived by LEMNA, would make intuitive sense to domain experts.</p><p>As shown in Table <ref type="table">5</ref>, we analyze the explanation results and find that classifiers indeed learned new knowledge. We select five representative cases (ID 5-9). Case-5 shows that "31" is detected as the function start because of the subsequent [ed 5e]. " [31 ed 5e]" corresponds to the start of utility function _start (namely [xor ebp, ebp; pop esi]). This illustrates that our explanation method can help summarize unique prologues pertaining to special functions. Note that the function start "31" itself is not necessarily an important indicator. In fact, "31" represents an opcode (xor) that often appears in the middle of the functions. It is "[ed 5e]" that leads to the correct detection.</p><p>Case-6 illustrates another interesting pattern where "2b" is the most important feature to detect the function start at "b8". "2b" resides in instruction following the pattern [mov eax, CONS1; sub eax, CONS2] where CONS1 and CONS2 are constant values and CONS1 -CONS2 = 0 or 3. This pattern appears only in the prologues of "register_tm_clones" and "deregister_tm_clones", which are utility functions for transactional memory. Again this is a function-specific pattern to detect function start.</p><p>Case-7, Case-8 and Case-9 all have some types of "preparations" at the function start. In Case-7, "[83, ec]" is marked as the most important feature, which corresponds to the instruction [sub esp, 0x1c]. Instructions of this form are frequently used at function start to prepare the stack frame. For Case-8, [mov eax, DWORD PTR [esp+0x4]] is marked as the most indicative feature. This instruction is usually inserted to fetch the first argument of a function. Note that "04" has the red color, which is because "04" is used as the offset for [esp+0x4] to fetch the argument of the function. If this offset is of a different value, this instruction would not necessarily be an indicator of the function start. For Case-9, it starts with preserving the registers that are later modified ([push ebp; push edi; push esi]). Preservation those registers, which is required by the calling convention (a common ABI standard), also frequently appears at the function start.</p><p>Overall, LEMNA validates that the classifiers' decision-making has largely followed explainable logics, which helps to establish the trust to these classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Troubleshooting Classification Errors</head><p>The deep neural networks, although highly accurate, still have errors. These errors should not be simply ignored since they often indicate insufficient training, which may be amplified in practice (due to the biased training). Our explanation method seeks to provide insights into "what caused the error" for a given misclassification.</p><p>By inspecting the reason of errors, we seek to provide actionable guidelines for targeted error correction.</p><p>Reasons for False Negatives (R.F.N.). For the binary analysis application, the classifiers would occasionally miss the real function start. As shown in Table <ref type="table">5</ref> (under "R.F.N."), given a false negative, we explain "why the real function start is not classified as a function start". Specifically, we feed the tuple (Code-sequence, Real-function-start) into LEMNA, and the red-colored features are the reasons for not recognizing the function start. For example, in Case-10, "[50 fd]" is marked as the main reason, which correspond to "[jmp 0xfffffd50]". This instruction almost always appears in the middle of routines or functions, which misleads the classifier to think the substantial 31 is not a function start. This is an outlier case because this "[50 fd]" happens to be the last instruction of a special region .plt, which is followed by the _start function. Case-11 and Case-12 are mis-classified due to instructions "[mov edx,eax]" and "[mov eax,ds:0x82014d0]", which often appear in the middle of functions.</p><p>Reasons for False Positives (R.F.P.). Table <ref type="table">5</ref> also show examples where the classifier picked the wrong function start. Here, we feed the tuple (Code-Sequence, Wrong-function-start) into LEMNA to explain why the wrong function start is picked. For example, Case-13 highlighted "c3" in red which represents the "ret" instruction. Typically, "ret" is located at the end of a function to make the exit, which makes the next byte "83" a strong candidate for the function start. However, Case-13 is special because "ret" is actually placed in the middle of a function for optimization purposes. Case-14 and Case-15 are both misled by the padding instruction [lea esi,[esi+eiz*1+0x0]] which is often used to align functions. However, in both cases, this padding instruction is actually used to align the basic blocks inside of the function.</p><p>Overall, LEMNA shows that the errors are largely caused by the fact that the misleading patterns are dominating over the real indicators. To mitigate such errors, we need to pinpoint the corresponding areas in the feature space and suppress the misleading patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Targeted Patching of ML Classifiers</head><p>Based on the above results, we now develop automatic procedures to convert the"insights" into actions to patch the classifiers.</p><p>Patching Method. To patch a specific classification error, our idea is to identify the corresponding parts of the classifier that are undertrained. Then we craft targeted training samples to augment the original training data. Specifically, given a misclassified instance, we apply LEMNA to pinpoint the small set of features (F x ) that cause the errors. Often cases, such instances are outliers in the training data, and do not have enough "counter examples". To this end, our strategy is to augment the training data by adding related "counter examples", by replacing the feature values of F x with random values.</p><p>We use an example (Case-10 in Table <ref type="table">5</ref>) to describe the patching procedure. The classifier missed the function start due to "[50 fd]", a hex pattern that often exists in the middle of a function. Ideally, the classifier should have picked up the other pattern "[31 ed 5e]" to locate the function start. Unfortunately, the impact of the wrong pattern is too dominating. To this end, we can add new samples to reduce the impact of the misleading features (" <ref type="bibr">[</ref> Evaluation Results. To demonstrate the effectiveness of patching, we perform the above procedure on all 5 classifiers. For each false positive and false negative, we generate k p and k n new samples respectively. Note that k p and k n are not necessarily the same, but they both need to be small. After all, we want to patch the targeted errors without hurting the already high accuracy of the classifiers. Consistently for all the classifiers, we replace the top 5 misleading features and retrain the models with 40 epochs. Table <ref type="table" target="#tab_9">6</ref> shows the classifier performance before and after the patching. We have tested the sensitivity of the parameters and find the results remain relatively consistent as long as we set k p and k n between 2 to 10 (Appendix-F). Due to the space limit, Table <ref type="table" target="#tab_9">6</ref> only presents one set of the results for each classifier. Our experiment shows that both false positives and false negatives can be reduced after retraining for all five classifiers. These results demonstrate that by understanding the model behavior, we can identify the weaknesses of the model and enhance the model accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION</head><p>Benefits v.s. Risks. LEMNA is designed to assist security analysts to understand, scrutinize and even patch a deep learning based security system. While designed from the defense perspective, it might be used by an attacker to seek the weakness of a deep learning classifier. However, we argue that this should not dilute the value of LEMNA, and should not be a reason for not developing explanation tools. The analogy is the software fuzzing techniques <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b73">73]</ref>: while fuzzing tools can be used by hackers to seek vulnerabilities to exploit, the fuzzing techniques have significantly benefited the software industry by facilitating software testing to find and fix vulnerabilities before the software release.</p><p>Guidelines for Analyzing LEMNA's Outputs. LEMNA outputs an "explanation" to each testing case. To thoroughly examine a classifier, developers might need to run a large number of testing cases through LEMNA. Manually reading each case's explanation is timeconsuming, and thus we suggest a more efficient method, which is to group similar explanations first. In ?6, we grouped explanations that are exactly the same before picking the "most representative" cases. In practice, developers can use any other clustering techniques to group explanations as needed.</p><p>Broader Security Applications. LEMNA is evaluated using two popular security applications. There are many other security applications such as detecting the "function end" for binary code, pinpointing the function types and detecting vulnerable code <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b66">66]</ref>. They can also potentially benefit from LEMNA, given that their deep learning architectures are RNN or MLP. Note that models like CNN share some similarities with MLP, and thus LEMNA can potentially help with related applications (e.g., image analysis). Future work will explore the applicability of LEMNA in broader application domains.</p><p>Other Deep Learning Architectures. In addition to MLP and RNN, there are other deep learning architectures such as sequence-tosequence networks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b60">60]</ref>, and hybrid networks <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b71">71]</ref>. Although, these architectures primarily find success in fields such as machine translation <ref type="bibr" target="#b3">[4]</ref> and image captioning <ref type="bibr" target="#b24">[25]</ref>, initial evidence shows that they have the potential to play a bigger role in security <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b71">71]</ref>. Once concrete security applications are built in the future, we plan to test LEMNA on these new architectures.</p><p>Feature Obfuscation. LEMNA is useful when features are interpretable, but this may not be true for all applications. In particular, researchers recently proposed various methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b67">67,</ref><ref type="bibr" target="#b70">70]</ref> to obfuscate input features to increase the difficulty of running adversarial attacks. Possibly because feature obfuscation often degrades classifier accuracy, these techniques haven't received a wide usage yet. LEMNA is not directly applicable to classifiers trained on obfuscated features. However, if the model developer has a mapping between the raw and obfuscated features, the developer can still translate LEMNA's output to the interpretable features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">OTHER RELATED WORK</head><p>Since most related works have been discussed in ?2 and ?3, we briefly discuss other related works here.</p><p>Improving Machine Learning Robustness. A deep learning model can be deceived by an adversarial sample (i.e., a malicious input crafted to cause misclassification) <ref type="bibr" target="#b61">[61]</ref>. To improve the model resistance, researchers have proposed various defense methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b67">67]</ref>. The most relevant work is adversarial training <ref type="bibr" target="#b19">[20]</ref>. Adversarial training seeks to add adversarial examples to the training dataset to retrain a more robust model. Various techniques are available to craft adversarial examples for adversarial training <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b72">72]</ref>. A key difference between our patching method and the standard adversarial training is that our patching is based on the understanding of the errors. We try to avoid blindly retraining the model which may introduce new vulnerabilities.</p><p>Mitigating the Influence of Contaminated Data. Recent research has explored ways to mitigate misclassifications introduced by contaminated training data <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b65">65]</ref>. A representative method is "machine unlearning" <ref type="bibr" target="#b9">[10]</ref>, which is to remove the influence of certain training data by transforming the standard training algorithms into a summation form. A more recent work <ref type="bibr" target="#b28">[29]</ref> proposes to utilize an influence function to identify data points that contribute to misclassification. Our approach is complementary to existing works: we propose to augment training data to fix undertrained components (instead of removing bad training data). More importantly, LEMNA helps the human analysts to understand these errors before patching them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSION</head><p>This paper introduces LEMNA, a new method to derive high-fidelity explanations for individual classification results for security applications. LEMNA treats a target deep learning model as a blackbox and approximates its decision boundary through a mixture regression model enhanced by fused lasso. By evaluating it on two popular deep learning based security applications, we show that the proposed method produces highly accurate explanations. In addition, we demonstrate how machine learning developers and security analysts can benefit LEMNA to better understand classifier behavior, troubleshoot misclassification errors, and even perform automated patches to enhance the original deep learning model.</p><formula xml:id="formula_10">E-Step M-Step E-Step M-Step ?... ? 1 2 ? 2 2 ? 1 2 ? 2 2 ? 2 2 ? 2 2 ? 1 2 ? 1 2 ?2?x ?2?x ?1?x ?2?x ?2?x ?1?x ?1?x ?1?x ? 1 ? 2 ? 1 ? 2 ? 1 ? 2 ? 1 ? 2</formula><p>First Iteration Last Iteration </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX -A. DETAIL OF EM ALGORITHM</head><p>As is specified in Section ?4, we utilize EM algorithm to estimate parameters while learning a mixture regression model enhanced by fused lasso. Here, we provide more detail about this process.</p><p>Recall that a mixture regression model contains K components, each of which indicates an individual linear regression model. In the E-Step, we assign each data sample x i to a Gaussian distribution corresponding to one of the components. To achieve this, we introduce a set of latent variables {z i1 , z i2 , ..., z iK }, and use it to indicate to which distribution a data sample is assigned. Note that we use z ik = 1 to represent that the data sample x i is assigned to the k th distribution. To compute values for latent variables, we define</p><formula xml:id="formula_11">p(z ik = 1) = ? k ,<label>(8)</label></formula><p>and thus have the following</p><formula xml:id="formula_12">p(y i |x i , z i1:K ) = K k =1 [N (y i |? k x i , ? 2 k )] z ik ,<label>(9)</label></formula><p>where N (y i |? k x i , ? 2 k ) indicates the k th Gaussian distribution with the mean and variance equal to ? k x i and ? 2 k respectively. From the Equation ( <ref type="formula" target="#formula_12">9</ref>), we can derive a likelihood function below</p><formula xml:id="formula_13">p(y, z|x, ?) = N i=1 p(y i , z i1 , . . . , z in |?x i , ? 2 ) = K k=1 N i=1 [? k N (x i |? k x i , ? 2 k )] z ik = K k =1 ? n k k N i=1 [N (x i |? k x i , ? 2 k )] z ik<label>(10)</label></formula><p>from which we can further compute the expectation of this loglikelihood function (i.e., Q function) as follow:</p><formula xml:id="formula_14">Q(?, ? (t ) ) =E[logp(y, z|x, ?)|y, x, ? (t ) ] = K k =1 {n k log? k + N i=1 ?ik ? [log( 1 ? 2? ) -log? k - 1 ? 2 k (y i -? k x i ) 2 ]} .<label>(11)</label></formula><p>Here, n k = N k =1 Ez ik . ? indicates all of the parameters. ?ik = Ez ik which can be further represented as</p><formula xml:id="formula_15">? ik = ? k N (y i |? k x i , ? 2 k ) K k=1 ? k N (y i |? k x i , ? 2 k ) , i = 1, . . . , N , k = 1, . . . , K ,<label>(12)</label></formula><p>With the latent variables computed through the Equation ( <ref type="formula" target="#formula_15">12</ref>), we can assign each data sample to a corresponding Gaussian distribution. Then, in the M-step, we re-compute the parameters by maximizing the aforementioned Q function with respect to each parameter. More specifically, we can compute parameter ? 2 k and ? k by using the following equations</p><formula xml:id="formula_16">? 2 k = N i=1 ?ik (y i -? k x i ) 2 n k , k = 1, 2, . . . , K , ? k = n k N , k = 1, 2, . . . , K .<label>(13)</label></formula><p>Recall that we re-compute parameter ? 1:K by minimizing the Equation <ref type="bibr" target="#b6">(7)</ref> shown in Section ?4. While it can be resolved by using MLE, in order to improve the efficiency of resolving this equation, we can also an alternative algorithm introduced in <ref type="bibr" target="#b64">[64]</ref>. As is depicted in Figure <ref type="figure" target="#fig_13">7</ref>, we can repeatedly perform E-step and then M-step until the parameters converge, and thus output the mixture regression model enhanced by fused lasso.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX -B. MULTI-CLASS VS MULTIPLE SINGLE-CLASS APPROXIMATION</head><p>As is mentioned in Section 4.3, we choose to perform model approximation with multiple single-class approximation rather than a single muti-class approximation. Here, we discuss the rationale behind our choice.</p><p>As is stated in Section 4.1, the Equation ( <ref type="formula" target="#formula_4">4</ref>) represents a practice that estimates parameters for a binary classifier, in which there are K ? (2 + M) parameters involved in the process of model learning. For a single mixture regression model that classifies a data sample x i into one of L categories (L &gt; 2), the parameter ? k and ? 2 k no longer represent a vector and a singular value. Rather, they denote matrices with the dimensionality of L ? M and L ? L respectively. In the process of learning a mixture regression model, this means that, in addition to ? 1:K which still represents K parameters, the learning algorithm needs to estimate ? 1:K and ? 2 1:K , which denote L ? K ? M and L 2 ? K parameters respectively.</p><p>According to learning heuristics <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b44">44]</ref>, the more parameters a learning algorithm needs to estimate, the more data samples it would typically need. Technically speaking, following the data point sampling approach commonly used by other model induction explanation techniques, we have no difficulty in synthesizing sufficient data samples to perform model learning (i.e., parameter estimation) reasonably well. However, the practice shows that learning a model with a large amount of data samples typically requires substantial amount of computation resources. Recall that for each data sample we have to train an individual mixture regression model in order to derive an explanation. Therefore, we select the single-class approximation scheme that can yield an explanation in a more efficient fashion, even though both of the approximation schemes could yield model(s) representing the equally good approximation for the corresponding local decision boundary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX -C. HYPER-PARAMETERS OF TARGET DEEP LEARNING MODEL</head><p>In Table <ref type="table" target="#tab_10">7</ref>, we show the hyper-parameters used for training corresponding deep learning models. Regarding function start detector, we utilized a recurrent neural network in which its first, second and output layers are an embedding layer with 256 units, a bi-directional RNN with 8 hidden units and a softmax classifier respectively. With respect to the application of PDF malware classification, we used a standard MLP which contains one input layer, three hidden layers and one output layer. The number of hidden units tied to each layer is presented in Table <ref type="table" target="#tab_10">7</ref>.        </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX -E. MALWARE CLASSIFIER CASES</head><p>Table <ref type="table" target="#tab_12">8</ref> shows 4 cases studies on the PDF Malware classifier's decisions, which correspond to true positives, true negatives, false positives and fale negatives respectively. We also present the labels assigned by the classifier.</p><p>Catching Well-known Heuristics (C.W.H.). Case-16 classified as a malware primarily because feature F 31 and F 33 are set to nonzero values. As is shown in Table <ref type="table" target="#tab_12">8</ref>, these features are related to javascript objects, which match well-known heuristics and indicators of malicious PDF files. In the contrary, Case-17 has a benign file and features related to javascripts have zero values (e.g., no javascript code in the file).</p><p>Reasons for False Positives/Negative (R.F.P., R.F.N). Case-18 and Case-19 represents false positives and negatives. Our explanation results show that the two instances are mis-classified because they violated the well-known heuristics learned by the classifier. For example, Case-18 is a malware that contains "ng" injected in the javascript. As a result, the Features F 31 and F 33 both have a zero value, and the classifier cannot detect this type of malware. On the contrary, if the benign file somehow contains some javascript code (e.g., Case-19), the classifier will incorrectly label them as malware.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX -F. SENSITIVITY OF k n AND k p</head><p>In section ?6.3, the patching method has two hyper-parameters k n and k p . Here, we show the results of the sensitivity tests on these two these parameters. We select the classifier trained for binary function start detection using the O2 dataset. Our experiment mythology is to fix one parameter and swap the other one. Then we observe the changes of the re-trained classifier's false positives and false negatives. In Figure <ref type="figure">9a</ref>, we fix k n = 4 and then set K p = 1, 3, 5, 7, 9. In Figure <ref type="figure">9b</ref>, we fix k p = 5 and set k n = 2, 4, 6, 8, 10.</p><p>The results show that increasing k p will reduce false positives but may increase false negatives. On the contrary, increasing k n will reduce false negatives but may increase false positives. The results confirm our statements in ?6.3. Targeted patching should limit to using small k p and k n to patch the target errors while avoiding introducing new errors. By adjusting k p and k n , security analysts can reduce false positives and false negatives at the same time. In ?6.3 we present the selected results where the false positives and the false negatives are relatively balanced (k n = 4 and k p = 5 for this classifier).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Not worth the price for the durability. Cool effects, ... to a vacuum that lasts more than 60 days ... (b) Sentiment analysis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples of machine learning explanation: (a) the image is classified as an "orange" due to the highlighted pixels; (b) The sentence is classified as "negative sentiment" due to the highlighted keywords. by augmenting training samples for each of the explainable errors, and improve the classifier performance via targeted re-training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustrating how a Blackbox Explanation Method works. The key idea is to use a local linear model (?, the blue straight line) to approximate the detection boundary f near the input instance x. Then the linear model can help to select the key contributing features to classifying x.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Approximating a locally non-linear decision boundary. The linear regression model (a) can easily make mistakes; Our mixture regression model (b) achieves a more accurate approximation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Applying LEMNA to explain binary function start. 83 is the real function start, and 0.99 is the output probability of the RNN classifier. By sending the tuple (hex-sequence, 83)to LEMNA, our system explains the classification decision by color-coding the most important hex. Feature importance decreases from red to yellow. on the accuracy of the explanation through a series of fidelity metrics. In the next section ( ?6), we will present practical use cases of LEMNA to understand classifier behavior, troubleshoot classification errors, and patch the errors of the classifiers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(a) Input Image. (b) Explanation. (c) Deduc. test. (d) Augme. test. (e) Synthet. Test.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5:We use an image classifier as an toy example to explain the fidelity test. Figure5ais the original input image ("sweater"). Figure5bis the explanation produced by LEMNA where important features (pixels) are highlighted in red. Figure5c-5e are three testing instances we generated to test the fidelity of the explanation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fidelity Tests.</head><label></label><figDesc>Figure 6a shows the results from feature deduction test. Recall feature deduction test is to remove important features from the input instances. A lower PCR indicates that selected features are more important to the classification decision. By only nullifying the top 5 features produced by LEMNA, the function start detector drops the PCR to 25% or lower. Considering the extremely high accuracy of the classifier (99.5%+, see</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The illustration of an EM algorithm. In each iteration, the algorithm first assigns each data sample to a corresponding Gaussian distribution obtained from the previous iteration (E-Step). Then, it re-computes the Gaussian distributions based on the assignment of the data samples (M-Step). The algorithm repeatedly perform E-Step and M-Step until there is no change to the Gaussian distributions or the assignment of the data samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>Feature Deduction testing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>Feature Augmentation testing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>Synthetic testing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Fidelity validation results of O1 and O2. y-axis denotes the positive classification rate PCR and y-axis denote the number of selected features NFeature by the explanation method.</figDesc><graphic url="image-40.png" coords="16,470.40,93.12,75.33,54.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>Fixing k p = 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 9 :Figure 8</head><label>98</label><figDesc>Figure 9: Sensitivity tests on k n and k p .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Classification accuracy of the trained classifiers.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 ,</head><label>3</label><figDesc>LEMNA has a RMSE an order of magnitude smaller than that of LIME. This</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="2">Binary Function Start</cell><cell>PDF malware</cell></row><row><cell></cell><cell>O0</cell><cell>O1</cell><cell>O2</cell><cell>O3</cell></row><row><cell>LIME</cell><cell cols="4">0.1784 0.1532 0.1527 0.1750</cell><cell>0.1178</cell></row><row><cell cols="5">LEMNA 0.0102 0.0196 0.0113 0.0110</cell><cell>0.0264</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>The Root Mean Square Error (RMSE) of local approximation. LEMNA is more accurate than LIME.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>(N , K , S )</cell><cell cols="4">RMSE Deduc. test Augme. test Synthet. test</cell></row><row><cell cols="2">(500, 6, 1e-4) 0.0102</cell><cell>5.79%</cell><cell>93.94%</cell><cell>98.04%</cell></row><row><cell cols="2">(300, 6, 1e-4) 0.0118</cell><cell>5.94%</cell><cell>94.32%</cell><cell>98.18%</cell></row><row><cell cols="2">(500, 4, 1e-4) 0.0105</cell><cell>5.80%</cell><cell>93.71%</cell><cell>97.89%</cell></row><row><cell cols="2">(500, 6, 1e-3) 0.0114</cell><cell>5.83%</cell><cell>93.21%</cell><cell>97.73%</cell></row></table><note><p>), this drastic decrease of PCR indicates the small set of features are highly important to the classification. Note that the feature nullification is consider minor since the top 5 features only count of 2.5% of the</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Hyper-parameters sensitivity testing results. 200 total features in the input sequence. If we nullify the top 35 features, the PCR is dropped to almost 0.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>50 fd]") and Classification result before and after patching. k n (k p ) referes to the number of augmented samples generated for each false negative (false positive). Note that for function start detection, the number of samples refers to the number of total hex code in the testing set. promote the right indicator ("[31 ed The new samples are generated by replacing the hex value of "[50 fd]" with random hex values. By adding the new samples to the training data, we seek to reduce the errors in the retrained classifier.</figDesc><table><row><cell>Application</cell><cell cols="3">Num. of k n k p</cell><cell cols="2">Before</cell><cell cols="2">After</cell></row><row><cell></cell><cell>Samples</cell><cell></cell><cell></cell><cell cols="4">FN FP FN FP</cell></row><row><cell>Binary O0</cell><cell cols="2">4,891,200 5</cell><cell>5</cell><cell>3</cell><cell>1</cell><cell>0</cell><cell>0</cell></row><row><cell>Binary O1</cell><cell cols="2">4,001,820 3</cell><cell>4</cell><cell>48</cell><cell>33</cell><cell cols="2">23 29</cell></row><row><cell>Binary O2</cell><cell cols="2">4,174,000 4</cell><cell cols="5">5 107 129 59 62</cell></row><row><cell>Binary O3</cell><cell cols="2">5,007,800 2</cell><cell>5</cell><cell>83</cell><cell>41</cell><cell cols="2">15 39</cell></row><row><cell>PDF Malware</cell><cell>3,000</cell><cell cols="3">6 15 28</cell><cell>13</cell><cell>10</cell><cell>5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>The hyper-parameters of corresponding deep learning models. Here "model structure" depicts the number of layers in the model as well as the number of units in each layer. Note that for the four model in the function start identification application (i.e., O0-O3), we use the same set of hyper-parameters.</figDesc><table><row><cell>Application</cell><cell cols="7">Model Structure Activation Optimizer Learning Rate Dropout Rate Batch Size Epoch</cell></row><row><cell>Binary Func. Start</cell><cell>255-8-2</cell><cell>relu</cell><cell>adam</cell><cell>0.001</cell><cell>0.5</cell><cell>100</cell><cell>100</cell></row><row><cell>PDFmalware</cell><cell>135-100-50-10-2</cell><cell>sigmoid</cell><cell>adam</cell><cell>0.001</cell><cell>0.2</cell><cell>100</cell><cell>30</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>CasesID Classifier's Label Explanation (Important features) C.W.H. 16 Malware F 31 [JavaScript]=1 F 114 [prod. oth]=0 F 33 [JS Obfu.]=1 F 56 [crea. uc]=0 F 112 [producer mis.]=0 17 Benign F 114 [prod. oth]=1 F 112 [producer mis.]=1 F 31 [JavaScript]=0 F 33 [JS Obfu.]=0, F 56 [crea. uc]=1 R.F.N 18 Benign F 114 [prod. oth]=1 F 33 [JS Obfu.]=0 F 112 [producer mis.]=1 F 31 [JavaScript]=0 F 56 [crea. uc]=1 R.F.P 19 Malware F 31 [JavaScript]=1 F 33 [JS Obfu.]=1 F 114 [prod. oth]=0 F 56 [crea. uc]=0 F 112 [producer mis.]=0</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Case study for PDF malware classification (4 cases). The feature 31 and 33 are related to "JavaScript Object Markers" and "Obfuscated JavaScript Object Markers" which are indicators of "malware" files; Feature 56, 112 and 114 refer to "Creator: Upper Case Characters", "Differences in Producer Values", and "Producer: Other Characters" which are indicators of "benign" files. The feature values have been normalized to 0 or 1. We mark the most important features as red , followed by orange , gold , yellow .</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>In sentiment analysis, "not" does not always carry negative sentiment, e.g., "not bad".Session 2D: ML 2 CCS'18, October 15-19, 2018, Toronto, ON, Canada</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>? 1:K indicates parameters ? 1 , ? ? ? , ? K . ? 1:K represents parameters ? 1 , ? ? ? , ? K . ? 2 1:K are the parameters ? 2 1 , ? ? ? , ? K 1 ,each of which describes the variance of the normal distribution that ? k follows, i.e., ? k ? N(0, ? 2 k ).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>We have tested SHAP<ref type="bibr" target="#b33">[34]</ref>, which is an extension of LIME. We find that SHAP is very slow and its performance is worse than LIME for our applications.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>The image is selected from the Fashion-mnist dataset<ref type="bibr" target="#b69">[69]</ref>.Session 2D: ML 2 CCS'18, October 15-19, 2018, Toronto, ON, Canada</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="10">ACKNOWLEDGMENTS</head><p>We gratefully acknowledge the support of <rs type="institution">NVIDIA Corporation</rs> with the donation of the Tesla K40 GPU used for this research. We also would like to thank the anonymous reviewers and <rs type="person">Jia Yan</rs> for their helpful feedback. This project was supported in part by <rs type="funder">NSF</rs> grants <rs type="grantNumber">CNS-1718459</rs>, <rs type="grantNumber">CNS-1750101</rs> and <rs type="grantNumber">CNS-1717028</rs>. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of any funding agencies.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_VCkDwDp">
					<idno type="grant-number">CNS-1718459</idno>
				</org>
				<org type="funding" xml:id="_yV67vqa">
					<idno type="grant-number">CNS-1750101</idno>
				</org>
				<org type="funding" xml:id="_FwA5nTh">
					<idno type="grant-number">CNS-1717028</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Mimcus</title>
		<ptr target="https://github.com/srndic/mimicus" />
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">DREBIN: Effective and Explainable Detection of Android Malware in Your Pocket</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Arp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Spreitzenbarth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malte</forename><surname>Hubner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Gascon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konrad</forename><surname>Rieck</surname></persName>
		</author>
		<author>
			<persName><surname>Siemens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Network and Distributed System Security Symposium (NDSS)</title>
		<meeting>the 20th Network and Distributed System Security Symposium (NDSS)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gr?goire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederick</forename><surname>Klauschen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Byteweight: Learning to recognize functions in binary code</title>
		<author>
			<persName><forename type="first">Tiffany</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johnathon</forename><surname>Burket</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maverick</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Brumley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd USENIX Security Symposium</title>
		<meeting>the 23rd USENIX Security Symposium</meeting>
		<imprint>
			<publisher>USENIX Security</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Osbert</forename><surname>Bastani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carolyn</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamsa</forename><surname>Bastani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08504</idno>
		<title level="m">Interpreting blackbox models via model extraction</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Malicious behavior detection using windows audit logs</title>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Berlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Slater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Saxe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th Workshop on Artificial Intelligence and Security (AISec)</title>
		<meeting>the 8th Workshop on Artificial Intelligence and Security (AISec)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Dimensionality reduction as a defense against evasion attacks on machine learning classifiers</title>
		<author>
			<persName><forename type="first">Nitin</forename><surname>Arjun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Bhagoji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prateek</forename><surname>Cullina</surname></persName>
		</author>
		<author>
			<persName><surname>Mittal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02654</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mitigating evasion attacks to deep neural networks via region-based classification</title>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Zhenqiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gong</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual Computer Security Applications Conference (ACSAC)</title>
		<meeting>the 33rd Annual Computer Security Applications Conference (ACSAC)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Towards making systems forget with machine unlearning</title>
		<author>
			<persName><forename type="first">Yinzhi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junfeng</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th IEEE Symposium on Security and Privacy (S&amp;P)</title>
		<meeting>the 36th IEEE Symposium on Security and Privacy (S&amp;P)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th IEEE Symposium on Security and Privacy (S&amp;P)</title>
		<meeting>the 38th IEEE Symposium on Security and Privacy (S&amp;P)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Incremental and decremental support vector machine learning</title>
		<author>
			<persName><forename type="first">Gert</forename><surname>Cauwenberghs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>the 13th Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Angora: Efficient Fuzzing by Principled Search</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th IEEE Symposium on Security and Privacy (S&amp;P)</title>
		<meeting>the 39th IEEE Symposium on Security and Privacy (S&amp;P)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Fran?ois</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Keras</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural Nets Can Learn Function Type Signatures From Binaries</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Leong Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prateek</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenkai</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th USENIX Security Symposium</title>
		<meeting>the 26th USENIX Security Symposium</meeting>
		<imprint>
			<publisher>USENIX Security</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Large-scale malware classification using random projections and neural networks</title>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Stokes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the 38th International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Interpretable Explanations of Black Boxes by Meaningful Perturbation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Computer Vision (ICCV)</title>
		<meeting>the 16th International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Devnet: A deep event network for multimedia event detection and evidence recounting</title>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 28th Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">AI 2 : Safety and Robustness Certification of Neural Networks with Abstract Interpretation</title>
		<author>
			<persName><forename type="first">Timon</forename><surname>Gehr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Mirman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dana</forename><surname>Drachsler-Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Tsankov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swarat</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Vechev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th IEEE Symposium on Security and Privacy</title>
		<meeting>the 39th IEEE Symposium on Security and Privacy</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations (ICLR)</title>
		<meeting>the 3rd International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Kathrin</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveen</forename><surname>Manoharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Backes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04435</idno>
		<title level="m">Adversarial perturbations against deep neural networks for malware classification</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m">The Santa Cruz Operation Inc. 1997. System V application binary interface</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dimensionality and Sample Size Considerations in Pattern Recognition Practice</title>
		<author>
			<persName><forename type="first">K</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><surname>Chandrasekaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Statistics</title>
		<imprint>
			<date type="published" when="1982">1982. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A deep learning approach for network intrusion detection system</title>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Javaid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quamar</forename><surname>Niyaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mansoor</forename><surname>Alam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Conference on Bio-inspired Information and Communications Technologies (BIONETICS)</title>
		<meeting>the 9th International Conference on Bio-inspired Information and Communications Technologies (BIONETICS)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Densecap: Fully convolutional localization networks for dense captioning</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the 29th Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Principal component analysis and factor analysis</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">T</forename><surname>Jolliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Principal component analysis</title>
		<imprint>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hierarchical mixtures of experts and the EM algorithm</title>
		<author>
			<persName><forename type="first">I</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">A</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="1994">1994. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Variable selection in finite mixture of regression models</title>
		<author>
			<persName><forename type="first">Abbas</forename><surname>Khalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahua</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the american Statistical association</title>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Understanding Black-box Predictions via Influence Functions</title>
		<author>
			<persName><forename type="first">Pang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koh</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning (ICML)</title>
		<meeting>the 34th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>the 25th Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Interpretable decision sets: A joint framework for description and prediction</title>
		<author>
			<persName><forename type="first">Himabindu</forename><surname>Lakkaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<meeting>the 22nd International Conference on Knowledge Discovery and Data Mining (KDD)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08220</idno>
		<title level="m">Understanding Neural Networks through Representation Erasure</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Delving into transferable adversarial examples and black-box attacks</title>
		<author>
			<persName><forename type="first">Yanpei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations (ICLR)</title>
		<meeting>the 5th International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A unified approach to interpreting model predictions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Su-In</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>the 30th Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Bayesian modelling and inference on mixtures of distributions. Handbook of statistics</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mengersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Robert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Magnet: a two-pronged defense against adversarial examples</title>
		<author>
			<persName><forename type="first">Dongyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM Conference on Computer and Communications Security (CCS)</title>
		<meeting>the 24th ACM Conference on Computer and Communications Security (CCS)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Finite mixture modeling with mixture outcomes using the EM algorithm</title>
		<author>
			<persName><forename type="first">Bengt</forename><surname>Muth?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kerby</forename><surname>Shedden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<date type="published" when="1999">1999. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Tutorial on maximum likelihood estimation</title>
		<author>
			<persName><forename type="first">In</forename><surname>Jae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myung</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of mathematical Psychology</title>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Emergence of simple-cell receptive field properties by learning a sparse code for natural images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bruno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName><surname>Field</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="1996">1996. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Distillation as a defense to adversarial perturbations against deep neural networks</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th IEEE Symposium on Security and Privacy (S&amp;P)</title>
		<meeting>the 37th IEEE Symposium on Security and Privacy (S&amp;P)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ga?l</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bertrand</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m">Session 2D: ML 2 CCS&apos;18</title>
		<meeting><address><addrLine>Toronto, ON, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">October 15-19, 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deepxplore: Automated whitebox testing of deep learning systems</title>
		<author>
			<persName><forename type="first">Kexin</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinzhi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junfeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suman</forename><surname>Jana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>the 26th Symposium on Operating Systems Principles (SOSP)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dyninst: An application program interface (api) for runtime code generation</title>
		<ptr target="http://www.dyninst.org" />
	</analytic>
	<monogr>
		<title level="j">Online</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<publisher>Paradyn Project</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Small Sample Size Effects in Statistical Pattern Recognition: Recommendations for Practitioners</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sarunas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anil</forename><forename type="middle">K</forename><surname>Raudys</surname></persName>
		</author>
		<author>
			<persName><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="1991">1991. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Why should i trust you?: Explaining the predictions of any classifier</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<meeting>the 22nd International Conference on Knowledge Discovery and Data Mining (KDD)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Incremental and decremental learning for linear support vector machines</title>
		<author>
			<persName><forename type="first">Enrique</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ignacio</forename><surname>Barrio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llu?s</forename><surname>Belanche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Artificial Neural Networks (ICANN)</title>
		<meeting>the 17th International Conference on Artificial Neural Networks (ICANN)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Detecting P2P botnets through network behavior analysis and machine learning</title>
		<author>
			<persName><forename type="first">Sherif</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Issa</forename><surname>Traore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bassam</forename><surname>Sayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Payman</forename><surname>Hakimian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Conference on Privacy, Security and Trust</title>
		<meeting>the 9th International Conference on Privacy, Security and Trust</meeting>
		<imprint>
			<publisher>PST</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep neural network based malware detection using two dimensional binary program features</title>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Berlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Malicious and Unwanted Software (MALWARE)</title>
		<meeting>the 10th International Conference on Malicious and Unwanted Software (MALWARE)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The relation of control charts to analysis of variance and chi-square tests</title>
		<author>
			<persName><forename type="first">Henry</forename><surname>Scheffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<date type="published" when="1947">1947. 1947</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Ramprasaath R Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><surname>Batra</surname></persName>
		</author>
		<idno>arxiv. org/abs/1610.02391 v3</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Automatic reverse engineering of malware emulators</title>
		<author>
			<persName><forename type="first">Monirul</forename><surname>Sharif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Lanzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Giffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenke</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th IEEE Symposium on Security and Privacy (S&amp;P)</title>
		<meeting>the 30th IEEE Symposium on Security and Privacy (S&amp;P)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Recognizing Functions in Binaries with Neural Networks</title>
		<author>
			<persName><forename type="first">Eui</forename><surname>Chul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Moazzezi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th USENIX Security Symposium</title>
		<meeting>the 24th USENIX Security Symposium</meeting>
		<imprint>
			<publisher>USENIX Security</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning Important Features Through Propagating Activation Differences</title>
		<author>
			<persName><forename type="first">Avanti</forename><surname>Shrikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peyton</forename><surname>Greenside</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anshul</forename><surname>Kundaje</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning (ICML)</title>
		<meeting>the 34th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6034</idno>
		<title level="m">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Malicious PDF detection using metadata and structural features</title>
		<author>
			<persName><forename type="first">Charles</forename><surname>Smutz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelos</forename><surname>Stavrou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual Computer Security Applications Conference (ACSAC)</title>
		<meeting>the 28th Annual Computer Security Applications Conference (ACSAC)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">BitBlaze: A new approach to computer security via binary analysis</title>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Brumley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Heng Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><forename type="middle">Gyung</forename><surname>Jager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenkai</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pongsin</forename><surname>Newsome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prateek</forename><surname>Poosankam</surname></persName>
		</author>
		<author>
			<persName><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 4th International Conference on Information Systems Security (ICISS)</title>
		<meeting>4th International Conference on Information Systems Security (ICISS)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6806</idno>
		<title level="m">Striving for simplicity: The all convolutional net</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Practical evasion of a learning-based classifier: A case study</title>
		<author>
			<persName><forename type="first">Nedim</forename><surname>Srndic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Laskov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th IEEE Symposium on Security and Privacy (S&amp;P)</title>
		<meeting>the 35th IEEE Symposium on Security and Privacy (S&amp;P)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">Mukund</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Taly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiqi</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02639</idno>
		<title level="m">Gradients of counterfactuals</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>the 27th Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<title level="m">Intriguing properties of neural networks</title>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Deep learning approach for network intrusion detection in software defined networking</title>
		<author>
			<persName><forename type="first">Tuan</forename><forename type="middle">A</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lotfi</forename><surname>Mhamdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Des</forename><surname>Mclernon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><forename type="middle">Raza</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mounir</forename><surname>Zaidi</surname></persName>
		</author>
		<author>
			<persName><surname>Ghogho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Wireless Networks and Mobile Communications</title>
		<meeting>the 12th International Conference on Wireless Networks and Mobile Communications</meeting>
		<imprint>
			<publisher>WINCOM</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Theano: A Python framework for fast computation of mathematical expressions</title>
		<author>
			<orgName type="collaboration">Theano Development Team</orgName>
		</author>
		<idno>arxiv.org/abs/1605.02688</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Sparsity and smoothness via the fused lasso</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saharon</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Incremental and decremental training for linear classification</title>
		<author>
			<persName><forename type="first">Cheng-Hao</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chieh-Yen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<meeting>the 20th International Conference on Knowledge Discovery and Data Mining (KDD)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Deep belief networks for spam filtering</title>
		<author>
			<persName><forename type="first">Grigorios</forename><surname>Tzortzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aristidis</forename><surname>Likas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Tools with Artificial Intelligence (ICTAI)</title>
		<meeting>the 19th International Conference on Tools with Artificial Intelligence (ICTAI)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<author>
			<persName><forename type="first">Qinglong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbo</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ororbia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C Lee</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Giles</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.01401</idno>
		<title level="m">Learning adversary-resistant deep neural networks</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Adversary resistant deep neural networks with an application to malware detection</title>
		<author>
			<persName><forename type="first">Qinglong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbo</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Ororbia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">I</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C Lee</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<meeting>the 23rd International Conference on Knowledge Discovery and Data Mining (KDD)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Mitigating adversarial effects through randomization</title>
		<author>
			<persName><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Learning Representations (ICLR)</title>
		<meeting>the 6th International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Neural Network-based Graph Embedding for Cross-Platform Binary Code Similarity Detection</title>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Heng Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Conference on Computer and Communications Security (CCS)</title>
		<meeting>the 24th Conference on Computer and Communications Security (CCS)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Trojanning Attack on Neural Networks</title>
		<author>
			<persName><forename type="first">Zhaogui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqing</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuofei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baowen</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Network and Distributed System Security Symposium (NDSS)</title>
		<meeting>the 25th Network and Distributed System Security Symposium (NDSS)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">American fuzzy lop</title>
		<author>
			<persName><forename type="first">Michal</forename><surname>Zalewski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th European Conference on Computer Vision (ECCV)</title>
		<meeting>the 13th European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Control Flow Integrity for COTS Binaries</title>
		<author>
			<persName><forename type="first">Mingwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sekar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd USENIX Conference on Security (USENIX Security)</title>
		<meeting>the 22nd USENIX Conference on Security (USENIX Security)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Visualizing deep neural network decisions: Prediction difference analysis</title>
		<author>
			<persName><forename type="first">Luisa</forename><forename type="middle">M</forename><surname>Zintgraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tameem</forename><surname>Taco S Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations (ICLR)</title>
		<meeting>the 5th International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
