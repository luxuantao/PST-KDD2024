<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Juan</forename><forename type="middle">P</forename><surname>Dominguez-Morales</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Architecture and Technology of Computers</orgName>
								<orgName type="laboratory">Robotic and Technology of Computers Labora-tory</orgName>
								<orgName type="institution">University of Seville</orgName>
								<address>
									<postCode>41012</postCode>
									<settlement>Seville</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Architecture and Technology of Computers</orgName>
								<orgName type="laboratory">Robotic and Technology of Computers Labora-tory</orgName>
								<orgName type="institution">University of Seville</orgName>
								<address>
									<postCode>41012</postCode>
									<settlement>Seville</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Angel</forename><forename type="middle">F</forename><surname>Jimenez-Fernandez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Architecture and Technology of Computers</orgName>
								<orgName type="laboratory">Robotic and Technology of Computers Labora-tory</orgName>
								<orgName type="institution">University of Seville</orgName>
								<address>
									<postCode>41012</postCode>
									<settlement>Seville</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Manuel</forename><forename type="middle">J</forename><surname>Dominguez-Morales</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Architecture and Technology of Computers</orgName>
								<orgName type="laboratory">Robotic and Technology of Computers Labora-tory</orgName>
								<orgName type="institution">University of Seville</orgName>
								<address>
									<postCode>41012</postCode>
									<settlement>Seville</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Gabriel</forename><surname>Jimenez-Moreno</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Architecture and Technology of Computers</orgName>
								<orgName type="laboratory">Robotic and Technology of Computers Labora-tory</orgName>
								<orgName type="institution">University of Seville</orgName>
								<address>
									<postCode>41012</postCode>
									<settlement>Seville</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7BDFD28CC236325CD46CD70D4442ED8E</idno>
					<idno type="DOI">10.1109/TBCAS.2017.2751545</idno>
					<note type="submission">received April 8, 2017; revised August 3, 2017; accepted September 5, 2017.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Audio processing</term>
					<term>Caffe</term>
					<term>convolutional neural networks</term>
					<term>deep learning</term>
					<term>heart murmur</term>
					<term>neuromorphic sensor</term>
					<term>pattern recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Auscultation is one of the most used techniques for detecting cardiovascular diseases, which is one of the main causes of death in the world. Heart murmurs are the most common abnormal finding when a patient visits the physician for auscultation. These heart sounds can either be innocent, which are harmless, or abnormal, which may be a sign of a more serious heart condition. However, the accuracy rate of primary care physicians and expert cardiologists when auscultating is not good enough to avoid most of both type-I (healthy patients are sent for echocardiogram) and type-II (pathological patients are sent home without medication or treatment) errors made. In this paper, the authors present a novel convolutional neural network based tool for classifying between healthy people and pathological patients using a neuromorphic auditory sensor for FPGA that is able to decompose the audio into frequency bands in real time. For this purpose, different networks have been trained with the heart murmur information contained in heart sound recordings obtained from nine different heart sound databases sourced from multiple research groups. These samples are segmented and preprocessed using the neuromorphic auditory sensor to decompose their audio information into frequency bands and, after that, sonogram images with the same size are generated. These images have been used to train and test different convolutional neural network architectures. The best results have been obtained with a modified version of the AlexNet model, achieving 97% accuracy (specificity: 95.12%, sensitivity: 93.20%, PhysioNet/CinC Challenge 2016 score: 0.9416). This tool could aid cardiologists and primary care physicians in the auscultation process, improving the decision making task and reducing type-I and type-II errors.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>H EART disease is a major health problem and is one of the main causes of death in the world. Cardiovascular disease (CVD) causes nearly half of the deaths in Europe (48%) <ref type="bibr" target="#b0">[1]</ref> and 34.3% in America (1 in 2.9 deaths in the United States) <ref type="bibr" target="#b1">[2]</ref>. Detecting CVDs at an early stage is crucial for applying the corresponding treatment and reduce the potential risk factors. Auscultation is one of the most used techniques for this purpose, and can provide clues to the diagnosis of many cardiac abnormalities by listening and analyzing the heart sound components using a stethoscope. It is very cheap and requires minimal equipment. However, physicians need extensive training and experience for auscultating <ref type="bibr" target="#b2">[3]</ref>. Moreover, the accuracy rate of primary care physicians and medical students on the auscultation process is between 20-40%, as reported in <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b6">[7]</ref>, and only roughly 80% is achieved by expert cardiologists <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>.</p><p>Heart murmurs are sounds produced when blood flows across one of the heart valves that are loud enough to produce audible noise. Murmurs may be harmless (innocent), which are primarily due to physiologic conditions outside the heart, or abnormal, which may be a sign of a more serious heart condition or a structural defect in the heart itself. The most common problems that cause abnormal heart murmurs are mitral or aortic stenosis and mitral or aortic regurgitation. The sounds can also be categorized by timing, into systolic and diastolic, differing in the part of the heartbeat on which they can be heard (between the S1 and S2 heart sounds, or starting at or after S2 and ending before or at S1, respectively).</p><p>Heart murmurs are the most common abnormal finding when a patient visits the physician for auscultation. A heart murmur does not necessarily lead to having a CVD; it could be an innocent murmur instead of a pathological one, which does not represent current or future illness. The physician must decide if the patient is healthy or not, but, due to the fact that the accuracy is not great, the expert could be wrong, making type-I or type-II errors. A type-I error (alpha error) is the detection of an effect that is not present (i.e., healthy patients are sent for echocardiogram), while a type-II error (beta error) is failing on the detection of an effect that is present (i.e., pathological patients are sent home without medication or treatment). It is clear that, in this case, type-II errors are more important to avoid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE I COMPARATIVE STUDY BETWEEN STATE-OF-THE-ART STUDIES ABOUT HEART SOUND DIAGNOSIS SYSTEMS</head><p>However, echocardiograms cost between $750 and $1500 <ref type="bibr" target="#b3">[4]</ref> per patient, making type-I errors also important to avoid. The probability of needing this costly procedure could be reduced for both healthy people and pathological patients if a reliable (with a high accuracy rate) diagnostic tool were available as an aide for physicians.</p><p>The classification of heart sounds is not a new topic. Many studies have worked toward designing practical murmur classifier systems to improve the diagnostic accuracy of physicians. Most of them use neural networks (NNs), support vector machines (SVMs) or some complex preprocessing algorithms to carry out this task <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Many studies like <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b14">[15]</ref> have used a processing step where a person selects the best portion of the sound signal that should be used as input to the system, making this solution not ideal for a real scenario because of the need of human interaction. Some of them have used NNs to classify between different kinds of heart murmurs <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref>, but have only trained the network with simulated heart sounds with no noise, obtaining very bad accuracy results when testing the classifier with real heart sounds (48.5%). Others have used only a small amount of real heart sounds <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b16">[17]</ref>, which is not representative when it comes to testing it in a real scenario. Table <ref type="table">I</ref> summarizes the main information about the preprocessing and the classification steps that have been performed in some of the state-of-the-art studies that have been discussed in this section, along with the two leading approaches from the PhysioNet/CinC Challenge 2016. Works like <ref type="bibr" target="#b19">[20]</ref> use similar preprocessing techniques and classification algorithms, but focusing on cough sounds identification instead of heart murmurs.</p><p>The main aim of this work is to develop a classifier system using a Convolutional Neural Network (CNN) that accepts heart sound recordings directly after preprocessing the information, and classifies the input to identify if the person whose heart sound is acquired, is either a healthy person or a pathological patient. The preprocessing step automatically divides the heart sound recordings into windows of a specific time length. Heart murmurs are located in the 195 Hz band <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, but can reach up to 700 Hz <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b20">[21]</ref>, which confirms that they can be identified and extracted from the heart sound signal in the frequency domain. For this purpose, these segments of the original sound are sent to a Neuromorphic Auditory Sensor (NAS) <ref type="bibr" target="#b21">[22]</ref>, which tries to mimic the way in which the inner ear works, decomposing the audio into frequency bands, and packetizes the information using the Address-Event Representation (AER) communication protocol <ref type="bibr" target="#b22">[23]</ref>. Then, this information is converted to sonogram images, which are then used as input to the CNN for further classification using deep learning algorithms.</p><p>The rest of the paper is structured as follows: Section II presents an overview of the system architecture using a block diagram to explain each of the components in it. Then, Section III describes the Neuromorphic Auditory Sensor (NAS) <ref type="bibr" target="#b21">[22]</ref> and how its output information is saved into AEDAT files <ref type="bibr" target="#b23">[24]</ref> in the computer using a USBAERmini2 board <ref type="bibr" target="#b24">[25]</ref>. After this, in Section IV, the dataset acquisition is explained, describing the heart sound database that has been used in this work. Section V presents the preprocessing algorithms executed using the data before applying them as input to the classifier system. Caffe <ref type="bibr" target="#b25">[26]</ref>, which is one of the most used deep learning frameworks, is described in Section VI along with the Convolutional Neural Networks (CNNs) that have been trained and tested in order to classify the heart sound dataset. Section VII presents the classification results and the comparison between the different experiments that have been carried out in this work. Finally, the conclusions of this work are presented in Section VIII.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. SYSTEM OVERVIEW</head><p>The system consists of different modules and steps to achieve its purpose. Most of them are carried out in the computer; however, one of the most important parts of the preprocessing is done outside of it, on a Field Programmable Gate Array (FPGA). A block diagram of the whole system is presented in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>The heart sound recordings used in this work are obtained from the PhysioNet/CinC Challenge database <ref type="bibr" target="#b26">[27]</ref>. It consists of 3,126 heart sound recordings, lasting from 5 to over 120 seconds. The main idea is that, after some preprocessing functions are applied to the information, one image is obtained for each of the audio samples contained in the dataset, so that it could be used as input to feed the CNN. In order to generate images with the same width and height, the first preprocessing step is to divide the heart sound recordings into segments of the same length. In this work, the accuracy of the system has been tested using segmentation windows of 1, 1.25 and 1.5 seconds (without overlapping), which were chosen because they are large enough to contain the information from, at least, one full cardiac cycle, but at the same time small enough to generate as many samples as possible.</p><p>After this process is completed (generating 77573, 61518 and 51009 samples when using a segmentation window of 1, 1.25 and 1.5 seconds length, respectively), audio samples are sent to the audio input of an AER-Node platform <ref type="bibr" target="#b27">[28]</ref>. A 64channel mono NAS (Neuromorphic Auditory Sensor) <ref type="bibr" target="#b21">[22]</ref> is programmed on the Spartan-6 FPGA that the AER-Node board has, which decomposes the audio signal into frequency bands and packetizes the information using the AER (Address-Event Representation) protocol <ref type="bibr" target="#b22">[23]</ref>. An USBAERmini2 board <ref type="bibr" target="#b24">[25]</ref> receives this information and sends it to the computer through a USB port. Then, a script running on MATLAB collects the AER packets received and stores them into AEDAT files <ref type="bibr" target="#b23">[24]</ref> (one file per audio sample), which is the standard format used for storing this kind of information.</p><p>A grayscale sonogram image is generated for each AEDAT file using Neuromorphic Auditory VISualizer Tool (NAVIS) <ref type="bibr" target="#b28">[29]</ref>, which is a desktop software application that is able to load AEDAT files and postprocess the information obtained from the NAS, generating useful charts like the cochleogram, sonogram, histogram, etc. The whole set of images obtained are then divided into three different datasets: one for training the CNN (75% of the total amount of images), a second one for validation (15%) and the last one to test the CNN and obtain the accuracy ratio of the system (10%). Different CNN models have been trained and tested using Caffe and their accuracy results have been compared. Each of these elements and steps will be described in detail in the next sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. NEUROMORPHIC AUDITORY SENSOR</head><p>Neuromorphic Auditory Sensor (NAS) is an audio sensor for FPGAs inspired by Lyon's model of the biological cochlea <ref type="bibr" target="#b29">[30]</ref>. This sensor is able to process an excitatory audio signal using Spike Signal Proccesing (SSP) techniques <ref type="bibr" target="#b30">[31]</ref>, decomposing incoming audio in its frequency components, and providing this information as a stream of events using the Address-Event Representation (AER) <ref type="bibr" target="#b22">[23]</ref>. Current state-of-the-art of silicon cochleae process audio in an analog way <ref type="bibr" target="#b31">[32]</ref>, using a bank of low-pass filters (modeling the basilar membrane), and convert the filters' output to spikes (modeling the inner hair cells). However, NAS works in the opposite way: first, it converts the incoming audio to spikes, and directly processes these spikes using a Spike Low-pass Filter (SLPF) bank with a cascade topology. Due to the use of SSP filters, circuits are very simple and do not need complex operating units or dedicated resources (e.g. floating point ALUs, hardware multipliers, RAM memory, etc...). As a consequence, NAS designers are able to replicate SLPFs in low-cost FPGAs, building large scale NAS with a low-clock frequency working fully in parallel.</p><p>To digitalize audio signals we use a commercial analog-todigital audio converter (CS5344, with a resolution of 24 bits and a sample rate of 96 kSamples/sec.), that provides the audio samples using an I2S bus. Inside the FPGA, audio samples from the I2S bus are decoded to 24 bits digital words with two's complement. Digital audio samples are written in a synthetic spike generator (SSG), which provides a spike stream with a frequency that is proportional to the digital amplitude. These spikes are used as input to a bank of 64 SSP filters with a cascade topology, known as Cascade Filter Bank (CFB), which processes audio spikes decomposing them in frequency. Finally, output spikes from CFB are connected to an AER-Monitor <ref type="bibr" target="#b32">[33]</ref>. This gives a unique address to the fired spikes following the Address-Event Representation, and propagates them using an asynchronous AER bus. Fig. <ref type="figure" target="#fig_1">2</ref> shows the block diagram of the architecture of a mono-aural NAS.</p><p>A 64-channel mono-aural NAS for FPGA with a cascade topology has been used together with a USB-AERmini2 interface <ref type="bibr" target="#b24">[25]</ref>, as can be seen in Fig. <ref type="figure" target="#fig_2">3</ref>. NAS response is stored as AEDAT files and the output information can be seen in the second image (b) of Fig. <ref type="figure">4</ref>, where each dot corresponds to an event that has been fired in a particular AER address at a specific time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DATASET ACQUISITION</head><p>The heart sound dataset used in this work contains the recordings used in the PhysioNet/CinC Challenge 2016 <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b26">[27]</ref>, which comprises nine heart sound databases from different research groups. Heart sound recordings were sourced from several contributors around the world from both healthy subjects and pathological patients including children and adults, and contains a total of 3,126 heart sound recordings, lasting from 5 seconds to over 120 seconds. The heart sound recordings were collected from different locations on the body: aortic area, pulmonic area, tricuspid area and mitral area. These recordings are divided into two types: normal and abnormal heart sound recordings. The normal recordings were from healthy subjects and the abnormal ones were from patients with a confirmed cardiac diagnosis, which is not specified, but typically they are coronary artery diseases and heart valve defects like mitral valve prolapse, mitral regurgitation, aortic stenosis and valvular surgery.</p><p>Audio recordings were resampled to 2000 Hz and have been divided into three different sets of mutually exclusive populations, using 75% of them to train the network, 15% for validation and 10% to test the network. These recordings are not clean and contain noise from various sources due to the uncontrolled environment, such as talking, breathing, stethoscope motion and intestinal sounds, which is important to note because training the system with these real sounds will make it more robust and noise tolerant.</p><p>Using only 75% of the samples that this dataset has (which is only a total of 2345 heart recordings) for training the CNN is not sufficient if we want our system to be robust enough for a test with different recordings that are not included in that collection. Moreover, working with audio files with variable lengths is neither appropriate nor optimal for training a CNN: dividing these files into shorter ones (in terms of duration) would generate more samples that could be used to both train and test the network, making the system more reliable. For this purpose, the heart recordings obtained from the PhysioNet dataset were segmented using a fixed window length. The segmentation is one of the steps that have been carried out in the preprocessing phase, which is described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. PREPROCESSING OF THE INFORMATION</head><p>Sound recordings from the PhysioNet database do not have the same length (each file lasts from 5 to 120 seconds) and CNNs need the input images to have the same width and height for sonogram(in_addr(i), in_tStamp(i)/integPeriod)++ 5: end for training and testing the network. For this purpose, a segmentation algorithm is applied to each of the samples before sending the audio signal to the NAS' audio input connector. In this work, different experiments have been carried out, using 1, 1.25 and 1.5 second-long windows in the segmentation process, obtaining 77573, 61518 and 51009 samples, respectively. This way, the number of samples available is also increased (more than 16 times the amount of samples in the default heart recordings database), which will provide more information in the training process of the CNN (these algorithms need a huge amount of images to train the system more robustly). Each of these three datasets has been used to feed different CNN models and the classification results are presented in Section VII.</p><p>These length values were selected due to the fact that they can contain the information from a full cardiac cycle at least (from the phase of relaxation diastole to the phase of contraction systole; or, in terms of sound, the whole "lub-dub" sequence including S1 and S2).</p><p>As was presented in the introduction (Section I), heart murmurs are located in the 195 Hz band <ref type="bibr" target="#b8">[9]</ref>, but can reach up to 700 Hz <ref type="bibr" target="#b20">[21]</ref>, which confirms that they can be identified and extracted from the heart sound signal in the frequency domain. For this purpose, each of the audio segments obtained from the original sound in the previous step are sent to a NAS, which mimics the way in which the inner ear works, decomposing the audio into frequency bands, and packetizes the information using the AER communication protocol. These packets are sent to the computer through a USB port using the USBAERmini2 board. A script in MATLAB is then used to generate an AEDAT file, which is the standard format used for storing this kind of information, for each of the audio samples. These files contain information about the address and timestamp of every event that has been fired in the NAS when feeding its input with an analog audio signal.</p><p>NAVIS is a GPL-licensed desktop software application that allows to post-process the information obtained from a NAS. This tool implements a set of charts that allows to represent the auditory information as cochleograms, histograms and sonograms, among others. It can also split the auditory information into different sets depending on the activity level of the spike streams. Due to the open-source nature of the project <ref type="bibr">[35]</ref>, it has been modified to automatically take the AER information contained in the AEDAT files that were obtained after sending each of the segmented samples to the NAS, and generate grayscale sonogram images based on the activity levels of the sound recordings in the frequency domain across the NAS' channels.</p><p>The pseudocode shown in Algorithm 1 presents the algorithm that has been used to calculate the sonogram's matrix of values (pixels of the image). These values are then normalized between 0 and 255, and a grayscale tone is set based on each value (0 being black, and 255 being white). Image (c) in Fig. <ref type="figure">4</ref> shows the output sonogram from one of the 1 second-long heart sound recordings.</p><p>The whole preprocessing step can be seen in Fig. <ref type="figure">4</ref>. The first image (a) shows the audio signal that corresponds to one of the 1-second samples after being segmented from the original heart recording. Then, the second one (b) is the cochleogram of the information contained in the AEDAT file that was obtained after sending the audio signal to the NAS and capturing the output information using MATLAB and the USBAERmini2 board. Each dot of the cochleogram is an event that has been fired for a particular AER address (there are 128 addresses in a 64-channel mono NAS: each channel has two addresses, for positive and negative spikes) at a specific time (timestamp). The sonogram of the AEDAT file (c) was calculated using the equation that was previously described, resulting in a grayscale image with a width of 50 pixels (using time windows of 20000 µs in length for integrating the information) and a height of 64 pixels (the number of both negative and positive spikes from the same channel add up).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CAFFE</head><p>Caffe (Convolutional Architecture for Fast Feature Embedding) is a customizable framework for state-of-the-art deep learning algorithms. It allows to train and deploy general pur-   pose CNNs and other deep models efficiently and in an easy way. Caffe is capable of processing over 40 million images a day on a single K40 or Titan GPU (∼2.5 ms per image) thanks to CUDA GPU computation. It has been used in many research fields like vision, speech recognition, robotics, neuroscience and astronomy.</p><p>Caffe provides a complete toolkit for training, testing and deploying models, which can be described using the BSDlicensed C++ library with Python and Matlab bindings. The framework also provides a collection of reference models and well-documented examples for all of these tasks, including the "AlexNet" ImageNet model <ref type="bibr" target="#b34">[36]</ref> and the "LeNet" MNIST model <ref type="bibr" target="#b35">[37]</ref>. These models can be modified, allowing to add/remove layers to/from the network, change the input dataset format and train it with different activation functions and parameters, which are already implemented. Caffe model definitions are written using the Protocol Buffer language [38], which is a language-neutral platform-neutral and easy to use mechanism for serializing structured data.</p><p>In this work, a modified version of the LeNet-5 CNN <ref type="bibr" target="#b35">[37]</ref> has been used, where the number of outputs has been changed to two, as the goal is to distinguish between two classes: healthy subject and pathological patient. This model was designed for handwritten and machine-printed character recognition, but it is also well known for its high accuracy results for image recognition and feature extraction. Many studies have used this model for a wide variety of purposes, like freehand sketch recognition <ref type="bibr" target="#b36">[39]</ref>, Alzheimer's disease recognition <ref type="bibr" target="#b37">[40]</ref> or even horse gait classification <ref type="bibr" target="#b38">[41]</ref>, obtaining very good results.</p><p>Fig. <ref type="figure" target="#fig_4">5</ref> shows the block diagram representation of the LeNet-5 model. The input dataset and the input image size have been set to match our requirements.</p><p>Several tests have been performed, using different values on some of the parameters of the Solver Prototxt file (which is the file that contains the network's training configuration) for each of the three datasets that were obtained after the preprocessing step (using 1 second, 1.25 seconds and 1.5 seconds audio length windows on the segmentation phase). The parameters that have been changed from the Solver Prototxt file are: (1) the base learning rate of the network (base_lr); (2) the momentum, which indicates how much of the previous weight will be retained in the new calculation (momentum); (3) the weight decay, which is the factor of penalization of large weights (weight_decay); (4) the test interval, which has been set to 10000 training iterations (test_interval); (5) the number of test iterations that should occur per test_interval (test_iter), to match the number of samples that the dataset has; and (6) the maximum training iterations, indicating when the network should stop training, which has been set to 500000 (max_iter). These parameters were optimized by repetition and comparison. The solver mode has been changed from CPU to GPU, due to the fact that the training process has been carried out using a NVIDIA GeForce GTX 1060 with 6GB of GDDR5 memory, and CUDA Toolkit 8.  Other CNNs like the AlexNet (Fig. <ref type="figure">6</ref>), which is a much more complex network, has also been tested for this purpose and the accuracy results and comparison between this and the LeNet-5 models are presented in the next sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. RESULTS AND DISCUSSION</head><p>Three different window lengths have been used in the segmentation process in this work: 1, 1.25 and 1.5 seconds. As presented in the section where the preprocessing of the information is described, using these three sample lengths leads to obtaining up to 77573, 61518 and 51009 samples, respectively, which is enough for training and testing a CNN. In this work, modified versions of two widely-known CNN models have been used. The accuracy of the network has been obtained for each of the experiments. The sensitivity (Se), specificity (Sp) and the PhysioNet/Computing in Cardiology Challenge 2016 score (MAcc) have been calculated for the approaches that achieved the best accuracy results using the equations that are defined in <ref type="bibr" target="#b39">[42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Using the LeNet-5 Model</head><p>First, the accuracy of the system was tested using the LeNet-5 model <ref type="bibr" target="#b35">[37]</ref>. The architecture of the model is presented in Fig. <ref type="figure" target="#fig_4">5</ref>: it consists of a convolutional layer followed by a pooling layer, another convolutional layer followed by a pooling layer, and then two fully connected layers similar to the conventional multilayer perceptrons. The classifier was trained and tested using each of the three datasets described before without applying any modification to the training parameters or to the configuration of the CNN's layers. The accuracy results can be seen in Fig. <ref type="figure" target="#fig_6">7</ref> for every 10000 training iterations up to a total of 500000 using a base learning rate of 0.01, the inv learning policy, 0.9 as momentum and 0.0005 as weight decay. The inv learning policy updates the learning rate based on the equation shown in <ref type="bibr" target="#b0">(1)</ref>, where gamma is set to 0.0001 and power to 0.75. Table <ref type="table" target="#tab_0">II</ref> summarizes the training parameters and layer configurations (kernel sizes and strides for each convolution and pooling layer) for each of the CNN models used in this work. l rate = l rate * (1 + gamma * iter) (-power )  (1)</p><p>After the default LeNet-5 CNN was trained and tested, 82.11% was achieved for the 1-second dataset, 82.39% for the 1.25-seconds dataset, and 80.00% for the 1.5-seconds dataset. Se, Sp and MAcc were calculated for the dataset that achieved the best accuracy, obtaining 83.26%, 78.58% and 0.8092, respectively. Even though the model was not modified from its default state to improve the classification, the obtained results were very similar to the accuracy that expert cardiologists are able to achieve when auscultating. Fig. <ref type="figure" target="#fig_7">8</ref> shows the features that the default LeNet-5 model is learning on each of its convolution layers. It can be seen that the first layer extract vertical information from the images and the second one is able to detect more complex patterns. However, the results obtained could be improved by changing the network configuration.</p><p>In this context, the next experiment consisted in modifying the same CNN model and its training parameters to improve the accuracy results of the system. As in the previous case, the input layer was adapted to be able to work with the proper image size that matches its corresponding dataset (50x64 for the 1 s sample length dataset, 63 × 64 for the 1.25 s dataset and 75 × 64 for the 1.5 s dataset). Moreover, kernel sizes were reduced from 5 to 3 and the stride from 2 to 1, for a more detailed analysis of the input images, which allows the extraction of more features Fig. <ref type="figure">9</ref>. Accuracy results achieved for each dataset (1s in blue, 1.25 s in green and 1.5 s in red) per 10000 training iterations using the modified version of the LeNet-5 model. Accuracy ratios obtained after 500000 training iterations: 93.68%, 93.57% and 91.14%, respectively, which are better that the ones obtained previously. from them. Training parameters were optimized by repetition and comparison until the best results were obtained for each of the datasets.</p><p>Fig. <ref type="figure">9</ref> presents the accuracy results for every 10000 training iterations up to a total of 500000 using a base learning rate of 0.013, the inv learning policy, 0.6 as momentum and 0.000875 as weight decay. As can be seen, the 1 s dataset achieves the best result (93.68%), while the 1.25 s and the 1.5 s datasets achieve 93.57% and 91.14% accuracy ratios, respectively. The chart also shows that using smaller window length values in the segmentation step makes the network take a higher number of iterations to converge when training the CNN, due to the fact that more images are generated in the process. Se, Sp and MAcc were calculated for the 1.25 s dataset, obtaining 92.84%, 91.48% and 0.9216, respectively. Training the system took an average of four hours to complete when using the default model, and six hours (∼ 375 minutes) for the modified model, for each of the experiments and datasets with a NVIDIA GeForce GTX 1060 GPU. The first approaches were carried out using the CPU (3.2 GHz Intel i5-4460) instead of the GPU, which increased the training process execution time more than 24 hours. An average of 13.7% improvement over the default LeNet-5 model was achieved in this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Using the AlexNet Model</head><p>The same experiments that were performed using the LeNet-5 model were then tested with a more complex architecture: the AlexNet <ref type="bibr" target="#b34">[36]</ref>. The network is made up of 5 convolutional layers, max-pooling layers, dropout layers and 3 fully connected layers. It was released in 2012 by Alex Krizhevsky and scaled the insights of the LeNet-5 model into a much deeper and wider neural network that could be used to learn much more complex objects. It was used to win by a large margin the 2012 ILSVRC (ImageNet Large-Scale Visual Recognition Challenge) <ref type="bibr" target="#b40">[43]</ref>.</p><p>First, the network was trained and tested without modifying the architecture or the training parameters (only the input and output layers were adapted to accept the image sizes that are being used in this work, and to classify between two different categories). The accuracy results can be seen in Fig. <ref type="figure" target="#fig_8">10</ref>, where a base learning rate of 0.01 is used along with the step learning policy and 0.9 and 0.0005 as momentum and weight decay,  respectively. Se, Sp and MAcc were calculated for the dataset that achieved the best accuracy, obtaining 94.52%, 90.48% and 0.9250, respectively. As can be seen, the results do not differ much from the ones obtained with the modified version of the LeNet-5 model while using the default training parameters. Other learning policies like fixed and inv (which is the one that the LeNet-5 model uses) were used without modifying the rest of the network, but the results did not improve significantly. The step learning policy updates the learning rate based on the equation shown in <ref type="bibr" target="#b1">(2)</ref>, where gamma is set to 0.1 and step to 100000.</p><p>l rate = l rate * gamma ( f loor(iter/step))</p><p>(2)</p><p>In the next experiment, the AlexNet model was modified, reducing kernel sizes and the stride value for each convolutional layer. Training parameters were changed to the ones with whom the LeNet5 obtained the best results, and, after that, they were optimized by repetition and comparison. Fig. <ref type="figure" target="#fig_9">11</ref> presents the accuracy results for every 10000 training iterations up to a total of 500000 using a base learning rate of 0.013, the step learning policy, 0.6 as momentum and 0.000875 as weight decay. In this case, the 1.5 s dataset achieved the best result (97.05%), while the 1s and the 1.25 s datasets achieved 94.88% and 95.95% accuracy ratios, respectively. This could be due to the fact that training a more complex CNN like the AlexNet allows to extract more information from the 1.5 s images, which was not possible with the LeNet-5 model. Se, Sp and MAcc were calculated for the dataset that achieved the best accuracy, obtaining 95.12%, 93.20% and 0.9416, respectively.  An average of 65 hours for the default model and 107 hours for the modified model were needed to train the AlexNet CNN using the GPU. CPU was intended to be used instead of the GPU in the first place, but the training process was estimated around three months (for the version) to complete per experiment, which is an unreasonable amount of time. However, as can be seen in the images, the system converges after the first 150000 training iterations, approximately, which corresponds to 20 and 32 hours, respectively. Hence, the whole system could be trained for less than half of the iterations and obtain a very similar accuracy while spending much less time in the training process.</p><p>The modified version of the AlexNet model achieved the best results. However, it is important to point out that this CNN only improves the accuracy of the modified version of the LeNet-5 (which is a much simpler CNN model) by around 3.5%, while taking almost eighteen times the time needed to train the second one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION</head><p>In this work the authors have presented a useful tool to aid cardiologists and primary care physicians in the auscultation process. The system uses heart sound recordings from both healthy patients and pathological patients directly, which are first split using windows with a fixed length (1, 1.25 and 1.5 seconds) and then sent to a NAS where the frequency components of the audio are extracted. After this, sonogram images are generated for each of the samples using NAVIS. These images were used to feed different CNN models (LeNet-5 and AlexNet) capable of extracting interesting features from them, which have been trained and tested with different configurations in Caffe to classify between the two categories that were described.</p><p>The obtained results using different LeNet-5 and AlexNet configurations achieve up to 97.05% accuracy rate in the best case (with a modified version of the AlexNet model), and 80.00% in the worst case (with the default LeNet-5 configuration). These accuracy rates include the 80% accuracy level of an expert cardiologist (see Table <ref type="table" target="#tab_1">III</ref> for a comparative study of the obtained results), proving that the system could be very useful as an aide for cardiologists and primary care physicians in the auscultation process, reducing the number of both type-I and type-II errors made. Thereby, the authors have presented a reliable diagnostic tool that could improve the detection of pathological heart murmurs when auscultating and, by aiding the physician, achieve almost 100% accuracy between both. Also, the results have been compared in terms of sensitivity, specificity and the PhysioNet/Computing in Cardiology Challenge 2016 score (obtaining 95.12%, 93.20% and 0.9416 for the best case, respectively) to the ones of leading approaches from the competition (Se: 94.24%, Sp: 77.81%, MAcc: 0.8602, in the best case), showing a clear improvement, especially in terms of specificity.</p><p>Using a NAS in this context instead of a traditional digital audio processing approach allows us not only to achieve a very good accuracy result, but also the possibility to develop a portable diagnosis device based on the system that has been described in this paper as the next step in this line of research. This device would be fully implemented in an FPGA (see Fig. <ref type="figure" target="#fig_10">12</ref>) where a NAS, a configurable real-time segmentation and sonogram generator, and a full-custom CNN accelerator would be programmed. The input to this system would be generated by a PDM microphone that would be placed on each of the four main auscultatory areas: Aortic area, Pulmonic area, Tricuspid area, Mitral Area (Apex). The PDM microphone directly transmits the audio signal information in a spike-based codification, which would feed the NAS' input. The fact that this device uses a NAS to decompose the audio into frequency bands instead of using a Fourier Transform leads to having a lower power consumption. As it is presented in <ref type="bibr" target="#b41">[44]</ref>, a low-power radix-2 FFT accelerator for FPGA achieves a power consumption of 125 mW; however, the NAS' is only 29.7 mW <ref type="bibr" target="#b21">[22]</ref>, which is less than 24% of the power consumption of the FFT. Additionally, the NAS could interface directly with Spiking Convolutional Neural Networks (SCNN) without the need of the segmentation of the information and the sonogram generation, processing the auditory information in a continuous way. When connected to an SCNN, the system would only need to compute and classify the input signal when spikes are being fired. This means that if there is no activity in the input, the power consumption of the device would be even less. This "neuromorphic stethoscope" would also consist of a button to start the analysis and two LEDs, which would indicate the result of the CNN's classification result in real time as either healthy subject or pathological patient.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Block diagram of the system architecture.</figDesc><graphic coords="3,54.71,68.69,470.54,201.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Mono-aural Neuromorphic Auditory Sensor for FPGA with an I2S audio ADC and AER interface.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. NAS connected to an USBAERmini2.</figDesc><graphic coords="4,307.67,68.30,246.24,183.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .Algorithm 1 :</head><label>41</label><figDesc>Fig. 4. Outputs of the different preprocessing steps: the first image (a) is the original audio signal after the segmentation process; the second one (b) is the AER information obtained from the NAS' output; and the last one (c) is the grayscale sonogram image obtained with NAVIS, where a whiter tone in a specific section means that that section has more activity. (a) Audio signal. (b)Cochleogram. (c) Sonogram.</figDesc><graphic coords="5,54.83,68.81,480.20,97.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Block diagram of the LeNet-5 model architecture.</figDesc><graphic coords="6,58.07,97.61,59.57,60.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig.</head><label></label><figDesc>Fig. Block diagram of the AlexNet model architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig.7. Accuracy results achieved for each dataset (1s in blue, 1.25 s in green and 1.5 s in red) per 10000 training iterations using the default LeNet-5 model. Accuracy ratios obtained after 500000 training iterations: 82.11%, 82.39% and 80.00%, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Features learned for the two convolution layers (20 and 50 filters, respectively) with the default version of the LeNet-5 model.</figDesc><graphic coords="7,52.74,225.95,221.53,243.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Accuracy results achieved for each dataset (1s in blue, 1.25 s in green and 1.5 s in red) per 10000 training iterations using the default version of the AlexNet model. Accuracy ratios obtained after 500000 training iterations: 89.61%, 90.70% and 89.91%, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Accuracy results achieved for each dataset (1s in blue, 1.25 s in green and 1.5 s in red) per 10000 training iterations using the modified version of the AlexNet model. Accuracy ratios obtained after 500000 training iterations: 94.88%, 95.95% and 97.05%, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 .</head><label>12</label><figDesc>Fig.<ref type="bibr" target="#b11">12</ref>. Block diagram of the complete system implemented on an FPGA using a PDM microphone for real-time analysis of the heart sound directly from the patient.</figDesc><graphic coords="9,54.83,68.57,414.86,145.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE II TRAINING</head><label>II</label><figDesc>PARAMETERS AND LAYER CONFIGURATIONS FOR EACH OF THE CNNS USED</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE III ACCURACY</head><label>III</label><figDesc>, SENSITIVITY, SPECIFICITY AND PHYSIONET/CINC CHALLENGE 2016 SCORE OF THE DIFFERENT STUDIED APPROACHES</figDesc><table><row><cell></cell><cell cols="3">Accuracy Sensitivity(Se) Specificity(Sp)</cell><cell>MAcc</cell></row><row><cell>Primary care</cell><cell>40%</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>physicians</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Expert</cell><cell>80%</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>cardiologists</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>[18] Potes et al.</cell><cell>-</cell><cell>94.24%</cell><cell>77.81%</cell><cell>0.8602</cell></row><row><cell>[19] Zabihi et al.</cell><cell>-</cell><cell>86.91%</cell><cell>84.90%</cell><cell>0.8590</cell></row><row><cell>Default LeNet-5</cell><cell>82.39%</cell><cell>83.26%</cell><cell>78.58%</cell><cell>0.8092</cell></row><row><cell cols="2">Modified LeNet-5 93.68%</cell><cell>92.84%</cell><cell>91.48%</cell><cell>0.9216</cell></row><row><cell>Default AlexNet</cell><cell>90.70%</cell><cell>94.52%</cell><cell>90.48%</cell><cell>0.9250</cell></row><row><cell cols="2">Modified AlexNet 97.05%</cell><cell>95.12%</cell><cell>93.20%</cell><cell>0.9416</cell></row><row><cell cols="3">Best cases for the 1, 1.25 and 1.5 datasets are selected.</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by the Spanish government under Grant (with support from the European Regional Development Fund) COFNET (TEC2016-77785-P). The work of J. P. Dominguez-Morales was supported by a Formación de Personal Universitario Scholarship from the Spanish Ministry of Education, Culture and Sport. This paper was recommended by Associate Editor S. Renaud.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">European cardiovascular disease statistics 2012: European heart network. brussels</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nichols</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Townsend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Scarborough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Luengo-Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rayner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Soci. Cardiology, Sophia Antipolis</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Heart disease and stroke statistics-2010 update a report from the American heart association</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lloyd-Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Circulation</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="46" to="e215" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Helping family physicians improve their cardiac auscultation skills with an interactive CD-ROM</title>
		<author>
			<persName><forename type="first">D</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sargeant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hoyt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fleming</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Continuing Edu. Health Professions</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="152" to="159" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Does this patient have an abnormal systolic murmur?</title>
		<author>
			<persName><forename type="first">E</forename><surname>Etchells</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Robb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Jama</title>
		<imprint>
			<biblScope unit="volume">277</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="564" to="571" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cardiac auscultatory skills of internal medicine and family practice trainees: a comparison of diagnostic proficiency</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mangione</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Z</forename><surname>Nieman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Jama</title>
		<imprint>
			<biblScope unit="volume">278</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="717" to="722" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Factors influencing cardiac auscultation proficiency in physician trainees</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Singapore Med. J</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="14" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Detection and classification of cardiac murmurs using segmentation techniques and artificial neural networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Strunic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rios-Gutiérrez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Alba-Flores</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Nordehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bums</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Symp</title>
		<meeting>IEEE Symp</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="397" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A heart murmur detection system using spectrograms and artificial neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ejaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Nordehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Alba-Flores</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rios-Gutierrez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Andrisevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Circuits, Signals, Syst</title>
		<meeting>Int. Conf. Circuits, Signals, Syst</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="374" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recognition and classification of cardiac murmurs using ANN and segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Rios-Gutierrez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Alba-Flores</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Strunic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd Int. Conf. Electr</title>
		<meeting>22nd Int. Conf. Electr</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="219" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Classification of heart sound based on S-transform and neural network</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Hadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Mashor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Z</forename><surname>Suboh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th Int. Conf</title>
		<meeting>10th Int. Conf</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="189" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Classification of heart sounds using wavelets and neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mashor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th Int. Conf. Electr</title>
		<meeting>5th Int. Conf. Electr</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Heart sounds classification with a fuzzy neural network method with structure learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symp. Neural Netw</title>
		<meeting>Int. Symp. Neural Netw</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="130" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Heart sounds classification using feature extraction of phonocardiography signal</title>
		<author>
			<persName><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cheema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Appl</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="13" to="17" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Classification of heart sounds using time-frequency method and artificial neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Collis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Salmon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd Annul</title>
		<meeting>22nd Annul</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="988" to="991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Phonospectrographic analysis of heart murmur in children</title>
		<author>
			<persName><forename type="first">A.-L</forename><surname>Noponen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lukkarinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Angerla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sepponen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Pediatrics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="33" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic classification of systolic heart murmurs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Markaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Germanakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Stylianou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2013 IEEE Int. Conf. Acoust., Speech Signal Process</title>
		<meeting>2013 IEEE Int. Conf. Acoust., Speech Signal ess</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1301" to="1305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automated diagnosis of cardiac abnormalities using heart sounds</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Perera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Muthalif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Selvarathnam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Liyanaarachchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Nanayakkara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2013 IEEE Point-of-Care Healthcare Technol</title>
		<meeting>2013 IEEE Point-of-Care Healthcare Technol</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="252" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ensemble of featurebased and deep learning-based classifiers for detection of abnormal heart sounds</title>
		<author>
			<persName><forename type="first">C</forename><surname>Potes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parvaneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Conroy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="621" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Heart sound anomaly and quality detection using ensemble of neural networks without segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zabihi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kiranyaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gabbouj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="613" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep neural networks for identifying cough sounds</title>
		<author>
			<persName><forename type="first">J</forename><surname>Amoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Odame</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Circuits Syst</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1003" to="1011" />
			<date type="published" when="2016-10">Oct. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural network classification of homomorphic segmented heart sounds</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Palaniappan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="286" to="297" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A binaural neuromorphic auditory sensor for FPGA: A spike signal processing approach</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jiménez-Fernández</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="804" to="818" />
			<date type="published" when="2016-04">Apr. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The Adress-Event Representation communication protocol</title>
		<ptr target="https://www.ini.uzh.ch/amw/scx/std002.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The Adress-Event Representation communication protocol</title>
		<ptr target="https://www.ini.uzh.ch/∼amw/scx/std002.pdf" />
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A 5 Meps $100 USB2.0 address-event monitor-sequencer interface</title>
		<author>
			<persName><forename type="first">R</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Civit-Balcells</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Linares-Barranco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2007 IEEE Int. Symp. Circuits Syst</title>
		<meeting>2007 IEEE Int. Symp. Circuits Syst</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="2451" to="2454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd ACM Int. Conf. Multimedia, ACM</title>
		<meeting>22nd ACM Int. Conf. Multimedia, ACM</meeting>
		<imprint>
			<date type="published" when="2014-11">Nov. 2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Physiobank, physiotoolkit, and physionet components of a new research resource for complex physiologic signals</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Goldberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Circulation</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="215" to="e220" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An AER handshake-less modular infrastructure PCB with x8 2.5 Gbps LVDS serial links</title>
		<author>
			<persName><forename type="first">T</forename><surname>Iakymchuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Symp. Circuits Syst</title>
		<meeting>IEEE Int. Symp. Circuits Syst</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1556" to="1559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">NAVIS: Neuromorphic Auditory VI-Sualizer tool</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Dominguez-Morales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jimenez-Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dominguez-Morales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jimenez-Moreno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">237</biblScope>
			<biblScope unit="page" from="418" to="422" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An analog electronic cochlea</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Lyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mead</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust., Speech, Signal Process</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1119" to="1134" />
			<date type="published" when="1988-07">Jul. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Building blocks for spikes signals processing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jimenez-Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Linares-Barranco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Paz-Vicente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jiménez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Civit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. Neural Netw</title>
		<meeting>Int. Joint Conf. Neural Netw</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A 0.5 V 55 µW 64 × 2 channel binaural silicon cochlea for event-driven stereo-audio sensing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2554" to="2569" />
			<date type="published" when="2016-11">Nov. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Spikes monitors for FPGAs, an experimental comparative study</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cerezuela-Escudero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Dominguez-Morales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jiménez-Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Paz-Vicente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Linares-Barranco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jiménez-Moreno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Work-Conf. Artif. Neural Netw</title>
		<meeting>Int. Work-Conf. Artif. Neural Netw</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="179" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An open access database for the evaluation of heart sound algorithms</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physiological Meas</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2181" to="2213" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Inf</title>
		<meeting>Adv. Neural Inf</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">LeNet-5, convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/lenet" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Freehand sketch recognition using deep features</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Sarvadevabhatla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
		<idno>abs/1502.00254</idno>
		<ptr target="http://arxiv.org/abs/1502.00254" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep learning-based pipeline to recognize alzheimer&apos;s disease using fMRI data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sarraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tofighi</surname></persName>
		</author>
		<ptr target="http://www.biorxiv.org/content/early/2016/07/31/066910" />
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A sensor fusion horse gait classification by a spiking neural network on SpiNNaker</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rios-Navarro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Dominguez-Morales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tapiador-Morales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dominguez-Morales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jimenez-Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Linares-Barranco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Artif</title>
		<meeting>Int. Conf. Artif</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="36" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Classification of normal/abnormal heart sound recordings: The physionet/computing in cardiology challenge 2016</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Clifford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="609" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A low power radix-2 FFT accelerator for FPGA</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mookherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Debrunner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Debrunner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 49th Asilomar Conf. Signals, Syst. Comput</title>
		<meeting>49th Asilomar Conf. Signals, Syst. Comput</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="447" to="451" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
