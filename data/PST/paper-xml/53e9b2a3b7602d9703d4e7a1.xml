<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Learning for Efficient Discriminative Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
							<email>ronan@collobert.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IDIAP Research Institute Martigny</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Learning for Efficient Discriminative Parsing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a new fast purely discriminative algorithm for natural language parsing, based on a "deep" recurrent convolutional graph transformer network (GTN). Assuming a decomposition of a parse tree into a stack of "levels", the network predicts a level of the tree taking into account predictions of previous levels. Using only few basic text features which leverage word representations from Collobert and Weston ( <ref type="formula">2008</ref>), we show similar performance (in F 1 score) to existing pure discriminative parsers and existing "benchmark" parsers (like Collins parser, probabilistic context-free grammars based), with a huge speed advantage.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Parsing has been pursued with tremendous efforts in the Natural Language Processing (NLP) community. Since the introduction of lexicalized 1 probabilistic context-free grammar (PCFGs) parsers <ref type="bibr" target="#b24">(Magerman, 1995;</ref><ref type="bibr" target="#b7">Collins, 1996)</ref>, improvements have been achieved over the years, but generative PCFGs parsers of the last decade from <ref type="bibr">Collins (1999)</ref> and <ref type="bibr" target="#b4">Charniak (2000)</ref> still remain standard benchmarks. Given the success of discriminative learning algorithms for classical NLP tasks (Part-Of-Speech (POS) tagging, Name Entity Recognition, Chunking...), the generative nature of such parsers has been questioned. First discriminative parsing algorithms <ref type="bibr" target="#b29">(Ratnaparkhi, 1999</ref>; † Part of this work has been achieved when Ronan Collobert was at NEC Laboratories America.</p><p>1 Which leverage head words of parsing constituents.</p><p>Appearing in Proceedings of the 14 th International Conference on Artificial Intelligence and Statistics (AISTATS) 2011, Fort Lauderdale, FL, USA. Volume 15 of JMLR: W&amp;CP 15. Copyright 2011 by the authors. <ref type="bibr" target="#b12">Henderson, 2004)</ref> did not reach standard PCFG-based generative parsers. <ref type="bibr" target="#b12">Henderson (2004)</ref> outperforms Collins parser only by using a generative model and performing re-ranking. <ref type="bibr" target="#b5">Charniak and Johnson (2005)</ref> also successfully leveraged re-ranking. Pure discriminative parsers from <ref type="bibr" target="#b32">Taskar et al. (2004)</ref> and <ref type="bibr" target="#b33">Turian and Melamed (2006)</ref> finally reached Collins' parser performance, with various simple template features. However, these parsers were slow to train and were both limited to sentences with less than 15 words. Most recent discriminative parsers <ref type="bibr" target="#b15">(Finkel et al., 2008;</ref><ref type="bibr" target="#b16">Petrov and Klein, 2008)</ref> are based on Conditional Random Fields (CRFs) with PCFG-like features. In the same spirit, <ref type="bibr" target="#b17">Carreras et al. (2008)</ref> use a global-linear model (instead of a CRF), with PCFG and dependency features.</p><p>We motivate our work with the fundamental question: how far can we go with discriminative parsing, with as little task-specific prior information as possible?</p><p>We propose a fast new discriminative parser which not only does not rely on information extracted from PCFGs, but does not rely on most classical parsing features. In fact, with only few basic text features and Part-Of-Speech (POS), it performs similarly to Taskar and Turian's parsers on small sentences, and similarly to Collins' parser on long sentences.</p><p>There are two main achievements in this paper. (1) We trade the reduction of features for a "deeper" architecture, a.k.a. a particular deep neural network, which takes advantage of word representations from <ref type="bibr" target="#b9">Collobert and Weston (2008)</ref> trained on a large unlabeled corpus.</p><p>(2) We show the task of parsing can be efficiently implemented by seeing it as a recursive tagging task. We convert parse trees into a stack of levels, and then train a single neural network which predicts a "level" of the tree based on predictions of previous levels. This approach shares some similarity with the finite-state parsing cascades from <ref type="bibr" target="#b0">Abney (1997)</ref>. However, Abney's algorithm was limited to partial parsing, because each level of the tree was predicted by its own tagger: the maximum depth of the tree had to be cho- sen beforehand.</p><p>We acknowledge that training a neural network is a task which requires some experience, which differs from the experience required for choosing good parsing features in more classical approaches. From our perspective, this knowledge allows however flexible and generic architectures. Indeed, from a deep learning point of view, our approach is quite conventional, based on a convolutional neural network (CNN) adapted for text. CNNs were successful very early for tasks involving sequential data <ref type="bibr" target="#b21">(Lang and Hinton, 1988)</ref>. They have also been applied to NLP <ref type="bibr" target="#b1">(Bengio et al., 2001;</ref><ref type="bibr" target="#b9">Collobert and Weston, 2008;</ref><ref type="bibr" target="#b10">Collobert et al., 2011)</ref>, but limited to "flat" tagging problems.</p><p>We combine CNNs with a structured tag inference in a graph, the resulting model being called a Graph Transformer Network (GTN) <ref type="bibr" target="#b3">(Bottou et al., 1997)</ref>. Again, this is not a surprising architecture: GTNs are for deep models what CRFs are for linear models <ref type="bibr">(Lafferty et al., 2001)</ref>, and CRFs had great success in NLP <ref type="bibr" target="#b31">(Sha and Pereira, 2003;</ref><ref type="bibr" target="#b26">McCallum and Li, 2003;</ref><ref type="bibr" target="#b6">Cohn and Blunsom, 2005)</ref>. We show how GTNs can be adapted to parsing, by simply constraining the inference graph at each parsing level prediction.</p><p>In Section 2 we describe how we convert trees to (and from) a stack of levels. Section 3 describes our GTN architecture for text. Section 4 shows how to implement necessary constraints to get a valid tree from a level decomposition. Evaluation of our system on standard benchmarks is given in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Parse Trees</head><p>We consider linguistic parse trees as described in Figure <ref type="figure" target="#fig_0">1a</ref>. The root spans all of the sentence, and is recursively decomposed into sub-constituents (the nodes of the tree) with labels like NP (noun phrase), VP (verb phrase), S (sentence), etc. The tree leaves contain the sentence words. All our experiments were performed using the Penn Treebank dataset <ref type="bibr" target="#b25">(Marcus et al., 1993)</ref>, on which we applied several standard pre-processing steps: (1) functional labels as well as traces were removed (2) the label PRT was converted into ADVP (see <ref type="bibr" target="#b24">Magerman, 1995)</ref> (3) duplicate constituents (spanning the same words and with the same label) were removed. The resulting dataset contains 26 different labels, that we will denote L in the rest of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Parse Tree Levels</head><p>Many NLP tasks involve finding chunks of words in a sentence, which can be viewed as a tagging task. For instance, "Chunking" is a task related to parsing, where one wants to obtain the label of the lowest parse tree node in which each word ends up. For the tree in Figure <ref type="figure" target="#fig_0">1a</ref>, the pairs word/chunking tags could be written as: But/O stocks/S-NP kept/B-VP falling/E-VP. We chose here to adopt the IOBES tagging scheme to mark chunk boundaries. Tag "S-NP" is used to mark a noun phrase containing a single word.</p><p>Otherwise tags "B-NP", "I-NP", and "E-NP" are used to mark the first, intermediate and last words of the noun phrase. An additional tag "O" marks words that are not members of a chunk.</p><p>As illustrated in Figure <ref type="figure" target="#fig_0">1c</ref> and Figure <ref type="figure">2</ref>, one can rewrite a parse tree as a stack of tag levels. We achieve this tree conversion by first transforming the lowest nodes of the parse tree into chunk tags ('Level 1").</p><p>Tree nodes which contain sub-nodes are ignored at this stage<ref type="foot" target="#foot_0">2</ref> . Words not into one of the lowest nodes are tagged as "O". We then strip the lowest nodes of the tree, and apply the same principle for "Level 2". We repeat the process until one level contains the root node. We chose a bottom-up approach because one can rely very well on lower level predictions: the chunking task, which describes in an other way the lowest parse tree nodes, has a very good performance record <ref type="bibr" target="#b31">(Sha and Pereira, 2003)</ref>.  <ref type="figure" target="#fig_0">1a</ref>, rewritten as four levels of tagging tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">From Tagging Levels To Parse Trees</head><p>Even if it had success with partial parsing <ref type="bibr" target="#b0">(Abney, 1997)</ref>, the simplest scheme where one would have a different tagger for each level of the parse tree is not attractive in a full parsing setting. The maximum number of levels would have to be chosen at train time, which limits the maximum sentence length at test time. Instead, we propose to have a unique tagger for all parse tree levels:</p><p>1. Our tagger starts by predicting Level 1.</p><p>2. We then predict next level according to a history of previous levels, with the same tagger.</p><p>3. We update the history of levels and go to 2.</p><p>This setup fits naturally into the recursive definition of the levels. However, we must insure the predicted tags correspond to a parse tree. In a tree, a parent node fully includes child nodes. Without constraints during the level predictions, one could face a chunk partially spanning another chunk at a lower level, which would break this tree constraint.</p><p>We can guarantee that the tagging process corresponds to a valid tree, by adding a constraint enforcing higher level chunks to fully include lower level chunks. This iterative process might however never end, as it can be subject to loops: for instance, the constraint is still satisfied if the tagger predicts the same tags for two consecutive levels. We propose to tackle this problem by (a) modifying the training parse trees such that nodes grow strictly as we go up in the tree and (b) enforcing the corresponding constraints in the tagging process.</p><p>Tree nodes spanning the same words for several consecutive level are first replaced by one node in the whole training set. The label of this new node is the concatenation of replaced node labels (see Figure <ref type="figure" target="#fig_0">1b</ref>). At test time, the inverse operation is performed on nodes having concatenated labels. Considering all possible label combinations would be intractable<ref type="foot" target="#foot_1">3</ref> . We kept in the training set concatenated labels which were occurring at least 30 times (corresponding to the lowest number of occurrences of the less common non-concatenated tag). This added 14 extra labels to the 26 we already had. Adding the extra O tag and using the IOBES tagging scheme 4 led us to 161 ((26 + 14) × 4 + 1) different tags produced by our tagger. We denote T this ensemble of tags.</p><p>With this additional pre-processing, any tree node is strictly larger (in terms of words it spans) than each of its children. We enforce the corresponding Constraint 1 during the iterative tagging process.</p><p>Constraint 1 Any chunk at level i overlapping a chunk at level j &lt; i must span at least this overlapped chunk, and be larger.</p><p>As a result, the iterative tagging process described above will generate a chunk of size N in at most N levels, given a sentence of N words. At this time, the iterative loop is stopped, and the full tree can be deduced.</p><p>The process might also be stopped if no new chunks were found (all tags were O). Assuming our simple tree pre-processing has been done, this generic algorithm could be used with any tagger which could handle a history of labels and tagging constraints. Even though the tagging process is greedy because there is no global inference of the tree, we will see in Section 5 that it can perform surprisingly well. We propose in the next section a tagger based on a convolutional Graph Transformer Network (GTN) architecture. We will see in Section 4 how we keep track of the history and how we implement Constraint 1 for that tagger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Architecture</head><p>We chose to use a variant of the versatile convolutional neural network architecture first proposed by <ref type="bibr" target="#b1">Bengio et al. (2001)</ref> for language modeling, and reintroduced later by <ref type="bibr" target="#b9">Collobert and Weston (2008)</ref> for various NLP tasks involving tagging. Our network outputs a graph over which inference is achieved with a Viterbi algorithm. In that respect, one can see the whole architecture (see Figure <ref type="figure">3</ref>) as an instance of GTNs <ref type="bibr" target="#b3">(Bottou et al., 1997;</ref><ref type="bibr" target="#b22">Le Cun et al., 1998)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text</head><p>The cat sat on the mat Feature 1</p><formula xml:id="formula_0">w 1 1 w 1 2 . . . w 1 N . . . Feature K w K 1 w K 2 . . . w K N LT W 1 . . . LT W K B-NP I-NP . . . O D Padding Padding |T | M 2 h(M 1 •) Aij Figure 3: Our neural network architecture.</formula><p>Words and other desired discrete features (caps, tree history, ...) are given as input. The lookup-tables embed each feature in a vector space, for each word. This is fed in a convolutional network which outputs a score for each tag and each word. Finally, a graph is output with network scores on the nodes and additional transition scores on the edges. A Viterbi algorithm can be performed to infer the word tags.</p><p>derivations are provided in the supplementary material attached to this paper. We will show in Section 4 how one can further adapt this architecture for parsing, by introducing a tree history feature and few graph constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Word Embeddings</head><p>We consider a fixed-sized word dictionary 5 W. Given a sentence of N words {w 1 , w 2 , . . . , w N }, each word w n ∈ W is first embedded into a D-dimensional vector space, by applying a lookup-table operation:</p><formula xml:id="formula_1">LT W (w n ) = W 0, • • • 0, 1 at index wn , 0, • • • 0 T = W wn ,<label>(1)</label></formula><p>5</p><p>Unknown words are mapped to a special unknown word. Also, we map numbers to a number word.</p><p>where the matrix W ∈ R D×|W| represents the parameters to be trained in this lookup layer. Each column W n ∈ R D corresponds to the embedding of the n th word in our dictionary W.</p><p>Having in mind the matrix-vector notation in (1), the lookup-table applied over the sentence can be seen as an efficient implementation of a convolution with a kernel width of size 1. Parameters W are thus initialized randomly and trained as any other neural network layer. However, we show in the experiments that one can obtain a significant performance boost by initializing<ref type="foot" target="#foot_2">6</ref> these embeddings with the word representations found by <ref type="bibr" target="#b9">Collobert and Weston (2008)</ref>. These representations have been trained on a large unlabeled corpus (Wikipedia), using a language modeling task. They contain useful syntactic and semantic information, which appears to be useful for parsing. This corroborates improvements obtained in the same way by Collobert &amp; Weston on various NLP tagging tasks.</p><p>In practice, it is common that one wants to represent a word with more than one feature. In our experiments we always took at least the low-caps words and a "caps" feature: w n = (w lowcaps n , w caps n ). In this case, we apply a different lookup-table for each discrete feature (LT W lowcaps and LT W caps ), and the word embedding becomes the concatenation of the output of all these lookup-tables:</p><formula xml:id="formula_2">LT W words (w n ) = LT W lowcaps (w lowcaps n ) T , LT W caps (w caps n )) T .</formula><p>(2)</p><p>For simplicity, we consider only one lookup-table in the rest of the architecture description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Word Scoring</head><p>Scores for all tags T and all words in the sentence are produced by applying a classical convolutional neural network over the lookup-table embeddings (1). More precisely, we consider all successive windows of text (of size K), sliding over the sentence, from position 1 to N . At position n, the the network is fed with the vector x n resulting from the concatenation of the embeddings:</p><formula xml:id="formula_3">x n = W T w n−(K−1)/2 , . . . , W T w n+(K−1)/2</formula><p>T .</p><p>The words with indices exceeding the sentence boundaries (n − (K − 1)/2 &lt; 1 or n + (K − 1)/2 &gt; N) are mapped to a special padding word. As any classical neural network, our architecture performs several matrix-vector operations on its inputs, interleaved with some non-linear transfer function h(•). It outputs a vector of size |T | for each word at position n, interpreted as a score for each tag in T and each word w n in the sentence:</p><formula xml:id="formula_4">s(x n ) = M 2 h(M 1 x n ) ,<label>(3)</label></formula><p>where the matrices M 1 ∈ R H×(KD) and M 2 ∈ R |T |×H are the trained parameters of the network. The number of hidden units H is a hyper-parameter to be tuned. As transfer function, we chose in our experiments a (fast) "hard" version of the hyperbolic tangent:</p><formula xml:id="formula_5">h(x) =    −1 if x &lt; −1 x if − 1 &lt;= x &lt;= 1 1 if x &gt; 1 . (<label>4</label></formula><formula xml:id="formula_6">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Long-Range Dependencies</head><p>The "window" approach proposed above assume that the tag of a word is solely determined by the surrounding words in the window. As we will see in our experiments, this approach falls short on long sentences. Inspired by <ref type="bibr" target="#b9">Collobert and Weston (2008)</ref>, we consider a variant of this architecture, where all words {w 1 , w 2 , . . . , w N } are considered for tagging a given word w n . To indicate to the network that we want to tag the word w n , we introduce an additional lookuptable in (2), which embeds the relative distance (m−n) of each word w m in the sentence with respect to w n . At each position 1 ≤ m ≤ N , the outputs of the all lookup-tables (2) (low caps word, caps, relative distance...) LT W words (w m ) are first combined together by applying a mapping M 0 . We then extract a fixed-size "global" feature vector 7 x n by performing a max over the sentence:</p><formula xml:id="formula_7">[x n ] i = max 1≤m≤N M 0 LT W words (w m ) i ∀i<label>(5)</label></formula><p>This feature vector is then fed to scoring layers (3).</p><p>The matrix M 0 is trained by back-propagation, as any other network parameter. We will refer this approach as "sentence approach" in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Structured Tag Inference</head><p>We know that there are strong dependencies between parsing tags in a sentence: not only are tags organized in chunks, but some tags cannot follow other tags. It is thus natural to infer tags from the scores in (3) using a structured output approach. We introduce a transition 7</p><p>Here, the concatenation of lookup-tables outputs LT W words includes relative position embeddings with respect to word n. Because of this notation shortcut, the right-hand side of (5) depends on n implicitly. score A tu for jumping from tag t ∈ T to u ∈ T in successive words, and an initial score A t0 for starting from the t th tag. The last layer of our network outputs a graph with |T | × N nodes G tn (see Figure <ref type="figure">3</ref>). Each node G tn is assigned a score s(x n )</p><p>t from the previous layer (3) of our architecture. Given a pair of nodes G tn and G u(n+1) , we add an edge with transition score A tu on the graph. For compactness, we use the sequence notation [t] N 1 ∆ = {t 1 , . . . , t n } for now. We score a tag path [t] N 1 in the graph G, as the sum of scores along</p><formula xml:id="formula_8">[t] N 1 in G: S([w] N 1 , [t] N 1 , θ) = N n=1 A tn−1tn + s(x n ) tn ,<label>(6)</label></formula><p>where θ represents all the trainable parameters of our complete architecture (W , M 1 , M 2 and A). The sentence tags [t ] N 1 are then inferred by finding the path which leads to the maximal score:</p><formula xml:id="formula_9">[t ] N 1 = argmax [t] N 1 ∈T N S([w] N 1 , [t] N 1 , θ) . (<label>7</label></formula><formula xml:id="formula_10">)</formula><p>The <ref type="bibr" target="#b34">Viterbi (1967)</ref> algorithm is the natural choice for this inference. We will show now how to train all the parameters of the network θ in a end-to-end way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Training Likelihood</head><p>Following the GTN's training method introduced in <ref type="bibr" target="#b3">(Bottou et al., 1997;</ref><ref type="bibr" target="#b22">Le Cun et al., 1998)</ref>, we consider a probabilistic framework, where we maximize a likelihood over all the sentences [w] N 1 in our training set, with respect to θ. The score (6) can be interpreted as a conditional probability over a path by taking it to the exponential (making it positive) and normalizing with respect to all possible paths (summing to 1 over all paths). Taking the log(•) leads to the following conditional log-probability:</p><formula xml:id="formula_11">log p([t] N 1 | [w] N 1 , θ) = S([w] N 1 , [t] N 1 , θ) − logadd ∀[u] N 1 ∈T N S([w] N 1 , [u] N 1 , θ) , (<label>8</label></formula><formula xml:id="formula_12">)</formula><p>where we adopt the notation logadd i z i = log( i e zi ). This likelihood is the same as the one found in Conditional Random Fields (CRFs) <ref type="bibr">(Lafferty et al., 2001)</ref> over temporal sequences. The CRF model is however linear (which would correspond in our case to a linear neural network, with fixed word embeddings).</p><p>Computing the log-likelihood (8) efficiently is not straightforward, as the number of terms in the logadd grows exponentially with the length of the sentence. Fortunately, in the same spirit as the Viterbi algorithm, one can compute it in linear time with the fol-lowing classical recursion over n:</p><formula xml:id="formula_13">δ n (v) ∆ = logadd {[u] n 1 ∩ un=v} S([w] n 1 , [u] n 1 , θ) ∀v ∈ T = s(x n ) v + logadd t (δ n−1 (t) + A tv ) ,<label>(9)</label></formula><p>followed by the termination logadd</p><formula xml:id="formula_14">∀[u] N 1 S([w] N 1 , [u] N 1 , θ) = logadd u δ N (u) .</formula><p>As a comparison, the Viterbi algorithm used to perform the inference ( <ref type="formula" target="#formula_9">7</ref>) is achieved with the same recursion, but where the logadd is replaced by a max, and then tracking back the optimal path through each max.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Stochastic Gradient</head><p>We maximize the log-likelihood (8) using stochastic gradient ascent, which has the main advantage to be extremely scalable <ref type="bibr" target="#b2">(Bottou, 1991)</ref>. Random training sentences [w] N 1 and their associated tag labeling [t] N 1 are iteratively selected. The following gradient step is then performed:</p><formula xml:id="formula_15">θ ←− θ + λ ∂ log p([t] N 1 | [w] N 1 , θ) ∂θ ,<label>(10)</label></formula><p>where λ is a chosen learning rate. The gradient in ( <ref type="formula" target="#formula_15">10</ref>) is efficiently computed via a classical backpropagation <ref type="bibr" target="#b30">(Rumelhart et al., 1986)</ref>: the differentiation chain rule is applied to the recursion (9), and then to all network layers (3), including the word embedding layers (1). Derivations are simple (but fastidious) algebra which can be found in the supplementary material of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Chunk History and Tree Constraints</head><p>The neural network architecture we presented in Section 3 is made "recursive" by adding an additional feature (and its corresponding lookup-table (1)) describing a history of previous tree levels. For that purpose, we gather all chunks which were discovered in previous tree levels. If several chunks were overlapping at different levels, we consider only the largest one. Assuming Constraint 1 is true, a word can be at most in one of the remaining chunks. This is our history 8 C. The corresponding IOBES tags of each word will be fed as feature to the GTN. For instance, assuming the labeling in Figure <ref type="figure">2</ref> was found up to Level 3, 8 Some other kind of history could have been chosen (e.g. a feature for each arbitrary chosen L ∈ N previous levels). However we still need to "compute" the proposed history for implementing Constraint 1.</p><p>the chunks we would consider in C for tagging Level 4 would be only the NP around "stocks" and the VP around "kept falling". We would discard the S and VP around "falling" as they are included by the larger VP chunk.</p><p>We now implement Constraint 1 by constraining the inference graph introduced in Section 3.4 using the chunk history C. For each chunk c ∈ C, we adapt the graph output by our network in Figure <ref type="figure">3</ref> such that any new candidate chunk c overlapping c includes c, and is strictly larger than c. Because the chunk history C includes the largest chunks up to the last predicted tree level, the new candidate chunk c will be strictly larger than any chunk predicted in previous tree levels. Constraint 1 is then always satisfied.</p><p>Constraining the inference graph can be achieved by noticing that the condition "c strictly includes c" is equivalent to say that the new chunk c satisfies one of the following conditions:</p><p>• Starts at the same position but ends after c</p><p>• Starts before c, and ends at the same position</p><p>• Starts before and ends after c.</p><p>Using a IOBES tagging scheme, we implement (see Figure <ref type="figure">4</ref> In addition to these 3 × |L| possible paths overlapping c, there is an additional path where no chunk is found over c, in which case all tags stay in O while overlapping c. Finally, as c must be strictly larger than c, any S-tag is discarded for the duration of c. Parts of the graph not overlapping with the chunk history C remain fully connected, as previously described in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We conducted our experiments on the standard English Penn Treebank benchmark <ref type="bibr" target="#b25">(Marcus et al., 1993)</ref>. Sections 02-21 were used for training, section 22 for validation, and section 23 for testing. Standard preprocessing as described in Section 2 was performed. In addition, the training set trees were transformed such that two nodes spanning the same words were concatenated as described in Section 2.2. We report results on the test set in terms of recall (R), precision (P ) and F 1 score. Scores were obtained using the Evalb implementation 9 .</p><p>Our architecture (see Section 3) was trained on all possible parse tree levels (see Section 2.1), for all sentences available in the training set. Random levels in random sentences were presented to the network until convergence on the validation set. We fed our network with (1) lower cap words (to limit the number of words),</p><p>(2) a capital letter feature (is low caps, is all caps, had first letter capital, or had one capital) to keep the upper case information (3) the relative distance to the word of interest (only for the "sentence approach") (4) a POS feature 10 (unless otherwise mentioned) (5) the history of previous levels (see Section 4). During training, the true history was given. During testing the history and the tags were obtained recursively from the network outputs, starting from Level 1, (see Section 2.2). All features had a corresponding lookuptable (1) in the network.</p><p>Only few hyper-parameters were tried in our models (chosen according to the validation  sizes for the low cap words, caps, POS, relative distance (in the "sentence approach") and history features were respectively 50, 5, 5, 5 and 10. The window size of our convolutional network was K = 5. The word dictionary size was 100, 000. We used the word embeddings obtained from the language model (LM) <ref type="bibr" target="#b9">(Collobert and Weston, 2008)</ref> to initialize the word lookup-table. Finally, we fixed the learning rate λ = 0.01 during the stochastic gradient procedure (10). The only neural network "tricks" we used were (1) the initialization of the parameters was done according to the fan-in, and (2) the learning rate was divided by the fan-in <ref type="bibr" target="#b28">(Plaut and Hinton, 1987)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Small Scale Experiments</head><p>First discriminative parse trees were very computationally expensive to train. <ref type="bibr" target="#b32">Taskar et al. (2004)</ref> proposed a comparison setup for discriminative parsers limited to Penn Treebank sentences with ≤ 15 words. <ref type="bibr" target="#b33">Turian and Melamed (2006)</ref> reports almost 5 days of training for their own parser, using parallelization, on this setup. They also report several months of training for Taskar et al.'s parser. In comparison, our parser takes only few hours to train (on a single CPU) on this setup. We report in Table <ref type="table" target="#tab_4">1</ref> test performance of our window approach system ("GTN Parser", with H = 300 hidden units) against Taskar and Turian's discriminative parsers. We also report performance of <ref type="bibr">Collins (1999)</ref> parser, a reference in non-discriminative parsers. Not initializing the word lookup table with the language model (LM) and not using POS features performed poorly, similar to experiments reported by <ref type="bibr" target="#b9">Collobert and Weston (2008)</ref>. It is known that POS is an fundamental feature for all existing parsers. The LM is crucial for the performance of the architecture, as most of the capacity of the network lies into the word lookup-table (100, 000 words × dimension 50). Without the LM, rare words cannot be properly trained.<ref type="foot" target="#foot_3">11</ref> Initializing with the LM but not using POS or using POS but not LM gave similar improvements in performance. Combining LM and POS compares well with other parsers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Large Scale Experiments</head><p>We also trained ( Table <ref type="table" target="#tab_5">2</ref>) our GTN parsers (both the "window" and "sentence" approach) on the full Penn Treebank dataset. Both takes a few days to train on a single CPU in this setup. The number of hidden units was set to H = 700. The size of the embedding space obtained with M 0 in the "sentence approach" was 300.</p><p>Our "window approach" parser compares well against the first lexical PCFG parsers: <ref type="bibr" target="#b24">Magerman (1995)</ref> and <ref type="bibr" target="#b7">Collins (1996)</ref>. The "sentence approach" (leveraging long-range dependencies) provides a clear boost and compares well against Collins (1999) parser<ref type="foot" target="#foot_4">12</ref> , a standard benchmark in NLP. More refined parsers like <ref type="bibr" target="#b5">Charniak &amp; Johnson (2005)</ref> (which takes advantage of re-ranking) or recent discriminative parsers (which are based on PCFGs features) have higher F 1 scores.</p><p>Our parser performs comparatively well, considering we only used simple text features. Finally, we report some timing results on Penn Treebank test set (many implementations are not available). The GTN parser was an order of magnitude faster than other available parsers<ref type="foot" target="#foot_5">13</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We proposed a new fast and scalable purely discriminative parsing algorithm based on Graph Transformer Networks. With only few basic text features (thanks to word representations from <ref type="bibr" target="#b9">Collobert and Weston (2008)</ref>), it performs similarly to existing pure discriminative algorithms, and similarly to <ref type="bibr">Collins (1999)</ref> "benchmark" parser. Many paths remain to be explored: richer features (in particular head words, as do lexicalized PCFGs), combination with generative parsers, less greedy bottom-up inference (e.g. using Kbest decoding), or other alternatives to describe trees.  <ref type="bibr" target="#b24">Magerman (1995)</ref> 84.6 84.9 84.8 <ref type="bibr" target="#b7">Collins (1996)</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Parse Tree representations. As in Penn Treebank (a), and after concatenating nodes spanning same words (b). In (c) we show our definition of "levels".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>) these three conditions by allowing only three corresponding possible paths c in the inference graph, for each candidate label (e.g. VP): • The first tag of c is B-VP, and remaining tags overlapping with c are maintained at I-VP • The last tag of c is E-VP, and previous tags overlapping with c are maintained at I-VP • The path c is maintained on I-VP while overlapping c.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Comparison of parsers trained and tested on Penn Treebank, on sentences ≤ 15 words, against our GTN parser (window approach).</figDesc><table><row><cell>Model</cell><cell>R</cell><cell>P</cell><cell>F 1</cell></row><row><cell>Collins (1999)</cell><cell cols="3">88.2 89.2 88.7</cell></row><row><cell>Taskar et al. (2004)</cell><cell cols="3">89.1 89.1 89.1</cell></row><row><cell cols="4">Turian and Melamed (2006) 89.3 89.6 89.4</cell></row><row><cell>GTN Parser</cell><cell cols="3">82.4 82.8 82.6</cell></row><row><cell>GTN Parser (LM)</cell><cell cols="3">86.1 87.2 86.6</cell></row><row><cell>GTN Parser (POS)</cell><cell cols="3">87.1 86.2 86.7</cell></row><row><cell>GTN Parser (LM+POS)</cell><cell cols="3">89.2 89.0 89.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Parsers comparison trained on the full Penn Treebank, and tested on sentences with ≤ 40 and ≤ 100 words. We also report testing time on the test set (Section 23).</figDesc><table><row><cell>R</cell><cell>≤ 40 Words P F 1</cell><cell>≤ 100 Words R P F 1</cell><cell>Test Time (sec.)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">E.g. in Figure1a, "kept" is not tagged as "S-VP" in Level 1, as the node "VP" still contains sub-nodes "S" and "VP" above "falling".</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">Note that more than two labels might be concatenated. E.g., the tag SBAR#S#VP is quite common in the training set.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2">Only the initialization differs. The parameters are trained in any case.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_3">About 15% of the most common words appear 90% of the time, so many words are rare.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_4">We picked Bikel's implementation available at http: //www.cis.upenn.edu/ ~dbikel.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_5">Available at http://ml.nec-labs.com/senna.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The author would like to thank NEC Laboratories America for its support, as well as Léon Bottou and Vincent Etter for their comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Partial parsing via finite-state cascades</title>
		<author>
			<persName><forename type="first">S</forename><surname>Abney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="337" to="344" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 13</title>
				<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Stochastic gradient learning in neural networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neuro-Nîmes 91</title>
				<meeting>Neuro-Nîmes 91<address><addrLine>Nimes, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991">1991. EC2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tag, dynamic programming, and the perceptron for efficient, feature-rich parsing</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">X</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Koo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="1997">1997. 2008</date>
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
	<note>CoNLL &apos;08</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A maximum-entropy-inspired parser</title>
		<author>
			<persName><forename type="first">E</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the first conference on North American chapter</title>
				<meeting>the first conference on North American chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="132" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Coarse-to-fine n-best parsing and maxent discriminative reranking</title>
		<author>
			<persName><forename type="first">E</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on ACL</title>
				<meeting>the 43rd Annual Meeting on ACL</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semantic role labelling with tree conditional random fields</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ninth Conference on Computational Natural Language (CoNLL)</title>
				<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A new statistical parser based on bigram lexical dependencies</title>
		<author>
			<persName><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th annual meeting on ACL</title>
				<meeting>the 34th annual meeting on ACL</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="184" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Head-Driven Statistical Models for Natural Language Parsing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Natural language processing (almost) from scratch. JMLR</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kuksa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient, feature-based, conditional random field parsing</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kleeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
				<meeting>ACL-08: HLT</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2008-06">June 2008</date>
			<biblScope unit="page" from="959" to="967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Discriminative training of a neural network statistical parser</title>
		<author>
			<persName><forename type="first">J</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting on ACL</title>
				<meeting>the 42nd Annual Meeting on ACL</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Conditional random fields: Probabilistic models for segmenting 85</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<idno>86.3 86.1 85.3 85.7 85.5</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName><surname>Collins</surname></persName>
		</author>
		<idno>88.5 88.7 88.6 88.1 88.3 88.2 2640 Charniak (2000) 90.1 90.1 90.1 89.6 89.5 89.6</idno>
		<imprint>
			<date type="published" when="1020">1999. 1020. 2005</date>
			<publisher>Charniak and Johnson</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><surname>Finkel</surname></persName>
		</author>
		<idno>89.2 89.0 87.8 88.2 88.0</idno>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page">88</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Klein</forename><surname>Petrov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName><surname>Carreras</surname></persName>
		</author>
		<idno>91.1 90.5</idno>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page">89</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName><surname>Gtn Parser</surname></persName>
		</author>
		<idno>81.3 81.9 81.6 80.3 81.0 80.6</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<idno>LM+POS) 85.6 86.8 86.2 84.8 86.2 85.5</idno>
		<title level="m">GTN Parser</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">76 and labeling sequence data</title>
		<idno>GTN Parser (sentence, LM+POS) 88.1 88.8 88.5 87.5 88.3 87.9</idno>
	</analytic>
	<monogr>
		<title level="m">Eighteenth International Conference on Machine Learning, ICML</title>
				<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The development of the time-delay neural network architecture for speech recognition</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>CMU-CS-88-152</idno>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gradient based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Le Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A learning scheme for asymmetric threshold networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Cognitiva 85</title>
				<meeting>Cognitiva 85<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1985">1985</date>
			<biblScope unit="page" from="599" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Statistical decision-tree models for parsing</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Magerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual Meeting of the ACL</title>
				<meeting>the 33rd Annual Meeting of the ACL</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="276" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of English: the penn treebank</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL</title>
				<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="188" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sparse multi-scale grammars for discriminative latent variable parsing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP &apos;08</title>
				<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="867" to="876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning sets of filters using back-propagation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Plaut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="35" to="61" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to parse natural language with maximum entropy models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ratnaparkhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
				<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="151" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning internal representations by backpropagating errors</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parallel Distributed Processing: Explorations in the Microstructure of Cognition</title>
				<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1986">1986</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="318" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Shallow parsing with conditional random fields</title>
		<author>
			<persName><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL 2003</title>
				<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="134" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Max-margin parsing</title>
		<author>
			<persName><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Advances in discriminative parsing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">D</forename><surname>Melamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the ACL</title>
				<meeting>the 21st International Conference on Computational Linguistics and the 44th annual meeting of the ACL</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="873" to="880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Error bounds for convolutional codes and an asymptotically optimal decoding algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Viterbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="260" to="269" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
