<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">From Local Structures to Size Generalization in Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Gilad</forename><surname>Yehudai</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ethan</forename><surname>Fetaya</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Eli</forename><surname>Meirom</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Gal</forename><surname>Chechik</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Haggai</forename><surname>Maron</surname></persName>
						</author>
						<title level="a" type="main">From Local Structures to Size Generalization in Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural networks (GNNs) can process graphs of different sizes, but their ability to generalize across sizes, specifically from small to large graphs, is still not well understood. In this paper, we identify an important type of data where generalization from small to large graphs is challenging: graph distributions for which the local structure depends on the graph size. This effect occurs in multiple important graph learning domains, including social and biological networks. We first prove that when there is a difference between the local structures, GNNs are not guaranteed to generalize across sizes: there are "bad" global minima that do well on small graphs but fail on large graphs. We then study the size-generalization problem empirically and demonstrate that when there is a discrepancy in local structure, GNNs tend to converge to nongeneralizing solutions. Finally, we suggest two approaches for improving size generalization, motivated by our findings. Notably, we propose a novel Self-Supervised Learning (SSL) task aimed at learning meaningful representations of local structures that appear in large graphs. Our SSL task improves classification accuracy on several popular datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Graphs are a flexible representation, widely used for representing diverse data and phenomena. In recent years, graph neural networks (GNNs), deep models that operate over graphs, have become the leading approach for learning on graph-structured data <ref type="bibr" target="#b4">(Bruna et al., 2013;</ref><ref type="bibr" target="#b24">Kipf &amp; Welling, 2016a;</ref><ref type="bibr" target="#b45">Veličković et al., 2017;</ref><ref type="bibr" target="#b15">Gilmer et al., 2017)</ref>.</p><p>In many domains, graphs vary significantly in size. This is the case in molecular biology, where molecules are represented as graphs and their sizes vary from few-atom com-Figure <ref type="figure">1</ref>. We study the ability of GNNs to generalize from small to large graphs, focusing on graphs in which the local structure depends on the graph size. The figure shows two graph distributions that differ in size and degree distribution. We show that when the local structures in the test set are different from the local structures in the training set, it is difficult for GNNs to generalize. Additionally, we suggest ways to improve generalization. pounds to proteins with thousands of nodes. Graph sizes are even more heterogeneous in social networks, ranging from dozens of nodes to billions of nodes. Since a key feature of GNNs is that they can operate on graphs regardless of their size, a fundamental question arises: "When do GNNs generalize to graphs of sizes that were not seen during training?".</p><p>Aside from being an intriguing theoretical question, the size-generalization problem has important practical implications. In many domains, it is hard to collect ground-truth labels for large graphs. For instance, many combinatorial optimization problems can be represented as graph classification problems, but labeling large graphs for training may require solving large and hard optimization problems. In other domains, it is often very hard for human annotators to correctly label complex networks. It would therefore be highly valuable to develop techniques that can train on small graphs and generalize to larger graphs. This first requires that we develop an understanding of size generalization.</p><p>In some cases, GNNs can naturally generalize to graphs whose size is different from what they were trained on, but it is largely unknown when such generalization occurs. Empirically, several papers report good size-generalization performance <ref type="bibr" target="#b27">(Li et al., 2018;</ref><ref type="bibr" target="#b30">Luz et al., 2020;</ref><ref type="bibr" target="#b40">Sanchez-Gonzalez et al., 2020)</ref>. Other papers <ref type="bibr">(Velickovic et al., arXiv:2010.08853v3 [cs.</ref>LG] 15 Jul 2021 2019; <ref type="bibr" target="#b23">Khalil et al., 2017;</ref><ref type="bibr" target="#b22">Joshi et al., 2020)</ref> show that size generalization can be hard. <ref type="bibr">Recently, Xu et al. (2020)</ref> provided theoretical evidence of size generalization capabilities in one-layer GNNs.</p><p>The current paper characterizes an important type of graph distributions where size generalization is challenging. Specifically, we analyze graphs for which the distribution of local structures (defined formally in Sec. 4) depends on the size of the graph. See Fig. <ref type="figure">1</ref> for an illustrative example. This dependency is prevalent in a variety of graphs, including for instance, the preferential attachment (PA) model <ref type="bibr" target="#b1">(Barabási &amp; Albert, 1999)</ref>, which captures graph structure in social networks <ref type="bibr" target="#b2">(Barabâsi et al., 2002)</ref>, biological networks <ref type="bibr" target="#b11">(Eisenberg &amp; Levanon, 2003;</ref><ref type="bibr" target="#b29">Light et al., 2005)</ref> and internet link data <ref type="bibr" target="#b6">(Capocci et al., 2006)</ref>. In PA, the maximal node degree grows with the graph size. As a second example, in a graph representation of dense point clouds, the node degree grows with the cloud density, and hence with the graph size <ref type="bibr" target="#b17">(Hermosilla et al., 2018)</ref>.</p><p>To characterize generalization to new graph sizes, we first formalize a representation of local structures that we call d-patterns, inspired by <ref type="bibr">(Weisfeiler &amp; Lehman, 1968;</ref><ref type="bibr">Morris et al., 2019;</ref><ref type="bibr">Xu et al., 2018)</ref>. d-patterns generalize the notion of node degrees to a d-step neighborhood of a given node, capturing the values of a node and its d-step neighbors, as seen by GNNs. We then prove that even a small discrepancy in the distribution of d-patterns between the test and train distributions may result in weight assignments that do not generalize well. Specifically, it implies that when training GNNs on small graphs there exist "bad" global minima that fail to generalize to large graphs.</p><p>We then study empirically the relation between size generalization and d-pattern discrepancy in synthetic graphs where we control the graph structure and size. We find that as d-pattern discrepancy grows, the generalization of GNNs to new graph sizes deteriorates. Finally, we discuss approaches for improving size generalization. We take a self-supervised learning approach and propose a novel pretext task aimed at learning useful dpattern representations from both small and large graphs. We show that when training on labeled small graphs and with our new self-supervised task on large graphs, classification accuracy increases on large graphs by 4% on average on real datasets. This paper makes the following contributions: (1) We identify a family of important graph distributions where size generalization is difficult, using a combination of theoretical and empirical results. (2) We suggest approaches for improving size generalization when training on such distributions and show that they lead to a noticeable performance gain. The ideas presented in this paper can be readily extended to other graph learning setups where there is a discrepancy between the local structures of the train and test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head><p>Notation. We denote by {(a 1 , m a1 ), . . . , (a n , m an )} a multiset, that is, a set where we allow multiple instances of the same element. Here a 1 , . . . , a n are distinct elements, and m ai is the number of times a i appears in the multiset. Bold-face letters represent vectors.</p><p>Graph neural networks. In our theoretical results, we focus on the message-passing architecture from <ref type="bibr">(Morris et al., 2019)</ref>. Let G = (V, E) be a graph, and for each node v ∈ V let h (0) v ∈ R d0 be a node feature vector and N (v) its set of neighbors. The t-th layer of first-order GNN is defined as follows for t &gt; 0:</p><formula xml:id="formula_0">h (t) v = σ   W (t) 2 h (t−1) v + u∈N (v) W (t) 1 h (t−1) u + b (t)   .</formula><p>Here,</p><formula xml:id="formula_1">W (t) 1 , W (t) 2 ∈ R dt×dt−1 , b (t)</formula><p>∈ R dt denotes the parameters of the t-th layer of the GNN, and σ is some nonlinear activation (e.g ReLU). It was shown in <ref type="bibr">(Morris et al., 2019)</ref> that GNNs composed from these layers have maximal expressive power with respect to all message-passing neural networks. For node prediction, the output of a T -layer GNN for node v is h (T ) v . For graph prediction tasks an additional readout layer is used: g (T ) = v∈V h (T ) v , possibly followed by a fully connected network.</p><p>Graph distributions and local structures. In this paper we focus on graph distributions for which the local structure of the graph (formally defined in Sec. 4) depends on the graph size. A well-known distribution family with this property is G(n, p) graphs, also known as Erdős-Rényi. A graph sampled from G(n, p) has n nodes, and edges are drawn i.i.d. with probability p. The mean degree of each node is n • p; hence fixing p and increasing n changes the local structure of the graph, specifically the node degrees.</p><p>As a second example, we consider the preferential attachment model <ref type="bibr" target="#b1">(Barabási &amp; Albert, 1999)</ref>. Here, n nodes are drawn sequentially, and each new node is connected to exactly m other nodes, where the probability to connect to other nodes is proportional to their degree. As a result, high degree nodes have a high probability that new nodes will connect to them. Increasing the graph size, causes the maximum degree in the graph to increase, and thus changes its local structure. We also show that, in real datasets, the local structures of small and large graphs differ. This is further discussed in Sec. 7 and Appendix G.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Overview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The size-generalization problem</head><p>We are given two distributions over graphs P 1 , P 2 that contain small and large graphs accordingly, and a task that can be solved for all graph sizes using a GNN. We train a GNN on a training set S sampled i.i.d. from P 1 and study its performance on P 2 . In this paper, we focus on distributions that have a high discrepancy between the local structure of the graphs sampled from P 1 and P 2 .</p><p>Size generalization is not trivial. Before we proceed with our main results, we argue that even for the simple regression task of counting the number of edges in a graph, which is solvable for all graph sizes by a 1-layer GNN, GNNs do not naturally generalize to new sizes. Specifically, we show that training a 1-layer GNN on a non-diverse dataset reaches a non-generalizing solution with probability 1 over the random initialization. In addition, we show that, in general, the generalizing solution is not the least L1 or L2 norm solution, hence cannot be reached using standard regularization methods. See full derivation in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Summary of the main argument</head><p>This subsection describes the main flow of the next sections in the paper. We explore the following arguments:</p><p>(i) d-patterns are a correct notion for studying the expressivity of GNNs. To study size generalization, we introduce a concept named d-patterns, which captures the local structure of a node and its d-step neighbors, as captured by GNNs. This notion is formally defined in Section 4. For example, for graphs without node features, a 1-pattern of a node represents its degree, and its 2-pattern represents its degree and the set of degrees of its immediate neighbors. We argue that d-patterns are a natural abstract concept for studying the expressive power of GNNs: first, we extend a result by <ref type="bibr">(Morris et al., 2019)</ref> and prove that d-layer GNNs (with an additional node-level network) can be programmed to output any value on any d-pattern independently. Conversely, as shown in <ref type="bibr">(Morris et al., 2019)</ref>, GNNs output a constant value when given nodes with the same d-pattern, meaning that the expressive power of GNNs is limited by their values on d-patterns.</p><p>(ii) d-pattern discrepancy implies the existence of bad global minima. In Section 5, we focus on the case where graphs in the test distribution contain d-patterns that are not present in the train distribution. In that case, we prove that for any graph task solvable by a GNN, there is a weight assignment that succeeds on training distribution and fails on the test data. In particular, when the training data contains small graphs and the test data contains large graphs, if there is a d-pattern discrepancy between large and small graphs, then there are "bad" global minima that fail to generalize to larger graphs.</p><p>(iii) GNNs converge to non-generalizing solutions. In Section 6 we complement these theoretical results with a controlled empirical study that investigates the generalization capabilities of the solutions that GNNs converge to. We show, for several synthetic graph distributions in which we have control over the graph structure, that the generalization of GNNs in practice is correlated with the discrepancy between the local distributions of large and small graphs. Specifically, when the d-patterns in large graphs are not found in small graphs, GNNs tend to converge to a global minimum that succeeds on small graphs and fail on large graphs. This happens even if there is a "good" global minimum that solves the task for all graph sizes. This phenomenon is also prevalent in real datasets as we show in Section 7.</p><p>(iv) Size generalization can be improved. Lastly, In Section 7, we discuss two approaches for improving size generalization, motivated by our findings. We first formulate the learning problem as a domain adaptation (DA) problem where the source domain consists of small graphs and the target domain consists of large graphs. We then suggest two learning setups: (1) Training GNNs on a novel self-supervised task aimed at learning meaningful representations for d-patterns from both the target and source domains. (2) A semi-supervised learning setup with a limited number of labeled examples from the target domain. We show that both setups are useful in a series of experiments on synthetic and real data. Notably, training with our new SSL task increases classification accuracy on large graphs in real datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">GNNs and local graph patterns</head><p>We wish to understand theoretically the conditions where a GNN trained on graphs with a small number of nodes can generalize to graphs with a large number of nodes. To answer this question, we first analyze what information is available to each node after a graph is processed by a d-layer GNN. It is easy to see that every node can receive information from its neighbors which are at most d hops away. We note, however, that nodes do not have full information about their d-hop neighborhood. For example, GNNs cannot determine if a triangle is present in a neighborhood of a given node <ref type="bibr" target="#b7">(Chen et al., 2020)</ref>.</p><p>To characterize the information that can be found in each node after a d-layer GNN, we introduce the notion of dpatterns, motivated by the structure of the node descriptors used in the Weisfeiler-Lehman test <ref type="bibr">(Weisfeiler &amp; Lehman, 1968</ref>): a graph isomorphism test which was recently shown to have the same representational power as GNNs <ref type="bibr">((Xu et al., 2018;</ref><ref type="bibr">Morris et al., 2019)</ref>). In other words, the d-pattern of a node is an encoding of the (d − 1)-patterns of itself and its neighbors. For example, assume all the nodes in the graphs start with the same node feature. The 1-pattern of each node is its degree. The 2pattern of each node is for each possible degree i ∈ N the number of neighbors with degree i, concatenated with the degree of the current node. In the same manner, the 3pattern of a node is for each possible 2-pattern, the number of its neighbors with this exact 2-pattern.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="0">-pattern</head><formula xml:id="formula_2">{ } , 1 -pattern 2 -pattern } { } , , { , ( ) { } , ,<label>( , ) ( ) ) , ( { } , , ) , ( , Figure 2</label></formula><p>Fig. <ref type="figure">2</ref>. illustrates 0, 1 and 2-patterns for a graph with three categorical node features, represented by three colors (yellow, grey, and black). For this case, which generalizes the uniform node feature case discussed above, the 0-pattern is the node's categorical feature; 1-patterns count the number of neighbors with a particular feature. The same definition applies to high-order patterns.</p><p>We claim that the definition of d-patterns gives an exact characterization to the potential knowledge that a d-layer GNN has on each node. First, Theorem 4.2 is a restatement of Theorem 1 in <ref type="bibr">(Morris et al., 2019)</ref>  We note that the width of the required GNN from the theorem is not very large if d is small, where d represents the depth of the GNN. In practice, shallow GNNs are very commonly used and are empirically successful. The d + 2 layers in the theorem can be split into d message-passing layers plus 2 fully connected layers that are applied to each node independently. Thm. 4.3 can be readily extended to a vector output for each d-pattern, at the cost of increasing the width of the layers.</p><p>Combining Thm. 4.2 and Thm. 4.3 shows that we can independently control the values of d-layer GNNs on the set of d-patterns (possibly with an additional node-wise function) and these values completely determine the GNN's output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">"Bad" global minima exist</head><p>We now consider any graph-prediction task solvable by a d-layer GNN. Assume we have a training distribution of (say, small) graphs and a possibly different test distribution of (say, large) graphs. We show that if the graphs in the test distribution introduce unseen d-patterns, then there exists a (d + 3)-layer GNN that solves the task on the train distribution and fails on the test distribution. We will consider both graph-level tasks (i.e. predicting a single value for the entire graph, e.g., graph classification) and node-level tasks (i.e. predicting a single value for each node, e.g., node classification).</p><p>Theorem 5.1. Let P 1 and P 2 be finitely supported distributions of graphs. Let P d 1 be the distribution of d-patterns over P 1 and similarly P d 2 for P 2 . Assume that any graph in P 2 contains a node with a d-pattern in P d 2 \ P d 1 . Then, for any graph regression task solvable by a GNN with depth d there exists a GNN with depth at most d + 3 that perfectly solves the task on P 1 and predicts an answer with arbitrarily large error on all graphs from P 2 .</p><p>The proof directly uses the construction from Thm. 4.3, and can be found in Appendix C. The main idea is to leverage the unseen d-patterns from P d 2 to change the output on graphs from P 2 .</p><p>As an example, consider the task of counting the number of edges in the graph. In this case, there is a simple GNN that generalizes to all graph sizes: the GNN first calculates the node degree for each node using the first message-passing layer and then uses the readout function to sum the node outputs. This results in the output 2|E|, which can be scaled appropriately. To define a network that outputs wrong answers on large graphs under our assumptions, we can use Thm. 4.3 and make sure that the network outputs the node degree on patterns in P d 1 and some other value on patterns in P d 2 \ P d 1 . Note that although we only showed in Thm. 4.3 that the output of GNNs can be chosen for nodes, the value of GNNs on the nodes has a direct effect on graph-level tasks. This happens because of the global readout function used in GNNs, which aggregates the GNN output over all the nodes.</p><p>Next, we prove a similar theorem for node tasks. Here, we show a relation between the discrepancy of d-pattern distributions and the error on the large graphs.</p><p>Theorem 5.2. Let P 1 and P 2 be finitely supported distributions on graphs, and let P d 1 be the distribution of d-patterns over P 1 and similarly P d 2 for P 2 . For any node prediction task which is solvable by a GNN with depth d and &gt; 0 there exists a GNN with depth at most d + 2 that has 0-1 loss (averaged over the nodes) smaller then on P 1 and 0-1 loss ∆( ) on P 2 , where ∆( ) = max</p><formula xml:id="formula_3">A:P d 1 (A)&lt; P d 2 (A).</formula><p>Here, A is a set of d-patterns, and P (A) is the total probability mass for that set under P .</p><p>This theorem shows that for node prediction tasks, if there is a large discrepancy between the graph distributions (a set of d-patterns with small probability in P d 1 and large probability in P d 2 ), then there is a solution that solves the task on P 1 , and generalizes badly for P 2 . The full proof can be found in Appendix C.</p><p>Examples. The above results show that even for simple tasks, GNNs may fail to generalize to unseen sizes, here are two examples. (i) Consider the task of counting the number of edges in a graph. From Thm. 5.1 there is a GNN that successfully outputs the number of edges in graphs with max degree up to N , and fails on graphs with larger max degrees. (ii) Consider some node regression task, when the training set consists of graphs sampled i.i.d from an Erdős-Rényi graph G(n, p), and the test set contains graphs sampled i.i.d from G(2n, p). In this case, a GNN trained on the training set will be trained on graphs with an average degree np, while the test set contains graphs with an average degree 2np. When n is large, with a very high probability, the train and test set will not have any common d-patterns, for any d &gt; 0. Hence, by Thm. 5.2 there is a GNN that solves the task for small graphs and fails on large graphs.</p><p>The next section studies the relation between size generalization and local graph structure in controlled experimental settings on synthetic data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">A controlled empirical study</head><p>The previous section showed that there exist bad global minima that fail to generalize to larger graphs. In this section, we study empirically whether common training procedures lead to bad global minima in practice. Specifically, we demonstrate on several synthetic graph distributions, that reaching bad global minima is tightly connected to the discrepancy of d-pattern distributions between large and small graphs. We identify two main phenomena: (A) When there is a large discrepancy between the d-pattern distributions of large and small graphs, GNNs fail to generalize; (B) As the discrepancy between these distributions gets smaller, GNNs get better at generalizing to larger graphs.</p><p>Tasks. In the following experiments, we use a controlled regression task in a student-teacher setting. In this setting, we sample a "teacher" GNN with random weights (drawn i.i.d from U ([−0.1, 0.1])), freeze the network, and label each graph in the dataset using the output of the "teacher" network. Our goal is to train a "student" network, which has the same architecture as the "teacher" network, to fit the labels of the teacher network. The advantages of this setting are twofold: (1) A solution is guaranteed to exist: We know that there is a weight assignment of the student network which perfectly solves the task for graphs of any size. (2) Generality: It covers a diverse set of tasks solvable by GNNs. As the evaluation criterion, we use the squared loss.</p><p>Graph distribution. Graphs were drawn from a G(n, p) distribution. This distribution is useful for testing our hypothesis since we can modify the distribution of d-patterns simply by changing either p or n. For example, 1-patterns represent node degrees, and in this model, the average degree of graphs generated from G(n, p) is np. We provide experiments on additional graph distributions like PA in Appendix D. Architecture and training protocol. We use a GNN as defined in <ref type="bibr">(Morris et al., 2019)</ref> with ReLU activations. The number of GNN layers in the network we use is either 1, 2 or 3; the width of the teacher network is 32 and of the student network 64, providing more expressive power to the student network. We obtained similar results when testing with a width of 32, the same as the teacher network. We use a summation readout function followed by a two-layer fully connected suffix. We use ADAM with a learning rate of 10 −3 . We added weight decay (L 2 regularization) with λ = 0.1. We performed a hyper-parameters search on the learning rate and weight decay and use validation-based early stopping on the source domain (small graphs). The results are averaged over 10 random seeds. We used Pytorch Geometric <ref type="bibr">(Fey &amp; Lenssen, 2019)</ref> on NVIDIA DGX-1.</p><p>Experiments We conducted four experiments, shown in Figure <ref type="figure" target="#fig_0">3 (a-d</ref>). We note that in all the experiments, the loss on the validation set was effectively zero. First, we study the generalization of GNNs by training on a bounded size range n ∈ [40, 50] and varying the test size in <ref type="bibr">[50,</ref><ref type="bibr">150]</ref>.</p><p>Figure <ref type="figure" target="#fig_0">3</ref> (a) shows that when p is kept constant while increasing the test graph sizes, size generalization degrades. Indeed, in this case, the underlying d-pattern distribution diverges from the training distribution. In Appendix D we demonstrate that this problem persists to larger graphs with up to 500 nodes.</p><p>On the flip side, Figure <ref type="figure" target="#fig_0">3</ref> (b) shows that when p is properly normalized to keep the degree np constant while varying the graph size then we have significantly better generalization to large graphs. In this case, the d-pattern distribution remains similar.</p><p>In the next experiment, shown in Figure <ref type="figure" target="#fig_0">3</ref>  In this case we can see that as we train on graph sizes that approach the test graph sizes, the d-pattern discrepancy reduces and generalization improves.</p><p>In our last experiment shown in Figure <ref type="figure" target="#fig_0">3</ref> (d), we train on n ∈ [40, 50] and p = 0.3 and test on G(n, p) graphs with n = 100 and p varying from 0.05 to 0.5. As mentioned before, the expected node degree of the graphs is np, hence the distribution of d-patterns is most similar to the one observed in the training set when p = 0.15. Indeed, this is the value of p where the test loss is minimized.</p><p>Conclusions. First, our experiments confirm phenomena (A-B). Another conclusion is that size generalization is more difficult when using deeper networks. This is consistent with our theory since in these cases the pattern discrepancy becomes more severe: for example, 2-patterns divide nodes into significantly more d-pattern classes than 1-patterns.</p><p>Further results on real datasets appear in Sec. 7.</p><p>Additional experiments. in Appendix D, We show that the conclusions above are consistent along different tasks (max clique, edge count, node regression), distributions (PA and point cloud graphs), and architectures <ref type="bibr">(GIN (Xu et al., 2018)</ref>). We also tried other activation functions (tanh and sigmoid). Additionally, we experimented with generalization from large to small graphs. Our previous understanding is confirmed by the findings of the present experiment: generalization is better when the training and test sets have similar graph sizes (and similar d-pattern distribution).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Towards improving size generalization</head><p>The results from the previous sections imply that the problem of size generalization is not only related to the size of the graph in terms of the number of nodes or edges but to the distribution of d-patterns. Based on this observation, we now formulate the size-generalization problem as a domain adaptation (DA) problem. We consider a setting where we are given two distributions over graphs: a source distribution D S (say, for small graphs) and a target distribution D T (say, for large graphs). The main idea is to adapt the network to unseen d-patterns appearing in large graphs.</p><p>We first consider the unsupervised DA setting, where we have access to labeled samples from the source D S but the target data from D T is unlabeled. Our goal is to infer labels on a test dataset sampled from the target D T . To this end, we devise a novel SSL task that promotes learning informative representations of unseen d-patterns. We show that this approach improves the size-generalization ability of GNNs.</p><p>Second, we consider a semi-supervised setup, where we also have access to a small number (e.g., 1-10) of labeled examples from the target D T . We show that such a setup, when feasible, can lead to equivalent improvement, and benefits from our SSL task as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">SSL for DA on graphs</head><p>In SSL for DA, a model is trained on unlabeled data to learn a pretext task, which is different from the main task at hand. If the pretext task is chosen wisely, the model learns useful representations <ref type="bibr" target="#b9">(Doersch et al., 2015;</ref><ref type="bibr" target="#b14">Gidaris et al., 2018)</ref> that can help with the main task. Here, we train the pretext task on both the source and target domains, as was done for images and point clouds <ref type="bibr" target="#b43">(Sun et al., 2019;</ref><ref type="bibr" target="#b0">Achituve et al., 2020)</ref>. The idea is that the pretext task aligns the representations of the source and target domains leading to better predictions of the main task for target graphs.</p><p>Pattern-tree pretext task. We propose a novel pretext task which is motivated by sections 5-6: one of the main causes for bad generalization is unseen d-patterns in the test set. Therefore, we design a pretext task to encourage the network to learn useful representations for these d-patterns. Our pretext task is a node prediction task in which the output node label is specifically designed to hold important information about the node's d-pattern. For an illustration of a label see Figure <ref type="figure" target="#fig_2">4</ref>. The construction of those labels is split into two procedures.</p><p>First, we construct a tree that fully represents each node's d-pattern.</p><p>The tree is constructed for a node v in the following way: we start by creating a root node that represents v. We then create nodes for all v's neighbors and connect them to the root. All these nodes hold the features of the nodes they represent in the original graph. We continue to grow the tree recursively up to depth d by adding new nodes that represent the neighbors (in the original graph) of the current leaves in our tree.</p><p>This is a standard construction, see e.g., <ref type="bibr">(Xu et al., 2018)</ref>.</p><p>For more details about the construction of the pattern tree see Appendix E.</p><p>We then calculate a descriptor of the tree that will be used as the SSL output label for each node. The descriptor is a concatenation of histograms of the different node features in each layer of the tree. The network is then trained in a node regression setup with a dedicated SSL head to predict this descriptor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Experiments</head><p>Baselines. We compare our new pretext task to the following baselines: In each iteration, we obtain two similar versions of each graph, which are used to compute a contrastive loss <ref type="bibr" target="#b37">(Qiu et al., 2020;</ref><ref type="bibr">You et al., 2020a)</ref> against other graphs. We follow the protocol of (You et al., 2020a), using a corruption function of edge perturbation that randomly adds and removes 5% of the edges in the graph.</p><p>Datasets. We use datasets from <ref type="bibr" target="#b34">(Morris et al., 2020)</ref> and <ref type="bibr" target="#b38">(Rozemberczki et al., 2020)</ref> (Twitch egos and Deezer egos).</p><p>We selected datasets that have a sufficient number of graphs (more than 1,000) and with a non-trivial split to small and large graphs as detailed in Appendix G.4. In total, we used 7 datasets, 4 from molecular biology (NCI1, NCI109, D&amp;D, Proteins), and 3 from social networks (Twitch ego nets, Deezer ego nets, IMDB-Binary). In all datasets, 50% smallest graphs were assigned to the training set, and the largest 10% of graphs were assigned to the test set. We further split a random 10% of the small graphs as a validation set.</p><p>Architecture and training protocol. The setup is the same as in Sec. 6 with a three-layer GNN in all experiments. 48.2 ± 10.9 54.6 ± 6.6 52.2 ± 6.8 55.7 ± 5.8 76.6 ± 7.7 59.4 ± 3.5 63.6 ± 15.0 58.6% CL PT 47.6 ± 9.7 53.6 ± 7.5 57.4 ± 8.1 57.3 ± 6.1 77.6 ± 4.7 53.9 ± 7.1 69.2 ± 5.5 59.5% PATTERN MTL (OURS) 45.6 ± 8.8 56.8 ± 9.2 60.5 ± 7.5 67.9 ± 7.2 75.8 ± 11.1 61.6 ± 3.5 76.8 ± 3.0 76.8 ± 3.0 76.8 ± 3.0 63.6% PATTERN PT (OURS)</p><p>44.0 ± 7.7 61.9 ± 3.2 61.9 ± 3.2 61.9 ± 3.2 67.8 ± 11.7 67.8 ± 11.7 67.8 ± 11.7 74.8 ± 5.7 74.8 ± 5.7 74.8 ± 5.7 84.7 ± 5.1 84.7 ± 5.1 84.7 ± 5.1 64.5 ± 3.3 64.5 ± 3.3 64.5 ± 3.3 74.9 ± 5.2 67.5%</p><p>Table <ref type="table">1</ref>. Test accuracy of compared methods in 7 binary classification tasks. The Pattern tree method with pretraining achieves the highest accuracy in most tasks and increases the average accuracy from 63% to 67% compared with the second-best method. High variance is due to the domain shift between the source and target domain.  <ref type="formula" target="#formula_2">2</ref>) Pretraining (PT) <ref type="bibr" target="#b18">(Hu et al., 2019)</ref>. For MTL we use equal weights for the main and SSL tasks. In the semi-supervised setup, we used equal weights for the source and target data. More details on the training procedures and the losses can be found in Appendix F.</p><p>d-pattern distribution in real datasets. In Appendix G.5 we study the discrepancy between the local patterns between small and large graphs on all the datasets mentioned above. The second row of Table <ref type="table">1</ref> summarizes our findings with the total variation (T V ) distances between d-pattern distributions of small and large graphs. The difference between these distributions is severe for all social network datasets (T V ≈ 1), and milder for biological datasets (T V ∈ [0.15, 0.48]).</p><p>Next, we will see that a discrepancy between the d-patterns leads to bad generalization and that correctly representing the patterns of the test set improves performance.</p><p>Results for unsupervised DA setup. Table <ref type="table">1</ref> compares the effect of using the Pattern-tree pretext task to the baselines described above. The small graphs row presents vanilla results on a validation set with small graphs for comparison.</p><p>The small graph accuracy on 5 out of 7 datasets is larger by 7.3%-15.5% than on large graphs, indicating that the sizegeneralization problem is indeed prevalent in real datasets.</p><p>Pretraining with the d-patterns pretext task outperforms other baselines in 5 out 7 datasets, with an average 4% improved accuracy on all datasets. HOMO-GNN slightly improves over the vanilla while other pretext tasks do not improve average accuracy. Specifically, for the datasets with high discrepancy of local patterns (namely, IMDB, Deezer, Proteins, and Twitch), pretraining with our SSL task improves nicely over vanilla training (by 5.4% on average). Naturally, the accuracy here is lower than SOTA on these datasets because the domain shift makes the problem harder. Additional experiments We provide additional experi- were averaged over all the datasets in table <ref type="table">1</ref>.</p><p>ments on the synthetic tasks discussed in Sec. 6 in Appendix G. We show that the pattern-tree pretext task improves generalization in the student-teacher setting (while not solving the edge count or degree prediction tasks). In addition, adding even a single labeled sample from the target distribution significantly improves performance. We additionally tested our SSL task on a combinatorial optimization problem of finding the max clique size in the graph, our SSL improves over vanilla training by a factor of 2, although not completely solving the problem. Also, we tested on several tasks from the "ogbg-molpcba" dataset (see <ref type="bibr" target="#b19">(Hu et al., 2020)</ref>), although the results are inconclusive. This is further discussed in Sec. 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Related work</head><p>Size generalization. Several papers observed successful generalization across graph sizes, but the underlying reasons were not investigated <ref type="bibr" target="#b27">(Li et al., 2018;</ref><ref type="bibr" target="#b31">Maron et al., 2018;</ref><ref type="bibr" target="#b30">Luz et al., 2020)</ref>. More recently, <ref type="bibr" target="#b47">(Veličković et al., 2019)</ref> showed that when training GNNs to perform simple graph algorithms step by step they generalize better to graphs of different sizes. Unfortunately, such training procedures cannot be easily applied to general tasks. <ref type="bibr" target="#b26">(Knyazev et al., 2019)</ref> studied the relationship between generalization and attention mechanisms. <ref type="bibr" target="#b3">(Bevilacqua et al., 2021</ref>) study graph extrapolation using causal modeling. On the more practical side, <ref type="bibr" target="#b21">(Joshi et al., 2019;</ref><ref type="bibr">2020;</ref><ref type="bibr" target="#b23">Khalil et al., 2017)</ref>, study the Traveling Salesman Problem (TSP), and show empirically that size generalization on this problem is hard. <ref type="bibr" target="#b8">(Corso et al., 2020)</ref>   <ref type="bibr" target="#b7">Bruna, 2020;</ref><ref type="bibr" target="#b5">Bueno &amp; Hylton, 2020)</ref>. In <ref type="bibr" target="#b41">(Santoro et al., 2018)</ref> the authors study generalization in abstract reasoning.</p><p>Generalization in graph neural networks. Several works studied generalization bounds for certain classes of GNNs <ref type="bibr" target="#b13">(Garg et al., 2020;</ref><ref type="bibr" target="#b36">Puny et al., 2020;</ref><ref type="bibr" target="#b48">Verma &amp; Zhang, 2019;</ref><ref type="bibr" target="#b28">Liao et al., 2020;</ref><ref type="bibr" target="#b10">Du et al., 2019)</ref>, but did not discuss size generalization. <ref type="bibr" target="#b42">(Sinha et al., 2020)</ref> proposed a benchmark for assessing the logical generalization abilities of GNNs.</p><p>self-supervised and unsupervised learning on graphs.</p><p>One of the first papers to propose an unsupervised learning approach for graphs is <ref type="bibr" target="#b25">(Kipf &amp; Welling, 2016b)</ref>, which resulted in several subsequent works <ref type="bibr" target="#b35">(Park et al., 2019;</ref><ref type="bibr" target="#b39">Salha et al., 2019)</ref>. <ref type="bibr" target="#b46">(Velickovic et al., 2019)</ref> suggested an unsupervised learning approach based on predicting global graph properties from local node descriptors. <ref type="bibr" target="#b18">(Hu et al., 2019)</ref> suggested several unsupervised learning tasks that can be used for pretraining. More recently, <ref type="bibr" target="#b20">(Jin et al., 2020;</ref><ref type="bibr">You et al., 2020b)</ref> proposed several self-supervised tasks on graphs, such as node masking. These works mainly focused on a single graph learning setup. <ref type="bibr">(You et al., 2020a;</ref><ref type="bibr" target="#b37">Qiu et al., 2020)</ref> applied contrastive learning techniques for unsupervised representation learning on graphs. The main difference between our SSL task and contrastive learning is that following our theoretical observation, our SSL task focuses on representing the local structure of each node, rather than a representation that takes into account the entire graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Conclusion and Discussion</head><p>This work is a step towards gaining an understanding of the size-generalization problem in graph neural networks. We showed that for important graph distributions, GNNs do not naturally generalize to larger graphs even on simple tasks. We started by defining d-patterns, a concept that captures the expressivity of GNNs. We then characterized how the failure to generalize depends on d-patterns. Lastly, we suggested two approaches that can improve generalization. Although these approaches are shown to be useful for multiple tasks, there are still some tasks where generalization could not be improved.</p><p>A limitation of our approach is that it assumes categorical node features and bidirectional edges with no features. We plan to expand our approach in the future to address these important use cases. As a final note, our characterization of d-patterns, as well as the methods we proposed, can be applied to other cases where generalization is hindered by distribution shifts, and may also be able to improve results in these situations. We start our discussion on size generalization with a theoretical analysis of a simple setup. We consider a single-layer GNN and an easy task and show that: (1) The training objective has many different solutions, but only a small subset of these solutions generalizes to larger graphs (2) Simple regularization techniques cannot mitigate the problem.</p><p>Assume we train on a distribution of graphs. Our task is to predict the number of edges in the graph using a first-order GNN with a single linear layer and additive readout function, for simplicity also consider the squared loss. We first note that the task of edge count can be solved with this architecture for graphs of any size. The intuition is to count the number of neighbors for each node (can be done with a 1-layer GNN), and summing over all nodes using the readout function would give us 2|E|, where E is the set of edges.</p><p>The objective boils down to the following function for any graph G in the training set:</p><formula xml:id="formula_4">L(w 1 , w 2 , b; G) =   u∈V (G)   w 1 • x u + v∈N (u) w 2 • x v + b   − y   2 .</formula><p>Here, G is an input graph, V (G) are the nodes of G, N (v) are all the neighbors of node v, w 1 , w 2 and b are the trainable parameters, y is the target and x v is the node feature for node v. Further, assume that we have no additional information on the nodes, so we can just embed each node as a one-dimensional feature vector with a fixed value of 1. In this simple case, the trainable parameters are also one-dimensional.</p><p>For a graph with n nodes and m edges the training objective can also be written in the following form:</p><formula xml:id="formula_5">L(w 1 , w 2 , b; G) = (nw 1 + 2mw 2 + nb − m) 2 ,</formula><p>One can easily find the solution space, which is an affine subspace defined by</p><formula xml:id="formula_6">w 2 = m−n(w1+b) 2m = 1 2 − n 2m • w1+b 2m .</formula><p>In particular, the solutions with w 1 + b = 0, w 2 = 1/2 are the only ones which do not depend on the specific training set graph size n, and generalize to graphs of any size and with any number of edges.</p><p>It can be readily seen that when training the model on graphs where the ratio n/m between the number of nodes and number of edges is fixed, gradient descent with a small enough learning rate will favor the global solution closest to the initialized point. Hence, by using a standard initialization scheme (e.g. <ref type="bibr" target="#b16">(Glorot &amp; Bengio, 2010)</ref>), with probability 1, the solution that gradient descent converges to is not a generalizing solution. Note that we could train on many graphs with different sizes and still end up in a non-generalizing solution, as long as n/m is fixed. On the other hand, training on graphs with different node/edge ratios necessarily leads to some generalizing solution. This is because the problem is convex, and the generalizing solutions are the only solutions that minimize the loss for graphs with different ratios.</p><p>We also note that the generalizing solution (w 1 + b = 0, w 2 = 1/2) is not the least norm solution in general (with respect to both L 1 and L 2 norms) so simple regularization will not help here (it is the least L 1 norm solution if 2m &gt; n). As we show in Sec. 6, the problem gets worse when considering GNNs with multiple non-linear layers, and this simple solution will not help in this case: we can train deeper GNNs on a wide variety of sizes and the solution will not generalize to other sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proofs from Sec. 4</head><p>Proof of Thm. 4.2. We show that from the definition of d-patterns, and the 1-WL algorithm (see <ref type="bibr">(Weisfeiler &amp; Lehman, 1968</ref>)), the color given by the WL algorithm to two nodes is equal iff their d-pattern is equal. For the case of d = 0, it is clear. Suppose it is true for d − 1, the WL algorithm at iteration d give node v a new color based on the colors given in iteration d − 1 to the neighbors of v. This means, that the color of v at iteration d depends on the multiset of colors at iteration d − 1 of its neighbors, which by induction is the (d − 1)-pattern of the neighbors of v. To conclude, we use Theorem 1 from <ref type="bibr">(Morris et al., 2019)</ref> which shows that GNNs are constant on the colors of WL, hence also constant on the d-patterns.</p><p>To prove Thm. 4.3 we will first need to following claim from <ref type="bibr">(Yun et al., 2019)</ref> about the memorization power of ReLU networks:</p><formula xml:id="formula_7">Theorem B.1. Let {x i , y i } N i=1 ∈ R d × R</formula><p>such that all the x i are distinct and y i ∈ [−1, 1] for all i. Then there exists a 3-layer fully connected ReLU neural network f : R d → R with width 2 √ N such that f (x i ) = y i for every i.</p><p>We will also need the following lemma which will be used in the construction of each layer of the GNN:</p><formula xml:id="formula_8">Lemma B.2. Let N ∈ N and f : N → R be a function defined by f (n) = w 2 σ(w 1 n − b) where w 1 , w 2 , b ∈ R N ,</formula><p>and σ is the ReLU function. Then for every y 1 , . . . , y N ∈ R there exists</p><formula xml:id="formula_9">w 1 , w 2 , b such that f (n) = y n for n ≤ N and f (n) = (n − N + 1)y N − (n − N )y N −1 for n &gt; N . Proof. Define w 1 =    1 . . . 1   , b =      0 1 . . . N − 1     </formula><p>. Let a i be the i-th coordinate of w 2 , we will define a i recursively in the following way: Let a 1 = y 1 , suppose we defined a 1 , . . . , a i−1 , then define</p><formula xml:id="formula_10">a i = y i − 2a i−1 − • • • − ia 1 . Now we have for every n ≤ N : f (n) = w 2 σ(w 1 n − b) = na 1 + (n − 1)a 2 + • • • + a n = y n .</formula><p>For n &gt; N we can write n = N + k for k ≥ 1, then we have:</p><formula xml:id="formula_11">f (n) = w 2 σ(w 1 (N + k) − b) = (N + k)a 1 + (N + k − 1)a 2 + • • • + (k + 1)a N = y N + k(a 1 + a 2 + • • • + a N ) = y N + k(y N − a N −1 − 2a N −2 − • • • − (N − 1)a 1 ) = (k + 1)y N − ky N −1</formula><p>Now we are ready to prove the main theorem:</p><p>Proof of Thm. 4.3. We assume w.l.o.g that at the first iteration each node i is represented as a one-hot vector h (0) i of dimension |C|, with its corresponding node feature. Otherwise, since there are |C| node features we can use one GNN layer that ignores all neighbors and only represent each node as a one-hot vector. We construct the first d layers of the 1-GNN F by induction on d. Denote by a i = |C| • N i + N i−1 the dimension of the i-th layer of the GNN for 1 ≤ i ≤ d, and a 0 = |C|.</p><p>The mapping has two parts, one takes the neighbors information and maps it into a feature representing the multiset of d-patterns, the other part is simply the identity-saving information regarding the d-pattern of the node itself.</p><p>The d layer structure is</p><formula xml:id="formula_12">h (d) v = U (d+1) σ   W (d) 2 h (d−1) v + u∈N (v) W (d) 1 h (d−1) u − b (d)   We set W (d) 2 = [0, I] T , W (d) 1 = [ W (d)T 1 , 0] T and U (d+1) = [ Ũ (d+1)T , 0] T with W (d) 1 ∈ R N a d−1 ×a d−1 and Ũ (d+1) ∈ R N a d−1 ×N a d−1 . For W (d) 1 we set w<label>(1)</label></formula><p>i , its i-th row, to be equal to e n where n = i N . Let b</p><p>(1) i be the i-th coordinate of b (d) , be equal to i − 1 (mod N ) for i ≤ N • a d−1 and zero otherwise. What this does for the first</p><formula xml:id="formula_13">a d−1 elements of W (d) 2 h (d) v + u∈N (v) W (d) 1 h (d)</formula><p>u is that each dimension i hold the number of neighbors with a specific (d-1)-pattern. We then replicate this vector N times, and for each replica we subtract a different bias integer ranging from 0 to N − 1. To that output we concatenate the original h</p><formula xml:id="formula_14">(d−1) v Next we construct Ũ (d+1) ∈ R N a d−1 ×N a d−1</formula><p>in the following way: Let u (d+1) i be its i-th row, and u (d+1) i,j its j-th coordinate.</p><p>We set u (d+1) i,j = 0 for every j with j = i N and the rest N coordinates to be equal to the vector w 2 from Lemma B.2 with labels y = 0 for ∈ {1, . . . , N } \ {(i mod N ) + 1} and y = 1 for = (i mod N ) + 1.</p><p>Using the above construction we encoded the output on node v of the first layer of F as a vector:</p><p>This encoding is such that the i-th coordinate of h</p><formula xml:id="formula_15">(d+1) v for 1 ≤ i ≤ N • a d−1 is equal to 1 iff node v have (i mod N ) + 1 neighbors with node feature i N ∈ {1, . . . , |C|}.</formula><p>The last a d−1 rows are a copy of h</p><formula xml:id="formula_16">(d−1) v .</formula><p>Construction of the suffix. Next, we construct the last two layers. First we note that for a node v with d-pattern p there is a unique vector z p such that the output of the GNN on node v, h</p><p>v , is equal to z p . From our previous construction one can reconstruct exactly the (d-1)-pattern of each node, and the exact number of neighbors with each (d-1)-pattern and therefore can recover the d-pattern correctly from the h</p><formula xml:id="formula_18">(d) v embedding.</formula><p>Let y max := max i |y i |, and define ỹi = y i /y max . Finally, we use Thm. B.1 to construct the 3-layer fully connected neural network with width at most 2 |P | such that for every pattern p i ∈ P with corresponding unique representation z pi and label ỹi , the output of the network on z pi is equal to ỹi . We construct the last two layers of the GNN such that W d+3) are the matrices produced from Thm. B.1. Note that W (d+3) is the linear output layer constructed from the theorem, thus the final output of the GNN for node v is</p><formula xml:id="formula_19">(d+1) 1 , W (d) 1 = 0, and W (d+1) 2 , b (d+1) , W (d+2) 2 , b (d+2) , W (</formula><formula xml:id="formula_20">W (d+3) • h (d+2) v</formula><p>, where h (d+2) v is the output after d + 2 layers. We multiply the final linear output layer by y max , such that the output of the entire GNN on pattern p i is exactly y i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Proofs from Sec. 5</head><p>Proof of Thm. 5.1. By the assumption, there is a depth d GNN that solves the task for graphs of any size, denote this GNN by F . We can write F operating on a graph G as F (G) = M v∈V (G) H(G) v , where V (G) are the nodes of G, H is a d-layer GNN with H(G) v is the output of H on node v, and M is an MLP. Note that we used the summation readout function which sums the output of the GNN over all the nodes of the input graph.</p><p>We will construct a new GNN F in the following way: Let P be the set of d-patterns which appear in P d 1 and P all the d-patterns which appear in P d 2 but not in P d 1 . Note that by the assumption that the distributions are with finite support, both P and P are finite. Suppose that H(G) v ∈ R k , that is the dimension of the output feature vector for each v ∈ V (G) is k-dimensional. We define a d + 2-layer GNN H with output dimension k + 1, such that:</p><p>1. For each p ∈ P and node v with d-pattern p, the output of H on v is equal to H(G) v 0 .</p><p>2. For each p ∈ P and node ṽ with d-pattern p, the output of H on ṽ is equal to 0 1 .</p><p>This construction is possible using Thm. 4.3 since both P and P are finite. We define an MLP M which have one more layer than M in the following way: All the weights in all the layers except the last one are block matrices of the form W 0 0 0 0 0 0 1 , where W is the original weight matrix from M , and 0 0 0 is a vector of zeroes of the corresponding size. Let y ∈ R be the maximum output of the task (in absolute value) over all the graphs from both P 1 and P 2 . We define the last layer of M as:</p><formula xml:id="formula_21">1 0 0 2y . Finally, define F (G) = M v∈V (G) H (G) v .</formula><p>We will now show the correctness of the construction. For any d-pattern p ∈ P , and v with d-pattern p it is clear that</p><formula xml:id="formula_22">H (G) v = H(G) v 0</formula><p>. Hence, for every graph G coming from the distribution P 1 we have that</p><formula xml:id="formula_23">F (G) = M v∈V (G) H(G) v 0 = F (G)</formula><p>On the other side, let G be some graph with from the distribution P 2 . Then, there is some ṽ ∈ V (g) with ṽ ∈ P , hence we</p><formula xml:id="formula_24">have that v∈V (G) H (G) v = x x x z</formula><p>, where x x x is some vector and z ≥ 1. Hence, we have that</p><formula xml:id="formula_25">F (G) = M x x x z &gt; y.</formula><p>This means that the output of F on any graph drawn from P 2 is not the correct output for the task.</p><p>Proof of Thm. 5.2. By the assumption, the output of the task is determined by the d-patterns of the nodes. For each node with pattern p i let y i be the output of the node prediction task. Define A = arg max</p><formula xml:id="formula_26">A :P d 1 (A )&lt; P d 2 (A )<label>(1)</label></formula><p>By Thm. 4.3 there exists a first order GNN such that for any d-pattern p i ∈ A gives a wrong label and for every pattern p j ∈ (P 1 ∪ P 2 ) \ A gives the correct label. Note that we can use Thm. 4.3 since both A and (P 1 ∪ P 2 ) \ A are finite, because we assumed that distributions on the graphs have finite supports. The 0-1 loss for small and large graphs is exactly P d 1 (A) and P d 2 (A) respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Additional experiments from Sec. 6 D.1. Experiments on Larger Graphs</head><p>We conducted the experiment from Fig. <ref type="figure" target="#fig_0">3</ref> (a) with much larger graphs. We used a three-layer GNN and tested on graphs with n ∈ [50, 500] nodes. See Fig. <ref type="figure" target="#fig_8">8</ref> The problem of size generalization persists, where increasing the graph size also significantly increases the loss on the test set. We stress that in all the experiments, the loss on the validation set is effectively zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Max-clique Size</head><p>We consider the max-clique problem. The goal of this problem is given a graph, to output the size of the maximal clique. This is in general an NP-hard problem, hence a constant depth GNN will not be able to solve it for graphs of all sizes. For this task we sampled both the train and test graphs from a geometrical distribution, which resembles how graphs are created from point clouds, defined as follows: given a number of nodes n and radius ρ we draw n points uniformly in [0, 1] 2 , each point correspond to a node in the graph, and two nodes are connected if their corresponding points have a distance less than ρ. We further analyzed the effects of how the network depth and architecture affect size generalization.</p><p>Table <ref type="table">2</ref> presents the test loss on the max-clique problem. Deeper networks are substantially more stricken due to the domain shift. If the test domain has a similar pattern distribution, increasing the neural network depth from one layer to three layers results in a small decrement of at most 25% to the loss. However, if the pattern distribution is different than the train pattern distribution, such change may increase the loss by more than 5.5×. We also show that the problem is consistent in both first-order GNN and GIN architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Different Architectures and Node/Graph Level Tasks</head><p>We tested on the following tasks: (1) student-teacher task, on both graph and node levels, (2) per node degree prediction task, and (3) predicting the number of edges in the graph. The goal of these experiments is to show that the size-generalization problem persists on different tasks, different architectures, and its intensity is increased for deeper GNN. In all the experiments we draw the graphs from G(n, p) distribution, wherein the test set n is drawn uniformly between 40 and 50, and p = 0.3, and in the test set n = 100 and p is either 0.3 or 0.15. We note that when p = 0.15, the average degree of the test graph is equal to (approximately) the average degree of the train graph, while when p = 0.3 it is twice as large. We would expect that the model will generalize better when the average degree in the train and test set is similar because then their d-patterns will also be more similar.  Table <ref type="table">3</ref> compares the performance of these tasks when changing the graph size of the test data. We tested the performance with the squared loss. We note that the GNNs (both first-order GNN and GIN) successfully learned all the tasks presented here on the train distribution, here we present their generalization capabilities to larger sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4. Different data distribution -Preferential Attachment</head><p>We performed an experiment on the preferential attachment graph model. In this model, the graph distribution is determined by two parameters n and m. Each graph is created sequentially, where at each iteration a new node is added to the graph, up to n nodes. Each new node is connected to exactly m existing nodes, where the probability to connect to node v is proportional to its degree. This means that higher degree nodes have a higher degree to have more nodes connected to them.</p><p>We trained on the task of edge count (i.e. predicting the number of edges in a graph) for graphs sampled from a preferential attachment model with n uniformly sampled from [10, 50] and m = 4. We tested on graphs also sampled from a preferential attachment model with n nodes for n varying from 50 to 500, and m = 4. Note that this task can be solved efficiently for any graph distribution. Fig. <ref type="figure" target="#fig_9">7</ref> depicts the results. It is clear that for depth 2 and 3 GNNs, as the graph size gets larger, the GNN fails to predict the number of edges. This shows that although for this problem there is a solution that works on all graph sizes, and we trained on graphs of varying sizes, GNNs tend to converge to solutions that do not generalize to larger sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5. Generalization From Large to Small Graphs</head><p>We additionally tested size generalization in the opposite direction, i.e. generalizing from large to small graphs. We used the same teacher-student setting as in Fig. <ref type="figure" target="#fig_0">3</ref>, where the graphs are drawn from a G(n, p) distribution with a constant p = 0.3. We consider three separate training sets with graphs of sizes [90, 100], <ref type="bibr">[140,</ref><ref type="bibr">150]</ref>, <ref type="bibr">[190,</ref><ref type="bibr">200]</ref> and tested on graphs of sizes 50, 75. The rows of Table <ref type="table" target="#tab_4">4</ref> clearly show that when the size difference between the graphs in the train and test sets decreases, the loss also decreases, which is consistent with our theory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. SSL task on d-pattern tree</head><p>First, we will need the following definition which constructs a tree out of the d-pattern introduced in Sec. 4. This tree enables us to extract certain properties for each node which can, later on, be learned using a GNN. This definition is similar to the definition of "unrolled tree" from <ref type="bibr" target="#b32">(Morris &amp; Mutzel, 2019)</ref>. contains examples labeled by the SSL task from both the source and target distributions. Let : R × R → R be a loss function, in all our experiments we use the loss for classification tasks and squared loss for regression tasks. We construct the following models:</p><p>(1) f GN N is a GNN feature extractor. Its input is a graph and its output is a feature vector for each node in the graph. (2) h M ain is a head (a small neural network) for the main task. Its inputs are the node feature and it outputs a prediction (for graph prediction tasks this head contains a global pooling layer). (3) h SSL is the head for the SSL task. Its inputs are node features, and it outputs a prediction for each node of the graph, depending on the specific SSL task used.</p><p>Pretraining. Here, there are two phases for the learning procedure. In the first phase, at each iteration we sample a batch (x 1 , y 1 ) from X SSL , and train by minimizing the objective: (h SSL • f GN N (x 1 ), y 1 ). In this phase both h SSL and f GN N are trained. In the second phase, at each iteration we sample (x 2 , y 2 ) from X main and train on the loss (h M ain • f GN N (x 2 ), y 2 ), where we only train the head h M ain , while the weights of f GN N are fixed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multitask training.</head><p>Here we train all the functions at the same time. At each iteration we sample a batch (x 1 , y 1 ) from X SSL and a batch (x 2 , y 2 ) from X M ain and train by minimizing the objective:</p><formula xml:id="formula_27">α • (h SSL • f GN N (x 1 ), y 1 ) + (1 − α) • (h M ain • f GN N (x 2 ), y 2 ).</formula><p>Here α ∈ [0, 1] is the weight for the SSL task, in all our experiments we used α = 1/2.</p><p>For an illustration of the training procedures see Fig. <ref type="figure">9</ref>. These procedures are common practices for training with SSL tasks (see e.g. <ref type="bibr">(You et al., 2020b)</ref>).</p><p>We additionally use a semi-supervised setup in which we are given a dataset X F S of few-shot examples from the target distribution with their correct label. In both training procedures, at each iteration we sample a batch (x 3 , y 3 ) from X F S and add a loss term β (h M ain • f GN N (x 3 ), y) where β ∈ [0, 1] is the weight of the few-shot loss. In pretraining, this term is only added to the second phase, with weight 1/2 and adjust the weight of the main task to 1/2 as well (equal weight to the main task). In the multitask setup, we add this term with weight 1/3 and adjust the weights of the two other losses to 1/3 as well, so all the losses have the same weight.</p><p>G. More experiments from Sec. 7</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1. Synthetic datasets</head><p>We used the setting of Section 6. Source graphs were generated with G(n, p) with n sampled uniformly in <ref type="bibr">[40,</ref><ref type="bibr">50]</ref> and p = 0.3. Target graphs were sampled from G(n, p) with n = 100 and p = 0.3.</p><p>Table <ref type="table" target="#tab_5">5</ref> depicts the results of using the d-patterns SSL tasks, in addition to using the semi-supervised setting. It can be seen  that adding the d-patterns SSL task significantly improves the performance on the teacher-student task, although it does not completely solve it. We also observe that adding labeled examples from the target domain significantly improves the performance of all tasks. Note that adding even a single example significantly improves performance. In all the experiments, the network was successful at learning the task on the source domain with less than 0.15 averaged error, and in most cases much less. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2. Max clique</head><p>We tested our SSL task on the task of finding the max-clique size of a given graph, similar to the experiment presented in Table <ref type="table">2</ref>. For the train distribution, we constructed graphs by drawing n ∈ [10, 50] points uniformly in the unit square and connecting two points if their distance is less than ρ = 0.3. The test set graphs are drawn similarly, but with n = 100, effectively increasing the density by 2. We trained on 20, 000 graphs and tested on 2000 graphs, where the task is to predict the size of the maximal clique in the graph. We used squared loss. We ran the experiment 10 times and averaged the loss over all the experiments.</p><p>Without our SSL, the squared loss on average is 2325, hence not generalizing well to larger sizes. With our SSL task and using pretraining (PT) the loss on average is 1327, improving over vanilla training, but still not solving the problem. Using our SSL task with multitask training (MTL) the average loss provided worse results than vanilla training. We note that this is in general an NP-hard problem, hence solving it might require a specific solution, while our SSL is a general framework Twitch dataset similar to the one of Deezer, where the distributions of 2-patterns are disjoint).</p><p>It is clear that for the NCI1, NCI109, and D &amp; D datasets there is a very high overlap of 2-patterns between small and large graphs. Indeed, in the result from Table <ref type="table">1</ref> it can be seen that adding an SSL task (and specifically our tree pattern task) does not improve significantly over vanilla training (except for NCI109). On the other hand, for the other datasets, there is a very clear discrepancy between the 2-patterns of small and large graphs. Indeed for these datasets, our SSL task improved over vanilla training. For the social network datasets, it is even more severe, where there is almost no overlap between the 2-patterns, meaning that the small and large graphs have very different local structures. We also calculated the total variation distance between the distributions for every dataset, this appears in the first row of Table <ref type="table">1</ref>. . Histograms in percentage of 2-patterns of graphs. We used the 50% smallest graphs and 10% largest graphs in each dataset.(same as the split in the experiment from Sec. 7). The X-axis represent the 20 most common 2-patterns from each split, and the y-axis their percentage of appearance. The x-axis contain from 20 to 40 bars -given by how much overlap of 2-patterns there is between small and large graphs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The effect of graph size and d-pattern distribution on generalization in G(n, p) graphs in a student-teacher graph regression task. The y-axis represents the squared loss in log 10 scale. (a) Bounded training size n ∈ [40, 50] and varying test size with constant p = 0.3 (b) Bounded training training size n ∈ [40, 50] and varying test size while keeping node degrees constant by changing p ∈ [0.15, 0.3] . (c) Varying train size with constant test size. We train on graphs with n nodes and constant p = 0.3. Here, n is drawn uniformly from [40, x] and x varies; test on n = 150, p = 0.3. (d) Train on n drawn uniformly from [40, 50] and p = 0.3 test on n = 100 and varying p. See discussion in the text.</figDesc><graphic url="image-2.png" coords="6,61.42,67.06,111.77,83.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(c) we keep the test size constant n = 150 and vary the training size n ∈ [40, x] where x varies in [50, 150] and p = 0.3 remains constant.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Top left: a graph with node features represented by colors. Top right: A tree that represents the d-patterns for the black node. Bottom: The tree descriptor is a vector with each coordinate containing the number of nodes from each class in each layer of the tree.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(1) Vanilla: standard training on the source domain; (2) HomoGNN<ref type="bibr" target="#b44">(Tang et al., 2020)</ref> a homogeneous GNN without the bias term trained on the source domain; (3) Graph autoencoder (GAE) pretext task<ref type="bibr" target="#b25">(Kipf &amp; Welling, 2016b)</ref>; (4) Node masking (NM) pretext task from<ref type="bibr" target="#b18">(Hu et al., 2019)</ref> where at each training iteration we mask 10% of the node features and the goal is to reconstruct them. In case the graph does not have node features then the task was to predict the degree of the masked nodes. (5) Node metric learning (NML): we use metric learning to learn useful node representations. We use a corruption function that given a graph and corruption parameter p ∈ [0, 1], replaces p|E| of the edges with random edges, and thus can generate positive (p = 0.1) and negative (p = 0.3) examples for all nodes of the graph. We train with the triplet loss(Weinberger &amp; Saul, 2009). (6) Contrastive learning (CL):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Average accuracy on different size splits in the unsupervised setup for (i) d-pattern pretraining and (ii) no SSL (Vanilla).Accuracy is averaged over all the datasets in table 1.</figDesc><graphic url="image-6.png" coords="8,60.05,249.86,222.28,166.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5</head><label>5</label><figDesc>Fig. 5 shows two additional experiments, conducted on all datasets using different size splits. First, using a gap of 65% (training on the 30% smallest graphs and testing on the 5% largest graphs), and second, using a gap of 10% (training on the 50% smallest graphs and testing on graphs in the 60-70-percentile). The results are as expected: (1) When training without SSL, larger size gaps hurt more (2) SSL improves over Vanilla training with larger gaps. Results for semi-supervised DA setup. Figure 6 compares the performance of vanilla training versus pretraining with the pattern-tree pretext task in the semi-supervised setup. As expected, the accuracy monotonically increases with respect to the number of labeled examples in both cases. Still, we would like to highlight the improvement we get by training on only a handful of extra examples. Pretraining with the pretext task yields better results in the case of 0,1,5 labeled examples and comparable results with 10 labeled examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Average classification results in the semi-supervised setup for (i) d-pattern pretraining and (ii) no SSL (Vanilla). Resultswere averaged over all the datasets in table 1.</figDesc><graphic url="image-7.png" coords="9,55.44,67.06,243.00,121.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>study several multitask learning problems on graphs and evaluate how the performance changes as the size of the graphs change. In another line of work, Tang et al. (2020); Nachmani &amp; Wolf (2020) considered adaptive depth GNNs. In our paper, we focus on the predominant GNN architecture with a fixed number of message-passing layers. Several works also studied size generalization and expressivity when learning set-structured inputs (Zweig &amp;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Extending the experiment in Fig. 3 (a) to larger graphs. We train on graphs with bounded size n ∈ [40, 50] and test on graphs with size up to n = 500 with constant p = 0.3.</figDesc><graphic url="image-8.png" coords="17,419.94,292.33,121.50,91.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Size generalization in PA graphs. The networks were trained on the edge count task, with graphs sampled from a preferential attachment model with n drawn uniformly from [10, 50] and m = 4. Tested on graphs also sampled the preferential attachment model with n varies (x-axis) and m = 4. For depth 2 and 3 GNN, as the graphs in the test set gets larger, the generalization worsens.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 .</head><label>10</label><figDesc>Figure10. Teacher-student setup with a 3-layer GNN. Training is on graphs drawn i.i.d from G(n, p) with n ∈ {40, ..., 50} uniformly and p = 0.3. Testing is done on graphs with n = 100 and p vary (x-axis). The "Pattern tree" plot represent training with our pattern tree SSL task, using the pretraining setup.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 10</head><label>10</label><figDesc>Fig.10depicts a side-by-side plot of the 3-layer case ofFig. 3 (d), where training is done on graphs sampled from G(n, p) with 40 to 50 nodes and p = 0.3, and testing on graphs with 100 nodes and p varies. We compare Vanilla training, and our pattern tree SSL task with pretraining. It is clear that for all values of p our SSL task improves over vanilla training, except for p = 0.15.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Figure11. Histograms in percentage of 2-patterns of graphs. We used the 50% smallest graphs and 10% largest graphs in each dataset.(same as the split in the experiment from Sec. 7). The X-axis represent the 20 most common 2-patterns from each split, and the y-axis their percentage of appearance. The x-axis contain from 20 to 40 bars -given by how much overlap of 2-patterns there is between small and large graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>C for every node v ∈ V . We define the d-pattern of a node v ∈ V for d ≥ 0 recursively: For d = 0, the 0-pattern is c v . For d &gt; 0, the d-pattern of v is p = (p v , {(p i1 , m pi 1 ), . . . , (p i , m pi )}) iff node v has (d − 1)pattern p v and for every j ∈ {1, . . . , } the number of neighbors of v with (d − 1)-pattern p ij is exactly m pi j . Here, is the number of distinct neighboring d − 1 patterns of v.</figDesc><table><row><cell>. Top: A graph with 4 nodes. Each color represent a</cell></row><row><cell>different feature. Bottom: The 0,1 and 2-patterns of the black</cell></row><row><cell>node.</cell></row><row><cell>Definition 4.1 (d-patterns). Let C be a finite set of node</cell></row><row><cell>features, and let G = (V, E) be a graph with node feature</cell></row><row><cell>c v ∈</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>|P | and ReLU activation such that for every graph G with nodes v 1 , . . . , v n and corresponding d-patterns p 1 , . . . , p n ⊆ P , the output of this GNN on v i is exactly y pi .</figDesc><table><row><cell>information regarding the d-neighborhood (d hops away</cell></row><row><cell>from the node), and different neighborhoods could have the</cell></row><row><cell>same d-patterns. The full proof can be found in Appendix B</cell></row><row><cell>and follows directly from the analogy between the iterations</cell></row><row><cell>of the WL algorithm and d-patterns.</cell></row><row><cell>Next, the following theorem shows that given a set of d-</cell></row><row><cell>patterns and the desired output for each such pattern, there</cell></row><row><cell>is an assignment of weights to a GNN with d + 2 layers that</cell></row><row><cell>perfectly fits the output for each pattern.</cell></row><row><cell>Theorem 4.3. Let C be a finite set of node features, P be</cell></row><row><cell>a finite set of d-patterns on graphs with maximal degree</cell></row><row><cell>N ∈ N, and for each pattern p ∈ P let y p ∈ R be some tar-</cell></row><row><cell>get label. Then there exists a GNN with d + 2 layers, width</cell></row><row><cell>bounded by max (N + 1) d • |C|, 2 The full proof is in Appendix B. This theorem strengthens</cell></row><row><cell>Theorem 2 from (Morris et al., 2019) in two ways: (1) We</cell></row><row><cell>prove that one can specify the output for every d-pattern</cell></row><row><cell>while (Morris et al., 2019) show that there is a d-layer GNN</cell></row><row><cell>that can distinguish all d-patterns; (2) Our network construc-</cell></row><row><cell>tion is more efficient in terms of width and dependence on</cell></row><row><cell>the number of d-patterns (2 |P | instead of |P |).</cell></row><row><cell>in terms of d-patterns:</cell></row><row><cell>Theorem 4.2. Any function that can be represented by a</cell></row><row><cell>d-layer GNN is constant on nodes with the same d-patterns.</cell></row><row><cell>The theorem states that any d-layer GNN will output the</cell></row><row><cell>same result for nodes with the same d-pattern. Thus, we can</cell></row><row><cell>refer to the output of a GNN on the d-patterns themselves.</cell></row><row><cell>We stress that these d-patterns only contain a part of the</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>± 0.8 63.2 ± 3.3 75.5 ± 1.6 78.4 ± 1.4 75.4 ± 3.1 69.7 ± 0.2 71.1 ± 4.4 70.0% VANILLA 41.1 ± 6.8 55.9 ± 7.8 65.9 ± 4.3 68.9 ± 3.8 76.0 ± 8.5 60.5 ± 3.6 76.3 ± 3.2 63.5% HOMO-GNN 40.5 ± 6.6 56.3 ± 7.0 66.0 ± 3.7 68.8 ± 3.2 77.1 ± 10.0 60.8 ± 2.3 76.8 ± 3.0 76.8 ± 3.0 76.8 ± 3.0 63.8% NM MTL 51.6 ± 8.5 51.6 ± 8.5 51.6 ± 8.5 55.6 ± 6.8 49.9 ± 7.8 61.7 ± 5.7 78.8 ± 8.4 49.5 ± 2.8 67.4 ± 5.4 59.2% NM PT 50.1 ± 7.5 54.9 ± 6.7 51.7 ± 6.6 55.8 ± 5.0 78.2 ± 8.2 48.4 ± 4.0 60.3 ± 15.9 57.1% GAE MTL 49.4 ± 11.0 55.5 ± 6.0 51.2 ± 9.9 57.6 ± 9.4 79.5 ± 11.7 62.5 ± 5.1 67.8 ± 10.0 60.5% GAE PT 47.1 ± 10.0 54.1 ± 6.8 58.9 ± 7.6 67.2 ± 5.6 70.5 ± 9.4 53.6 ± 4.7 69 ± 7.1 60.1% NML MTL 46.4 ± 9.5 54.4 ± 7.0 52.3 ± 6.3 56.2 ± 6.5 78.7 ± 6.8 57.4 ± 4.1 64.7 ± 11.9 58.6% NML PT 48.4 ± 10.7 53.8 ± 6.1 54.6 ± 6.2 56.1 ± 8.1 76.3 ± 8.0 54.9 ± 4.7 61.4 ± 15.1 57.9% CL MTL</figDesc><table><row><cell>DATASETS</cell><cell>DEEZER</cell><cell>IMDB -B</cell><cell>NCI1</cell><cell>NCI109</cell><cell>PROTEINS</cell><cell>TWITCH</cell><cell>DD</cell><cell>AVERAGE</cell></row><row><cell>TOTAL-VAR. DISTANCE</cell><cell>1</cell><cell>0.99</cell><cell>0.16</cell><cell>0.16</cell><cell>0.48</cell><cell>1</cell><cell>0.15</cell><cell>-</cell></row><row><cell>SMALL GRAPHS</cell><cell>56.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Weinberger, K. Q. and Saul, L. K. Distance metric learning for large margin nearest neighbor classification. Journal of Machine Learning Research, 10(2), 2009.</figDesc><table><row><cell>Supplementary Material: From Local Structures to Size Generalization in</cell></row><row><cell>Weisfeiler, B. and Lehman, A. A. A reduction of a graph to a canonical form and an algebra arising during this Graph Neural Networks</cell></row><row><cell>reduction. Nauchno-Technicheskaya Informatsia, 2(9):</cell></row><row><cell>12-16, 1968.</cell></row><row><cell>Xu, K., Hu, W., Leskovec, J., and Jegelka, S. How powerful are graph neural networks? arXiv preprint A. Size generalization in Single-layer GNNs</cell></row><row><cell>arXiv:1810.00826, 2018.</cell></row><row><cell>Xu, K., Li, J., Zhang, M., Du, S. S., Kawarabayashi, K.-i.,</cell></row><row><cell>and Jegelka, S. How neural networks extrapolate: From</cell></row><row><cell>feedforward to graph neural networks. arXiv preprint</cell></row><row><cell>arXiv:2009.11848, 2020.</cell></row><row><cell>You, Y., Chen, T., Sui, Y., Chen, T., Wang, Z., and Shen, Y.</cell></row><row><cell>Graph contrastive learning with augmentations. Advances</cell></row><row><cell>in Neural Information Processing Systems, 33, 2020a.</cell></row><row><cell>You, Y., Chen, T., Wang, Z., and Shen, Y. When does self-</cell></row><row><cell>supervision help graph convolutional networks? arXiv</cell></row><row><cell>preprint arXiv:2006.09136, 2020b.</cell></row><row><cell>Zweig, A. and Bruna, J. A functional perspective on learning</cell></row><row><cell>symmetric functions with neural networks. arXiv preprint</cell></row><row><cell>arXiv:2008.06952, 2020.</cell></row></table><note>Yun, C., Sra, S., and Jadbabaie, A. Small relu networks are powerful memorizers: a tight analysis of memorization capacity. In Advances in Neural Information Processing Systems, pp. 15558-15569, 2019.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Training on large graphs and testing on smaller graphs. Teacher-student task with graphs drawn from a G(n, p) distribution with p = 0.3 and n varies, as described in the table. Results are on a logarithmic scale. As expected, even generalizing from large to small graphs is difficult, but the results improve as the sizes of the graphs in the train set are close to the sizes of the graphs in the test set.Figure9. Two training procedures for learning with SSL tasks. Left: Learning with pretraining: Here, a GNN is trained on the SSL task with a specific SSL head. After training, the weights of the GNN are fixed, and only the main head is trained on the main task. Right: Multitask learning: Here, there is a shared GNN and two separate heads, one for the SSL task and one for the main task. The GNN and both heads are trained simultaneously.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Train graph size</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">[90, 100] [140, 150] [190, 200]</cell></row><row><cell></cell><cell>50</cell><cell>1.87</cell><cell>3.1</cell><cell>4.36</cell></row><row><cell>Test graph size</cell><cell>75</cell><cell>1.93</cell><cell>4.19</cell><cell>4.29</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Results on synthetic datasets (Vanilla vs. d-pattern PT).</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">NVIDIA</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">Bar-Ilan University. Correspondence to: Gilad Yehudai &lt;gilad.yehudai@weizmann.ac.il&gt;.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr">[40,</ref><ref type="bibr">50]</ref> <p>points uniformly in the unit square, and connecting two points if their distance is less than ρtrain = 0.3. The test set domain graphs contain n = 100 nodes, effectively increasing their density by 2. We tested with two different values of ρtrain/ρtest, the ratio between the train and test connectivity radius. A proper scaling that keeps the expected degree of each node is ρ = √ 2. Here, although proper scaling does not help solve the problem completely, it does improve performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Node regression</head><p>Graph where the graph label is the number of edges; (c) A student-teacher node regression task; (d) A node regression task, where the node label is its degree. In the edge count/degree tasks the loss is the mean difference from the ground-truths, divided by the average degree/number of edges. In the student-teacher tasks the loss is the mean L2 loss between the teacher's value and the student's prediction, divided by the averaged student's prediction. Both the student and teacher share the same 3-layer architecture Definition E.1 (d-pattern tree). Let G = (V, E) a graph, C a finite set of node features, where each v ∈ V have a corresponding feature c v , and d ≥ 0. For a node v ∈ V , its d-pattern tree T</p><p>v ) is directed tree where each node corresponds to some node in G. It is defined recursively in the following way:</p><p>, and let</p><p>(i.e. nodes without incoming edges). We define:</p><p>The main advantage of pattern trees is that they encode all the information that a GNN can produce for a given node by running the same GNN on the pattern tree.</p><p>This tree corresponds to the local patterns in the following way: the d-pattern tree of a node can be seen as a multiset of the children of the root, and each child is a multiset of its children, etc. This completely describes the d-pattern of a node. In other words, there is a one-to-one correspondence between d-patterns and pattern trees of depth d. Thus, a GNN that successfully represents the pattern trees of the target distribution will also successfully represent the d-patterns of the target distribution.</p><p>Using the d-pattern tree we construct a simple regression SSL task where its goal is for each node to count the number of nodes in each layer and of each feature of its d-pattern tree. This is a simple descriptor of the tree, which although loses some information about connectivity, does hold information about the structure of the layers.</p><p>For example, in Fig. <ref type="figure">4</ref> the descriptor for the tree would be that the root (zero) layer has a single black node, the first layer has two yellow nodes, the second layer has two yellow, two gray, and two black nodes, and the third layer has ten yellow, two black and two gray nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Training Procedure</head><p>In this section, we explain in detail the training procedure used in the experiments of Sec. 7. Let X M ain and X SSL be two labeled datasets, the first contains the labeled examples for the main task from the source distribution, and the second Table <ref type="table">6</ref>. Results on the ogbg-molpcba datasets from the OGB collection. The dataset contains 128 different tasks, we used the tasks with the most balanced labels (there is no class with more than 90% of the samples). In both settings, we emitted the node and edge features as our method does not support edge features and continuous node features (only categorical node features). The results are inconclusive, with a slight edge toward vanilla training. for improving size generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3. OGB dataset</head><p>We additionally tested our framework on 8 tasks from the "ogbg-molpcba" dataset from the OGB dataset collection <ref type="bibr" target="#b19">(Hu et al., 2020)</ref>. These are binary classification tasks, where we choose tasks for which there is no one class with more than 90% of the samples (i.e. the tasks with the most balanced labels). We compare our SSL task with pretraining vs. vanilla training. We note that since our task assumes only categorical features and bidirectional edges with no features, we did not use the node and edge features. We split the datasets by size, in the same manner, we did in the experiments from Table <ref type="table">1</ref>, where we trained on the 50% smallest graphs and tested on the 10% largest graphs. We report the accuracy. For the results see Table <ref type="table">6</ref>. The results are inconclusive due to the lack of features which is valuable information not used by our model. We also tested using the given split from the OGB repository but found the same inconclusive results. In the future, it will be interesting to generalize our method to handle continuous node features and edge features. We hope that by using these features our method could also improve on the size generalization task for datasets from the OGB collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.4. Datasets statistics</head><p>Table <ref type="table">G</ref>.4 shows the statistics of the datasets that were used in the paper. In particular, the table presents the split that was used in the experiments, we trained on graphs with sizes smaller or equal to the 50-th percentile and tested on graphs with sizes larger or equal to the 90-th percentile. Note that all the prediction tasks for these datasets are binary classification. In all the datasets there is a significant difference between the graph sizes in the train and test sets, and in some datasets, there is also a difference between the distribution of the output class in the small and large graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.5. Correlation Between Size Discrepancy and d-pattern Discrepancy</head><p>In this section, we show that in many real datasets, there is a high correlation between the sizes of the graphs and their d-patterns. This motivates the effect that d-patterns have on the ability of GNNs to generalize to larger sizes than they were trained on.</p><p>We focused on the datasets that were tested in Sec. 7: Biological datasets: NCI1, NCI109, D &amp; D, Proteins. Social networks: IMDB-Binary, Deezer ego nets, Twitch ego nets. To this end, we split each dataset to the 50% smallest graphs and 10% largest graphs. We calculated the distribution of 2-patterns for the 20 most common two patterns in each split (smallest and largest graphs) and compared these two distributions. The results are depicted in Fig. <ref type="figure">11</ref> (The plot for the Table <ref type="table">8</ref>. Total variation distance, between the 2-patterns of the 50% smallest graphs and 10% largest graphs for all the real datasets that we tested on.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Self-supervised learning for domain adaptation on point-clouds</title>
		<author>
			<persName><forename type="first">I</forename><surname>Achituve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.12641</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Emergence of scaling in random networks</title>
		<author>
			<persName><forename type="first">A.-L</forename><surname>Barabási</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Albert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">286</biblScope>
			<biblScope unit="issue">5439</biblScope>
			<biblScope unit="page" from="509" to="512" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Evolution of the social network of scientific collaborations</title>
		<author>
			<persName><forename type="first">A.-L</forename><surname>Barabâsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Néda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ravasz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vicsek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica A: Statistical mechanics and its applications</title>
		<imprint>
			<biblScope unit="volume">311</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="590" to="614" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">On single-environment extrapolations in graph classification and regression tasks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ribeiro</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=wXBt-7VM2JE" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Limitations for learning from point clouds</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bueno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hylton</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1x63grFvH" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Preferential attachment in the growth of social networks: The internet encyclopedia wikipedia</title>
		<author>
			<persName><forename type="first">A</forename><surname>Capocci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">D</forename><surname>Servedio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Colaiori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Buriol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Donato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Leonardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Caldarelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review E</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">36116</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Can graph neural networks count substructures?</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.04025</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Principal neighbourhood aggregation for graph nets</title>
		<author>
			<persName><forename type="first">G</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cavalleri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05718</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Póczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13192</idno>
		<title level="m">Graph neural tangent kernel: Fusing graph neural networks with graph kernels</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Preferential attachment in the protein network evolution</title>
		<author>
			<persName><forename type="first">E</forename><surname>Eisenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Y</forename><surname>Levanon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review letters</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page">138701</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Fast graph representation learning with pytorch geometric</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02428</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Generalization and representational limits of graph neural networks</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06157</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07728</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01212</idno>
		<title level="m">Neural message passing for quantum chemistry</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
				<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Monte carlo convolution for learning on non-uniformly sampled point clouds</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hermosilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ritschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-P</forename><surname>Vázquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">À</forename><surname>Vinacua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ropinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12265</idno>
		<title level="m">Strategies for pre-training graph neural networks</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Self-supervised learning on graphs: Deep insights and new direction</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Derr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10141</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">An efficient graph convolutional network technique for the travelling salesman problem</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01227</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cappart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-M</forename><surname>Rousseau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07054</idno>
		<title level="m">Learning tsp requires rethinking generalization</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning combinatorial optimization algorithms over graphs</title>
		<author>
			<persName><forename type="first">E</forename><surname>Khalil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dilkina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6348" to="6358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016">2016b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Understanding attention and generalization in graph neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Knyazev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Amer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4202" to="4212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Combinatorial optimization with graph convolutional networks and guided tree search</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="539" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A pac-bayesian approach to generalization bounds for graph neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.07690</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Preferential attachment in the evolution of metabolic networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Light</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kraulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elofsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bmc Genomics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning algebraic multigrid using graph neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Luz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Galun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Yavneh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.05744</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Invariant and equivariant graph networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.09902</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Towards a practical kdimensional weisfeiler-leman algorithm</title>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01543</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4602" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">URL www.graphlearning. io. Nachmani, E. and Wolf, L. Molecule property prediction and classification with graph hypernetworks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.00240</idno>
	</analytic>
	<monogr>
		<title level="m">ICML 2020 Workshop on Graph Representation Learning and Beyond (GRL+ 2020)</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Tudataset: A collection of benchmark datasets for learning with graphs</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Symmetric graph convolutional autoencoder for unsupervised graph representation learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6519" to="6528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">From graph low-rank global attention to 2-fwl approximation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Puny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07846</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Graph contrastive coding for graph neural network pre-training</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><surname>Gcc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1150" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Karate Club: An API Oriented Open-source Python Framework for Unsupervised Learning on Graphs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Rozemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Information and Knowledge Management (CIKM &apos;20)</title>
				<meeting>the 29th ACM International Conference on Information and Knowledge Management (CIKM &apos;20)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3125" to="3132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Keep it simple: Graph autoencoders without graph convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Salha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hennequin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vazirgiannis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.00942</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning to simulate complex physics with graph networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Godwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pfaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.09405</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Measuring abstract reasoning in neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4477" to="4486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Evaluating logical generalization in graph neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sodhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.06560</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation through self-supervision</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11825</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Towards scale-invariant graph-related problem solving by iterative homogeneous graph neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2020 Workshop on Graph Neural Networks &amp; Beyond</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep graph infomax</title>
		<author>
			<persName><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Padovano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10593</idno>
		<title level="m">Neural execution of graph algorithms</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Stability and generalization of graph convolutional neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1539" to="1548" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
