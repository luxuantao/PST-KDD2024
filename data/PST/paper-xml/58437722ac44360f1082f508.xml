<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint CTC-Attention based End-to-End Speech Recognition using Multi-task Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Suyoun</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Mitsubishi Electric Research Laboratories</orgName>
								<address>
									<country>MERL</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Mitsubishi Electric Research Laboratories</orgName>
								<address>
									<country>MERL</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Mitsubishi Electric Research Laboratories</orgName>
								<address>
									<country>MERL</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">¬©</forename><surname>Merl</surname></persName>
						</author>
						<title level="a" type="main">Joint CTC-Attention based End-to-End Speech Recognition using Multi-task Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0" /><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>‚Ä¢‚Ä¢</head><label></label><figDesc>Introduction and motivation ‚Ä¢ Our proposed model: Joint CTC/Attention ‚Ä¢ Experiments and results ‚Ä¢ Conclusion Conventional ASR is Complicated ‚Ä¢ Many sub-components -System development is complicated -Separate modeling may cause suboptimal -Decoding algorithm is complex ‚Ä¢ Many assumptions -Assumes future process only depends on current state not previous state (Markovian, Stationary) ‚Ä¢ ‚Ä¢ -Assumes observations are independent given state (Conditional independent) ‚Ä¢ -Assumes all pronunciations can be represented by several phonemes (hand-crafted knowledge) -End ASR is transcribing speech signal to text directly with a single model, one step training Our Joint CTC/Attention model for End-to-End ASR ‚Ä¢ Key insight: -We can address the weaknesses of two main End-to-End approaches 1) CTC, and 2) Attention model by combining the two, as they have complementary characteristics It uses intermediate label representation ùúã allowing repetitions and blank labels "_" ‚Ä¢ It maximizes the total probability of all possible label sequence ùúã ‚Ä¢ It uses forward-backward algorithm for the efficient training Strength: There is no need for pronunciation model Weakness: It still relies on conditional independence assumption, typically separate LM is combined End-to-End approach 1/Attention model for End-to-End ASR ‚Ä¢ We keep our model simple -By using Attention model to learn LM jointly two RNNs 1) Encoder 2) AttentionDecoder ‚Ä¢ For each output step, it estimates weight vector(alignment) over inputs and then decoder uses weighted sum input ‚Ä¢ Decoder estimates each label conditioning on previous outputs (no conditional independent assumption) Strength: It can learn acoustic and language model within a single network Weakness: The alignment can be easily distorted End-to-End approach 2: Attention-based Encoder-Decoder [Chorowski(2014)] regularize input/output alignment of attention HMM or CTC case Example of distorted alignment! Attention model case ‚Ä¢ Unlike CTC, Attention model does not preserve order of inputs ‚Ä¢ Our desired alignment in ASR task is monotonic ‚Ä¢ Not regularized alignment makes the model hard to learn from scratch Input output Example of monotonic alignment! Input output ‚Ä¢ We keep our model simple -By using Attention model to learn inter-character dependencies jointly ‚Ä¢ We improve the learning speed and performance -By using CTC to regularize the input/output alignment si284) -80 hours clean -CHiME4 -18 hours noisy -Input -120d filterbank (+d, +dd) -Output -32 distinct label (+26 char, + apostrophe, period, ‚Ä¶, sos/eos) ‚Ä¢ Baselines -CTC -4 layer BLSTM (320 cells) -Attention -4 layer BLSTM encoder (320 cells) + 1 layer LSTM decoder (320 cells), location-based attention mechanism ‚Ä¢ Our Joint CTC/Attention model -4 layer BLSTM encoder (320 cells) + 1 layer LSTM decoder (320 cells) -With ùû¥= {0.2 0.5 0.8} ‚Ä¢ Evaluation -Character Error Rate (CER)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-163.png" coords="12,137.28,181.22,444.56,334.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-170.png" coords="13,144.22,185.84,430.68,321.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-198.png" coords="17,125.18,140.40,512.78,320.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-205.png" coords="18,197.57,94.01,323.52,216.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-212.png" coords="19,197.80,88.41,323.52,216.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-219.png" coords="20,197.80,94.01,323.52,216.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-226.png" coords="21,21.60,389.06,677.26,124.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-227.png" coords="21,21.13,185.63,676.87,124.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>performs best! Larger ùû¥ gives more weight on CTC 14</head><label></label><figDesc>Faster convergence compared to Attention model .6% relative improvement of CER on WSJ0(15hr)</figDesc><table><row><cell cols="5">9.9% relative improvement of CER on WSJ1(80hr) 5.4% relative improvement of CER on CHiME4(18hr) More robust input/output alignment of attention</cell></row><row><cell cols="4">‚Ä¢ Alignment of one selected utterance from CHiME4</cell><cell></cell></row><row><cell>Good!</cell><cell></cell><cell></cell><cell cols="2">Lower is Better! Lower is Better! Lower is Better!</cell></row><row><cell>Converge Fast</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ùû¥=0.2 ùû¥=0.2</cell><cell></cell><cell>Dev Dev Dev</cell><cell>Eval Eval Eval</cell><cell></cell></row><row><cell>performs best! performs best!</cell><cell>CTC CTC CTC</cell><cell>11.5 27.4 37.6</cell><cell>9.0 20.3 48.8</cell><cell></cell></row><row><cell>Larger ùû¥ gives more weight on CTC Larger ùû¥ gives more weight on CTC</cell><cell>Attention OurModel ( ùû¥ =0.2) OurModel ( ùû¥ =0.5) OurModel (ùû¥ =0.8) Attention OurModel ( ùû¥ =0.2) OurModel ( ùû¥ =0.5) OurModel (ùû¥ =0.8) Attention OurModel ( ùû¥ =0.2) OurModel ( ùû¥ =0.5) OurModel (ùû¥ =0.8)</cell><cell>12.0 11.3 12.0 11.7 25.0 23.0 26.3 32.2 35.0 32.1 34.6 35.4</cell><cell>8.2 7.4 8.3 8.5 17.0 14.5 16.2 21.3 47.6 45.0 46.5 48.3</cell><cell>9.9% improvement 14.6% improvement 5.4% improvement</cell></row></table><note>WER of our best system was 18.2% WER of (Bahdanau, et al. ICASSP 2016) was 18.6% ùû¥=0.2</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Attention Model Our Joint CTC/Attention Model</head><label></label><figDesc>Attention model -does not use any linguistic information -shows 5.4 -14.6 % relative improvements in CER, compared to Attention-based Encoder-Decoder -speeds up learning process -requires small additional computational cost but only in training mode, not in decoding mode.‚Ä¢ Our framework can be applied to other seq2seq tasks where its alignment is monotonicCurrent research‚Ä¢ Further experimental results on Corpus of Spontaneous Japanese (CSJ) -581hr -Achieved comparable performance to state-</figDesc><table><row><cell>Outline Conclusion</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">‚Ä¢ Introduction and motivation ‚Ä¢ Joint CTC/</cell><cell></cell><cell></cell></row><row><cell cols="4">‚Ä¢ Our proposed model: Joint CTC/Attention Input ‚Ä¢ Experiments and results output ‚Ä¢ Conclusion</cell><cell>Corrupted!</cell></row><row><cell>Epoch 1</cell><cell>Epoch 3</cell><cell>Epoch 5</cell><cell>Epoch 7</cell><cell>Epoch 9</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Good! Monotonic!</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>of-the-art</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>task1</cell><cell>task2</cell><cell>task3</cell></row><row><cell>Attention (581h)</cell><cell>11.5</cell><cell>7.9</cell><cell>9.0</cell></row><row><cell>OurModel (581h)</cell><cell>10.9</cell><cell>7.8</cell><cell>8.3</cell></row><row><cell>OurModel2 (581h)</cell><cell>9.5</cell><cell>7.0</cell><cell>7.8</cell></row><row><cell>DNN/sMBR-hybrid (236h for AM/ 581h for LM)</cell><cell>9.0</cell><cell>7.2</cell><cell>9.6</cell></row><row><cell>CTC-syllable (581h)</cell><cell>9.4</cell><cell>7.3</cell><cell>7.5</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">¬© MERL</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1">Questions &amp; Answers</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Our Joint CTC/Attention model for End-to-End ASR 1. We share the encoder part 2. We train Attention model with CTC jointly 3. We use AttentionDecoder on decoding mode -The cost for CTC exists only on training mode Larger ùû¥ will give more weight on CTC objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global normalization</head><p>Local normalization</p></div>			</div>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
