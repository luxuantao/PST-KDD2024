<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Goal-based Course Recommendation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Weijie</forename><surname>Jiang</surname></persName>
							<email>jiangwj@berkeley.edu</email>
						</author>
						<author>
							<persName><forename type="first">Zachary</forename><forename type="middle">A</forename><surname>Pardos</surname></persName>
							<email>pardos@berkeley.edu</email>
						</author>
						<author>
							<persName><forename type="first">Qiang</forename><surname>Wei</surname></persName>
							<email>weiq@sem.tsinghua.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="laboratory">LAK19</orgName>
								<address>
									<addrLine>March 4-8</addrLine>
									<postCode>2019</postCode>
									<settlement>Tempe</settlement>
									<region>AZ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Goal-based Course Recommendation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3303772.3303814</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With cross-disciplinary academic interests increasing and academic advising resources over capacity, the importance of exploring dataassisted methods to support student decision making has never been higher. We build on the findings and methodologies of a quickly developing literature around prediction and recommendation in higher education and develop a novel recurrent neural network-based recommendation system for suggesting courses to help students prepare for target courses of interest, personalized to their estimated prior knowledge background and zone of proximal development. We validate the model using tests of grade prediction and the ability to recover prerequisite relationships articulated by the university. In the third validation, we run the fully personalized recommendation for students the semester before taking a historically difficult course and observe differential overlap with our would-be suggestions. While not proof of causal effectiveness, these three evaluation perspectives on the performance of the goal-based model build confidence and bring us one step closer to deployment of this personalized course preparation affordance in the wild.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The terrain of a university degree program can be difficult to successfully traverse. Challenging decisions abound, such as which major to declare, subject(s) to explore, and level of difficulty of course load to take on. These decisions involve hard to balance risk vs. reward trade-offs, made more difficult by the multiple objectives students want to maximize and risks they want to hedge against (e.g., choosing challenging courses of value to employers while maintaining high GPA). Given the abundance of historic data on student enrollments, grades, and majors, a question naturally arises if learning analytics approaches can extract any wisdom from these records that may aid students in achieving their goals. In this paper, we build on the findings and methodologies of a quickly developing literature around prediction and recommendation in higher education to introduce an approach to goal-based course recommendation.</p><p>As multidisciplinary educational interests increase, such as with data science, so too does the importance of providing appropriate intellectual on-ramps to subject matter that promotes equity and inclusion in these pursuits. This means providing pathways to success for students from various disciplinary backgrounds. We focus on this particular goal of finding appropriate preparation course(s), given one's existing curricular exposure, for a target course of interest. There are a variety of reasons why existing prerequisite course information provided by the university may not be satisfactory:</p><p>(1) the prerequisites may not be up to date (2) they may not be comprehensive, neglecting to include combinations of courses from different departments that together would cover the requirement material (3) they do not take into account what an individual student already knows, and are thus often bypassed by students if not enforced, and (4) they may consist of often oversubscribed courses for which students may have no choice but to seek alternatives for. Our proposed approach addresses these four potential shortcomings, especially by tailoring suggestions of the preparatory class based on a model of the knowledge a student has already acquired.</p><p>The task of recommending a set of appropriate courses personalized to any student's course history and any arbitrary target course is arguably of intractable difficulty for one person. Faculty tend to be local experts with deep knowledge within their subject area. Non-faculty academic advisers have broader course familiarity, but at the expense of depth, and both resources are scarce in higher education compared to the number of students enrolled. Machine learning models can scale and benefit from the breadth and depth of representations learned from big data but lack the ability to easily tease apart the difference between correlation and causation based on observations. We explore if, given enough constraints, reasonable suggestions can be reliably extracted from such a model. We choose three prediction validations (grade prediction, prerequisite prediction, and course selection prediction) intended to, collectively, suggest if the approach may warrant testing in the wild. We choose Recurrent Neural Networks (RNN) as the framework to extend to this goal-based recommendation task due to their robust representational and temporal capabilities. While RNNs have been previously applied to make recommendations based on collaborative filtering principles <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23]</ref>, they have not been re-purposed to make more targeted personalized goal-based recommendations in any domain. Our validation and application of RNN typologies to a goal-based task is thus a novel contribution of the work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Recent findings suggest that models capturing co-enrollment information surpass those only using features of students or of courses in capturing variation in student performance <ref type="bibr" target="#b10">[11]</ref>. Part of the benefit of co-enrollment information in predicting performance stems from capturing the interaction effect that can occur when a student takes two difficult courses at once <ref type="bibr" target="#b5">[6]</ref>. Other feature engineering approaches to course grade prediction found that better discrimination between grade labels could be achieved by binarizing the ordinal grade label <ref type="bibr" target="#b25">[26]</ref>. This same work found that student features better predicted high grades compared to course features, which predicted the low grade class, though low grades remained difficult to predict in part due to being the minority class. Features of student and course proved useful to the task of elective course selection prediction but were substantially outperformed when feature sets of student-course ratings could be taken into account <ref type="bibr" target="#b9">[10]</ref>. Classical collaborative filtering was used to predict course grades, which served as weights on an inferred pre-requisite graph meant to potentially help students towards higher rates of on-time graduation <ref type="bibr" target="#b3">[4]</ref>. Though high accuracy was achieved, their context was limited, using only 72 courses from a single department. Work focusing on prediction of on-time graduation has found neural embeddings of students from enrollment sequences to be accurate, particularly after a student's second year of study <ref type="bibr" target="#b20">[21]</ref>.</p><p>The use of an RNN to predict student course grades can be seen as a type of assessment model. RNNs have been used for assessment in the educational contexts of games, to predict outcomes based on game activity <ref type="bibr" target="#b1">[2]</ref>, and to predict responses to questions of various skills given response histories in math tutoring systems <ref type="bibr" target="#b24">[25]</ref>. Their inferences have been used to produce pre-requisite graphs in the same math tutoring contexts as well as in MOOCs <ref type="bibr" target="#b8">[9]</ref>. They have also been used as models of behavior prediction, to suggest the next resource a learner is likely to spend time on next <ref type="bibr" target="#b23">[24]</ref>. In the context of higher education, they have been used in a deployed campus system to predict the next courses a student is likely to enroll in, given their course taking history <ref type="bibr" target="#b22">[23]</ref>.</p><p>The intended or realized operationalization of grade prediction models have primarily been applied to early-warning type systems meant to signal to students or advisers when struggle is occurring or is imminent <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27]</ref>. Early-warning implementations like this can experience unintended consequences, however, such as leading to greater course drop-out <ref type="bibr" target="#b18">[19]</ref>. One system attempted a more pre-emptive approach, showing grade distributions of courses to students and common course sequences for each course before enrollment; however, an evaluation of the system found that course selection behaviors were not affected by the system but GPA was, leading to an unexpected quarter of a grade point decrease in GPA <ref type="bibr" target="#b7">[8]</ref>. These findings underscore the daunting task of achieving meaningful academic improvement as a result of analytic intervention; however, a more specific lesson can be learned. Both instances of intervention underachievement have in common grade related data being shown directly to students. This may have the effect of signaling that pathways are closed off or that a particular achievement can be expected regardless of effort. We believe it is therefore important for analytic-based interventions to encourage learners to set goals and for these interventions to strive to be more prescriptive in scaffolding avenues to achieving them.</p><p>While on-campus course analytics have focused on betweencourse enrollment data, tangential research in MOOCs has focused on models of predicting course outcomes based on within-course activities <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">20]</ref>. It should be noted that even given a positive validation and eventual real-world beneficial effect of our approach, it would not be a panacea for student success. There is no shortage of dimensions to the story of student achievement in higher education. Work showing the correlation between course outcomes and timely access to course materials among late enrolling students <ref type="bibr" target="#b0">[1]</ref> serves as a reminder of this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GOAL-BASED RECOMMENDATION APPROACH</head><p>We build our approach around several assumptions. The first is that students have a zone of proximal development <ref type="bibr" target="#b6">[7]</ref> with respect to course material and that course recommendations should be limited to courses they are expected to be able to succeed in. This necessitates a predictive model of course grades to be trained, akin to the Deep Knowledge Tracing neural framework <ref type="bibr" target="#b24">[25]</ref> applied to tutoring system. The second assumption is that such a model of course performance is capable of inferring prerequisite information that can subsequently be used to recommend courses anticipated to be appropriate preparation for a target course. To validate this assumption, we use the university's existing prerequisite courses list and test the grade prediction model's ability to infer these existing dependencies. Lastly, we assume that the recommendations generated by our model ought to be followed more frequently by students who succeed in a target course than students who underachieve.</p><p>This assumption gives way to our third validation, predicting the previous semester course enrollments before a historically difficult course in the next semester. A relevant example of correlation not equalling causation is the case of students who take an honors course being likely to do well in courses in the subsequent semester, not because of the intrinsic preparatory value of the honors course but because the self-selected students who take them are generally high achieving. We acknowledge this confound but believe that this validation, paired with the first assumption, of not recommending courses to students they are unlikely to pass, mitigates the concern. Furthermore, we limit recommendation to courses that are not of a higher level than the target course, according to the three division levels indicated by the course number (i.e., lower division, upper division, and graduate). We also constrain the recommendations to departments that contain prerequisite courses to other courses found in the target course's department. We hypothesize that these constraints mitigate the chances of an egregiously poor recommendation being made due to confounds in the data. Traditional Recurrent Neural Networks (RNNs) have been used to predict the next action in a sequence. This amounts to a collaborative recommendation of the nature of "most people like you did X next. " When it comes to students' diverse intentions in selecting courses, a student's goal may not align with what most people have done. A simple approach could be to only train on students who achieved the intended goal; however, this approach is unsatisfying as it would eliminate data points that could be used to learn more robust representations of the domain. It is also undesirable as it would require that thousands of independent models be trained to correspond to our task of arbitrary target course preparation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Definition</head><p>A canonical RNN takes as input a sequence of vectors x 1 , ..., x T , mapped to a predicted output sequence of vectors y 1 , ..., y T . This is achieved by computing a sequence of 'hidden' states h 1 , ..., h T , which can be viewed as successive encodings of relevant information from past observations that will be useful for future predictions.</p><p>We use a popular variant of RNNs called Long Short-Term Memory (LSTM) <ref type="bibr" target="#b17">[18]</ref>, which helps RNNs learn temporal dependencies with the addition of several gates which can retain and forget select information 1 . They have been shown to generalize using long sequences more effectively than simple RNNs <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14]</ref> by using a gated structure to mitigate the vanishing gradient phenomenon. While our sequences are not long, our prediction task can benefit from the ability to selectively treat certain course grades as irrelevant (i.e. forgettable) to the prediction of future course grades. The decision of what information to forget at time step t is made by the forget gate f t , where д t is the course grades representation at time step t and h t −1 is the hidden state of the network at time step t-1.</p><formula xml:id="formula_0">f t = σ (W f д д t + W f h h t −1 + b f )</formula><p>The decision of which information will be stored into the cell is determined by calculating the input gate i t and a candidate value C t to be added to the state.</p><formula xml:id="formula_1">i t = σ (W iд д t + W ih h t −1 + b i ) C t = tanh(W Cд д t + W Ch h t −1 + b C )</formula><p>We denote C t as the internal cell state and update it by the intermediate results calculated below.</p><formula xml:id="formula_2">C t = f t × C t −1 + i t × C t</formula><p>Finally, we calculate LSTM's hidden state of time step t, denoted as</p><formula xml:id="formula_3">h t . o t = σ (W oд д t + W oh h t −1 + b o ) h t = o t × tanh(C t )</formula><p>Intuitively, a simple LSTM can be applied to the grade prediction task, where the output in each time slice is a vector representing the probability of receiving a certain grade for each of courses in the next time slice (semester), as is shown Figure <ref type="figure" target="#fig_0">1</ref>, referred to as Model 1 in the rest of the paper.</p><p>However, recent findings suggest that not only students' grades in previous semesters will influence their grades in the current semester, but also the course co-enrollment composition in the current semester will impact their performance <ref type="bibr" target="#b5">[6]</ref>. The reasons lie in interaction effects among courses enrolled in together, such as (1) 1 RNNs were evaluated; however, consistently underperformed LSTMs  <ref type="formula" target="#formula_5">2</ref>) positive synergistic effective among courses, for example, learning Data Structures and Discrete Math together may reinforce learning between courses because they share similar content. Hence, we present a variation on the simple LSTM which concatenates a multi-hot of courses co-enrolled in for the current semester t + 1 (without grades) to the input of course grades from the previous semester t as the input, aiming at predicting grades for semester t + 1. See Figure <ref type="figure" target="#fig_1">2</ref> for illustration of the model, referred to as Model 2 in the rest of the paper.</p><p>Moreover, we assume that student major may affect the expected grade distributions of courses. For example, it may be expected that students achieve higher or lower grades in courses outside their major. Therefore, we present another variant of LSTM which concatenates major to grades in semester t. We feed the multi-hot of courses co-enrolled to a linear layer concatenated with the hidden layer. This means courses co-enrolled information in semester t + 1 will only influence that semester without influencing all the hidden states and outputs in the following time slices<ref type="foot" target="#foot_0">2</ref> . See Figure <ref type="figure" target="#fig_2">3</ref> for illustration of the model, referred to as Model 3 in the rest of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Input and Output Time Series</head><p>In order to train an LSTM on student enrollment grades sequences, it is necessary to convert an enrollment grades sequence into a sequence of fixed length input vectors д t . Note that a student can select several courses in a semester and get either a categorical grade, e.g., A, B, C, D, or a binary grade, i.e., Pass and No-pass for each enrolled course. Hence, the input should be designed to consider all the information in the enrollment sequences. Assume that there are n courses and m categorical grades for each course in total. Let д t represent the grades of a student for all the courses enrolled in semester t, and specifically, д i t to be a student's grade in semester t for course i. Therefore, д t is set to be a multi-hot encoding to represent the combination of which courses were enrolled and which grades were received for those courses, and д i t is set to be a one-hot encoding to represent the grade for course i in semester t. </p><formula xml:id="formula_4">д i t = (s 1 i , s 2 i , ..., s m i , s P ass i , s N o−P ass i )<label>(1)</label></formula><formula xml:id="formula_5">and д t = (д 1 t , д 2 t , ..., д n t )<label>(2)</label></formula><p>So д i t ∈ {0, 1} m+2 and д t ∈ {0, 1} (m+2) * n . We set c t to be a multi-hot encoding of multiple courses enrolled in semester t, where</p><formula xml:id="formula_6">c t = (c 1 t , c 2 t , ..., c n i )<label>(3)</label></formula><p>Additionally, a student can have multiple majors in a semester. Therefore, we set m t to be another multi-hot encoding of a student's majors in semester t,</p><formula xml:id="formula_7">m t = (m 1 t , m 2 t , ..., m k i ) (4)</formula><p>where k is the number of all possible majors.</p><p>For the models proposed in Section 3.1, inputs are different concatenations of д t , c t and m t , and outputs are always д t , meaning that we only predict student grades in next semester with variations of input feeding to LSTM, as are shown in Figures <ref type="figure" target="#fig_6">1, 2 and 3</ref>. It is worth mentioning that in Figure <ref type="figure" target="#fig_0">1</ref>, the output in a certain time slice t + 1 is predicting the conditional probability of course grades in semester t + 1 given all the historical course grades of a student, i.e., P(д t +1 |д 1 , ..., д t ). In Figure <ref type="figure" target="#fig_1">2</ref>, the output in a certain time slice t + 1 is predicting the conditional probability of course grades in semester t +1 given all the historical enrolled courses and their grades of a student , i.e., P(д t +1 |д 1 c 2 , ..., д t c t +1 ). Lastly, in Figure <ref type="figure" target="#fig_2">3</ref>, the output in a certain time slice t + 1 is predicting the conditional probability of course grades in semester t + 1 given all the historical course grades of a student, the courses taken in semester t + 1, and all the student's historical majors, i.e., P(д t +1 |д 1 c 2 m 1 , ..., д t c t +1 m t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Custom Masked Loss Function for Optimization</head><p>Only grade labels for the courses a student enrolls in and completes can be used to calculate the loss. Therefore, the predictions of courses not taken in the next semester must be masked so as not to affect the loss calculation. The training objective is the negative log-likelihood of the observed sequence of student grades under the model. Let дt+1 be the multi-hot encoding of which grade is received for which courses in semester t + 1, which is the label for our prediction, normally, cross entropy loss will be applied to maximize the similarity between the distributions of the output of softmax layer and the label.</p><formula xml:id="formula_8">loss = − t дT t +1 logд t +1<label>(5)</label></formula><p>However, as mentioned, in the course enrollment grade prediction scenario, the course grades in a semester should not affect the grades of the courses not enrolled in for the next semester. For the grade of a course, д i t , a student can receive only letter grade or Pass/No-Pass grade, i.e., one-hot in either (s 1 i , s 2 i , ..., s n i ) or (s P ass i , s N o−P ass i ). Therefore, two modifications to the loss function in Equation 5 are designed to deal with the two points above.</p><p>• Not-enrolled in Courses Grade (first level) Mask Grades for not-enrolled in courses in semester t + 1 according to the labels, дt+1 , are masked in the loss function. ) and mask the unrelated one.</p><p>The architecture of the two-level masked loss is illustrated in Figure <ref type="figure" target="#fig_3">4</ref>, which covers all the cases in the masked loss function. Specifically, Figure <ref type="figure" target="#fig_3">4</ref> assumes three courses in which course 2 and course 3 are enrolled in and with A and Pass received, respectively, according to the labels where a blue circle represents the real grade. Therefore, according to the not-enrolled in courses grade (first level) mask, all the outputs related to course 1 should be masked according to the unrelated grade type (second level) mask, Pass/No-Pass part and letter grade part of course 2 and course 3 in the outputs should be masked, respectively. The first level and second level masked losses are not calculated and back-propagated while only loss 1 and loss 2 are calculated and back-propagated through the network to update the weights of the model.</p><p>The overall modified loss function is naturally expressed as:</p><formula xml:id="formula_9">loss = − t i, дi t +1 0 ( д i1 T t +1 logд i1 t +1 + д i2 T t +1 logд i2 t +1 )<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DATASET</head><p>We used a dataset from University of California, Berkeley, which contained anonymized student course enrollments from Fall 2008 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">STUDENT GRADE PREDICTION</head><p>In this section, we explain how we train the models proposed in Section 3.1 to predict course grades. Based on the sequential information in our data set, we separated the data set into three parts by time for evaluation, i.e., data from Fall 2008 to Fall 2015 as training set, data in Spring 2016 as validation set and data in Spring 2017 as test set. The loss function in Equation 6 was minimized using stochastic gradient descent on minibatches. To prevent overfitting during training, dropout was applied to the last linear layer to compute the output. Weight decay and gradient clipping were also applied to prevent overfitting. We tuned the model on the dimensionality of the hidden layer and found 50 to be consistently the best for all the models. We fixed the mini-batch size to 32. Notably, in the goalbased recommendation scenario, we choose to allow for students to set the achievement level desired in the target course to either the A or B level. We refined the input and output of our model by setting this threshold (A or B), and then converted the categorical grades to binary classes, i.e., 'above or equal to grade threshold' and 'below threshold'. In all the following experiments, we tried both A and B as thresholds <ref type="foot" target="#foot_1">3</ref> . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results</head><p>We applied accuracy and F-score for the two binary classification tasks, i.e., letter grade classification and Pass/No-pass grade classification, which are shown in Table <ref type="table" target="#tab_2">2</ref>. The column 'letter grade' shows the classification accuracy of 'above or equal to grade threshold' for the letter grade type, while the column 'pass' represents the classification accuracy of pass grade for the Pass/No-pass grade type. The column 'F-score' is for the letter grade classification. The results show modest prediction performance. Current semester coenrollment information was useful (Model 2 vs. Model 1) in the case of both threshold models. Major (Model 3) was not useful in either A or B threshold models compared to without (Model 2). The reason may lie in that the differences among students' majors may be already embedded in their various enrollment patterns, which means major information is not able to further boost the model's discriminative power in grade prediction. While the B threshold models performed better in terms of accuracy, they were also much closer to the performance of the majority class baseline (88.05 vs. 85.46). It will be tested in the next section if this close proximity to baseline performance prohibits the model from containing valid prerequisite relationships in its course embedding. The A model performed better than the B model in terms of F-score (60.24 vs. 42.01) and in terms of gain over baseline. In the case of the A model, major information was able to improve predictions in Pass grade, whereas major may have led to overfitting in the B model, given the strong majority class. In the following sections, we only evaluate these best models depicted in Figures <ref type="figure" target="#fig_6">2 and 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">PREREQUISITE COURSES PREDICTION</head><p>Instructor specification of prerequisites for his or her course is meant to ensure students have the necessary foundation and experience to be able to learn and succeed in the course. In order for our grade prediction based model to have utility in recommending preparation courses, it ought to encode the prerequisite relations between course pairs already specified by some courses. We designed a technique, inspired from <ref type="bibr" target="#b24">[25]</ref>, to explore those pairs based on the learned model, which does not need any other model training.  Note that, for this evaluation, only one time slice input of the grade prediction trained LSTM is needed. The illustrative structure for predicting the prerequisite course for a target course of Model 2 is shown in Figure <ref type="figure" target="#fig_4">5</ref>. With a student specified grade threshold of A as an example, the steps are 45 :</p><p>(1) Set c 1 to be a one-hot vector with only the position for the target course to be 1 and others positions to be 0. (2) Iterate д 1 over all the courses with only one-hot embedded in the 'above or equal to grade A' position for that course, and feed the input, which is a concatenation of c 1 and д 1 , to the learned model described in section 5. ( <ref type="formula" target="#formula_6">3</ref> .</p><p>(4) Rank P A t ar дet and select the top 10 6 input courses with regard to the value of its output of P A t ar дet . We used a set of 2,300 prerequisite course pairs, provided by the UC Berkeley Office of the Registrar, which contains 1,215 target courses, as a source of ground truth to serve as a validation set to test our assertion that the model encodes such relationships and others like them. The basic structure of the prerequisite pairs set is shown in Table <ref type="table" target="#tab_3">3</ref>. Given the over 9,000 course vocabulary of our models, this inference is significantly more challenging than the 69 4 The steps are the same for the grade threshold of B 5 For Model 1 and Model 3, the steps are the same but only differ in the non-grade input 6 The reason we selected top 10 candidate prerequisite courses is that the largest number of prerequisite courses a target course has in our validation set is 8. So 10 is a proper number to ensure the list may be able to cover all the eight prerequisites.</p><p>exercise vocabulary used to construct the prerequisite graph inference in Deep Knowledge Tracing applied to tutoring systems <ref type="bibr" target="#b24">[25]</ref>. To aid this inference, we applied common sense constraints leveraging existing domain knowledge. We extracted the department information of the prerequisite courses for other courses in the same department as the target course as a filter before step <ref type="bibr" target="#b1">(2)</ref>. For example, assuming that Table <ref type="table" target="#tab_3">3</ref> shows all of the prerequisite pairs in the university, when we evaluate on Computer Science 189 as the target course, we only consider candidate prerequisite courses from its own department (i.e., Computer Science) and departments which host the prerequisite courses for the other computer science courses (e.g., Computer Science 266). In this case, the candidate departments for prediction are computer science and statistics given that Statistics 5 is the prerequisite course for Computer Science 266.</p><p>The second filter we added before step (2) is to filter out higher level courses by course number. At UC Berkeley, three levels of courses are provided based on intended year of study, in which the first level courses include courses with numbers lower than 100 (intended for first and second year students), the second level courses include courses with numbers between 100 and 199 (intended for 3rd and 4th year students), and the third level courses include courses with numbers 200 and above (graduate level courses). This filter represents the common sense constraint that a higher difficulty level course should not act as the preparation course for a lower difficulty level course. Since the registrar's list is treated as suggested prerequisites, not enforced by the enrollment system, it is not a highly maintained list, and other alternative prerequisite courses can be expected to exist. The proposed goal-based recommendation method would be ideally able to suggest preparatory courses for any target courses, including preparation courses that may not be in the maintained university list of prerequisites, but may nevertheless be valid. Therefore, this evaluation may represent the lower-bound of prerequisite course retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Results</head><p>Given that a target (post-requisite) course may have several prerequisite courses, We evaluated on the prerequisite course pairs set by calculating:</p><p>(1) the overall accuracy of predicting the correct prerequisite of each of the prerequisites pairs:</p><p># correctly predicted prerequisites pairs # total prerequisites pairs</p><p>(2) the overall accuracy of predicting at least one prerequisite for each of the post-requisite courses listed in the validation set: # target courses with at least one prerequisite course correctly predicted # total target courses</p><p>The results are shown in Table <ref type="table" target="#tab_5">4</ref> and suggest similarity in performance among all models. They show that close to one third of preexisting prerequisites are recovered by all models. For around 44% of post-requisite courses (courses with at least one prerequisite course), at least one prerequisite course was successfully identified among the top 10 inferred candidates. This result suggests that if these instructor-defined prerequisites are the only courses that could cover the required material, then our prerequisite courses prediction will fail to surface useful recommendations for a little over half of possible candidate courses. Because this list of prerequisites is not expected to be comprehensive, we proceed to a third validation to observe which courses students were choosing to take the semester before a difficult course, assuming that some of their selections may have been made in preparation for the difficult course.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">STUDENT GOAL-BASED COURSE SELECTION PREDICTION</head><p>In the grade prediction validation presented in section 5, the A threshold models performed considerably above baseline, while the B threshold models only slightly outperformed baseline. In spite of this, the course embeddings still encoded essential prerequisite relationships as are justified in section 6, key to the goal-based recommendation in this section. The -goal-in this work refers to a student's desired grade on a target course. In addition, personalization is brought to bear in this section, using a student's personal course history to produce the types of recommendations the framework would make in a real-world setting. This personalization allows for a different set of courses to be tailored to students based on their various enrollment histories, discrepancies in the grasp of course knowledge, and different majors. For example, different prerequisite courses may be tailored to a Global Studies major than a Mechanical Engineering major in preparation for a course on machine learning.</p><p>To achieve this, we propose the goal-based recommendation in this section to generate personalized suggestions to students by means of the same learned grade prediction models described in section 5 which implicitly model students' developing hidden performance aptitude, or "knowledge" states, across semesters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Personalized Goal-based Prerequisite Course Recommendation Framework</head><p>The goal-based recommendation task can be defined as a response to a student's query, "To perform well on a target course of interest, which course shall I take the semester before, given my course enrollment and grade history?" The framework for generating the recommended courses for the model is shown in Figure <ref type="figure" target="#fig_5">6</ref>, which also leverages the same grade prediction LSTM models we have been testing in the two previous evaluations. The framework considers a student's enrollment histories, enrolled courses' grades and a grade goal (A or B) for a target course to generate recommendations for Assuming that a student has course enrollment histories for t − 1 semesters, and hopes to perform well (set A as the threshold) in a target course in semester t + 1. The steps for generating recommended courses for the target course in semester t are:</p><p>(1) Input the student's enrollment histories and grade histories to the model and then retrieve the value for h t −1 and д t −1 .</p><p>Note that c t should be set to 0 for the t − 1-th input of the model because it will be unknown in the real recommendation scenario. (2) Iterate д t over all the courses with a one-hot representation of the grade A position for that course and give д t , h t −1 and c t +1 = tarдet course to the hidden layer. .</p><p>(4) Rank P A t ar дet and select the top 10 input courses with regard to the value of its output of P A t ar дet . In order to leverage domain knowledge to limit the number of courses for enumeration without reducing the rigorousness of evaluation, we applied several filters to the enumerated input courses before step <ref type="bibr" target="#b1">(2)</ref>, which are listed as follows, with the first two filters being similar to those we employed to prerequisite course prediction.</p><p>(1) We only consider courses in target course's own department and the departments which host the prerequisite courses for the courses in the same department as the target course. (2) Filter out the higher level courses by the course number.</p><p>(3) Filter out courses which are unavailable in semester t. (4) Filter out courses the student has already taken. <ref type="bibr" target="#b4">(5)</ref> Filter out the target course. <ref type="bibr" target="#b5">(6)</ref> Only consider the courses with predicted grades higher than the threshold (A in Figure <ref type="figure" target="#fig_5">6</ref>), because the recommended course should also be within the student's zone of proximal development.</p><p>We selected 10 historically difficult courses from different departments across STEM and non-STEM disciplines as the target and set Fall 2016 as the target semester. Because sgnificantly fewer students take courses in summer semesters, we considered Spring 2016 as the semester for recommendation. The test set for this goal-based recommendation validation consists of the students with enrollment histories in Fall 2016 and at least two semester before Fall 2016 but without enrollment histories in Summer 2016.</p><p>We considered all students who took the target courses in Fall 2016. Our model was considered to have made a successful set of recommendations if at least one of the 10 would-be suggested courses, recommended for 2016 Spring, matched a student's actual enrollment in that semester. The overall accuracy was calculated by the number of students we have correct predictions for over the number of total students. In addition, we assume that students who performed well on the target course and students who did not may show differences in the courses they chose to take in the previous semester. Students who performed well on the target course may show more preparation in their enrollment histories that lay solid foundation for the target course. We hypothesize that the recommendation accuracy on high performance students may therefore be higher than that on poorer performance students and that this difference is support, but not proof, for the reasonableness of preparation course recommendations being made. Hence, we report the accuracy on both high performance students and poorer performance students with respect to the target by setting the threshold grade (A or B) to distinguish them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Results</head><p>The specific results for each course we selected for models with grade threshold set as A and B are shown in Figure <ref type="figure">7</ref> and 8, respectively. Summary results (Table <ref type="table" target="#tab_6">5</ref>) show that students who performed under-threshold on the target course were also far less likely to have taken a course that would have been suggested by the algorithm in the previous semester. For both the A and B threshold scenarios, recommendations matched at least one preparatory course taken by 57% of students scoring a B or above and 38% of students scoring an A on the target course. For both threshold models, recommendation matches on students scoring below threshold on the target course were 21 raw percentage points lower in accuracy.</p><p>All three evaluation results are summarized in Table <ref type="table" target="#tab_6">5</ref>, combining the course selection evaluation with results from grade prediction (only letter grade) and the prerequisite inference evaluation (using average accuracy). The best performing model for each evaluation is bold. Note that it is the same trained model on each row that is being validated from difference perspectives. The grade prediction validation is closest to the objective function used to train the model. Its direct affect on recommendation is that it is used to apply the filter of not showing students preparatory courses they are not ready for (not predicted to perform above threshold on). In spite of raw accuracy differences and differences between model and baseline accuracies among the different threshold models, all models performed well in recovering existing prerequisites, maintained by the university.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Discussion</head><p>Model 2 with a B threshold scored considerably higher than the same model with an A threshold on the preparation semester course prediction evaluation (57.65% vs. 36.09%). One explanation is that students are trying to match course difficulty with their ability (i.e., ZPD) when making their selections <ref type="bibr" target="#b21">[22]</ref> and that, since the B models ). If the grade discriminating power of the B model is not its source of strength in the goal-based evaluation, an alternative explanation is that an A threshold for a preparatory class is too high. The similar scores between A and B models on the pre-req inference evaluation suggests that receiving an A does not carry substantially more information about pre-requisite relationships than does receiving a B. Therefore, receiving a B may be, on average, sufficient preparation for post-requisite courses and therefore a better threshold. It is worth noting that grade prediction outputs are meant to be an estimate of expected achievement in courses given the current knowledge state of a student and a normative amount of effort. If a student decides to dedicate above average time to a class, they can overachieve the model's estimate and gain more from the class than anticipated. Therefore, the predictions should not be treated as boundaries but rather distances from a student's ZPD, traversable with appropriate effort and support.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONTRIBUTIONS</head><p>We introduced a novel approach to personalized course prerequisite inference for goal-based recommendation based on adaptations of a recurrent neural network. We validated several model variants against test sets representing the tasks of grade prediction, prerequisite inference, and preparation semester course selection. The model allows students to specify an arbitrary course offered at the university along with the level of achievement they wish to attain (a grade of A or B). The algorithm then tailors 10 candidate preparation courses to consider based on their personal course enrollment histories and grades, their specified target course and achievement level. As this is a causal inference problem and we had only observational data to train the model, we used these three sources of validation of a model trained on grade prediction to help gauge the plausibility of the model performing adequately in the real-world. B target threshold models scored slightly above baseline in the grade prediction task, achieving a high of 88% accuracy on the binary classification task while the A model scored lower at 75% but substantially beat out the lower performing majority class baseline of 50%. The grade prediction performance is important in order to accurately filter out preparatory courses from recommendation that a student may not be ready for and may, themselves, require additional preparations for. Since the preparation recommendations are in essence a personalized inference of prerequisite information, we tested the models' ability to encode this information by validating against a preexisting prerequisite graph kept by the university. On this validation set, at least one prerequisite course was recovered for nearly half of the post-requisite courses in the graph and around one third of the total prerequisite pairs were recovered. The prerequisite graph was not expected to be comprehensive and students may be finding ways to prepare for courses in ways other than consulting the courses in this graph.</p><p>We therefore used student course selections in the semester before a historically difficult course as a third source of validation. The hypothesis being that students who achieved above threshold in the target difficult course would be more likely to have taken a wouldbe recommendation of our algorithm than students who performed below threshold. Our full personalized goal-based recommendation pipeline was enacted to conduct this evaluation, utilizing students previous course histories before the recommendation semester. Our results showed that students who performed above threshold on the difficult course were nearly twice as likely to have taken at least one of the ten would-be recommended preparation courses of our algorithm compared to students who performed below the specified performance threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">LIMITATIONS AND FUTURE WORK</head><p>Inherent limitations of observational data exist in all applications that wish to infer causal relationships. While real-world evaluation with experimental controls is the gold standard; this would be a very expensive undertaking to realize, given that two semesters would need to elapse in order to make the recommendations direct to students, or as an advising tool, and then observe students' performance on the target course. Furthermore, given the high stakes of such a recommendation, we would want to be highly confident in the algorithm's performance in order to ethnically justify such a real-world evaluation. Therefore, back-testing validations, such as those conducted in this paper, are a necessary first step to reaching the goal of real-world impact. Additional sources of validation that could be sought before deployment would be instructor ratings of suggested prerequisite courses for their own course given to students with different exemplar course histories, academic adviser ratings of such recommendations, and ratings from students themselves. Although among these three stakeholders, students may be least capable of judging if the content of a course they haven't yet taken would serve as appropriate preparation for a course currently outside their proximal zone of development. Methodologically, our recommendation was set up to recommend course preparation for a single semester. The best preparation course may be one that the student is not ready to take, and would be filtered out from recommendation. A sequence of curricular preparation may be more desirable, depending on how distant of courses from their current level they wish to engage with and how far in advance they plan.</p><p>There are many other goals students wish to achieve, from the micro to the macro. Future work includes evaluating if these goals, such as preparation for an intended career path, fit well into the modeling paradigm described and what additional augmentations, if any, are needed to aid students in navigating those decision spaces.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Model 1 -Simple course grade prediction model</figDesc><graphic url="image-1.png" coords="3,53.80,83.69,251.95,86.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Model 2 -Course grade prediction model with previous semester course grades and current semester coenrollment as input to the hidden layer</figDesc><graphic url="image-2.png" coords="3,317.96,83.68,251.93,93.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Model 3 -Course grade prediction model with previous semester course grades, previous semester declared major(s), and current semester co-enrollment as input directly to the output layer</figDesc><graphic url="image-3.png" coords="4,53.80,83.67,251.97,158.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Illustration for the two-level masked loss architecture</figDesc><graphic url="image-4.png" coords="4,317.96,83.69,251.93,71.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Prerequisite course prediction model illustration</figDesc><graphic url="image-5.png" coords="6,65.94,83.69,215.99,143.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Goal-based recommendation model illustration</figDesc><graphic url="image-6.png" coords="7,317.96,83.69,251.94,100.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>( 3 )</head><label>3</label><figDesc>Calculate the predicted probability of getting an A for the target course for each input by P A t ar дet =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Figure 7: Goal-based recommendation evaluation (Grade threshold: A)</figDesc><graphic url="image-8.png" coords="9,53.80,334.55,518.41,216.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Example student enrollments from our dataset The dataset consisted of per-semester course enrollment information for 164,196 students (both undergraduates and graduates) with a total of 4.8 million enrollments. A course enrollment meant that the student was still enrolled in the course at the conclusion of the semester. The median course load during students' active semesters was four. There were 10,430 unique courses, including 9,714 unique primary lecture courses from 197 subjects in 124 different departments hosted in 17 different divisions of 6 colleges. In all analyses in this paper, we only considered primary courses (lecture) and courses with at least 20 enrollments total over the 10 year period. The raw data were provided in CSV format by the university's Enterprise Data Warehouse. Each row of the course enrollment data contained semester and grade information,</figDesc><table><row><cell>Semester</cell><cell>STU ID</cell><cell cols="3">Major Dept Course</cell><cell>Grade</cell></row><row><cell>Year</cell><cell>(anon)</cell><cell></cell><cell></cell><cell>Num</cell><cell></cell></row><row><cell>Spring 2014</cell><cell>x137905</cell><cell>Law</cell><cell>Law</cell><cell>178</cell><cell>B</cell></row><row><cell cols="2">Summer 2014 x137905</cell><cell>Law</cell><cell>Law</cell><cell>165</cell><cell>C</cell></row><row><cell>Fall 2014</cell><cell>x282243</cell><cell cols="3">Math Math 140</cell><cell>D</cell></row><row><cell>Fall 2014</cell><cell>x282243</cell><cell cols="3">Math Math 121</cell><cell>A</cell></row><row><cell cols="2">through Spring 2017.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>an anonymous student ID, entry type (transfer student or new freshman), and declared major(s) at each semester. Course information included course name, course number, department, instructor, subject, enrollment count, and capacity. The basic structure of the enrollment data is shown in Table1.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Evaluation of student course grade prediction</figDesc><table><row><cell cols="2">model settings</cell><cell cols="2">letter grade</cell><cell>pass/no-pass</cell></row><row><cell>threshold</cell><cell>model</cell><cell cols="2">accuracy F-score</cell><cell>accuracy</cell></row><row><cell>B</cell><cell>Baseline-B</cell><cell>85.46</cell><cell>None</cell><cell>90.97</cell></row><row><cell>B</cell><cell>Model 1</cell><cell>87.75</cell><cell>39.05</cell><cell>90.71</cell></row><row><cell>B</cell><cell>Model 2</cell><cell>88.05</cell><cell>42.01</cell><cell>91.78</cell></row><row><cell>B</cell><cell>Model 3</cell><cell>87.78</cell><cell>40.21</cell><cell>91.76</cell></row><row><cell>A</cell><cell>Baseline-A</cell><cell>50.31</cell><cell>None</cell><cell>83.69</cell></row><row><cell>A</cell><cell>Model 1</cell><cell>74.61</cell><cell>55.05</cell><cell>85.42</cell></row><row><cell>A</cell><cell>Model 2</cell><cell>75.23</cell><cell>60.24</cell><cell>85.81</cell></row><row><cell>A</cell><cell>Model 3</cell><cell>75.19</cell><cell>58.86</cell><cell>86.05</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Sample of data from prerequisite pairs set prerequisite course target course</figDesc><table><row><cell cols="2">Computer Science 188 Computer Science 189</cell></row><row><cell>Mathematics 53</cell><cell>Computer Science 189</cell></row><row><cell>Statistics 5</cell><cell>Computer Science 266</cell></row><row><cell>Chemistry 3A</cell><cell>Chemistry 3B</cell></row><row><cell>Economics 1</cell><cell>Economics 100B</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>) Calculate the predict probability of getting an above or equal to A grade for the target course for each input by P A t ar дet =</figDesc><table><row><cell>(e</cell><cell>s A t ar дe t t ar дe t +e e s A s N A t ar дe t )</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Evaluation of prerequisite courses prediction</figDesc><table><row><cell></cell><cell>model</cell><cell>accuracy</cell></row><row><cell cols="3">threshold model pairs target courses</cell></row><row><cell>B</cell><cell>Model 2 30.48</cell><cell>44.86</cell></row><row><cell>B</cell><cell>Model 3 29.61</cell><cell>43.54</cell></row><row><cell>A</cell><cell>Model 2 29.72</cell><cell>43.77</cell></row><row><cell>A</cell><cell>Model 3 30.08</cell><cell>45.61</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Summary of all evaluations Calculated by averaging the third and the fourth accuracy columns of Table4.have higher grade prediction accuracy than the A models (88.05% vs. 75.23%), they are better at filtering out courses that are not a good match. The higher letter grade accuracy of the B model, however, is not very meaningful and not directly comparable to the A model since the B model benefits greatly from having a higher majority class proportion than the A model. In terms of grade prediction F-score, the A model is superior (60.24 for Model 2 A vs 42.01 for Model 2 B</figDesc><table><row><cell></cell><cell>model</cell><cell cols="2">goal-based</cell><cell cols="2">grade pred. pre-req</cell></row><row><cell cols="2">thres. model</cell><cell cols="2">pos pos-neg</cell><cell>F-score</cell><cell>average 1</cell></row><row><cell>B</cell><cell cols="2">Model 2 57.65</cell><cell>21.21</cell><cell>42.01</cell><cell>37.67</cell></row><row><cell>B</cell><cell cols="2">Model 3 56.34</cell><cell>19.33</cell><cell>40.21</cell><cell>36.58</cell></row><row><cell>A</cell><cell cols="2">Model 2 36.09</cell><cell>22.93</cell><cell>60.24</cell><cell>36.74</cell></row><row><cell>A</cell><cell cols="2">Model 3 41.36</cell><cell>22.61</cell><cell>58.86</cell><cell>37.84</cell></row></table><note>1 </note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">We made this choice after observing worse performance on a development set of a model which fed the courses co-enrolled information to the hidden layer</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">We published our code for the paper regarding the model, and three validations, at https://github.com/CAHLR/goal-based-recommendation</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank UC Berkeley staff Andrew Eppig, Tom O'Brien, Rebecca Sablo, and Walter Wong for their assistance with the anonymized data and for sharing their wisdom on the complexities of the enrollment context. This material is based in part upon work supported by the National Science Foundation under Grant Number 1446641.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Impact of Student Choice of Content Adoption Delay on Course Outcomes</title>
		<author>
			<persName><forename type="first">Lalitha</forename><surname>Agnihotri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alfred</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Learning Analytics and Knowledge Conference (LAK &apos;17)</title>
				<meeting>the Seventh International Learning Analytics and Knowledge Conference (LAK &apos;17)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improving Stealth Assessment in Game-based Learning with LSTM-based Analytics</title>
		<author>
			<persName><forename type="first">Bita</forename><surname>Akram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bradford</forename><surname>Mott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wookhee</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristy</forename><forename type="middle">Elizabeth</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Lester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Educational Data Mining Conference</title>
				<meeting>the 11th International Educational Data Mining Conference</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="208" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Studying MOOC Completion at Scale Using the MOOC Replication Framework</title>
		<author>
			<persName><forename type="first">Juan</forename><surname>Miguel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragan</forename><surname>Gašević</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Siemens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">A</forename><surname>Crossley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srećko</forename><surname>Joksimović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Learning Analytics and Knowledge (LAK &apos;18)</title>
				<meeting>the 8th International Conference on Learning Analytics and Knowledge (LAK &apos;18)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Data-Driven Approach Towards a Personalized Curriculum</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Backenköhler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Scherzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adish</forename><surname>Singla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Verena</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th EDM Conference</title>
				<meeting>the 11th EDM Conference</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-02">2003. Feb (2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Conceptualizing Co-enrollment: Accounting for Student Experiences Across the Curriculum</title>
		<author>
			<persName><forename type="first">R</forename><surname>Michael Geoffrey Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><forename type="middle">D</forename><surname>Matthew Demonbrun</surname></persName>
		</author>
		<author>
			<persName><surname>Teasley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Learning Analytics and Knowledge (LAK &apos;18)</title>
				<meeting>the 8th International Conference on Learning Analytics and Knowledge (LAK &apos;18)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The zone of proximal development in Vygotsky&apos;s analysis of learning and instruction. Vygotsky&apos;s educational theory in cultural context</title>
		<author>
			<persName><forename type="first">Seth</forename><surname>Chaiklin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="39" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">How a data-driven course planning tool affects college students&apos; GPA: evidence from two field experiments</title>
		<author>
			<persName><forename type="first">Sorathan</forename><surname>Chaturapruek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Dee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Johari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">René</forename><surname>Kizilcec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Stevens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning @ Scale</title>
				<meeting>the 5th International Conference on Learning @ Scale</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Behavioral Analysis at Scale: Learning Course Prerequisite Structures from Learner Clickstreams</title>
		<author>
			<persName><forename type="first">Weiyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">S</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Brinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mung</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th EDM Conference</title>
				<meeting>the 11th EDM Conference</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A Hybrid Multi-Criteria approach using a Genetic Algorithm for Recommending Courses to University Students</title>
		<author>
			<persName><forename type="first">Aurora</forename><surname>Esteban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amelia</forename><surname>Zafra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristóbal</forename><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th EDM Conference</title>
				<meeting>the 11th EDM Conference</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Coenrollment Networks and Their Relationship to Grades in Undergraduate Education</title>
		<author>
			<persName><forename type="first">Josh</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Learning Analytics and Knowledge (LAK &apos;18)</title>
				<meeting>the 8th International Conference on Learning Analytics and Knowledge (LAK &apos;18)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Evaluating Predictive Models of Student Success: Closing the Methodological Gap</title>
		<author>
			<persName><forename type="first">Josh</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of learning Analytics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="105" to="125" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Student success prediction in MOOCs</title>
		<author>
			<persName><forename type="first">Josh</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">User Modeling and User-Adapted Interaction</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="127" to="203" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning to forget: Continual prediction with LSTM</title>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Felix A Gers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName><surname>Cummins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Session-based recommendations with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Balázs</forename><surname>Hidasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linas</forename><surname>Baltrunas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Domonkos</forename><surname>Tikk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Parallel recurrent neural network architectures for feature-rich session-based recommendations</title>
		<author>
			<persName><forename type="first">Balázs</forename><surname>Hidasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimo</forename><surname>Quadrana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Conference on Recommender Systems</title>
				<meeting>the 10th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="241" to="248" />
		</imprint>
	</monogr>
	<note>Alexandros Karatzoglou, and Domonkos Tikk</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ouroboros: Early Identification of At-risk Students Without Models Based on Legacy Data</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Hlosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zdenek</forename><surname>Zdrahal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaroslav</forename><surname>Zendulka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Learning Analytics and Knowledge Conference</title>
				<meeting>the Seventh International Learning Analytics and Knowledge Conference</meeting>
		<imprint/>
	</monogr>
	<note>n. d.</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Early alert of academically at-risk students: An open source analytics initiative</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sandeep</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">W</forename><surname>Jayaprakash</surname></persName>
		</author>
		<author>
			<persName><surname>Moody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Eitel</surname></persName>
		</author>
		<author>
			<persName><surname>Lauría</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">D</forename><surname>James R Regan</surname></persName>
		</author>
		<author>
			<persName><surname>Baron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Learning Analytics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6" to="47" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Communication at Scale in a MOOC Using Predictive Engagement Analytics</title>
		<author>
			<persName><forename type="first">Zachary</forename><forename type="middle">A</forename><surname>Christopher V Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">D</forename><surname>Pardos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><surname>Thorp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence in Education</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="239" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Diagnosing University Student Subject Proficiency and Predicting Degree Completion in Vector Space</title>
		<author>
			<persName><forename type="first">Yuetian</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><forename type="middle">A</forename><surname>Pardos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI)</title>
				<meeting>the Eighth AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI)</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7920" to="7927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Using institutional data to predict student course selections in higher education</title>
		<author>
			<persName><forename type="first">Ivana</forename><surname>Ognjanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragan</forename><surname>Gasevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Dawson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Internet and Higher Education</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="49" to="62" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Connectionist Recommendation in the Wild: On the utility and scrutability of neural networks for personalized course guidance. User Modeling and User-Adapted Interaction</title>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Zachary A Pardos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><surname>Jiang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1803.09535" />
		<imprint/>
	</monogr>
	<note>In press. In press</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Enabling real-time adaptivity in MOOCs with a personalized next-step recommendation framework</title>
		<author>
			<persName><forename type="first">Zachary</forename><forename type="middle">A</forename><surname>Pardos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth</title>
				<meeting>the Fourth</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="23" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Chris</forename><surname>Piech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Bassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehran</forename><surname>Sahami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<title level="m">Deep knowledge tracing. neural information processing systems</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="505" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Feature extraction for classifying students based on their academic performance</title>
		<author>
			<persName><forename type="first">Agoritsa</forename><surname>Polyzou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th EDM Conference</title>
				<meeting>the 11th EDM Conference</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Early Identification of At-Risk Students Using Iterative Logistic Regression</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huzefa</forename><surname>Rangwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence in Education</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="613" to="626" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
