<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PAIRNORM: TACKLING OVERSMOOTHING IN GNNS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
							<email>lingxia1@andrew.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
							<email>lakoglu@andrew.cmu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PAIRNORM: TACKLING OVERSMOOTHING IN GNNS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The performance of graph neural nets (GNNs) is known to gradually decrease with increasing number of layers. This decay is partly attributed to oversmoothing, where repeated graph convolutions eventually make node embeddings indistinguishable. We take a closer look at two different interpretations, aiming to quantify oversmoothing. Our main contribution is PAIRNORM, a novel normalization layer that is based on a careful analysis of the graph convolution operator, which prevents all node embeddings from becoming too similar. What is more, PAIRNORM is fast, easy to implement without any change to network architecture nor any additional parameters, and is broadly applicable to any GNN. Experiments on real-world graphs demonstrate that PAIRNORM makes deeper GCN, GAT, and SGC models more robust against oversmoothing, and significantly boosts performance for a new problem setting that benefits from deeper GNNs. Code is available at https://github.com/LingxiaoShawn/PairNorm.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph neural networks (GNNs) is a family of neural networks that can learn from graph structured data. Starting with the success of GCN <ref type="bibr" target="#b4">(Kipf &amp; Welling, 2017)</ref> on achieving state-of-the-art performance on semi-supervised classification, several variants of GNNs have been developed for this task; including <ref type="bibr">GraphSAGE (Hamilton et al., 2017)</ref>, GAT <ref type="bibr" target="#b14">(Velickovic et al., 2018)</ref>, SGC <ref type="bibr" target="#b15">(Wu et al., 2019)</ref>, and GMNN <ref type="bibr" target="#b9">(Qu et al., 2019)</ref> to name a few most recent ones.</p><p>A key issue with GNNs is their depth limitations. It has been observed that deeply stacking the layers often results in significant drops in performance for GNNs, such as GCN and GAT, even beyond just a few (2-4) layers. This drop is associated with a number of factors; including the vanishing gradients in back-propagation, overfitting due to the increasing number of parameters, as well as the phenomenon called oversmoothing. <ref type="bibr" target="#b7">Li et al. (2018)</ref> was the first to call attention to the oversmoothing problem. Having shown that the graph convolution is a type of Laplacian smoothing, they proved that after repeatedly applying Laplacian smoothing many times, the features of the nodes in the (connected) graph would converge to similar values-the issue coined as "oversmoothing". In effect, oversmoothing hurts classification performance by causing the node representations to be indistinguishable across different classes. Later, several others have alluded to the same problem <ref type="bibr" target="#b16">(Xu et al., 2018;</ref><ref type="bibr" target="#b5">Klicpera et al., 2019;</ref><ref type="bibr" target="#b10">Rong et al., 2019;</ref><ref type="bibr" target="#b6">Li et al., 2019)</ref>  <ref type="bibr">(See §5 Related Work)</ref>.</p><p>In this work, we address the oversmoothing problem in deep GNNs. Specifically, we propose (to the best of our knowledge) the first normalization layer for GNNs that is applied in-between intermediate layers during training. Our normalization has the effect of preventing the output features of distant nodes to be too similar or indistinguishable, while at the same time allowing those of connected nodes in the same cluster become more similar. We summarize our main contributions as follows.</p><p>• Normalization to Tackle Oversmoothing in GNNs: We introduce a normalization scheme, called PAIRNORM, that makes GNNs significantly more robust to oversmoothing and as a result enables the training of deeper models without sacrificing performance. Our proposed scheme capitalizes on the understanding that most GNNs perform a special form of Laplacian smoothing, which makes node features more similar to one another. The key idea is to ensure that the total pairwise feature distances remains a constant across layers, which in turn leads to distant pairs having less similar features, preventing feature mixing across clusters.</p><p>• Speed and Generality: PAIRNORM is very straightforward to implement and introduces no additional parameters. It is simply applied to the output features of each layer (except the last one) consisting of simple operations, in particular centering and scaling, that are linear in the input size. Being a simple normalization step between layers, PAIRNORM is not specific to any particular GNN but rather applies broadly.</p><p>• Use Case for Deeper GNNs: While PAIRNORM prevents performance from dropping significantly with increasing number of layers, it does not necessarily yield increased performance in absolute terms. We find that this is because shallow architectures with no more than 2-4 layers is sufficient for the often-used benchmark datasets in the literature. In response, we motivate a real-world scenario wherein a notable portion of the nodes have no feature vectors. In such settings, nodes benefit from a larger range (i.e., neighborhood, hence a deeper GNN) to "recover" effective feature representations. Through extensive experiments, we show that GNNs employing our PAIRNORM significantly outperform the 'vanilla' GNNs when deeper models are beneficial to the classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">UNDERSTANDING OVERSMOOTHING</head><p>In this work, we consider the semi-supervised node classification (SSNC) problem on a graph. In the general setting, a graph G = (V, E, X) is given in which each node i ∈ V is associated with a feature vector x i ∈ R d where X = [x 1 , . . . , x n ] T denotes the feature matrix, and a subset V l ⊂ V of the nodes are labeled, i.e. y i ∈ {1, . . . , c} for each i ∈ V l where c is the number of classes. Let A ∈ R n×n be the adjacency matrix and D = diag(deg 1 , . . . , deg n ) ∈ R n×n be the degree matrix of G. Let Ã = A + I and D = D + I denote the augmented adjacency and degree matrices with added self-loops on all nodes, respectively. Let Ãsym = D−1/2 Ã D−1/2 and Ãrw = D−1 Ã denote symmetrically and nonsymmetrically normalized adjacency matrices with self-loops.</p><p>The task is to learn a hypothesis that predicts y i from x i that generalizes to the unlabeled nodes V u = V\V l . In Section 3.2, we introduce a variant of this setting where only a subset F ⊂ V of the nodes have feature vectors and the rest are missing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">THE OVERSMOOTHING PROBLEM</head><p>Although GNNs like GCN and GAT achieve state-of-the-art results in a variety of graph-based tasks, these models are not very well-understood, especially why they work for the SSNC problem where only a small amount of training data is available. The success appears to be limited to shallow GNNs, where the performance gradually decreases with the increasing number of layers. This decrease is often attributed to three contributing factors: (1) overfitting due to increasing number of parameters, (2) difficulty of training due to vanishing gradients, and (3) oversmoothing due to many graph convolutions. Among these, perhaps the least understood one is oversmoothing, which indeed lacks a formal definition. In their analysis of GCN's working mechanism, <ref type="bibr" target="#b7">Li et al. (2018)</ref> showed that the graph convolution of GCN is a special form of Laplacian smoothing. The standard form being (I−γI)X+ γ Ãrw X, the graph convolution lets γ = 1 and uses the symmetrically normalized Laplacian to obtain X = Ãsym X, where the new features x of a node is the weighted average of its own and its neighbors' features. This smoothing allows the node representations within the same cluster become more similar, and in turn helps improve SSNC performance under the cluster assumption <ref type="bibr" target="#b1">(Chapelle et al., 2006)</ref>. However when GCN goes deep, the performance can suffer from oversmoothing where node representations from different clusters become mixed up. Let us refer to this issue of node representations becoming too similar as node-wise oversmoothing.</p><p>Another way of thinking about oversmoothing is as follows. Repeatedly applying Laplacian smoothing too many times would drive node features to a stationary point, washing away all the information from these features. Let x •j ∈ R n denote the j-th column of X. Then, for any x •j ∈ R n :</p><formula xml:id="formula_0">lim k→∞ Ãk sym x •j = π j and π j π j 1 = π ,<label>(1)</label></formula><p>where the normalized solution π ∈ R n satisfies π i =</p><formula xml:id="formula_1">√ degi i √ degi for all i ∈ [n].</formula><p>Notice that π is independent of the values x •j of the input feature and is only a function of the graph structure (i.e., degree). In other words, (Laplacian) oversmoothing washes away the signal from all the features, making them indistinguishable. We will refer to this viewpoint as feature-wise oversmoothing.</p><p>To this end we propose two measures, row-diff and col-diff, to quantify these two types of oversmoothing. Let H (k) ∈ R n×d be the representation matrix after k graph convolutions, i.e. k) . Then we define row-diff(H (k) ) and col-diff(H (k) ) as follows.</p><formula xml:id="formula_2">H (k) = Ãk sym X. Let h (k) i ∈ R d be the i-th row of H (k) and h (k) •i ∈ R n be the i-th column of H (</formula><formula xml:id="formula_3">row-diff(H (k) ) = 1 n 2 i,j∈[n] h (k) i − h (k) j 2 (2) col-diff(H (k) ) = 1 d 2 i,j∈[d] h (k) •i / h (k) •i 1 − h (k) •j / h (k) •j 1 2</formula><p>(3)</p><p>The row-diff measure is the average of all pairwise distances between the node features (i.e., rows of the representation matrix) and quantifies node-wise oversmoothing, whereas col-diff is the average of pairwise distances between (L 1 -normalized<ref type="foot" target="#foot_0">1</ref> ) columns of the representation matrix and quantifies feature-wise oversmoothing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">STUDYING OVERSMOOTHING WITH SGC</head><p>Although oversmoothing can be a cause of performance drop with increasing number of layers in GCN, adding more layers also leads to more parameters (due to learned linear projections W (k) at each layer k) which magnify the potential of overfitting. Furthermore, deeper models also make the training harder as backpropagation suffers from vanishing gradients.</p><p>In order to decouple the effect of oversmoothing from these other two factors, we study the oversmoothing problem using the SGC model <ref type="bibr" target="#b15">(Wu et al., 2019)</ref>. (Results on other GNNs are presented in §4.) SGC is simplified from GCN by removing all projection parameters of graph convolution layers and all nonlinear activations between layers. The estimation of SGC is simply written as:</p><formula xml:id="formula_4">Y = softmax( ÃK sym X W)<label>(4</label></formula><p>) where K is the number of graph convolutions, and W ∈ R d×c denote the learnable parameters of a logistic regression classifier.</p><p>Note that SGC has a fixed number of parameters that does not depend on the number of graph convolutions (i.e. layers). In effect, it is guarded against the influence of overfitting and vanishing gradient problem with more layers. This leaves us only with oversmoothing as a possible cause of performance degradation with increasing K. Interestingly, the simplicity of SGC does not seem to be a sacrifice; it has been observed that it achieves similar or better accuracy in various relational classification tasks <ref type="bibr" target="#b15">(Wu et al., 2019)</ref>.  3 TACKLING OVERSMOOTHING</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">PROPOSED PAIRNORM</head><p>We start by establishing a connection between graph convolution and an optimization problem, that is graph-regularized least squares (GRLS), as shown by <ref type="bibr" target="#b8">NT &amp; Maehara (2019)</ref>. Let X ∈ R n×d be a new node representation matrix, with xi ∈ R d depicting the i-th row of X. Then the GRLS problem is given as min</p><formula xml:id="formula_5">X i∈V xi − x i 2 D + (i,j)∈E xi − xj 2 2 (5)</formula><p>where</p><formula xml:id="formula_6">z i 2 D = z T i Dz i .</formula><p>The first term can be seen as total degree-weighted least squares. The second is a graph-regularization term that measures the variation of the new features over the graph structure. The goal of the optimization problem can be stated as estimating new "denoised" features xi 's that are not too far off of the input features x i 's and are smooth over the graph structure.</p><p>The GRLS problem has a closed form solution X = (2I − Ãrw ) −1 X, for which Ãrw X is the firstorder Taylor approximation, that is Ãrw X ≈ X. By exchanging Ãrw with Ãsym we obtain the same form as the graph convolution, i.e., X = Ãsym X ≈ X. As such, graph convolution can be viewed as an approximate solution of ( <ref type="formula">5</ref>), where it minimizes the variation over the graph structure while keeping the new representations close to the original.</p><p>The optimization problem in (5) facilitates a closer look to the oversmoothing problem of graph convolution. Ideally, we want to obtain smoothing over nodes within the same cluster, however avoid smoothing over nodes from different clusters. The objective in (5) dictates only the first goal via the graph-regularization term. It is thus prone to oversmoothing when convolutions are applied repeatedly. To circumvent the issue and fulfill both goals simultaneously, we can add a negative term such as the sum of distances between disconnected pairs as follows. min</p><formula xml:id="formula_7">X i∈V xi − x i 2 D + (i,j)∈E xi − xj 2 2 − λ (i,j) / ∈E xi − xj 2 2 (6)</formula><p>where λ is a balancing scalar to account for different volume and importance of the two goals.<ref type="foot" target="#foot_1">2</ref> By deriving the closed-form solution of ( <ref type="formula">6</ref>) and approximating it with first-order Taylor expansion, one can get a revised graph convolution operator with hyperparameter λ. In this paper, we take a different route. Instead of a completely new graph convolution operator, we propose a general and efficient "patch", called PAIRNORM, that can be applied to any form of graph convolution having the potential of oversmoothing.</p><p>Let X (the output of graph convolution) and Ẋ respectively be the input and output of PAIRNORM.</p><p>Observing that the output of graph convolution X = Ãsym X only achieves the first goal, PAIRNORM serves as a normalization layer that works on X to achieve the second goal of keeping disconnected pair representations farther off. Specifically, PAIRNORM normalizes X such that the total pairwise squared distance TPSD( Ẋ)</p><formula xml:id="formula_8">:= i,j∈[n] ẋi − ẋj 2 2 is the same as TPSD(X). That is, (i,j)∈E ẋi − ẋj 2 2 + (i,j) / ∈E ẋi − ẋj 2 2 = (i,j)∈E x i − x j 2 2 + (i,j) / ∈E x i − x j 2 2 . (7)</formula><p>By keeping the total pairwise squared distance unchanged, the term (i,j) / ∈E ẋi − ẋj 2 2 is guaranteed to be at least as large as the original value</p><formula xml:id="formula_9">(i,j) / ∈E x i − x j 2 2 since the other term (i,j)∈E ẋi − ẋj 2 2 ≈ (i,j)∈E xi − xj 2 2 is shrunk through the graph convolution.</formula><p>In practice, instead of always tracking the original value TPSD(X), we can maintain a constant TPSD value C across all layers, where C is a hyperparameter that could be tuned per dataset.</p><p>To normalize X to constant TPSD, we need to first compute TPSD( X). Directly computing TPSD involves n 2 pairwise distances that is O(n 2 d), which can be time consuming for large datasets.</p><p>Equivalently, normalization can be done via a two-step approach where TPSD is rewritten as</p><formula xml:id="formula_10">3 TPSD( X) = i,j∈[n] xi − xj 2 2 = 2n 2 1 n n i=1 xi 2 2 − 1 n n i=1 xi 2 2 . (<label>8</label></formula><formula xml:id="formula_11">)</formula><p>The first term (ignoring the scale 2n 2 ) in Eq. ( <ref type="formula" target="#formula_10">8</ref>) represents the mean squared length of node representations, and the second term depicts the squared length of the mean of node representations. To simplify the computation of ( <ref type="formula" target="#formula_10">8</ref>), we subtract the row-wise mean from each xi , i.e., (9) In summary, our proposed PAIRNORM (with input X and output Ẋ) can be written as a two-step, center-and-scale, normalization procedure:</p><formula xml:id="formula_12">xc i = xi − 1 n n i xi</formula><formula xml:id="formula_13">xc i = xi − 1 n n i=1 xi (Center) (10) ẋi = s • xc i 1 n n i=1 xc i 2 2 = s √ n • xc i Xc 2 F (Scale)<label>(11)</label></formula><p>After scaling the data remains centered, that is,</p><formula xml:id="formula_14">n i=1 ẋi 2 2 = 0.</formula><p>In Eq. ( <ref type="formula" target="#formula_13">11</ref>), s is a hyperparameter that determines C. Specifically,</p><formula xml:id="formula_15">TPSD( Ẋ) = 2n Ẋ 2 F = 2n i s • xc i 1 n i xc i 2 2 2 2 = 2n s 2 1 n i xc i 2 2 i xc i 2 2 = 2n 2 s 2</formula><p>(12) Then, Ẋ := PAIRNORM( X) has row-wise mean 0 (i.e., is centered) and constant total pairwise squared distance C = 2n 2 s 2 . An illustration of PAIRNORM is given in Figure <ref type="figure" target="#fig_2">2</ref>. The output of PAIRNORM is input to the next convolution layer.  We also derive a variant of PAIRNORM by replacing</p><formula xml:id="formula_16">n i=1 xc i 2 2 in Eq. (11) with n xc i 2 2 , such that the scaling step computes ẋi = s • xc i xc i 2 .</formula><p>We call it PAIRNORM-SI (for Scale Individually), which imposes more restriction on node representations, such that all have the same L 2 -norm s. In practice we found that both PAIRNORM and PAIRNORM-SI work well for SGC, whereas PAIRNORM-SI provides better and more stable results for GCN and GAT. The reason why GCN and GAT require stricter normalization may be because they have more parameters and are more prone to overfitting. In Appx. A.6 we provide additional measures to demonstrate why PAIRNORM and PAIRNORM-SI work. In all experiments, we employ PAIRNORM for SGC and PAIRNORM-SI for both GCN and GAT.</p><p>PAIRNORM is effective and efficient in solving the oversmoothing problem of GNNs. As a general normalization layer, it can be used for any GNN. Solid lines in Figure <ref type="figure">1</ref> present the performance of SGC on Cora with increasing number of layers, where we employ PAIRNORM after each graph convolution layer, as compared to 'vanilla' versions. Similarly, Figure <ref type="figure" target="#fig_3">3</ref> is for GCN and GAT (PAIRNORM is applied after the activation of each graph convolution). Note that the performance decay with PAIRNORM-at-work is much slower. (See Fig.s 5-6 in Appx. A.3 for other datasets.)</p><p>While PAIRNORM enables deeper models that are more robust to oversmoothing, it may seem odd that the overall test accuracy does not improve. In fact, the benchmark graph datasets often used in the literature require no more than 4 layers, after which performance decays (even if slowly). In the next section, we present a realistic use case setting for which deeper models are more likely to provide higher performance, where the benefit of PAIRNORM becomes apparent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">A CASE WHERE DEEPER GNNS ARE BENEFICIAL</head><p>In general, oversmoothing gets increasingly more severe as the number of layers goes up. A task would benefit from employing PAIRNORM more if it required a large number of layers to achieve its best performance. To this effect we study the "missing feature setting", where a subset of the nodes lack feature vectors. Let M ⊆ V u be the set where ∀m ∈ M, x m = ∅, i.e., all of their features are missing. We denote with p = |M|/|V u | the missing fraction. We call this variant of the task as semi-supervised node classification with missing vectors (SSNC-MV). Intuitively, one would require a larger number of propagation steps (hence, a deeper GNN) to be able to "recover" effective feature representations for these nodes. SSNC-MV is a general and realistic problem that finds several applications in the real world. For example, the credit lending problem of identifying low-vs. high-risk customers (nodes) can be modeled as SSNC-MV where a large fraction of nodes do not exhibit any meaningful features (e.g., due to low-volume activity). In fact, many graph-based classification tasks with the cold-start issue (entity with no history) can be cast into SSNC-MV. To our knowledge, this is the first work to study the SSNC-MV problem using GNN models.</p><p>Figure <ref type="figure">4</ref> presents the performance of SGC, GCN, and GAT models on Cora with increasing number of layers, where we remove feature vectors from all the unlabeled nodes, i.e. p = 1. The models with PAIRNORM achieve a higher test accuracy compared to those without, which they typically reach at a larger number of layers. (See Fig. <ref type="figure">7</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In section 3 we have shown the robustness of PAIRNORM-enhanced models against increasing number of layers in SSNC problem. In this section we design extensive experiments to evaluate the effectiveness of PAIRNORM under the SSNC-MV setting, over SGC, GCN and GAT models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">EXPERIMENT SETUP</head><p>Datasets. We use 4 well-known benchmark datasets in GNN domain: Cora, Citeseer, Pubmed <ref type="bibr" target="#b12">(Sen et al., 2008)</ref>, and CoauthorCS <ref type="bibr" target="#b13">(Shchur et al., 2018)</ref>. Their statistics are reported in Appx. A.2. For Cora, Citeseer and Pubmed, we use the same dataset splits as <ref type="bibr" target="#b4">Kipf &amp; Welling (2017)</ref>, where all nodes outside train and validation are used as test set. For CoauthorCS, we randomly split all nodes into train/val/test as 3%/10%/87%, and keep the same split for all experiments. Models. We use three different GNN models as our base model: SGC <ref type="bibr" target="#b15">(Wu et al., 2019)</ref>, GCN <ref type="bibr" target="#b4">(Kipf &amp; Welling, 2017)</ref>, and GAT <ref type="bibr" target="#b14">(Velickovic et al., 2018)</ref>. We compare our PAIRNORM with residual connection method <ref type="bibr" target="#b2">(He et al., 2016)</ref> over base models (except SGC since there is no "resid-ual connected" SGC), as we surprisingly find it can slow down oversmoothing and benefit SSNC-MV problem. Similar to us, residual connection is a general technique that can be applied to any model without changing its architecture. We focus on the comparison between the base models and PAIRNORM-enhanced models, rather than achieving the state of the art performance for SSNC and SSNC-MV. There exist a few other work addressing oversmoothing <ref type="bibr" target="#b5">(Klicpera et al., 2019;</ref><ref type="bibr" target="#b7">Li et al., 2018;</ref><ref type="bibr" target="#b10">Rong et al., 2019;</ref><ref type="bibr" target="#b16">Xu et al., 2018)</ref> however they design specialized architectures and not simple "patch" procedures like PAIRNORM that can be applied on top of any GNN. Hyperparameters. We choose the hyperparameter s of PAIRNORM from {0.1, 1, 10, 50, 100} over validation set for SGC, while keeping it fixed at s = 1 for both GCN and GAT due to resource limitations. We set the #hidden units of GCN and GAT (#attention heads is set to 1) to 32 and 64 respectively for all datasets. Dropout with rate 0.6 and L 2 regularization with penalty 5 • 10 −4 are applied to GCN and GAT. For SGC, we vary number of layers in {1, 2, . . . 10, 15, . . . , 60} and for GCN and GAT in {2, 4, . . . , 12, 15, 20, . . . , 30}. Configurations. For PAIRNORM-enhanced models, we apply PAIRNORM after each graph convolution layer (i.e., after activation if any) in the base model. For residual-connected models with t skip steps, we connect the output of l-th layer to (l + t)-th, that is, H l) where H (l) denotes the output of l-th graph convolution (after activation). For the SSNC-MV setting, we randomly erase p fraction of the feature vectors from nodes in validation and test sets (for which we input vector 0 ∈ R d ), whereas all training (labeled) nodes keep their original features (See 3.2). We run each experiment within 1000 epochs 5 times and report the average performance. We mainly use a single GTX-1080ti GPU, with some SGC experiments ran on an Intel i7-8700k CPU.</p><formula xml:id="formula_17">(l+t) new = H (l+t) + H (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">EXPERIMENT RESULTS</head><p>We first show the global performance gain of applying PAIRNORM to SGC for SSNC-MV under varying feature missing rates as shown in Table <ref type="table" target="#tab_1">1</ref>. PAIRNORM-enhanced SGC performs similar or better over 0% missing, while it significantly outperforms vanilla SGC for most other settings, especially for larger missing rates. #L denotes the best number of layers for the model that yields the largest average validation accuracy (over 5 runs), for which we report the average test accuracy (Acc). Notice the larger #L values for SGC-PN compared to vanilla SGC, which shows the power of PAIRNORM for enabling "deep" SGC models by effectively tackling oversmoothing.</p><p>Similar to <ref type="bibr" target="#b15">Wu et al. (2019)</ref> who showed that the simple SGC model achieves comparable or better performance as other GNNs for various tasks, we found PAIRNORM-enhanced SGC to follow the same trend when compared with PAIRNORM-enhanced GCN and GAT, for all SSNC-MV settings. Due to its simplicity and extreme efficiency, we believe PAIRNORM-enhanced SGC sets a strong baseline for the SSNC-MV problem. We next employ PAIRNORM-SI for GCN and GAT under the same setting, comparing it with the residual (skip) connections technique. Results are shown in Table <ref type="table" target="#tab_2">2</ref> and Table <ref type="table" target="#tab_3">3</ref> respectively for GCN and GAT. Due to space and resource limitations, we only show results for 0% and 100% missing rate scenarios. (We provide results for other missing rates (70, 80, 90%) over 1 run only in Appx. A.5.) We observe similar trend for GCN and GAT: (1) vanilla model suffers from performance drop under SSNC-MV with increasing missing rate; (2) both residual connections and PAIRNORM-SI enable deeper models and improve performance (note the larger #L and Acc); (3) GCN-PN and GAT-PN achieve performance that is comparable or better than just using skips; (4) performance can be further improved (albeit slightly) by using skips along with PAIRNORM-SI.<ref type="foot" target="#foot_3">4</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>Oversmoothing in GNNs: <ref type="bibr" target="#b7">Li et al. (2018)</ref> was the first to call attention to the oversmoothing problem. <ref type="bibr" target="#b16">Xu et al. (2018)</ref> introduced Jumping Knowledge Networks, which employ skip connections for multi-hop message passing and also enable different neighborhood ranges. <ref type="bibr" target="#b5">Klicpera et al. (2019)</ref> proposed a propagation scheme based on personalized Pagerank that ensures locality (via teleports) which in turn prevents oversmoothing. <ref type="bibr" target="#b6">Li et al. (2019)</ref> built on ideas from ResNet to use residual as well as dense connections to train deep GCNs. DropEdge <ref type="bibr" target="#b10">Rong et al. (2019)</ref> proposed to alleviate oversmoothing through message passing reduction via removing a certain fraction of edges at random from the input graph. These are all specialized solutions that introduce additional parameters and/or a different network architecture.  <ref type="formula">2016</ref>), and so on. Conceptually these have substantially different goals (e.g., reducing training time), and were not proposed for graph neural networks nor the oversmoothing problem therein. Important difference to note is that larger depth in regular neural-nets does not translate to more hops of propagation on a graph structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Normalization Schemes for Deep</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We investigated the oversmoothing problem in GNNs and proposed PAIRNORM, a novel normalization layer that boosts the robustness of deep GNNs against oversmoothing. PAIRNORM is fast to compute, requires no change in network architecture nor any extra parameters, and can be applied to any GNN. Experiments on real-world classification tasks showed the effectiveness of PAIRNORM, where it provides performance gains when the task benefits from more layers. Future work will explore other use cases of deeper GNNs that could further showcase PAIRNORM's advantages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><p>A.1 DERIVATION OF EQ. 8</p><formula xml:id="formula_18">TPSD( X) = i,j∈[n] xi − xj 2 2 = i,j∈[n] (x i − xj ) T (x i − xj ) (13) = i,j∈[n] (x T i xi + xT j xj − 2x T i xj ) (14) = 2n i∈[n] xT i xi − 2 i,j∈[n] xT i xj (15) = 2n i∈[n] xi 2 2 − 21 T X XT 1 (16) = 2n i∈[n] xi 2 2 − 2 1 T X 2 2 (17) = 2n 2 1 n n i=1 xi 2 2 − 1 n n i=1 xi 2 2 . (<label>18</label></formula><formula xml:id="formula_19">)</formula><p>A.2 DATASET STATISTICS  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 ADDITIONAL EXPERIMENTS UNDER SSNC-MV WITH INCREASING MISSING FRACTION p</head><p>In this section we report additional experiment results under the SSNC-MV setting with varying missing fraction, in particular p = {0.7, 0.8, 0.9, 1} and also report the base case where p = 0 for comparison.</p><p>Figure <ref type="figure">8</ref> presents results on all four datasets for GCN vs. PAIRNORM-enhanced GCN (denoted PN for short). The models without any skip connections are denoted by *-0, with one-hop skip connection by *-1, and with one and two-hop skip connections by *-2. Barcharts on the right report the best layer that each model produced the highest validation accuracy, and those on the left report the corresponding test accuracy. Figure <ref type="figure">9</ref> presents corresponding results for GAT.</p><p>We discuss the take-aways from these figures on the following page. We make the following observations based on Figures <ref type="figure">8 and 9</ref>:</p><formula xml:id="formula_20">G C N -0 P N -0 G C N -1 P N -1 G C N -2 P N -2 G C N -0 P N -0 G C N -1 P N -1 G C N -2 P N -2 G C N -0 P N -0 G C N -1 P N -1 G C N -2 P N -2 G C N -0 P N -0 G C N -1 P N -1 G C N -2 P N -2 G C N -0 P N -0 G C N -1 P N -1 G C N -2 P N -2 0 5 10 0% missing 70% missing 80% missing 90% missing 100% missing Best Layer G C N -0 P N -0 G C N -1 P N -1 G C N -2 P N -2 G C N -0 P N -0 G C N -1 P N -1 G C N -2 P N -2 G C N -0 P N -0 G C N -1 P N -1 G C N -2 P N -2 G C N -0 P N -0 G C N -1 P N -1 G C N -2 P N -2 G C N -0 P N -0 G C N -1 P N -1 G C N -2 P N -2 0.6 0.7 0.8 0% missing 70% missing 80% missing 90% missing 100% missing Test Acc cora GCN G C N -0 P N -0 G C N -1 P N -1 G C N -2 P N -2 G C N -0 P N -0 G C N -1 P N -1 G C N -2 P N -2 G C N -0 P N -0 G C N -1 P N -1 G C N -2 P N -2 G C N -0 P N -0 G C N -1 P N -1 G C N -2 P N -2 G C N -0 P N -0 G C N -1 P N -1 G C N -2 P N -2 0 5 10 0% missing 70% missing 80% missing 90% missing 100% missing Best Layer G C N -0 P N -0 G C N -1 P N -1 G C N -2 P N -2 G C N -0 P N -0 G C N -1 P N -1 G C N -2 P N -2 G C N -0 P N -0 G C N -1 P N -1 G C N -2 P N -2 G C N -0 P N -0 G C N -1 P N -1 G C N -2 P N -2 G C N -0 P N -0 G C N -1 P N -1 G C N -2 P N -2 0.4 0.6 0% missing 70% missing 80% missing 90% missing 100% missing Test Acc citeseer GCN G C N -0 P N -0 G C N -1 P N -1 G C N -2 P N -2 G C N -0 P N -0 G C N -1 P N -1 G C N -2 P N -2 G C N -0 P N -0 G C N -1 P N -1 G C N -2 P N -2 G C N -0 P N -0 G C N -1 P N -1 G C N -2 P N -2 G C N -0 P N -0 G C N -1 P N -1 G C N -2 P N -2 0 5 10 0% missing 70% missing 80% missing 90% missing 100% missing Best Layer G C N -0 P N -0 G C N -1 P N -1 G C N -2 P N -2 G C N -0 P N -0 G C N -1 P N -1 G C N -2 P N -2 G C N -0 P N -0 G C N -1 P N -1 G C N -2 P N -2 G C N -0 P N -0 G C N -1 P N -1 G C N -2 P N -2 G C N -0 P N -0 G C N -1 P N -1 G C N -2 P N -2 0.4 0.6 0.8 0% missing 70% missing 80% missing 90% missing 100% missing Test Acc pubmed GCN G C N -0 P N -0 G C N -1 P N -1 G C N -2 P N -2 G C N -0 P N -0 G C N -1 P N -1 G C N -2 P N -2 G C N -0 P N -0 G C N -1 P N -1 G C N -2 P N -2 G C N -0 P N -0 G C N -1 P N -1 G C N -2 P N -2 G C N -0 P N -0 G C N -1 P N -1 G C N -2 P N -2 0 5 10 0% missing 70% missing 80% missing 90% missing 100% missing Best Layer G C N -0 P N -0 G C N -1 P N -1 G C N -2 P N -2 G C N -0 P N -0 G C N -1 P N -1 G C N -2 P N -2 G C N -0 P N -0 G C N -1 P N -1 G C N -2 P N -2 G C N -0 P N -0 G C N -1 P N -1 G C N -2 P N -2 G C N -0 P N -0 G C N -1 P N -1 G C N -2 P N -<label>2</label></formula><p>• Performance of 'vanilla' GCN and GAT models without skip connections (i.e., GCN-0 and GAT-0) drop monotonically as we increase missing fraction p. • PAIRNORM-enhanced 'vanilla' models (PN-0, no skips) perform comparably or better than GCN-0 and GAT-0 in all cases, especially as p increases. In other words, with PAIRNORM at work, model performance is more robust against missing data. • Best number of layers for GCN-0 as we increase p only changes between 2-4. For GAT-0, it changes mostly between 2-6. • PAIRNORM-enhanced 'vanilla' models (PN-0, no skips) can go deeper, i.e., they can leverage a larger range of #layers (2-12) as we increase p. Specifically, GCN-PN-0 (GAT-PN-0) uses equal number or more layers than GCN-0 (GAT-0) in almost all cases. • Without any normalization, adding skip connections helps-GCN/GAT-1 and GCN/GAT-2 are better than GCN/GAT-0, especially as we increase p. • With PAIRNORM but no-skip, performance is comparable or better than just adding skips.</p><p>• Adding skips on top of PAIRNORM does not seem to introduce any notable gains.</p><p>In summary, simply employing our PAIRNORM for GCN and GAT provides robustness against oversmoothing that allows them to go deeper and achieve improved performance under SSNC-MV.   The results are shown in Figure <ref type="figure" target="#fig_7">10</ref>. Without normalization, SGC suffers from fast diminishing APD and APSD of random pairs. As we have proved, PAIRNORM normalizes APSD to be constant across layers, however it does not normalize APD, which appears to decrease linearly with increasing number of layers. Surprisingly, although PAIRNORM-SI is not theoretically proved to have a constant APSD and APD, empirically it achieves more stable APSD and APD than PAIRNORM. We were not able to prove this phenomenon mathematically, and leave it for further investigation. APD does not capture the full information of the distribution of pairwise distances. To show how the distribution changes by increasing number of layers, we use Tensorboard to plot the histograms of pairwise distances, as shown in Figure <ref type="figure">11</ref>. Comparing SGC and SGC with PAIRNORM, adding PAIRNORM keeps the left shift (shrinkage) of the distribution of random pair distances much slower than without normalization, while still sharing similar behavior of the distribution of connected pairwise distances. PAIRNORM-SI seems to be more powerful in keeping the median and mean of the distribution of random pair distances stable, while "spreading" the distribution out by increasing the variance. The performance of PAIRNORM and PAIRNORM-SI are similar, however it seems that PAIRNORM-SI is more powerful in stabilizing TPD and TPSD. Notice that oversmoothing occurs very quickly for GCN without any normalization, where both connected and random pair distances reach zero (!). In contrast, GCN with PAIRNORM or PAIRNORM-SI is able to keep random pair distances relatively apart while allowing connected pair distances to shrink. As also stated in main text, using PAIRNORM-SI for GCN and GAT is relatively more stable than using PAIRNORM in general cases (notice the near-constant random pair distances in the rightmost subfigures). There are several possible explanations for why PAIRNORM-SI is more stable. First, as shown in Figure <ref type="figure" target="#fig_7">10</ref> and Figure <ref type="figure" target="#fig_2">12</ref>, PAIRNORM-SI not only keeps APSD stable but also APD, further, the plots of distributions of pairwise distances (Figures <ref type="figure" target="#fig_9">11 and 13</ref>) also show the power of PAIRNORM-SI (notice the large gap between smaller connected pairwise distances and the larger random pairwise distances). Second, we conjecture that restricting representations to reside on a sphere can make training stable and faster, which we also observe empirically by studying the training curves. Third, GCN and GAT tend to overfit easily for the SSNC problem, due to many learnable parameters across layers and limited labeled input data, therefore it is possible that adding more restriction on these models helps reduce overfitting. All in all, these empirical measurements as illustrated throughout the figures in this section demonstrates that PAIRNORM and PAIRNORM-SI successfully address the oversmoothing problem for deep GNNs. Our work is the first to propose a normalization layer specifically designed for graph neural networks, which we hope will kick-start more work in this area toward training more robust and effective GNNs.</p><formula xml:id="formula_21">G A T -0 P N -0 G A T -1 P N -1 G A T -2 P N -2 G A T -0 P N -0 G A T -1 P N -1 G A T -2 P N -2 G A T -0 P N -0 G A T -1 P N -1 G A T -2 P N -2 G A T -0 P N -0 G A T -1 P N -1 G A T -2 P N -2 G A T -0 P N -0 G A T -1 P N -1 G A T -2 P N -2 0 5 10 0% missing 70% missing 80% missing 90% missing 100% missing Best Layer G A T -0 P N -0 G A T -1 P N -1 G A T -2 P N -2 G A T -0 P N -0 G A T -1 P N -1 G A T -2 P N -2 G A T -0 P N -0 G A T -1 P N -1 G A T -2 P N -2 G A T -0 P N -0 G A T -1 P N -1 G A T -2 P N -2 G A T -0 P N -0 G A T -1 P N -1 G A T -2 P N -2 0.6 0.8 0% missing 70% missing 80% missing 90% missing 100% missing Test Acc cora GAT G A T -0 P N -0 G A T -1 P N -1 G A T -2 P N -2 G A T -0 P N -0 G A T -1 P N -1 G A T -2 P N -2 G A T -0 P N -0 G A T -1 P N -1 G A T -2 P N -2 G A T -0 P N -0 G A T -1 P N -1 G A T -2 P N -2 G A T -0 P N -0 G A T -1 P N -1 G A T -2 P N -2 0 5 10 0% missing 70% missing 80% missing 90% missing 100% missing Best Layer G A T -0 P N -0 G A T -1 P N -1 G A T -2 P N -2 G A T -0 P N -0 G A T -1 P N -1 G A T -2 P N -2 G A T -0 P N -0 G A T -1 P N -1 G A T -2 P N -2 G A T -0 P -0 G A T -1 P N -1 G A T -2 P N -2 G A T -0 P N -0 G A T -1 P N -1 G A T -2 P N -<label>2</label></formula><formula xml:id="formula_22">G A T -0 P N -0 G A T -1 P N -1 G A T -2 P N -2 G A T -0 P N -0 G A T -1 P N -1 G A T -2 P N -2 G A T -0 P N -0 G A T -1 P N -1 G A T -2 P N -2 G A T -0 P N -0 G A T -1 P N -1 G A T -2 P N -2 G A T -0 P N -0 G A T -1 P N -1 G A T -2 P N -2 0 5 10 0% missing 70% missing 80% missing 90% missing 100% missing Best Layer G A T -0 P N -0 G A T -1 P N -1 G A T -2 P N -2 G A T -0 P N -0 G A T -1 P N -1 G A T -2 P N -2 G A T -0 P N -0 G A T -1 P N -1 G A T -2 P N -2 G A T -0 P N -0 G A T -1 P N -1 G A T -2 P N -2 G A T -0 P N -0 G A T -1 P N -1 G A T -2 P N -2 0.</formula><formula xml:id="formula_23">G A T -0 P N -0 G A T -1 P N -1 G A T -2 P N -2 G A T -0 P N -0 G A T -1 P N -1 G A T -2 P N -2 G A T -0 P N -0 G A T -1 P N -1 G A T -2 P N -2 G A T -0 P N -0 G A T -1 P N -1 G A T -2 P N -2 G A T -0 P N -0 G A T -1 P N -1 G A T -2 P N -2 0 5 10 0% missing 70% missing 80% missing 90% missing 100% missing Best Layer G A T -0 P N -0 G A T -1 P N -1 G A T -2 P N -2 G A T -0 P N -0 G A T -1 P N -1 G A T -2 P N -2 G A T -0 P N -0 G A T -1 P N -1 G A T -2 P N -2 G A T -0 P N -0 G A T -1 P N -1 G A T -2 P N -2 G A T -0 P N -0 G A T -1 P N -1 G A T -2 P N -<label>2</label></formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>where xc i denotes the centered representation. Note that this shifting does not affect the TPSD, and furthermore drives the term 1 , where computing TPSD( X) boils down to calculating the squared Frobenius norm of Xc and overall takes O(nd). That is, TPSD( X) = TPSD( Xc ) = 2n Xc 2 F .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of PAIRNORM, comprising centering and rescaling steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (best in color) Performance comparison of the original (dashed) vs. PAIRNORM-enhanced (solid) GCN and GAT models with increasing layers on Cora.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Figure4presents the performance of SGC, GCN, and GAT models on Cora with increasing number of layers, where we remove feature vectors from all the unlabeled nodes, i.e. p = 1. The models with PAIRNORM achieve a higher test accuracy compared to those without, which they typically reach at a larger number of layers. (See Fig.7in Appx. A.4 for results on other datasets.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>-NNs: There exist various normalization schemes proposed for deep neural networks, including batch normalization Ioffe &amp; Szegedy (2015), weight normalization Salimans &amp; Kingma (2016), layer normalization Ba et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :Figure 6 :Figure 7 :</head><label>567</label><figDesc>Figure 5: Comparison of 'vanilla' vs. PAIRNORM-enhanced SGC, corresponding to Figure 1, for datasets (from top to bottom) Citeseer, Pubmed, and CoauthorCS. PAIRNORM provides improved robustness to performance decay due to oversmoothing with increasing number of layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Figure10: Measuring average distance (squared and not-squared) between representations at each layer for SGC, SGC with PAIRNORM, and SGC with PAIRNORM-SI. The setting is the same with Figure1and they share the same performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :Figure 12 :</head><label>1112</label><figDesc>Figure 11: Measuring distribution of distances between representations at each layer for SGC, SGC with PAIRNORM, and SGC with PAIRNORM-SI. Supplementary results for Figure 10.</figDesc><graphic url="image-2.png" coords="16,111.33,176.31,126.82,79.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Measuring distribution of distances between representations at each layer for GCN, GCN with PAIRNORM, and GCN with PAIRNORM-SI. Supplementary results for Figure 12.</figDesc><graphic url="image-12.png" coords="17,365.23,299.02,128.62,80.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of 'vanilla' vs. PAIRNORM-enhanced SGC performance in Cora, Citeseer, Pubmed, and CoauthorCS for SSNC-MV problem, with missing rate ranging from 0% to 100%. Showing test accuracy at #L (K in Eq. 4) layers, at which model achieves best validation accuracy.</figDesc><table><row><cell cols="2">Missing Percentage</cell><cell>0%</cell><cell>20%</cell><cell>40%</cell><cell>60%</cell><cell>80%</cell><cell>100%</cell></row><row><cell>Dataset</cell><cell>Method</cell><cell cols="6">Acc #L Acc #L Acc #L Acc #L Acc #L Acc #L</cell></row><row><cell>Cora</cell><cell cols="7">SGC SGC-PN 0.811 7 0.799 7 0.797 7 0.783 20 0.780 25 0.745 40 0.815 4 0.806 5 0.786 3 0.742 4 0.733 3 0.423 15</cell></row><row><cell>Citeseer</cell><cell cols="7">SGC SGC-PN 0.706 3 0.695 3 0.653 4 0.641 5 0.590 50 0.486 50 0.689 10 0.684 6 0.668 8 0.657 9 0.565 8 0.290 2</cell></row><row><cell>Pubmed</cell><cell cols="7">SGC SGC-PN 0.782 9 0.781 7 0.778 60 0.782 7 0.772 60 0.719 40 0.754 1 0.748 1 0.723 4 0.746 2 0.659 3 0.399 35</cell></row><row><cell>CoauthorCS</cell><cell cols="7">SGC SGC-PN 0.915 2 0.909 2 0.899 3 0.891 4 0.880 8 0.860 20 0.914 1 0.898 2 0.877 2 0.824 2 0.751 4 0.318 2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of 'vanilla' and (PAIRNORM-SI/ residual)-enhanced GCN performance on Cora, Citeseer, Pubmed, and CoauthorCS for SSNC-MV problem, with 0% and 100% feature missing rate. t represents the skip-step of residual connection. (See A.5 Fig. 8 for more settings.)</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>Cora</cell><cell cols="2">Citeseer</cell><cell cols="2">Pubmed</cell><cell cols="2">CoauthorCS</cell></row><row><cell>Missing(%)</cell><cell>0%</cell><cell>100%</cell><cell>0%</cell><cell>100%</cell><cell>0%</cell><cell>100%</cell><cell>0%</cell><cell>100%</cell></row><row><cell>Method</cell><cell cols="8">Acc #L Acc #L Acc #L Acc #L Acc #L Acc #L Acc #L Acc #L</cell></row><row><cell>GCN</cell><cell cols="8">0.821 2 0.582 2 0.695 2 0.313 2 0.779 2 0.449 2 0.877 2 0.452 4</cell></row><row><cell>GCN-PN</cell><cell cols="8">0.790 2 0.731 10 0.660 2 0.498 8 0.780 30 0.745 25 0.910 2 0.846 12</cell></row><row><cell>GCN-t1</cell><cell cols="8">0.822 2 0.721 15 0.696 2 0.441 12 0.780 2 0.656 25 0.898 2 0.727 12</cell></row><row><cell cols="9">GCN-t1-PN 0.780 2 0.724 30 0.648 2 0.465 10 0.756 15 0.690 12 0.898 2 0.830 20</cell></row><row><cell>GCN-t2</cell><cell cols="8">0.820 2 0.722 10 0.691 2 0.432 20 0.779 2 0.645 20 0.882 4 0.630 20</cell></row><row><cell cols="9">GCN-t2-PN 0.785 4 0.740 30 0.650 2 0.508 12 0.770 15 0.725 30 0.911 2 0.839 20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison of 'vanilla' and (PAIRNORM-SI/ residual)-enhanced GAT performance on Cora, Citeseer, Pubmed, and CoauthorCS for SSNC-MV problem, with 0% and 100% feature missing rate. t represents the skip-step of residual connection. (See A.5 Fig. 9 for more settings.)</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>Cora</cell><cell cols="2">Citeseer</cell><cell cols="2">Pubmed</cell><cell cols="2">CoauthorCS</cell></row><row><cell>Missing(%)</cell><cell>0%</cell><cell>100%</cell><cell>0%</cell><cell>100%</cell><cell>0%</cell><cell>100%</cell><cell>0%</cell><cell>100%</cell></row><row><cell>Method</cell><cell cols="2">Acc #L Acc #L</cell><cell cols="6">Acc #L Acc #L Acc #L Acc #L Acc #L Acc #L</cell></row><row><cell>GAT</cell><cell cols="8">0.823 2 0.653 4 0.693 2 0.428 4 0.774 6 0.631 4 0.892 4 0.737 4</cell></row><row><cell>GAT-PN</cell><cell cols="8">0.787 2 0.718 6 0.670 2 0.483 4 0.774 12 0.714 10 0.916 2 0.843 8</cell></row><row><cell>GAT-t1</cell><cell cols="8">0.822 2 0.706 8 0.693 2 0.461 6 0.769 4 0.698 8 0.899 4 0.842 10</cell></row><row><cell cols="9">GAT-t1-PN 0.787 2 0.710 10 0.658 6 0.500 10 0.757 4 0.684 12 0.911 2 0.844 20</cell></row><row><cell>GAT-t2</cell><cell cols="8">0.820 2 0.691 8 s0.692 2 0.461 6 0.774 8 0.702 8 0.895 4 0.803 6</cell></row><row><cell cols="9">GAT-t2-PN 0.788 4 0.738 12 0.672 4 0.517 10 0.776 15 0.704 12 0.917 2 0.855 30</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Dataset statistics.</figDesc><table><row><cell>Name</cell><cell cols="5">#Nodes #Edges #Features #Classes Label Rate</cell></row><row><cell>Cora</cell><cell>2708</cell><cell>5429</cell><cell>1433</cell><cell>7</cell><cell>0.052</cell></row><row><cell>Citeseer</cell><cell>3327</cell><cell>4732</cell><cell>3703</cell><cell>6</cell><cell>0.036</cell></row><row><cell>Pubmed</cell><cell cols="2">19717 44338</cell><cell>500</cell><cell>3</cell><cell>0.003</cell></row><row><cell cols="3">CoauthorCS 18333 81894</cell><cell>6805</cell><cell>15</cell><cell>0.030</cell></row><row><cell cols="6">A.3 ADDITIONAL PERFORMANCE PLOTS WITH INCREASING NUMBER OF LAYERS</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Supplementary results toTable 2 for GCN on (from top to bottom) Cora, Citeseer, Pubmed, and CoauthorCS.</figDesc><table><row><cell>Test Acc</cell><cell>coauthor_CS GCN</cell></row><row><cell>0.8</cell><cell></cell></row><row><cell>0.6</cell><cell></cell></row><row><cell>0.4</cell><cell></cell></row><row><cell cols="2">0% missing 70% missing 80% missing 90% missing 100% missing</cell></row><row><cell>Figure 8:</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Supplementary results to Table3for GAT on (from top to bottom) Cora, Citeseer, Pubmed, and CoauthorCS.A.6 CASE STUDY: ADDITIONAL MEASURES FOR PAIRNORM AND PAIRNORM-SI WITH SGC AND GCNTo better understand why PAIRNORM and PAIRNORM-SI are helpful for training deep GNNs, we report additional measures for (SGC and GCN) with (PAIRNORM and PAIRNORM-SI) over the Cora dataset. In the main text, we claim TPSD (total pairwise squared distances) is constant across layers for SGC with PAIRNORM (for GCN/GAT this is not guaranteed because of the influence of activation function and dropout layer). In this section we empirically measure pairwise (squared) distances for both SGC and GCN, with PAIRNORM and PAIRNORM-SI.A.6.1 SGC WITH PAIRNORM AND PAIRNORM-SITo verify our analysis of PAIRNORM for SGC, and understand how the variant of PAIRNORM (PAIRNORM-SI) works, we measure the average pairwise squared distance (APSD) as well as the average pairwise distance (APD) between the representations for two categories of node pairs: (1) connected pairs (nodes that are directly connected in graph) and (2) random pairs (uniformly randomly chosen among the node set). APSD of random pairs reflects the TPSD, and APD of random pairs reflects the total pairwise distance (TPD). Under the homophily assumption of the labels w.r.t. the graph structure, we want APD or APSD of connected pairs to be small while keeping APD or APSD of random pairs relatively large.</figDesc><table><row><cell>Test Acc</cell><cell>coauthor_CS GAT</cell></row><row><cell>0.9</cell><cell></cell></row><row><cell>0.8</cell><cell></cell></row><row><cell>0.7</cell><cell></cell></row><row><cell cols="2">0% missing 70% missing 80% missing 90% missing 100% missing</cell></row><row><cell>Figure 9:</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">We normalize each column j as the Laplacian smoothing stationary point πj is not scale-free. See Eq. (1).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">There exist other variants of (6) that achieve similar goals, and we leave the space for future exploration.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">See Appendix A.1 for the detailed derivation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">Notice a slight performance drop when PAIRNORM is applied at 0% rate. For this setting, and the datasets we have, shallow networks are sufficient and smoothing through only a few (2-4) layers improves generalization ability for the SSNC problem (recall Figure1solid lines). PAIRNORM has a small reversing effect in these scenarios, hence the small performance drop.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>CoRR, abs/1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Zien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2006">2006. 2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
	<note>Semi-Supervised Learning</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>2016 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>CoRR, abs/1502.03167</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR). OpenReview.net</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Combining neural networks with personalized pagerank for classification on graphs</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Can GCNs go as deep as CNNs?</title>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<idno>CoRR, abs/1904.03751</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning</title>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd AAAI Conference on Artificial Intelligence</title>
				<meeting>the 32nd AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3538" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Revisiting graph neural networks: All we have is low-pass filters</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">T</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takanori</forename><surname>Maehara</surname></persName>
		</author>
		<idno>CoRR, abs/1905.09550</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gmnn: Graph markov neural networks</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5241" to="5250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The truly deep graph convolutional networks for node classification</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<idno>CoRR, abs/1907.10903</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><surname>Durk</surname></persName>
		</author>
		<author>
			<persName><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="901" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05868</idno>
		<title level="m">Pitfalls of graph neural network evaluation</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR). OpenReview.net</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><forename type="middle">H</forename><surname>Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, volume 97 of Proceedings of Machine Learning Research</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Representation Learning on Graphs with Jumping Knowledge Networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
				<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="5453" to="5462" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
