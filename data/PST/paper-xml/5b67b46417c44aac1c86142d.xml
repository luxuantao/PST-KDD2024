<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bing</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Mathematical Sciences</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haoteng</forename><surname>Yin</surname></persName>
							<email>htyin@pku.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Academy for Advanced Interdisciplinary Studies</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Center for Data Science</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
							<email>zhanxing.zhu@pku.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">Center for Data Science</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Beijing Institute of Big Data Research (BIBDR)</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Timely accurate traffic forecast is crucial for urban traffic control and guidance. Due to the high nonlinearity and complexity of traffic flow, traditional methods cannot satisfy the requirements of mid-and-long term prediction tasks and often neglect spatial and temporal dependencies. In this paper, we propose a novel deep learning framework, Spatio-Temporal Graph Convolutional Networks (STGCN), to tackle the time series prediction problem in traffic domain. Instead of applying regular convolutional and recurrent units, we formulate the problem on graphs and build the model with complete convolutional structures, which enable much faster training speed with fewer parameters. Experiments show that our model STGCN effectively captures comprehensive spatio-temporal correlations through modeling multi-scale traffic networks and consistently outperforms state-of-the-art baselines on various real-world traffic datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transportation plays a vital role in everybody's daily life. According to a survey in 2015, U.S. drivers spend about 48 minutes on average behind the wheel daily. 1 Under this circumstance, accurate real-time forecast of traffic conditions is of paramount importance for road users, private sectors and governments. Widely used transportation services, such as flow control, route planning, and navigation, also rely heavily on a high-quality traffic condition evaluation. In general, multiscale traffic forecast is the premise and foundation of urban traffic control and guidance, which is also one of main functions of the Intelligent Transportation System (ITS).</p><p>In the traffic study, fundamental variables of traffic flow, namely speed, volume, and density are typically chosen as indicators to monitor the current status of traffic conditions and ⇤ Equal contributions.</p><p>† Corresponding author. 1 https://aaafoundation.org/american-driving-survey-2014-2015/ to predict the future. Based on the length of prediction, traffic forecast is generally classified into two scales: short-term (5 ⇠ 30 min), medium and long term (over 30 min). Most prevalent statistical approaches (for example, linear regression) are able to perform well on short interval forecast. However, due to the uncertainty and complexity of traffic flow, those methods are less effective for relatively long-term predictions.</p><p>Previous studies on mid-and-long term traffic prediction can be roughly divided into two categories: dynamical modeling and data-driven methods. Dynamical modeling uses mathematical tools (e.g. differential equations) and physical knowledge to formulate traffic problems by computational simulation <ref type="bibr">[Vlahogianni, 2015]</ref>. To achieve a steady state, the simulation process not only requires sophisticated systematic programming but also consumes massive computational power. Impractical assumptions and simplifications among the modeling also degrade the prediction accuracy. Therefore, with rapid development of traffic data collection and storage techniques, a large group of researchers are shifting their attention to data-driven approaches.</p><p>Classic statistical and machine learning models are two major representatives of data-driven methods. In timeseries analysis, autoregressive integrated moving average (ARIMA) and its variants are one of the most consolidated approaches based on classical statistics <ref type="bibr" target="#b2">[Ahmed and Cook, 1979;</ref><ref type="bibr">Williams and Hoel, 2003</ref>]. However, this type of model is limited by the stationary assumption of time sequences and fails to take the spatio-temporal correlation into account. Therefore, these approaches have constrained representability of highly nonlinear traffic flow. Recently, classic statistical models have been vigorously challenged by machine learning methods on traffic prediction tasks. Higher prediction accuracy and more complex data modeling can be achieved by these models, such as k-nearest neighbors algorithm (KNN), support vector machine (SVM), and neural networks (NN).</p><p>Deep learning approaches have been widely and successfully applied to various traffic tasks nowadays. Significant progress has been made in related work, for instance, deep belief network (DBN) <ref type="bibr">[Jia et al., 2016;</ref><ref type="bibr">Huang et al., 2014]</ref>, stacked autoencoder (SAE) <ref type="bibr">[Lv et al., 2015;</ref><ref type="bibr">Chen et al., 2016]</ref>. However, it is difficult for these dense networks to extract spatial and temporal features from the input jointly. Moreover, within narrow constraints or even complete absence of spatial attributes, the representative ability of these networks would be hindered seriously.</p><p>To take full advantage of spatial features, some researchers use convolutional neural network (CNN) to capture adjacent relations among the traffic network, along with employing recurrent neural network (RNN) on time axis. By combining long short-term memory (LSTM) network <ref type="bibr">[Hochreiter and Schmidhuber, 1997]</ref>  <ref type="table" target="#tab_1">and 1-</ref> <ref type="bibr">D CNN, Wu and Tan [2016]</ref> presented a feature-level fused architecture CLTFP for shortterm traffic forecast. Although it adopted a straightforward strategy, CLTFP still made the first attempt to align spatial and temporal regularities. <ref type="bibr">Afterwards, Shi et al. [2015]</ref> proposed the convolutional LSTM, which is an extended fullyconnected LSTM (FC-LSTM) with embedded convolutional layers. However, the normal convolutional operation applied restricts the model to only process grid structures (e.g. images, videos) rather than general domains. Meanwhile, recurrent networks for sequence learning require iterative training, which introduces error accumulation by steps. Additionally, RNN-based networks (including LSTM) are widely known to be difficult to train and computationally heavy.</p><p>For overcoming these issues, we introduce several strategies to effectively model temporal dynamics and spatial dependencies of traffic flow. To fully utilize spatial information, we model the traffic network by a general graph instead of treating it separately (e.g. grids or segments). To handle the inherent deficiencies of recurrent networks, we employ a fully convolutional structure on time axis. Above all, we propose a novel deep learning architecture, the spatio-temporal graph convolutional networks, for traffic forecasting tasks. This architecture comprises several spatio-temporal convolutional blocks, which are a combination of graph convolutional layers <ref type="bibr">[Defferrard et al., 2016]</ref> and convolutional sequence learning layers, to model spatial and temporal dependencies. To the best of our knowledge, it is the first time that to apply purely convolutional structures to extract spatio-temporal features simultaneously from graph-structured time series in a traffic study. We evaluate our proposed model on two realworld traffic datasets. Experiments show that our framework outperforms existing baselines in prediction tasks with multiple preset prediction lengths and network scales. not independent but linked by pairwise connection in graph. Therefore, the data point v t can be regarded as a graph signal that is defined on an undirected graph (or directed one) G with weights w ij as shown in Figure <ref type="figure">1</ref>. At the t-th time step, in graph G t = (V t , E, W ), V t is a finite set of vertices, corresponding to the observations from n monitor stations in a traffic network; E is a set of edges, indicating the connectedness between stations; while W 2 R n⇥n denotes the weighted adjacency matrix of G t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Traffic Prediction on Road Graphs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Convolutions on Graphs</head><p>A standard convolution for regular grids is clearly not applicable to general graphs. There are two basic approaches currently exploring how to generalize CNNs to structured data forms. One is to expand the spatial definition of a convolution <ref type="bibr">[Niepert et al., 2016]</ref>, and the other is to manipulate in the spectral domain with graph Fourier transforms <ref type="bibr">[Bruna et al., 2013]</ref>. The former approach rearranges the vertices into certain grid forms which can be processed by normal convolutional operations. The latter one introduces the spectral framework to apply convolutions in spectral domains, often named as the spectral graph convolution. Several followingup studies make the graph convolution more promising by reducing the computational complexity from O(n 2 ) to linear <ref type="bibr">[Defferrard et al., 2016;</ref><ref type="bibr">Kipf and Welling, 2016]</ref>.</p><p>We introduce the notion of graph convolution operator "⇤ G " based on the conception of spectral graph convolution, as the multiplication of a signal x 2 R n with a kernel ⇥,</p><formula xml:id="formula_0">⇥ ⇤ G x = ⇥(L)x = ⇥(U ⇤U T )x = U ⇥(⇤)U T x,<label>(2)</label></formula><p>where graph Fourier basis U 2 R n⇥n is the matrix of eigenvectors of the normalized graph Laplacian 3 Proposed Model</p><formula xml:id="formula_1">L = I n D 1 2 W D 1 2 = U ⇤U T 2 R n⇥n (I n is an identity matrix, D 2 R n⇥n is the diagonal degree matrix with D ii = ⌃ j W ij ); ⇤ 2 R n⇥n</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network Architecture</head><p>In this section, we elaborate on the proposed architecture of spatio-temporal graph convolutional networks (STGCN). As shown in Figure <ref type="figure" target="#fig_1">2</ref>, STGCN is composed of several spatiotemporal convolutional blocks, each of which is formed as a "sandwich" structure with two gated sequential convolution layers and one spatial graph convolution layer in between.</p><p>The details of each module are described as follows. </p><formula xml:id="formula_2">ST-Conv Block ST-Conv Block Output Layer (vt-M+1, … vt) W Spatial Graph-Conv, C=16 Temporal Gated-Conv, C=64 Temporal Gated-Conv, C=64 GLU 1-D Conv W v (vt-M+1, … vt ) l l Temporal Gated-Conv ST-Conv Block (vt-M+K , … vt ) l l t v l v l+1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Graph CNNs for Extracting Spatial Features</head><p>The traffic network generally organizes as a graph structure.</p><p>It is natural and reasonable to formulate road networks as graphs mathematically. However, previous studies neglect spatial attributes of traffic networks: the connectivity and globality of the networks are overlooked, since they are split into multiple segments or grids. Even with 2-D convolutions on grids, it can only capture the spatial locality roughly due to compromises of data modeling. Accordingly, in our model, the graph convolution is employed directly on graphstructured data to extract highly meaningful patterns and features in the space domain. Though the computation of kernel ⇥ in graph convolution by Eq. ( <ref type="formula" target="#formula_0">2</ref>) can be expensive due to O(n 2 ) multiplications with graph Fourier basis, two approximation strategies are applied to overcome this issue.</p><p>Chebyshev Polynomials Approximation To localize the filter and reduce the number of parameters, the kernel ⇥ can be restricted to a polynomial of ⇤ as</p><formula xml:id="formula_3">⇥(⇤) = P K 1 k=0 ✓ k ⇤ k , where ✓ 2 R K is a vector of polynomial coefficients. K</formula><p>is the kernel size of graph convolution, which determines the maximum radius of the convolution from central nodes. Traditionally, Chebyshev polynomial T k (x) is used to approximate kernels as a truncated expansion of order K 1 as</p><formula xml:id="formula_4">⇥(⇤) ⇡ P K 1 k=0 ✓ k T k ( ⇤)</formula><p>with rescaled ⇤ = 2⇤/ max I n ( max denotes the largest eigenvalue of L) <ref type="bibr">[Hammond et al., 2011]</ref>. The graph convolution can then be rewritten as,</p><formula xml:id="formula_5">⇥ ⇤ G x = ⇥(L)x ⇡ K 1 X k=0 ✓ k T k ( L)x,<label>(3)</label></formula><p>where T k ( L) 2 R n⇥n is the Chebyshev polynomial of order k evaluated at the scaled Laplacian L = 2L/ max I n . By recursively computing K-localized convolutions through the polynomial approximation, the cost of Eq. ( <ref type="formula" target="#formula_0">2</ref>) can be reduced to O(K|E|) as Eq. ( <ref type="formula" target="#formula_5">3</ref>) shows <ref type="bibr">[Defferrard et al., 2016]</ref>.</p><p>1 st -order Approximation A layer-wise linear formulation can be defined by stacking multiple localized graph convolutional layers with the first-order approximation of graph Laplacian <ref type="bibr">[Kipf and Welling, 2016]</ref>. Consequently, a deeper architecture can be constructed to recover spatial information in depth without being limited to the explicit parameterization given by the polynomials. Due to the scaling and normalization in neural networks, we can further assume that max ⇡ 2. Thus, the Eq. ( <ref type="formula" target="#formula_5">3</ref>) can be simplified to,</p><formula xml:id="formula_6">⇥ ⇤ G x ⇡ ✓ 0 x + ✓ 1 ( 2 max L I n )x ⇡ ✓ 0 x ✓ 1 (D 1 2 W D 1 2 )x,<label>(4)</label></formula><p>where ✓ 0 , ✓ 1 are two shared parameters of the kernel. In order to constrain parameters and stabilize numerical performances, ✓ 0 and ✓ 1 are replaced by a single parameter ✓ by letting ✓ = ✓ 0 = ✓ 1 ; W and D are renormalized by W = W + I n and Dii = ⌃ j Wij separately. Then, the graph convolution can be alternatively expressed as,</p><formula xml:id="formula_7">⇥ ⇤ G x = ✓(I n + D 1 2 W D 1 2 )x = ✓( D 1 2 W D 1 2 )x.</formula><p>(5)</p><p>Applying a stack of graph convolutions with the 1 st -order approximation vertically that achieves the similar effect as Klocalized convolutions do horizontally, all of which exploit the information from the (K 1)-order neighborhood of central nodes. In this scenario, K is the number of successive filtering operations or convolutional layers in a model instead.</p><p>Additionally, the layer-wise linear structure is parametereconomic and highly efficient for large-scale graphs, since the order of the approximation is limited to one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalization of Graph Convolutions</head><p>The graph convolution operator "⇤ G " defined on x 2 R n can be extended to multi-dimensional tensors. For a signal with C i channels X 2 R n⇥Ci , the graph convolution can be generalized by,</p><formula xml:id="formula_8">y j = Ci X i=1 ⇥ i,j (L)x i 2 R n , 1  j  C o (6) with the C i ⇥ C o vectors of Chebyshev coefficients ⇥ i,j 2 R K (C i , C</formula><p>o are the size of input and output of the feature maps, respectively). The graph convolution for 2-D variables is denoted as "⇥ ⇤ G X" with ⇥ 2 R K⇥Ci⇥Co . Specifically, the input of traffic prediction is composed of M frame of road graphs as Figure <ref type="figure">1</ref> shows. Each frame v t can be regarded as a matrix whose column i is the C i -dimensional value of v t at the i th node in graph G t , as X 2 R n⇥Ci (in this case, C i = 1). For each time step t of M , the equal graph convolution operation with the same kernel ⇥ is imposed on X t 2 R n⇥Ci in parallel. Thus, the graph convolution can be further generalized in 3-D variables, noted as "⇥ ⇤ G X " with X 2 R M ⇥n⇥Ci .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Gated CNNs for Extracting Temporal Features</head><p>Although RNN-based models become widespread in timeseries analysis, recurrent networks for traffic prediction still suffer from time-consuming iterations, complex gate mechanisms, and slow response to dynamic changes. On the contrary, CNNs have the superiority of fast training, simple structures, and no dependency constraints to previous steps. Inspired by <ref type="bibr">[Gehring et al., 2017]</ref>, we employ entire convolutional structures on time axis to capture temporal dynamic behaviors of traffic flows. This specific design allows parallel and controllable training procedures through multi-layer convolutional structures formed as hierarchical representations.</p><p>As Figure <ref type="figure" target="#fig_1">2</ref> (right) shows, the temporal convolutional layer contains a 1-D causal convolution with a width-K t kernel followed by gated linear units (GLU) as a non-linearity. For each node in graph G, the temporal convolution explores K t neighbors of input elements without padding which leading to shorten the length of sequences by K t -1 each time. Thus, input of temporal convolution for each node can be regarded as a length-M sequence with C i channels as Y 2 R M ⇥Ci . The convolution kernel 2 R Kt⇥Ci⇥2Co is designed to map the input Y to a single output element [P Q] 2 R (M Kt+1)⇥(2Co) (P , Q is split in half with the same size of channels). As a result, the temporal gated convolution can be defined as,</p><formula xml:id="formula_9">⇤ T Y = P (Q) 2 R (M Kt+1)⇥Co ,<label>(7)</label></formula><p>where P , Q are input of gates in GLU respectively; denotes the element-wise Hadamard product. The sigmoid gate (Q) controls which input P of the current states are relevant for discovering compositional structure and dynamic variances in time series. The non-linearity gates contribute to the exploiting of the full input filed through stacked temporal layers as well. Furthermore, residual connections are implemented among stacked temporal convolutional layers. Similarly, the temporal convolution can also be generalized to 3-D variables by employing the same convolution kernel to every node Y i 2 R M ⇥Ci (e.g. sensor stations) in G equally, noted as " ⇤ T Y" with Y 2 R M ⇥n⇥Ci .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Spatio-temporal Convolutional Block</head><p>In order to fuse features from both spatial and temporal domains, the spatio-temporal convolutional block (ST-Conv block) is constructed to jointly process graph-structured time series. The block itself can be stacked or extended based on the scale and complexity of particular cases.</p><p>As illustrated in Figure <ref type="figure" target="#fig_1">2</ref> (mid), the spatial layer in the middle is to bridge two temporal layers which can achieve fast spatial-state propagation from graph convolution through temporal convolutions. The "sandwich" structure also helps the network sufficiently apply bottleneck strategy to achieve scale compression and feature squeezing by downscaling and upscaling of channels C through the graph convolutional layer. Moreover, layer normalization is utilized within every ST-Conv block to prevent overfitting.</p><p>The input and output of ST-Conv blocks are all 3-D tensors. For the input v l 2 R M ⇥n⇥C l of block l, the output v l+1 2 R (M 2(Kt 1))⇥n⇥C l+1 is computed by,</p><formula xml:id="formula_10">v l+1 = l 1 ⇤ T ReLU(⇥ l ⇤ G ( l 0 ⇤ T v l )),<label>(8)</label></formula><p>where l 0 , l 1 are the upper and lower temporal kernel within block l, respectively; ⇥ l is the spectral kernel of graph convolution; ReLU(•) denotes the rectified linear units function. After stacking two ST-Conv blocks, we attach an extra temporal convolution layer with a fully-connected layer as the output layer in the end (See the left of Figure <ref type="figure" target="#fig_1">2</ref>). The temporal convolution layer maps outputs of the last ST-Conv block to a single-step prediction. Then, we can obtain a final output Z 2 R n⇥c from the model and calculate the speed prediction for n nodes by applying a linear transformation across c-channels as v = Zw + b, where w 2 R c is a weight vector and b is a bias. We use L2 loss to measure the performance of our model. Thus, the loss function of STGCN for traffic prediction can be written as,</p><formula xml:id="formula_11">L(v; W ✓ ) = X t ||v(v t M +1 , ..., v t , W ✓ ) v t+1 || 2 , (<label>9</label></formula><formula xml:id="formula_12">)</formula><p>where W ✓ are all trainable parameters in the model; v t+1 is the ground truth and v(•) denotes the model's prediction.</p><p>We now summarize the main characteristics of our model STGCN in the following,</p><p>• STGCN is a universal framework to process structured time series. It is not only able to tackle traffic network modeling and prediction issues but also to be applied to more general spatio-temporal sequence learning tasks.</p><p>• The spatio-temporal block combines graph convolutions and gated temporal convolutions, which can extract the most useful spatial features and capture the most essential temporal features coherently.</p><p>• The model is entirely composed of convolutional structures and therefore achieves parallelization over input with fewer parameters and faster training speed. More importantly, this economic architecture allows the model to handle large-scale networks with more efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset Description</head><p>We verify our model on two real-world traffic datasets, BJER4 and PeMSD7, collected by Beijing Municipal Traffic Commission and California Deportment of Transportation, respectively. Each dataset contains key attributes of traffic observations and geographic information with corresponding timestamps, as detailed below.</p><p>BJER4 was gathered from the major areas of east ring No.4 routes in Beijing City by double-loop detectors. There are 12 roads selected for our experiment. The traffic data are aggregated every 5 minutes. The time period used is from 1st July to 31st August, 2014 except the weekends. We select the first month of historical speed records as training set, and the rest serves as validation and test set respectively.</p><p>PeMSD7 was collected from Caltrans Performance Measurement System (PeMS) in real-time by over 39, 000 sensor stations, deployed across the major metropolitan areas of California state highway system <ref type="bibr">[Chen et al., 2001]</ref>. The dataset is also aggregated into 5-minute interval from 30-second data samples. We randomly select a medium and a large scale among the District 7 of California containing 228 and 1, 026 stations, labeled as PeMSD7(M) and PeMSD7(L), respectively, as data sources (shown in the left of Figure <ref type="figure" target="#fig_2">3</ref>). The time range of PeMSD7 dataset is in the weekdays of May and June of 2012. We split the training and test sets based on the same principles as above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Data Preprocessing</head><p>The standard time interval in two datasets is set to 5 minutes. Thus, every node of the road graph contains 288 data points per day. The linear interpolation method is used to fill missing values after data cleaning. In addition, data input are normalized by Z-Score method.</p><p>In BJER4, the topology of the road graph in Beijing east No.4 ring route system is constructed by the deployment diagram of sensor stations. By collating affiliation, direction and origin-destination points of each road, the ring route system can be digitized as a directed graph.</p><p>In PeMSD7, the adjacency matrix of the road graph is computed based on the distances among stations in the traffic network. The weighted adjacency matrix W can be formed as,</p><formula xml:id="formula_13">w ij = 8 &lt; : exp( d 2<label>ij</label></formula><p>2 ), i 6 = j and exp(</p><formula xml:id="formula_14">d 2 ij 2 ) ✏ 0 , otherwise. (<label>10</label></formula><formula xml:id="formula_15">)</formula><p>where w ij is the weight of edge which is decided by d ij (the distance between station i and j). 2 and ✏ are thresholds to control the distribution and sparsity of matrix W , assigned to 10 and 0.5, respectively. The visualization of W is presented in the right of Figure <ref type="figure" target="#fig_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Settings</head><p>All experiments are compiled and tested on a Linux cluster (CPU: Intel(R) Xeon(R) CPU E5-2620 v4 @ 2.10GHz, GPU: NVIDIA GeForce GTX 1080). In order to eliminate atypical traffic, only workday traffic data are adopted in our experiment <ref type="bibr">[Li et al., 2015]</ref>. We execute grid search strategy to locate the best parameters on validations. All the tests use 60 minutes as the historical time window, a.k.a. 12 observed data points (M = 12) are used to forecast traffic conditions in the next 15, 30, and 45 minutes (H = 3, 6, 9). Root Mean Squared Errors (RMSE) are adopted. We compare our framework STGCN with the following baselines: 1). Historical Average (HA); 2). Linear Support Victor Regression (LSVR); 3 ) with the 1 st -order approximation. We train our models by minimizing the mean square error using RMSprop for 50 epochs with batch size as 50. The initial learning rate is 10 3 with a decay rate of 0.7 after every 5 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Metric &amp; Baselines</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experiment Results</head><p>Table <ref type="table" target="#tab_1">1</ref> and 2 demonstrate the results of STGCN and baselines on the datasets BJER4 and PeMSD7(M/L). Our proposed model achieves the best performance with statistical significance (two-tailed T-test, ↵ = 0.01, P &lt; 0.01) in all three evaluation metrics. We can easily observe that traditional statistical and machine learning methods may perform well for short-term forecasting, but their long-term predictions are not accurate because of error accumulation, memorization issues, and absence of spatial information. ARIMA model performs the worst due to its incapability of handling complex spatio-temporal data. Deep learning approaches generally achieved better prediction results than traditional machine learning models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Benefits of Spatial Topology</head><p>Previous methods did not incorporate spatial topology and modeled the time series in a coarse-grained way. Differently, through modeling spatial topology of the sensors, our model STGCN has achieved a significant improvement on short and mid-and-long term forecasting. The advantage of STGCN is more obvious on dataset PeMSD7 than BJER4, since the sensor network of PeMS is more complicated and structured (as illustrated in Figure <ref type="figure" target="#fig_2">3</ref>), and our model can effectively utilize spatial structure to make more accurate predictions.</p><p>To compare three methods based on graph convolution: GCGRU, STGCN(Cheb) and STGCN(1 st ), we show their   predictions during morning peak and evening rush hours, as shown in Figure <ref type="figure" target="#fig_3">4</ref>. It is easy to observe that our proposal STGCN captures the trend of rush hours more accurately than other methods; and it detects the ending of the rush hours earlier than others. Stemming from the efficient graph convolution and stacked temporal convolution structures, our model is capable of fast responding to the dynamic changes among the traffic network without over-reliance on historical average as most of recurrent networks do.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Efficiency and Generalization</head><p>To see the benefits of the convolution along time axis in our proposal, we summarize the comparison of training time between STGCN and GCGRU in Table <ref type="table" target="#tab_5">3</ref>. In terms of fairness, GCGRU consists of three layers with 64, 64, 128 units respectively in the experiment for PeMSD7(M), and STGCN uses the default settings as described in Section 4.3. Our model STGCN only consumes 272 seconds, while RNN-type of model GCGRU spends 3, 824 seconds on PeMSD7(M). This 14 times acceleration of training speed mainly benefits from applying the temporal convolution instead of recurrent structures, which can achieve fully parallel training rather than exclusively relying on chain structures as RNN  In order to further investigate the performance of compared deep learning models, we plot the RMSE and MAE of the test set of PeMSD7(M) during the training process, see Figure <ref type="figure" target="#fig_4">5</ref>. Those figures also suggest that our model can achieve much faster training procedure and easier convergences. Thanks to the special designs in ST-Conv blocks, our model has superior performances in balancing time consumption and parameter settings. Specifically, the number of parameters in STGCN (4.54 ⇥ 10 5 ) only accounts for around two third of GCGRU, and saving over 95% parameters compared to FC-LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Works</head><p>There are recent deep learning studies that are also motivated by graph convolution in spatio-temporal tasks. <ref type="bibr">Seo et al. [2016]</ref> introduced graph convolutional recurrent network (GCRN) to identify jointly spatial structures and dynamic variation from structured sequences of data. The key challenge of this study is to determine the optimal combinations of recurrent networks and graph convolution under specific settings. Based on principles above, Li et al. <ref type="bibr">[2018]</ref> successfully employed the gated recurrent units (GRU) with graph convolution for long-term traffic forecasting. In contrast to these works, we build up our model completely from convolutional structures; The ST-Conv block is specially designed to uniformly process structured data with residual connection and bottleneck strategy inside; More efficient graph convolution kernels are employed in our model as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this paper, we propose a novel deep learning framework STGCN for traffic prediction, integrating graph convolution and gated temporal convolution through spatio-temporal convolutional blocks. Experiments show that our model outperforms other state-of-the-art methods on two real-world datasets, indicating its great potentials on exploring spatiotemporal structures from the input. It also achieves faster training, easier convergences, and fewer parameters with flexibility and scalability. These features are quite promising and practical for scholarly development and large-scale industry deployment. In the future, we will further optimize the network structure and parameter settings. Moreover, our proposed framework can be applied into more general spatiotemporal structured sequence forecasting scenarios, such as evolving of social networks, and preference prediction in recommendation systems, etc.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>is the diagonal matrix of eigenvalues of L, and filter ⇥(⇤) is also a diagonal matrix. By this definition, a graph signal x is filtered by a kernel ⇥ with multiplication between ⇥ and graph Fourier transform U Tx[Shuman et al., 2013].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Architecture of spatio-temporal graph convolutional networks. The framework STGCN consists of two spatio-temporal convolutional blocks (ST-Conv blocks) and a fully-connected output layer in the end. Each ST-Conv block contains two temporal gated convolution layers and one spatial graph convolution layer in the middle. The residual connection and bottleneck strategy are applied inside each block. The input v t M +1, ..., vt is uniformly processed by ST-Conv blocks to explore spatial and temporal dependencies coherently. Comprehensive features are integrated by an output layer to generate the final prediction v.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: PeMS sensor network in District 7 of California (left), each dot denotes a sensor station; Heat map of weighted adjacency matrix in PeMSD7(M) (right).</figDesc><graphic url="image-1.png" coords="5,196.88,56.53,79.81,79.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Speed prediction in the morning peak and evening rush hours of the dataset PeMSD7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Test RMSE versus the training time (left); Test MAE versus the number of training epochs (right). (PeMSD7(M))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>.23/ 6.12 10.11/ 12.70/ 14.95 5.91/ 7.27/ 8.81 ARIMA 5.99/ 6.27/ 6.70 15.42/ 16.36/ 17.67 8.19/ 8.38/ 8.72 FNN 4.30/ 5.33/ 6.14 10.68/ 13.48/ 15.82 5.86/ 7.31/ 8.58 FC-LSTM 4.24/ 4.74/ 5.22 10.78/ 12.17/ 13.60 5.71/ 6.62/ 7.44 GCGRU 3.84/ 4.62/ 5.32 9.31/ 11.41/ 13.30 5.22/ 6.35/ 7.58 STGCN(Cheb) 3.78/ 4.45/ 5.03 9.11/ 10.80/ 12.27 5.20/ 6.20/ 7.21 STGCN(1 st ) 3.83/ 4.51/ 5.10 9.28/ 11.19/ 12.79 5.29/ 6.39/ 7.39 Performance comparison of different approaches on the dataset BJER4.</figDesc><table><row><cell>Model</cell><cell>MAE</cell><cell>BJER4 (15/ 30/ 45 min) MAPE (%)</cell><cell>RMSE</cell></row><row><cell>HA</cell><cell>5.21</cell><cell>14.64</cell><cell>7.56</cell></row><row><cell>LSVR</cell><cell>4.24/ 5</cell><cell></cell><cell></cell></row></table><note>To measure and evaluate the performance of different methods, Mean Absolute Errors (MAE), Mean Absolute Percentage Errors (MAPE), and</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>). Auto-Regressive Integrated Moving Average (ARIMA); 4). Feed-Forward Neural Network (FNN); 5). Full-Connected LSTM (FC-LSTM) [Sutskever et al., 2014]; 6). Graph Convolutional GRU (GCGRU) [Li et al., 2018].STGCN Model For BJER4 and PeMSD7(M/L), the channels of three layers in ST-Conv block are 64, 16, 64 respectively. Both the graph convolution kernel size K and temporal convolution kernel size K t are set to 3 in the model STGCN(Cheb) with the Chebyshev polynomials approximation, while the K is set to 1 in the model STGCN(1 st</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison of different approaches on the dataset PeMSD7.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Time consumptions of training on the dataset PeMSD7.do. For PeMSD7(L), GCGRU has to use the half of batch size since its GPU consumption exceeded the memory capacity of a single card (results marked as "*" in Table2); while STGCN only need to double the channels in the middle of ST-Conv blocks. Even though our model still consumes less than a tenth of the training time of model GCGRU under this circumstance. Meanwhile, the advantages of the 1 st -order approximation have appeared since it is not restricted to the parameterization of polynomials. The model STGCN(1 st ) speeds up around 20% on a larger dataset with a satisfactory performance compared with STGCN(Cheb).</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1">Proceedings of the Twenty-Seventh Joint Conference on Artificial Intelligence </note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"> Box-Jenkins techniques. 1979.  [Bruna et al., 2013]  <p>Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally connected networks on graphs. arXiv preprint arXiv:1312.6203, 2013. [Chen et al., 2001] Chao Chen, Karl Petty, Alexander Skabardonis, Pravin Varaiya, and Zhanfeng Jia. Freeway performance measurement system: mining loop detector data. Transportation Research Record: Journal of the Transportation Research Board, (1748):96-102, 2001. [Chen et al., 2016] Quanjun Chen, Xuan Song, Harutoshi Yamada, and Ryosuke Shibasaki. Learning deep representation from big and heterogeneous data for traffic accident inference. In AAAI, pages 338-344, 2016. [Defferrard et al., 2016] Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In NIPS, pages 3844-3852, 2016. [Gehring et al., 2017] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional sequence to sequence learning. arXiv preprint arXiv:1705.03122, 2017. [Hammond et al., 2011] David K Hammond, Pierre Vandergheynst, and Rémi Gribonval. Wavelets on graphs via spectral graph theory. Applied and Computational Harmonic Analysis, 30(2):129-150, 2011. [Hochreiter and Schmidhuber, 1997] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735-1780, 1997. [Huang et al., 2014] Wenhao Huang, Guojie Song, Haikun Hong, and Kunqing Xie. Deep architecture for traffic flow prediction: deep belief networks with multitask learning. IEEE Transactions on Intelligent Transportation Systems, 15(5):2191-2201, 2014. [Jia et al., 2016] Yuhan Jia, Jianping Wu, and Yiman Du. Traffic speed prediction using deep learning method. In ITSC, pages 1217-1222. IEEE, 2016. [Kipf and Welling, 2016] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016. [Li et al., 2015] Yexin Li, Yu Zheng, Huichu Zhang, and Lei Chen. Traffic prediction in a bike-sharing system. In SIGSPATIAL, page 33. ACM, 2015. [Li et al., 2018] Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. Diffusion convolutional recurrent neural network: Data-driven traffic forecasting. In ICLR, 2018. [Lv et al., 2015] Yisheng Lv, Yanjie Duan, Wenwen Kang, Zhengxi Li, and Fei-Yue Wang. Traffic flow prediction with big data: a deep learning approach. IEEE Transactions on Intelligent Transportation Systems, 16(2):865-873, 2015. [Niepert et al., 2016] Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. Learning convolutional neural networks for graphs. In ICML, pages 2014-2023, 2016. [Seo et al., 2016] Youngjoo Seo, Michaël Defferrard, Pierre Vandergheynst, and Xavier Bresson. Structured sequence modeling with graph convolutional recurrent networks. arXiv preprint arXiv:1612.07659, 2016. [Shi et al., 2015] Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun Woo. Convolutional lstm network: A machine learning approach for precipitation nowcasting. In NIPS, pages 802-810, 2015. [Shuman et al., 2013] David I Shuman, Sunil K Narang, Pascal Frossard, Antonio Ortega, and Pierre Vandergheynst. The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains. IEEE Signal Processing Magazine, 30(3):83-98, 2013. [Sutskever et al., 2014] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In NIPS, pages 3104-3112, 2014. [Vlahogianni, 2015] Eleni I Vlahogianni. Computational intelligence and optimization for transportation big data: challenges and opportunities. In Engineering and Applied Sciences Optimization, pages 107-128. Springer, 2015. [Williams and Hoel, 2003] Billy M Williams and Lester A Hoel. Modeling and forecasting vehicular traffic flow as a seasonal arima process: Theoretical basis and empirical results. Journal of transportation engineering, 129(6):664-672, 2003. [Wu and Tan, 2016] Yuankai Wu and Huachun Tan. Shortterm traffic flow forecasting with spatial-temporal correlation in a hybrid deep learning framework. arXiv preprint arXiv:1612.01022, 2016.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Model PeMSD7(M) (15/ 30/ 45 min) PeMSD7(L) (15/ 30/ 45 min)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Mae Mape (%) Rmse Mae</forename><surname>Mape</surname></persName>
		</author>
		<idno>HA 4.01 10.61 7.20 4.60 12.50 8.05 LSVR 2.50/ 3.63/ 4.54 5.81/ 8.88/ 11.50 4.55/ 6.67/ 8.28 2.69/ 3.85/ 4.79 6.27/ 9.48/ 12.42 4.88/ 7.10/ 8.72</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Analysis of freeway traffic time-series data by using</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allen</forename><forename type="middle">R</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><surname>Cook</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
