<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rethinking TLB Designs in Virtualized Environments: A Very Large Part-of-Memory TLB</title>
				<funder ref="#_xbW2ReR">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jee</forename><surname>Ho</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin ? Texas Instruments</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ryoo</forename><surname>Nagendra Gulur</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin ? Texas Instruments</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuang</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin ? Texas Instruments</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lizy</forename><forename type="middle">K</forename><surname>John</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin ? Texas Instruments</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Rethinking TLB Designs in Virtualized Environments: A Very Large Part-of-Memory TLB</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3079856.3080210</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Address Translation</term>
					<term>Very Large TLB</term>
					<term>Virtualization</term>
					<term>Die-Stacked DRAM</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With increasing deployment of virtual machines for cloud services and server applications, memory address translation overheads in virtualized environments have received great attention. In the radix-4 type of page tables used in x86 architectures, a TLB-miss necessitates up to 24 memory references for one guest to host translation. While dedicated page walk caches and such recent enhancements eliminate many of these memory references, our measurements on the Intel Skylake processors indicate that many programs in virtualized mode of execution still spend hundreds of cycles for translations that do not hit in the TLBs.</p><p>This paper presents an innovative scheme to reduce the cost of address translations by using a very large Translation Lookaside Buffer that is part of memory, the POM-TLB. In the POM-TLB, only one access is required instead of up to 24 accesses required in commonly used 2D walks with radix-4 type of page tables. Even if many of the 24 accesses may hit in the page walk caches, the aggregated cost of the many hits plus the overhead of occasional misses from page walk caches still exceeds the cost of one access to the POM-TLB. Since the POM-TLB is part of the memory space, TLB entries (as opposed to multiple page table entries) can be cached in large L2 and L3 data caches, yielding significant benefits. Through detailed evaluation running SPEC, PARSEC and graph workloads, we demonstrate that the proposed POM-TLB improves performance by approximately 10% on average. The improvement is more than 16% for 5 of the benchmarks. It is further seen that a POM-TLB of 16MB size can eliminate nearly all TLB misses in 8-core systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Computer systems organization ? Heterogeneous (hybrid) systems;</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="formula">6</ref><p>) hL3 <ref type="bibr" target="#b6">(7)</ref> hL2 <ref type="bibr" target="#b7">(8)</ref> hL1 <ref type="bibr" target="#b8">(9)</ref> gL3 <ref type="bibr" target="#b9">(10)</ref> hL4 <ref type="bibr" target="#b10">(11)</ref> hL3 <ref type="bibr" target="#b11">(12)</ref> hL2 <ref type="bibr" target="#b12">(13)</ref> hL1 <ref type="bibr" target="#b13">(14)</ref> gL2 <ref type="bibr" target="#b14">(15)</ref> hL4 <ref type="bibr" target="#b15">(16)</ref> hL3 <ref type="bibr" target="#b16">(17)</ref> hL2 <ref type="bibr" target="#b17">(18)</ref> hL1 <ref type="bibr" target="#b18">(19)</ref> gL1 <ref type="bibr" target="#b19">(20)</ref> hL4 <ref type="bibr" target="#b20">(21)</ref> hL3 <ref type="bibr" target="#b21">(22)</ref> hL2 <ref type="bibr" target="#b22">(23)</ref> hL1 <ref type="bibr" target="#b23">(24)</ref> hPA </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Cloud services such as Amazon EC2 <ref type="bibr" target="#b0">[1]</ref> or Rackspace OpenStack <ref type="bibr" target="#b39">[40]</ref> use virtualization platforms to provide their services. These platforms use hypervisors (ESX, KVM, Xen) to enable easier scalability of applications and higher system utilization by abstracting the underlying host resources. While virtualization provides many benefits, the overhead of virtualization is not universally low <ref type="bibr" target="#b16">[17]</ref>.</p><p>One of the largest contributors of performance overhead in virtualized environments is memory virtualization. An application executing on a guest OS generates guest virtual addresses (gVA) that need to be translated to host physical addresses (hPA). Since physical memory is under the exclusive control of a hypervisor, every guest physical address (gPA) needs to be translated to host physical before the guest application-issued memory access can complete. This requires navigating through two sets of page tables: a guest page table that the guest OS implements (gVA ? gPA), and a host page table that the hypervisor implements (gPA ? hPA). In x86 architectures, both the guest and host page tables employ a 4-level radix-tree table organization. Translating a virtual address to physical address takes 4 memory references in a bare metal case using a radix-4 table, and in the virtualized case, it becomes a full 2D translation with up to 24 memory accesses as depicted in Figure <ref type="figure" target="#fig_1">1</ref>. In order to bridge the performance gap associated with address translations, recent processors have added architecture support in the form of nested page tables <ref type="bibr" target="#b6">[7]</ref> and extended page tables <ref type="bibr" target="#b21">[22]</ref> that cache guest-to-host translations. Processor vendors have also added dedicated MMU/page walk caches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23]</ref> to cache the contents of guest and host page tables. Additional techniques to reduce the overhead of page walks include caching of page table entries in data caches, agile paging <ref type="bibr" target="#b16">[17]</ref>, TLB prefetching <ref type="bibr" target="#b9">[10]</ref>, shared L2 TLBs <ref type="bibr" target="#b8">[9]</ref>, transparent huge pages (THP) <ref type="bibr" target="#b2">[3]</ref>, speculative TLB fetching <ref type="bibr" target="#b4">[5]</ref> and splintering <ref type="bibr" target="#b37">[38]</ref>. These page walk enhancements have significantly reduced translation costs, however the translation overhead continues to be a source of inefficiency in virtualized environments.</p><p>Figure <ref type="figure" target="#fig_2">2</ref> based on our experiments on a state-of-the-art Intel system (Skylake i7-6700) shows the average number of cycles spent in address translation (per L2 TLB miss) in several SPEC, PARSEC and graph workloads. The workloads are run on a VM, with Linux THP support enabled and the performance overhead is measured using the Linux perf utility. Further details of our experimental setup are presented in Section 3. The translation overhead per L2 TLB miss is seen to range from 61 cycles in the canneal benchmark to 1158 cycles in the connected component graph benchmark despite the fact that Skylake core includes special MMU/paging structure caches (PSCs). Translation overhead running into 100+ cycles has also been reported in prior work <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b37">38]</ref>.</p><p>The experiments on the Intel Skylake platform also shed light on the virtualization overhead compared to native execution of the same workload. Figure <ref type="figure">3</ref> plots the ratio of translation cycles in virtualized and native setups. Workloads such as gups (1.5x), con_comp (26x), gcc (1.9x), lbm (2.5x) and mcf (2.5x) have far higher translation overhead in virtualized execution compared to native execution. Many benchmarks spend up to 14% execution time in translation even in the bare metal case and hence will benefit from the proposed scheme which improves both native and virtualized cases.</p><p>With the increased number of cores and big data sets, the conventional two-level SRAM TLBs cannot hold translations of all pages in the working set. Increasing L2 TLB sizes to sufficiently reduce TLB misses is not feasible because larger SRAM TLBs incur higher access latencies. We used CACTI <ref type="bibr" target="#b46">[47]</ref> to show the access latency sensitivity study with larger L2 TLB capacities in Figure <ref type="figure">4</ref>. The access latency is normalized to that of 16KB SRAM. As seen, naively increasing the SRAM capacity does not scale.</p><p>In the light of the above discussion, it would be desirable to have a TLB with a large reach at tolerable latency, so that a large   By making the POM-TLB as part of memory, it becomes possible to automatically take advantage of the growing L2 and L3 data cache capacities. While data caches already cache page table entries, multiple page table entries will be required per each translation, whereas a single POM-TLB entry will be sufficient to accomplish the virtualized translation. Hence caching TLB entries is more effective and beneficial than caching page table entries.</p><p>This paper makes the following contributions:</p><p>? We present a characterization of the virtual memory overheads on state-of-the-art hardware in several SPEC, PAR-SEC and graph workloads, while executing in bare metal and virtualized environments. ? We demonstrate that slow memory structures like DRAM can be used to house a large capacity TLB that can hold nearly all required address translations. On average, the proposed POM-TLB can achieve 10% performance improvement over a baseline system. For 5 of the benchmarks, the speedup is 16% or higher.   ? We present several solutions to the challenges encountered while implementing a TLB in DRAM. We present a low overhead TLB location predictor and other enhancements to make a DRAM-based L3 TLB a feasible option.</p><p>The rest of this paper is organized as follows: Section 2 gives a detailed architectural description of POM-TLB; Section 3 describes our experimental setup; Section 4 presents our evaluation results; Section 5 discusses additional design benefits of our work; finally, Section 7 concludes the paper. In this section, we describe the overall operation of POM-TLB, our very large L3 TLB, which can be implemented in off-chip memory or die-stacked DRAMs. Implementing in emerging die-stack DRAMs gives some advantages, although conceptually that is not a requirement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">System Level Organization</head><p>Most modern processors have private multi-level TLBs. In this work, we assume the TLB organization in a system similar to Intel Skylake architecture <ref type="bibr" target="#b22">[23]</ref> where there are two levels of TLBs. Figure <ref type="figure" target="#fig_5">6</ref> shows an overview of POM-TLB. We add a large shared L3 TLB after the private L2 TLBs. An L2 TLB miss looks up the large shared TLB and initiate a page walk if this shared TLB also suffered a miss. In practice, since DRAM look-up is slow, we make our POM-TLB addressable thereby enabling caching of TLB entries in data caches and faster translations. Cached POM-TLB entries are stored in L2 and L3 data caches, so an L2 TLB miss looks up data caches before accessing the large L3 TLB. Only when the translation request misses in L3 TLB, then a page walk is initiated. In reality, since POM-TLB is so large that virtually almost all page walks are eliminated. This is discussed in detail in Section 2.1.3.</p><p>2.1.1 POM-TLB Organization. While conceptually not a requirement, implementing POM-TLB in emerging die-stacked DRAMs integrated onto the processor gives bandwidth and possibly small latency advantages. Die-stacked DRAM has a DRAM-like organization, typically with access granularities such as 64B.</p><p>Figure <ref type="figure">5</ref> shows the metadata format for POM-TLB in a single channel of a typical die-stacked DRAM with multiple banks. We show the detailed layout of a single row in a bank. Each row can house multiple TLB entries as the row size is 2KB. Each entry has a valid bit, process ID, Virtual Address (VA), and Physical Address (PA) as in on-chip TLBs. To facilitate the translation in virtualized platforms, we also have Virtual Machine (VM) ID to distinguish addresses coming from different virtual machines as in Intel's Virtual Process ID (VPID) <ref type="bibr" target="#b21">[22]</ref>. The attributes include information such as replacement and protection bits. Each entry is 16B and four entries make 64B. We implement our TLB as a four way associative structure since 1) we have found that the associativity lower than four invokes significantly higher conflict misses and 2) 64B is the common die-stacked DRAM burst length where no memory controller design modifications are necessary. Upon a request, four entries are fetched from a single die-stacked DRAM row. Each row can incorporate 128 TLB entries and with 4 way associativity, a row can hold 32 sets of TLB entries. As we will discuss in Section 4.4, due to spatio-temporal locality, the accesses to L3 TLB entries exhibit a very high row-buffer hit rate resulting in low average access times.</p><p>2.1.2 Support for Two Page Sizes. In order to support both small page (4KB) and large page (2MB) TLB entries, and to avoid complexity in addressing a single TLB structure with two page sizes, we simply partitioned TLBs into two, one dedicated to hold 4KB page entries (denoted POM_T LB Small ) and the other 2MB page entries (denoted POM_T LB Large ) <ref type="foot" target="#foot_0">1</ref> . In our implementation, their sizes are statically set and remain fixed. As they are DRAM-based and can afford large capacities, we observed that their exact sizes do not matter much. We use a page size predictor (described in Section 2.1.4) to minimize having to perform two DRAM look-ups for the two page sizes. Based on the predicted page size, the corresponding TLB is accessed first. If it is a miss, the other TLB is accessed next. As discussed in Section 4.3, the page size predictor is highly accurate thereby almost always requiring just a single DRAM access.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.1.3</head><p>Caching TLB Entries. While L1 and L2 TLBs are designed for fast look-up, the POM-TLB is designed for very large reach and consequently its DRAM-based implementation incurs higher access latency. In order to alleviate this, we map the POM-TLB into the physical address space. By making the TLB addressable, we achieve the important benefit of enabling the caching of TLB entries in data caches. Both POM_T LB Small and POM_T LB Large are assigned address ranges. A POM-TLB comprising N sets is assigned an address range of 64 ? N bytes as each set holds four 16-byte TLB entries. The virtual address (VA) of the L2 TLB miss is converted to a POM-TLB set index by extracting log 2 N bits of the VA (after XOR-ing them with the VM ID bits to distribute the set-mapping evenly). For the POM_T LB Small , the memory address of the set that the VA maps to is given by:</p><formula xml:id="formula_0">Addr POM_TLB VA = VA ?V M_ID &gt;&gt; 6 &amp; 1 &lt;&lt; log 2 N -1 * 64 + Base_Addr POM_TLB (1)</formula><p>where Base_Addr POM_T LB is the starting address of the POM_T LB. Base_POM_T LB addresses are different depending on the page size since POM-TLB is physically partitioned between large and small pages.</p><p>In our scheme, L2 TLB misses do not initiate page walks. Instead, for each L2 TLB miss, the MMU computes the POM-TLB (say POM_T LB Large ) set address where the TLB entry for the virtual address of the miss may be found. The MMU then issues a load request to the L2D$ with this address. At this point, this becomes a normal cache access. If the entry is found in the L2D$, then the MMU reads the L2D$ cache block (64B) to access all the 4 translation entries stored in it. It performs associative search of the 4 entries to find a match for the incoming virtual address. If a match is found, then the corresponding entry provides the translation for this address. Being a normal read access, if the L2D$ does not contain the POM_T LB Large address, then the request is issued to the L3D$. If no match was found in the L3D$, then the physical memory (in this case a POM_T LB Large location) is accessed. Associative search of the set stored in the POM_T LB Large is used to identify if a translation of the virtual address is present or not. Like data misses, TLB entries that are misses in data caches are filled into the caches after resolving them at the POM-TLB or via page walks.</p><p>Since the POM-TLB provides two potential set locations where the translation for a given VA may be found (POM_T LB Small and POM_T LB Large ), we would have to perform two cache look-ups starting with the L2D$. Assuming an equal number of accesses to 4KB and 2MB pages, this results in 50% additional TLB look-up accesses into the L2D$. This has both latency and power implications. In order to address this, we design a simple yet highly accurate Page Size Predictor whose implementation is described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.4">Page Size Prediction.</head><p>We implement a simple yet highly effective page size predictor. The predictor comprises 512 2-bit entries, with one of the bits used to predict the page size and the other bit used to predict whether to bypass the caches (see next Section). The predictor is indexed using 9 bits of the virtual address of the L2 TLB miss (ignoring the lower order 12 bits). If the predicted page size is incorrect (0 means 4KB, 1 means 2MB), then the prediction entry for the index is updated<ref type="foot" target="#foot_1">2</ref> . While consuming very little SRAM storage (128 bytes per core), it achieves very high accuracy as discussed in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.5">Cache Bypass Prediction.</head><p>In workloads where the data load/store access rate to the data caches far exceeds the rate of L2 TLB misses, the caches tend to contain very few POM-TLB entries since they get evicted to make room to fill in data misses. In such a scenario, looking up the data caches before reaching the POM-TLB is wasteful in terms of both power and latency. Thus we incorporate a 1-bit bypass predictor to bypass the caches. The predictor implementation is shared with the page size predictor described above. As shown in Figure <ref type="figure" target="#fig_6">7</ref>, the cache bypass prediction takes effects after the POM-TLB address is generated since this new address is needed to access POM-TLB. Once the bypass decision is made, the request directly accesses POM-TLB by saving tens of lookup cycles in data caches.</p><p>2.1.6 Pu ing It All Together. Our scheme relies on making the large TLB addressable to achieve both a high hit rate in data caches as well as lower the access latency. Figure <ref type="figure" target="#fig_6">7</ref> shows an overall flow of POM-TLB. L2 TLB misses start out by consulting the page size predictor. If the predictor indicates a cache bypass, then the MMU directly accesses the predicted POM-TLB depending on the predicted page size. If a translation entry is found, the PFN (Physical Page Frame) is returned. If it is predicted not bypass, the MMU checks the POM-TLB entries in data caches. In the common case, the predictor does not predict cache-bypassing. The L2D$ is first probed with the address of the predicted POM-TLB set location. If a match is found with correct VA,V M_ID, then the translation is done with a single cache access by returning the corresponding PFN. If no match is found, then the MMU probes the L3D$ similarly. If the probed POM-TLB address misses in both L2D$ and L3D$, then the MMU computes a new POM-TLB address (corresponding to the size that was not predicted) and initiates cache look-up. If the new POM-TLB address misses in both levels of data caches, then a page walk is initiated. In practice, we observed that a vast majority of POM-TLB entries hit in the L2D$ and L3D$ resulting in very few DRAM accesses. As discussed in Section 4, the caching of POM-TLB entries in data caches causes very little degradation to the data cache hit rate for normal load/store accesses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Implementation Considerations</head><p>We explain the key design decisions of the POM-TLB here:</p><p>Consistency: Since POM-TLB is shared across cores, the consistency requirement between entries in L3 and underlying L1/L2 TLB has to be met. Although strictly inclusive L3 TLB is desirable, it adds significant hardware complexity. Since our TLB operates at DRAM latency, which is already much slower than on-chip SRAM TLBs, adding such structure is not a practical option. Similar to prior work <ref type="bibr" target="#b8">[9]</ref>, we adopt the mostly inclusive implementation, which is adopted in x86 caches <ref type="bibr" target="#b19">[20]</ref>. In this design, each TLB can make independent replacement decisions, which makes it possible that some entries in L1/L2 TLBs are missing from L3 TLB. However, this significantly reduces the hardware overheads associated with keeping strictly inclusive. Therefore, our TLB is designed to be aware of TLB-shootdowns. TLB-shootdowns require that all corresponding TLBs are locked until the consistency issue is resolved. Yet, TLBshootdowns are rare occurrences and recent work <ref type="bibr" target="#b34">[35]</ref> has shown a shootdown mechanism that can significantly reduce the overheads. Thus, the benefits of having simpler consistency check hardware outweigh the shootdown overheads, and hence such a design can be adopted.</p><p>In addition, the consistency across different virtual machines is already handled by modern virtual machine managers such as KVM hypervisor <ref type="bibr" target="#b1">[2]</ref>. Upon a change in TLB, a memory notifier is called to let the host system know that a guest TLB has been updated. Then, the host OS invalidates all related TLBs in other VMs. Therefore, issues such as dirty page handling, process ID recycling, etc are already incorporated in KVM and host OS. The recent adoption of VM ID facilitates this process, and thus, POM-TLB can maintain consistency in the presence of multiple virtual machines. Although not all virtual machine managers have such feature, since such feature is implemented in software, future virtual machines can incorporate it.</p><p>Channel Contention: Memory systems share a common command/data bus to exchange data between controllers and multiple banks. Many of today's applications experience memory contention as the bandwidth is either saturated or near saturation <ref type="bibr" target="#b18">[19]</ref>. Implementing the L3 TLB in an integrated die-stacked DRAM offers advantages from this perspective. Our proposal adds additional traffic only to the integrated DRAM to retrieve translation entries and not to the off-chip DRAM. Also this additional traffic is minor and only incurred when the L2D$ and L3D$ return cache misses when probed for cached VL_TLB entries. The path from last level caches to die-stacked DRAM architecture is different from one to off-chip DRAM as it has its own dedicated high-speed bus to communicate with processors. Hence, additional traffic due to our L3 TLB does not interfere with existing main memory traffic. In fact, our TLB's high hit rate reduces a significant amount of page table walks that result in main memory accesses, so it is likely that the main memory traffic sees considerable performance benefits as well.</p><p>Entry Replacement: Since our structure is four way associative, the attribute metadata (annoted as attr in Figure <ref type="figure">5</ref>) contains 2 LRU bits. These bits are updated upon each L3 TLB access and the appropriate eviction candidate is chosen using these bits. Since LRU bits of four entries are fetched in a DRAM burst, the replacement decision can be made without incurring additional die-stacked DRAM accesses.</p><p>Other Die-Stacked DRAM Use: Die-stacked DRAM capacity is growing to multi-gigabytes, and in our experiments, POM-TLB achieves good performance at capacities like 32MB. The remaining die-stacked DRAM capacity can be used as a large last level data cache or a part of memory as proposed by prior work <ref type="bibr">[11, 13-15, 18, 21, 25-27, 31-33, 39, 43, 44, 50]</ref>. Since JEDEC standard incorporates multiple channels in the HBM specification <ref type="bibr" target="#b23">[24]</ref>, we assume we are using one dedicated channel to service the POM-TLB requests. When the large die-stacked DRAM is used as both a large TLB and a large last level cache, the performance improvement will be even higher than results shown in this work, which only presents performance improvement from address translation.</p><p>Assuming 16MB capacity of POM-TLB, there can be a tradeoff between using this additional capacity as L4 data cache vs L3 TLB. In a cache design, a hit saves one memory access. However, in the case of an L3 TLB, especially in virtualized environment, the L3 TLB hit can save up to <ref type="bibr" target="#b23">24</ref>   <ref type="table">1</ref>: Experimental Parameters the total number of overall memory accesses. Furthermore, data accesses are non-blocking accesses where multiple requests can be on the fly. The access latency can be hidden by means of memory level parallelism such as bank level parallelism, which is common in today's DRAM. On the other hand, an address translation is a critical blocking request where upon a TLB miss, the processor execution stalls. Therefore, the impact of serving the translation request is much higher. Consequently, using the same capacity as a large TLB is likely to save more cycles than using it as L4 data cache. Note that 16MB is a small fraction of a die-stacked DRAM, and as previously mentioned, the rest of die-stacked DRAM can be used as a large data cache via separate channel without translation traffic contention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTAL EVALUATION</head><p>We evaluate the performance of POM-TLB using a combination of real system measurement, PIN-based and Ramulator-like <ref type="bibr" target="#b29">[30]</ref> simulation, and performance models. Our virtualization platform is QEMU 2.0 with KVM support. Our host system is Ubuntu 14.04 running on Intel Skylake <ref type="bibr" target="#b22">[23]</ref> with Transparent Huge Pages (THP) <ref type="bibr" target="#b2">[3]</ref> turned on. Our host system has Intel VT-x with support for Extended Page Tables while the guest OS is Ubuntu 14.04 installed on QEMU also with THP turned on. The host system parameters are listed in Table <ref type="table">1</ref> under Processor, MMU, and PSC categories. The system has separate L1 TLBs for each page size (4KB, 2MB, and 1GB in our system) though our applications do not use 1GB size. The L2 TLB is a unified TLB for both 4KB and 2MB pages. Finally, the specific performance counters (e.g., 0x0108, 0x1008, 0x0149, 0x1049) that we used read page walk cycles taking MMU cache hits into account, so the page walk cycles we use in this paper are the average cycles spent after a translation request misses in L2 TLB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Workloads</head><p>The main focus of this work is on memory subsystems, and thus, applications, which do not spend a considerable amount of time in memory, are not meaningful. Consequently, we chose a subset of SPEC CPU and PARSEC applications that are known to be memory intensive. In addition, we also ran graph workloads such as the graph500 and big data workloads such as connected components and pagerank. The benchmark characteristics are collected from the Intel Skylake platform, and they are presented in Table <ref type="table" target="#tab_5">2</ref>. We included applications whose page walk cycles, walk overheads, etc are in a wide range of spectrum (low to high). Since SPEC CPU applications are single threaded, we run multiple copies of SPEC CPU applications (as in the SPECrate mode), to evaluate performance on our multicore simulator. We ensure that they do not share the physical memory space via proper virtual-to-physical address translation. For multithreaded workloads, we profiled benchmarks with 8 threads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation Methodology</head><p>A combination of measurement (on real hardware), simulation and performance modeling is used to estimate the performance of the proposed scheme. First, the workloads listed in Table <ref type="table" target="#tab_5">2</ref> are executed to completion and the Linux perf utility is used to measure the total instructions (I total ), cycles (C total ), number of L2 TLB misses (M total ) and total L2 TLB miss penalty cycles (P total ) in a manner similar to the methodology in prior work <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b47">48]</ref>. We obtain the baseline IPC as: IPC baseline = I total C total . We also compute the ideal cycles C ideal and average translation penalty cycles per L2 TLB miss P Baseline Avg as:</p><formula xml:id="formula_1">C ideal = C total -P total<label>(2)</label></formula><formula xml:id="formula_2">P Baseline Avg = P total M total<label>(3)</label></formula><p>Note that the effects of various caching techniques like page walk caches, caching of PTEs in data caches, Intel extended page tables and nested TLBs are already included in the performance measurement since they are part of the base commodity processor. The average translation costs per L2 TLB miss as computed above are also listed in the workloads table.</p><p>Next, we use PIN and the Linux pagemap to generate memory traces for our workloads. For each workload, all load and store requests are recorded. The Linux pagemap is used to extend the PIN tool to include page size and other OS related metadata. Our trace contains virtual address, instruction count, read/write flag, thread ID and page size information of each reference. Memory instructions are traced in detail while the non-memory instructions are abstracted. We collected the memory traces for <ref type="bibr" target="#b19">20</ref>  Furthermore, we developed a detailed memory hierarchy simulator that simulates two levels of private TLBs, two levels of private data caches, a 3rd level shared data cache, and finally, the proposed 3rd level shared memory-based TLB. The simulator also models the size predictor and the cache bypass predictor, which are indexed using memory addresses. The simulator executes memory references from multiple traces while we schedule them at the proper issue cadence by using their instruction order in a manner similar to Ramulator. Information on the number of instructions in between the memory instructions are captured in the traces and thus memory level parallelism and overlap/lack of overlap between memory instructions are simulated. Note that our simulator is simulating both address translation traffic as well as data request traffic that go into underlying data caches. Finally, our simulator reports the L2 TLB miss cycles and detailed statistics such as hits and misses in the L1, L2 TLBs, data caches, the POM-TLB and predictor performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Performance Simulation of POM-TLB</head><p>The detailed cache and memory system parameters used in the simulation are listed in Table <ref type="table">1</ref>. DRAM simulation accounts for access latencies resulting from row-buffer hits and misses. It may also be noted that, since our baseline performance (obtained from real system measurements) already includes the benefits of hardware structures such as large pages, EPT and Page Structure Caches, we do not model these in our simulator and instead, use the baseline ideal cycles together with the estimated cost incurred by the POM-TLB when the DRAM-based TLB incurs a miss.</p><p>Total cycles taken by the POM-TLB and the resulting IPC for each core are obtained as:</p><formula xml:id="formula_3">C POM_T LB total = C ideal + M total * P POM_T LB Avg (4)</formula><p>IPC POM_T LB = I total C POM_T LB total (5) P POM_T LB Avg denotes the average L2 TLB miss cycles in POM-TLB obtained from simulation. Having obtained the baseline and POM-TLB IPCs for each core, we obtain the overall performance improvement of the POM-TLB. It may be observed that we use the linear additive formula to add the L2 TLB miss cycles to the ideal cycles. This linear performance model ignores potential overlap of TLB processing cycles with execution cycles but is similar to models used in previous research <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b47">48]</ref>. Such effects not only exist in POM-TLB but also in the baseline, so the performance impact of this exists equally in all schemes.</p><p>In addition, we compare POM-TLB against another prior work, which we annotate as Shared_L2 in the rest of this paper. We implemented this scheme similar to <ref type="bibr" target="#b8">[9]</ref> to the best of our knowledge. Shared_L2 combines private SRAM based L2 TLBs into a single shared TLB, so when a request misses in L1 TLB, the large SRAM based shared TLB is looked up.</p><p>Finally, we implemented Translation Storage Buffer (TSB) that exists in SPARC processors. Since TLB misses are handled by OS in SPARC processors, TSB is managed by OS although indexing and address calculation is done by dedicated hardware. Unlike x86 architecture, upon a TLB miss in the SPARC architecture, an OS trap is called and appropriate TSB lookup or a software page table walk is initiated. TSB can be considered as a large MMU cache implemented in a large software-allocated buffer. We compare POM-TLB against such a TSB design to show the benefits of POM-TLB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RESULTS</head><p>This section presents performance improvement for POM-TLB in comparison to other related schemes proposed in prior research for multicore systems <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> as well as a current existing system feature that is most closely similar to ours, SPARC's TSB. The results presented in this section are on top of a baseline with dedicated page structure caches as in the Intel Skylake processors. Here, the baseline is the execution time gathered from our experimental runs on SkyLake processors. Note that the improvement is shown in percentage (%) and 2 different comparables are presented in addition to POM-TLB.</p><p>The improvement ranges from 1% in streamcluster to 17% in soplex. We observe that workloads with high page walk overheads in virtualized platforms (see Table <ref type="table" target="#tab_5">2</ref>) have the highest improvement (such as mcf, soplex, GemsFDTD, astar and gups), which indicates that POM-TLB is effective in reducing costly page walks. The streamcluster benchmark does not contain significant page walk overhead to begin with (2.11%). Therefore, this benchmark does not possess a lot of headroom for improvement from POM-TLB. On average, POM-TLB is able to achieve a performance improvement of 9.57%. It may also be noted that these performance gains are obtained on top of the use of large pages. Even where a large fraction of pages are 2MB pages (for example, mcf has 70% and astar has 40% large pages in Table <ref type="table" target="#tab_5">2</ref>), the workloads exhibit considerable performance improvements.</p><p>Shared_L2 is able to achieve 6.10% performance improvement on average. This scheme does benefit from sharing of the combined L2 TLB capacities, yet it is still limited in terms of capturing the hot set of TLB entries. Thus, it encounters high shared L2 TLB miss penalty. On the other hand, POM-TLB is able to capture much more TLB entries. First, the physical capacity of our design is 16MB, which is a few orders of magnitude higher than TLBs in existing systems or L2 TLB capacity in Shared_L2 design. Upon a TLB miss in Shared_L2, the page walk is initiated when data caches are also searched, since intermediate PTEs are stored in data caches. However, in order to get a complete translation, many of these intermediate entries must be searched until the last level PTE is found. Only then, the translation is done. Even though the access is done in SRAM latency in this case, multiple accesses have to be made in order to complete the page walk. MMU caches, such as PSC, help to reduce the number of such intermediate entry accesses, yet their capacity is very limited, so it only caches a small amount of TLB misses. In addition, even though POM-TLB is located in die-stacked DRAM, which incurs an access latency similar to DRAM, we cache many of TLB entries in data caches. This enables us to achieve much lower access latency, as many of these entries are cached in data caches. Also, the use of L2D$ and L3D$ allow us to have a lot of TLB entries stored in caches. An additional advantage is that single virtual address only requires single entry in data caches whereas Shared_L2 has multiple intermediate PTEs stored in caches, which consumes much more capacity in data caches.</p><p>TSB achieves an average performance improvement of 4.27% across our workloads. This is surprising considering that it uses 16MB capacity as in POM-TLB. However, the performance of this scheme is limited as each TLB miss incurs a trap operation, which is required in the operation of TSB as it is software managed. Also, unlike POM-TLB which has an associativity of 4, TSB is a direct mapped organization, so it sees more conflict misses. POM-TLB uses the 64B cacheline size, so each cacheline has 4 TLB entries. Since the data transfer granularity between die-stacked DRAM and on-chip caches is done at 64B, we exploit this and allow POM-TLB to have an associativity of 4. Furthermore, TSB entries are not direct guest-VA to host-PA translations, and require multiple accesses to the TSB to complete the virtual-to-physical translation thereby incurring higher access latency. An interesting observation is made for the gups benchmark. This benchmark is known to have low locality in page tables, so an ability to achieve high performance for  such low spatial locality workloads can show how well each scheme retains translation entries. In case of TSB, it is not able to capture many of these entries even with 16MB as it only achieves 1.80% improvement. However, POM-TLB achieves performance improvement of 16%, approximately an order of difference in performance. Therefore, we can see that POM-TLB makes much better use of the 16MB space that is located in die-stacked DRAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Hit Ratio</head><p>The effectiveness of POM-TLB can be shown using the hit ratio.</p><p>Here, we analyze how well POM-TLB can capture L2 TLB misses, thereby reducing costly page walks. Figure <ref type="figure">9</ref> shows the hit ratio perceived at different level of the memory subsystem where TLB entries are stored by our scheme. First, the L2D$ has a very high hit rate of 89.7% on average. Since L2 data caches are private, it is not affected by interference from other cores. Since the L2 capacity is much larger than other private TLB structures in the processor, it keeps a lot of translation traffic from performing page walks. Note that caching of TLB entries is only feasible since we have a very large TLB and that is addressable. In conventional systems, the TLB entries are not visible as they are entirely managed by MMUs, yet our novel idea of making the large TLB addressable has enabled a larger number of TLB requests to be cached in rather large on-chip caches.</p><p>When a request misses in L2D$, then it is looked up in shared L3D$. The hit ratio here is not as good as L2D$. First, it is a shared data structure, so interference starts degrading performance. Also, a majority of TLB requests are filtered by L2D$, so only requests with a low degree of locality are passed down to the L3D$. However, POM-TLB in die-stacked DRAM again picks up a lot of these requests as shown by a higher hit ratio of 88% on average. POM-TLB can achieve this as the capacity is rather large, so it can recapture many translation requests that missed in a smaller L3D$. It may also be noted that the data caches are also caching the normal data accesses made by the cores and are not being used solely for TLB entries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Predictor Accuracy</head><p>In our scheme, we implemented two predictors, which are size and bypass predictors. The size predictor speculates whether the incoming translation address is going to be a request for a large or small pages. Although there are proposals <ref type="bibr" target="#b41">[42]</ref> that enable simultaneous accesses to TLB structures to check both small and large pages, we avoid doing this as it requires sophisticated design/verification efforts as well as consumes more power. We rather used a simple predictor, but as seen in Figure <ref type="figure" target="#fig_8">10</ref>, the size predictor is highly accurate as it achieves an average accuracy of 95%. The accuracy is calculated by dividing the total number of correct speculations by the total number of speculations. In such cases, we can reduce 95% of the second TLB accesses to look for the TLB entry of the other size. Yet, in comparison to performing a serialized access, our predictor can fetch the correct TLB entry in a single POM-TLB access.</p><p>Our implementation adds a miss penalty if translations miss in data caches as additional on-chip cache lookups are performed prior to accessing POM-TLB. The bypass predictor effectively eliminates such latency and forwards the request directly to POM-TLB upon L2 TLB miss. Our predictor is achieving a low accuracy of 45.8% on average. Although some workloads such as bwaves, lbm and libquantum are able to achieve close to perfect accuracy, others such as soplex and pagerank have a low hit rate. Although the data cache access latencies are an order of magnitude lower than page walk cycles, the misprediction penalty keeps POM-TLB from achieving the best performance. We leave it to the future work to perform other approaches such as using the instruction address to increase the accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Row Buffer Hits (RBH) in the L3 TLB</head><p>We quantify the intuition that the spatial locality of TLB accesses leads to a high Row Buffer Hits (RBH) in the stacked DRAM. Figure <ref type="figure" target="#fig_9">11</ref> plots the RBH values. As reported, the stacked DRAM achieves a high average RBH of 71%, thereby ensuring a low latency L3 TLB lookup. Each row contains 128 TLB entries, which is similar in capacity as an on-chip L2 TLB. Since they are located in the same row, these TLB accesses are likely to hit in the row buffer. As expected, applications with high spatial locality show high RBH values. For example, streamcluster has streaming behaviors, which thus has high spatial locality <ref type="bibr" target="#b35">[36]</ref>, explaining its high RBH value in Figure <ref type="figure" target="#fig_9">11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">POM-TLB without Data Caches</head><p>In this section, we quantify the performance benefits POM-TLB gets from storing TLB entries in data caches. Figure <ref type="figure" target="#fig_2">12</ref> shows the performance improvement when TLB entries are cached in data caches and when not cached. As shown, caching significantly helps </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">POM-TLB Discussion</head><p>We have experimented with different POM-TLB capacities. Although not presented as they do not provide meaningful data, we have found that varying POM-TLB capacity to either 8MB or 32MB changes our performance improvement less than 1%. We have found that it is extremely difficult to find workloads whose memory footprint exceed such large TLB sizes. In fact, 16MB capacity used in our default value is a scaled down version of die-stacked DRAM capacity to be a representative fraction of our workloads working set. In the market today, we see die-stacked DRAM capacity reaching several gigabytes <ref type="bibr" target="#b33">[34]</ref>. Therefore, if we scale up and use a fraction of the capacity of today's die-stacked DRAM, we expect that POM-TLB will be able to eliminate a significant amount of page walks for even emerging applications with vastly larger footprint than ones we see today. Note that the TLB reach of POM-TLB is orders of magnitude larger than today's on-chip TLBs even with only 16MB. Similarly, we have also varied the core count to 4 and 32 cores. Yet, the performance improvement stays approximately the same as again even with fewer cores or more cores, POM-TLB is so large that, most of the page walks are eliminated <ref type="foot" target="#foot_2">3</ref> . POM-TLB is the first of its kind that implemented a TSB-like structure in purely hardware. TSB acts like a partial translation cache for SPARC's expensive software page walkers. Although the fact that both use a very large capacity in DRAM is similar, our contribution is fundamentally different in that we implement it as hardware TLB, so that page walks are not even initiated. Once the page walk is initiated, the processor is stalled, whereas POM-TLB does not block any execution as almost all translation requests hit in POM-TLB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">TLB-Aware Caching</head><p>Since TLB entries are addressable and get cached in L2D$ and L3D$, it is possible to design cache allocation and replacement policies that can adaptively allocate appropriate cache capacity to hold TLB entries and normal data. In workloads where L2 TLB misses incur high penalty, the data caches could prioritize retaining POM-TLB entries, while in workloads with higher data misses as compared to L2 TLB misses, the caches could prioritize retaining data contents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Efficient Virtual Machine Switching</head><p>Even though modern TLBs can simultaneously hold translations for multiple virtual machines (identified by VM ID), the major bottleneck is the small size of the SRAM-based TLBs. Running multiple VMs is common in today's large scale virtual platforms such as Amazon EC2 <ref type="bibr" target="#b0">[1]</ref> and each VM contains a full OS, making the TLB reach again the problem. The large L3 TLB can alleviate this capacity issue by simultaneously retaining the address translations of multiple VMs. With the L3 TLB, when different VMs interfere with each other, the L3 TLB can provide a high hit rate that results in avoiding the vast majority of expensive page walks caused by L1, L2 TLB conflicts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Reduction in Design Complexity</head><p>Modern processors employ an array of complex hardware techniques to accelerate page walks, including sophisticated page walk caches <ref type="bibr" target="#b3">[4]</ref> and intra/inter-core TLB prefetchers <ref type="bibr" target="#b9">[10]</ref>. These techniques result in significant design complexity to ensure TLB consistency and integrity of page table contents. With the introduction of the L3 TLB that offers very high hit rates at moderately low latencies, these structures could be simplified or even eliminated, resulting in an overall simpler hardware implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>Caching and speculation techniques have been proposed to improve the two dimensional address translation overheads in virtualized platforms <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b37">38]</ref>. Caching schemes such as page walk cache <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b16">17]</ref> attempt to bypass the intermediate level walks. Speculation schemes <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b37">38]</ref> let the processor execution continue with speculated page table entries and invalidate speculated instructions upon detecting misspeculation. These schemes are motivated by the fact that conventional TLBs are likely to cause more page table walks <ref type="bibr" target="#b6">[7]</ref> for emerging big data workloads with large memory footprints. Therefore, they focus on reducing/hiding the overheads of page table walks. In our work, we address a more fundamental problem that very large TLBs can withstand increased address translation pressure from virtualization by offering a translation storage with high capacity and high bandwidth, thereby significantly reducing the number of walks. Some other schemes <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b28">29]</ref> attempt to reduce the levels of page table walks in either native or virtualized system. However, our scheme tackles the fundamental problem of current TLB's insufficient capacities. Thus, by increasing the capacity significantly, we are solving the inherent structural bottleneck in today's system. However, our scheme is orthogonal to aforementioned schemes, as we do not alter the existing or proposed page walk hardware structures. Thus, our scheme can easily be augmented to these schemes. Moreover, TLB prefetching <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b40">41]</ref> improves the TLB hit rate by fetching entries ahead of time. These TLB prefetchers are also orthogonal to our scheme. Our POM-TLB augmented with a prefetcher can reduce the prefetching latency and achieve considerable performance improvement.</p><p>The Linux Transparent Huge Page <ref type="bibr" target="#b2">[3]</ref> along with various schemes <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b48">49]</ref> try to increase the fraction of large pages either in hardware or by OS to reduce the number of TLB misses. Although we do not take advantage of such schemes, but rather use large pages created by existing OS, using their scheme can even further increase the data cache and POM-TLB hit rates as a single 2MB entry incorporates 512 4KB entries, thereby further increasing the already large the reach of POM-TLB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>In this work, we evaluated the feasibility of building very large level 3 TLBs in DRAMs. We have presented a TLB which is part of the memory space, thereby allowing the possibility of caching TLB entries in conventional L2 and L3 data caches. Our thorough analysis using a combination of measurements on state of the art Skylake processors, detailed simulation and an additive performance model show that the proposed POM-TLB can practically eliminate the page walk overhead in most memory intensive workloads, particularly multi-threaded workloads and virtualized environments. In addition, we have shown that the proposed POM-TLB provides higher performance improvement than previously proposed shared TLB or prefetching techniques at the L1/L2 TLB level. Simulation studies using SPEC, PARSEC and graph workloads demonstrate that more than 16% performance improvement can be obtained in a third of the experimented benchmarks (with an average of 10% over all benchmarks). In most configurations 99% of the page walks can be eliminated by a very large TLB of size 16 MB.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: x86 2D Page Walk In Virtualized Environment</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Average Translation Cycles per L2 TLB Miss (Virtualized Platform)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Ratio of Virtualized to Native Translation Costs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1 Figure 5 :</head><label>15</label><figDesc>Figure 5: Metadata Format for POM-TLB</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: POM-TLB Overview</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: POM-TLB Access Flow Chart</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Performance Improvement of POM-TLB (8 Core)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Predictor Accuracy (8 core)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Row Buffer Hits in L3 TLB (8 core)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Entry 1 Entry 2 Entry 3 ??? 16 bytes (single entry)</head><label></label><figDesc></figDesc><table><row><cell></cell><cell cols="2">Bank 0</cell><cell>Die-Stacked DRAM</cell></row><row><cell>valid</cell><cell>VM ID</cell><cell>Process ID</cell><cell>VPN PPN Attr</cell></row></table><note><p>? We present a mechanism that makes it possible to cache TLB entries (not page table entries) into data caches. To the best of our knowledge, no prior work proposed caching of TLB entries into general (non-dedicated) caching structures.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>accesses. This significantly reduces</figDesc><table><row><cell>Processor</cell><cell>Values</cell></row><row><cell>Frequency</cell><cell>4 GHz</cell></row><row><cell>L1 I-Cache</cell><cell>32KB, 8 way, 4 cycles</cell></row><row><cell>L1 D-Cache</cell><cell>32KB, 8 way, 4 cycles</cell></row><row><cell>L2 Unified Cache</cell><cell>256KB, 4 way, 12 cycles</cell></row><row><cell>L3 Unified Cache</cell><cell>8MB, 16 way, 42 cycles</cell></row><row><cell>MMU</cell><cell>Values</cell></row><row><cell>L1 TLB (4KB)</cell><cell>64 entries</cell></row><row><cell></cell><cell>9 cycle miss penalty</cell></row><row><cell>L1 TLB (2MB)</cell><cell>32 entries</cell></row><row><cell></cell><cell>9 cycle miss penalty</cell></row><row><cell></cell><cell>L1 TLBs 4 way associative</cell></row><row><cell>L2 Unified TLB</cell><cell>1536 entries</cell></row><row><cell></cell><cell>17 cycle miss penalty</cell></row><row><cell></cell><cell>L2 TLBs 12 way associative</cell></row><row><cell>PSC</cell><cell>Values</cell></row><row><cell>PML4</cell><cell>2 entries, 2 cycle</cell></row><row><cell>PDP</cell><cell>4 entries, 2 cycle</cell></row><row><cell>PDE</cell><cell>32 entries, 2 cycle</cell></row><row><cell>Die-Stacked DRAM</cell><cell>Values</cell></row><row><cell>Bus Frequency</cell><cell>1 GHz (DDR 2 GHz)</cell></row><row><cell>Bus Width</cell><cell>128 bits</cell></row><row><cell>Row Buffer Size</cell><cell>2KB</cell></row><row><cell>tCAS-tRCD-tRP</cell><cell>11-11-11</cell></row><row><cell>DDR</cell><cell>Values</cell></row><row><cell>Type</cell><cell>DDR4-2133</cell></row><row><cell>Bus Frequency</cell><cell>1066 MHz</cell></row><row><cell></cell><cell>(DDR 2133 MHz)</cell></row><row><cell>Bus Width</cell><cell>64 bits</cell></row><row><cell>Row Buffer Size</cell><cell>2KB</cell></row><row><cell>tCAS-tRCD-tRP</cell><cell>14-14-14</cell></row><row><cell>Table</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>billion instructions. Benchmark Characteristics Related to TLB misses</figDesc><table><row><cell></cell><cell>astar</cell><cell>bwaves</cell><cell cols="2">canneal ccomponent</cell><cell>gcc</cell></row><row><cell>Overhead Native (%)</cell><cell>13.89</cell><cell>0.73</cell><cell>3.19</cell><cell>0.73</cell><cell>0.30</cell></row><row><cell>Overhead Virtual (%)</cell><cell>16.08</cell><cell>7.70</cell><cell>6.34</cell><cell>7.40</cell><cell>12.12</cell></row><row><cell>Average Cycles-per-L2TLB-miss Native</cell><cell>98</cell><cell>128</cell><cell>53</cell><cell>44</cell><cell>46</cell></row><row><cell>Average Cycles-per-L2TLB-miss Virtual</cell><cell>114</cell><cell>151</cell><cell>61</cell><cell>1158</cell><cell>88</cell></row><row><cell>Frac Large Pages (%)</cell><cell>41.7</cell><cell>0.8</cell><cell>16.0</cell><cell>50.0</cell><cell>29.0</cell></row><row><cell></cell><cell cols="2">GemsFDTD graph500</cell><cell>gups</cell><cell>lbm</cell><cell>libquantum</cell></row><row><cell>Overhead Native (%)</cell><cell>10.58</cell><cell>1.03</cell><cell>12.20</cell><cell>0.05</cell><cell>0.02</cell></row><row><cell>Overhead Virtual (%)</cell><cell>16.01</cell><cell>7.66</cell><cell>17.20</cell><cell>12.02</cell><cell>7.37</cell></row><row><cell>Average Cycles-per-L2TLB-miss Native</cell><cell>129</cell><cell>79</cell><cell>43</cell><cell>110</cell><cell>70</cell></row><row><cell>Average Cycles-per-L2TLB-miss Virtual</cell><cell>133</cell><cell>80</cell><cell>70</cell><cell>290</cell><cell>75</cell></row><row><cell>Frac Large Pages (%)</cell><cell>71.0</cell><cell>7.0</cell><cell>2.59</cell><cell>57.4</cell><cell>32.9</cell></row><row><cell></cell><cell>mcf</cell><cell cols="3">pagerank soplex streamcluster</cell><cell>zeusmp</cell></row><row><cell>Overhead Native (%)</cell><cell>10.32</cell><cell>4.07</cell><cell>4.16</cell><cell>0.07</cell><cell>0.01</cell></row><row><cell>Overhead Virtual (%)</cell><cell>19.01</cell><cell>6.96</cell><cell>17.07</cell><cell>2.11</cell><cell>10.22</cell></row><row><cell>Average Cycles-per-L2TLB-miss Native</cell><cell>66</cell><cell>51</cell><cell>144</cell><cell>74</cell><cell>136</cell></row><row><cell>Average Cycles-per-L2TLB-miss Virtual</cell><cell>169</cell><cell>61</cell><cell>145</cell><cell>76</cell><cell>137</cell></row><row><cell>Frac Large Pages (%)</cell><cell>60.7</cell><cell>60.0</cell><cell>12.3</cell><cell>87.2</cell><cell>72.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>The performance aspect that caching helps is not in reducing the number of page walks. Whether data caches are used or not does not affect the number of page walks as this reduction is performed by the large capacity of POM-TLB. Instead, what caching enables is hiding the long latency of die-stacked DRAM accesses, bridging the latency gap between on-chip TLBs and die-stacked DRAM-based POM-TLB.</figDesc><table><row><cell>Performance Improvement (%)</cell><cell>0 2 4 6 8 10 12 14 16 18 20</cell><cell>astar</cell><cell>bwaves</cell><cell>canneal</cell><cell>ccomponent</cell><cell>gcc</cell><cell>GemsFDTD</cell><cell>graph500</cell><cell>gups</cell><cell>lbm</cell><cell>libquantum</cell><cell>mcf</cell><cell>pagerank</cell><cell>soplex</cell><cell>streamcluster</cell><cell>zeusmp</cell><cell>geomean</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">With Data Caching</cell><cell></cell><cell cols="7">Without Data Caching</cell><cell></cell></row><row><cell cols="18">Figure 12: POM-TLB With and Without Data Caching (8 core)</cell></row><row><cell cols="18">performance as it provides an additional performance improvement</cell></row><row><cell cols="2">of 5%.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Unified designs with more complex addressing schemes such as skew-associativity<ref type="bibr" target="#b41">[42]</ref> could be explored; we leave this for future work.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>One could improve accuracy by adding hysteresis via a multi-bit saturating predictor or by using a larger predictor table.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Per-core L2D$ continue to provide the bulk of the latency improvements in all core counts.5 UNLOCKING ADDITIONAL BENEFITSIn this section, we present additional benefits unlocked by our addressable POM-TLB and briefly discuss those benefits.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8">ACKNOWLEDGEMENT</head><p>This work is supported in part by the <rs type="funder">National Science Foundation</rs> under Grant <rs type="grantNumber">1337393</rs> and the <rs type="institution">Texas Advanced Computing Center (TACC)</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_xbW2ReR">
					<idno type="grant-number">1337393</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Amazon EC2 -Virtual Server Hosting</title>
		<author>
			<persName><surname>Amazon</surname></persName>
		</author>
		<ptr target="https://aws.amazon.com/ec2/" />
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Arcangeli</surname></persName>
		</author>
		<ptr target="http://www.linux-kvm.org/page/KVM_Forum_2008" />
		<title level="m">Linux KVM Forum</title>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Transparent hugepage support</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Arcangeli</surname></persName>
		</author>
		<ptr target="http://www.linux-kvm.org/page/KVM_Forum_2010" />
	</analytic>
	<monogr>
		<title level="m">KVM Forum</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Translation Caching: Skip, Don&apos;t Walk (the Page Table)</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">W</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Rixner</surname></persName>
		</author>
		<idno type="DOI">10.1145/1815961.1815970</idno>
		<ptr target="https://doi.org/10.1145/1815961.1815970" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th Annual International Symposium on Computer Architecture (ISCA &apos;10)</title>
		<meeting>the 37th Annual International Symposium on Computer Architecture (ISCA &apos;10)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="48" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SpecTLB: A Mechanism for Speculative Address Translation</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">W</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Rixner</surname></persName>
		</author>
		<idno type="DOI">10.1145/2000064.2000101</idno>
		<ptr target="https://doi.org/10.1145/2000064.2000101" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th Annual International Symposium on Computer Architecture (ISCA &apos;11)</title>
		<meeting>the 38th Annual International Symposium on Computer Architecture (ISCA &apos;11)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="307" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Efficient Virtual Memory for Big Memory Servers</title>
		<author>
			<persName><forename type="first">Arkaprava</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayneel</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jichuan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Swift</surname></persName>
		</author>
		<idno type="DOI">10.1145/2485922.2485943</idno>
		<ptr target="https://doi.org/10.1145/2485922.2485943" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual International Symposium on Computer Architecture (ISCA &apos;13)</title>
		<meeting>the 40th Annual International Symposium on Computer Architecture (ISCA &apos;13)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="237" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Accelerating Two-dimensional Page Walks for Virtualized Systems</title>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Serebrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Spadini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srilatha</forename><surname>Manne</surname></persName>
		</author>
		<idno type="DOI">10.1145/1346281.1346286</idno>
		<ptr target="https://doi.org/10.1145/1346281.1346286" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS XIII)</title>
		<meeting>the 13th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS XIII)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="26" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Large-reach memory management unit caches: Coalesced and shared memory management unit caches to accelerate TLB miss handling</title>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Bhattacharjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 46th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<publisher>MICRO</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="383" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Shared last-level TLBs for chip multiprocessors</title>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Lustig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Martonosi</surname></persName>
		</author>
		<idno type="DOI">10.1109/HPCA.2011.5749717</idno>
		<ptr target="https://doi.org/10.1109/HPCA.2011.5749717" />
	</analytic>
	<monogr>
		<title level="m">2011 IEEE 17th International Symposium on High Performance Computer Architecture</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="62" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Inter-core Cooperative TLB for Chip Multiprocessors</title>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Martonosi</surname></persName>
		</author>
		<idno type="DOI">10.1145/1736020.1736060</idno>
		<ptr target="https://doi.org/10.1145/1736020.1736060" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Edition of ASPLOS on Architectural Support for Programming Languages and Operating Systems (ASPLOS XV)</title>
		<meeting>the Fifteenth Edition of ASPLOS on Architectural Support for Programming Languages and Operating Systems (ASPLOS XV)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="359" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Die Stacking (3D) Microarchitecture</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murali</forename><surname>Annavaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ned</forename><surname>Brekelbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Devale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Don</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pat</forename><surname>Mccaule</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><forename type="middle">W</forename><surname>Morrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Pantuso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadasivan</forename><surname>Rupley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clair</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><surname>Webb</surname></persName>
		</author>
		<idno type="DOI">10.1109/MICRO.2006.18</idno>
		<ptr target="https://doi.org/10.1109/MICRO.2006.18" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 39th Annual IEEE/ACM International Symposium on Microarchitecture<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="469" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Methodology for performance analysis of VMware vSphere under Tier-1 applications</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Buell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalyan</forename><surname>Saladi</surname></persName>
		</author>
		<author>
			<persName><surname>Taheri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VMware Technical Journal</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">CAMEO: A Two-Level Memory Organization with Capacity of Main Memory and Flexibility of Hardware-Managed Cache</title>
		<author>
			<persName><forename type="first">Chiachen</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aamer</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moinuddin</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
		<idno type="DOI">10.1109/MICRO.2014.63</idno>
		<ptr target="https://doi.org/10.1109/MICRO.2014.63" />
	</analytic>
	<monogr>
		<title level="m">2014 47th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">BEAR: Techniques for mitigating bandwidth bloat in gigascale DRAM caches</title>
		<author>
			<persName><forename type="first">Chiachen</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aamer</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moinuddin</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
		<idno type="DOI">10.1145/2749469.2750387</idno>
		<ptr target="https://doi.org/10.1145/2749469.2750387" />
	</analytic>
	<monogr>
		<title level="m">2015 ACM/IEEE 42nd Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="198" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Simple but Effective Heterogeneous Main Memory with On-Chip Memory Controller Support</title>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Muralimanohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norman</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<idno type="DOI">10.1109/SC.2010.50</idno>
		<ptr target="https://doi.org/10.1109/SC.2010.50" />
	</analytic>
	<monogr>
		<title level="m">2010 ACM/IEEE International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient Memory Virtualization: Reducing Dimensionality of Nested Page Walks</title>
		<author>
			<persName><forename type="first">Jayneel</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arkaprava</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Swift</surname></persName>
		</author>
		<idno type="DOI">10.1109/MICRO.2014.37</idno>
		<ptr target="https://doi.org/10.1109/MICRO.2014.37" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 47th Annual IEEE/ACM International Symposium on Microarchitecture<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="178" to="189" />
		</imprint>
	</monogr>
	<note>-47)</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Agile Paging: Exceeding the Best of Nested and Shadow Paging</title>
		<author>
			<persName><forename type="first">Jayneel</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Swift</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISCA.2016.67</idno>
		<ptr target="https://doi.org/10.1109/ISCA.2016.67" />
	</analytic>
	<monogr>
		<title level="m">2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="707" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Mahesh</forename><surname>Nagendra Dwarakanath Gulur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mehendale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manikantan</surname></persName>
		</author>
		<author>
			<persName><surname>Govindarajan</surname></persName>
		</author>
		<ptr target="http://dblp.uni-trier.de/db/conf/micro/micro2014.html#GulurMMG14" />
		<title level="m">Proceedings of the 47th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO-47)</title>
		<meeting>the 47th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO-47)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="38" to="50" />
		</imprint>
	</monogr>
	<note>Bi-Modal DRAM Cache: Improving Hit Rate, Hit Latency and Bandwidth</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Toward Dark Silicon in Servers</title>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Hardavellas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Ailamaki</surname></persName>
		</author>
		<idno type="DOI">10.1109/MM.2011.77</idno>
		<ptr target="https://doi.org/10.1109/MM.2011.77" />
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="6" to="15" />
			<date type="published" when="2011-07">2011. July 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The microarchitecture of the Pentium? 4 processor</title>
		<author>
			<persName><forename type="first">Glenn</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dave</forename><surname>Sager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Upton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intel Technology Journal</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>Darrell Boggs, and others</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">ATCache: Reducing DRAM Cache Latency via a Small SRAM Tag Cache</title>
		<author>
			<persName><forename type="first">Cheng-Chieh</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Nagarajan</surname></persName>
		</author>
		<idno type="DOI">10.1145/2628071.2628089</idno>
		<ptr target="https://doi.org/10.1145/2628071.2628089" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Parallel Architectures and Compilation (PACT &apos;14)</title>
		<meeting>the 23rd International Conference on Parallel Architectures and Compilation (PACT &apos;14)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="51" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Intel(R) Virtualization Technology</title>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<ptr target="http://www.intel.com/content/www/us/en/virtualization/virtualization-technology/intel-virtualization-technology.html" />
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">6th Generation Intel Core i7-6700K and i5-6600K Processors</title>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<ptr target="http://www.intel.com/content/www/us/en/processors/core/6th-gen-core-family-desktop-brief.html" />
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><surname>Jedec</surname></persName>
		</author>
		<ptr target="https://www.jedec.org" />
		<title level="m">High Bandwidth Memory (HBM) DRAM Gen 2 (JESD235A)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unison Cache: A Scalable and Effective Die-Stacked DRAM Cache</title>
		<author>
			<persName><forename type="first">Djordje</forename><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cansu</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><surname>Falsafi</surname></persName>
		</author>
		<idno type="DOI">10.1109/MICRO.2014.51</idno>
		<ptr target="https://doi.org/10.1109/MICRO.2014.51" />
	</analytic>
	<monogr>
		<title level="m">47th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting><address><addrLine>MICRO; Cambridge, United Kingdom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-13">2014. 2014. December 13-17, 2014</date>
			<biblScope unit="page" from="25" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Die-stacked DRAM Caches for Servers: Hit Ratio, Latency, or Bandwidth? Have It All with Footprint Cache</title>
		<author>
			<persName><forename type="first">Djordje</forename><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stavros</forename><surname>Volos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
		</author>
		<idno type="DOI">10.1145/2485922.2485957</idno>
		<ptr target="https://doi.org/10.1145/2485922.2485957" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual International Symposium on Computer Architecture (ISCA &apos;13)</title>
		<meeting>the 40th Annual International Symposium on Computer Architecture (ISCA &apos;13)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="404" to="415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">CHOP: Integrating DRAM Caches for CMP Server Platforms</title>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niti</forename><surname>Madan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Upton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srihari</forename><surname>Makineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Don</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Solihin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajeev</forename><surname>Balasubramonian</surname></persName>
		</author>
		<idno type="DOI">10.1109/MM.2010.100</idno>
		<ptr target="https://doi.org/10.1109/MM.2010.100" />
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="99" to="108" />
			<date type="published" when="2011-01">2011. Jan 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Going the distance for TLB prefetching: an application-driven study</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gokul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anand</forename><surname>Kandiraju</surname></persName>
		</author>
		<author>
			<persName><surname>Sivasubramaniam</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISCA.2002.1003578</idno>
		<ptr target="https://doi.org/10.1109/ISCA.2002.1003578" />
	</analytic>
	<monogr>
		<title level="m">Proceedings 29th Annual International Symposium on Computer Architecture</title>
		<meeting>29th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="195" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Energy-efficient address translation</title>
		<author>
			<persName><forename type="first">Vasileios</forename><surname>Karakostas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayneel</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Cristal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><forename type="middle">S</forename><surname>Mckinley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Nemirovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Swift</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Osman</forename><forename type="middle">S</forename><surname>Unsal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="631" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Ramulator: A Fast and Extensible DRAM Simulator</title>
		<author>
			<persName><forename type="first">Yoongu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weikun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
		<idno type="DOI">10.1109/LCA.2015.2414456</idno>
		<ptr target="https://doi.org/10.1109/LCA.2015.2414456" />
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Archit. Lett</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="45" to="49" />
			<date type="published" when="2016-01">2016. Jan. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficiently Enabling Conventional Block Sizes for Very Large Die-stacked DRAM Caches</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><surname>Hill</surname></persName>
		</author>
		<idno type="DOI">10.1145/2155620.2155673</idno>
		<ptr target="https://doi.org/10.1145/2155620.2155673" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 44th Annual IEEE/ACM International Symposium on Microarchitecture<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="454" to="464" />
		</imprint>
	</monogr>
	<note type="report_type">MICRO-44</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Challenges in Heterogeneous Die-Stacked and Off-Chip Memory Systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nuwan</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Jayasena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike O'</forename><surname>Mcgrath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoong</forename><surname>Reinhardt</surname></persName>
		</author>
		<author>
			<persName><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 3rd Workshop on SoCs, Heterogeneous Architectures and Workloads</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Heterogeneous memory architectures: A HW/SW approach for mixing die-stacked and off-package memories</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mitesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serigey</forename><surname>Meswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Blagodurov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Slice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><forename type="middle">H</forename><surname>Ignatowski</surname></persName>
		</author>
		<author>
			<persName><surname>Loh</surname></persName>
		</author>
		<idno type="DOI">10.1109/HPCA.2015.7056027</idno>
		<ptr target="https://doi.org/10.1109/HPCA.2015.7056027" />
	</analytic>
	<monogr>
		<title level="m">2015 IEEE 21st International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="126" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Micron&apos;s Hybrid Memory Cube Earns High Praise in Next-Generation Supercomputer</title>
		<ptr target="http://investors.micron.com/releasedetail.cfm?ReleaseID=805283" />
	</analytic>
	<monogr>
		<title level="j">Micron</title>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A Software-Managed Approach to Die-Stacked DRAM</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Oskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><surname>Loh</surname></persName>
		</author>
		<idno type="DOI">10.1109/PACT.2015.30</idno>
		<ptr target="https://doi.org/10.1109/PACT.2015.30" />
	</analytic>
	<monogr>
		<title level="m">2015 International Conference on Parallel Architecture and Compilation</title>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-10-18">2015. 2015. October 18-21, 2015</date>
			<biblScope unit="page" from="188" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Analyzing and Quantifying Dynamc Program Behavior in Terms of Regularities and Patterns</title>
		<author>
			<persName><forename type="first">Celal</forename><surname>Ozturk</surname></persName>
		</author>
		<ptr target="http://digitalcommons.uri.edu/oa_diss/63" />
	</analytic>
	<monogr>
		<title level="m">Open Access Dissertations. 63</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Increasing TLB reach by exploiting clustering in page translations</title>
		<author>
			<persName><forename type="first">Binh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasuko</forename><surname>Eckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
		<idno type="DOI">10.1109/HPCA.2014.6835964</idno>
		<ptr target="https://doi.org/10.1109/HPCA.2014.6835964" />
	</analytic>
	<monogr>
		<title level="m">2014 IEEE 20th International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="558" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Large Pages and Lightweight Memory Management in Virtualized Environments: Can You Have It Both Ways?</title>
		<author>
			<persName><forename type="first">Binh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J?n</forename><surname>Vesel?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><surname>Bhattacharjee</surname></persName>
		</author>
		<idno type="DOI">10.1145/2830772.2830773</idno>
		<ptr target="https://doi.org/10.1145/2830772.2830773" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th International Symposium on Microarchitecture (MICRO-48)</title>
		<meeting>the 48th International Symposium on Microarchitecture (MICRO-48)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fundamental Latency Trade-off in Architecting DRAM Caches: Outperforming Impractical SRAM-Tags with a Simple and Practical Design.. In MICRO</title>
		<author>
			<persName><forename type="first">K</forename><surname>Moinuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><forename type="middle">H</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><surname>Loh</surname></persName>
		</author>
		<ptr target="http://dblp.uni-trier.de/db/conf/micro/micro2012.html#QureshiL12" />
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Society</title>
		<imprint>
			<biblScope unit="page" from="235" to="246" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">OPENSTACK -The Open Alternative To Cloud Lock-In</title>
		<author>
			<persName><surname>Rackspace</surname></persName>
		</author>
		<ptr target="https://www.rackspace.com/en-us/cloud/openstack" />
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Recency-based TLB Preloading</title>
		<author>
			<persName><forename type="first">Ashley</forename><surname>Saulsbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fredrik</forename><surname>Dahlgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Per</forename><surname>Stenstr?m</surname></persName>
		</author>
		<idno type="DOI">10.1145/339647.339666</idno>
		<ptr target="https://doi.org/10.1145/339647.339666" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Annual International Symposium on Computer Architecture (ISCA &apos;00)</title>
		<meeting>the 27th Annual International Symposium on Computer Architecture (ISCA &apos;00)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="117" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Concurrent Support of Multiple Page Sizes on a Skewed Associative TLB</title>
		<author>
			<persName><forename type="first">Andr?</forename><surname>Seznec</surname></persName>
		</author>
		<idno type="DOI">10.1109/TC.2004.21</idno>
		<ptr target="https://doi.org/10.1109/TC.2004.21" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Computers</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="924" to="927" />
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Transparent Hardware Management of Stacked DRAM as Part of Memory</title>
		<author>
			<persName><forename type="first">Jaewoong</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alaa</forename><forename type="middle">R</forename><surname>Alameldeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeshan</forename><surname>Chishti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyesoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1109/MICRO.2014.56</idno>
		<ptr target="https://doi.org/10.1109/MICRO.2014.56" />
	</analytic>
	<monogr>
		<title level="m">2014 47th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A Mostly-Clean DRAM Cache for Effective Hit Speculation and Self-Balancing Dispatch</title>
		<author>
			<persName><forename type="first">Jaewoong</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyesoon</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mithuna</forename><surname>Oconnor</surname></persName>
		</author>
		<author>
			<persName><surname>Thottethodi</surname></persName>
		</author>
		<idno type="DOI">10.1109/MICRO.2012.31</idno>
		<ptr target="https://doi.org/10.1109/MICRO.2012.31" />
	</analytic>
	<monogr>
		<title level="m">2012 45th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="247" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Synergistic TLBs for High Performance Address Translation in Chip Multiprocessors</title>
		<author>
			<persName><forename type="first">Shekhar</forename><surname>Srikantaiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahmut</forename><surname>Kandemir</surname></persName>
		</author>
		<idno type="DOI">10.1109/MICRO.2010.26</idno>
		<ptr target="https://doi.org/10.1109/MICRO.2010.26" />
	</analytic>
	<monogr>
		<title level="m">2010 43rd Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="313" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Tradeoffs in Supporting Two Page Sizes</title>
		<author>
			<persName><forename type="first">Madhusudhan</forename><surname>Talluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shing</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISCA.1992.753337</idno>
		<ptr target="https://doi.org/10.1109/ISCA.1992.753337" />
	</analytic>
	<monogr>
		<title level="m">Proceedings the 19th Annual International Symposium on Computer Architecture</title>
		<meeting>the 19th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1992">1992. 1992</date>
			<biblScope unit="page" from="415" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">CACTI: an enhanced cache access and cycle time model</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norman</forename><forename type="middle">P</forename><surname>Wilton</surname></persName>
		</author>
		<author>
			<persName><surname>Jouppi</surname></persName>
		</author>
		<idno type="DOI">10.1109/4.509850</idno>
		<ptr target="https://doi.org/10.1109/4.509850" />
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="677" to="688" />
			<date type="published" when="1996-05">1996. May 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Hash, Don&apos;T Cache (the Page Table)</title>
		<author>
			<persName><forename type="first">Idan</forename><surname>Yaniv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Tsafrir</surname></persName>
		</author>
		<idno type="DOI">10.1145/2896377.2901456</idno>
		<ptr target="https://doi.org/10.1145/2896377.2901456" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Science</title>
		<meeting>the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Science<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="337" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Enigma: Architectural and Operating System Support for Reducing the Impact of Address Translation</title>
		<author>
			<persName><forename type="first">Lixin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Speight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ram</forename><surname>Rajamony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1145/1810085.1810109</idno>
		<ptr target="https://doi.org/10.1145/1810085.1810109" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International Conference on Supercomputing (ICS &apos;10)</title>
		<meeting>the 24th ACM International Conference on Supercomputing (ICS &apos;10)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="159" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Exploring DRAM cache architectures for CMP server platforms</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Illikkal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Don</forename><surname>Newell</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCD.2007.4601880</idno>
		<ptr target="https://doi.org/10.1109/ICCD.2007.4601880" />
	</analytic>
	<monogr>
		<title level="m">2007 25th International Conference on Computer Design</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="55" to="62" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
