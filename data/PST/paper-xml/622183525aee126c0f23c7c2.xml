<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Context Enhanced Short Text Matching using Clickthrough Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mao</forename><forename type="middle">Yan</forename><surname>Chen</surname></persName>
							<email>chenmaoy19@mails.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Tsinghua Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haiyun</forename><surname>Jiang</surname></persName>
							<email>haiyunjiang@tencent.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yujiu</forename><surname>Yang</surname></persName>
							<email>yang.yujiu@sz.tsinghua.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Context Enhanced Short Text Matching using Clickthrough Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The short text matching task employs a model to determine whether two short texts have the same semantic meaning or intent. Existing short text matching models usually rely on the content of short texts which are lack information or missing some key clues. Therefore, the short texts need external knowledge to complete their semantic meaning. To address this issue, we propose a new short text matching framework for introducing external knowledge to enhance the short text contextual representation. In detail, we apply a self-attention mechanism to enrich short text representation with external contexts. Experiments on two Chinese datasets and one English dataset demonstrate that our framework outperforms the state-of-the-art short text matching models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Short text matching is an essential task that has been applied in question answering <ref type="bibr" target="#b0">(Berger et al., 2000)</ref>, paraphrase identification <ref type="bibr" target="#b13">(Socher et al., 2011)</ref> and information retrieval <ref type="bibr" target="#b7">(Huang et al., 2013)</ref>. In recent years, deep neural networks achieve surprising performance in this field. We can roughly classify deep text matching models into two types: 1) representation-based text matching <ref type="bibr" target="#b7">(Huang et al., 2013;</ref><ref type="bibr" target="#b12">Shen et al., 2014)</ref> and 2) interaction-based text matching <ref type="bibr" target="#b17">(Wang and Jiang, 2015;</ref><ref type="bibr" target="#b18">Wang et al., 2017;</ref><ref type="bibr" target="#b6">Devlin et al., 2018)</ref>. The interactive-based framework is usually performs better than the representation-based framework. Though interactive-based framework achieved very promising results, their performance still suffers from the lack of enough contextual information since words or expressions in a short text usually have ambiguous meanings.</p><p>Especially in Chinese scenarios, both characterlevel and word-level tokenization introduce serious  semantic information errors or missing. Recent studies show that encoding multi-granularity information <ref type="bibr" target="#b8">(Lai et al., 2019;</ref><ref type="bibr" target="#b3">Chen et al., 2020)</ref> and word sense information <ref type="bibr" target="#b10">(Lyu et al., 2021)</ref> into sentences can mitigate this problem. They further improve the performance, while this word-level information reinforces is still helpless in many cases that need relevant sentence-level contextual information supplement.</p><p>As seen in Figure <ref type="figure" target="#fig_0">1</ref>, sentences 1 and 2 refer to the same question but the word-level semantic information is not enough to connect them. Therefore, we take the original sentences as queries to search related contexts by search engines. The retrieved contexts usually contain enough contextual information to relate the two original short texts. In this case, both short texts refer to "interest of bank loan", where the matching model could easily classify them to be matched.</p><p>From this insight, we propose a context-aware BERT matching model (CBM) for short text matching, which enrich the semantic representation of a short text by external semantic-related sentences, arXiv:2203.01849v1 [cs.CL] 3 Mar 2022 instead of word-levle knowledge. As seen in Figure <ref type="figure" target="#fig_0">1</ref>, both sentences have multiple related contextual sentences 1 . CBM selects the needed contexts and updates the short text representation according to the context-enhanced attention mechanism. Our experiments on two Chinese datasets and one English dataset show that our model achieves new state-of-the-art performance. Our contributions are three folds:</p><p>? We are the first to propose a framework that enhances short text representation by external sentence-level knowledge.</p><p>? We crawled a huge amount of contextual sentences for the three commonly used benchmark datasets, which benefits future research.</p><p>? We design a simple but efficient model to utilize the sentences for short text representation reinforcement. Experiments show that our model achieves new SoTA performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Framework</head><p>Given two sentences, S a = {s 1 a , s 2 a , ..., s i a , ..., s n a } and S b = {s 1 b , s 2 b , ..., s j b , ..., s m b }, we aim to decide whether two sentences have the same semantic meaning. s i a and s j b denotes the i-th and j-th token in sentence a and b, respectively. Different from existing methods, we not only use the sentences in datasets, but also utilize the external sentences crawled from search engines to enhance the context. Each sentence S i has a set of contexts:</p><formula xml:id="formula_0">C i = {c 1 i , c 2 i , ..., c j i , ..., c n i }</formula><p>, where c j i represents the j-th context for sentence S i .</p><p>Our framework has three modules: 1) Contexts Crawler, 2) Context Selector, and 3) Contextenhanced Text Matcher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Context Crawler</head><p>For each sentence S i , we obtain the set of contexts C i corresponding to S i by crawling the search engine results. The retrieved contexts C i are noisy and dirty, so we first remove the noise by preprocessing and perform a regular cleaning. Also, all contents related to personal information is removed. Finally, we will have a clean context set C i for each sentence S i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Context Selector</head><p>First, we use BERT baseline model to perform semantic similarity task for each pair of sentence and 1 We denote a contextual sentence as a context for short.</p><p>context, S a with c j b or S b with c j a . Aftrer that, each pair of a sentence and a context has a similarity score of d j i , higher means higher semantic similarity, lower means lower semantic similarity. For instance, d j a is the similarity score for the pair of S a and c j b . For all positive samples (S a and S b are semantically matched), we use the hyperparameter d a to classify the context and sentence pairs into similar or dissimilar. All d j i &gt; d a is similar and others are dissimilar. Otherwise, for all negative samples (S j a and S j b are not semantically matched), we use the hyperparameter d b to classify the context and sentence pairs into similar and dissimilar, with all d j i &gt; d b being similar and the rest being dissimilar.</p><p>For all positive samples, S + a and S + b , we want the context of S + a to have similar semantic information as S + b . Also, we want the context of S + b to have similar semantic information as S + a . On the contrary, for all negative samples, S - a and S - b , we expect the contexts of S - a to be semantically dissimilar to S - b . It is also expected that the contexts of S - b are not semantically similar to S - a . However, we do not have ground truth labels for test set to make the context selection by labels. Therefore, we construct a context selector to determine whether we want to use the context based on the semantic information of the two sentences and the context.</p><p>When we construct contexts for the above positive and negative samples, using similar semantic contexts for positive samples and dissimilar semantic contexts for negative samples. We first put this data as pseudo labels into a BERT classifier for training. The inputs are S a , S b , c j i , where i ? [a, b], and the model output will be [0, 1], indicating whether this context will be used or not. Finally, the context selector integrates all relevant contexts into one context set, ?.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Context-enhanced Text Matcher</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Matching Classifier</head><p>Our model predict predict the text similarity of two context-enhanced text representations.</p><formula xml:id="formula_1">h f inal = [h a ; h b ; |h a -h b |]</formula><p>(1)</p><formula xml:id="formula_2">p i = F F N (h f inal )<label>(2)</label></formula><p>where FFN(?) is a feed forward network with two hidden layers and a output layer, a relu activation after each hidden layer.</p><p>For each training sample {S a , S b , y}, we aim to minimize the BCE loss:</p><formula xml:id="formula_3">L = - N i=1 (ylog(p i ) + (1 -y)log(1 -p)) (3)</formula><p>where y ? {0, 1} is the label of the i-th training sample and p ? {0, 1} is the prediction of our model taking the sentence pair as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Result Selector</head><p>Since not every pair of short texts need context enhancement, for those pairs have high confidence with BERT baseline, we will keep the results and logits. We set the output logits of BERT baseline and our model to be ?i and ?i , respectively. Then, the final result will be as follow:</p><formula xml:id="formula_4">y i = ?i + ?i -1 (4)</formula><p>where y i ? {0, 1} is the final predicted label of i-th sample, and y i equal to 1 if y i is larger than or equal to 0.5. Otherwise, y i will be set to 0. 3 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>We conduct our experiments on Bank Question (BQ) <ref type="bibr" target="#b1">(Chen et al., 2018)</ref>, large-scale Chinese question matching corpus (LCQMC) <ref type="bibr" target="#b9">(Liu et al., 2018)</ref> and the Quora Question Paraphrasing corpus (QQP) datasets for semantic textual similarity task. BQ is a large-scale domain-specific Chinese corpus for sentence semantic matching. It is collected from customer service logs by a Chinese bank. LCQMC is a large-scale chinese question matching corpus. It focuses on intent matching rather than paraphrase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experiments</head><p>? BERT-Baseline: A chinese pretrained BERT, called Chinese-BERT-wwm, provided by <ref type="bibr" target="#b5">(Cui et al., 2019)</ref>.</p><p>? ERNIE 2.0: A continual pre-training framework named ERNIE 2.0 which incrementally builds pre-training tasks and then learn pretrained models on these constructed tasks via continual multi-task learning. <ref type="bibr" target="#b15">(Sun et al., 2021)</ref> ? LET-BERT <ref type="bibr" target="#b10">(Lyu et al., 2021)</ref>: A Linguistic knowledge Enhanced graph Transformer (LET) to deal with word ambiguity using HowNet.</p><p>? ZEN 2.0 Base <ref type="bibr" target="#b14">(Song et al., 2021)</ref>: An updated n-gram enhanced pre-trained encoder on Chinese and Arabic.</p><p>? GMN-BERT: A neural graph matching method (GMN) for Chinese short text matching.</p><p>? Glyce+BERT <ref type="bibr" target="#b11">(Meng et al., 2019)</ref>: Glyce provide glyph-vectors for logographic language representations.</p><p>? RoBERTa-wwm-ext-large: A chinese pretrained RoBERTa model which is also provided by <ref type="bibr" target="#b5">(Cui et al., 2019)</ref>.</p><p>In Table <ref type="table" target="#tab_2">1</ref>, both Bert Baseline and our model are the results of our tuning of the hyperparameters to the best. All other experimental results are using the best results on the corresponding paper. In comparison, our model results outperform all baselines on the BQ dataset and outperform the previous best model by nearly 2% in F1 values. On the LCQMC dataset, we also achieve the state of the art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Details</head><p>The BERT models used in our experiments are the  As seen in Table <ref type="table" target="#tab_4">2</ref>, removing the context selector will hurt the performance significantly because the unfiltered contexts contain serious noise. If we randomly select k contexts, then the model could not take advantage of the context information since the contexts may be irrelevant. However, when we select K most relevant contexts, due to the properities of the search engine, contexts will be exactly the same or similar to short texts. Therefore, the contexts will turn out to be useless for the model. As a result, the ablation studies proves that the context selector module can effectively filter the noisy contexts and provide high-quality contexts for each short text.</p><p>Our model shares the parameters in context encoder and short text encoder, then they could encode the contexts and short texts into the same semantic space. Finally, the ablation studies demostrate that sharing parameters boosts the performance and efficiency of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this work, we proposed a novel external knowledge enhanced BERT for Chinese short text matching. Our model takes two sentences and some related contexts as input and integrates the external information to moderate word ambiguity. The proposed method is evaluated on two Chinese benchmark datasets and obtains the best performance. Theablation studies also demonstrate that both semantic information and multi-granularity information are important for text matching modeling.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sentence 1 and 2 are the short texts. Context 1 and 2 are their crawled contexts from search engine. Highlighted spans are the matched part between short texts and contexts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>First</head><label></label><figDesc>, we encode S a and S b by sentence BERT to obtain their embedding, h a and h b . Then, we use context BERT model to encode ?a , ?b to obtain the embeddings of the contexts, h c a and h c b , respectively. Afterward, we concatenate h a , h b , h c a and h c b together and input them into a 3-layer Transformer model. Finally, we obtain the representation h a , h b , which are the final context-enhanced text representation of S a and S b .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Experiments on BQ and LCQMC Datasets. * marks the results reproduced by us.</figDesc><table><row><cell>Model</cell><cell cols="2">BQ ACC F1</cell><cell cols="2">LCQMC ACC F1</cell></row><row><cell>BERT-Baseline*</cell><cell>84.8</cell><cell>84.6</cell><cell>87.6</cell><cell>88.0</cell></row><row><cell>ERNIE 2.0</cell><cell>85.2</cell><cell>-</cell><cell>87.9</cell><cell>-</cell></row><row><cell>LET-BERT</cell><cell>85.3</cell><cell cols="3">84.98 88.38 88.85</cell></row><row><cell>ZEN 2.0 Base</cell><cell cols="2">85.42 -</cell><cell cols="2">88.71 -</cell></row><row><cell>GMN-BERT</cell><cell>85.6</cell><cell>85.5</cell><cell>87.3</cell><cell>88.0</cell></row><row><cell>Glyce+bERT</cell><cell>85.8</cell><cell>85.5</cell><cell>88.7</cell><cell>88.8</cell></row><row><cell cols="3">ROBERTA-wwm-ext-large* 85.79 -</cell><cell>90.4</cell><cell>-</cell></row><row><cell>Ours-BERT*</cell><cell cols="3">86.16 87.44 88.8</cell><cell>89.1</cell></row><row><cell>Ours-RoBERTa*</cell><cell cols="3">86.66 86.69 89.2</cell><cell>88.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Ablation studies on BQ dataset</figDesc><table><row><cell>Ours-share: Removing share parameter mecha-</cell></row><row><cell>nism between context encoder and sentence en-</cell></row><row><cell>coder.</cell></row><row><cell>Ours-cs+random: Removing context selector</cell></row><row><cell>module and randomly choosing k contexts.</cell></row><row><cell>Ours-cs+topk: Removing context selector module</cell></row><row><cell>and choosing top k relevant contexts.</cell></row><row><cell>Ours-rs: Removing result selector module.</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bridging the lexical chasm: statistical approaches to answer-finding</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 23rd annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="192" to="199" />
		</imprint>
	</monogr>
	<note>Dayne Freitag, and Vibhu Mittal</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The bq corpus: A large-scale domain-specific chinese corpus for sentence semantic equivalence identification</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haijun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daohe</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Buzhou</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m">Proceedings of the 2018 conference on empirical methods in natural language processing</title>
		<meeting>the 2018 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<biblScope unit="page" from="4946" to="4951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural graph matching networks for chinese short text matching</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanbin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boer</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lesheng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Su</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6152" to="6158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Revisiting pretrained models for Chinese natural language processing</title>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="657" to="668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08101</idno>
		<title level="m">Pre-training with whole word masking for chinese bert</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Information &amp; Knowledge Management</title>
		<meeting>the 22nd ACM international conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2333" to="2338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lattice cnns for matching based chinese question answering</title>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6634" to="6641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Lcqmc: A large-scale chinese question matching corpus</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongfang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Buzhou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1952" to="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Let: Linguistic knowledge enhanced graph transformer for chinese short text matching</title>
		<author>
			<persName><forename type="first">Boer</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Su</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12671</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Yuxian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.10125</idno>
		<title level="m">Glyce: Glyph-vectors for chinese character representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning semantic representations using convolutional neural networks for web search</title>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gr?goire</forename><surname>Mesnil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on world wide web</title>
		<meeting>the 23rd international conference on world wide web</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="373" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="801" to="809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Yan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Fu</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.01279</idno>
		<title level="m">Continue training and adaption for n-gram enhanced text encoders</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyuan</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanbin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.02137</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.08849</idno>
		<title level="m">Learning natural language inference with lstm</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.03814</idno>
		<title level="m">Bilateral multi-perspective matching for natural language sentences</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
