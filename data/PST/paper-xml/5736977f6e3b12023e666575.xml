<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Statistically Significant Detection of Linguistic Change</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>Oct Jan Apr Jul Oct Jan Apr Jul Oct</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Vivek</forename><surname>Kulkarni</surname></persName>
							<email>vvkulkarni@cs.stonybrook.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stony Brook University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
							<email>ralrfou@cs.stonybrook.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Stony Brook University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
							<email>bperozzi@cs.stonybrook.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Stony Brook University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
							<email>skiena@cs.stonybrook.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">Stony Brook University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Statistically Significant Detection of Linguistic Change</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published">Oct Jan Apr Jul Oct Jan Apr Jul Oct</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/2736277.2741627</idno>
					<note type="submission">Normalized Frequency Oct Jan Apr Jul Oct Jan Apr Jul Oct 12 13 0 20 40 60 80 100 Normalized Frequency (a) Frequency method (Google Trends) Nov Jan Mar May Jul Sep Nov Jan Mar May Jul Sep 12 13 1 2 3 4 5 6 7 Z¡Score Nov Jan Mar May Jul Sep Nov Jan Mar May Jul Sep 12 13</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H</term>
					<term>3</term>
					<term>3 [Information Storage and Retrieval]: Information Search and Retrieval Web Mining;Computational Linguistics</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a new computational approach for tracking and detecting statistically significant linguistic shifts in the meaning and usage of words. Such linguistic shifts are especially prevalent on the Internet, where the rapid exchange of ideas can quickly change a word's meaning. Our meta-analysis approach constructs property time series of word usage, and then uses statistically sound change point detection algorithms to identify significant linguistic shifts.</p><p>We consider and analyze three approaches of increasing complexity to generate such linguistic property time series, the culmination of which uses distributional characteristics inferred from word co-occurrences. Using recently proposed deep neural language models, we first train vector representations of words for each time period. Second, we warp the vector spaces into one unified coordinate system. Finally, we construct a distance-based distributional time series for each word to track its linguistic displacement over time.</p><p>We demonstrate that our approach is scalable by tracking linguistic change across years of micro-blogging using Twitter, a decade of product reviews using a corpus of movie reviews from Amazon, and a century of written books using the Google Book Ngrams. Our analysis reveals interesting patterns of language usage change commensurate with each medium.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Natural languages are inherently dynamic, evolving over time to accommodate the needs of their speakers. This effect is especially prevalent on the Internet, where the rapid exchange of ideas can change a word's meaning overnight. In this paper, we study the problem of detecting such linguistic shifts on a variety of media including micro-blog posts, product reviews, and books. Specifically, we seek to detect the broadening and narrowing of semantic senses of words, as they continually change throughout the lifetime of a medium.</p><p>We propose the first computational approach for tracking and detecting statistically significant linguistic shifts of words. To model the temporal evolution of natural language, we construct a time series per word. We investigate three methods to build our word time series. First, we extract Frequency based statistics to capture sudden changes in word usage. Second, we construct Syntactic time series by analyzing each word's part of speech (POS) tag distribution. Finally, we infer contextual cues from word co-occurrence statistics to construct Distributional time series. In order to detect and establish statistical significance of word changes over time, we present a change point detection algorithm, which is compatible with all methods.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> illustrates a 2-dimensional projection of the latent semantic space captured by our Distributional method. We clearly observe the sequence of semantic shifts that the word gay has undergone over the last century . Initially, gay was an adjective that meant cheerful or dapper. Observe for the first 50 years, that it stayed in the same general region of the semantic space. However by 1975, it had begun a transition over to its current meaning -a shift which accelerated over the years to come.</p><p>The choice of the time series construction method determines the type of information we capture regarding word usage. The difference between frequency-based approaches and distributional methods is illustrated in Figure <ref type="figure" target="#fig_2">2</ref>. Figure <ref type="figure" target="#fig_7">2a</ref> shows the frequencies of two words, Sandy (red), and Hurricane (blue) as a percentage of search queries according to Google Trends 1 . Observe the sharp spikes in both words' usage in October 2012, which corresponds to a storm called Hurricane Sandy striking the Atlantic Coast of the United States. However, only one of those words (Sandy) actually acquired a new meaning. Note that while the word Hurricane definitely experienced a surge in frequency of usage, it did not undergo any change in meaning. Indeed, using our distributional method (Figure <ref type="figure" target="#fig_7">2b</ref>), we observe that only the word Sandy shifted in meaning where as Hurricane did not.</p><p>Our computational approach is scalable, and we demonstrate this by running our method on three large datasets. Specifically, we investigate linguistic change detection across years of micro-blogging using Twitter, a decade of product reviews using a corpus of movie reviews from Amazon, and a century of written books using the Google Books Ngram Corpus.</p><p>Despite the fast pace of change of the web content, our method is able to detect the introduction of new products, movies and books. This could help semantically aware web applications to better understand user intentions and requests. Detecting the semantic shift of a word would trigger such applications to apply focused sense disambiguation analysis.</p><p>In summary, our contributions are as follows:</p><p>• Word Evolution Modeling: We study three different methods for the statistical modeling of word evolution over time. We use measures of frequency, part-of-speech tag distribution, and word co-occurrence to construct time series for each word under investigation.(Section 3) • Statistical Soundness: We propose (to our knowledge) the first statistically sound method for linguistic shift detection. Our approach uses change point detection in time series to assign significance of change scores to each word. (Section 4) • Cross-Domain Analysis: We apply our method on three different domains; books, tweets and online reviews. Our corpora consists of billions of words and spans several time scales. We show several interesting instances of semantic change identified by our method. (Section 6) The rest of the paper is structured as follows. In Section 2 we define the problem of language shift detection over time. Then, we outline our proposals to construct time series modeling word evolution in Section 3. Next, in Section 4, we describe the method we developed for detecting significant changes in natural language. We describe the datasets we used in Section 5, and then evaluate our system both qualitatively and quantitatively in Section 6. We follow this with a treatment of related work in Section 7, and finally conclude with a discussion of the limitations and possible future work in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PROBLEM DEFINITION</head><p>Our problem is to quantify the linguistic shift in word meaning (semantic or context change) and usage across time. Given a temporal corpora C that is created over a time span  Observe how Google Trends shows spikes in frequency for both Hurricane (blue) and Sandy (red). Our method, in contrast, models change in usage and detects that only Sandy changed its meaning and not Hurricane.</p><p>S, we divide the corpora into n snapshots Ct each of period length P . We build a common vocabulary V by intersecting the word dictionaries that appear in all the snapshots (i.e, we track the same word set across time). This eliminates trivial examples of word usage shift from words which appear or vanish throughout the corpus.</p><p>To model word evolution, we construct a time series T (w) for each word w ∈ V. Each point Tt(w) corresponds to statistical information extracted from corpus snapshot Ct that reflects the usage of w at time t. In Section 3, we propose several methods to calculate Tt(w), each varying in the statistical information used to capture w's usage.</p><p>Once these time series are constructed, we can quantify the significance of the shift that occurred to the word in its meaning and usage. Sudden increases or decreases in the time series are indicative of shifts in the word usage. Specifically we pose the following questions:</p><p>1. How statistically significant is the shift in usage of a word w across time (in T (w))? 2. Given that a word has shifted, at what point in time did the change happen?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">TIME SERIES CONSTRUCTION</head><p>Constructing the time series is the first step in quantifying the significance of word change. Different approaches capture various aspects of a word's semantic, syntactic and usage patterns. In this section, we describe three approaches (Frequency, Syntactic, and Distributional ) to building a time series, that capture different aspects of word evolution across time. The choice of time series significantly influences the types of changes we can detect -a phenomenon which we discuss further in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Frequency Method</head><p>The most immediate way to detect sequences of discrete events is through their change in frequency. Frequency based methods are therefore quite popular, and include tools like Google Trends and Google Books Ngram Corpus, both of which are used in research to predict economical and public health changes <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9]</ref>. Such analysis depends on keyword search over indexed corpora.</p><p>Frequency based methods can capture linguistic shift, as changes in frequency can correspond to words acquiring or losing senses. Although crude, this method is simple to implement. We track the change in probability of a word appearing over time. We calculate for each time snapshot corpus Ct, a unigram language model. Specifically, we construct the time series for a word w as follows:</p><formula xml:id="formula_0">Tt(w) = log #(w ∈ Ct) |Ct| ,<label>(1)</label></formula><p>where #(w ∈ Ct) is the number of occurrences of the word w in corpus snapshot Ct. An example of the information we capture by tracking word frequencies over time is shown in Figure <ref type="figure" target="#fig_3">3</ref>. Observe the sudden jump in late 1980s of the word gay in frequency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Syntactic Method</head><p>While word frequency based metrics are easy to calculate, they are prone to sampling error introduced by bias in domain and genre distribution in the corpus. Temporal events and popularity of specific entities could spike the word usage frequency without significant shift in its meaning, recall Hurricane in Figure <ref type="figure" target="#fig_7">2a</ref>.</p><p>Another approach to detect and quantify significant change in the word usage involves tracking the syntactic functionality it serves. A word could evolve a new syntactic functionality by acquiring a new part of speech category. For example, apple used to be only a "Noun" describing a fruit, but over time it acquired the new part of speech "Proper Noun" to indicate the new sense describing a technology company (Figure <ref type="figure">4</ref>). To leverage this syntactic knowledge, we annotate our corpus with part of speech (POS) tags. Then we calculate the probability distribution of part of speech tags Qt given the word w and time snapshot t as follows: Qt = Pr X∼POS Tags (X|w, Ct). We consider the POS tag distribution at t = 0 to be the initial distribution Q0. To quantify the temporal change between two time snapshots corpora, for a specific word w, we calculate the divergence between the POS distributions in both snapshots. We construct the time series as follows:</p><formula xml:id="formula_1">Tt(w) = JSD(Q0, Qt)</formula><p>(2) where JSD is the Jenssen-Shannon divergence <ref type="bibr" target="#b20">[21]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>JS(Q</head><formula xml:id="formula_2">0 ;Q t ) Noun Proper Noun Adjective JS(Q 0 ;Q t )</formula><p>Figure <ref type="figure">4</ref>: Part of speech tag probability distribution of the word apple (stacked area chart). Observe that the "Proper Noun" tag has dramatically increased in 1980s. The same trend is clear from the time series constructed using Jenssen-Shannon Divergence (dark blue line).</p><p>Figure <ref type="figure">4</ref> shows that the JS divergence (dark blue line) reflects the change in the distribution of the part of speech tags given the word apple. In 1980s, the "Proper Noun" tag (blue area) increased dramatically due to the rise of Apple Computer Inc., the popular consumer electronics company.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Distributional Method</head><p>Semantic shifts are not restricted to changes to part of speech. For example, consider the word mouse. In the 1970s it acquired a new sense of "computer input device", but did not change its part of speech categorization (since both senses are nouns). To detect such subtle semantic changes, we need to infer deeper cues from the contexts a word is used in.</p><p>The distributional hypothesis states that words appearing in similar contexts are semantically similar <ref type="bibr" target="#b12">[13]</ref>. Distributional methods learn a semantic space that maps words to continuous vector space R d , where d is the dimension of the vector space. Thus, vector representations of words appearing in similar contexts will be close to each other. Recent developments in representation learning (deep learning) <ref type="bibr" target="#b4">[5]</ref> have enabled the scalable learning of such models. We use a variation of these models <ref type="bibr" target="#b27">[28]</ref> to learn word vector representation (word embeddings) that we track across time.</p><p>Specifically, we seek to learn a temporal word embedding φt : V, Ct → R d . Once we learn a representation of a specific word for each time snapshot corpus, we track the changes of the representation across the embedding space to quantify the meaning shift of the word (as shown in Figure <ref type="figure" target="#fig_0">1</ref>).</p><p>In this section we present our distributional approach in detail. Specifically we discuss the learning of word embeddings, the aligning of embedding spaces across different time snapshots to a joint embedding space, and the utilization of a word's displacement through this semantic space to construct a distributional time series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Learning Embeddings</head><p>Given a time snapshot Ct of the corpus, our goal is to learn φt over V using neural language models. At the beginning of the training process, the word vector representations are randomly initialized. The training objective is to maximize the probability of the words appearing in the context of word wi. Specifically, given the vector representation wi of a word wi (wi = φt(wi)), we seek to maximize the probability of wj through the following equation:</p><formula xml:id="formula_3">Pr(wj | wi) = exp (w T j wi) w k ∈V exp (w T k wi)<label>(3)</label></formula><p>In a single epoch, we iterate over each word occurrence in the time snapshot Ct to minimize the negative log-likelihood J of the context words. Context words are the words appearing to the left or right of wi within a window of size m. Thus J can be written as:</p><formula xml:id="formula_4">J = w i ∈C t i+m j=i−m j!=i − log Pr(wj | wi)<label>(4)</label></formula><p>Notice that the normalization factor that appears in Eq. ( <ref type="formula" target="#formula_3">3</ref>) is not feasible to calculate if |V| is too large. To approximate this probability, we map the problem from a classification of 1out-of-V words to a hierarchical classification problem <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>. This reduces the cost of calculating the normalization factor from O(|V|) to O(log |V|). We optimize the model parameters using stochastic gradient descent <ref type="bibr" target="#b5">[6]</ref>, as follows:</p><formula xml:id="formula_5">φt(wi) = φt(wi) − α × ∂J ∂φt(wi) ,<label>(5)</label></formula><p>where α is the learning rate. We calculate the derivatives of the model using the back-propagation algorithm <ref type="bibr" target="#b33">[34]</ref>. We use the following measure of training convergence:</p><formula xml:id="formula_6">ρ = 1 |V| w∈V φ k T (w)φ k+1 (w) φ k (w) 2 φ k+1 (w) 2 ,<label>(6)</label></formula><p>where φ k is the model parameters after epoch k. We calculate ρ after each epoch and stop the training if ρ ≤ 1.0 −4 .</p><p>After training stops, we normalize word embeddings by their L2 norm, which forces all words to be represented by unit vectors.</p><p>In our experiments, we use the gensim implementation of skipgram models 2 . We set the context window size m to 10 unless otherwise stated. We choose the size of the word embedding space dimension d to be 200. To speed up the training, we subsample the frequent words by the ratio 10 −5 <ref type="bibr" target="#b26">[27]</ref>.</p><p>2 https://github.com/piskvorky/gensim</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Aligning Embeddings</head><p>Having trained temporal word embeddings for each time snapshot Ct, we must now align the embeddings so that all the embeddings are in one unified coordinate system. This enables us to characterize the change between them. This process is complicated by the stochastic nature of our training, which implies that models trained on exactly the same data could produce vector spaces where words have the same nearest neighbors but not with the same coordinates. The alignment problem is exacerbated by actual changes in the distributional nature of words in each snapshot.</p><p>To aid the alignment process, we make two simplifying assumptions: First, we assume that the spaces are equivalent under a linear transformation. Second, we assume that the meaning of most words did not shift over time, and therefore, their local structure is preserved. Based on these assumptions, observe that when the alignment model fails to align a word properly, it is possibly indicative of a linguistic shift.</p><p>Specifically, we define the set of k nearest words in the embedding space φt to a word w to be k-NN(φt(w)). We seek to learn a linear transformation W t →t (w) ∈ R d×d that maps a word from φ t to φt by solving the following optimization:</p><formula xml:id="formula_7">W t →t (w) = argmin W w i ∈ k-NN(φ t (w)) φ t (wi)W − φt(wi) 2 2 ,<label>(7)</label></formula><p>which is equivalent to a piecewise linear regression model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Time Series Construction</head><p>To track the shift of word position across time, we align all embeddings spaces to the embedding space of the final time snapshot φn using the linear mapping (Eq. 7). This unification of coordinate systems allows us to compare relative displacements that occurred to words across different time periods.</p><p>To capture linguistic shift, we construct our distributional time series by calculating the distance in the embedding space between φt(w)Wt →n(w) and φ0(w)W0 →n(w) as</p><formula xml:id="formula_8">Tt(w) = 1 − (φt(w)Wt →n(w)) T (φ0(w)W0 →n(w)) φt(w)Wt →n(w) 2 φ0(w)W0 →n(w) 2<label>(8)</label></formula><p>Figure <ref type="figure" target="#fig_5">5</ref> shows the time series obtained using word embeddings for tape, which underwent a semantic change in the 1950s with the introduction of magnetic tape recorders. As such recorders grew in popularity, the change becomes more pronounced, until it is quite apparent by the 1970s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CHANGE POINT DETECTION</head><p>Given a time series of a word T (w), constructed using one of the methods discussed in Section 3, we seek to determine whether the word changed significantly, and if so estimate the change point. We believe a formulation in terms of changepoint detection is appropriate because even if a word might change its meaning (usage) gradually over time, we expect a time period where the new usage suddenly dominates (tips over) the previous usage (akin to a phase transition) with the word gay serving as an excellent example.</p><p>There exists an extensive body of work on change point detection in time series <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b37">38]</ref>. Our approach models the time series based on the Mean Shift model described in <ref type="bibr" target="#b37">[38]</ref>. First, our method recognizes that language exhibits a general stochastic drift. We account for this by first normalizing the time series for each word. Our method then attempts to p-value ← minj∈C p-value(w, j) 13: ECP ← argmin j∈C p-value(w, j) 14: return p-value, ECP detect a shift in the mean of the time series using a variant of mean shift algorithms for change point analysis. We outline our method in Algorithm 1 and describe it below. We also illustrate key aspects of the method in Figure <ref type="figure">6</ref>.</p><p>Given a time series of a word T (w), we first normalize the time series. We calculate the mean µi = 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>|V|</head><p>w∈V Ti(w) and variance V ari = 1 |V| w∈V (Ti(w) − µi) 2 across all words. Then, we transform T (w) into a Z-Score series using:</p><formula xml:id="formula_9">Zi(w) = Ti(w) − µi √ V ari ,<label>(9)</label></formula><p>where Zi(w) is the Z-Score of the time series for the word w at time snapshot i.</p><p>We model the time series Z(w) by a Mean shift model <ref type="bibr" target="#b37">[38]</ref>. Let S = Z1(w), Z2(w), . . . , Zn(w) represent the time series. We model S to be an output of a stochastic process where each Si can be described as Si = µi + i where µi is the mean and i is the random error at time i. We also assume that the errors i are independent with mean 0. Generally µi = µi−1 except for a few points which are change points.</p><p>Based on the above model, we define the mean shift of a general time series S as follows:</p><formula xml:id="formula_10">K(S) = 1 l − j l k=j+1 S k − 1 j j k=1 S k<label>(10)</label></formula><p>This corresponds to calculating the shift in mean between two parts of the time series pivoted at time point j. Change points can be thus identified by detecting significant shifts in the mean. <ref type="foot" target="#foot_0">3</ref>Given a normalized time series Z(w), we then compute the mean shift series K(Z(w)) (Line 2). To estimate the statistical significance of observing a mean shift at time point j, we use bootstrapping <ref type="bibr" target="#b11">[12]</ref> (see Figure <ref type="figure">6</ref> and Lines 3-10) under the null hypothesis that there is no change in the mean. In particular, we establish statistical significance by first obtaining B (typically B = 1000) bootstrap samples obtained by permuting Z(w) (Lines 3-10). Second, for each bootstrap sample P, we calculate K(P ) to yield its corresponding bootstrap statistic and we estimate the statistical significance (p-value) of observing the mean shift at time i compared to the null distribution (Lines 8-10). Finally, we estimate the change point by considering the time point j with the minimum p-value score (described in <ref type="bibr" target="#b37">[38]</ref>). While this method does detect significant changes in the mean of the time series, observe that it does not account for the magnitude of the change in terms of Z-Scores. We extend this approach to obtain words that changed significantly compared to other words, by considering only those time points where the Z-Score exceeds a user-defined threshold γ (we typically set γ to 1.75). We then estimate the change point as the time point with the minimum p-value exactly as outlined before (Lines 11-14).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">DATASETS</head><p>Here we report the details of the three datasets that we consider -years of micro-blogging from Twitter, a decade of movie reviews from Amazon, and a century of written books using the Google Books Ngram Corpus. Table <ref type="table" target="#tab_0">1</ref> shows a summary of three different datasets spanning different modes of expression on the Internet: books, an online forum and a micro-blog.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Google Books Ngram Corpus.</head><p>The Google Books Ngram Corpus project enables the analysis of cultural, social and linguistic trends. It contains the frequency of short phrases of text (ngrams) that were extracted from books written in eight languages over five centuries <ref type="bibr" target="#b24">[25]</ref>. These ngrams vary in size (1-5) grams. We use the 5-gram phrases which restrict our context window size m to 5. The 5-grams include phrases like 'thousand pounds less then nothing' and 'to communicate to each other'. We focus on the time span from 1900 − 2005, and set the time snapshot period to 5 years (21 points). We obtain the POS Distribution of each word in the above time range by using the Google Syntactic Ngrams dataset <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Amazon Movie Reviews.</head><p>Amazon Movie Reviews dataset consists of movie reviews from Amazon. This data spans August 1997 to October 2012 (13 time points), including all 8 million reviews. However, we consider the time period starting from 2000 as the number of reviews from earlier years is considerably small. Each review includes product and user information, ratings, and a plain-text review. The reviews describe user's opinions of a movie, for example: 'This movie has it all. Drama, action, amazing battle scenes -the best I've ever seen. It's definitely a must see.'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Twitter Data.</head><p>This dataset consists of a sample that spans 24 months starting from September 2011 to October 2013. Each tweet includes the tweet ID, tweet and the geo-location if available. A tweet is a status message with up to 140 characters: 'I hope sandy doesn't rip the roof off the pool while we're swimming ...'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EXPERIMENTS</head><p>In this section, we apply our methods to each dataset presented in Section 5 and identify words that have changed usage over time. We describe the results of our experiments below. The code used for running these experiments is available at the first author's website. <ref type="foot" target="#foot_1">4</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Time Series Analysis</head><p>As we shall see in Section 6.4.1, our proposed time series construction methods differ in performance. Here, we use the detected words to study the behavior of our construction methods.</p><p>Table <ref type="table">2</ref> shows the time series constructed for a sample of words with their corresponding p-value time series, displayed in the last column. A dip in the p-value is indicative of a shift in the word usage. The first three words, transmitted, bitch, and sex, are detected by both the Frequency and Distributional methods. Table <ref type="table" target="#tab_2">3</ref> shows the previous and current senses of these words demonstrating the changes in usage they have gone through.</p><p>Observe that words like her and desk did not change signifantly in meaning, however, the Frequency method detects a change. The sharp increase of the word her in frequency around the 1960's could be attributed to the concurrent rise and popularity of the feminist movement. Sudden temporary popularity of specific social and political events could lead the Frequency method to produce many false positives. These results confirm our intuition we illustrated in Figure <ref type="figure" target="#fig_2">2</ref>. While frequency analysis (like Google Trends) is an extremely useful tool to visualize trends, it is not very well suited for the task of detecting linguistic shift.</p><p>The last two rows in Table <ref type="table">2</ref> display two words (apple and diet) that Syntactic method detected. The word apple was detected uniquely by the Syntactic method as its most frequent part of speech tag changed significantly from "Noun" to "Proper Noun". While both Syntactic and Distributional methods indicate the change in meaning of the word diet, it is only the Distributional method that detects the right point of change (as shown in Table <ref type="table" target="#tab_2">3</ref>). The Syntactic method is indicative of having low false positive rate, but suffers from a high false negative rate, given that only two words in the table were detected. Furthermore, observe that Syntactic method relies on good linguistic taggers. However, linguistic taggers require annotated data sets and also do not work well across domains.</p><p>We find that the Distributional method offers a good balance between false positives and false negatives, while requiring no linguistic resources of any sort. Having analyzed the words detected by different time series we turn our attention to the analysis of estimated changepoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Historical Analysis</head><p>We have demonstrated that our methods are able to detect words that shifted in meaning. We seek to identify the inflection points in time where the new senses are introduced. Moreover, we are interested in understanding how the new acquired senses differ from the previous ones.</p><p>Table <ref type="table" target="#tab_2">3</ref> shows sample words that are detected by Syntactic and Distributional methods. The first set represents words which the Distributional method detected (Distributional better) while the second set shows sample words which Syntactic method detected (Syntactic better).</p><p>Our Distributional method estimates that the word tape changed in the early 1970s to mean a "cassette tape" and not only an "adhesive tape". The change in the meaning of tape commences with the introduction of magnetic tapes in 1950s (Figure <ref type="figure" target="#fig_5">5</ref>). The meaning continues to shift with the mass production of cassettes in Europe and North America for pre-recorded music industry in mid 1960s until it is deemed statistically significant. The word plastic is yet another example, where the introduction of new products inflected a shift in the word meaning. The introduction of Polystyrene in 1950 popularized the term "plastic" as a synthetic polymer, which was once used only to denote the physical property of "flexibility". The popularity of books on dieting started with the best selling book Dr. Atkins' Diet Revolution by Robert C. Atkins in 1972 <ref type="bibr" target="#b15">[16]</ref>. This changed the use of the word diet to mean a life-style of food consumption behavior and not only the food consumed by an individual or group.</p><p>The Syntactic section of Table <ref type="table" target="#tab_2">3</ref> shows that words like hug and sink were previously used mainly as verbs. Over time organizations and movements started using hug as a noun which dominated over its previous sense. On the other hand, the words click and handle, originally nouns, started being used as verbs.</p><p>Another clear trend is the use of common words as proper nouns. For example, with the rise of the computer industry, the word apple acquired the sense of the tech company Apple in mid 1980s and the word windows shifted its meaning to the operating system developed by Microsoft in early 1990s. Additionally, we detect the word bush that is widely used as a proper noun in 1989, which coincides with George H. W. Bush's presidency in USA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Cross Domain Analysis</head><p>Semantic shift can occur much faster on the web, where words can acquire new meanings within weeks, or even days. In this section we turn our attention to analyzing linguistic shift on Amazon Reviews and Twitter (content that spans a much shorter time scale as compared to Google Books Ngram Corpus).   <ref type="table">4</ref>: Sample of words detected by our Distributional method on Amazon Reviews and Twitter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word</head><p>Table <ref type="table">4</ref> shows results from our Distributional method on the Amazon Reviews and Twitter datasets. New technologies and products introduced new meanings to words like streaming, ray, rays, and combo. The word twilight acquired a new sense in 2009 concurrent with the release of the Twilight movie in November 2008.</p><p>Similar trends can be observed on Twitter. The introduction of new games and cellphone applications changed the meaning of the words candy, mystery and rally. The word sandy acquired a new sense in September 2012 weeks before Hurricane Sandy hit the east coast of USA. Similarly we see that the word shades shifted its meaning with the release of the bestselling book "Fifty Shades of Grey" in June 2012.</p><p>These examples illustrate the capability of our method to detect the introduction of new products, movies and books. This could help semantically aware web applications to understand user intentions and requests better. Detecting the semantic shift of a word would trigger such applications to apply a focused disambiguation analysis on the sense intended by the user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Quantitative Evaluation</head><p>The lack of any reference (gold standard) data, poses a challenge to quantitatively evaluate our methods. Therefore, we assess the performance of our methods using multiple approaches. We begin with a synthetic evaluation, where we have knowledge of ground-truth changes. Next we create a  reference data set based on prior work and evaluate all three methods using it. We follow this with a human evaluation, and conclude with an examination of the agreement between the methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.1">Synthetic Evaluation</head><p>To evaluate the quantitative merits of our approach, we use a synthetic setup which enables us to model linguistic shift in a controlled fashion by artificially introducing changes to a corpus.</p><p>Our synthetic corpus is created as follows: First, we duplicate a copy of a Wikipedia corpus<ref type="foot" target="#foot_2">5</ref> 20 times to model time snapshots. We tagged the Wikipedia corpora with part of speech tags using the TextBlob tagger <ref type="foot" target="#foot_3">6</ref> . Next, we introduce changes to a word's usage to model linguistic shift. To do this, we perturb the last 10 snapshots. Finally, we use our approach to rank all words according to their p-values, and then we calculate the Mean Reciprocal Rank (M RR = 1/|Q| |Q| i=1 1/rank(wi)) for the words we perturbed. We rank the words that have lower p-value higher, therefore, we expect the MRR to be higher in the methods that are able to discover more words that have changed.</p><p>To introduce a single perturbation, we sample a pair of words out of the vocabulary excluding functional words and stop words <ref type="foot" target="#foot_4">7</ref> . We designate one of them to be a donor and the other to be a receptor. The donor word occurrences will be replaced with the receptor word with a success probability p replacement . For example, given the word pair (location, equation), some of the occurrences of the word location (Donor) were replaced with the word equation (Receptor) in the second half snapshots of our temporal corpus.</p><p>Figure <ref type="figure" target="#fig_9">7</ref> illustrates the results on two types of perturbations we synthesized. First, we picked our (Donor, Receptor) pairs such that both of them have the same most frequent part of speech tag. For example, we might use the pair (boat, car) but not (boat, running). We expect the frequency of the receptor to change and its context distribution but no significant syntactic changes. Figure <ref type="figure" target="#fig_9">7a</ref> shows the MRR of the receptor words on Distributional and Frequency methods. We observe that both methods improve their rankings as the degree of induced change increases (measured, here, by p replacement ). Second, we observe that the Distributional approach outperforms Frequency method consistently for different values of p replacement .</p><p>Second, to compare Distributional and Syntactic methods we sample word pairs without the constraint of being from  the same part of speech categories. Figure <ref type="figure" target="#fig_9">7b</ref> shows that the Syntactic method while outperforming Distributional method when the perturbation statistically is minimal, its ranking continue to decline in quality as the perturbation increases. This could be explained by noting that the quality of the tagger annotations decreases as the corpus at inference time diverges from the training corpus. It is quite clear from both experiments, that the Distributional method outperforms other methods when p replacement &gt; 0.4 without requiring any language specific resources or annotators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.2">Evaluation on a Reference Dataset</head><p>In this section, we attempt to gauge the performance of the various methods on a reference data set. We created a reference data set D of 20 words that have been suggested by prior work <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b38">39]</ref> as having undergone a linguistic change <ref type="foot" target="#foot_5">8</ref> . For each method, we create a list L of its changed words ordered by the significance scores of the change, and evaluate the Precision@k with respect to the reference data set constructed. Specifically, the Precision@k between L and D can be defined as:</p><formula xml:id="formula_11">Precision@k(L, D) = |L[1 : k] ∩ D| |D|<label>(11)</label></formula><p>Figure <ref type="figure" target="#fig_11">8a</ref> depicts the performance of the different methods on this reference data set. Observe that the Distributional method outperforms other methods with the Frequency method performing the poorest (due to its high false positive rate). The Syntactic method which does not capture semantic changes well also performs worse than the Distributional method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.3">Human Evaluation</head><p>We chose the top 20 words claimed to have changed by each method and asked 3 human evaluators to independently decide whether each word experienced a linguistic shift. For each method, we calculated the percentage of words each rater believes have changed and report the mean percentage. We observed that on an average the raters believe that only 13.33% of the words reported by Frequency method and only 21.66% of the words reported by Syntactic method changed. However, in the case of Distributional method we observed that on an average the raters believe that 53.33% of the words changed. We conclude thus from this evaluation that the Distributional method outperforms other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.4">Method Agreement</head><p>In order to investigate the agreement between the various methods, we again consider the top k words that each method is most confident have changed. For each pair of methods, we then compute the fraction of words both methods agree on in their top k lists. Specifically given methods M1 and M2 let M1(k) and M2(k) represent the top k lists for M1 and M2 respectively. We define the agreement between these 2 lists as follows:</p><formula xml:id="formula_12">AG(M1(k), M2(k)) = |M1(k) ∩ M2(k)| |M1(k) ∪ M2(k)|<label>(12)</label></formula><p>which is the Jaccard Similarity between M1(k) and M2(k).</p><p>Figure <ref type="figure" target="#fig_11">8b</ref> shows the agreement scores between each pair of methods for different values of k. We first note that the agreement between all methods is low, suggesting that the methods differ in aspects of word change captured. Observe that the agreement between Distributional and Syntactic is higher compared to that of Syntactic and Frequency. This can be explained by noting that Distributional method captures semantic changes along with elements of syntactic changes, and therefore agrees more with Syntactic method. We leave it to future work to investigate whether a single improved method can capture all of these aspects of word usage effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">RELATED WORK</head><p>Here we discuss the four most relevant areas of related work: linguistic shift, word embeddings, change point detection, and Internet linguistics. Linguistic Shift: There has been a surge in the work about language evolution over time. Michel et al. <ref type="bibr" target="#b24">[25]</ref> detected important political events by analyzing frequent patterns. Juola <ref type="bibr" target="#b17">[18]</ref> compared language from different time periods and quantified the change. Lijffijt et al. <ref type="bibr" target="#b19">[20]</ref> and Säily et al. <ref type="bibr" target="#b34">[35]</ref> study variation in noun/pronoun frequencies, and lexical stability in a historical corpus. Different from these studies, we quantify linguistic change by tracking individual shifts in words meaning. This fine grain detection and tracking still allows us to quantify the change in natural language as a whole, while still being able to interpret these changes.</p><p>Gulordava and Baroni <ref type="bibr" target="#b14">[15]</ref> propose a distributional similarity approach to detecting semantic change in the Google Book Ngram corpus between 2 time periods. Wijaya and Yeniterzi <ref type="bibr" target="#b38">[39]</ref> study evolution of words using a topic modeling approach but do not suggest an explicit change point detection algorithm. Our work differs from the above studies by tracking word evolution through multiple time periods and explicitly providing a change point detection algorithm to detect significant changes. Mitra et al. <ref type="bibr" target="#b28">[29]</ref> use a graph based approach relying on dependency parsing of sentences. Our proposed time series construction methods require minimal linguistic knowledge and resources enabling the application of our approach to all languages and domains equally. Compared to the sequential training procedure proposed by Kim et al. <ref type="bibr" target="#b18">[19]</ref> work, our technique warps the embeddings spaces of the different time snapshots after the training, allowing for efficient training that could be parallelized for large corpora. Moreover, our work is unique in the fact that our datasets span different time scales, cover larger user interactions and represent a better sample of the web. Word Embeddings: Bengio et al. <ref type="bibr" target="#b3">[4]</ref> used word embeddings to develop a neural language model that outperforms traditional ngram models. These word embeddings have been shown to capture fine grain structures and regularities in the data <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32]</ref>. Moreover, they have proved to be useful for a wide range of natural language processing tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10]</ref>. The same technique of learning word embeddings has been applied recently to graph representations <ref type="bibr" target="#b32">[33]</ref>. Change Point Detection: Change point detection and analysis is an important problem in the area of time series analysis and modeling. Taylor <ref type="bibr" target="#b37">[38]</ref> describes control charts and CUSUM based methods in detail. Adams and MacKay <ref type="bibr" target="#b0">[1]</ref> presents a Bayesian approach to online change point detection. The method of bootstrapping and establishing statistical significance is outlined in <ref type="bibr" target="#b11">[12]</ref>. Basseville and Nikiforov <ref type="bibr" target="#b2">[3]</ref> provides an excellent survey on several elementary change point detection techniques and time series models. Internet Linguistics: Internet Linguistics is concerned with the study of language in media influenced by the Internet (online forums, blogs, online social media) and also other related forms of electronic media like text messaging. Schiano et al. <ref type="bibr" target="#b35">[36]</ref> and Tagliamonte and Denis <ref type="bibr" target="#b36">[37]</ref> study how teenagers use messaging media focusing on their usage patterns and the resulting implications on design of e-mail and instant messaging. Merchant <ref type="bibr" target="#b23">[24]</ref> study the language use by teenagers in online chat forums. An excellent survey on Internet Linguistics is provided by Crystal <ref type="bibr" target="#b10">[11]</ref> and includes linguistic analyses of social media like Twitter, Facebook or Google+.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">CONCLUSIONS AND FUTURE WORK</head><p>In this work, we have proposed three approaches to model word evolution through different time series construction methods. Our computational approach then uses statistically sound change point detection algorithms to detect significant linguistic shifts. Finally, we demonstrated our method's effectiveness on three different data sets each representing a different medium. By analyzing the Google Books Ngram Corpus, we were able to detect historical semantic shifts that happened to words like gay and bitch. Moreover, in faster evolving media like Tweets and Amazon Reviews, we were able to detect recent events like storms, game and book releases. This capability of detecting meaning shift, should help decipher the ambiguity of dynamical systems like natural languages. We believe our work has implications for the fields of Semantic Search and the recently burgeoning field of Internet Linguistics.</p><p>Our future work in the area will focus on the real time analysis of linguistic shift, the creation of better resources for the quantitative evaluation of computational methods, and the effects of attributes like geographical location and content source on the underlying mechanisms of meaning change in language.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A 2-dimensional projection of the latent semantic space captured by our algorithm. Notice the semantic trajectory of the word gay transitioning meaning in the space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1</head><label></label><figDesc>http://www.google.com/trends/</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Comparison between Google Trends and our method. Observe how Google Trends shows spikes in frequency for both Hurricane (blue) and Sandy (red). Our method, in contrast, models change in usage and detects that only Sandy changed its meaning and not Hurricane.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Frequency usage of the word gay over time, observe the sudden change in frequency in the late 1980s.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>DistanceFigure 5 :</head><label>5</label><figDesc>Figure 5: Distributional time series for the word tape over time using word embeddings. Observe the change of behavior starting in the 1950s, which is quite apparent by the 1970s.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Pr 4 x 1 Figure 6 :</head><label>416</label><figDesc>Figure 6: Our change point detection algorithm. In Step x, we normalize the given time series T (w) to produce Z(w). Next, we shuffle the time series points producing the set π(Z(w)) (Step y). Then, we apply the mean shift transformation (K) on both the original normalized time series Z(w) and the permuted set (Step z). In Step {, we calculate the probability distribution of the mean shifts possible given a specific time (t = 1985) over the bootstrapped samples. Finally, we compare the observed value in K(Z(w)) to the probability distribution of possible values to calculate the p-value which determines the statistical significance of the observed time series shift (Step |).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Table 2 :</head><label>2</label><figDesc>Comparison of our different methods of constructing linguistic shift time series on the Google Books Ngram Corpus. The first three columns represent time series for a sample of words. The last column shows the p-value for each time step of each method, as generated by our change point detection algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Performance of our proposed methods under different scenarios of perturbation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Method performance and agreement on changed words in the Google Books Ngram Corpus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of our datasets</figDesc><table><row><cell></cell><cell cols="2">Google Ngrams Amazon</cell><cell>Twitter</cell></row><row><cell>Span (years)</cell><cell>105</cell><cell>12</cell><cell>2</cell></row><row><cell>Period</cell><cell>5 years</cell><cell>1 year</cell><cell>1 month</cell></row><row><cell># words</cell><cell>∼10 9</cell><cell cols="2">∼9.9 × 10 8 ∼10 9</cell></row><row><cell>|V|</cell><cell>∼50K</cell><cell>∼50K</cell><cell>∼100K</cell></row><row><cell cols="2"># documents ∼7.5 × 10 8</cell><cell>8. × 10 6</cell><cell>∼10 8</cell></row><row><cell>Domain</cell><cell>Books</cell><cell>Movie</cell><cell>Micro</cell></row><row><cell></cell><cell></cell><cell>Reviews</cell><cell>Blogging</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Estimated change point (ECP) as detected by our approach for a sample of words on Google Books Ngram Corpus. Distributional method is better on some words (which Syntactic did not detect as statistically significant eg. sex, transmitted, bitch, tape, peck) while Syntactic method is better on others (which Distributional failed to detect as statistically significant eg. apple, windows, bush)</figDesc><table><row><cell></cell><cell>Word</cell><cell>p-value</cell><cell>ECP Past Usage</cell><cell>Present Usage</cell></row><row><cell>Amazon Reviews</cell><cell>instant twilight rays streaming ray delivery combo</cell><cell>0.016 0.022 0.001 0.002 0.002 0.002 0.002</cell><cell>2010 instant hit, instant dislike 2009 twilight as in dusk 2008 x-rays 2008 sunlight streaming 2006 ray of sunshine 2006 delivery of dialogue 2006 combo of plots</cell><cell>instant download Twilight (The movie) blu-rays streaming video Blu-ray timely delivery of products combo DVD pack</cell></row><row><cell></cell><cell>candy</cell><cell>&lt;0.001</cell><cell>Apr 2013 candy sweets</cell><cell>Candy Crush (The game)</cell></row><row><cell>Twitter</cell><cell>rally snap mystery stats sandy</cell><cell>&lt;0.001 &lt;0.001 &lt;0.001 &lt;0.001 0.03</cell><cell>Mar 2013 political rally Dec 2012 snap a picture Dec 2012 mystery books Nov 2012 sport statistics Sep 2012 sandy beaches</cell><cell>rally of soldiers (Immortalis game) snap chat Mystery Manor (The game) follower statistics Hurricane Sandy</cell></row><row><cell></cell><cell>shades</cell><cell>&lt;0.001</cell><cell>Jun 2012 color shade, shaded glasses</cell><cell>50 shades of grey (The Book)</cell></row><row><cell></cell><cell>Table</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0">This is similar to the CUSUM based approach used for detecting change points which is also based on mean shift model.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1">http://vivekkulkarni.net</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2">http://mattmahoney.net/dc/text8.zip</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3">http://textblob.readthedocs.org/en/dev/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4">NLTK Stopword List: http://www.nltk.org/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5">The reference data set and the human evaluations are available at http://vivekkulkarni.net</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Andrew Schwartz for providing us access to the Twitter data. This research was partially supported by NSF Grants DBI-1355990 and IIS-1017181, a Google Faculty Research Award, a Renaissance Technologies Fellowship and the Institute for Computational Science at Stony Brook University.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Bayesian online changepoint detection</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Mackay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<pubPlace>Cambridge, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Polyglot: Distributed word representations for multilingual nlp</title>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
		<editor>CoNLL</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Detection of Abrupt Changes: Theory and Application</title>
		<author>
			<persName><forename type="first">M</forename><surname>Basseville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>Nikiforov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Prentice-Hall, Inc</publisher>
			<pubPlace>Upper Saddle River, NJ, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural probabilistic language models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Innovations in Machine Learning</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="137" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stochastic gradient learning in neural networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neuro-Nîmes</title>
				<meeting>Neuro-Nîmes<address><addrLine>Nimes, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">C2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Google trends: A webbased tool for real-time surveillance of disease outbreaks</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mylonakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clinical Infectious Diseases</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1557" to="1564" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The expressive power of word embeddings</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
		<idno>CoRR, abs/1301.3226</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Predicting the present with google trends</title>
		<author>
			<persName><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Varian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Economic Record</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="2" to="9" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-11">Nov. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Internet Linguistics: A Student Guide. Routledge</title>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2011">10001. 2011</date>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
	<note>1st edition</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">An introduction to the bootstrap</title>
		<author>
			<persName><forename type="first">B</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Firth</surname></persName>
		</author>
		<title level="m">Papers in Linguistics 1934-1951: Repr</title>
				<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1961">1961</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A dataset of syntacticngrams over time from a very large corpus of english books</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Orwant</surname></persName>
		</author>
		<editor>*SEM</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A distributional similarity approach to the detection of semantic change in the google books ngram corpus</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gulordava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GEMS</title>
				<imprint>
			<date type="published" when="2011-07">July 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The books of the century</title>
		<author>
			<persName><forename type="first">D</forename><surname>Immerwahr</surname></persName>
		</author>
		<ptr target="http://www.ocf.berkeley.edu/~immer/books1970s" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A framework for analyzing semantic change of words across time</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jatowt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Duh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint JCDL/TPDL Digital Libraries Conference</title>
				<meeting>the Joint JCDL/TPDL Digital Libraries Conference</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The time course of language change</title>
		<author>
			<persName><forename type="first">P</forename><surname>Juola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and the Humanities</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="96" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Temporal analysis of language through neural language models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-I</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hanaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Ceecing the baseline: Lexical stability and significant change in a historical corpus</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lijffijt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Säily</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nevalainen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>VARIENG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Divergence measures based on the shannon entropy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="145" to="151" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Syntactic annotations for the google books ngram corpus</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Aiden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Orwant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Enhanced search with wildcards and morphological inflections in the google books ngram viewer</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL Demonstrations Track</title>
				<meeting>ACL Demonstrations Track</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06">June 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Teenagers in cyberspace: an investigation of language use and language change in internet chatrooms</title>
		<author>
			<persName><forename type="first">G</forename><surname>Merchant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Research in Reading</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="293" to="306" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Quantitative analysis of culture using millions of digitized books</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">331</biblScope>
			<biblScope unit="issue">6014</biblScope>
			<biblScope unit="page" from="176" to="182" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
				<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno>CoRR, abs/1301.3781</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">That&apos;s sick dude!: Automatic identification of word sense change across different timescales</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A scalable hierarchical distributed language model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1081" to="1088" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hierarchical probabilistic neural network language model</title>
		<author>
			<persName><forename type="first">F</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international workshop on artificial intelligence and statistics</title>
				<meeting>the international workshop on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="246" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Inducing language networks from continuous space word representations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Complex Networks V</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">549</biblScope>
			<biblScope unit="page" from="261" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014-08">August 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive modeling</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">213</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Variation in noun and pronoun frequencies in a sociohistorical corpus of english</title>
		<author>
			<persName><forename type="first">T</forename><surname>Säily</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nevalainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Siirtola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Literary and Linguistic Computing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="188" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Teen use of messaging media</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Schiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Isaacs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ginsberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Gretarsdottir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huddleston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Human Interaction</title>
				<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="594" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Linguistc Ruin? LOL! Instant messaging and teen language</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Tagliamonte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Denis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Speech</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="3" to="34" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Change-point analysis: A powerful new tool for detecting changes</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Understanding semantic change of words over centuries</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Wijaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yeniterzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DETECT</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
