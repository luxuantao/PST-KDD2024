<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attention-driven salient edge(s) and region(s) extraction with application to CBIR</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2009-05-30">30 May 2009</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Songhe</forename><surname>Feng</surname></persName>
							<email>songhe_feng@163.com</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<postCode>100044</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">De</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<postCode>100044</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xu</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<postCode>100044</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Attention-driven salient edge(s) and region(s) extraction with application to CBIR</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2009-05-30">30 May 2009</date>
						</imprint>
					</monogr>
					<idno type="MD5">AF3E75F5867C5B1CB5A510A232E6CD20</idno>
					<idno type="DOI">10.1016/j.sigpro.2009.05.017</idno>
					<note type="submission">Received 8 August 2008 Received in revised form 29 April 2009 Accepted 14 May 2009</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Localized content-based image retrieval Selective visual attention model Salient edge Salient region</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Selective visual attention plays an important role for humans to understand an image by intuitively emphasizing some salient parts. Such mechanism can be well applied in localized content-based image retrieval, due to the fact that in the context of CBIR, the user is only interested in a portion of the image and the rest of the image is irrelevant. Being aware of this, in this paper, the selective visual attention model (SVAM) is incorporated in the CBIR task to estimate the user's retrieval concept. In contrast with existing learning based retrieval algorithms which need relevance feedback strategy to get user's high-level semantic information, the proposed method does not need any user's interaction to provide the training data. From this point of view, our method can be regarded as the purely bottom-up manner while learning based algorithms belong to the top-down manner. Specifically, an improved saliency map computing algorithm is employed first. Then, based on the saliency map, an efficient salient edges and regions detection method is introduced. Moreover, the concepts of salient edge histogram descriptors (SEHDs) and salient region adjacency graphs (SRAGs) are proposed, respectively, for images' similarity comparison. Finally, an integrated strategy is adopted for content-based image retrieval. Experiments show that the proposed algorithm can characterize the human perception well and achieve satisfying retrieval performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With increased digital images available on Internet, efficient indexing and searching becomes an essential issue for large image archives. Content-based image retrieval (CBIR) has been widely developed in the last decade <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">17]</ref>. In the context of CBIR, an image is usually represented by a set of low-level visual features, and most current approaches rely on visual similarity to judge semantic similarity. Since there does not exist the direct correlation between low-level visual features with highlevel semantic concepts, the well-known semantic gap becomes the major difficulty that hinders further development of CBIR systems.</p><p>Most existing CBIR systems adopt the ''query-byexample'' paradigm, i.e., the user is asked to submit a query image to the system in order to retrieve similar images. Usually, the user intents one or several important objects in the query image only and does not care about the background <ref type="bibr" target="#b14">[15]</ref>. Earlier work on CBIR tried to use global low-level visual features (e.g. color histogram) to fulfill the retrieval task. Since the information of the interesting objects may be concealed by the background using global features, the retrieval results are very unsatisfying. One possibility to extract those interesting objects is using region-based approaches. Image segmentation technique is usually utilized to segment the image into regions with similar local visual features. As images often consist of different numbers of regions, how to solve the many-to-many matching problem becomes a prerequisite for measuring image similarity in region level. The representative works on this topic include integrated region matching (IRM) strategy <ref type="bibr" target="#b20">[21]</ref> and the earth mover's distance (EMD) matching approach <ref type="bibr" target="#b12">[13]</ref>. However, since the user typically does not provide any indication of which portion of the image is of interest, identifying important regions in an image is still an open problem.</p><p>Relevance feedback mechanism provides another solution to narrow down the semantic gap via user interaction. It is an interactive process by which the user marks the images retrieved by the system as relevant or irrelevant. According to user's feedback, either the matching criterion or the ideal query may be updated which subsequently improves the retrieval performance. More recently, machine learning techniques have been combined with relevance feedback mechanism to estimate the user's retrieval concept. The representative works include employing SVM <ref type="bibr" target="#b16">[17]</ref> and multiple-instance learning <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> for relevance feedback. Nevertheless, since users are usually unwilling to provide much feedback, the insufficiency of training samples limits the success of relevance feedback.</p><p>By observing the fact that in most cases, the example images provided by users only characterize partial information of users' true search targets, Rahmani et al. <ref type="bibr" target="#b19">[20]</ref> defined localized content-based image retrieval as a CBIR task where the user is only interested in a portion of the image, and the rest is irrelevant. However, such localized CBIR must rely on multiple images (labeled as positive and negative) to learn which portion of the image is of interest. So the query set should be provided by the user using relevance feedback mechanism. Such method can be regarded as a top-down learning manner due to the fact that the region-of-interest is learnt by the query set provided by the user. Since how to learn the user's concept becomes the crucial step to improve the performance of CBIR, here we make an assumption that under most circumstances, the salient parts of an image are usually be consistent with user's retrieval concept. We give an example here, when a user submits an image which describes a tiger in the grass, usually the tiger is of user's retrieval concept where it also occupies the salient part of the image.</p><p>Being aware of this, we try to estimate the user's retrieval concept via a purely bottom-up manner which only relies on the image visual content. Selective visual attention model (SVAM) provides just the mechanism to locate the visual salient parts of an image, which is helpful for extraction of salient objects and reduction of computation complexity of global image-level processing. In contrast to existing learning based methods which need users' interaction, our method exploits the salient parts of the image based on the selective visual attention model. Since our method does not need the relevance feedback mechanism, the drawbacks of relevance feedback can be avoided. The essential difference between <ref type="bibr" target="#b18">[19]</ref> and the proposed method is how to acquire the region-ofinterest in the image. In <ref type="bibr" target="#b18">[19]</ref>, the user's concept is learnt by the query set under multiple-instance learning frame-work. While in the proposed method, the selective visual attention model is incorporated here to discover the salient parts of each image.</p><p>There exist some computational models of visual attention including <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b23">24]</ref>. Most existing works used SVAM just for salient region extraction including our previous work <ref type="bibr" target="#b4">[5]</ref>. Not much attention has been paid to extracting salient edges based on the saliency map. Hu et al. <ref type="bibr" target="#b6">[7]</ref> indicated that without edge information, the saliency map was only a blur map which could only provide the location of the attention, but not exact the scope of the region. This observation prompts us to incorporate edge saliency into the process of CBIR. Specifically, in this paper, an efficient and effective salient edge detection algorithm based on the SVAM is proposed and the concept of salient edge histogram descriptors (SEHDs) is employed. Moreover, salient region is also detected and the concept of salient region adjacency graphs (SRAGs) is introduced. After both global SEHDs and local SRAGs are extracted, the overall similarity is computed as a weighted combination of global and local image level similarity measures incorporating all features. Since both global and local features are integrated for image retrieval, the combination can well represent the human visual perception. The proposed method is evaluated on three very different datasets and we show that it outperforms the state-of-the-art competitive approaches.</p><p>The organization of the paper is as follows. The detail of salient edge and region detection algorithm is proposed in Section 2. Then, the concepts of salient edge histogram descriptors and salient region adjacency graphs are employed and the similarity strategy between images is discussed in Section 3. Experimental results are reported in Section 4. Finally, conclusions will be presented in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Salient edge and region detection based on selective visual attention model</head><p>Selective visual attention is one of the most important functions of human's early vision system. Our gaze can be directly oriented towards salient objects in a cluttered visual scene. This is surely an attractive characteristic for artificial vision systems, as selected attention gaining and shifting will enable efficient pre-processing of the image and fast locating of the most important regions for further processing. Since the motivation of this paper is image retrieval based on the salient edges and regions information, how to create the saliency map is at the basis of our algorithm. We use Fig. <ref type="figure" target="#fig_1">1</ref> to illustrate the motivation of this paper. It gives the flowchart of the proposed algorithm.</p><p>From the figure, one can clearly observe that the saliency map is at the heart of the proposed algorithm. More details are shown in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">An improved saliency value computing based on SVAM</head><p>The key issue in selective visual attention model is the feature contrast theory, which means that attentive parts show highly contrast to the surrounding parts. Several computational models have been explored to address the saliency map generation process <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b23">24]</ref>. Saliency map contains information about where in the image interesting information can be found. High saliency regions are likely to contain objects, while lower saliency regions are associated to background. One of the most important works related to visual attention is proposed by Itti et al. <ref type="bibr" target="#b0">[1]</ref>. By fusing centre-surround differences of multi-scale features, Itti et al. <ref type="bibr" target="#b0">[1]</ref> proposed an integrated approach which combined a saliency map and a dynamical neural network to select attended locations from the saliencies of pixels. This model was shown to be able to locate some objects such as a road sign from a natural image. However, it brings high computational complexity. Ma et al. <ref type="bibr" target="#b1">[2]</ref> only considered the color contrast for computational simplicity, but it might not be robust for the cases where color was not the most useful feature to detect saliency. In addition, Sun et al. <ref type="bibr" target="#b5">[6]</ref> computed the grouping-based saliency map, but how to define the grouping still remains a difficult problem. More recently, Hou et al. <ref type="bibr" target="#b23">[24]</ref> proposed a spectral residual approach to compute the saliency map which is independent of features, categories or other forms of prior knowledge of the objects. However, such method still overlooked the spatial homogeneity of an object.</p><p>Combining these ideas together, we introduce an improved saliency map construction algorithm.</p><p>Specifically, we simply define the multi-scale contrast features as a linear combination of contrasts in the Gaussian image pyramid. For a given color image in the original RGB color space, suppose x is a given pixel in M Â N image, y x is a size of d Â d window centered at x for the computation of contrast. In our algorithm, we take d ¼ 3. Then we compute the feature contrast between x and other pixels in y x . Many features can attract human's attention. Here we follow <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6]</ref> method which considering color, intensity, and orientation features. So the saliency value for the pixel x can be calculated as:</p><formula xml:id="formula_0">SPðxÞ ¼ X L l¼1 X y2yx ðg CI S l CI ðx; yÞ þ g O S l</formula><p>O ðx; yÞÞ</p><p>where y is the pixel belong to y x Á y x is a 3 Â 3 window centered at x Á S CI (x,y) and S O (x,y) denote the colorintensity contrast and orientation contrast between</p><p>x and y. Here, g CI and g O are the weighting coefficients and set to 1 for simplicity <ref type="bibr" target="#b5">[6]</ref>. l denotes the lth level image in the pyramid and the number of pyramid levels L is 3. Note that four color channels R (red), G (green), B (blue), and Y (yellow) channels are used here to compute the color contrast, and the intensity channel is adopted to compute the intensity contrast (see <ref type="bibr" target="#b5">[6]</ref> for more details).</p><p>The Gabor filter with four orientations (01, 451, 901, 1351) is adopted here to calculate the orientation information and further for contrast computation. The example of</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head><p>Fig. <ref type="figure">2</ref>. An example of orientation contrast. From left to right: input image, orientation maps after four orientations Gabor filtering, the contrast maps from the corresponding orientation maps, and the fusion contrast map.  orientation contrast in the original resolution is given in Fig. <ref type="figure">2</ref>.</p><p>Note that in <ref type="bibr" target="#b0">[1]</ref>, nine pyramid levels are constructed to compute the contrast between different levels. In contrast, we only computed the contrast in each pyramid level and linearly combined them together to form the final saliency map. By experiment we found that L ¼ 3 can achieve satisfying results. After the linear combination is finished, we use a Gaussian filter to remove the noise points S x ^¼ S x G x , where G x is a Gaussian filter with the standard deviation s ¼ 1. Fig. <ref type="figure" target="#fig_2">3</ref> gives an example of our proposed computational model to build saliency map.</p><p>In order to validate the effectiveness of the proposed saliency map generation algorithm in a more detailed manner, here we compare our method with previous methods in the field of saliency map computation. Specifically, both Itti's <ref type="bibr" target="#b0">[1]</ref> and Hou's method <ref type="bibr" target="#b23">[24]</ref> are adopted for comparison. Images sampled from the SIVAL data set <ref type="bibr" target="#b18">[19]</ref> with totally 25 different categories and the corresponding saliency maps generated by the above three algorithms are listed below in Fig. <ref type="figure">4</ref>. From the result, we observe that our method provides overall better performance than Itti's method, as well as Hou's method. Since the saliency map generated by the proposed method provides much more exact information where the target object exists in the image, both salient edges and regions which fit with the human's perspective can be well extracted. Moreover, the final retrieval accuracy on the SIVAL data set can be improved subsequently as compared with other two algorithms mentioned above. The details are reported in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Salient edge detection</head><p>Edge information is a basic feature of images since human eyes are sensitive to edge features for image perception. It contains contour information of a valuable object in the image and can be used to represent the image content, recognize the objects and further for object-based image retrieval <ref type="bibr" target="#b13">[14]</ref>. Although some edge detectors such as Canny detector can filter out part of the background edges of an image, not all the extracted edges derived from Canny detector are beneficial to describe the image content which may be composed of a large number of short lines or curves. Some graph theory based methods have been developed to extract salient boundaries from noisy images. Most of the existing methods are mainly based on the well-known Gestalt laws of closure, proximity and continuity <ref type="bibr" target="#b7">[8]</ref>. They provide abundant information on boundary length, tangent directions, and curvatures, which can greatly facilitate the incorporation of advanced perceptual rules like boundary smoothness. Elder et al. <ref type="bibr" target="#b9">[10]</ref> used the shortest-path algorithm to connect fragments to form salient closed boundaries. Wang et al. <ref type="bibr" target="#b8">[9]</ref> proposed the ratio cut algorithm which formulated the salient boundary detection problem into a problem for finding an optimal cycle in an undirected graph. Nevertheless, since gap filling and smoothing are very time-consuming, these algorithms might not be suitable in the scenario of content based image retrieval.</p><p>To avoid the high computational cost of salient closed boundary extraction, in this section, we present a new attention model based approach to extract salient edges from a set of fragments detected from real images. This approach seeks a good balance between the complexity of the saliency definition and the effectiveness of the saliency representation. Selective visual attention provides a mechanism to locate the visual salient part of the image. However, due to the lack of object edge information, the visual saliency map of the image is a blur map <ref type="bibr" target="#b6">[7]</ref>. Since the human visual system is sensitive to the salient part of the image, it would also be sensitive to the edges around the saliency map as well. So here we extend the saliency map for salient edge detection. To achieve this goal, we introduce different edge saliency definition. We would like to define our saliency measure for an edge to be related to its length and the saliency value around it. Specifically, we first use the standard Canny edge detector to obtain the edge map from a given image I. Let E I ¼ fe I 1 ; e I 2 ; . . . ; e I jE I j g be the set of all segments in the initial edge map, where jE I j denotes the total number of segments in I. As mentioned above, not all the edges in the edge map are beneficial to the retrieval task, especially the cluttered background edges. So we define the edge saliency below:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head><formula xml:id="formula_2">SEðe I i Þ ¼ l L Lðe I i Þ þ l S SAðe I i Þ; i ¼ 1; . . . ; jE I j<label>(2)</label></formula><p>where Lðe I i Þ is the length of edge e I i , SAðe I i Þ denotes the average saliency value of edge e i based on the saliency map. l L and l S denote two weights of Lðe I i Þ and SAðe I i Þ. Here, we set l L and l S equal to 0:3 and 0:7, respectively. Both Lðe I i Þ and SAðe I i Þ are normalized to lie between a common interval. SAðe I i Þ is defined as follows. Different from directly compute the saliency value of each pixel on the edge, we consider the 3 Â 3 neighborhood of each pixel on the edge to make the definition be more robust. Let p i n 2 e I i ; n ¼ 1; . . . ; Lðe I i Þ be the set of all pixels on edge e I i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SAðe</head><formula xml:id="formula_3">I i Þ ¼ X Lðe I i Þ n¼1 X x2Y P i n SPðxÞ=Lðe I i Þ<label>(3)</label></formula><p>where Y p i n denotes the 3 Â 3 window centered at pixel p i n ; SPðxÞ is the saliency value of pixel x. After all the saliency values of edges are calculated, an empirical threshold T E ¼ maxðSAðe I i ÞÞ=4 is adopted here. So the final salient edge set is defined as:</p><formula xml:id="formula_4">Y SE ¼ fe I i jSEðe I i Þ4T E ; i ¼ 1; . . . ; jE I jg<label>(4)</label></formula><p>Fig. <ref type="figure" target="#fig_3">5</ref> shows some examples of the extracted salient edges in the images. From the figure, we can clearly see that the proposed salient edge extraction algorithm can get satisfying results. The final salient edges are consistent with human perception.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Salient region detection</head><p>Except for salient edges which provide perceptual features to represent the image content, detection of salient regions in images is also very useful for objectbased image retrieval and browsing applications. Several algorithms based on the selective visual attention model have been developed for this task. The traditional method <ref type="bibr" target="#b22">[23]</ref> incorporated visual attention with seeded region growing to extract the attention objects. However, since finding the best seed areas is a crucial issue in region growing, the blurred saliency map could not always provide such reliable information. Fu et al. <ref type="bibr" target="#b14">[15]</ref> proposed an iterative object popping-out algorithm to obtain the combined regions with the maximal attention value at each iteration step. To the best of our knowledge, it is the first attempt that try to discover the salient regions via popping-out strategy which does not need to calculate the saliency map as the preprocess stage. Nevertheless, Fig. <ref type="figure">4</ref>. The result of our method on the SIVAL dataset in comparison with Itti's method and the result of Hou's method. In each group (a-c) from left to right: (1) the input image, (2-4) saliency map computed by Itti's method, Hou's method and the proposed method, respectively. the computational complexity is 2 N which is computationally expensive especially when the number of segments N is large. So in order to overcome the above shortages, our study focuses on the combination of segmented regions with the attentive properties to determine the salient regions in a given image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1.">Image segmentation using JSEG method</head><p>In order to acquire a region-based signature, a key step is to segment images. A lot of image segmentation algorithms have been proposed. Among these algorithms, JSEG algorithm <ref type="bibr" target="#b21">[22]</ref> is adopted for its flexibility of adjusting the number of regions. After segmentation, the given image I can be represented by a set of regions R I ¼ fr I 1 ; r I 2 ; . . . ; r I jR I j g, where jR I j denotes the segmented region number in I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2.">Salient region(s) detection using maximum entropybased algorithm</head><p>Given the segmented image and the corresponding saliency map, the next step is to determine the salient regions. In this subsection, a robust salient region(s) detection method using maximum entropy-based algorithm is proposed. The maximum entropy-based algorithm has been proved to be efficient for image threshold selection in <ref type="bibr" target="#b11">[12]</ref>. Here we use this principle for salient region detection. For each segmented region r I i in the image I, the region saliency value SRðr I i Þ and region average saliency value ASRðr I i Þ can be calculated as follows:</p><formula xml:id="formula_5">SRðr I i Þ ¼ X x2r I i SPðxÞ ASRðr I i Þ ¼ SRðr I i Þ=Areaðr I i Þ<label>(5)</label></formula><p>where SP(x) denotes the saliency value of pixel x and Areaðr I i Þ represents the area of r I i . Two kinds of regions have to be removed from the salient regions candidate queues. The first kind regions are those that belong to the background of the image with large areas and low average saliency values. The second kind regions are those that are too small due to the oversegmentation with high average saliency values. In order to take these two restrictions into consideration, here we introduce a double-threshold strategy to extract the salient regions from a given image.</p><p>First, in order to remove some regions which have a large area but with the low region average saliency values, we bring a threshold t r . If regions with average saliency values lower than t r , they should be removed from the salient regions candidate queues. Here we set t r equals to the 10% of average saliency values of regions in the whole candidate image database.</p><p>Then, assume the scope of saliency value for each region is between [1yM]. The threshold to determine the salient regions is calculated as follows:</p><formula xml:id="formula_6">T r ¼ arg max Tr À X Tr u¼1 N u P Tr v¼1 N v log N u P Tr v¼1 N v À X M u¼Tr þ1 N u P M v¼Tr þ1 N v log N u P M v¼Tr þ1 N v !<label>(6)</label></formula><p>ARTICLE IN PRESS Fig. <ref type="figure">6</ref>. Two examples of extracted salient regions. where N u is the number of regions with saliency value u, M is the total number of saliency value levels and T r is the threshold.</p><p>After both thresholds t r and T r are determined, the final salient regions are defined as the set of regions whose saliency values satisfy the following restrictions:</p><formula xml:id="formula_7">fr I i jSRðr I i Þ ! T r and ASRðr I i Þ ! t r g<label>(7)</label></formula><p>The examples can be seen in Fig. <ref type="figure">6</ref>. In addition, the attentive objects extracted by our method appear to have an appealing natural contour which is hardly achieved by the existing models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Combining both salient edges and regions for image retrieval</head><p>Once both salient edges and regions are determined based on the SVAM, low-level visual features are extracted, respectively, to represent the image content and integrated together for the similarity measure. Specifically, the concept of salient edge histogram descriptors is introduced as the global features where the concept of salient region adjacency graphs is developed as the local features. Compared with existing exclusively using global or local features for image retrieval, inspired by <ref type="bibr" target="#b3">[4]</ref>, we fuse both local and global features into an integrated comprehensive feature for more accurate image retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Salient edge histogram descriptor feature extraction</head><p>Since edges play an important role for image perception, they are frequently used as a shape describing feature descriptor in content-based image retrieval. The edge histogram descriptor (EHD) is an example, which is one of three normative texture descriptors proposed for MPEG-7. It captures the spatial distribution of edges in an image and has been proven to be useful for image retrieval, especially for natural images with non-uniform textures and clip art images. Five types of edges, namely vertical, horizontal, 451 diagonal, 1351 diagonal, and nondirectional edges (i.e., the edges with no particular directionality), have been utilized to represent the edge orientation.</p><p>Qi and Han <ref type="bibr" target="#b3">[4]</ref> proposed global and semi-global edge histograms based on the EHD. Nevertheless, all the edges derived from Canny detector were adopted which did not distinguish between the salient and non-salient edges in the image, so the retrieval accuracy would not be satisfying. Instead, we propose an improved global salient edge histogram descriptors where only extracted salient edges are taken into consideration. Since most non-salient edges are removed, the SEHDs can well reflect the human perception. The distance between two SEHDs of I A and I B is denoted as</p><formula xml:id="formula_8">Dis E ðI A ; I B Þ ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi X On i¼1 ðEH i I A À EH i IB Þ 2 v u u t<label>(8)</label></formula><p>where EH i I A and EH i IB denote the ith orientation of edge histograms in I A and I B . O n is the total number of orientations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Salient region adjacency graph feature extraction</head><p>Region Adjacency Graph (RAG) is an effective way to represent an image because it provides a ''spatial view'' of the image. In constructing a RAG, each region of the image is represented as a node and each pair of adjacency regions is connected with an edge. The features of the region are recorded as parameters of the node. Fig. <ref type="figure" target="#fig_4">7</ref> shows the corresponding RAG of the segmented image. By using graphs to represent images, the region correspondence estimation problem can be transformed into an inexact graph matching problem. Chang et al. <ref type="bibr" target="#b10">[11]</ref> proposed the concept of Basic RAG and employed the sub-graph isomorphism mechanism for image retrieval. The main motivation behind such mechanism is that the smaller the graph structure, the simpler the matching, and make the matching complexity lower. Specifically, the RAG is firstly decomposed into several small graphs, called Basic RAGs. The decomposed graphs are used to compare the similarity between two images. However, they constructed Basic RAGs for each segmented region without considering the property of saliency. So the computational cost would be very high when the number of segmented regions is large. By observing this, here we propose the concept of salient region adjacency graphs (SRAGs). Specifically, given an image I and its graph representation I G , set each salient node r i in I G as root node, extract node r i and those nodes that are adjacent to the current salient node from the original graph I G as the SRAG i . Therefore, the graph I G with n salient nodes (regions) will be decomposed to n SRAG i . Such method is based on two considerations: on one hand, each SRAG can denote the concept of ''salient group'' which is useful to simulate human's selective visual attention model; on the other hand, using SRAG for image retrieval can effectively reduce the computation cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head><p>Once all the SRAGs are built well, we use an improved sub-graphic isomorphism algorithm presented in <ref type="bibr" target="#b10">[11]</ref> for the graph matching. First, the SRAGs of the query image and all reference SRAGs in the database are constructed and ordered by the root node's salience value. In other words, we put the SRAGs in descending order according to the saliency value of each root node (salient region). Second, an exhaust search method is adopted as the matching strategy. Suppose that the input image I A has m B SRAGs and the image in the database I B has m A SRAGs. The distance between I A and I B is given by:</p><formula xml:id="formula_9">Dis R ðI A ; I B Þ ¼ 1 m A X m A i¼1 min j¼1...mB fw r Â dis R ðR ir I A ; R jr I B Þ þ w b Â dis R ðR ib I A ; R jb IB Þg<label>(9)</label></formula><p>where dis R ðR ir I A ; R jr IB Þ is the distance between two root nodes (salient regions) which separately belong to</p><formula xml:id="formula_10">SRAG I i A and SRAG I j B . dis R ðR ib I A ; R jb IB Þ</formula><p>is the distance between the branch nodes derived from the root nodes. w r and w b are the weighting of root node distance and branch nodes distance, respectively. In the experiment we set w r to 0.9, while w b to 0.1. Since each root node may have different branch nodes, the minimal Hausdorff distance is adopted here to calculate dis R ðR ib</p><formula xml:id="formula_11">I A ; R jb IB Þ, i.e., dis R ðR ib I A ; R jb IB Þ ¼ min ra2R ib I A min r b 2R jb I B disðr a ; r b Þ<label>(10)</label></formula><p>Here we use HSV color features to represent the regions and the Euclidean distance is considered to measure the difference between two regions. Specifically, for each segmented region, we firstly transform the image from the original RGB color space into the HSV color since the HSV color space is more consistent with the human's perspective. The color distribution can be characterized well by its moments. Since most of the color distribution information can be captured by the low-order moments, using only the first and second moments is a good approximation and has been proved to be efficient and effective in representing color distribution of images <ref type="bibr" target="#b24">[25]</ref>.</p><p>Here the color moments features of each channel are adopted here, i.e., 6-dimentional color features are used here to represent the segmented region. Since attentive regions are emphasized during the retrieval process, the satisfying retrieval results can be achieved which fits with the human's perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Fusion of salient edge and region features for image retrieval</head><p>Unlike existing methods exclusively using global features or local features for image retrieval, we fuse both edge and region features into an integrated comprehensive feature for more accurate image retrieval. Concretely, two types of similarity measures, namely edge-based and region-based similarity measures, will be computed and combined to automatically measure the overall similarity between two images. The overall image similarity is measured by a weighted scheme in similarity measures integrating SEHDs and SRAGs matching. Similar to <ref type="bibr" target="#b3">[4]</ref>, we also define a linear combination of the above two similarity measures. The larger weight is assigned to the SRAGs similarity measure since the salient region-based color features capture more details in the image while the smaller weight is assigned to the SEHDs similarity measure. The overall image similarity measure between image I A and I B is computed as:</p><formula xml:id="formula_12">DisðI A ; I B Þ ¼ lDis R ðI A ; I B Þ þ ð1 À lÞDis E ðI A ; I B Þ (11)</formula><p>where l is the weight for SRAGs similarity while 1Àl for SEHDs similarity, respectively. Both Dis R ðI A ; I B Þ and Dis E ðI A ; I B Þ are normalized to [0À1]. In this experiment, we find taking l ¼ 0.7 can achieve better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental results</head><p>We have implemented the proposed algorithm using MATLAB 6.5 on a platform of Pentium IV 3.0 GHZ CPU and 1G memory. We conduct several experiments to demonstrate the proposed method. We firstly illustrate specifically the effectiveness of the proposed saliency map generation method. In addition, in order to measure the importance of the saliency map in the scenario of the retrieval process, we compare the retrieval performance of our algorithm with other two approaches (i.e., Itti's method <ref type="bibr" target="#b0">[1]</ref> and Hou's method <ref type="bibr" target="#b23">[24]</ref>) only differ in the generation of saliency map. We also show the retrieval performance of the proposed algorithm as compared with <ref type="bibr" target="#b18">[19]</ref> using the SIVAL benchmark. Since <ref type="bibr" target="#b18">[19]</ref> can be regarded as a top-down learning based method to perform the localized CBIR, the proposed method is deemed as a pure bottom-up method. Finally, for the purpose of evaluating the feasibility of the proposed saliency based image retrieval algorithm, we measure the retrieval performance as compared with the classical global-based and region-based algorithms using the well-known COREL database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment 1</head><p>To the best of our knowledge, how to measure the effectiveness of the salient edges is still a subjective problem. So here we only focus on the case of salient regions evaluation. More recently, <ref type="bibr" target="#b19">[20]</ref> firstly created a large image database for quantitative evaluation of visual attention algorithms. People may have different ideas about what a salient object in an image is. To address the problem of ''what is the most likely salient object in a given image'', <ref type="bibr" target="#b19">[20]</ref> took a voting strategy by labeling a ''ground truth'' salient object in the image by multiple users. The user is asked to draw a rectangle on each image which encloses the most salient object according to his/ her own understanding. The rectangles labeled by different users usually are not the same. However in this experiment, we chose the image dataset which consists of 5000 highly consistent images, i.e., this dataset of images has less ambiguity of what the salient object is. Fig. <ref type="figure" target="#fig_5">8</ref> shows some images randomly sampled from the database.</p><p>In order to quantitative measure the effectiveness of our proposed salient region detection algorithm, here we use the precision for region-based measurement. Precision is the ratio of correctly detected salient region to the ''ground truth'' salient region:</p><formula xml:id="formula_13">Precision ¼ 1 N X N i¼1 sign c i g i À x (<label>12</label></formula><formula xml:id="formula_14">)</formula><p>where N is the number of images in the database. c i denotes the correctly detected area of salient region and g i is the ground truth of the i-th image. sign( * ) is an indicator function, i.e., sign( * ) returns a value of 1 if its argument is positive and 0 otherwise. The symbol x is a predefined threshold used to declare the overlapping degree between the correctly detected salient regions and the ground truth.</p><formula xml:id="formula_15">sign c i g i À x ¼ 0; if c i g i À xo0 1; if c i g i À x ! 0 8 &gt; &gt; &lt; &gt; &gt; : 9 &gt; &gt; = &gt; &gt; ;<label>(13)</label></formula><p>Fig. <ref type="figure" target="#fig_7">9</ref> gives some results that compared the detected salient regions with the ground truth. Fig. <ref type="figure" target="#fig_1">10</ref> shows the ratio of correctly detected salient regions along with x varies. From the figure we can clearly see that even when x ¼ 0.9, nearly half of the images in the database are correctly dealt with where the detected salient regions are accordance with the ground truth. So based on the quantitative evaluation, the proposed algorithm has been proven to be effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiment 2</head><p>In this subsection, we use the SIVAL (Spatially Independent, Variable Area, and Lighting) data set obtained from <ref type="bibr" target="#b18">[19]</ref> to conduct the retrieval performance. The SIVAL benchmark which is created by <ref type="bibr" target="#b18">[19]</ref> consists of 25 different objects, and 10 scenes. There are 6 different images taken for each object-scene pair, and a total of 1500 images in the database. SIVAL emphasizes the task of localized CBIR through nearly identical scenes which only vary by the localized target objects. It ensures there is one and only one target object in each image. For each object class the same physical object was used in all scenes. It is also a difficult data set in that the scenes are highly diverse and often complex. Furthermore, the objects may occur anywhere spatially in the image and also may be photographed at a wide-angle or close up or with different orientations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head><p>Fig. <ref type="figure" target="#fig_9">11</ref> gives the example of images and the corresponding salient region maps randomly selected from the 10 different scenes of category ''cokecan''. Most of the results are satisfying except the one on the third row in Fig. <ref type="figure" target="#fig_9">11</ref> which shows an example of failure salient region extraction. Note that the process of detecting salient region from a given image is determined by both the corresponding saliency map and the segmentation result. Since in that given image, the visual appearance of the foreground object (i.e., the red cokecan) is apparently similar to the background (i.e., the red chair), the  unsatisfying segmentation result makes the failure of the final salient region detection. Fig. <ref type="figure" target="#fig_8">12</ref> shows some images with their corresponding salient edge and region maps randomly sampled from the 25 categories in a more detailed manner. Rahmani et al. <ref type="bibr" target="#b18">[19]</ref> used labeled images in conjunction with a multiple-instance learning algorithm to first identify the desired object and re-weigh the features, and then to rank images in the database using a similarity measure that is based upon individual regions within the image. Rahmani et al. <ref type="bibr" target="#b18">[19]</ref> used a top-down learning manner which needed the user to label a set of positive and negative images. Given such query set, the multipleinstance learning algorithm was then adopted to learn the target concept, which could be regarded as the users' retrieval concept. In contrast, the proposed method can be regarded as a bottom-up manner which was based on the image visual content only without any user's interaction. The proposed method can be regarded as an explicit bottom-up manner for image retrieval, while <ref type="bibr" target="#b18">[19]</ref> needs the user's interaction to provide a set of positive and negative images, in order to learn the user's concept under MIL framework. Such manner can be regarded as a pure  top-down manner. Although they adopt totally different methods, the intrinsic aim is almost the same, i.e., try to estimate the user's concept close to human perception.</p><p>We compare the performance of the propose method and ACCIO for all 25 object categories of SIVAL. Consistent with existing works, we use AUC as the performance measure. In the image retrieval, the area under the receiver-operating characteristic (ROC) curve (AUC) is a good measure of the retrieval performance. The ROC curve plots the true positive rate as a function of the false positive rate. The AUC is equivalent to the probability that a randomly chosen positive image will be ranked higher than a randomly chosen negative image <ref type="bibr" target="#b18">[19]</ref>. For ACCIO, 8 random positive and 8 random negative examples from the image dataset are placed in the query image set, which are used for target concept detection under MIL framework. The retrieval performance is presented in Fig. <ref type="figure" target="#fig_11">13</ref>. It shows that the proposed method and ACCIO have similar retrieval accuracy. Nevertheless, our method did not need the users' interaction to provide the query set which contains both positive and negative samples. Furthermore, the computational cost of our method is apparently lower than ACCIO since ACCIO needed the iterative computing under diverse density framework to acquire the target concept.</p><p>In addition, in order to measure the importance of the saliency map in the scenario of the retrieval process, we also compare the retrieval performance of our algorithm with other two approaches, i.e., Itti's <ref type="bibr" target="#b0">[1]</ref> and Hou's <ref type="bibr" target="#b23">[24]</ref> methods only differ in the generation of saliency map. Since the SIVAL benchmark ensures there exists one target object in each image, it is still used here to conduct the comparison. We also use AUC as the performance measure. Fig. <ref type="figure" target="#fig_10">14</ref> shows the evaluation results of three algorithms on the SIVAL image data set. From the experimental results, our approach achieves totally better retrieval performance as compared with Itti's and Hou's methods. As mentioned before, favorable retrieval results can be achieved since salient information is emphasized well during the retrieval, which fits with the human's perspective.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experiment 3</head><p>In this subsection, we evaluate the performance of image retrieval. The retrieval experiment was conducted on the 5,000 COREL images dataset. The data set was provided by <ref type="bibr" target="#b15">[16]</ref> in which there are 5,000 images from 50 Corel Photo CDs with each category contains 100 images. In terms of image representation, all images were normalized to size 168 Â 192 or 192 Â 168. For the stage of saliency edges and regions computation, it can be finished off-line in order not to affect the retrieval performance. More results are shown in Fig. <ref type="figure" target="#fig_12">15</ref>, from which we can conclude that the proposed method can well provide the salient information.</p><p>To evaluate the efficiency of the proposed algorithm, we compared its performance with the following methods.</p><p>(1) The global 72-bin HSV color histogram method, noted as ''HSV 72-bin''; (2) Integrated region matching proposed in SIMPLIcity system <ref type="bibr" target="#b20">[21]</ref>, noted as ''IRM''; (3) Salient region only, noted as ''SR Only''; (4) Salient edge only, noted as ''SE Only''; <ref type="bibr" target="#b4">(5)</ref> The proposed fusion of salient region &amp; salient edge information, noted as ''Proposed Fusion'';</p><p>For the ''IRM'' method, since the region matching strategy is the essential of the SIMPLIcity system, here we only adopt its region matching algorithm and still use the JSEG based image segmentation algorithm instead of the adoptive k-means algorithm used in SIMPLIcity system. We use each image in the dataset as a query. The remaining 4999 images are considered as candidate images. The results are averaged over the 5,000 queries. In all cases, performance is measured with precision and recall, a classical measure widely used by the image retrieval community. We use the standard measures, precision and recall, in different forms to evaluate the results. The precision is the fraction of the returned images that is indeed relevant for the query while the recall is the fraction of relevant images that is returned by the query. Fig. <ref type="figure" target="#fig_1">16</ref> gives some examples of query results. The average precision and recall rates are seen in Fig. <ref type="figure" target="#fig_4">17</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head><p>From the experiment, we can see that, once the combination of salient edges and regions information is considered, the retrieval performance of the proposed algorithm is slightly better than the method proposed in our previous work <ref type="bibr" target="#b4">[5]</ref>. Since IRM <ref type="bibr" target="#b20">[21]</ref> did not consider the saliency variances among regions, the retrieval performance is slightly worse than the proposed method and <ref type="bibr" target="#b4">[5]</ref>. Besides, we indicate that the combination of salient edges and regions features is still useful for other purposes, such as salient object extraction and recognition. We will extend the algorithm for further application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper presents a novel selective visual attention based image retrieval algorithm which integrates both salient edges and regions information for retrieval. The selective visual attention model (SVAM) is extended for  not only salient region but also salient edge extraction. After both salient edges and regions are extracted, the concept of salient edge histogram descriptors (SEHDs) and salient region adjacency graphs (SRAGs) are proposed and combined for further image retrieval. The advantage of the combination lies in that the extracted features can represent the salient parts of an image and characterize the human perception well. Experimental results have demonstrated the proposed algorithm outperforms existing CBIR systems. In contrast to the existing learningbased CBIR systems which need users' interaction, the proposed method provides a purely bottom-up manner to fulfill the localized CBIR. There are many directions for future work. We believe that the proposed method can be further improved with application to image semantic annotation and object recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Contents lists available at ScienceDirect journal homepage: www.elsevier.com/locate/sigpro Signal Processing ARTICLE IN PRESS 0165-1684/$ -see front matter &amp; 2009 Elsevier B.V. All rights reserved. doi:10.1016/j.sigpro.2009.05.017</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The flowchart of the proposed algorithm.</figDesc><graphic coords="3,38.36,367.10,347.83,151.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. An example of saliency map construction. From left to right: input images at multiple scales, contrast maps at multiple scales, and the saliency map from linearly combining the contrasts at multiple scales.</figDesc><graphic coords="4,41.25,480.51,465.84,187.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Two examples of extracted salient edge maps.</figDesc><graphic coords="6,113.23,398.51,321.84,129.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. An example of RAG construction.</figDesc><graphic coords="7,109.24,463.98,321.84,212.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Sample images of the salient object image database.</figDesc><graphic coords="9,73.80,422.42,392.40,253.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>5 Fig. 10 .</head><label>510</label><figDesc>Fig.10. Precision of the extracted most salient region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Comparison of our algorithm and ground-truth.</figDesc><graphic coords="10,113.77,56.57,320.40,431.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 12 .</head><label>12</label><figDesc>Fig.<ref type="bibr" target="#b11">12</ref>. Images with their corresponding salient region and edge maps randomly sampled from 25 categories.</figDesc><graphic coords="11,37.27,322.51,465.84,353.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Images randomly sampled from category ''cokecan'' under 10 different scenes.</figDesc><graphic coords="11,37.65,56.57,219.60,219.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 14 .</head><label>14</label><figDesc>Fig.<ref type="bibr" target="#b13">14</ref>. Comparisons of retrieval results using different saliency map generation algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Retrieval performance comparison between two methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. More results of the proposed method on Corel image database. From left to right: (1) the input image, (2) the segmented image, (3) the Canny edge map, (4) the saliency map, (5) the extracted salient region and (6) the extracted salient edge, respectively.</figDesc><graphic coords="13,73.80,221.55,392.40,446.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 16 .Fig. 17</head><label>1617</label><figDesc>Fig. 16. Retrieval from Corel5000, the first column shows the query image and columns 2-5 the top 4 database matches.</figDesc><graphic coords="14,113.23,255.41,321.84,221.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>.</head><label></label><figDesc>Fig. 17. The comparison of average precision and recall rates, respectively.</figDesc><graphic coords="14,310.28,516.74,190.87,134.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="5,38.40,56.57,463.32,299.09" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>S. Feng et al. / Signal Processing 90 (2010) 1-15</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to thank Jian Sun <ref type="bibr" target="#b19">[20]</ref>, Sally A. Goldman <ref type="bibr" target="#b18">[19]</ref> and Kobus Barnard <ref type="bibr" target="#b15">[16]</ref> for kindly providing their datasets available. This work was supported by National Nature Science Foundation of China (60803072, 90820013), National High Technology Research and Development Program of China (2007AA01Z168) and Science Foundation of Beijing Jiaotong University (2007XM008).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Contrast-based image attention analysis by using fuzzy growing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>ACM Multimedia</publisher>
			<biblScope unit="page" from="374" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Image retrieval: ideas, influences, and trends of the new age</title>
		<author>
			<persName><forename type="first">R</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="60" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A novel fusion approach to content-based image retrieval</title>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="2449" to="2465" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A novel region-based image retrieval algorithm using selective visual attention model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">3708</biblScope>
			<biblScope unit="page" from="235" to="242" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Object-based visual attention for computer vision</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="page" from="77" to="123" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Salient object extraction combining visual attention and edge information</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Structural saliency: the detection of globally salient structures using a locally connected network</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shashua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="90" to="94" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Salient closed boundary extraction with ratio contour</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kubota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Siskind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="546" to="561" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Computing contour closure</title>
		<author>
			<persName><forename type="first">J</forename><surname>Elder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="399" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Region-based image retrieval using edgeflow segmentation and region adjacency graph</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia &amp; Expro</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="1883" to="1886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Thresholding using two-dimensional histogram and fuzzy entropy principle</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="732" to="735" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An efficient and effective regionbased image retrieval framework</title>
		<author>
			<persName><forename type="first">F</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="699" to="709" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Edge-based structural features for contentbased image retrieval</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="457" to="468" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Attention-driven image interpretation with application to image retrieval</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1604" to="1621" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Object recognition as machine translation: learning a lexicon for a fixed image vocabulary</title>
		<author>
			<persName><forename type="first">P</forename><surname>Duygulu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="97" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A survey of content-based image retrieval with high-level semantics</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="262" to="282" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A dynamic user concept pattern learning framework for content-based image retrieval</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics-Part C: Applications and Reviews</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="772" to="783" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Localized content based image retrieval</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krettek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fritts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGMM International Workshop on Multimedia Information Retrieval</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="227" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to detect a salient object</title>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SIMPLIcity: semantics-sensitive integrated matching for picture libraries</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wiederhold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Recognition and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="947" to="963" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised segmentation of colortexture regions in images and video</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Learning</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="800" to="810" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised extraction of visual attention objects in color images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Ngan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="145" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Saliency detection: a spectral residual approach</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Color texture moments for contentbased image retrieval</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="929" to="932" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
