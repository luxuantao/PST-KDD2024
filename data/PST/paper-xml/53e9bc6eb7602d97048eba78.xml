<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data Access History Cache and Associated Data Prefetching Mechanisms</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yong</forename><surname>Chen</surname></persName>
							<email>chenyon1@iit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Illinois Institute of Technology</orgName>
								<address>
									<postCode>60616</postCode>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Surendra</forename><surname>Byna</surname></persName>
							<email>sbyna@iit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Illinois Institute of Technology</orgName>
								<address>
									<postCode>60616</postCode>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Computing Division</orgName>
								<orgName type="institution">Fermi National Accelerator Laboratory</orgName>
								<address>
									<postCode>60510</postCode>
									<settlement>Batavia</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Data Access History Cache and Associated Data Prefetching Mechanisms</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>C.4 [Performance of Systems]: Design Studies Performance</term>
					<term>Design</term>
					<term>Verification Data access performance</term>
					<term>Memory performance</term>
					<term>Data prefetching</term>
					<term>Prefetching simulation</term>
					<term>Cache memory</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Data prefetching is an effective way to bridge the increasing performance gap between processor and memory. As computing power is increasing much faster than memory performance, we suggest that it is time to have a dedicated cache to store data access histories and to serve prefetching to mask data access latency effectively. We thus propose a new cache structure, named Data Access History Cache (DAHC), and study its associated prefetching mechanisms. The DAHC behaves as a cache for recent reference information instead of as a traditional cache for instructions or data. Theoretically, it is capable of supporting many well known history-based prefetching algorithms, especially adaptive and aggressive approaches. We have carried out simulation experiments to validate DAHC design and DAHC-based data prefetching methodologies and to demonstrate performance gains. The DAHC provides a practical approach to reaping data prefetching benefits and its associated prefetching mechanisms are proven more effective than traditional approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>While microprocessor performance improved by 52% a year until 2004 and has been increasing by 25% from then, memory speed is only increasing by roughly 9% each year <ref type="bibr" target="#b8">[9]</ref> . The performance disparity between processor and memory keeps expanding. Deeper memory hierarchies were introduced to bridge this gap <ref type="bibr" target="#b8">[9]</ref> . Each memory level closer to the processor is smaller and faster than the next lower level. The rationale behind memory hierarchy design is the principle of data locality, which states that programs tend to reuse data and instructions which are accessed recently (temporal locality) or to access those items whose addresses are close to one another (spatial locality). However, when applications lack locality due to a working set size larger than the cache and/or non-contiguous memory accesses, cache memories are ineffective.</p><p>The data prefetching approach was thus proposed to reduce the processor stall time when applications lack temporal or spatial locality. As the name indicates, data prefetching is a technique to fetch data in advance. The essential idea is to observe data referencing patterns, then to speculate future references, and to fetch the predicted reference data closer to the processor before the processor demands them. Numerous studies have been conducted and many strategies have been proposed for data prefetching <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>[8] <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref>[19] <ref type="bibr" target="#b22">[23]</ref> . These studies concluded that prefetching is a promising solution to reducing access latency. The ultimate goal of data prefetching is to reduce access delay. However, the performance gain (how much we can reduce access delay) depends on many factors, such as prefetch coverage and accuracy. While computing (c) 2007 Association for Computing Machinery. ACM acknowledges that this contribution was authored or co-authored by a contractor or affiliate of the U.S. Government. As such, the Government retains a nonexclusive, royalty-free right to publish or reproduce this article, or to allow others to do so, for Government purposes only. SC07 November 10-16, 2007, Reno, Nevada, USA (c) 2007 ACM 978-1-59593-764-3/07/0011â€¦$5.00 capability is still increasing with a much faster pace than memory performance, more aggressive prefetching algorithms are desired, which provide wider coverage and higher accuracy. In the meantime, application features dominate referencing patterns.</p><p>There is no single universal prefetching algorithm suitable for all applications. It is beneficial to support adaptive algorithms based on data access histories.</p><p>As the processor-memory performance gap increases, application features demand faster access to data, and hardware technologies evolve, we argue that it is time to dedicate one cache for prefetching to fully harvest benefits of aggressive, adaptive and other data prefetching strategies. We thus propose a dedicated prefetching cache structure, named Data Access History Cache (DAHC), and present data prefetching mechanisms to address this fundamental issue. The rest of this paper is organized as follows. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">DATA ACCESS HISTORY CACHE</head><p>The main purpose of the proposed DAHC is to track recent data access histories and maintain the correlations from different perspectives. Those histories and correlations are valuable information for data prefetching, especially for aggressive and adaptive strategies. In existing work, only very limited correlations are maintained, which limits the prefetching accuracy, coverage, and aggressiveness. Moreover, they only target a specific algorithm and have difficulty applying to diverse applications.</p><p>However, with advances of processor technologies and the rapidly growing performance gap between processor unit and memory unit, it would be beneficial to trade computing power for a reduction in data access latency. With this idea, we propose to dedicate a cache (DAHC) for tracking data accesses and letting the processing unit perform comprehensive data prefetching. Therefore, processor stall time due to data accesses could be reduced and the overall system performance would be increased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Design and Methodologies</head><p>The key idea of the DAHC is that history-based prefetching algorithms must rely on correlations within either program counter stream or data address stream, or both. Thus, the DAHC is designed to have three tables: one data access history table (DAH)   and two index tables (PC index table and address index table). The DAH table accommodates history details, while the PC index table and the address index table maintain correlations from the PC and data address stream viewpoints respectively. A prefetching implementation can access these two tables to obtain the required correlations as necessary. Figure <ref type="figure">1</ref> illustrates the general design of DAHC and a high-level view of how it can be applied to support various prefetching algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1. DAHC general design and high-level view</head><p>The detailed design of the DAHC is shown in Figure <ref type="figure" target="#fig_2">2</ref>     In this case, a complex structured stride pattern of (4, 8, 4, 8) is detected for instruction 403C20 after examining address 7FFF8000, 7FFF8004, 7FFF800C, 7FFF8010 and 7FFF8018; therefore, data at address 7FFF801C and 7FFF8024 could be prefetched to memory in advance to avoid cache misses when 7FFF801C and 7FFF8024 are accessed as predicted. Such a complex structured pattern is a general case of stride pattern. However, the conventional stride prefetching approach <ref type="bibr" target="#b2">[3]</ref> is unable to detect it without the DAHC support. This example also shows an address correlation between 100003F8 and 100003FA, which is often observed and utilized for prediction in the Markov prefetching algorithm <ref type="bibr" target="#b9">[10]</ref> . The following section discusses data prefetching methodologies based on the proposed DAHC. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Stride Prefetching</head><p>Stride prefetching predicts future references based on strides of recent references. This approach monitors data accesses and detects constant stride access patterns. Stride prefetching is usually implemented with a Reference Prediction Table (RPT) <ref type="bibr">[3][7]</ref> as shown in Figure <ref type="figure" target="#fig_5">4</ref>. RPT acts like a separate cache and holds data reference information of recent memory instructions. Since stride prefetching involves tracking the difference between two consecutive accesses and predicting the next access based on the stride, it is straightforward to design such an RPT table for stride prefetching implementation. Each entry in RPT is the instruction address, and it contains the last access address, the stride and the state transition information to predict future accesses. The right part of Figure <ref type="figure" target="#fig_5">4</ref> shows the state transitions. Once a pattern enters steady state or remains at steady state, which means a constant stride is found, a prefetch is triggered. The prefetched data address is simply calculated by adding the stride to the previous address.</p><p>Although RPT is effective for capturing constant stride of data accesses, it has several limitations. The first limitation is that RPT only calculates the stride between two consecutive accesses. It is hard to detect variable strides and impossible to find complex patterns, such as a repeating pattern of length n (e.g., 2, 4, 8, 2, 4, 8, â€¦). Those complex patterns are common in user-defined data types. The second limitation is that RPT only tracks the last two accesses and omits many useful history references; thus, the accuracy in detecting patterns is relatively low. Those issues are addressed well in our proposed DAHC structure. Since DAHC tracks a large set of working histories, it is capable of detecting variable strides. Those detailed histories can also be used to  First, when a data access happens at monitoring level and is tracked by added DAHC component and related logic (see Section 3.1 for more details), the instruction address is searched for in the PC index table. If the instruction address does not match any entry in the PC index table, which means it is the first time that we see this instruction address in current working window, no prefetching action is triggered. If the instruction address matches one entry (it will match only one entry because the entries in index tables are unique), we follow the index pointer to traverse previous access addresses and detect whether a strided pattern or a structured pattern is present. If a pattern is detected, one or more data blocks are prefetched to data cache or a separate prefetch cache. The prefetching degree and prefetching distance can vary depending on the actual implementation. Finally, a new entry with this data access is created and inserted into the DAH table. The PC index table and address index table are updated correspondingly. Notice that the approach described above is enhanced stride prefetching with detection of variable and complex stride patterns. The conventional stride prefetching <ref type="bibr">[3][7]</ref> can be implemented by detecting constant strides only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Markov Prefetching</head><p>Markov prefetching is another classical prefetching strategy. The Markov prefetching algorithm builds a state transition diagram through past data accesses. The probability of each transition from one state to another state is calculated and updated dynamically.</p><p>The algorithm assumes the future data accesses might repeat the histories. Therefore, once a new data access is captured, the future references predicted from the state transition diagram are prefetched in advance. For instance, Figure <ref type="figure">5</ref> shows the correlation table and state transition diagram for the data access stream 7FFF8000, 1010FF00, 10B0C600, 7FFF8000, 7FF3CA00, 7FFF8000, 10B0C600 and 7FF3CA00.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 5. Markov prefetching correlation table and state transition diagram</head><p>The conventional Markov prefetching strategy treats all history accesses with the same weight. In practice, we usually give the highest weight to the latest access. This approach is essentially a combination of Markov model and LAST model <ref type="bibr" target="#b5">[6]</ref> . The rationale is that the next data access is most probably the one that had followed the current access in the nearest past. For example, if we have a sequence of accesses to address A, B, A, C, D, A, then it is likely that the next access is C. With DAHC support, Markov prefetching can be implemented as follows. First, the data reference address is searched for within the address index following the index and address pointer as shown in Figure <ref type="figure" target="#fig_6">6</ref>. Each address next to these entries we visit is a prefetching candidate because each of this address was immediately accessed following the present access address in histories. Similar as in stride prefetching, different prefetching degree and prefetching distance can be supported depending on the actual implementation. If the prefetching degree is greater than one, we fetch multiple continuous data addresses following these entries we visit. We can also increase prefetching distance to initiate multiple visits. Continuing with the previous example and as shown in Figure <ref type="figure" target="#fig_6">6</ref>   It is an easy task to implement aggressive strategies with the DAHC because the DAHC is designed to support aggressive strategies naturally. The Multi-Level Difference Table (MLDT) prediction algorithm is such a representative aggressive strategy <ref type="bibr" target="#b20">[21]</ref> . This prediction strategy forms a difference table of depth d of recent data accesses. Figure <ref type="figure">7</ref> demonstrates an example of the difference table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Aggressive Prefetching Strategies</head><p>If a constant difference can be found in the first depth, which means a constant stride is found among data access histories, then the k th future access from access A r is predicted as</p><formula xml:id="formula_0">* A A k B r k r = + + ,</formula><p>where B is the constant difference among accesses. Some polynomial formula is used to predict the future access for general cases. For example, if a constant difference is found in the third depth, the future access is predicted as</p><formula xml:id="formula_1">* ( 1) * * 1 2 2 k k A A k B C M D r k r r r k + = + + + + âˆ’ âˆ’ .</formula><p>Here similarly as in the conventional stride prefetching case. Figure <ref type="figure" target="#fig_3">3</ref> shows an example where a complex structure pattern (4, 8, 4, 8) is detected when we perform the MLDT prefetching with the DAHC.</p><formula xml:id="formula_2">M k = 2 * ( 1) * ( 2) 6 k k k k âˆ’ âˆ’ + , where k = 1, 2â€¦<label>Figure</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Implementation Issues</head><p>The DAHC is straightforward and an effective prototype design of a prefetching-dedicated structure. For instance, if we suppose a DAHC with 1024 entries is implemented, which is a reasonable window size for a regular working set, then the required DAHC size is about 22KB. Our experiments simulated DAHC functionalities, and the conclusion is that DAHC is feasible in terms of hardware implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SIMULATION AND PERFORMANCE ANALYSIS</head><p>We have conducted simulation experiments to study the feasibility of our proposed generic prefetching-dedicated cache, DAHC, for various prefetching strategies. Stride prefetching, Markov prefetching and MLDT aggressive prefetching algorithms were selected for simulation. This section discusses simulation details of DAHC-based data prefetching and presents the analysis results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Simulation Methodology</head><p>The SimpleScalar simulator <ref type="bibr" target="#b0">[1]</ref> was enhanced with data prefetching We chose the sim-outorder simulator for our experiments. Figure <ref type="figure" target="#fig_10">8</ref> shows our modified SimpleScalar simulator architecture. We  Another improvement is in instruction issue phase. During this phase, when we have available issue bandwidth, i.e. if there is idle bandwidth after issuing normal instructions, the prefetch queue is walked through and prefetch instructions are allocated with functional units to fetch the predicted data to data cache. Second, the memory module was modified to introduce a prefetch command to the memory component in addition to a load and a store command. The cache module was augmented with prefetch access handlers. Prefetch accesses can be handled similarly to load instructions except prefetch accesses do not cause any exceptions.</p><p>Some additional statistics counters were added for measuring the effectiveness of prefetching.  <ref type="table" target="#tab_6">1</ref> shows the configuration of our simulator.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Matrix Multiplication Simulation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">SPEC CPU2000 Benchmark Simulation</head><p>We conducted several sets of SPEC CPU2000 benchmark <ref type="bibr" target="#b23">[24]</ref> simulation for performance evaluation. Twenty-one of the total twenty-six benchmarks were tested successfully in our experiments. The other five benchmarks (apsi, facerec, fma3d, perlbmk and wupwise) had problems working under the SimpleScalar simulator (even in the original simulator) and did not finish the test.</p><p>The target of the first set of experiments was to compare the performance gain of traditional RPT-based stride prefetching approach and enhanced DAHC-based stride prefetching approach.</p><p>Figure <ref type="figure" target="#fig_12">9</ref> shows the experimental results. The first bar in each test represents the level-one cache miss rate of the base case in which no prefetching was performed. The second and the third bar represent the miss rate in the case of RPT-based conventional stride prefetching and enhanced DAHC-based stride prefetching, respectively. As shown in Figure <ref type="figure" target="#fig_12">9</ref>, the traditional approach reduced miss rates, and the enhanced approach reduced miss rates further. The rationale comes from that, with DAHC support, enhanced stride prefetching is able to detect complex structured patterns, and in addition, the prediction accuracy was improved through observing more histories. In contrast, many important and helpful histories were not considered and not fully utilized in traditional stride prefetching based on RPT.  With DAHC support, the prefetching accuracy increases by taking advantage of all available history information. As we can see from</p><p>Figure <ref type="figure">11</ref>, the replacement rate only increased slightly in DAHC-supported data prefetching. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RELATED WORK</head><p>There are extensive research efforts in data prefetching area. Data prefetching is frequently classified as software prefetching and hardware prefetching <ref type="bibr" target="#b21">[22]</ref> . Software prefetching instruments prefetch instructions to the source code either by a programmer or by a complier during the optimization phase. Recent work in helper threads <ref type="bibr" target="#b18">[19]</ref> , software-based speculative precomputation <ref type="bibr">[12] [17]</ref> and data-driven multithreading <ref type="bibr" target="#b17">[18]</ref> are such examples. The techniques include simple prefetching, unrolling the loop and software pipelining <ref type="bibr" target="#b21">[22]</ref> . Software prefetching is usually used for large amount of loops. Such loops are very common in scientific computation, and these loops often exhibit poor cache utilization but have predictable memory-referencing patterns, and thus provide excellent prefetching opportunities.</p><p>Hardware-based prefetching does not require modifications to binary or source code and can benefit directly existing binary code.</p><p>There is no need for programmer or compiler's intervention.</p><p>Commonly used hardware prefetching techniques include sequential prefetching, stride prefetching and Markov prefetching.</p><p>Sequential prefetching <ref type="bibr">[4][5]</ref> fetches consecutive cache blocks by taking advantage of locality. The one-block-lookahead (OBL) approach automatically prefetches the next block when an access of a block is initiated. However, the limitation of this approach is that the prefetch may not be initiated early enough prior to processor's demand for the data to avoid a processor stall. To solve this issue, a variation of OBL prefetching, which fetches k blocks (called prefetching degree) instead of one block, is proposed. Another variation is called adaptive sequential prefetching, which varies prefetching degree k based on the prefetching efficiency. The prefetching efficiency is a metric defined to characterize a program's spatial locality at runtime. The stride prefetching approach <ref type="bibr" target="#b2">[3]</ref> observes the pattern among strides of past accesses and thus predicts future accesses. Various strategies have been proposed</p><p>based on stride prefetching, and these strategies maintain a reference prediction table (RPT) to keep track of recent data accesses. RPT provides a practical approach to implement stride prefetching, but the limitation is that only constant strides are recognizable. To capture repetitiveness in data reference addresses, Markov prefetching <ref type="bibr" target="#b9">[10]</ref> was proposed. This strategy assumes the history might repeat itself among data accesses and build a state transition diagram with states denoting an accessed data block. The probability of each state transition is maintained so that the most probable predicted data are prefetched in advance and the least probable predicted data references can be dropped from prefetching.</p><p>Other recent efforts in hardware prefetching include Zhou's dual-core execution (DCE) approach <ref type="bibr" target="#b22">[23]</ref> , Ganusov et al's future execution (FE) approach <ref type="bibr" target="#b7">[8]</ref> , Sun et al's data push server architecture <ref type="bibr" target="#b20">[21]</ref> and Solihin et al.'s memory-side prefetching <ref type="bibr" target="#b19">[20]</ref> .</p><p>DCE and FE were proposed specifically for multi-core architecture. proposed a global history buffer for data prefetching in [14] and [15]. The similarity between their work and our work is that both attempt to facilitate data prefetching with a single structure. Their approach has demonstrated the feasibility of supporting different prefetching algorithms and achieved considerable performance gains. However, our work has substantial differences with theirs.</p><p>First of all, we focus on providing a generic and dedicated cache for prefetching purposes and we argue that such a generic cache is a must to fully achieve prefetching benefits that hide access delay.</p><p>Second, the global history buffer scheme is unable to support various algorithms simultaneously at runtime, and therefore, switching to different algorithms adaptively is impossible. Our work fully supports many history-based algorithms, as well as adaptive approaches, because we maintain two stream viewpoints concurrently. Third, we focus on supporting both algorithms' adaptability and aggressiveness. We believe that this strategy will help researchers fully utilize prefetching advantages. To our best knowledge, there is no other work targeting these directions.</p><p>Another work closely related to this study is the instruction pointer based prefetcher developed by Intel <ref type="bibr" target="#b6">[7]</ref> . The IP prefetcher is a RPT-like prefetcher; thus, it suffers the limitation that it only works for constant stride prefetching. Nevertheless, the Intel IP prefetcher provides us helpful guidelines in implementing the DAHC in hardware.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS AND FUTURE WORK</head><p>As memory performance lags far behind processor speed, data access delay has a severe impact on overall system performance.</p><p>This study targeted to resolve this issue through fully exploiting data prefetching benefits with a generic and prefetching-dedicated cache. Our main contributions in this study include: 1) introducing a novel concept of a prefetching-dedicated cache considering both We have demonstrated the power of the DAHC in supporting diverse prefetching algorithms in this study. In our future research, we plan to extend this work in various aspects. One of them is adapting to different prediction algorithms based on the data requirements of applications and making such decisions dynamically at runtime. We plan to define efficiency criteria for prefetching algorithms and to provide feedback for different algorithms and then to choose the best algorithm at runtime.</p><p>Another of our future works will be to devise even more comprehensive prefetching strategies to further explore the DAHC's potentials.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Section 2 introduces the proposed DAHC design and methodology to serve multiple prefetching algorithms. Section 3 discusses our simulation experiments and performance results in detail to verify DAHC design and to demonstrate the potential performance improvement brought by DAHC-based data prefetching. Section 4 reviews related works and compares them with our approaches. Finally, we summarize our current work and discuss future work in Section 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>through an example. The DAH table consists of PC, PC_Pointer, Addr, Addr_Pointer and State fields. PC and Addr fields store the instruction address and data address separately. The PC_Pointer and Addr_Pointer point to an entry where the last access from the same instruction or the last access of the same address is located. Therefore, PC_Pointer and Addr_Pointer link all accesses from the instruction stream and data stream perspectives. This design offers the fundamental mechanism to detect potential correlations and access patterns. The State field maintains state machine status used in prefetching algorithms. Various algorithms could occupy different bits of this field for maintaining their own states. The length of this field is implementation dependent, and the usage is decided by prefetching strategies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 ,</head><label>2</label><figDesc>Figure 2, the DAH table captured four data accesses, three of them issued by instruction 403C20 (stored in the PC field) and one by instruction 4010D8. The instruction 403C20 accessed data at address 7FFF8000, 7FFF8004 and 7FFF800C in sequence, which is shown through the Addr and PC_Pointer fields. The instruction 403C20 and 4010D8 are also stored in the PC index table, and the corresponding Index field tracks the latest access from the DAH</figDesc><graphic url="image-3.png" coords="3,316.74,177.00,246.48,114.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3</head><label>3</label><figDesc>Figure 3. DAHC snapshot 2.2 DAHC-based Data Prefetching Mechanisms</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>improve the accuracy of stride detection. Moreover, DAHC makes detection of complex structure patterns possible, as discussed in previous examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Reference prediction table and state transition diagram Stride prefetching can be implemented with the DAHC as follows.</figDesc><graphic url="image-4.png" coords="4,53.88,239.16,243.00,83.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Markov prefetching with DAHC</figDesc><graphic url="image-6.png" coords="5,53.88,237.90,241.50,85.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Since the DAHC maintains recent accesses in detail and the correlation among them, it is more powerful than supporting traditional prefetching approaches such as stride prefetching and Markov prefetching. It can support many other history-based prefetching strategies like more aggressive prefetching algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>functionality to demonstrate how different prefetching algorithms can be implemented with the DAHC. The SimpleScalar tool set provides a detailed and high-performance simulation of modern processors. It takes binaries compiled for SimpleScalar architecture as input and simulates their execution on provided processor simulators. It has several different execution-driven processor simulators, ranging from extremely fast functional simulator to a detailed and out-of-order issue simulator, called the sim-outorder simulator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>introduced two new modules: DAHC module and Prefetcher module. The DAHC module simulated the functionality of the proposed DAHC. Monitored data accesses were stored in the DAHC. The DAHC cache controller is responsible for updating all three tables. The Prefetcher module implemented the prefetching logic and different prefetching algorithms. In this module, a prefetch queue, similar to the ready queue of the original sim-outorder simulator, was created to store prefetch instructions.Prefetch instructions are similar to load instructions with a few exceptions. The first exception is that the effective address of each prefetch instruction is computed based on a data access pattern and prefetching strategy instead of computing the address using an integer-add functional unit. Another exception is that when prefetch instructions proceed through the pipeline, it is not necessary to walk through writeback and commit stages, and prefetch instructions do not cause any exceptions (prefetch instructions are silent). These similarities and differences provide us the guidelines to handle prefetch instructions. The implementation of prefetching strategies based on the DAHC follows the discussion given in Section 2.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Enhanced SimpleScalar simulator In addition to these two new modules, several existing modules were enhanced to incorporate the DAHC and data prefetching functionality. First, the simulator core module was revised to support the DAHC and Prefetcher modules. The pipeline was modified to have prefetching logic. The first improvement is each ready-to-issue load instruction is tracked to DAHC after the memory scheduler checks data dependencies. The prefetcher performs access pattern detection based on prefetching algorithms</figDesc><graphic url="image-8.png" coords="6,316.68,475.44,243.06,90.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>We first set up experiments to test the enhanced SimpleScalar simulator with DAHC-based data prefetching functionality. The prefetching strategy was set as the MLDT algorithm. Matrix multiplication was selected as the application because it is widely used in scientific computing and the correctness of its output results is easy to verify. The size of matrices was set as 200 200 Ã— . We randomly generated the input, conducted simulation and then compared the output result with standard output to verify the correctness of the enhanced simulator. The correctness was also validated through checking the number of instructions (normal instructions) issued by the original and the enhanced version. The simulation results are shown in Table 2. The simulation time is the elapsed time for simulation (how much time the simulator spent in simulating). The results confirm that the enhanced SimpleScalar simulator worked correctly, and cache misses were reduced significantly through DAHC-based data prefetching.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Stride prefetching with RPT vs. stride prefetching with DAHC Figure 10 compares L1 cache miss rates of all tested SPEC CPU2000 benchmarks for the base case and three prefetching cases. This set of experiments showed that DAHC-based data prefetching worked well and the cache miss rates were reduced obviously in most cases. Among the three prefetching strategies, both stride and aggressive MLDT algorithms reduced a large ratio of miss rates. The MLDT algorithm was slightly better than stride prefetching because it searches more levels to find patterns among accesses. The Markov prefetching performed worse than stride and MLDT algorithms in most cases. One possible reason is that Markov prefetching requires a large set of states to characterize the probability of transition among accesses well. If the state diagram space is limited, it is hard for the Markov prefetching to guarantee the accuracy and coverage. Figure 11 illustrates L1 cache replacement rate in these tests. Cache pollution is considered a side effect of prefetching. An incorrect prediction</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>hardware technologies and application feature trends; 2) providing the design of a prefetching cache structure DAHC, and simulating its functionalities with an enhanced SimpleScalar simulator; and 3) presenting DAHC-associated data prefetching methodologies and demonstrating its support for prefetching algorithms with three representative examples, stride prefetching, Markov prefetching and an aggressive prefetching algorithm, MLDT algorithm. Our simulation experiments showed that the DAHC is feasible and that DAHC-based data prefetching achieved considerable cache miss rate reductions and IPC improvements.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The PC index table has two fields, PC and Index. The PC field represents the instruction address, which is a unique index in this table. The Index field records the entry of the latest data access in the DAH table from the instruction stored in the correspondent PC field. It is the connection between the PC index table and the DAH table. The address index table is similarly defined. For instance, in</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Figure 2. DAHC blueprint: PC index table, address index table and DAH table Figure</head><label></label><figDesc>table, which are entry 3 and 1 respectively. The address index table keeps each accessed address and the latest entry, as shown in the bottom left of the figure, thus connecting all the data accesses on the basis of the address stream. Both PC index table and address index table can be implemented in a variety of ways including a fully associative structure and a set-associative structure. Notice that DAHC design is general and it does not imply any restriction to the system environment. It works in CMP or SMT environment, as well as in multiple applications environment. 3 shows a snapshot of the DAHC after capturing more data accesses. The PC index table, address index table and DAH table are updated. The latest access entries for instruction 403C20 and 4010D8 become index 9 and 8, respectively. The address accessed and the corresponding entry are updated in the address index table.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>table. If the newly accessed address does not match any existing entries, it is simply inserted into the DAH table. The PC index and address index table are also updated. If it matches an entry in the address index table, then we insert it to the DAH table and walk through the DAH table</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>, if a new data access address is 10B0C600, then a new entry is inserted into the DAH table at index 7, and the address index table is updated. After we walk through the DAH table following index 7, pointer 5 and pointer 2, data at address 7FF3CA00 and 7FFF8000 are prefetch candidates if we set prefetching degree as one and prefetching distance as two. Notice that Markov prefetching builds state transition based on data addresses. It does not need to use the state field.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>accesses to first level cache and to serve as a L1 cache prefetcher. It can also be placed at the second level cache and serves as a L2 cache prefetcher only. The straightforward design makes the implementation uncomplicated. The hardware implementation of the DAHC should be a specialized physical cache, like victim cache or trace cache. The PC index table and the address index table can be implemented with any associativity such as 2-way or 4-way. Since the index tables usually have less valid entries than the DAH table, it is unlikely that some entry is replaced due to a conflict miss. Even if a conflict miss occurs, it does not affect the correctness except discarding some access history. The DAH table can be implemented with a special structure where history information can be stored row by row and each row can be located by using its index. The logic to fill/update the DAHC comes from the cache controller. The cache controller traps data accesses at the monitored level and keeps a copy of the access information in the DAHC. If the DAH table is full, a victim</figDesc><table /><note>It is a cache for data access information compared with conventional cache for instructions or data. The proposed DAHC can be placed at different levels for various desired data prefetching. For instance, it can be used to track all entry will be selected and evicted out. The PC index table and the address index table are updated as well for consistency. The required DAHC size for normal applications' working set is trivial.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 1 . Simulator configuration</head><label>1</label><figDesc></figDesc><table><row><cell>Issue width</cell><cell>4 way</cell></row><row><cell>Load store queue</cell><cell>64 entries</cell></row><row><cell>RUU size</cell><cell>256 entries</cell></row><row><cell>L1 D-cache</cell><cell>32KB, 2-way set associative, 64 byte</cell></row><row><cell></cell><cell>line, 2 cycle hit time</cell></row><row><cell>L1 I-cache</cell><cell>32KB, 2-way set associative, 64 byte</cell></row><row><cell></cell><cell>line, 1 cycle hit time</cell></row><row><cell>L2 Unified-cache</cell><cell>1MB, 4-way set associative, 64 byte</cell></row><row><cell></cell><cell>line, 12 cycle hit time</cell></row><row><cell>Memory latency</cell><cell>120 cycles</cell></row><row><cell>DAHC</cell><cell>1024 entries</cell></row><row><cell>Prefetch queue</cell><cell>512 entries</cell></row><row><cell cols="2">3.2 Experimental Setup</cell></row><row><cell cols="2">We use the Alpha-ISA and configure the simulator as a 4-way issue</cell></row><row><cell cols="2">and 256-entry RUU processor. The level one instruction cache and</cell></row><row><cell cols="2">data cache are split. We configure L1 data cache as 32KB, 2-way</cell></row><row><cell cols="2">with 64B cache line size. The latency is 2 cycles. L2 unified cache</cell></row><row><cell cols="2">is configured as 1MB, 4-way with 64B cache line size. The latency</cell></row><row><cell cols="2">of L2 cache is 12 CPU cycles. The DAHC is set as 1024 entries, and</cell></row><row><cell cols="2">the replacement algorithm is FIFO. Both index tables are simulated</cell></row><row><cell cols="2">with 4-way associative structures. We assume each DAHC access,</cell></row></table><note>such as a lookup within index tables, costs one CPU cycle. This should be a reasonable assumption for a small 4-way cache. We also assume a traversal within DAH table costs one cycle. If a prefetching algorithm needs to traverse multiple locations to make predictions, it consumes multiple cycles. The prefetch queue is set as 512 entries. Table</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 2 . Simulation results for matrix multiplication</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell># of</cell><cell>Simulation</cell><cell>L1 cache</cell><cell>L1</cell></row><row><cell></cell><cell>instructions</cell><cell>Time</cell><cell>misses</cell><cell>replacements</cell></row><row><cell>Original</cell><cell cols="2">622140213 12633</cell><cell cols="2">1031047 1030023</cell></row><row><cell cols="3">Enhanced 622140213 13469</cell><cell>28772</cell><cell>1084326</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Figure 11. L1 cache replacement rate of SPEC CPU2000 benchmarks</head><label></label><figDesc></figDesc><table><row><cell cols="13">additional overhead compared to stride prefetching. Another</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="13">interesting fact shown in Figure 12 is that Markov strategy</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="13">outperformed the other two in the bzip2, eon and vortex</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="13">benchmarks. These facts confirmed that different strategies are</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="13">desired for different applications to obtain the best prefetching</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="13">benefits. It is necessary to support diverse algorithms and adapt to</cell><cell></cell><cell></cell><cell>30.00%</cell></row><row><cell cols="13">them dynamically based on distinct application features, and our</cell><cell></cell><cell></cell><cell>25.00%</cell></row><row><cell cols="13">proposed DAHC provides the essential structure support for adaptive strategies. Algorithm designers can utilize DAHC</cell><cell></cell><cell>L1 Cache Miss Rate</cell><cell>10.00% 15.00% 20.00%</cell></row><row><cell cols="13">functionalities to come up with and implement adaptive</cell><cell></cell><cell></cell><cell>5.00%</cell></row><row><cell cols="3">algorithms.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.00%</cell><cell>ammp</cell><cell>applu</cell><cell>art</cell><cell>bzip2</cell><cell>crafty</cell><cell>eon</cell><cell>equake</cell><cell>galgel</cell><cell>gap</cell><cell>gcc</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Base Case</cell><cell>Strided w ith DAHC</cell><cell>Markov w ith DAHC</cell><cell>MLDT w ith DAHC</cell></row><row><cell></cell><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>3.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>12.00%</cell></row><row><cell></cell><cell>2.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10.00%</cell></row><row><cell>IPC</cell><cell>0 0.5 1 1.5 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">L1 Cache Miss Rate</cell><cell>4.00% 6.00% 8.00%</cell></row><row><cell></cell><cell></cell><cell>ammp</cell><cell>applu</cell><cell>art</cell><cell>bzip2</cell><cell>crafty</cell><cell>eon</cell><cell cols="2">equake</cell><cell>galgel</cell><cell>gap</cell><cell>gcc</cell><cell></cell><cell></cell><cell>2.00%</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Base Case</cell><cell>Strided with DAHC</cell><cell cols="2">Markov with DAHC</cell><cell></cell><cell cols="2">MLDT with DAHC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.00%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>gzip</cell><cell>lucas</cell><cell>mcf</cell><cell>mesa</cell><cell>mgrid</cell><cell>parser</cell><cell>sixtrack</cell><cell>sw im</cell><cell>tw olf</cell><cell>vortex</cell><cell>vpr</cell></row><row><cell></cell><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Base Case</cell><cell>Strided w ith DAHC</cell><cell>Markov w ith DAHC</cell><cell>MLDT w ith DAHC</cell></row><row><cell></cell><cell>3.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>IPC</cell><cell>2.5 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Figure 10. L1 cache miss rate of SPEC2000 benchmarks</cell></row><row><cell></cell><cell>1.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">35.00%</cell></row><row><cell></cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">30.00%</cell></row><row><cell></cell><cell>0</cell><cell>gzip</cell><cell>lucas</cell><cell>mcf Base Case</cell><cell cols="2">mesa Strided with DAHC mgrid</cell><cell cols="2">parser Markov with DAHC sixtrack</cell><cell cols="2">swim MLDT with DAHC twolf</cell><cell>vortex</cell><cell>vpr</cell><cell>L1 Replacement Rate</cell><cell cols="2">10.00% 15.00% 20.00% 25.00%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">5.00%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.00%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>a m m p</cell><cell>a p p lu</cell><cell>a rt</cell><cell>b z ip 2</cell><cell>c ra ft y</cell><cell>e o n</cell><cell>e q u a k e</cell><cell>g a lg e l</cell><cell>g a p</cell><cell>g c c</cell><cell>g z ip</cell><cell>lu c a s</cell><cell>m c f</cell><cell>m e s a m g ri d p a rs e r s ix tr a c k s w im</cell><cell>tw o lf v o rt e x</cell><cell>v p r</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Base Case</cell><cell>Strided w ith DAHC</cell><cell>Markov w ith DAHC</cell><cell>MLDT w ith DAHC</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Figure 12 shows the overall IPC (Instructions Per Cycle)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">improvement brought by three prefetching strategies: stride,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Markov and MLDT prefetching based on DAHC. The</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">experimental results demonstrated that the IPC value was</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">improved considerably in most cases. The figure also reveals that</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">even though MLDT achieved the best cache miss rate reduction in</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">almost all cases, the IPC improvement was not always best. The</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">stride prefetching outperformed the MLDT in the applu, crafty,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">gcc, gzip, lucas, mcf, parser, swim, twolf and vpr benchmarks.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">This is because MLDT involves more prefetching overhead for its</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">aggressiveness due to more DAHC accesses. When we measured</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">the overall system performance gain in IPC value, it paid for its</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Figure 12. IPC value of SPEC CPU2000 benchmarks simulation</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Our proposed generic and prefetching-dedicated DAHC cache was designed to resolve these issues. There are a few recent efforts in this area. Nesbit and Smith</figDesc><table><row><cell>proactively upon prediction. It is usually distinguished as push</cell></row><row><cell>based prefetching from traditional pull based prefetching.</cell></row><row><cell>Without the benefit of programmer or compiler hints, the</cell></row><row><cell>effectiveness of hardware prefetching largely relies on the accuracy</cell></row><row><cell>of prediction strategies. Incorrect prediction brings useless blocks</cell></row><row><cell>into cache, consumes memory bandwidth and might cause cache</cell></row><row><cell>pollution. To increase prefetching accuracy and coverage, hardware</cell></row><row><cell>prefetching strategies should be more aggressive. On the other hand,</cell></row><row><cell>it is desired that data prefetching could support various algorithms</cell></row><row><cell>and make dynamic selections because patterns are decided by</cell></row><row><cell>application features and different prefetching algorithms are</cell></row><row><cell>required for assorted applications.</cell></row><row><cell>They use idle cores to pre-execute future loop iterations to warm up</cell></row><row><cell>cache (bring data to cache in advance). The data push server</cell></row><row><cell>architecture utilizes a separate processing unit such as a separate</cell></row><row><cell>core to conduct heuristic prefetching. The memory-side</cell></row><row><cell>prefetching approach uses a memory processor residing within</cell></row><row><cell>main memory to observe data access histories and prefetch data</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">ACKNOWLEDGEMENTS</head><p>We would like to thank the anonymous reviewers for their helpful comments and the shepherd for providing detailed and valuable suggestions. This research was supported in part by National Science Foundation under NSF grant EIA-0224377, CNS0509118, and CCF-0621435. Fermi National Laboratory is operated by Fermi Research Alliance, LLC under Contract No. DE-AC02-07CH11359 with the United States Department of Energy.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Evaluating Future Microprocessors: the SimpleScalar Tool Set</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bennett</surname></persName>
		</author>
		<idno>1308</idno>
		<imprint>
			<date type="published" when="1996-07">July, 1996</date>
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin-Madison Computer Sciences</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Impulse: Building a Smarter Memory Controller</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Carter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 5 th International Symposium on High Performance Computer Architecture</title>
				<meeting>of the 5 th International Symposium on High Performance Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Effective Hardware-Based Data Prefetching for High Performance Processors</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Baer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Computers</title>
		<imprint>
			<biblScope unit="page" from="609" to="623" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fixed and Adaptive Sequential Prefetching in Shared-memory Multiprocessors</title>
		<author>
			<persName><forename type="first">F</forename><surname>Dahlgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>StenstrÃ¶m</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1993 International Conference on Parallel Processing</title>
				<meeting>1993 International Conference on Parallel essing</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="I56" to="I63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sequential Hardware Prefetching in Shared-Memory Multiprocessors</title>
		<author>
			<persName><forename type="first">F</forename><surname>Dahlgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>StenstrÃ¶m</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="733" to="746" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Host Load Prediction Using Linear Models. Cluster Computing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dinda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>O'hallaron</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Inside Intel Core Microarchitecture and Smart Memory Access</title>
		<author>
			<persName><forename type="first">J</forename><surname>Doweck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>Intel White Paper</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Future Execution: A Hardware Prefetching Technique for Chip Multiprocessors</title>
		<author>
			<persName><forename type="first">I</forename><surname>Ganusov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burtscher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 14 th Annual International Conference on Parallel Architectures and Compilation Techniques</title>
				<meeting>of the 14 th Annual International Conference on Parallel Architectures and Compilation Techniques</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Computer Architecture: A Quantitative Approach. The 4 th edition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hennessy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Prefetching Using Markov Predictors</title>
		<author>
			<persName><forename type="first">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grunwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24 th Annual Symposium on Computer Architecture</title>
				<meeting>the 24 th Annual Symposium on Computer Architecture<address><addrLine>Denver-Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-04">June 2-4 1997</date>
			<biblScope unit="page" from="252" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An architecture for software-controlled data prefetching</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Klaiber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGARCH Comput. Arch. News</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="43" to="53" />
			<date type="published" when="1991-05">May. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Post-Pass Binary Adaptation Tool for Software-Based Speculative Precomputation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hoflehner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lavery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGPLAN Conference on Programming Language Design and Implementation</title>
				<meeting>ACM SIGPLAN Conference on Programming Language Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reducing DRAM latencies with an integrated memory hierarchy design</title>
		<author>
			<persName><forename type="first">W.-F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Reinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 7 th International Symposium on High Performance Computer Architecture</title>
				<meeting>of the 7 th International Symposium on High Performance Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2001-01">Jan 2001</date>
			<biblScope unit="page">312</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Prefetching Using a Global History Buffer</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Nesbit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 10 th Annual International Symposium on High Performance Computer Architecture (HPCA-10)</title>
				<meeting>of the 10 th Annual International Symposium on High Performance Computer Architecture (HPCA-10)<address><addrLine>Madrid, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-02">Feb. 2004</date>
			<biblScope unit="page" from="96" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Prefetching Using a Global History Buffer</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Nesbit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="90" to="97" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">MicroLab: A Case for the Quantitative Comparison of Micro-Architecture mechanisms</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mouchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Temam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 37 th International Symposium on Microarchitecture</title>
				<meeting>of the 37 th International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Compiler Orchestrated Pre-fetching via Speculation and Predication</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rodric</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 11 th International Conference on Architectural Support for Programming Languages and Operating Systems</title>
				<meeting>of the 11 th International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Speculative data-driven multithreading</title>
		<author>
			<persName><forename type="first">A</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Sohi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 7 th International Symposium on High Performance Computer Architecture</title>
				<meeting>of the 7 th International Symposium on High Performance Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Design and Implementation of A Compiler Framework for Helper Threading on Multi-Core Processors</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kalogeropulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tirumalai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 14 th International Conference on Parallel Architectures and Compilation Techniques</title>
				<meeting>of 14 th International Conference on Parallel Architectures and Compilation Techniques</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Using a User-Level Memory Thread for Correlation Prefetching</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Solihin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Torrellas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 8 th International Symposium on Computer Architecture</title>
				<meeting>8 th International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improving Data Access Performance with Server Push Architecture</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Byna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the NSF Next Generation Software Program Workshop in IPDPS&apos;07</title>
				<meeting>of the NSF Next Generation Software Program Workshop in IPDPS&apos;07</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">When caches aren&apos;t enough: Data prefetching techniques</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Vanderwiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Lilja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="23" to="30" />
			<date type="published" when="1997-07">Jul 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dual-Core Execution: Building a Highly Scalable Single-Thread Instruction Window</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 14 th International Conference on Parallel Architectures and Compilation Techniques</title>
				<meeting>of the 14 th International Conference on Parallel Architectures and Compilation Techniques</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Standard Performance Evaluation Corporation</title>
		<ptr target="http://www.spec.org/" />
	</analytic>
	<monogr>
		<title level="m">SPEC Benchmarks</title>
				<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
