<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Parameter-Efficient Tuning Makes a Good Classification Head</title>
				<funder ref="#_jzTpU98">
					<orgName type="full">National Science Foundation for Distinguished Young Scholars</orgName>
				</funder>
				<funder ref="#_dBGkbCq">
					<orgName type="full">Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_dXYAk4w">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-30">30 Oct 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhuoyi</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University ? Shandong Woman University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University ? Shandong Woman University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yanhui</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University ? Shandong Woman University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qingsong</forename><surname>Lv</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University ? Shandong Woman University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
							<email>jietang@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University ? Shandong Woman University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Parameter-Efficient Tuning Makes a Good Classification Head</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-30">30 Oct 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2210.16771v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, pretrained models revolutionized the paradigm of natural language understanding (NLU), where we append a randomly initialized classification head after the pretrained backbone, e.g. BERT, and finetune the whole model. As the pretrained backbone makes a major contribution to the improvement, we naturally expect a good pretrained classification head can also benefit the training. However, the final-layer output of the backbone, i.e. the input of the classification head, will change greatly during finetuning, making the usual head-only pretraining (LP-FT) ineffective. In this paper, we find that parameter-efficient tuning makes a good classification head, with which we can simply replace the randomly initialized heads for a stable performance gain. Our experiments demonstrate that the classification head jointly pretrained with parameter-efficient tuning consistently improves the performance on 9 tasks in GLUE and SuperGLUE. * Equal contribution. Codes are at https://github. com/THUDM/Efficient-Head-Finetuning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Fine-tuning is the most prevalent paradigm to leverage pretrained language models for the best performance on specific tasks <ref type="bibr" target="#b25">(Kenton and Toutanova, 2019)</ref>. Usually, a task-oriented classification head is grafted onto the final layer of the pretrained backbone, mostly Transformers <ref type="bibr" target="#b54">(Vaswani et al., 2017)</ref>, and then the whole model is trained for the downstream task. Compared with training a large Transformer from scratch, the pretrained backbone has already learned to extract features about grammar, semantics and high-level understanding, making it easy to adapt the model for NLP tasks.</p><p>Although the good initial weights of the pretrained backbone are the key factor of the effectiveness, the initial weights of the classification heads are, however, largely under-explored. To the best of our knowledge, the usage of randomly initialized classification heads is still the overwhelmingly dominant choice in NLP. Recently, LP-FT <ref type="bibr" target="#b27">(Kumar et al., 2022)</ref> finds that on many computer vision benchmarks, the performance can be promoted, especially for out-of-distribution (OOD) data, if we first only finetune the linear classification head (probe) with the pretrained backbone frozen, and then finetune the whole model. Does this kind of head-first finetuning technique also work in NLP? Furthermore, since the principle is basically to reduce the change of features during finetuning (Figure <ref type="figure" target="#fig_0">1</ref>), is it possible to upgrade LP-FT to a general method to improve generalization, instead of only OOD setting? In this paper, we give a positive answer via parameter-efficient tuning.</p><p>To answer these questions, we need to first understand why a well-initialized head helps. LP-FT explains that with a randomly initialized head, the extracted features from the backbone change greatly during fitting the in-distribution (ID) samples in the training set, but the extracted features for OOD samples change very little. Finally, the training makes the features inconsistent for ID and OOD samples. A good classification head from linear probing can reduce the intensity of change in the extracted features during finetuning, and thus keep the feature consistency.</p><p>Since the overfitting-like explanation above only relates to the training samples, it should still be reasonable if we replace the "ID/OOD samples" by "training/test samples" in the explanation. However, both our and the original LP-FT experiments suggest no significant improvement for the (ID) downstream task itself, why is that?</p><p>In our opinion, the inconsistency is attributed to that the head-only tuning in LP-FT cannot obtain a good enough classification head if the pretrained task is greatly different from the downstream task. The head-only tuning usually has much worse performance than fully funetuning, so that even with the pretrained head, in the fully finetuning stage the weights in the backbone still need to change greatly to a totally different local minima for better performance.</p><p>In this paper, we define the criterion of good classification heads as those with which the pretrained backbone can be optimized to a near-optimal point with little change. The recently rising parameterefficient tuning methods <ref type="bibr" target="#b33">(Li and Liang, 2021;</ref><ref type="bibr" target="#b36">Liu et al., 2021;</ref><ref type="bibr" target="#b12">Guo et al., 2021;</ref><ref type="bibr" target="#b65">Zaken et al., 2021;</ref><ref type="bibr" target="#b21">Hu et al., 2021)</ref> aim to approach the performance of fully finetuning by only changing a very small part of parameters, and thus fulfil our desires.</p><p>Our experiments show that the parameterefficient tuning methods, such as Prefix-tuning <ref type="bibr">(Li and</ref><ref type="bibr">Liang, 2021), BitFit (Zaken et al., 2021)</ref> and <ref type="bibr">LoRA (Hu et al., 2021)</ref> indeed make good classification heads. Initialized by these good clas-sification heads, the performances on 9 tasks of GLUE <ref type="bibr" target="#b58">(Wang et al., 2018)</ref> and SuperGLUE <ref type="bibr" target="#b57">(Wang et al., 2019)</ref> are consistently better than or equal to that of direct finetuning or LP-FT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Background</head><p>Consider a pretrained language model backbone f (x; ? f ), where x is the input text and ? f is the model parameters. f servers as a feature extractor. For a specific task, a learnable classification head g(f (x; ? f ); ? g ) takes the extracted features as input and makes prediction for the downstream tasks. In previous research, three methods to train the models are widely used:</p><p>Finetuning. In traditional fully fine-tuning, ? g is randomly initialized and trained simultaneously with the pretrained backbone ? f . Linear Probing. Linear probing refers to headonly tuning, mainly used for evaluating the selfsupervised learning representations in computer vision. The classification head ? g is randomly initialized and trainable, but the backbone ? f is frozen during training.</p><p>Linear-probing finetuning (LP-FT). LP-FT is two-stage tuning method recently proposed by <ref type="bibr" target="#b27">Kumar et al. (2022)</ref>. The stage 1 of LP-FT is linear probing, and the stage 2 is fully finetuning with the classification head initialized as the trained head in the stage 1. This method proves to be better than finetuning or linear probing for OOD samples.</p><p>Parameter-efficient Tuning. Recently, a collection of new tuning methods for pretrained models aims to approach the finetuning performance by only changing a small part of parameters, which is called parameter-efficient tuning. It includes methods by limiting the trainable parameters, e.g. <ref type="bibr">BitFit (Zaken et al., 2021)</ref>, and methods by adding a small trainable module, e.g. <ref type="bibr">LoRA (Hu et al., 2021)</ref> and Prefix-Tuning <ref type="bibr" target="#b33">(Li and Liang, 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Efficient Head Finetuning</head><p>We propose the Efficient Head Finetuning (EH-FT) to step further about the finetuning paradigm, which can improve the downstream task in a simple way.</p><p>EH-FT is also a two-stage method similar to LP-FT, but replacing the linear-probing in the first stage as parameter-efficient tuning. Figure <ref type="figure" target="#fig_1">2</ref> illustrates the difference between these methods. Specifically, the procedure of EH-FT can be described as follows:</p><p>Stage 1. We finetune the model consisting of the pretrained backbone ? 0 f and a randomly initialized classification head ? 0 g using a parameter-efficient tuning algorithm. At the end of training, we restore the pretrained backbone back to ? 0 f and only keep the trained head ? * g .</p><p>Stage 2. We fully finetune the model consisting of the pretrained backbone ? 0 f and the classification head ? * g from the first stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Working Principle</head><p>In this section, we will detail the working principle via comparing the optimizing path during finetuning with different initial heads, which is illustrated in Figure <ref type="figure" target="#fig_2">3</ref>.</p><p>The initialization of classification head matters.</p><p>As observed by <ref type="bibr" target="#b9">Dodge et al. (2020)</ref> via varying random seeds, some specific heads are consistently significantly better than others on a group of binary classification tasks. Our explanation is that if the pretrained backbone cannot be quickly optimized to a nearby local optimum, the features will change a lot, and thus affect the performance according to <ref type="bibr" target="#b27">Kumar et al. (2022)</ref>. However, most randomly initialized heads will back-propagate very chaotic gradients to the backbone, causing a large feature change and finally catastrophic forgetting and overfitting. Therefore, we need a way to stably find a good pretrained classification head for finetuning, which is also the motivation of LP-FT. LP-FT neglects the difference between the pretraining and downstream task. The experiments in the paper of LP-FT <ref type="bibr" target="#b27">(Kumar et al., 2022)</ref> are mainly about finetuning a contrastive pretrained backbone, e.g. CLIP <ref type="bibr" target="#b44">(Radford et al., 2021)</ref>, for classification tasks. However, when we apply generative pretrained backbones, e.g. <ref type="bibr">BERT (Kenton and Toutanova, 2019)</ref> and MAE <ref type="bibr" target="#b16">(He et al., 2022)</ref>, to classification tasks in NLP, the linear probing performs poorly because the extracted features are specialized for mask prediction and might not contain enough information for classification <ref type="bibr" target="#b56">(Wallat et al., 2020;</ref><ref type="bibr" target="#b64">Yosinski et al., 2014)</ref>. Even though we can find the best classification head w.r.t. the pretrained backbone in the stage 1, the features are learnt to change greatly to adapt for the downstream task, which goes against the theory of LP-FT.</p><p>Efficient head ensures a nearby near-optimal point for finetuing the backbone weights. Although parameter-efficient tuning is originally proposed to reduce memory usage, a recent study <ref type="bibr" target="#b55">(Vu et al., 2022)</ref> finds that these methods restrict the trainability of parameters and thus helps overcome catastrophic forgetting. Our experiments also show that most parameter-efficient tuning methods change the feature much less than finetuning, e.g. 20.0 for Prefix-Tuing verus 42.5 for finetuning measured in the L 2 distance in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>This property means that it could be easy to reach around the parameter-efficient tuning weights during a fully finetuning, and parameter-efficient tuning usually performs near optimally and much better than linear probing.</p><p>Here we can hypothesize that EH-FT works in a way as follows:</p><p>? In stage 1, parameter-efficient tuning acts like a weak surrogate of the fully finetuning on the backbone, and learns to add the most important information to the features for the downstream task. The efficient head adapts to these slightly changed features.</p><p>? In stage 2, the efficient head will guide the backbone quickly towards the region around the parameter-efficient tuning result by backpagation, because the region only differs in a small part of parameters and very near, and thus finally converges to a near local optimum.</p><p>As a proof, The L 2 distance (as measured in the in Figure <ref type="figure" target="#fig_0">1</ref>) between EH-FT (Prefix-Tuing) Stage 1 and Stage 2 is only 27.4 1 , less than the 36.7 of LP-FT. EH-FT indeed converges to a local optimum near the parameter-efficient tuning result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>SuperGLUE <ref type="bibr" target="#b57">(Wang et al., 2019</ref>) is a benchmark which contains 8 difficult natural language understanding tasks, including BoolQ, CB, COPA, Mul-tiRC, ReCoRD, RTE, WiC, and WSC. In our experiments, we exclude WSC, ReCoRD, MultiRC because they rely on a heavy pipeline instead of a single classification head on RoBERTa-Large to get a satisfying result. We further supply 4 widely used GLUE datasets, MRPC, COLA, QNLI and STS-B into our benchmark. We report our results on the dev set following most previous works <ref type="bibr" target="#b35">(Liu et al., 2022;</ref><ref type="bibr" target="#b61">Xu et al., 2021)</ref>.</p><p>For COLA, we evaluate performance using matthews correlation coefficient. For MRPC, we use F1 score. For STS-B, we use Pearson correlation coefficients with RoBERTa-Large and Spearman correlation coefficients with BERT-Large in order to be consistent with the baseline (Liu et al.,   1 Similarly, 27.9 for EH-FT (BitFit).</p><p>2019b; Kenton and Toutanova, 2019). We use accuracy in other tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Setup</head><p>Models We mainly use RoBERTa-Large <ref type="bibr">(Liu et al., 2019b)</ref> as the pretrained backbone in our experiments, and mostly follow the best hyperparameter settings from the original paper. To exhibit the generality of EH-FT, we also conduct some experiments on BERT-Large. The pretrained weights are obtained from HugggingFace <ref type="bibr" target="#b60">(Wolf et al., 2019)</ref>, and the codes are bases on the SwissArmyTransformer<ref type="foot" target="#foot_0">2</ref> framework. We set the classification head as a 2-layer MLP mapping from the hidden dimension of the model to 2,048, and then to the number of classes. Experiments are executed on DeepSpeed library and NVIDIA A100 GPUs with mixed precision floating point arithmetic. We report the average results over 4 random seeds. We implement BitFit, LoRA and Prefix-Tuning as the parameter-efficient module in Stage 1.</p><p>Hyperparameters Learning rate plays an important role in model training. However, the best learning rates on various datasets tend to be consistent. we fix the learning rate to 1e-5 for RoBERTa-Large (except for the WIC where we use 3e-5), and 3e-5 for Bert-Large. We set the batch size=32, and use AdamW <ref type="bibr" target="#b39">(Loshchilov and Hutter, 2017)</ref> optimizer with ? 1 =0.9, ? 2 =0.98, =1e-6, weight decay=0.1. Following the original finetuing strategy of BERT, we adopt a warmup for the early 10% iterations and then a linear learning rate decay. To fairly compare with two-stage method and the direct finetuning, we need to keep the same entire training iterations. This is implemented by recording the iterations to convergence for each dataset, and dividing the total number of iterations into 10% and 90% respectively for Stage 1 and Stage 2.</p><p>We set learning rate to 5e-4 for BitFit, LoRA and Linear probing, 5e-3 for Prefix-Tuning. Prefix number is set to 16 for Prefix-Tuning and intermediate dimension r is set to 32 for LoRA. The learning rates are determined by a grid search in {5e-3, 1e-3, 1e-4, 5e-4} on RTE and BoolQ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Other finetuning Strategies</head><p>We also compare the EH-FT results with other finetuning strategies, but note that they should not be directly seen as baselines because most of them are compatible with EH-FT.</p><p>Top-K Tuning Top-K Tuning <ref type="bibr" target="#b64">(Yosinski et al., 2014)</ref> finetunes only the top k layers while freezing the others. This method can be considered a strategy to prevent overfitting. Following the setting of <ref type="bibr" target="#b61">(Xu et al., 2021)</ref>, we report the best value by varying the layer number k from {3, 6, 12}.</p><p>Mixout Mixout <ref type="bibr" target="#b29">(Lee et al., 2020)</ref> randomly replaces model parameters by pretrained weights with probability p during finetuning in order to reduce the deviation of model parameters. Following this paper, we search the optimal p from {0.7,0.8,0.9}, and learning rate from {1e-5, 2e-4}.</p><p>Child-Tuning Child-Tuning <ref type="bibr" target="#b61">(Xu et al., 2021)</ref> updates a subset of parameters (called child network) during the backward process. The subset is chosen randomly (Child-Tuning F ) or chosen with the lowest Fisher Information (Child-Tuning D ). We search the optimal subset ratio p from {0.1, 0.2, 0.3} and only implement Child-Tuning D . We conduct experiments based on their public code 3 .</p><p>3 https://github.com/RunxinXu/ ChildTuning</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results</head><p>RoBERTa-Large We report the experimental results in Table <ref type="table" target="#tab_0">1</ref>. Besides fully finetune and EH-FT, we also show the results of enhanced methods for finetuning and parameter-efficient tuning. It can be seen that all three kinds of EH-FT outperform fully finetuning, providing the improvement of 0.9, 0.89 and 0.77 in average score. EH-FT works well on RTE, COPA, CB and WIC. In the other hand, on some very simple tasks (MRPC, COLA) or tasks with a large training set (QNLI), the overfitting and forgetting problem is not obvious and the performance is hard to improve with EH-FT.</p><p>For LP-FT, there is no significant improvement in the in-distribution data, which is consistent with our speculation in section 2.3. Other improvement methods also have been found perform poorly on certain datasets. Using BitFit and LoRA in Stage 1 can achieve stable performance, but prefix-tuning shows high variance on different datasets. We guess that this may be the consequence of its instability <ref type="bibr" target="#b5">(Chen et al., 2022)</ref>. Moreover, our approach has the same advantage as parameter-efficient tuning, preventing the model from overfitting on lowresources datasets (CB), which brings a great improvement in performance.   BERT-Large We also do comparison experiments between fully finetune and EH-FT BitFit in BERT-Large. Results are shown in Table <ref type="table" target="#tab_1">2</ref>. EH-FT yields improvement of up to 1.28 average score on BERT-Large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">From the view of continual learning</head><p>To reduce the change of parameters during finetuning is a very common technique in continual learning <ref type="bibr" target="#b6">(Chen et al., 2020)</ref>. We find EH-FT has a similar effect. We ran 3 random seeds under both fully finetuning and EH-FT BitFit (Stage 2) in RTE dataset and visualized the ||? f -? * f || 2 (Euclidean distance between finetuning parameters and pretrained parameters) in Figure <ref type="figure" target="#fig_5">5</ref>. The curve of EH-FT BitFit raises faster in the first 1,000 iterations than fully finetuning due to its large gradient in the beginning. But EH-FT BitFit converges to a small value than fully finetuning in the end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Better Initialized Distribution</head><p>We expect that the classification head obtained by parameter-efficient method can bring a better initialized distribution which can guide the model to converge fast with the prior knowledge learned in Stage 1.</p><p>Considering classes_number is usually small, we take the hidden states in the middle layer of the head, whose dimension is 2048 as features generated by head. We choose the CB dataset which has 250 train samples and 57 validation samples and reduces the dimension of feature using T-SNE. In Figure <ref type="figure" target="#fig_4">4</ref>, the classification head initialized by BitFit has a good clustering which is easier for the model to fit. As a comparison, linear probing makes a more concentrated clustering but with low accuracy. This type of clustering may make it difficult for the model to correct it back in Stage 2. The features obtained by a random head are uniformly distributed over the entire space, with no clear dividing lines among the different categories. This may cause chaotic gradients in the early stage of fully finetuning and lead to knowledge forgetting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Time and Space Consumption</head><p>Most of the current works to improve the finetuning performance in NLP introduce a nonnegligible computational cost. Finetuning time would increase a lot because of the extra loss terms (RecAdam <ref type="bibr" target="#b6">(Chen et al., 2020)</ref>, SMART <ref type="bibr" target="#b24">(Jiang et al., 2020)</ref>), extra modules (Mixout <ref type="bibr" target="#b29">(Lee et al., 2020)</ref>) or gradient preprocessing (Child-Tuning <ref type="bibr" target="#b61">(Xu et al., 2021)</ref>).</p><p>In addition, some of them also have large space consumption. RecAdam and Mixout need to keep a copy of the pretrained parameters and access them every training step, and Child-Tuning stores a boolean variable for every parameter.</p><p>We show that EH-FT is significantly more computationally efficient and spatially efficient than the above methods without hurting performance. It is noteworthy that thanks to the rapidly converging during Stage 2, the total epochs (Stage 1 + Stage 2) can be kept in line with fully finetuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Time Consumption</head><p>Space Consumption Just like the parameterefficient methods above, EH-FT only needs 0.01% to 3% additional parameters. For BitFit, EH-FT needs to memorize the original bias and restore them before Stage 2. After training, saved checkpoint does not need to store those additional parameters.</p><p>Furthermore, it is easy to implement EH-FT in various deep learning frameworks: one just need to start the training twice. Since we do fully finetuning in Stage 2, it can be combined with any other methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study on EH-FT</head><p>We study some factors that may affect the experimental performance, and compare the results varying the hyper-parameters.</p><p>Effect of epoch proportion in the Stage 1. We explore whether the training epochs of Stage 1 and Stage 2 will affect the performance when the total epochs remain the same. We increase the epoch proportion of Stage 1 from 10% to 90% and obtain the performance of RoBERTa-Large on CoLA, RTE, BoolQ and QNLI datasets. The results are shown in Figure <ref type="figure" target="#fig_7">6</ref>. We found that when the proportion exceeded 50%, there was a significant decrease in performance. This indicates that a sufficiently trained head cannot substitute the role of enough training in Stage 2. In this paper, we uniformly use 10% epochs as the first step of training for all the datasets, to ensure that parameters are fully trained.</p><p>Besides, we also study the effect of increasing the training iterations of Stage 1 on the performance of the model when the training iterations of Stage 2 are fixed. We change the epoch proportion of Stage 1 from 10% to 90% and keep the Stage 2 epoch fixed. The results are also illustrated in Figure <ref type="figure" target="#fig_7">6</ref>. It can be observed that doing parameterefficient tuning for a long time does not affect the performance in Stage 2. 10% epochs can provide a good enough initialization of the classification head.</p><p>Effect of epoch proportion on convergence of model. To verify whether a good classification head can help learning and convergence of the model, we analyze the rate of convergence on the training set of COLA and RTE. We have drawn the result curves of three experiments: fully finetuning, EH-FT which allocate 10% and 30% of the total epochs to Stage 1, as shown in Figure <ref type="figure" target="#fig_8">7</ref>. Because of the good initialized distribution brought by a good initialized head, EF-FT can converge more quickly than fully-finetune in Stage 2. But the convergence rate increases slightly when we adjust the Stage 1 ratio from 10% to 30%. That means if we do not run too many iterations in Stage 1, EH-FT (including the time for Stage 1) can converge almost at the same time as fully finetuning.</p><p>We also find that the classification head initialized by parameter-efficient tuning lead to larger gradients than the randomly initialized head by about two orders of magnitude. This may help the model to go directly to the period of rapid loss decrease.</p><p>What if we don't remove the additional parameters before Stage 2? We reserve the parameters tuned in Stage 1 and conduct experiments on all the above datasets. Results are shown in table <ref type="table" target="#tab_2">3</ref>. The EH-FT-reserve performs slightly worse than EH-FT but still better than finetuning. The loss and accuracy of EH-FT-reserve hardly change in Stage 2, which indicates it is likely to be trapped in a local minimum and hard to optimize. Since reserving weights requires greater space-time cost, it is less practical than EH-FT.</p><p>How does the percentage of tunable parameters in Stage 1 affect the final performance? There are two types of parameters to choose: original parameters (BitFit) and extra parameters (LoRA, P-Tuning). For original parameters, there is no guidance on which parameters we should train (parameters selected by BitFit are fixed). If we select randomly, the performance would be far below current PETs. EH-FT also doesn't work well in that setting. For extra parameters, we conduct experiments with EH-FT(LoRA) on RTE, BoolQ and QNLI. The result is in table 4. When the middle rank r increases, the score doesn't decrease significantly. This phenomena is similar to <ref type="bibr">LoRA (Hu et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Pretrained Language Models. (PLM) Transformer <ref type="bibr" target="#b54">(Vaswani et al., 2017)</ref> is a sequenceto-sequence language model with multi-head self-attention mechanism.</p><p>Its encoder and decoder has become the backbone of large-scale pretrained language models like GPT <ref type="bibr">(Radford et al., b)</ref> and <ref type="bibr">BERT (Kenton and Toutanova, 2019)</ref>. XLNet <ref type="bibr" target="#b62">(Yang et al., 2019)</ref>, RoBERTa <ref type="bibr">(Liu et al., 2019b)</ref> and DeBERTa <ref type="bibr" target="#b17">(He et al., 2020)</ref> are proposed as improved models. These models are first trained on a large corpus and then finetuned on downstream tasks, providing a significant performance gain in various NLP benchmarks.</p><p>Parameter-Efficient Tuning.</p><p>As pretrained models continue to get larger, it becomes unacceptable to store a copy of model for each downstream task. In that case, many studies focusing on reducing trainable parameters during finetuning. Adapter <ref type="bibr" target="#b19">(Houlsby et al., 2019)</ref> is the first to present the concept of parameter-efficient tuning, followed by many adapter-base methods. Adapter layers are inserted between transformer layers and initialized randomly. When finetuning, all pretrained parameters are frozen and only those new adapter layers are trainable. Based on low intrinsic dimension of pretrained models <ref type="bibr" target="#b0">(Aghajanyan et al., 2020</ref><ref type="bibr">), LoRA (Hu et al., 2021)</ref> injects rank decomposition matrices into Transformer layers. Prefix Tuning <ref type="bibr" target="#b33">(Li and Liang, 2021;</ref><ref type="bibr" target="#b35">Liu et al., 2022)</ref> shows that trainable continuous prompts are also good choice. Unlike the above works, BitFit <ref type="bibr" target="#b3">(Ben Zaken et al., 2022)</ref> chose to change the bias term of the original model.</p><p>Generalizable Finetuning For large-scale pretrained models, there are many recognized problems with traditional finetuning, such as catastrophic forgetting <ref type="bibr" target="#b41">(McCloskey and Cohen, 1989)</ref> and overfitting <ref type="bibr" target="#b24">(Jiang et al., 2020)</ref>. In order to alleviate the phenomenon of catastrophic forget- ting, many finetuning strategies were introduced to help model forget less. ULMFiT <ref type="bibr" target="#b20">(Howard and Ruder, 2018)</ref> proposes triangular learning rates and gradual unfreezing. As a variant of Droupout <ref type="bibr" target="#b53">(Srivastava et al., 2014)</ref> , Mixout <ref type="bibr" target="#b29">(Lee et al., 2020)</ref> randomly mix pretrained parameters during finetuning. RecAdam <ref type="bibr" target="#b6">(Chen et al., 2020)</ref> introduces L 2 distance penalty between pretrained weights and finetuned weights to prevent model weights deviating too much. Child-Tuning <ref type="bibr" target="#b61">(Xu et al., 2021)</ref> finds that only updating a subset of parameters can obtain better generalization performance on domain transfer and task transfer. Recently, LP-FT <ref type="bibr" target="#b27">(Kumar et al., 2022)</ref> finds that training classification head first can improve the performance for out-ofdistribution data (OOD) on some computer vision benchmark. Inspired by the above approaches, EH-FT use parameter-efficient tuning to pretrain the classification head at first, which can get a better generalization ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Finetuning the pretrained model using a randomly initialized classification head in a downstream task may result in the model output feature deviating too far. We propose Efficient Head Finetuning (EH-FT), an efficient head pretraining strategy using parameter-efficient tuning and only introduce a little extra time and space while improving the model with a stable performance gain in different tasks. EH-FT can make a good initialization of head which can guide model to a local minimum close to the pretrained point, alleviating the catastrophic forgetting and overfitting of large-scale pretrained models. Furthermore, this method can be applied to any pretrained model, as long as it only needs a classification head for downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Limitations</head><p>EH-FT is an empirical method with experiments proof currently. Although it is hard to theoretically analyze the training dynamics of large language models, it is possible to give bounds for a twolayer networks as in LP-FT, which could bring about more understanding, and we leave it for a follow-up work.</p><p>The main measurement for feature change in this paper is based on L 2 distance, which, however, not the best metric. It is very possible to add a very large value in a specific dimension to increase the distance, with most value unchanged. The method to measure the change of the feature space is also an important topic in understanding the behavior of finetuning PLMs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The amount of feature change after finetuning RoBERTa-large on RTE with different strategies, where "features" denote the final-layer output of the pretrained RoBERTa backbone, a.k.a. the input of the classification head. The feature change is measured by the L 2 distance between the features on the training set before and after finetuning.</figDesc><graphic url="image-1.png" coords="1,306.14,212.60,218.27,158.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration for different finetuning strategies. (a) Fully finetuning directly optimize all the parameters. (b) LP-FT first only trains the head then the whole model. (c) Our EH-FT first trains the head and a very small part (1%? 4%) of parameters by a parameter-efficient tuning algorithm at the stage 1, and then restart a fully finetuning with the trained head from the stage 1.</figDesc><graphic url="image-2.png" coords="2,70.87,70.86,453.54,118.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Diagram to show how different heads affect the optimizing path during finetuning. (a) Random head makes the early gradients very stochastic and largely changes the weights of the backbone. (b) Linear probing head is well adapted to the pretrained backbone, but the backbone weights still need to change a lot to provide specific information to reach a higher downstream performance. (c) Efficient head ensures the existence of a nearby good weight for the backbone (the result of the parameter-efficient tuning), the landscape helps quickly find a nearby optimum without largely changing the features.</figDesc><graphic url="image-3.png" coords="3,306.14,70.87,218.27,173.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Features distribution with different head initialization strategies. We reduce the dimension of the features output by classification using T-SNE. The upper plots show the distribution of samples in CB training set, and the lower plots show the samples in development set. Different labels are distinguished by point color. Parameterefficient tuning can make a good clustering which is not highly concentrated.</figDesc><graphic url="image-7.png" coords="6,75.37,198.89,146.56,107.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: We present the L 2 distance between model weights and pretrained weight (||? f -? * f || 2 ) on RTE dataset with multi runs. EH-FT will change parameters more quickly in the beginning but finally converge to a smaller distance.</figDesc><graphic url="image-11.png" coords="6,84.72,384.35,204.25,131.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Our method introduces two additional parts of extra computation during training. The first part is the time spent by additional parameter-efficient module added in Stage 1. However, most of those modules are computationally efficient, and BitFit does not even have extra computation. Furthermore, extra modules only exist in Stage 1 which has few epochs. Another is the overhead of restarting training in Stage 2. This part is negligible in most cases and can be also avoided by carefully programming.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Ablation study on Stage 1 epochs of EH-FT BitFit using RoBERTa-Large. (a) Results with different proportions of Stage1 epochs while keeping the total epochs fixed. Training model for more epochs in Stage 1 can not substitute fully-finetune. (b) Results with different proportions of Stage 1 epochs while keeping the Stage 2 epochs fixed. Epochs of Stage 1 do not have a significant effect on the results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Convergence rate with fully-finetune and EH-FT BitFit Stage 2. For EH-FT, we draw the curve with different Stage 1 epochs. Assisted by a good initialized head, the model can converge quickly in Stage 2. Increasing the training time of Stage 1 can increase the convergence rate of Stage 2. (a) On COLA training set. (b) On RTE training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Liu et al., 2019b) 86.60 ? 86.90 94.00 98.20 ? 75.60 90.90 ? 94.70 68.00 92.40 ? 87.48  Finetuning (reproduce)  87.52 86.32 93.75 94.64 73.90 90.81 94.75 68.72 92.27 86.96    Results with RoBERTa-Large. All scores are the mean result of 4 random seeds. Results with ? finetuned starting from the MNLI model, which is expected a better performance than single-task finetuning. It is recommended to compare the reproduced finetuing result with LP-FT and EH-FT, because they share the same code, only with different head initialization.</figDesc><table><row><cell>Methods</cell><cell></cell><cell cols="3">RTE BoolQ COPA</cell><cell>CB</cell><cell cols="3">WIC MRPC QNLI COLA STS-B Avg</cell></row><row><cell cols="2">Finetuning (Linear Probing</cell><cell cols="7">61.10 64.31 80.25 79.47 67.97 75.45 69.95 33.17 60.25 65.77</cell></row><row><cell cols="2">Prefix-Tuning</cell><cell cols="3">77.00 83.90 87.75</cell><cell>100</cell><cell cols="3">65.05 89.56 94.55 57.89 89.92 82.85</cell></row><row><cell>LoRA</cell><cell></cell><cell cols="3">88.50 86.29 94.75</cell><cell>100</cell><cell cols="3">73.04 90.43 94.37 67.86 92.17 87.49</cell></row><row><cell>BitFit</cell><cell></cell><cell cols="7">87.05 86.13 95.50 99.11 72.01 90.32 94.68 57.89 92.05 87.24</cell></row><row><cell cols="2">Top-K Tuning</cell><cell cols="7">86.55 85.34 93.00 98.66 73.71 90.94 94.12 65.78 91.47 86.62</cell></row><row><cell>Mixout</cell><cell></cell><cell cols="7">85.56 86.06 95.00 98.66 74.45 90.87 94.18 65.45 91.62 86.87</cell></row><row><cell cols="2">Child-TuningD</cell><cell cols="2">88.18 86.65</cell><cell>94.5</cell><cell cols="4">92.86 74.07 91.36 94.44 68.52 92.51 87.00</cell></row><row><cell>LP-FT</cell><cell></cell><cell cols="7">86.14 86.38 94.00 93.50 74.73 91.47 94.78 67.45 92.20 86.74</cell></row><row><cell>EH-FTLoRA</cell><cell></cell><cell cols="2">88.68 86.69</cell><cell>94.5</cell><cell cols="4">99.12 74.65 91.00 94.73 69.00 92.24 87.85</cell></row><row><cell>EH-FTPT</cell><cell></cell><cell>87.22</cell><cell>86.9</cell><cell>95.00</cell><cell>100</cell><cell cols="3">73.63 90.56 94.89 69.10 92.31 87.73</cell></row><row><cell>EH-FTBitFit</cell><cell></cell><cell cols="7">88.10 86.97 94.75 99.12 75.20 91.00 94.61 68.78 92.25 87.86</cell></row><row><cell>Methods</cell><cell>RTE</cell><cell cols="2">BoolQ COPA</cell><cell>CB</cell><cell cols="4">WIC MRPC QNLI COLA STS-B</cell><cell>Avg</cell></row><row><cell cols="2">Finetuning 76.08</cell><cell>79.79</cell><cell cols="3">75.50 91.52 71.71</cell><cell>90.90</cell><cell>92.54</cell><cell>63.44</cell><cell>90.60</cell><cell>81.34</cell></row><row><cell cols="2">EH-FTBitFit 76.35</cell><cell>80.95</cell><cell cols="3">77.00 97.77 71.52</cell><cell>92.10</cell><cell>92.34</cell><cell>64.91</cell><cell>90.64</cell><cell>82.62</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results in BERT-Large. All scores are the mean result of 4 random seeds. EH-FT BitFit outperforms fine-tuning in 7 of 9 tasks.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>We reserve the parameters tuned on Stage 1 in EH-FT(BitFit)-reserve and EH-FT(LoRA)-reserve. There is a slight decrease in overall results.</figDesc><table><row><cell>Methods</cell><cell></cell><cell>RTE</cell><cell cols="2">BoolQ COPA</cell><cell>CB</cell><cell cols="5">WIC MRPC QNLI COLA STS-B</cell><cell>Avg</cell></row><row><cell>Finetuning</cell><cell></cell><cell>87.52</cell><cell>86.32</cell><cell cols="3">93.75 94.64 73.90</cell><cell>90.81</cell><cell>94.75</cell><cell>68.72</cell><cell>92.27</cell><cell>86.96</cell></row><row><cell>EH-FTBitFit</cell><cell></cell><cell>88.10</cell><cell>86.97</cell><cell cols="3">94.75 99.12 75.20</cell><cell>91.00</cell><cell>94.61</cell><cell>68.78</cell><cell>92.25</cell><cell>87.86</cell></row><row><cell cols="3">EH-FTBitFit-reserve 87.37</cell><cell>86.40</cell><cell cols="3">95.50 99.55 75.00</cell><cell>90.32</cell><cell>94.54</cell><cell>69.31</cell><cell>92.27</cell><cell>87.80</cell></row><row><cell>EH-FTLoRA</cell><cell></cell><cell>88.68</cell><cell>86.69</cell><cell>94.5</cell><cell cols="2">99.12 74.65</cell><cell>91.00</cell><cell>94.73</cell><cell>69.00</cell><cell>92.24</cell><cell>87.85</cell></row><row><cell cols="3">EH-FTLoRA-reserve 87.37</cell><cell>80.38</cell><cell cols="3">92.22 99.55 75.01</cell><cell>90.38</cell><cell>94.62</cell><cell>68.57</cell><cell>92.22</cell><cell>87.12</cell></row><row><cell>r</cell><cell>RTE</cell><cell cols="2">BoolQ QNLI</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>8</cell><cell>88.68</cell><cell>86.89</cell><cell>94.73</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>16</cell><cell>87.73</cell><cell>86.42</cell><cell>94.62</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>32</cell><cell>87.54</cell><cell>86.30</cell><cell>94.73</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>256</cell><cell>87.82</cell><cell>86.40</cell><cell>94.81</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>512</cell><cell>87.20</cell><cell>86.54</cell><cell>94.75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">1024 88.26</cell><cell>86.65</cell><cell>94.79</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Results of EH-FT(LoRA) with different middle rank r in Stage 1.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://github.com/THUDM/ SwissArmyTransformer</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The authors would like to thank <rs type="person">Xiao Liu</rs> and <rs type="person">Yue Cao</rs> for their discussion, and the reviewers of EMNLP for their valuable suggestions.</p><p>This work is supported by <rs type="funder">National Key R&amp;D Program of China</rs> (<rs type="grantNumber">2021ZD0113304</rs>), <rs type="funder">National Science Foundation for Distinguished Young Scholars</rs> (No. <rs type="grantNumber">61825602</rs>) and <rs type="funder">Natural Science Foundation of China</rs> (No. <rs type="grantNumber">61836013</rs>)</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_dXYAk4w">
					<idno type="grant-number">2021ZD0113304</idno>
				</org>
				<org type="funding" xml:id="_jzTpU98">
					<idno type="grant-number">61825602</idno>
				</org>
				<org type="funding" xml:id="_dBGkbCq">
					<idno type="grant-number">61836013</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Intrinsic dimensionality explains the effectiveness of language model fine-tuning</title>
		<author>
			<persName><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonal</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.13255</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A framework for learning predictive structures from multiple tasks and unlabeled data</title>
		<author>
			<persName><forename type="first">Rie</forename><surname>Kubota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ando</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1817" to="1853" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scalable training of L1-regularized log-linear models</title>
		<author>
			<persName><forename type="first">Galen</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Machine Learning</title>
		<meeting>the 24th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">BitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models</title>
		<author>
			<persName><forename type="first">Elad</forename><surname>Ben Zaken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shauli</forename><surname>Ravfogel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-short.1</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Guanzheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaiqiao</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangsong</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.07962</idno>
		<title level="m">Revisiting parameterefficient tuning: Are we really there yet? arXiv preprint</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recall and learn: Fine-tuning deep pretrained language models with less forgetting</title>
		<author>
			<persName><forename type="first">Sanyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutai</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangzhan</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7870" to="7881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Nicola</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilker</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08164</idno>
		<title level="m">Editing factual knowledge in language models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Delta tuning: A comprehensive study of parameter efficient methods for pre-trained language models</title>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi-Min</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weize</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.06904</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06305</idno>
		<title level="m">Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Transformer feed-forward layers are key-value memories</title>
		<author>
			<persName><forename type="first">Mor</forename><surname>Geva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roei</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5484" to="5495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Parameter-efficient transfer learning with diff pruning</title>
		<author>
			<persName><forename type="first">Demi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4884" to="4896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Don&apos;t stop pretraining: Adapt language models to domains and tasks</title>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><surname>Marasovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8342" to="8360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Algorithms on Strings, Trees and Sequences</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Gusfield</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge, UK.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Towards a unified view of parameter-efficient transfer learning</title>
		<author>
			<persName><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.04366</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16000" to="16009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03654</idno>
		<title level="m">Deberta: Decoding-enhanced bert with disentangled attention</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Parameter-efficient transfer learning for nlp</title>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Giurgiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruna</forename><surname>Morrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>De Laroussilhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Attariyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2790" to="2799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1031</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="328" to="339" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shean</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09685</idno>
		<title level="m">Lora: Low-rank adaptation of large language models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Attention is not explanation</title>
		<author>
			<persName><forename type="first">Sarthak</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byron</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10186</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">What does bert learn about the structure of language?</title>
		<author>
			<persName><forename type="first">Ganesh</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beno?t</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Djam?</forename><surname>Seddah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2019-57th Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SMART: Robust and efficient fine-tuning for pretrained natural language models through principled regularized optimization</title>
		<author>
			<persName><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuo</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.197</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2177" to="2190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename><surname>Kristina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toutanova</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kieran</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agnieszka</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
		<meeting>the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="3521" to="3526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Fine-tuning can distort pretrained features and underperform out-ofdistribution</title>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditi</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robbie</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.10054</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mixout: Effective regularization to finetune large-scale pretrained language models</title>
		<author>
			<persName><forename type="first">Cheolhyoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanmo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR). International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Jaejun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03090</idno>
		<title level="m">What would elsa do? freezing layers during transformer fine-tuning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3045" to="3059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Measuring the intrinsic dimension of objective landscapes</title>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heerad</forename><surname>Farkhoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.08838</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Prefix-tuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4582" to="4597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Linguistic knowledge and transferability of contextual representations</title>
		<author>
			<persName><forename type="first">Matt</forename><surname>Nelson F Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixuan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yicheng</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weng</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-short.8</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="61" to="68" />
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10385</idno>
		<title level="m">Gpt understands, too</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Cutting down on prompts and parameters: Simple few-shot learning with language models</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>Robert L Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivana</forename><surname>Bala?evi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13353</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Decoupled weight decay regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning sparse neural networks through l_0 regularization</title>
		<author>
			<persName><forename type="first">Christos</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.01312</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Catastrophic interference in connectionist networks: The sequential learning problem</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neal</forename><forename type="middle">J</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Psychology of learning and motivation</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="109" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.05262</idno>
		<title level="m">Locating and editing factual knowledge in gpt</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Intermediate-task transfer learning with pretrained language models: When and why does it work?</title>
		<author>
			<persName><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haokun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mon</forename><surname>Phu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyi</forename><surname>Htut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">Yuanzhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katharina</forename><surname>Vania</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Kann</surname></persName>
		</author>
		<author>
			<persName><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5231" to="5247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Tim Salimans, and Ilya Sutskever. a. Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Know what you don&apos;t know: Unanswerable questions for squad</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="784" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Yara parser: A fast and accurate dependency parser</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Sadegh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rasooli</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">R</forename><surname>Tetreault</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.06733.Version2</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A primer in bertology: What we know about how bert works</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Kovaleva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="842" to="866" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Introduction to the conll-2003 shared task: Languageindependent named entity recognition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Erik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fien</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meulder</forename><surname>De</surname></persName>
		</author>
		<idno>arXiv preprint cs/0306050</idno>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Sofia</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.03731</idno>
		<title level="m">Is attention interpretable? arXiv preprint</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Overcoming catastrophic forgetting in zero-shot cross-lingual generation</title>
		<author>
			<persName><forename type="first">Tu</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Barua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.12647</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Mohit Iyyer, and Noah Constant</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Bertnesia: Investigating the capture and forgetting of knowledge in bert</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Wallat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaspreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avishek</forename><surname>Anand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="174" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Superglue: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="353" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Attention is not not explanation</title>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Wiegreffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Pinter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.04626</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03771</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Raise a child in large language model: Towards effective and generalizable fine-tuning</title>
		<author>
			<persName><forename type="first">Runxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9514" to="9528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Kformer: Knowledge injection in transformer feed-forward layers</title>
		<author>
			<persName><forename type="first">Yunzhi</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.05742</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
		<title level="m">How transferable are features in deep neural networks? Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked languagemodels</title>
		<author>
			<persName><forename type="first">Elad</forename><surname>Ben Zaken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shauli</forename><surname>Ravfogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.10199</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
