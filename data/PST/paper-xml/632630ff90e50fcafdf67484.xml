<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LoopPoint: Checkpoint-driven Sampled Simulation for Multi-threaded Applications</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Alen</forename><surname>Sabu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Trevor</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore Harish Patil</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Intel Corporation Wim Heirman</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Intel Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LoopPoint: Checkpoint-driven Sampled Simulation for Multi-threaded Applications</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/HPCA53966.2022.00051</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>checkpointing</term>
					<term>multi-threaded</term>
					<term>record-andreplay</term>
					<term>sampling</term>
					<term>simulation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generic multi-threaded sampled simulation has been a long-standing, challenging problem with the potential to help change how researchers study modern, complex computing systems. Yet, a practical solution for reducing complex multithreaded applications into a representative sample has been elusive. Existing techniques either do not provide significant speedups to be useful (Time-based Sampling techniques can show less than a 10× speedup compared to a fully-detailed simulation) or apply only to particular synchronization types (BarrierPoint for barrier-based workloads). In addition, workload-specific solutions can be rigid with respect to region selection, which can limit the overall simulation speedup when regions are large. A solution is needed that both supports generic multi-threaded applications, no matter the synchronization primitives used, as well as allows for ease of deployment and fast evaluation.</p><p>In this work, we aim to solve these challenges and propose a novel sampling technique for multi-threaded applications, called LoopPoint, that is both agnostic to the type of synchronization primitives used and scales by the similarity exhibited by the application. The proposed methodology combines several vital features, including (1) repeatable, up-front application analysis, (2) a novel clustering approach to take into account run-time parallelism, and (3) the use of loop-based simulation markers to divide the work into measurable chunks, even in the presence of spin-loops. LoopPoint identifies representative simulation regions that can be simulated in parallel to achieve speedups of up to 801× for the train input set of the multi-threaded SPEC CPU2017 benchmarks with an average simulation error of just 2.33%. For the ref inputs of CPU2017, we calculate the speedup with LoopPoint to be 11,587× on average (for parallel simulation), and up to 31,253×, demonstrating how the identification of application regularity and loops can lead to significant simulation improvements compared to state-of-the-art solutions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Sampling is a well-known application workload reduction technique that traces its roots back decades. From the earliest works <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, researchers have been able to identify regularity in single-threaded applications and exploit that to sample large applications into smaller application representatives. Because of the repeated execution of regions with similar behavior, these techniques have been shown to accurately predict the original workload behavior, and significantly reduce the simulation time needed <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>.</p><p>Apart from sampling, researchers have developed a number of complementary techniques to reduce the overall amount of work required to simulate applications in detail, including input size reduction <ref type="bibr" target="#b2">[3]</ref> and benchmark synthesis <ref type="bibr" target="#b3">[4]</ref>. While each Fig. <ref type="figure">1</ref>: Approximate time to evaluate the performance of multithreaded benchmarks with different methodologies. The average result and error bars represent the estimated simulation time for all benchmarks in the corresponding suite and input sets, assuming infinite simulation resources (the longest simulation region determines the overall simulation time). Benchmarks were configured with 8-threads and passive OpenMP wait policy, assuming a total simulation speed of 100 KIPS.</p><p>technique presents its benefits and challenges, sampling has emerged as a straightforward way to maintain the original application characteristics and accurately extrapolate performance while reducing the overall simulation burden.</p><p>With the increasing number of cores in modern processors, multi-threaded applications can exploit a large amount of compute through task and loop parallelism. Simulating these large, multi-threaded applications is extremely difficult, even on modern simulators. Ultra-fast FPGA-based simulators <ref type="bibr" target="#b4">[5]</ref> require detailed implementations and are capacity-limited, preventing the simulation of large processors and large parallel systems, and fast software-based simulators <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> still require a significant amount of time to run an entire large, parallel workload to completion. Multi-threaded applications are inherently difficult to analyze <ref type="bibr" target="#b7">[8]</ref> as the threads can go to sleep at any time, threads interfere with one another, and complex behavior emerges from regular application parameters like misalignment of threads to cores and unequal cache distribution.</p><p>Some of the earliest multi-threaded sampling solutions prove effective when the threads themselves do not synchronize but can still interact with the memory hierarchy <ref type="bibr" target="#b8">[9]</ref>. Any amount of synchronization requires thread progress to be measured in time to track the amount of progress or parallelism in the application. The move toward a time-based sampling methodology has led to the development of sampling techniques for synchronizing multi-threaded applications. These techniques <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> describe one of the first generic sampling solutions for multi-threaded applications. However, the overall simulation speed is still bound to the total application length, which dominates the simulation time of this methodology. Later proposals, in the form of application and synchronizationspecific methodologies <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b13">[14]</ref>, exceeded the performance of time-based sampling and allowed for the simulation complexity to be bound to application diversity, not application length. Unfortunately, these methodologies are tied to specific application characteristics (the use of barriers <ref type="bibr" target="#b11">[12]</ref> or tasks <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>), and therefore do not represent a general sampling solution that covers all application types. In fact, as Figure <ref type="figure">1</ref> demonstrates, both time-based sampling, and BarrierPoint (when inter-barrier regions exist to simulate), approach a simulation time of one year to simulate the sample when considering large, multi-threaded applications. Clearly, current methodologies are insufficient for simulating the largest, most realistic benchmarks like the multi-threaded SPEC CPU2017 with the ref input set.</p><p>In this work, we aim to overcome the limitations of these prior works to enable synchronization-agnostic application sampling for multi-threaded workloads while still scaling the amount of work based on the representative nature of the application. To accomplish this goal, we present the LoopPoint methodology that reduces an application to a few representative regions, called looppoints, by taking into account several key factors like understanding <ref type="bibr" target="#b0">(1)</ref> where to simulate which requires (1a) an accurate analysis methodology that can provide for reproducible analysis, and (1b) using a precise clustering mechanism that partitions the regions to reduce the workload into its representative components. In addition, our methodology presents <ref type="bibr" target="#b1">(2)</ref> how to simulate the regions to allow the application to take advantage of the underlying hardware, while not constraining execution to a deterministic path <ref type="bibr" target="#b14">[15]</ref> that might not exhibit true application behavior.</p><p>We make the following contributions in this work: 1) A representative simulation region selection methodology called LoopPoint suitable for the performance projection of multi-threaded programs (more details on supported workloads in Section III-K) based on using loop iterations as the unit of work. 2) A technique to enable multi-threaded sampled simulation by filtering out spin-loops during region identification, selecting repeatable loop boundaries of a practical region size, and accurately extrapolating performance characteristics.</p><p>3) The development of a process to record a constrained application checkpoint for accurate analysis and subsequently simulate the workload's unconstrained behavior during simulation. 4) A comprehensive evaluation of the LoopPoint methodology to demonstrate the potential for speedup while maintaining accuracy using the OpenMP-based multithreaded subset of SPEC CPU2017 benchmark suite and NAS Parallel Benchmarks (NPB). In the following sections of this work, we first provide an overview the LoopPoint methodology, results, and evaluation. In Section II, we detail each of the components needed for a fast, accurate, and generic multi-threaded sampled simulation. In Section III, we describe the LoopPoint methodology. We then detail the experimental infrastructure and setup in Section IV, evaluate the LoopPoint methodology in Section V. Finally, we compare to related work (Section VI) and conclude the paper (Section VII).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. FAST AND GENERIC MULTI-THREADED SIMULATION REQUIREMENTS</head><p>Time-based sampling methodologies <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> present the first workable solution to sample generic multi-threaded applications. However, the speed-ups achieved (up to 5.8×) using these methodologies are limited by the need to visit the entire application. To achieve high speed-up while maintaining accuracy during multi-threaded workload sampling, we need to consider the inherent application regularity and the amount of parallelism present in the workload at any particular time. We need to define a unit-of-work that is suitable to exploit the application regularity and, at the same time, is applicable across a variety application and synchronization types. The key is the ability to (1) recognize representative regions in a generic way across multi-threaded workload types, and to (2) classify these regions considering application parallelism. To this end, we present a new application sampling methodology called LoopPoint that (a) uses loop iterations as the main unit of work, (b) utilizes constrained pinballs <ref type="bibr" target="#b15">[16]</ref> (user-level checkpoints that allow for reproducible analysis), (c) employs heuristics to remove synchronization during analysis, but use them during simulation, and (d) performs unconstrained simulation of the selected simulation regions allowing for fast and accurate workload evaluation. Figure <ref type="figure">2</ref> shows the overall methodology.</p><p>Sampling methodologies that rely on instruction counting can perform poorly when dealing with multi-threaded applications <ref type="bibr" target="#b16">[17]</ref>. We demonstrate this by performing a naive adaptation of Simpoint <ref type="bibr" target="#b0">[1]</ref> for multi-threaded applications of SPEC CPU2017 using 8 threads. With this methodology, the average error obtained for the applications using active wait policy is 25% and as high as 68.44%, whereas errors for the passive wait policy are as high as 20%.</p><p>Previous works like the BarrierPoint <ref type="bibr" target="#b11">[12]</ref> methodology use inter-barrier regions as the unit of work, whereas the Task-Point <ref type="bibr" target="#b12">[13]</ref> methodology applies only to task-based applications that use task instances as the unit of work. Unfortunately, BarrierPoint, when used to sample large applications with a small number of barriers, can yield negligible simulation speedups. This can be common, especially while sampling realistic workloads for which the length of inter-barrier regions is a bottleneck. BarrierPoint, therefore, is not practical for such workloads. Figure <ref type="figure">1</ref> shows how the instruction count (and therefore simulation time) of an inter-barrier region grows How to simulate Fig. <ref type="figure">2</ref>: LoopPoint-based region selection and simulation for multi-threaded workloads. The workload is captured for analysis and region selection based on loop information. The representative regions are simulated using a checkpoint-driven method as well as by binary-driven unconstrained way allowing for extrapolation of performance and other metrics of interest.</p><p>with larger input sets of SPEC CPU2017 and NAS Parallel Benchmarks (NPB) <ref type="bibr" target="#b17">[18]</ref> with 8 threads. BarrierPoint works well for NPB with the A input size <ref type="bibr" target="#b11">[12]</ref>, but as the input sizes grow, for classes C, D and E, inter-barrier regions become so large that it becomes impractical to use BarrierPoint for those input sets. The same is the case with SPEC CPU2017 using ref inputs.</p><p>Instead, LoopPoint uses loop iterations as the unit of work with the goal to apply to generic multi-threaded programs. The idea of using loop iterations as slices for single-threaded programs was proposed in <ref type="bibr" target="#b18">[19]</ref>. With loop entries as slice boundaries, the simulation regions can then be specified using a (PC, count) pair for the starting and ending loop entry for each simulation region. By monitoring the amount of work, as represented by loops, and not instructions or barriers, we can isolate multi-threaded application representatives and understand the amount of global work completed. For multithreaded programs, one additional constraint is that the loop entries that are chosen to start and end slices should be those doing meaningful work. Automatically separating loops doing real work from synchronization can be a daunting task. However, we can use application knowledge or synchronization mechanism details to filter out synchronization loops. For example, the Intel OpenMP run-time uses functions in the libiomp5.so library for synchronization; hence loops from that library should not be counted towards work done while profiling the application. Alternatively, if the synchronization routines are known before-hand, the code from such routines can likewise be avoided.</p><p>Where to simulate. As detailed cycle-accurate simulation can be time-consuming, architects and researchers often use sampling to decide where to simulate by choosing small portions or regions of long-running program executions for simulation. Sampling requires (a) choosing the regions so that they are representative of the whole program behavior and (b) projecting the whole-program performance based on the simulation results of the selected regions. SimPoint <ref type="bibr" target="#b0">[1]</ref> is a popular simulation region selection approach. It works by dividing the program execution into smaller slices and collecting an execution signature for each slice. K-means clustering is used to determine phases from slice signatures. One representative per cluster is then chosen with the weight corresponding to the cluster size. Since these representatives are designed to be micro-architecture independent, the signature collected for each slice needs to be dependent only on the program execution and not based on any micro-architecture dependent metric. Typical signatures used include the BBV (Basic Block Vector) which contains execution counts of various basic blocks (single-entry/exit code blocks). How to slice a program's execution into regions is an important decision. For single-threaded programs, using a fixed instruction count called the slice size has been shown to work well <ref type="bibr" target="#b0">[1]</ref>. In our work, we keep slices of approximately similar sizes demarcated by loop entries. The region selection is based on the replay of a previously recorded whole-program execution as a pinball. According to the micro-architecture of the recording machine, the synchronization seen there can be different from the synchronization seen during unconstrained simulation. We, therefore, augment our region selection methodology to make a selection only on the real computation or work done. The heuristics described earlier to avoid synchronization loop entries as region boundaries can also be used to filter out (to execute but not count) synchronization code during profiling for region selection.</p><p>How to simulate. A critical decision that the simulator developers need to make is how to simulate, i.e., how to connect the application in consideration to a simulator. The most commonly used methods are (1) binary-driven where a program binary is executed during simulation feeding instructions to the simulator, (2) checkpoint-driven where a snapshot of selected region memory/register state and a list of injection events are used to drive the simulator, and (3) trace-driven where an instruction-by-instruction recorded state is fed to a timing-only simulator. The choice of how to simulate depends on several factors, such as ease of deployment, cost of generation, and flexibility of the evaluation. For this work, we use both binarydriven and checkpoint-driven simulations for our evaluation, although the implementation itself is generic and supports any of these simulation methods. Checkpoints are easier to share among multiple users than program binaries whose execution might require complex setup and input availability. We propose to capture regions selected by LoopPoint as pinball <ref type="bibr" target="#b19">[20]</ref> checkpoints so they can be used to drive PinPlaybased simulators.</p><p>By default, PinPlay supports constrained replay of pinballs where the shared memory accesses among threads are repeated in the order captured during recording. Simulation based on such constrained replay will repeat the thread ordering based on the micro-architecture of the machine on which the pinballs were generated. However, we ideally want the target, simulated micro-architecture to decide the thread behavior during simulation. To achieve that, we also use binary-driven simulation of the regions selected by LoopPoint using stable (PC, count)-based boundaries defining those regions. Therefore, the simulation proceeds as though the region was executed natively on the simulated micro-architecture. Another technique to achieve unconstrained simulation using pinballs is to convert them to executable checkpoints, called ELFies <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE LOOPPOINT METHODOLOGY</head><p>In this section, we explain the different parts of the proposed methodology, LoopPoint. We start with an upfront analysis of the application to determine its behavior and to identify loops, as shown in Figure <ref type="figure">2</ref>. This is a one-time step and we use the information collected here for clustering regions to choose representatives. The representative regions are then simulated with sufficient warmup. The simulation results enable us to reconstruct the overall application performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Selecting a Unit of Work</head><p>Multi-threaded applications may use different execution paths with different runs, and therefore the use of IPC to evaluate the performance of multi-threaded workloads is infeasible <ref type="bibr" target="#b16">[17]</ref>. LoopPoint proposes a strategy that identifies regions of interest in terms of work done by each thread. We define the unit of work as the actual amount of compute done within a slice of an application. For an unmodified application with the same input set, the unit of work chosen needs to remain the same for each application execution regardless of the properties of the underlying hardware, although the number of instructions executed may vary each time. The generality of the chosen unit of work is crucial for application sampling as this determines the amount of simulation speedup achieved. We would want the chosen unit of work to be large in number within the program, to be one that repeats itself, and to remain unchanged over multiple executions.</p><p>We consider the number of loop iterations as the unit of work done. Program loops are ubiquitous across application domains and the number of iterations of any particular loop doing real computation as opposed to synchronization can remain constant over multiple executions for an unmodified application and for a fixed input size. In a multi-threaded environment, we consider loop execution, ignoring spin-loops (one form of active synchronization), to compute the amount of work done. Spin-loops contribute to the IPC of the application and consume CPU cycles, however, they do not contribute to the meaningful work done by the particular thread (waiting cannot be considered work completed). This is the key to LoopPoint methodology we present here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Understanding Parallelism</head><p>One of the fundamental requirements of a multi-threaded sampling methodology is the ability to understand how the parallelism of an application changes, over time, and to use that information to drive the representative selection process. In fact, understanding parallelism in a generic way is one of the main insights of this work. To accomplish this, we continue to use worker loop instructions as the key metric for work completed.</p><p>Program phase behavior is an important aspect to consider while sampling applications. A phase is a set of slices in a program's execution that shows similar behavior, regardless of where they appear within the execution. The locations in source code whose executions correlate to a phase change in the application are called software phase markers <ref type="bibr" target="#b18">[19]</ref>. The software phase markers can accurately identify the phase changes which occur in an application execution irrespective of the underlying microarchitecture. These are execution points which can act as simulation region boundaries that are invariant across multiple application executions. We identify source level program loops as possible checkpoints which form the basic building blocks of a program.</p><p>Capturing BBVs is an essential way to understand the fingerprint of an application execution region. We consider the slice-size to be approximately N × 100 million global (allthreads) instructions, that align with loop boundaries, for a Nthreaded application. For example, we collect BBVs in intervals of approximately 800 million instructions for an 8-threaded application. We ignore the instructions executed in spin-loops or any other synchronization code while collecting the BBVs. The end of a region specified by a BBV is the next loop entry once the instruction-count target is achieved. Although this can be implemented in several ways (as described in <ref type="bibr" target="#b18">[19]</ref>), we do not currently differentiate between inner and outer loop markers, and do not restrict specific threads to indicate loop boundaries. The loop entries that serve as region markers need to be worker loops and not spin-loops. We assume that the spin-loops are found only in the synchronization library (for example, OpenMP) and therefore we end a region only at a loop entry that is present in the main image of the application. The per-region BBVs of each thread are concatenated into a longer, global BBV that represents a multi-threaded region. This guides the clustering phase when there are regions that exhibit non-homogeneous thread behavior. Figure <ref type="figure">3</ref> shows the ratio of the number of instructions executed by each thread as the application progresses. The application 657.xz_s.2, as an example, clearly exhibits a non-homogeneous thread behavior.</p><p>There are a number of reasons to maintain sufficiently large per-thread slices (approximately 100 million instructions). If a smaller slice-size is chosen, a large number of simulation points may be required, and such regions are highly sensitive to warmup and aliasing issues <ref type="bibr" target="#b10">[11]</ref>. At the same time, we also need to make sure that there are enough intervals in the application for the clustering algorithm to work efficiently <ref type="bibr" target="#b21">[22]</ref>.</p><p>Prior analyses <ref type="bibr" target="#b0">[1]</ref> on single-threaded applications showed that fixed size (of 100 million instructions) intervals of execution can be used to identify phase behavior. Using varying length intervals <ref type="bibr" target="#b22">[23]</ref> corresponding to the application periodicity can help mark the phases more accurately. In LoopPoint, we use approximately similar interval lengths, however, the methodology can also be used with varying length intervals.</p><p>While we profile an application for BBVs or any feature Fig. <ref type="figure">3</ref>: The above graphs show the variation in the share of the per-thread instruction count on a per-slice (with a slice size of 800M global instructions) basis as the application progresses. If we consider a multi-threaded region, the basicblock share is different for all threads. This is subtly captured by concatenating the per-thread execution fingerprints.</p><p>vectors, we make sure that all threads in the application make the same amount of forward progress during analysis. This is to stabilize the collected profile for any thread imbalance that is caused by external events on the host processor (and is unrelated to the analysis environment). We call this method to enforce equal progress between threads flow-control.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Marking Region Boundaries</head><p>Every region in an application has its boundaries at a loop entry. The regions need to be represented so that it is repeatable across multiple executions of the application. In the case of single-threaded applications, instruction count can be used to define regions reliably. However, for multi-threaded applications, this does not hold. We describe the start and end of each region as an ordered-pair (PC, count), where the PC is the address of the corresponding region boundary marker instruction and the count is the execution count of the marker at the start and end of the region. The value of count for a particular region size is invariant across multiple executions which represents the unit of work done. Hence, these markers remain valid simulation points even in the presence of spinloops.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Identifying Loops using DCFG</head><p>Loops are found often in typical applications and the number of loop iterations can remain constant for an unmodified application for a particular input over multiple executions. This is the key to our generic methodology which is explained below in detail.</p><p>We employ a Dynamic Control-Flow Graph (DCFG) to identify the regions that represent loops. A DCFG is similar to a classical control-flow graph with a primary difference: Each edge of a DCFG is augmented with a trip count to indicate the number of times the edge was traversed. The source code locations whose executions correlate with a phase change are called a software phase markers <ref type="bibr" target="#b18">[19]</ref>. The software phase markers identify the phase changes that occur in an application execution irrespective of the underlying microarchitecture. These phase markers need to repeat in number and order across multiple program executions so that this can meaningfully act as simulation region boundaries. We choose headers of loops that are in the main image of the program assuming that the synchronisation loops are in the libraries. The number of iterations of synchronisation loops may vary across different program executions. The DCFG of the whole program execution is instrumented for loop header instructions to identify a subset of loops from the main image. Loop header instructions are instrumented to emit Basic Block Vectors (BBVs) after slice-size number of instructions. Figure <ref type="figure" target="#fig_2">4</ref> shows a region identified using DCFG. The region is contained in the 638.imagick_s.1 application with train inputs and 8 threads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Clustering Representative Regions</head><p>Once an application is profiled, and region boundaries marked, we will have a collection of variable-length regions. These BBVs (with spin-loops filtered) represent the state of the application, and also allow one to understand the amount of work accomplished by each thread. For example, in regions where a single thread is active, the thread will no longer interfere with memory requests from other threads, potentially leading to faster single-thread execution. But, a fully populated system with N threads would continue to interfere, potentially slowing overall progress. The amount of time the application executes becomes the combination of the amount of work executed in one quantum, together with the runtime attributed to that quantum. These quanta can then be clustered in order to identify similar work, and therefore identify similar runtime behavior. Although BBVs are used in this work, other feature vector information <ref type="bibr" target="#b11">[12]</ref> can be concatenated on a per-thread basis and can be used in this methodology.</p><p>The BBVs are projected down to 100 dimensions by random linear projection to bring down the computing requirements for the clustering algorithm. We use K-means clustering technique <ref type="bibr" target="#b23">[24]</ref> along with a BIC goodness criteria <ref type="bibr" target="#b24">[25]</ref> to select clustering in a method similar to previous work <ref type="bibr" target="#b0">[1]</ref>. The K-means algorithm requires the selection of the maximum number of clusters that we can expect, maxK, for which we use maxK = 50.</p><p>Because we use BBV data that represents both parallelism and work executed, we can now cluster the regions, and use the resulting clusters for workload extrapolation. We choose the BBV that is closest to the centroid of each cluster to be the representative of the cluster. We generate the region that represents each cluster from the original application based on the region boundaries and call them looppoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Warmup</head><p>For high-performance, we will want to simulate each looppoint separately, in parallel, given enough resources. For accurate results, the microarchitectural state needs to be warmed up at the start of simulation of each region. There are several techniques <ref type="bibr" target="#b25">[26]</ref>- <ref type="bibr" target="#b27">[28]</ref> proposed to warmup cache state. For binary-driven simulation, we warm up each region from the start of the application to minimize warmup error. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Runtime Extrapolation</head><p>Once the representatives are simulated, we can estimate the overall application execution time through the use of weightbased extrapolation. In this methodology, we use the percentage of work that this region represents, based on the instruction count of the entire collection of representatives that have been clustered together relative to the total amount of work done in the original application (quantum multiplier), to extrapolate the final runtime performance. The instructions that contribute to spin-loops are not considered here. The final step of this methodology uses the simulation results of these identified representatives, along with the multiplier, to reconstruct the overall workload runtime.</p><p>Our runtime extrapolation uses the below mentioned formula considering N looppoints identified as rep 1 to rep N :</p><formula xml:id="formula_0">total runtime = repN i=rep1 runtime i × multiplier i (1)</formula><p>The multiplier of a looppoint is the ratio of the sum of the filtered instruction counts from all of the regions that are represented by the looppoint to the filtered instruction count of that looppoint.</p><formula xml:id="formula_1">multiplier j = m i=0 inscount i inscount j (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where m is the number of regions that are represented by the j th looppoint. We evaluate our region selection methodology by comparing the extrapolated runtime based on region simulation with the actual runtime based on the whole-application simulation to compute the prediction error. We demonstrate runtime extrapolation using the above formula, but this methodology can be used for any event of interest, such as cache and branch miss counts, for example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Reproducible Application Execution for Accurate Analysis</head><p>The execution path of a multi-threaded application can vary from run to run due to several factors. One requirement to use this methodology is the ability to analyze a multithreaded application in a repeatable way. Traditional execution environments do not support this type of execution to allow for reliable, reproducible execution. We leverage Intel's Pin <ref type="bibr" target="#b28">[29]</ref> and Pinplay <ref type="bibr" target="#b15">[16]</ref> tools to generate reproducible, constrained, multi-threaded execution snapshots, called pinballs, to allow for repeatable analysis. Pinballs are more advanced than a trace file in that they contain a snapshot of the execution state of an application (registers and memory). By replaying the Pinball, we can analyze the properties of an application to collect the microarchitecture-independent execution signatures of the application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Putting it All Together</head><p>Together, the combination of reproducible replay of applications, along with the identification and clustering of workload characteristics, allows us to build an end-to-end methodology to identify workload representatives for performance extrapolation. Previous works <ref type="bibr" target="#b11">[12]</ref> have shown that extrapolation in this manner does apply to runtime, as well as other metrics of interest. The insights with respect to the identification of application parallelism, as well as the constrained, reproducible execution of the workloads allow us to analyze, cluster and extrapolate multi-threaded workloads across a number of synchronization types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J. Speed-up Potential</head><p>One of the most significant benefits of a checkpoint-based methodology is the ability to substantially reduce the amount of work that needs to be simulated to estimate the entire application performance. Simulator performance relates directly to the required length and number of regions to simulate. In addition, checkpoints can be simulated in parallel, with enough resources available, speeding time-to-results significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K. Workload Applicability</head><p>The methodology that we present here targets statically scheduled generic multi-threaded workloads regardless of the synchronization mechanisms used in order to simulate them in a faster way that was not possible before. Dynamically scheduled multi-threaded applications could require a different methodology for sampling as such workloads might not be able to be analysed or sampled based on an upfront analysis of the application. This is because such applications can interact with other threads in ways that were not seen in the initial execution of the application, potentially leading to an incorrect simulation run-time extrapolation. All checkpointbased methodologies (including BarrierPoint) require upfront application analysis. We address the problem of workload imbalance among the threads (a heterogeneous workload) by keeping per-thread information intact while clustering the individual regions. Like other checkpoint-based methodologies, we also assume that the hardware configuration is known upfront. This configuration is free from any run-time-dependent configuration changes or unexpected events that trigger a configuration change while the application is running. An example of a dynamic event is thermal throttling resulting in a dynamic voltage and frequency scaling (DVFS) event, which can affect the application performance and is runtime-and hardware-dependent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL SETUP</head><p>In this section, we describe the setup on which we conducted our experiments to evaluate our generic multi-threaded sampling methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Simulation Infrastructure</head><p>In this work, we use Sniper multicore simulation infrastructure <ref type="bibr" target="#b5">[6]</ref> (version 7.4) with modifications to support PCbased simulation region specification. We configured Sniper to model a multicore out-of-order processor resembling the Intel Gainestown microarchitecture using an 8 or 16-core processor model to simulate 8 or 16-threaded (respectively) applications. The simulated system characteristics that we use are detailed in Table <ref type="table" target="#tab_0">I</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Workloads</head><p>In order to evaluate the proposed methodology, we consider the SPEC CPU2017 <ref type="bibr" target="#b30">[31]</ref> benchmark suite. SPEC CPU2017 is available in two different versions depending on the evaluation purpose: rate and speed <ref type="bibr" target="#b31">[32]</ref>. The rate version is used to estimate the throughput of the underlying system whereas   We also use NAS Parallel Benchmarks (NPB) <ref type="bibr" target="#b32">[33]</ref>, [34] version 3.3 with OpenMP based parallelization <ref type="bibr" target="#b34">[35]</ref> that use class C inputs. We evaluate all benchmarks in the suite with both 8 and 16 threads, but do not evaluate the npb-dc (data cube) benchmark because of the large amount of data generated by that application. These benchmarks are compiled using GCC 5.5 for applications in C and GFortran for Fortran applications with -O3 optimizations for the x86-64 architecture.</p><p>We consider both active and passive wait policies for thread synchronization of the SPEC CPU2017 OpenMP applications. We use the passive OpenMP wait policy to configure NPB benchmarks. In passive wait policy, the threads do not spin while waiting for other threads. Meanwhile in the case of active wait policy, the threads remain active and they consume processor cycles while waiting by executing spinloops. The use of (PC, count) region specification can accurately represent a region over multiple runs even in the presence of spin-loops, which is not possible if the region specification is based on global or per-thread instruction counts.</p><p>For each benchmark, we record the execution path of the whole application and keep it as a pinball so that it can be replayed in both constrained and unconstrained mode later on. We have developed Pintools <ref type="bibr" target="#b28">[29]</ref> to generate BBVs of the regions which are fed to Simpoint for clustering the regions to identify the representative regions. We also have employed Pintools to restrict the forward progress of all the threads in a well balanced way thereby avoiding the chances of recording a skewed trace because of CPU load imbalances. The representative regions identified are simulated in parallel. We evaluate the runtime accuracy of the chosen representatives by simulating in constrained and unconstrained modes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Constrained Execution Infrastructure</head><p>We use Intel's PinPlay <ref type="bibr" target="#b15">[16]</ref> infrastructure that provides tools to record and replay arbitrary regions of a program execution. The recorder captures the execution of an application in a set of files collectively called a pinball <ref type="bibr" target="#b19">[20]</ref> which can later be replayed on any machine since pinballs are portable. A pinball consists of a memory file (.text), the architecture register values at the beginning of the execution region in per-thread register files (.reg), a set of memory and register values in per-thread injection files (.sel), and a subset of shared-memory dependencies among various threads in perthread dependency files (.race). A pinball once captured is self-contained, which means that both the application binary and inputs are not needed during replay of the pinball.</p><p>The replayer loads the initial memory and register state and starts executing the restored program region like a regularly loaded binary. System calls are skipped and their side-effects are injected. Shared-memory access in all threads are monitored and the threads are artificially delayed as needed to enforce the access order as recorded in the pinball. Finally, the replay is ended gracefully when the exit condition is met. Since system calls are skipped during replay, a pinball can be replayed across different operating systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. DCFG and Basic Blocks</head><p>The Dynamic Control Flow Graph (DCFG) is created by executing the program via a pin-tool enabled with the DCFG library <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>. Internally, the pin-tool hooks the controlflow instructions and records a count of each of the resulting edges throughout the execution of the workload on a perthread basis. At the end of the execution, fall-through edges are created to ensure non-overlapping basic blocks. These basic blocks are guaranteed to have only one entry and one exit point and not overlap with each other. In this way, they differ from the basic block structures in Pin, which do not have these guarantees. The resulting basic blocks and the edges that connect them thus create a connected graph. From this graph, routine boundaries are identified based on call edges and heuristics to handle non-standard routines that are sometimes found in non-compiled code. Inside the sub-graph of each routine, the immediate dominators of each node are found. Loops are then identified using the immediate dominator relationships. The graph, including the identified routines and loops are recorded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Unconstrained Replay</head><p>PinPlay's replayer enforces determinism among the threads by injecting recorded system call side-effects and enforcing the recorded shared memory access thread order. We use this mode when analyzing the workload (collecting BBVs and DCFGs to be used in the clustering phase), to ensure different steps of the profiling methodology have a consistent view of the program's execution flow (as recorded during the initial whole-program recording). However, during performance simulation, we want the timing model to control thread progress and synchronization, not PinPlay as this can introduce artificial thread stalls<ref type="foot" target="#foot_0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Synchronization Handling</head><p>OpenMP active runs, enabled by setting the environment variable OMP WAIT POLICY to ACTIVE <ref type="bibr" target="#b38">[39]</ref>, have threads busy-waiting at user-level (as opposed to using futex() in the passive runs). We replay a pinball that was recorded earlier for reproducible analysis for the generation of BBVs. If we directly use the recording we encounter the busywaiting code that was originally executed by the application. However, the busy-waiting code can differ if the application is executed another time with different conditions. While busywaiting consumes processor cycles, they do not contribute to the real work done by the program. Therefore, we ignore busy-waiting during BBV profiling, yet include it during simulation. Identifying busy-waiting code automatically <ref type="bibr" target="#b39">[40]</ref> can be a challenge and is yet another research problem. In our methodology, we ignore the entire code from the relevant synchronization library (libiomp5.so in our case). Note that this idea can easily be extended to other compilers and threading libraries. For example, in the case of applications using pthread synchronization, we can ignore the code from the libpthread library. The filtered instruction count is up to 40% (for 657.xz_s.2) fewer than the original instruction count for the active runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EVALUATION</head><p>In this section, we present the evaluation results of Loop-Point methodology. We analyse the effect of various model parameters that make up the methodology. We also evaluate the accuracy and the speedup achieved using LoopPoint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Accuracy</head><p>We show the accuracy of LoopPoint methodology by comparing the predicted runtime and the actual runtime of the application. The predicted runtime is calculated by considering the performance of all the representative regions as mentioned in Section III-G. The representative regions are augmented with a warmup region so that the microarchitectural state is warmed when the detailed region starts simulating. The prediction error of our methodology is the percentage difference in the simulation performance of the whole application and the extrapolated performance making use of the performance of all the representative regions identified for the application.</p><p>1) Constrained and unconstrained simulations: The Loop-Point methodology is tested for applications using the active and passive wait policies, and the simulation results are given here. Synchronizing multi-threaded applications with active wait policy uses spinloops to synchronize the threads. Sampling such an application can be considered a difficult problem to solve. We ignore the instructions that contribute to spin-loops during BBV generation and clustering phases as described in Section IV-F.</p><p>We perform binary-driven unconstrained simulations of the whole application as well as the representatives to measure the performance. In order to mark the region boundaries using (PC, count) correctly, we need to keep spin-loops away from being the region boundaries as mentioned earlier. We limit the region boundaries to be from the application code and not from any of the library code. Here, we make an assumption that the synchronization code can only be present in the libraries.</p><p>The region checkpoints are generated as pinballs which can be used for constrained simulation. We assume a large enough warmup region added to the representative region while generating the pinball checkpoint. However, using constrained simulation introduces artificial thread delays and are therefore not reliable for performance extrapolation. There are several ways to simulate these pinball checkpoints in an unconstrained way. One such method is to convert them to ELF binaries, called ELFies, as discussed in a prior work <ref type="bibr" target="#b20">[21]</ref>. In this paper, however, we are not evaluating ELFies. Instead, we consider the region boundaries specified as (PC, count) to perform unconstrained simulation using the application binaries by providing perfect warmup before the start of detailed simulation. One caveat that we want to mention is that not all region boundaries specified using (PC, count) can provide stable regions. For instance, applications can have certain code blocks that are selectively executed with respect to the underlying microarchitecture. Such code blocks or PCs cannot serve as stable (PC, count) region boundaries. We assume that the users can choose the appropriate stable regions, and that, while straight-forward to accomplish in an automated way, we leave that analysis to future work.</p><p>Results when simulating constrained simulation can be misleading and can lead to high errors. For example, we observe a runtime error for 657.xz_s.2 of up to 19.6% while simulating in a constrained environment. One of the reasons that using constrained simulation infrastructure can result in high error rates is that the simulation itself does not properly mimic the real application run. Instead, the application tries to replicate the behavior that was recorded previously on a specific machine. For instance, constrained execution forces spin-loops to be replayed even though this would not occur in a real execution. This introduces high error for applications, like 657.xz_s.2, that have fewer synchronization points compared to other applications in the SPEC CPU2017 benchmark suite, and therefore can see high variability from run to run.</p><p>The runtime prediction results (Figure <ref type="figure">5a</ref>) using the unconstrained simulation of active applications yield an average absolute error of just 2.33%, whereas that of passive applications is 2.23%. These error rates are comparable to previous sampling methodologies <ref type="bibr" target="#b11">[12]</ref>.</p><p>The looppoints identified are representative of the application across microarchitectural configurations. Our up-front analysis is solely based on architecture-level details, not microarchitectural settings or simulation details. Figure <ref type="figure">5b</ref> shows the error in predicting the runtime of the same applications while simulated for an inorder core instead of the out-of-order Gainestown-like core, while keeping all other simulation parameters the default as in Table <ref type="table" target="#tab_0">I</ref>. The graph clearly shows that looppoints can be portable across microarchitectures.</p><p>2) Varying the number of threads: We show that LoopPoint supports varying the number of application threads. Figure <ref type="figure">6</ref> shows the error rates while predicting the runtime of the NPB benchmarks. The applications are evaluated using 8 threads and 16 threads. Note that the applications using a different number of threads need to be profiled separately, as discussed in Section III. We observe that the average absolute error obtained is 2.87% for 8-threaded applications while for the 16-threaded applications it is as low as 1.78%.</p><p>3) Comparison of other metrics: Figure <ref type="figure">7</ref> shows the performance prediction of several metrics while simulated on an unconstrained environment for applications using active and passive wait policies. LoopPoint can determine microarchitectural metrics like the number of cycles (Figure <ref type="figure">7a</ref>), branch miss rate or MPKI (Figure <ref type="figure">7b</ref>), the miss rates or MPKI of different components in the memory hierarchy (Figure <ref type="figure">7c</ref>), etc. In Figure <ref type="figure">7b</ref> and Figure <ref type="figure">7c</ref>, we show the absolute differences  in the metrics predicted, rather than the percentage error in prediction, because those metrics have small absolute values and a small difference can result in a high percentage error.</p><p>Previous research <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b40">[41]</ref> has presented differences in a similar manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Speedup</head><p>We consider speedup in two different ways: theoretical speedup and actual speedup. Theoretical speedup is the reduction in the number of instructions (ignoring the instructions that contribute to spinloops) to be simulated in detail when using LoopPoint methodology. We also define the actual speedup as the reduction in the simulated runtime using LoopPoint with respect to the simulated runtime of the whole application.</p><p>Serial speedup is the speedup achieved when all the representatives are simulated back-to-back. It is the overall reduction in work given the serial execution of both the full, and reduced workload. Parallel speedup assumes sufficient parallel resources, and evaluates the speedup given the execution of all regions in parallel.</p><p>In Figures <ref type="figure" target="#fig_4">8 and 9</ref>, we see both the serial and parallel speedups for these applications. We obtain a maximum speedup of 801× for the applications with train inputs and 31,253× for the applications with ref inputs. The average serial speedup for applications using train inputs and ref inputs are respectively 9× and 244× whereas the average parallel speedup for the applications are 303× and 11,587× respectively for train and ref inputs. This implies that a significant reduction of simulation resources is now possible using the LoopPoint methodology, where simulations that would take months to complete can now be finished in hours.</p><p>In Figure <ref type="figure" target="#fig_4">9</ref>, we compare the theoretical simulation speedup using LoopPoint and BarrierPoint for the benchmarks using ref inputs. Note that we do not plot the actual speedup values using the ref inputs. We first validate our methodology with train inputs, and by extension, we analyze and simulate ref input representatives to estimate the performance of the larger application with confidence. Unfortunately, it is not possible to validate the error rates for applications with ref inputs because the full runs take too long to simulate (a few months to years as shown in Figure <ref type="figure">1</ref>).</p><p>We observe that LoopPoint consistently achieves good speedup whereas BarrerPoint lags behind for a number of applications. LoopPoint is able to reduce the application into representative regions that can finish simulation in a reasonable time. Additionally, with the BarrierPoint methodology, there is no guarantee on the size of a representative region. For example, the 8-threaded 638.imagick_s.1 benchmark has a very large inter-barrier region (93.06 B instructions) that is comparable to the size of the entire application (93.35 B instructions), defeating the purpose of sampling. However, there are a few applications for which BarrierPoint outperforms LoopPoint. Those applications have a large number of barriers and the inter-barrier regions are typically smaller than the LoopPoint  regions. BarrierPoint is unsuitable to evaluate both of the 657.xz_s applications as they do not contain barriers at all. Overall, a hybrid approach can be chosen to speed up smaller applications, but LoopPoint provides the first methodology to allow for generic sampling of applications that results both in a high simulation speedup and accuracy. We also show the speedup achieved using NPB applications in Figure <ref type="figure" target="#fig_5">10</ref>. LoopPoint achieves good speedups while the applications are evaluated for 8 threads as well as 16 threads. The maximum parallel speedup achieved while evaluating the 8-threaded applications is 2,503× with an average of 1,031×, whereas for the 16-threaded applications, the maximum speedup achieved is 1,498× and an average of 606×. Do note that NPB applications are less complex and more repetitive in nature than SPEC CPU2017 applications. Therefore, the error rates are lower and the speedups achieved are larger when compared to the train inputs of the SPEC CPU2017 suite.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RELATED WORK</head><p>Before architects build new hardware designs, it is extremely useful to predict the hardware design's power, performance and area (cost). Existing circuit-design tools are able to simulate complex, modern applications on large, multi-core systems, but at a cost of significant simulation time that can be intractable (requiring months to years of simulation time for the SPEC CPU2017 benchmarks). While there have been many attempts to solve this problem, previous works were unable to provide a combination of three things for multi-threaded workloads: (1) choosing accurate representatives without detailed simulation, (2) demonstrating simulation speedup based on application representatives, not on overall application runtime and (3) allowing the simulation of hardware designs that might not yet have analytical models. Our proposal addresses all these concerns through the determination of application parallelism, clustering and the extrapolating the results based on this information.</p><p>Single-threaded Sampling Methodologies. One of the first works to utilize the structure within the code to create a representative sample application was the Simpoint methodology <ref type="bibr" target="#b0">[1]</ref>. The authors show that applications can be broken down into N smaller chunks of size 100M instructions, where N is the number of clusters to which the whole program can be clustered. This helps to determine where to simulate for a large singlethreaded application. SMARTS <ref type="bibr" target="#b1">[2]</ref> is another methodology which uses alternating fast-forward and detailed simulation phases for a large number of samples. They used a large number of intervals with regions having very small numbers of instructions. Large structures like caches are warmed in the fast-forward mode. The methodology could estimate the average IPC with high confidence. LiveSim <ref type="bibr" target="#b41">[42]</ref> is another such methodology that enables interactive simulation making use of in-memory checkpoints. A prior work on software phase markers <ref type="bibr" target="#b18">[19]</ref> uses loops to determine simulation regions, but is limited in that they only provide support for single-threaded applications using phase markers denoting phase changes.</p><p>Multi-threaded Sampling Methodologies. Ekman et al. <ref type="bibr" target="#b42">[43]</ref> propose a methodology to reduce the number of simulation points using a matched-pair comparison method to estimate the full application performance. SimFlex <ref type="bibr" target="#b8">[9]</ref> extends SMARTS methodology to support multiprocessor applications with an increased sample length. SMARTS and SimFlex use random sampling and therefore the samples are not necessarily representative. Perelman et al. in <ref type="bibr" target="#b43">[44]</ref> extend the Simpoint methodology to use for phase analysis of multi-threaded workloads.</p><p>The instruction-count-based sampling mechanisms for singlethreaded applications cannot be used directly for synchronizing multi-threaded applications. To sample multi-threaded applications, <ref type="bibr" target="#b9">[10]</ref> and <ref type="bibr" target="#b10">[11]</ref> were proposed which use time as a sampling unit by fast-forwarding between the detailed simulation intervals. The execution time during the fastforwarding phase is extrapolated. But these techniques need to functionally simulate the entire application, which limits the speedup of simulation. Another methodology which reduces the simulation time of MT applications significantly is <ref type="bibr" target="#b11">[12]</ref>, a microarchitecture-independent, Simpoint-like approach which operates on OpenMP barrier synchronized applications by identifying inter-barrier regions as the unit of work. This technique takes into account the fact that all the threads are synchronized after a barrier, but is less effective when interbarrier regions are very large or non-existent.</p><p>TaskPoint <ref type="bibr" target="#b12">[13]</ref> is another work which proposes a sampling methodology which is applicable to a subset of multi-threaded applications. The technique is applicable to task-based programs and considers task instances as sampling units. The intervals between detailed simulation phases are fast-forwarded.</p><p>Analytical Modeling. There has recently been some progress on the development of a completely analytical model for single-threaded <ref type="bibr" target="#b44">[45]</ref> [46] and multi-threaded <ref type="bibr" target="#b46">[47]</ref> workloads. One major drawback of analytical models is the inability to estimate the performance of next-generation hardware designs. New processor, cache, and memory techniques without analytical models will not be able to use these methodologies. The general evaluation of future hardware designs can therefore require the use of execution-driven analysis.</p><p>Constrained simulation. Multi-threaded checkpoints were used <ref type="bibr" target="#b37">[38]</ref> for constrained simulation. Their goal was to estimate the relative performance analysis of regions-of-interest across multiple micro-architectures. They describe a mechanism for speedup computation in the presence of artificial stalls added by the constrained replay of checkpoints during simulation. There could be cases where the speedup computation is inconclusive. We support unconstrained simulation as well as constrained simulation and also provide an absolute performance extrapolation methodology. For relative, cross-micro-architectural performance analysis, unconstrained simulation is desirable as it need not have to deal with artificial stalls.</p><p>Handling busy-waiting. The problem of busy-waiting is mentioned in <ref type="bibr" target="#b15">[16]</ref> although in the context of multi-process programs using Message Passing Interface (MPI). The work focuses on simulating a specific single-threaded process from multiple processes in an MPI program and uses the selective logging feature of PinPlay to exclude the busy waiting code from consideration, both in the profiling and simulation phases.</p><p>Statistical workload generation. There are different works that study the time-varying runtime behavior of standard benchmarks. Wu et al. <ref type="bibr" target="#b47">[48]</ref> study the phase behavior of SPEC CPU2017 workloads. Moreover, the work identifies the singlethreaded simulation points using SimPoint methodology and correlates them with the phase behavior. Nair et al. <ref type="bibr" target="#b48">[49]</ref> study the phase behavior of SPEC CPU2006 and SPEC CPU2000 using SimPoint methodology. The work identifies similar CPI prediction results using SimPoint for the applications in both suites and concludes that these applications have similar phase behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>The need to understand larger, more complex multi-core processors continues to increase. This becomes even more critical as the multi-core processors (and the serial code) tend to be the bottleneck in highly parallel applications. Generalpurpose applications are found on embedded devices, mobile phones, and back-end data center servers. While each platform has its requirements and demands, the need for an accurate understanding of the applications at hand is clear.</p><p>Simulation solutions alone are insufficient because of the significant slowdown (10,000× or more <ref type="bibr" target="#b49">[50]</ref>) seen when simulating applications with industrial-quality simulators. Simulation solutions today require alternatives like sampling to reduce the workloads to realistic simulation times. But, current sampling solutions either target single-threaded workloads or are only applicable to specific workload types.</p><p>In this work, we present a generic multi-threaded sampling methodology, one that considers the inherent parallelism of the application and allows for automatic reduction of workloads to sizes that are on the order of the representatives of the workloads themselves. We demonstrate how our classification methodology automatically partitions the workload into representatives and allows us to predict the performance of the workloads at hand with high accuracy.</p><p>2) Request access to the Sniper git repository <ref type="bibr" target="#b52">[53]</ref>  4) These steps should automatically download the required versions of Pin kit and Sniper, and apply the required patches. All the other tools that we use are provided along with the artifact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Experiment workflow</head><p>In this section, we describe the steps to generate the results shown in the paper. The end-to-end methodology of LoopPoint involves several steps. However, we have automated the process so that the user need not run every single step.</p><p>In the first stage, the selected benchmark is executed to record it as a pinball in whole_program.&lt;input&gt; directory. This pinball is then profiled by generating BBV data and making use of DCFG information generated by Pin. This is stored in &lt;basename&gt;.Data directory. The BBVs are clustered using K-means clustering and the representative region boundaries are identified.</p><p>In the following stage, the representative region information is used to launch region simulations, one after the other. Note that the region simulations can be launched in parallel as the runs are independent of each other. One can update these scripts to run several simulations in parallel or to run them on a collection of machines.</p><p>When all the region simulations are completed, the runtime of the full application is extrapolated using the runtimes of the representative regions and compared with that of the full application run. The estimated error and speedup numbers are displayed as the final output on the console.</p><p>All the profiling and simulation results are stored in the results directory. The end-to-end methodology is automated completely for ease of use.</p><p>The driver tool to process and evaluate the key results of sampling a multi-threaded application is the run-looppoint.py script which controls launching the profiling and simulation runs. The arguments are defined as follows:</p><p>• -p or --program: program to be executed, supplied in the format &lt;suite&gt;-&lt;application&gt;-&lt;input-num&gt; Multiple programs can be submitted as comma-separated values Default: demo-matrix-1 • -n or --ncores: number of threads Default: 8 • -i or --input-class: input class Default: test • -w or --wait-policy: OpenMP wait policy Options: passive, active Default: passive • --force: start a new set of end-to-end run • --reuse-profile: reuse the default profiling data (used along with --force) • --reuse-fullsim: reuse the default full application simulation (used along with --force) • --native: Run the application natively Below are some example commands that can be used for the users to readily run the tool after installation.</p><p>Usage Examples: 1) ./run-looppoint.py -p demo-matrix-1 -n 8</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>--force</head><p>This example starts a new end-to-end run for the demo-matrix-1 program using test inputs with 8 cores and using the passive wait policy. 2) ./run-looppoint.py -p demo-matrix-2,demo-matrix-3 -w active -i test --force This example starts a new end-to-end run for the demo-matrix-2 program followed by the demo-matrix-3 program using the active wait policy, 8 threads, and test inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Evaluation and expected results</head><p>To replicate the results shown in this paper, it is necessary to run each of the applications in SPEC CPU2017 benchmark suite. One can integrate any multi-threaded application in a similar fashion (the demo application matrix-omp, can be used as an example). Note that launching an end-to-end evaluation can be long-running for large applications as the full application simulation can take a long time.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: An example of a representative region identified by LoopPoint. (4a) The numbers represent iterations of the corresponding loops that form the 8-threaded region. The start point and end point of the chosen region is at line 3022, the entry point of loop u. (4b) The top graph shows the variation of IPC over time for the full application run, while the bottom graph shows that of the chosen region. The (PC, count) boundaries are marked inside the IPC graph of the region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :Fig. 6 :</head><label>56</label><figDesc>Fig.5: The runtime prediction errors of SPEC CPU2017 applications (train inputs) using active and passive wait policies that use 8 threads for unconstrained simulation. The y-axis represents the percent error in predicting the runtime of each of the applications along x-axis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: LoopPoint and BarrierPoint theoretical speedup for SPEC CPU2017 applications (passive wait policy) using ref inputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 10 :</head><label>10</label><figDesc>Fig. 10: A comparison of actual speedups achieved by Loop-Point when the applications use 8 and 16 cores. Speedups listed for the NPB suite using the C input set and a passive wait policy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>The primary characteristics of the simulated system.</figDesc><table><row><cell>Component</cell><cell>Features</cell></row><row><cell>Processor</cell><cell>8 &amp; 16 cores, Gainestown-like microarch.</cell></row><row><cell>Core</cell><cell>2.66 GHz, 128 entry ROB</cell></row><row><cell>Branch predictor</cell><cell>Pentium M</cell></row><row><cell>L1-I cache</cell><cell>32K, 4-way, LRU</cell></row><row><cell>L1-D cache</cell><cell>32K, 8-way, LRU</cell></row><row><cell>L2 cache</cell><cell>256K, 8-way, LRU</cell></row><row><cell>L3 cache</cell><cell>8M, 16-way, LRU</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II</head><label>II</label><figDesc></figDesc><table><row><cell cols="4">: SPEC CPU2017 speed application attributes.</cell></row><row><cell cols="4">F=Fortran, KLOC=thousand lines of code. From [30]</cell></row><row><cell>Application</cell><cell>Lang.</cell><cell>KLOC</cell><cell>Application Area</cell></row><row><cell>603.bwaves</cell><cell>F</cell><cell>1</cell><cell>Explosion modeling</cell></row><row><cell cols="2">607.cactuBSSN F, C++</cell><cell>257</cell><cell>Physics: relativity</cell></row><row><cell>619.lbm</cell><cell>C</cell><cell>1</cell><cell>Fluid dynamics</cell></row><row><cell>621.wrf</cell><cell>F, C</cell><cell>991</cell><cell>Weather forecasting</cell></row><row><cell>627.cam4</cell><cell>F, C</cell><cell>407</cell><cell>Atmosphere modeling</cell></row><row><cell>628.pop2</cell><cell>F, C</cell><cell cols="2">338 Wide-scale ocean modeling</cell></row><row><cell>638.imagick</cell><cell>C</cell><cell>259</cell><cell>Image manipulation</cell></row><row><cell>644.nab</cell><cell>C</cell><cell>24</cell><cell>Molecular dynamics</cell></row><row><cell>649.fotonik3d</cell><cell>F</cell><cell>14</cell><cell>Comp. Electromagnetics</cell></row><row><cell>654.roms</cell><cell>F</cell><cell>210</cell><cell>Regional ocean modeling</cell></row></table><note>the speed version is used to estimate the runtime of the benchmark on the system. Unlike prior versions of SPEC benchmarks, CPU2017 includes a set of synchronizing multithreaded programs that share memory consisting of OpenMPcompatible multi-threaded applications. We use the speed version of SPEC CPU2017 with train inputs and eight threads (See TableIIfor application descriptions) for our evaluation. The train input set is used so as to keep the full program simulation time to a reasonable length. As the detailed simulation of the full SPEC CPU2017 applications with ref inputs is not practical, computing the sampling error is also not feasible. Therefore, we utilize the ref inputs to estimate the potential speedup of the methodology in the paper. The benchmarks we use include OpenMP directives, with a summary of the primitives used described in (TableIII).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III</head><label>III</label><figDesc>All SPEC CPU2017 workloads except 657.xz_s runs are 8-threaded. 657.xz_s.2 runs with 4-threads whereas 657.xz_s.1 runs as a single-threaded application.All the benchmarks in the SPEC CPU2017 benchmark suite are compiled using the Intel compiler toolchain (Intel Parallel Studio XE, version 2019 Update 2) with optimizations enabled (-O2) and debug information available for binary to sourcelevel mapping, and built for the 64-bit x86 instruction-set architecture.</figDesc><table><row><cell cols="7">: SPEC CPU2017 speed synchronization primi-</cell></row><row><cell cols="7">tives used. sta4=static for, dyn4=dynamic for, bar=barrier,</cell></row><row><cell cols="7">ma=master, si=single, red=reduction, at=atomic, lck=lock.</cell></row><row><cell>Application</cell><cell cols="2">sta4 dyn4</cell><cell cols="2">bar ma</cell><cell>si</cell><cell>red</cell><cell>at lck</cell></row><row><cell>603.bwaves</cell><cell>Y</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Y</cell><cell>Y</cell></row><row><cell>607.cactuBSSN</cell><cell>Y</cell><cell>Y</cell><cell>Y</cell><cell></cell><cell></cell><cell>Y</cell><cell>Y</cell></row><row><cell>619.lbm</cell><cell>Y</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>621.wrf</cell><cell></cell><cell>Y</cell><cell></cell><cell>Y</cell><cell></cell></row><row><cell>627.cam4</cell><cell>Y</cell><cell>Y</cell><cell>Y</cell><cell>Y</cell><cell></cell></row><row><cell>628.pop2</cell><cell>Y</cell><cell></cell><cell>Y</cell><cell>Y</cell><cell></cell></row><row><cell>638.imagick</cell><cell>Y</cell><cell></cell><cell>Y</cell><cell>Y</cell><cell>Y</cell><cell>Y</cell></row><row><cell>644.nab</cell><cell></cell><cell>Y</cell><cell>Y</cell><cell></cell><cell></cell><cell>Y</cell><cell>Y</cell></row><row><cell>649.fotonik3d</cell><cell>Y</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>654.roms</cell><cell>Y</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>The prediction errors of various metrics for SPEC CPU2017 benchmarks using LoopPoint. The benchmarks use active and passive wait policies with train inputs and 8 threads, and are simulated in realistic unconstrained mode.</figDesc><table><row><cell>abs. cycles error%</cell><cell>1 2 3 4 5 6 7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">active</cell><cell></cell><cell></cell><cell cols="2">passive</cell><cell>branch MPKI abs. diff.</cell><cell>0.2 0.4 0.6 0.8</cell><cell></cell><cell cols="2">active</cell><cell></cell><cell></cell><cell cols="2">passive</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.36</cell><cell>L2 MPKI abs. diff.</cell><cell>1 2 3 4 5 6</cell><cell></cell><cell></cell><cell cols="2">active</cell><cell></cell><cell></cell><cell cols="2">passive</cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>603.bwaves_s.1</cell><cell>603.bwaves_s.2</cell><cell>607.cactuBSSN_s.1</cell><cell>619.lbm_s.1</cell><cell>621.wrf_s.1</cell><cell>627.cam4_s.1</cell><cell cols="2">628.pop2_s.1</cell><cell>638.imagick_s.1</cell><cell>644.nab_s.1</cell><cell>644.nab_s.2</cell><cell>649.fotonik3d_s.1</cell><cell>654.roms_s.1</cell><cell>657.xz_s.1</cell><cell>657.xz_s.2</cell><cell></cell><cell>0.0</cell><cell>603.bwaves_s.1</cell><cell>603.bwaves_s.2</cell><cell>607.cactuBSSN_s.1</cell><cell>619.lbm_s.1</cell><cell>621.wrf_s.1</cell><cell>627.cam4_s.1</cell><cell>628.pop2_s.1</cell><cell>638.imagick_s.1</cell><cell cols="2">644.nab_s.1</cell><cell>644.nab_s.2</cell><cell>649.fotonik3d_s.1</cell><cell>654.roms_s.1</cell><cell>657.xz_s.1</cell><cell>657.xz_s.2</cell><cell></cell><cell>0</cell><cell cols="2">603.bwaves_s.1</cell><cell>603.bwaves_s.2</cell><cell>607.cactuBSSN_s.1</cell><cell>619.lbm_s.1</cell><cell>621.wrf_s.1</cell><cell>627.cam4_s.1</cell><cell>628.pop2_s.1</cell><cell>638.imagick_s.1</cell><cell>644.nab_s.1</cell><cell>644.nab_s.2</cell><cell>649.fotonik3d_s.1</cell><cell>654.roms_s.1</cell><cell>657.xz_s.1</cell><cell>657.xz_s.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="10">(a) Number of Cycles</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="8">(b) Branch MPKI</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">(c) L2 MPKI</cell></row><row><cell cols="4">10 1 10 2 Fig. 7: 603.bwaves_s.1 Speedup 10 3</cell><cell>603.bwaves_s.2</cell><cell cols="4">607.cactuBSSN_s.1 Actual Serial 619.lbm_s.1</cell><cell cols="4">621.wrf_s.1 Theoretical 627.cam4_s.1 628.pop2_s.1</cell><cell></cell><cell>638.imagick_s.1</cell><cell>644.nab_s.1</cell><cell cols="3">644.nab_s.2 Actual Parallel 649.fotonik3d_s.1</cell><cell cols="4">654.roms_s.1 Theoretical 657.xz_s.1 657.xz_s.2</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Speedup</cell><cell cols="2">10 0 10 1 10 2 10 3 10 4</cell><cell cols="2">603.bwaves_s.1</cell><cell>603.bwaves_s.2</cell><cell cols="4">607.cactuBSSN_s.1 LoopPoint 619.lbm_s.1 Serial</cell><cell cols="3">621.wrf_s.1 Parallel 627.cam4_s.1</cell><cell>628.pop2_s.1</cell><cell></cell><cell>638.imagick_s.1</cell><cell cols="3">644.nab_s.1 BarrierPoint 649.fotonik3d_s.1 654.roms_s.1 Serial Parallel 657.xz_s.1</cell><cell>657.xz_s.2</cell></row><row><cell cols="24">Fig. 8: A comparison of theoretical and actual speedups</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="24">achieved by LoopPoint. The workload used is SPEC CPU2017</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="20">applications (active wait policy) using train inputs.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>to download Sniper. Provide a valid email address to receive the link to the Sniper Simulator (we used version 7.4 in this artifact). 3) Follow the below steps to setup and build the artifact once you have received the path to the Sniper git repository. a) Build the docker image Download and build the required tools once you have the Sniper gitid link $ make tools SNIPER_GIT_REPO="http:// snipersim.org/&lt;path-to-git-repo&gt;.git"</figDesc><table><row><cell>$ make build</cell></row><row><cell>b) Run the docker image</cell></row><row><cell>$ make</cell></row><row><cell>c) Build the provided applications</cell></row><row><cell>$ make apps</cell></row><row><cell>d)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">See<ref type="bibr" target="#b37">[38]</ref> for a methodology that uses constrained replay during multithreaded performance simulation, and which can, in limited cases, work around the artificial stalls.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work has benefited tremendously from the support, contribution, and suggestions of many individuals. We sincerely thank all of them. In particular, we acknowledge Chuck Yount, Carl Beckmann, Alexey Klimkin, Igor Ermolaev, and Ady Tal at Intel. The work was partially supported by a grant from Intel. We also thank the anonymous reviewers and our colleagues at NUS for their valuable comments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. ARTIFACT DESCRIPTION APPENDIX</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Abstract</head><p>In this artifact, we provide the required tools and information needed to replicate the primary experiments demonstrated in this paper. The artifact provides the necessary tools and scripts to run the three parts:</p><p>1) profiling the application to collect the necessary data needed for multi-threaded sampling; 2) sampled simulation of the selected regions; and 3) extrapolation of performance results, and plotting the key results This appendix describes these parts and how to run them to replicate our experiments. We have also included a demo multi-threaded application to test the methodology end-to-end. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Artifact check-list (meta-information)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Description</head><p>How to access: We use SPEC CPU2017 benchmark suite to evaluate the proposed methodology, LoopPoint. In this artifact, however, we do not include SPEC CPU2017 binaries and provide a demo application to test the end-to-end methodology.</p><p>The setup can be used to replicate any results that we show in the paper. Pin kit and Sniper will be downloaded while setting up the artifact (see Installation section for instructions). The tool binaries are provided that works with Intel Pin. The artifact is available on Zenodo with DOI 10.5281/zenodo.5667620 <ref type="bibr" target="#b50">[51]</ref> and on GitHub <ref type="bibr" target="#b51">[52]</ref>.</p><p>Hardware dependencies: The artifact is developed such that it runs on an x86-based Linux machine. We strongly recommend running the artifact using the provided Dockerfile. We expect the size of files generated in the profiling stage of LoopPoint to be a few GBs, hence we suggest a minimum free space of 50 GB when profiling SPEC CPU2017 with the train input set. Memory and space requirements are much lower for the included demo application.</p><p>Software dependencies:</p><p>Data sets:</p><p>• matrix-omp: An OpenMP based demo application that can be used to test the end-to-end methodology in a reasonable amount of time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Installation</head><p>1) Download the artifact from the Zenodo link and navigate to the artifact base directory.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automatically characterizing large scale program behavior</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sherwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Perelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hamerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
				<imprint>
			<date type="published" when="2002-10">Oct. 2002</date>
			<biblScope unit="page" from="45" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SMARTS: Accelerating microarchitecture simulation via rigorous statistical sampling</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Wunderlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Hoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture (ISCA)</title>
				<imprint>
			<date type="published" when="2003-06">Jun. 2003</date>
			<biblScope unit="page" from="84" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Minnespec: A new SPEC benchmark workload for simulation-based computer architecture research</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Kleinosowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Lilja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Architecture Letters (CAL)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="7" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improved automatic testcase synthesis for performance model validation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Supercomputing (SC)</title>
				<imprint>
			<date type="published" when="2005-06">Jun. 2005</date>
			<biblScope unit="page" from="111" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">FireSim: FPGAaccelerated cycle-exact scale-out system simulation in the public cloud</title>
		<author>
			<persName><forename type="first">S</forename><surname>Karandikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Biancolin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Amid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pemberton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Amaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kovacs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nikolic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Asanovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture (ISCA)</title>
				<imprint>
			<date type="published" when="2018-06">Jun. 2018</date>
			<biblScope unit="page" from="29" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sniper: Exploring the level of abstraction for scalable and accurate parallel multi-core simulation</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Heirman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for High Performance Computing, Networking, Storage and Analysis (SC)</title>
				<imprint>
			<date type="published" when="2011-11">Nov. 2011</date>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ZSim: Fast and accurate microarchitectural simulation of thousand-core systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture (ISCA)</title>
				<imprint>
			<date type="published" when="2013-06">Jun. 2013</date>
			<biblScope unit="page" from="475" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Variability in architectural simulations of multi-threaded workloads</title>
		<author>
			<persName><forename type="first">A</forename><surname>Alameldeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on High-Performance Computer Architecture (HPCA)</title>
				<imprint>
			<date type="published" when="2003-02">Feb. 2003</date>
			<biblScope unit="page" from="7" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">SimFlex: Statistical sampling of computer system simulation</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Wunderlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Hoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="31" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ESESC: A fast multicore simulator using time-based sampling</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Ardestani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Renau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on High Performance Computer Architecture (HPCA)</title>
				<imprint>
			<date type="published" when="2013-02">Feb. 2013</date>
			<biblScope unit="page" from="448" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sampled simulation of multi-threaded applications</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Heirman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
				<imprint>
			<date type="published" when="2013-04">Apr. 2013</date>
			<biblScope unit="page" from="2" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BarrierPoint: Sampled simulation of multi-threaded applications</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Heirman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Van Craeynest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
				<imprint>
			<date type="published" when="2014-03">Mar. 2014</date>
			<biblScope unit="page" from="2" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">TaskPoint: Sampled simulation of task-based programs</title>
		<author>
			<persName><forename type="first">T</forename><surname>Grass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moreto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ayguadé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
				<imprint>
			<date type="published" when="2016-04">Apr. 2016</date>
			<biblScope unit="page" from="296" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sampled simulation of task-based programs</title>
		<author>
			<persName><forename type="first">T</forename><surname>Grass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ceballos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ayguadé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moreto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Computers (TC)</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="255" to="269" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Evaluating non-deterministic multithreaded commercial workloads</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Alameldeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Mauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Sorin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Computer Architecture Evaluation using Commercial Workloads (CAECW)</title>
				<imprint>
			<date type="published" when="2002-02">Feb. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">PinPlay: A framework for deterministic replay and reproducible analysis of parallel programs</title>
		<author>
			<persName><forename type="first">H</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stallcup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lueck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cownie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Code Generation and Optimization (CGO)</title>
				<imprint>
			<date type="published" when="2010-04">Apr. 2010</date>
			<biblScope unit="page" from="2" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">IPC considered harmful for multiprocessor workloads</title>
		<author>
			<persName><forename type="first">A</forename><surname>Alameldeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="8" to="17" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">D</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Saphir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van Der Wijngaart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yarrow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NASA Ames Research Center, Tech. Rep</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
	<note>The NAS parallel benchmarks 2.0,&quot; NAS-95-020</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Selecting software phase markers with code structure analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Perelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Code Generation and Optimization (CGO)</title>
				<imprint>
			<date type="published" when="2006-03">Mar. 2006</date>
			<biblScope unit="page" from="135" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pinballs: portable and shareable user-level checkpoints for reproducible analysis and simulation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Reproducible Research Methodologies (REPRODUCE)</title>
				<imprint>
			<date type="published" when="2014-02">Feb. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">ELFies: Executable region checkpoints for performance analysis and simulation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Isaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Heirman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sabu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hajiabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Code Generation and Optimization (CGO)</title>
				<imprint>
			<date type="published" when="2021-03">Feb./Mar. 2021</date>
			<biblScope unit="page" from="126" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">How to use SimPoint to pick simulation points</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hamerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Perelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGMETRICS Performance Evaluation Review</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="25" to="30" />
			<date type="published" when="2004-03">Mar. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Motivation for variable length intervals and hierarchical phase behavior</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Perelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hamerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sherwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
				<imprint>
			<date type="published" when="2005-03">Mar. 2005</date>
			<biblScope unit="page" from="135" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cluster analysis of multivariate data: efficiency versus interpretability of classifications</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Forgy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="768" to="769" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Estimating the dimension of a model</title>
		<author>
			<persName><forename type="first">G</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="page" from="461" to="464" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Accelerating multiprocessor simulation with a memory timestamp record</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Asanovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
				<imprint>
			<date type="published" when="2005-03">Mar. 2005</date>
			<biblScope unit="page" from="66" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Simulation sampling with live-points</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Wunderlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Hoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
				<imprint>
			<date type="published" when="2006-03">Mar. 2006</date>
			<biblScope unit="page" from="2" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Directed statistical warming through time traveling</title>
		<author>
			<persName><forename type="first">N</forename><surname>Nikoleris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hagersten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Microarchitecture (MICRO)</title>
				<imprint>
			<date type="published" when="2019-10">Oct. 2019</date>
			<biblScope unit="page" from="1037" to="1049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pin: Building customized program analysis tools with dynamic instrumentation</title>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Muth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lowney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hazelwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Programming Language Design and Implementation (PLDI)</title>
				<imprint>
			<date type="published" when="2005-06">Jun. 2005</date>
			<biblScope unit="page" from="190" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">SPEC CPU®2017 documentation index</title>
		<ptr target="http://spec.org/cpu2017/Docs/index.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">SPEC CPU2017: Nextgeneration compute benchmark</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bucek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-D</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Kistowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Performance Engineering (ICPE)</title>
				<imprint>
			<date type="published" when="2018-04">Apr. 2018</date>
			<biblScope unit="page" from="41" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A workload characterization of the SPEC CPU2017 benchmark suite</title>
		<author>
			<persName><forename type="first">A</forename><surname>Limaye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Adegbija</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
				<imprint>
			<date type="published" when="2018-04">Apr. 2018</date>
			<biblScope unit="page" from="190" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The NAS parallel benchmarks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Barszcz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dagum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frederickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lasinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schreiber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Venkatakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Weeratunga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Browning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Supercomputer Applications</title>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The NAS parallel benchmarks summary and preliminary results</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Barszcz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Barton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Browning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dagum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Fatoohi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Frederickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Lasinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Schreiber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Venkatakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Weeratunga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Supercomputing (SC)</title>
				<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="158" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The OpenMP implementation of NAS parallel benchmarks and its performance</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Frumkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NASA Ames Research Center, Tech. Rep</title>
		<imprint>
			<date type="published" when="1999-10">Oct. 1999</date>
		</imprint>
	</monogr>
	<note type="report_type">NAS-99-011</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">DCFG generation with PinPlay</title>
		<ptr target="https://software.intel.com/content/www/us/en/develop/articles/pintool-dcfg.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Graph-matching-based simulationregion selection for multiple binaries</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yount</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Islam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
				<imprint>
			<date type="published" when="2015-03">Mar. 2015</date>
			<biblScope unit="page" from="52" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Reproducible simulation of multi-threaded workloads for architecture design exploration</title>
		<author>
			<persName><forename type="first">C</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Workload Characterization (IISWC)</title>
				<imprint>
			<date type="published" when="2008-09">Sep. 2008</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">OpenMP 3.1 API C/C++ Syntax Quick Reference Card</title>
		<ptr target="https://www.openmp.org/wp-content/uploads/OpenMP3.1-CCard.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Spin detection hardware for improved management of multithreaded systems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Lebeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Sorin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions on Parallel and Distributed Systems (TPDS)</title>
				<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="508" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pinpointing representative portions of large Intel Itanium programs with dynamic instrumentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Charney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karunanidhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Microarchitecture (MICRO)</title>
				<imprint>
			<date type="published" when="2004-12">Dec. 2004</date>
			<biblScope unit="page" from="81" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">LiveSim: Going live with microarchitecture simulation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Southern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Renau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on High Performance Computer Architecture (HPCA)</title>
				<imprint>
			<date type="published" when="2016-03">Mar. 2016</date>
			<biblScope unit="page" from="606" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Enhancing multiprocessor architecture simulation speed using matched-pair comparison</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stenstrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
				<imprint>
			<date type="published" when="2005-03">Mar. 2005</date>
			<biblScope unit="page" from="89" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Detecting phases in parallel applications on shared memory architectures</title>
		<author>
			<persName><forename type="first">E</forename><surname>Perelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Polito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Bouguet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sampson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dulong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Parallel Distributed Processing Symposium (IPDPS)</title>
				<imprint>
			<date type="published" when="2006-04">Apr. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Analytical processor performance and power modeling using micro-architecture independent characteristics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Van Den Steen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eyerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">De</forename><surname>Pestel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mechri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Black-Schaffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hagersten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Computers (TC)</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3537" to="3551" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Micro-architecture independent analytical processor performance and power modeling</title>
		<author>
			<persName><forename type="first">S</forename><surname>Van Den Steen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">De</forename><surname>Pestel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mechri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eyerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Black-Schaffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hagersten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
				<imprint>
			<date type="published" when="2015-03">Mar. 2015</date>
			<biblScope unit="page" from="32" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">RPPM: Rapid performance prediction of multithreaded workloads on multicore processors</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">De</forename><surname>Pestel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van Den Steen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Akram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
				<imprint>
			<date type="published" when="2019-03">Mar. 2019</date>
			<biblScope unit="page" from="257" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Invited paper for the hot workloads special session hot regions in SPEC CPU2017</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Flolid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Workload Characterization (IISWC)</title>
				<imprint>
			<date type="published" when="2018-10">Sep./Oct. 2018</date>
			<biblScope unit="page" from="71" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Simulation points for SPEC CPU 2006</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Design (ICCD)</title>
				<imprint>
			<date type="published" when="2008-10">Oct. 2008</date>
			<biblScope unit="page" from="397" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The gem5 simulator</title>
		<author>
			<persName><forename type="first">N</forename><surname>Binkert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Reinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Hower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sardashti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sewell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shoaib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vaish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2011-08">Aug. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">LoopPoint artifacts</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sabu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Heirman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.5667620</idno>
		<ptr target="https://doi.org/10.5281/zenodo.5667620" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">LoopPoint source code</title>
		<ptr target="https://github.com/nus-comparch/looppoint" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Sniper simulator</title>
		<ptr target="https://snipersim.org" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
