<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">How Can We Know What Language Models Know?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
							<email>zhengbaj@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University 1 Bosch Research North America</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Frank</forename><forename type="middle">F</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University 1 Bosch Research North America</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Araki</surname></persName>
							<email>jun.araki@us.bosch.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University 1 Bosch Research North America</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
							<email>gneubig@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University 1 Bosch Research North America</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">How Can We Know What Language Models Know?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent work has presented intriguing results examining the knowledge contained in language models (LM) by having the LM fill in the blanks of prompts such as "Obama is a by profession". These prompts are usually manually created, and quite possibly sub-optimal; another prompt such as "Obama worked as a " may result in more accurately predicting the correct profession. Because of this, given an inappropriate prompt, we might fail to retrieve facts that the LM does know, and thus any given prompt only provides a lower bound estimate of the knowledge contained in an LM. In this paper, we attempt to more accurately estimate the knowledge contained in LMs by automatically discovering better prompts to use in this querying process. Specifically, we propose miningbased and paraphrasing-based methods to automatically generate high-quality and diverse prompts and ensemble methods to combine answers from different prompts. Extensive experiments on the LAMA benchmark for extracting relational knowledge from LMs demonstrate that our methods can improve accuracy from 31.1% to 38.1%, providing a tighter lower bound on what LMs know. We have released the code and the resulting LM Prompt And Query Archive (LPAQA) at https:// github.com/jzbjyb/LPAQA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent years have seen the primary role of language models (LM) transition from generating or evaluating the fluency of natural text <ref type="bibr" target="#b25">(Mikolov and Zweig, 2012;</ref><ref type="bibr" target="#b24">Merity et al., 2018;</ref><ref type="bibr" target="#b23">Melis et al., 2018;</ref><ref type="bibr" target="#b10">Gamon et al., 2005)</ref> to being a powerful tool for text understanding. This understanding has mainly been achieved through the use of language modeling as a pre-training task for feature extractors,  where the hidden vectors learned through a language modeling objective are then used in downstream language understanding systems <ref type="bibr" target="#b6">(Dai and Le, 2015;</ref><ref type="bibr" target="#b22">Melamud et al., 2016;</ref><ref type="bibr" target="#b27">Peters et al., 2018;</ref><ref type="bibr" target="#b7">Devlin et al., 2019)</ref>.</p><p>Interestingly, it is also becoming apparent that LMs<ref type="foot" target="#foot_0">1</ref> themselves can be used as a tool for text understanding by formulating queries in natural language and either generating textual answers directly <ref type="bibr" target="#b21">(McCann et al., 2018;</ref><ref type="bibr" target="#b32">Radford et al., 2019)</ref>, or assessing multiple choices and picking the most likely one <ref type="bibr" target="#b44">(Zweig and Burges, 2011;</ref><ref type="bibr" target="#b33">Rajani et al., 2019)</ref>. For example, LMs have been used to answer factoid questions <ref type="bibr" target="#b32">(Radford et al., 2019)</ref>, answer common sense queries <ref type="bibr" target="#b41">(Trinh and Le, 2018;</ref><ref type="bibr" target="#b35">Sap et al., 2019)</ref>, or extract factual knowledge about relations between entities <ref type="bibr" target="#b29">(Petroni et al., 2019;</ref><ref type="bibr" target="#b1">Baldini Soares et al., 2019)</ref>. Regardless of the end task, the knowledge contained in LMs is probed by providing a prompt, and letting the LM either generate the continuation of a prefix (e.g. "Barack Obama was born in "), or predict missing words in a cloze-style template (e.g., "Barack Obama is a by profession").</p><p>However, while this paradigm has been used to achieve a number of intriguing results regarding the knowledge expressed by LMs, they all rely on prompts that were manually created based on the intuition of the experimenter. Thus it is quite possible that a fact that the LM does know cannot be retrieved due to the prompts not being effective queries for the fact. Thus, existing results are simply a lower bound on the extent of knowledge contained in LMs, and in fact, LMs may be even more knowledgeable than these initial results indicate. In this paper we ask the question: "How can we tighten this lower bound and get a more accurate estimate of the knowledge contained in state-of-the-art LMs?" This is interesting both scientifically, as a probe of the knowledge that LMs contain, and from an engineering perspective, as it will result in higher recall when using LMs as part of a knowledge extraction system.</p><p>In particular, we focus on the setting of <ref type="bibr" target="#b29">Petroni et al. (2019)</ref> who examine extracting knowledge regarding the relations between entities (definitions in ยง 2). We propose two automatic methods to systematically improve the breadth and quality of the prompts used to query the existence of a relation ( ยง 3). Specifically, as shown in Figure <ref type="figure" target="#fig_0">1</ref>, these are mining-based methods inspired by previous relation extraction methods <ref type="bibr" target="#b34">(Ravichandran and Hovy, 2002)</ref>, and paraphrasing-based methods that take a seed prompt (either manually created or automatically mined), and paraphrase it into several other semantically similar expressions. Further, we note that each prompt generated above can be viewed as an "expert" to retrieve knowledge from LMs, and different experts may work better when querying for particular subject-object pairs. We thus investigate lightweight ensemble methods to combine the answers from different prompts together ( ยง 4).</p><p>We experiment on the LAMA benchmark <ref type="bibr" target="#b29">(Petroni et al., 2019)</ref>, which is an English-language benchmark devised to test the ability of LMs to retrieve relations between entities ( ยง 5). We first demonstrate that improved prompts significantly improve accuracy on this task, with the one-best prompt extracted by our method raising accuracy from 31.1% to 34.1% on BERT-base <ref type="bibr" target="#b7">(Devlin et al., 2019)</ref>, with similar gains being obtained with BERT-large as well. We further demonstrate that using a diversity of prompts through ensembling further improves accuracy to 38.1%. We perform extensive analysis and glean insights about how to best query the knowledge stored in LMs, which both serves as useful ablation studies for our proposed methods, and elucidates potential future directions for incorporating knowledge into LMs themselves. Finally, we have released the resulting LM Prompt And Query Archive (LPAQA) to facilitate future experiments on probing knowledge contained in LMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Knowledge Retrieval from LMs</head><p>Retrieving factual knowledge from LMs is quite different from querying standard declarative knowledge bases (KB). In standard KBs, users formulate their information needs as a structured query defined by the KB schema and query language. For example, SELECT ?y WHERE {wd:Q76 wdt:P19 ?y} is a SPARQL query to search the birth place of Barack Obama. In contrast, LMs must be queried by natural language prompts, such as "Barack Obama was born in ", and the word assigned the highest probability in the blank will be returned as the answer. Unlike deterministic queries on KBs, this provides no guarantees of correctness or success.</p><p>While the idea of prompts is common to methods for extracting many varieties of knowledge from LMs, in this paper we specifically follow the formulation of <ref type="bibr" target="#b29">Petroni et al. (2019)</ref>, where factual knowledge is in the form of triples x, r, y . Here x indicates the subject, y indicates the object, and r is their corresponding relation. To query the LM, r is associated with a cloze-style prompt t r consisting of a sequence of tokens, two of which are placeholders for subjects and objects (e.g., "x plays at y position"). The existence of the fact in the LM is assessed by replacing x with the surface form of the subject, and letting the model predict the missing object (e.g., "Jordan plays at position"):</p><formula xml:id="formula_0">2 ลท = arg max y โV P LM (y |x, t r ),</formula><p>where V is the vocabulary, and P LM (y |x, t r ) is the LM probability of predicting y in the blank conditioned on the other tokens (i.e., the subject and the prompt). 3 We say that an LM has knowledge of a fact if ลท is the same as the ground-truth y. Because we would like our prompts to most effectively elicit any knowledge that may be contained in the LM itself, a "good" prompt should trigger the LM to predict the ground-truth objects as often as possible.</p><p>In previous work <ref type="bibr" target="#b21">(McCann et al., 2018;</ref><ref type="bibr" target="#b32">Radford et al., 2019;</ref><ref type="bibr" target="#b29">Petroni et al., 2019)</ref>, t r has been a single manually defined prompt based on the intuition of the experimenter. As noted in the introduction, this method has no guarantee of being optimal, and thus in the following sections we propose methods that learn effective prompts from a small set of training data consisting of gold subject-object pairs for each relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Prompt Generation</head><p>First, we tackle prompt generation: the task of generating a set of prompts {t r,i } T i=1 for each relation r, where at least some of the prompts effectively trigger LMs to predict ground-truth objects. We employ two practical methods to either mine prompt candidates from a large corpus ( ยง 3.1) or diversify a seed prompt through paraphrasing ( ยง 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Mining-based Generation</head><p>Our first method is inspired by template-based relation extraction methods <ref type="bibr" target="#b34">(Ravichandran and Hovy, 2002)</ref>, which are based on the observation that words in the middle of the subject x and object y in a large corpus often describe the relation r. Based on this intuition, we first identify all the Wikipedia sentences that contain both subjects and objects of a specific relation r, then use those words in the middle as prompts. For example, "Barack Obama was born in Hawaii" is converted into a prompt "x was born in y" by replacing the subject and the object with placeholders. To remove noise, we rank all the unique prompts based on their frequencies and use only the top T most frequent ones. 4  Notably, this variety of mining-based method does not rely on any manually-created prompts, 3 We restrict to masked LMs in this paper because the missing slot might not be the last token in the sentence and computing this probability in traditional left-to-right LMs using Bayes' theorem is not tractable. 4 Words along the dependency path between the entities might be even more indicative of the relation, as noted by <ref type="bibr" target="#b40">Toutanova et al. (2015)</ref>. It is quite possible that using these techniques may further improve results, but we did not test these at this time due to the increased complexity and computational load resulting from parsing the whole corpus. and can thus be flexibly applied to any relation where we can obtain a set of subject-object pairs. It will also result in diverse prompts, covering a wide variety of ways that the relation may be expressed in actual text. However, it may also be prone to noise, as many prompts acquired in this way may not be very indicative of the relation (e.g. "x, y"), even if they are frequent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Paraphrasing-based Generation</head><p>Our second method for generating prompts is more targeted -it aims to improve lexical diversity while remaining relatively faithful to the original prompt. Specifically, we do so by performing paraphrasing over the original prompt into other semantically similar or identical expressions. For example, if our original prompt is "x shares border with y", it may be paraphrased into "x has a common border with y" and "x adjoins y". This is conceptually similar to query expansion techniques used in information retrieval that reformulate a given query to improve retrieval performance <ref type="bibr" target="#b5">(Carpineto and Romano, 2012)</ref>.</p><p>While many methods could be used for paraphrasing, we follow the simple method of using back-translation <ref type="bibr" target="#b31">(Prabhumoye et al., 2018)</ref> to first translate the initial prompt into B candidates in another language, each of which is then back-translated into B candidates in the original language. We then rank B 2 candidates based on their round-trip probability (i.e., P forward ( t| t) โข P backward (t| t), where t is the initial prompt, t is the translated prompt in the other language, and t is the final prompt), and keep the top T prompts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Prompt Selection and Ensembling</head><p>In the previous section, we described methods to generate a set of candidate prompts {t r,i } T i=1 for a particular relation r. Each of these prompts may be more or less effective at eliciting knowledge from the LM, and thus it is necessary to decide how to use these generated prompts at test time. In this section, we describe three methods to do so.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Top-1 Prompt Selection</head><p>For each prompt, we can measure its accuracy of predicting the ground-truth objects (on a training dataset) using:</p><formula xml:id="formula_1">A(t r,i ) = x,y โr ฮด(y=arg max y P LM (y |x,t r,i )) |r| ,</formula><p>where ฮด(โข) is Kronecker's delta function, returning 1 if the internal condition is true, and 0 otherwise In the simplest method for querying the LM, we choose the prompt with the highest accuracy and query using only this prompt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Rank-based Ensemble</head><p>Next we examine methods that use not only the top-1 prompt, but combine together multiple prompts. The advantage to this is that the LM may have observed different entity pairs in different contexts within its training data, and having a variety of prompts may allow for elicitation of knowledge that appeared in these different contexts.</p><p>Our first method for ensembling is a parameterfree method that averages the predictions of the top-ranked prompts. We rank all the prompts based on their accuracy of predicting the objects, and use the average log probabilities from the top K prompts to calculate the probability of the object:</p><formula xml:id="formula_2">s(y|x, r) = K i=1 1 K log P (y|x, t r,i ),<label>(1)</label></formula><formula xml:id="formula_3">P (y|x, r) = softmax(s(โข|x, r)) y ,<label>(2)</label></formula><p>where t r,i is the prompt ranked at the i-th position.</p><p>Intuitively, due to the fact that we are combining together scores in the log space, this has the effect of penalizing objects that are very unlikely given any certain prompt in the collection. We also compare with linear combination in ablations in ยง 6.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Optimized Ensemble</head><p>The above method treats the top K prompts equally, which might be sub-optimal if some prompts are more reliable than others. Thus, we also propose a method that directly optimizes prompt weights. Formally, we re-define the score in Equation 1 as:</p><formula xml:id="formula_4">s(y|x, r) = T i=1 P ฮธr (t r,i |r) log P (y|x, t r,i ),<label>(3)</label></formula><p>where P ฮธr (t r,i |r) = softmax(ฮธ r ) is a distribution over prompts parameterized by ฮธ r , a T -sized realvalue vector. ฮธ r is optimized to maximize the probability of the gold-standard object P (y|x, r) over training data.</p><p>5 Main Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Settings</head><p>In this section, we assess the extent to which our prompts can improve fact prediction performance,  <ref type="table">1</ref>.</p><formula xml:id="formula_5">Properties T-REx T-REx-train #subject-</formula><p>Models As the models to probe, we use BERTbase and BERT-large <ref type="bibr" target="#b7">(Devlin et al., 2019)</ref>.</p><p>Evaluation Metrics We use two metrics to evaluate the success of prompts in probing LMs. The first evaluation metric, micro-averaged accuracy, follows the LAMA benchmark<ref type="foot" target="#foot_2">5</ref> in calculating the accuracy of all subject-object pairs for each relation, then averages these relation-level accuracies. However, we found that the object distributions of some relations are extremely skewed, e.g. more than half of the objects in relation native language are French. This can lead to deceptively high scores, even for a majorityclass baseline that picks the most common object for each relation, which achieves a score of 22.0%.</p><p>To mitigate this problem, we also report macroaveraged accuracy, which computes accuracy for each unique object separately, then averages them together to get the relation-level accuracy. This is a much stricter metric, with the majority-class baseline only achieving a score of 2.2%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>We attempted different methods for prompt generation and selection/ensembling, and compare them with the manually designed prompts used in <ref type="bibr" target="#b29">Petroni et al. (2019)</ref>. Majority refers to predicting the majority object for each relation, as mentioned above. Man is the baseline from <ref type="bibr" target="#b29">Petroni et al. (2019)</ref> that only uses the manually designed prompts for retrieval. Mine ( ยง 3.1) uses the prompts mined from Wikipedia, and Mine+Man combines them with the manual prompts. Mine+Para ( ยง 3.2) paraphrases the highest-ranked mined prompt for each relation, while Man+Para uses the manual one instead.</p><p>Those prompts are combined either by averaging the log probabilities from the TopK highest-ranked prompts ( ยง 4.2) or the weights after optimization ( ยง 4.3; Opti.). Oracle represents the upper bound of the performance of the generated prompts, where a fact is judged as correct if any one of the prompts allows the LM to successfully predict the object.</p><p>Implementation Details We keep T = 30 prompts either generated through mining or paraphrasing in all experiments, and the number of candidates in back-translation is set to B = 7. We use the round-trip English-German neural machine translation models pre-trained on WMT'19 <ref type="bibr" target="#b26">(Ng et al., 2019)</ref> for back-translation.<ref type="foot" target="#foot_3">6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Results</head><p>Micro-and macro-averaged accuracy of different methods are reported in Tables <ref type="table" target="#tab_3">2 and 3</ref> respectively.</p><p>Single Prompt Experiments When only one prompt is used (in the first Top1 column in both tables), the best paraphrase of the manual prompt improves the micro-averaged accuracy from 31.1% to 34.1% on BERT-base, and from 32.3% to 35.9% on BERT-large. This demonstrates that the manually created prompts are a somewhat weak lower bound; there are other prompts that further improve the ability to query knowledge from LMs. However, the manual prompts are indeed a strong baseline, often superior to the best mined prompts or their best paraphrases.</p><p>Table <ref type="table" target="#tab_4">4</ref> shows some of the mined prompts and that resulted in a large performance gain compared to the manual ones. For the relation religion, "x who converted to y" improved 60.0% over the manually defined prompt of "x is affiliated with the y religion", and for the relation subclass of, "x is a type of y" raised the accuracy by 22.7% over "x is a subclass of y". It can be seen that the largest gains from using mined prompts seem to occur in cases where the manually defined prompt is more complicated syntactically (e.g. the former), or when it uses less common wording (e.g. the latter) than the mined prompt.</p><p>Prompt Ensembling Next we turn to experiments that use multiple prompts to query the LM. Comparing the single-prompt results in Column 1 to the ensembled results in the following three columns, we can see that ensembling multiple prompts almost always leads to better performance. The simple average used in Top3 and Top5 outperforms Top1 across different prompt generation methods. The optimized ensemble further raises micro-averaged accuracy to 36.9% and 40.4% on BERT-base and BERT-large respectively, outperforming the rank-based ensemble by a large margin. These two sets of results demonstrate that diverse prompts can indeed query the LM in different ways, and that the optimization-based method is able to find weights that effectively combine different prompts together.</p><p>We list the learned weights of top-3 mined prompts and micro-averaged accuracy gain over only using the top-1 prompt in Table <ref type="table" target="#tab_5">5</ref>. Weights tend to concentrate on one particular prompt, and the other prompts serve as complements. We also depict the performance of the rank-based ensem-  ble method with respect to the number of prompts in Figure <ref type="figure" target="#fig_1">2</ref>. For mined prompts, top-2 or top-3 usually gives us the best results, while for paraphrased prompts, top-5 is the best. Incorporating more prompts does not always improve accuracy, a finding consistent with the rapidly decreasing weights learned by the optimization-based method.</p><p>Mining vs. Paraphrasing For the rank-based ensembles (Top1, 3, 5), prompts generated by paraphrasing usually perform better than mined prompts, while for the optimization-based ensemble (Opti.), mined prompts perform better. We conjecture this is because mined prompts exhibit more variation compared to paraphrases, and proper weighting is of central importance. This difference in the variation can be observed in the average edit distance between the different prompts, which is 3.27 and 2.73 for mined and paraphrased prompts respectively. However, the improvement led by ensembling paraphrases is still significant over just using one prompt (Top1 vs. Opti.), raising microaccuracy from 30.7% to 33.6% on BERT-base, and from 32.1% to 37.0% on BERT-large. This indicates that even small modifications to prompts can result in relatively large changes in predictions. Table 6 demonstrates cases where modification of one word (either function or content word) leads to significant accuracy improvements, indicating that large-scale LMs are still brittle to small changes in the ways they are queried. <ref type="table" target="#tab_2">2</ref> and Table 3, we can see that macro-averaged accuracy is much lower than micro-averaged accuracy, indicating that macro-averaged accuracy is a more challenging metric that evaluates how many unique objects LMs know. Our optimization-based method improves macro-averaged accuracy from 22.8% to 24.0% on BERT-base, and from 25.7% to 27.7% on BERT-base. This again confirms the effectiveness of ensembling multiple prompts, but the gains are somewhat smaller. Notably, in our optimizationbased methods, the ensemble weights are optimized on each example in the training set, which is more conducive to optimizing micro-averaged accuracy. Optimization to improve macro-averaged accuracy is potentially an interesting direction for future work that may result in prompts more generally applicable to different types of objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Micro vs. Macro Comparing Table</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Analysis</head><p>Next, we perform further analysis to better understand what type of prompts proved most suitable for facilitating retrieval of knowledge from LMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prediction Consistency by Prompt</head><p>We first analyze the conditions under which prompts will yield different predictions. We define the divergence between predictions of two prompts t r,i and t r,j using</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ID</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relations</head><p>Manual Prompts Mined Prompts Acc. Gain</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P140 religion</head><p>x is affiliated with the y religion x who converted to y +60.0 P159 headquarters location The headquarter of x is in y</p><p>x is based in y +4.9 P20 place of death x died in y x died at his home in y +4.6 P264 record label</p><p>x is represented by music label y x recorded for y +17.2 P279 subclass of</p><p>x is a subclass of y x is a type of y +22.7 P39 position held</p><p>x has the position of y x is elected y +7.9 the following equation: Div(t r,i , t r,j ) = x,y โr ฮด(R(x,y,t r,i ) =R(x,y,t r,j ))</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>|r|</head><p>, where R(x, y, t r,i ) = 1 if prompt t r,i can successfully predict y and 0 otherwise, and ฮด(โข) is Kronecker's delta. For each relation, we normalize the edit distance of two prompts into [0, 1] and bucket the normalized distance into 5 bins with intervals of 0.2. We plot a box chart for each bin to visualize the distribution of prediction divergence in Figure <ref type="figure" target="#fig_2">3</ref>, with the green triangles representing mean values and the green bars in the box representing median values. As the edit distance becomes larger, the divergence increases, which confirms our intuition that very different prompts tend to cause different prediction results. The Pearson correlation coefficient is 0.25, which shows that there is a weak correlation between these two quantities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>POS-based Analysis</head><p>Next, we try to examine which types of prompts tend to be effective in the abstract by examining the part-of-speech (POS) patterns of prompts that successfully extract knowledge from LMs. In open information extraction systems <ref type="bibr" target="#b2">(Banko et al., 2007)</ref>, manually defined patterns are often leveraged to filter out noisy relational phrases. For example, ReVerb <ref type="bibr" target="#b9">(Fader et al., 2011)</ref> incorporates three syntactic constraints listed in Table <ref type="table" target="#tab_7">7</ref> to improve the coherence and informativeness of the mined relational phrases. To test whether these patterns are also indicative of the ability of a prompt to retrieve knowledge from LMs, we use these three patterns to group prompts generated by our methods into four clusters, where one cluster is "other" containing prompts that do not match any pattern. We then calculate the rank of each prompt within the top 30 extracted prompts, and plot the distribution of rank using box plots in Figure <ref type="figure" target="#fig_3">4</ref>. <ref type="foot" target="#foot_4">7</ref> From this, we can see that the average rank of prompts matching these patterns is better than those in the "other" group, confirming our intuitions that good prompts should conform with those patterns. Some of the best performing prompts' POS signatures are "x VBD VBN IN y" (e.g., "x was born in y") and "x VBZ DT NN IN y" (e.g., "x is the capital of y").</p><formula xml:id="formula_6">x/y V y/x | x/y V P y/x | x/y V W* P y/x V = verb particle? adv? W = (noun | adj | adv | pron | det) P = (prep | particle | inf. marker)</formula><p>Cross-model Consistency Finally, it is of interest to know whether the prompts that we are extracting are highly tailored to a specific model, or whether they can generalize across models. To do so, we use both the BERT-base and BERT-large models, and compare when the optimization-based ensembles are trained on the same model, or when they are trained on one model and tested on the other model. As shown in Table <ref type="table" target="#tab_8">8</ref>, we found that in general that there is usually some drop in performance in the cross-model scenario (third and fifth columns), but the losses tend to be small, and the highest performance when querying BERT-base is actually achieved by the weights optimized on BERT-large. Notably, the best accuracies of 38.5% and 40.4% with the weights optimized on the other model are still much higher than those obtained by the manual prompts (31.1% and 32.3% respectively), indicating that optimized prompts still afford large gains in performance over the previous method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Omitted Design Elements</head><p>Finally, in addition to the elements of our main proposed methodology in ยง 3 and ยง 4, we experimented with a few additional methods that did not prove highly effective, and thus were omitted from our final design. We briefly describe these below, along with cursory experimental results. Prompts Top1 Top3 Top5 Opti. Oracle before 31.9 34.5 33.8 38.1 47.9 after 30.2 32.5 34.7 37.5 50.8</p><p>Table <ref type="table">9</ref>: Micro-accuracy (%) before and after LMaware prompt fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">LM-aware Prompt Generation</head><p>We examined methods to generate prompts by solving an optimization problem that maximizes the probability of producing the ground-truth objects with respect to the prompts:</p><formula xml:id="formula_7">t * r = arg max tr P LM (y|x, t r ),</formula><p>where P LM (y|x, t r ) is parameterized with a pretrained LM. In other words, this method directly searches for a prompt that causes the LM to assign ground-truth objects the highest probability.</p><p>Solving this problem of finding text sequences that optimize some continuous objective has been studied both in the context of end-to-end sequence generation <ref type="bibr" target="#b15">(Hoang et al., 2017)</ref>, and in the context of making small changes to an existing input in the context of adversarial attacks <ref type="bibr" target="#b8">(Ebrahimi et al., 2018;</ref><ref type="bibr" target="#b42">Wallace et al., 2019)</ref>. However, we found that directly optimizing prompts guided by gradients was unstable and usually yielded unreadable snippets in our preliminary experiments. Thus, we instead resorted to a more straightforward hillclimbing method that starts with an initial prompt, then masks out one token at a time and replaces it with the most probable token conditioned on the other tokens, inspired by the mask-predict decoding algorithm used in non-autoregressive machine translation <ref type="bibr" target="#b11">(Ghazvininejad et al., 2019</ref>):<ref type="foot" target="#foot_5">8</ref> </p><formula xml:id="formula_8">P LM (w i |t r \ i) = x,y โr P LM (w i |x, t r \ i, y) |r| ,</formula><p>where w i is the i-th token in the prompt and t r \ i is the prompt with the i-th token masked out. We followed a simple rule that modifies a prompt from left to right, and this is repeated until convergence. We used this method to refine all the mined and manual prompts on the T-REx-train dataset, and display their performance on the T-REx dataset in Table <ref type="table">9</ref>. After fine-tuning, the oracle performance increased significantly, while the ensemble performances (both rank-based and optimizationbased) dropped slightly. This indicates that LMaware fine-tuning has the potential to discover better prompts, but portion of the refined prompts may have over-fit to the training set upon which they were optimized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Linear vs. Log-linear Combination</head><p>As mentioned in ยง 4.2, we use log-linear combination of probabilities in the ensemble methods in our main experiments. However, it is also possible to calculate probabilities through regular linear interpolation:</p><formula xml:id="formula_9">P (y|x, r) = K i=1 1 K P LM (y|x, t r,i )<label>(4)</label></formula><p>We compare these two ways to combine predictions from multiple mined prompts in Figure <ref type="figure" target="#fig_4">5</ref> ( ยง 4.2). We assume that log-linear combination outperforms linear combination because log probabilities make it possible to penalize objects that are very unlikely given any certain prompt. Table 10: Performance (%) of using forward and backward features with BERT-base.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Forward and Backward Probabilities</head><p>Finally, given class imbalance and the propensity of the model to over-predict the majority object, we examine a method to encourage the model to predict subject-object pairs that are more strongly aligned. Inspired by the maximum mutual information objective used in <ref type="bibr" target="#b18">Li et al. (2016a)</ref>, we add the backward log probability log P LM (x|y, t r,i ) of each prompt to our optimization-based scoring function in Equation <ref type="formula" target="#formula_4">3</ref>. Due to the large search space for objects, we turn to an approximation approach that only computes backward probability for the most probable B objects given by the forward probability at both training and test time. As shown in Table <ref type="table">10</ref>, the improvement resulting from backward probability is small, indicating that a diversitypromoting scoring function might not be necessary for knowledge retrieval from LMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Much work has focused on understanding the internal representations in neural NLP models (Belinkov and Glass, 2019), either by using extrinsic probing tasks to examine whether certain linguistic properties can be predicted from those representations <ref type="bibr" target="#b36">(Shi et al., 2016;</ref><ref type="bibr" target="#b20">Linzen et al., 2016;</ref><ref type="bibr" target="#b3">Belinkov et al., 2017)</ref>, or by ablations to the models to investigate how behavior varies <ref type="bibr" target="#b19">(Li et al., 2016b;</ref><ref type="bibr" target="#b37">Smith et al., 2017)</ref>. For contextualized representations in particular, a broad suite of NLP tasks are used to analyze both syntactic and semantic properties, providing evidence that contextualized representations encode linguistic knowledge in different layers <ref type="bibr" target="#b14">(Hewitt and Manning, 2019;</ref><ref type="bibr">Tenney et al., 2019a,b;</ref><ref type="bibr" target="#b17">Jawahar et al., 2019;</ref><ref type="bibr" target="#b12">Goldberg, 2019)</ref>.</p><p>Different from analyses probing the representations themselves, our work follows <ref type="bibr" target="#b29">Petroni et al. (2019)</ref> in probing for factual knowledge with manually defined prompts. Somewhat unsurprisingly, some have found that the knowledge contained by pre-trained LMs is still limited, with performance becoming lower when tested on hard-to-guess facts <ref type="bibr" target="#b30">(Poerner et al., 2019)</ref>, leading to a conclusion that using LMs as reliable knowledge sources may still be far from reality. However, as our work points out, sub-optimal manually created prompts may be significantly under-estimating the true performance obtainable by LMs in these settings.</p><p>Orthogonally, some previous works integrate external knowledge bases so that the language generation process is explicitly conditioned on symbolic knowledge <ref type="bibr" target="#b0">(Ahn et al., 2016;</ref><ref type="bibr" target="#b43">Yang et al., 2017;</ref><ref type="bibr" target="#b16">IV et al., 2019;</ref><ref type="bibr" target="#b13">Hayashi et al., 2020)</ref>. Similar extensions have been applied to large-scale pre-trained LMs like BERT, where contextualized word representations are enhanced with external entity embeddings either at training time or solely at test time <ref type="bibr" target="#b28">(Peters et al., 2019;</ref><ref type="bibr" target="#b30">Poerner et al., 2019)</ref>. In contrast, we focus on better knowledge retrieval methods through prompts from pre-trained LMs as-is, without modifying them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this paper, we examined the importance of the prompts used in retrieving factual knowledge from language models. We propose mining-based and paraphrasing-based methods to systematically generate diverse prompts to query specific pieces of relational knowledge. Those prompts, when combined together, improve factual knowledge retrieval accuracy by 7%, outperforming manually designed prompts by a large margin. Our analysis indicates that LMs are indeed more knowledgeable than initially indicated by previous results, but they are also quite sensitive to how we query them. This indicates potential future directions such as (1) more robust LMs that can be queried in different ways but still return similar results, (2) methods to incorporate factual knowledge in LMs, and (3) further improvements in optimizing methods to query LMs for knowledge. Finally, we have released all our learned prompts to the community as the LM Prompt and Query Archive (LPAQA), available at: https://github.com/jzbjyb/LPAQA.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Top-5 predictions and their log probabilities using different prompts (manual, mined, and paraphrased) to query BERT. Correct answer is underlined.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Performance of different types of prompts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Correlation of edit distance between prompts and their prediction divergence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Ranking position distribution of prompts with different patterns. The lower is the better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Performance of two interpolation methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Micro-averaged accuracy of different methods (%). Majority gives us 22.0%. Italic indicates best single-prompt accuracy, and bold indicates the best non-oracle accuracy overall.</figDesc><table><row><cell>Prompts</cell><cell>Top1 Top3 Top5 Opti. Oracle</cell></row><row><cell></cell><cell>BERT-base (Man=31.1)</cell></row><row><cell>Mine</cell><cell>30.7 32.7 31.2 36.9 45.1</cell></row><row><cell cols="2">Mine+Man 31.9 34.5 33.8 38.1 47.9</cell></row><row><cell cols="2">Mine+Para 30.7 33.0 33.7 33.6 45.0</cell></row><row><cell cols="2">Man+Para 34.1 35.8 36.6 37.3 47.9</cell></row><row><cell></cell><cell>BERT-large (Man=32.3)</cell></row><row><cell>Mine</cell><cell>34.4 33.8 33.1 40.4 47.9</cell></row><row><cell cols="2">Mine+Man 36.0 38.6 37.1 41.9 50.8</cell></row><row><cell cols="2">Mine+Para 32.1 35.0 36.1 37.0 47.3</cell></row><row><cell cols="2">Man+Para 35.9 37.3 38.0 38.8 50.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Macro-averaged accuracy of different methods (%). Majority gives us 2.2%. Italic indicates best single-prompt accuracy, and bold indicates the best non-oracle accuracy overall.</figDesc><table><row><cell cols="3">Prompts</cell><cell cols="8">Top1 Top3 Top5 Opti. Oracle</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">BERT-base (Man=22.8)</cell><cell></cell></row><row><cell cols="2">Mine</cell><cell></cell><cell cols="8">21.2 22.1 21.4 24.0 32.2</cell></row><row><cell cols="11">Mine+Man 22.0 24.0 23.4 25.2 34.6</cell></row><row><cell cols="11">Mine+Para 20.2 22.1 22.6 22.6 32.2</cell></row><row><cell cols="11">Man+Para 22.8 23.8 24.6 25.0 34.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">BERT-large (Man=25.7)</cell><cell></cell></row><row><cell cols="2">Mine</cell><cell></cell><cell cols="8">24.8 25.0 24.1 27.7 36.4</cell></row><row><cell cols="11">Mine+Man 27.0 27.6 26.8 29.5 38.9</cell></row><row><cell cols="11">Mine+Para 23.4 24.8 25.7 25.8 36.2</cell></row><row><cell cols="11">Man+Para 25.9 27.8 28.3 28.0 39.3</cell></row><row><cell></cell><cell>40</cell><cell></cell><cell cols="2">Mine</cell><cell></cell><cell cols="2">Mine+Para</cell><cell></cell><cell></cell></row><row><cell>micro-accuracy (%)</cell><cell>30 32 34 36 38</cell><cell cols="3">Mine+Man</cell><cell></cell><cell cols="2">Man+Para</cell><cell></cell><cell></cell></row><row><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">top K prompts</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Micro-accuracy gain (%) of the mined prompts over the manual prompts. is owned by y .485 x was acquired by y .151 x division of y .151 +7.0 P140 religionx who converted to y .615 y tirthankara x .190 y dedicated to x .110 +12.2 P176 manufacturer y introduced the x .594 y announced the x .286 x attributed to the y .111 +7.0</figDesc><table><row><cell>ID Relations</cell><cell>Prompts and Weights</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Weights of top-3 mined prompts, and the micro-accuracy gain (%) over only using the top-1 prompt.</figDesc><table><row><cell>ID</cell><cell>Modifications</cell><cell>Acc. Gain</cell></row><row><cell cols="2">P413 x plays inโat y position</cell><cell>+23.2</cell></row><row><cell cols="2">P495 x was createdโmade in y</cell><cell>+10.8</cell></row><row><cell cols="2">P495 x wasโis created in y</cell><cell>+10.0</cell></row><row><cell cols="2">P361 x is a part of y</cell><cell>+2.7</cell></row><row><cell cols="2">P413 x plays in y position</cell><cell>+2.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Small modifications (update, insert, and delete) in paraphrase lead to large accuracy gain (%).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Three part-of-speech-based regular expressions used in ReVerb to identify relational phrases.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Micro-accuracy (%) of cross-model optimization. The first row is the model to test, and the second row is the model on which prompt weights are learned.</figDesc><table><row><cell>Test</cell><cell cols="2">BERT-base BERT-large</cell></row><row><cell>Train</cell><cell cols="2">base large large base</cell></row><row><cell>mine</cell><cell>36.9 36.6</cell><cell>40.4 39.0</cell></row><row><cell cols="2">mine+man 38.1 38.5</cell><cell>41.9 40.0</cell></row><row><cell cols="2">mine+para 33.6 33.7</cell><cell>37.0 35.3</cell></row><row><cell cols="2">man+para 37.3 35.6</cell><cell>38.8 37.5</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Technically bidirectional models like BERT and ELMo do not directly define a probability distribution over text, which is the underlying definition of an LM. Nonetheless, we call them LMs for simplicity.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">We can also go the other way around by filling in the objects and predicting the missing subjects. Since our focus is on improving prompts, we choose to be consistent with<ref type="bibr" target="#b29">Petroni et al. (2019)</ref> to make a fair comparison, and leave exploring other settings to future work.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2">In LAMA, it is called "P@1."</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3">https://github.com/pytorch/fairseq/ tree/master/examples/wmt19</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4">We use the ranking position of a prompt to represent its quality instead of its accuracy because accuracy distributions of different relations might span different ranges, making accuracy not directly comparable across relations.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5">In theory, this algorithm can be applied to both masked LMs like BERT and traditional left-to-right LMs, since the masked probability can be computed using Bayes' theorem for traditional LMs. However, in practice, due to the large size of vocabulary, it can only be approximated with beam search, or computed with more complicated continuous optimization algorithms<ref type="bibr" target="#b15">(Hoang et al., 2017)</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by a gift from Bosch Research and NSF Award No. 1815287. We would like to thank Paul Michel and Hiroaki Hayashi for their insightful comments and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Heeyoul</forename><surname>Sungjin Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanel</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Pรคrnamaa</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR, abs/1608.00318</idno>
		<title level="m">A neural knowledge language model</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Matching the blanks: Distributional similarity for relation learning</title>
		<author>
			<persName><forename type="first">Baldini</forename><surname>Livio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><surname>Kwiatkowski</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1279</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2895" to="2905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Open information extraction from the web</title>
		<author>
			<persName><forename type="first">Michele</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Broadhead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI 2007, Proceedings of the 20th International Joint Conference on Artificial Intelligence</title>
				<meeting><address><addrLine>Hyderabad, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-01-06">2007. January 6-12, 2007</date>
			<biblScope unit="page" from="2670" to="2676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">What do neural machine translation models learn about morphology?</title>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fahim</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Sajjad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1080</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="861" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Analysis methods in neural language processing: A survey</title>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">R</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="49" to="72" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A survey of automatic query expansion in information retrieval</title>
		<author>
			<persName><forename type="first">Claudio</forename><surname>Carpineto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Romano</surname></persName>
		</author>
		<idno type="DOI">10.1145/2071389.2071390</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">50</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semisupervised sequence learning</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015</title>
				<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07">2015. December 7-12, 2015</date>
			<biblScope unit="page" from="3079" to="3087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-02">2019. June 2-7, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">HotFlip: White-box adversarial examples for text classification</title>
		<author>
			<persName><forename type="first">Javid</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anyi</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Lowd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dejing</forename><surname>Dou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-2006</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="31" to="36" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">T-rex: A large scale alignment of natural language with knowledge base triples</title>
		<author>
			<persName><forename type="first">Hady</forename><surname>Elsahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavlos</forename><surname>Vougiouklis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arslen</forename><surname>Remaci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Gravier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><forename type="middle">S</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frรฉdรฉrique</forename><surname>Laforest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Simperl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">; Anthony</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation, LREC 2018</title>
				<meeting>the Eleventh International Conference on Language Resources and Evaluation, LREC 2018<address><addrLine>Miyazaki, Japan; Edinburgh, UK</addrLine></address></meeting>
		<imprint>
			<publisher>John McIntyre Conference Centre</publisher>
			<date type="published" when="2011-07">2018. May 7-12. 2018. 2011. July 2011</date>
			<biblScope unit="page" from="1535" to="1545" />
		</imprint>
	</monogr>
	<note>Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, EMNLP 2011. A meeting of SIG-DAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sentence-level MT evaluation without reference translations: Beyond language modeling</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Aue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martine</forename><surname>Smets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EAMT</title>
				<meeting>EAMT</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="103" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mask-predict: Parallel decoding of conditional masked language models</title>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1633</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6114" to="6123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Assessing bert&apos;s syntactic abilities</title>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno>CoRR, abs/1901.05287</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Chenyan Xiong, and Graham Neubig</title>
		<author>
			<persName><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zecong</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI)</title>
				<meeting><address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Latent relation language models</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A structural probe for finding syntax in word representations</title>
		<author>
			<persName><forename type="first">John</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-02">2019. June 2-7, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4129" to="4138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards decoding as continuous optimisation in neural machine translation</title>
		<author>
			<persName><forename type="first">Cong</forename><surname>Duy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1014</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="146" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Barack&apos;s wife hillary: Using knowledge graphs for fact-aware language modeling</title>
		<author>
			<persName><forename type="first">L</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<title level="s">Long Papers</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-07-28">2019. July 28-August 2, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5962" to="5971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">What does BERT learn about the structure of language?</title>
		<author>
			<persName><forename type="first">Ganesh</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoรฎt</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Djamรฉ</forename><surname>Seddah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<title level="s">Long Papers</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-07-28">2019. July 28-August 2, 2019</date>
			<biblScope unit="page" from="3651" to="3657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A diversity-promoting objective function for neural conversation models</title>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting><address><addrLine>San Diego California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-12">2016a. June 12-17, 2016</date>
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
	<note>NAACL HLT 2016</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Understanding neural networks through representation erasure</title>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno>CoRR, abs/1612.08220</idno>
		<imprint>
			<date type="published" when="2016">2016b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Assessing the ability of lstms to learn syntaxsensitive dependencies</title>
		<author>
			<persName><forename type="first">Tal</forename><surname>Linzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Dupoux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="521" to="535" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08730</idno>
		<title level="m">The natural language decathlon: Multitask learning as question answering</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">context2vec: Learning generic context embedding with bidirectional LSTM</title>
		<author>
			<persName><forename type="first">Oren</forename><surname>Melamud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning</title>
				<meeting>the 20th SIGNLL Conference on Computational Natural Language Learning<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08-11">2016. 2016. August 11-12, 2016</date>
			<biblScope unit="page" from="51" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On the state of the art of evaluation in neural language models</title>
		<author>
			<persName><forename type="first">Gรกbor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR 2018</title>
		<title level="s">Conference Track Proceedings</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30">2018. April 30 -May 3, 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Regularizing and optimizing LSTM language models</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR 2018, Vancouver</title>
		<title level="s">Conference Track Proceedings</title>
		<meeting><address><addrLine>BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30">2018. April 30 -May 3, 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Context dependent recurrent neural network language model</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Spoken Language Technology Workshop (SLT)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="234" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Facebook fair&apos;s WMT19 news translation task submission</title>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyra</forename><surname>Yee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Conference on Machine Translation, WMT 2019</title>
				<meeting>the Fourth Conference on Machine Translation, WMT 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-08-01">2019. August 1-2, 2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="314" to="319" />
		</imprint>
	</monogr>
	<note>Shared Task Papers</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018<address><addrLine>Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-01">2018. June 1-6, 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Knowledge enhanced contextual word representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vidur</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1005</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="43" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Language models as knowledge bases?</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktรคschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1250</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2463" to="2473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Bert is not a knowledge base (yet): Factual knowledge vs. name-based reasoning in unsupervised qa</title>
		<author>
			<persName><forename type="first">Nina</forename><surname>Poerner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulli</forename><surname>Waltinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schรผtze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03681</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Style transfer through back-translation</title>
		<author>
			<persName><forename type="first">Shrimai</forename><surname>Prabhumoye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1080</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="866" to="876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Explain yourself! leveraging language models for commonsense reasoning</title>
		<author>
			<persName><forename type="first">Nazneen</forename><surname>Fatema Rajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1487</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4932" to="4942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning surface text patterns for a question answering system</title>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
				<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="41" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Atomic: An atlas of machine commonsense for ifthen reasoning</title>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Allaway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Roof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3027" to="3035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Does string-based neural MT learn source syntax?</title>
		<author>
			<persName><forename type="first">Xing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inkit</forename><surname>Padhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1159</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1526" to="1534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">What do recurrent neural network grammars learn about syntax?</title>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
				<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Spain; Papers</addrLine></address></meeting>
		<imprint>
			<publisher>Long</publisher>
			<date type="published" when="2017-04-03">2017. 2017. April 3-7, 2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1249" to="1258" />
		</imprint>
	</monogr>
	<note type="report_type">Valencia</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">BERT rediscovers the classical NLP pipeline</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Tenney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<title level="s">Long Papers</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-07-28">2019a. July 28-August 2, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4593" to="4601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">What do you learn from context? probing for sentence structure in contextualized word representations</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Tenney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Berlin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Poliak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Thomas</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Najoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<idno>ICLR 2019</idno>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
				<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2019b. May 6-9, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Representing text for joint embedding of text and knowledge bases</title>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pallavi</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-17">2015. 2015. September 17-21, 2015</date>
			<biblScope unit="page" from="1499" to="1509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Trieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02847</idno>
		<title level="m">A simple method for commonsense reasoning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Universal adversarial triggers for attacking and analyzing NLP</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Kandpal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1221</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2153" to="2162" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Reference-aware language models</title>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09-09">2017. 2017. September 9-11, 2017</date>
			<biblScope unit="page" from="1850" to="1859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">The microsoft research sentence completion challenge</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><surname>Burges</surname></persName>
		</author>
		<idno>MSR-TR-2011-129</idno>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Microsoft Research</publisher>
			<pubPlace>Redmond, WA, USA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
