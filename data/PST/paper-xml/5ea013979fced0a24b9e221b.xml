<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">S</forename><surname>Doraisamy</surname></persName>
						</author>
						<author>
							<persName><forename type="first">A</forename><forename type="middle">Abdul</forename><surname>Halin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Mustaffa</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Engineering</orgName>
								<orgName type="department" key="dep2">Doroud Branch</orgName>
								<orgName type="institution">Isalamic Azad University</orgName>
								<address>
									<postCode>68816-99999</postCode>
									<settlement>Dorud</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Multimedia Department</orgName>
								<orgName type="department" key="dep2">Faculty of Computer Science and Information Technology</orgName>
								<orgName type="institution">Universiti Putra Malaysia</orgName>
								<address>
									<addrLine>Serdang 43400</addrLine>
									<country key="MY">Malaysia</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">822C6D20E08E0C2CE134FDEB0517B400</idno>
					<idno type="DOI">10.1109/TIP.2016.2552401</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ontology-Based Semantic Image Segmentation</head><p>Using Mixture Models and Multiple CRFs Mohsen Zand, Member, IEEE, Shyamala Doraisamy, Member, IEEE, Alfian Abdul Halin, Member, IEEE, and Mas Rina Mustaffa, Member, IEEE Abstract-Semantic image segmentation is a fundamental yet challenging problem, which can be viewed as an extension of the conventional object detection with close relation to image segmentation and classification. It aims to partition images into non-overlapping regions that are assigned predefined semantic labels. Most of the existing approaches utilize and integrate lowlevel local features and high-level contextual cues, which are fed into an inference framework such as, the conditional random field (CRF). However, the lack of meaning in the primitives (i.e., pixels or superpixels) and the cues provides low discriminatory capabilities, since they are rarely object-consistent. Moreover, blind combinations of heterogeneous features and contextual cues exploitation through limited neighborhood relations in the CRFs tend to degrade the labeling performance. This paper proposes an ontology-based semantic image segmentation (OBSIS) approach that jointly models image segmentation and object detection. In particular, a Dirichlet process mixture model transforms the low-level visual space into an intermediate semantic space, which drastically reduces the feature dimensionality. These features are then individually weighed and independently learned within the context, using multiple CRFs. The segmentation of images into object parts is hence reduced to a classification task, where object inference is passed to an ontology model. This model resembles the way by which humans understand the images through the combination of different cues, context models, and rule-based learning of the ontologies. Experimental evaluations using the MSRC-21 and PASCAL VOC'2010 data sets show promising results.</p><p>Index Terms-Semantic image segmentation, ontology, mixture models, CRF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>I MAGE segmentation is a basic low-level vision problem in image analysis. Its main goal is to divide an image into non-overlapping regions corresponding to structural units or objects of interest. In spite of the great efforts devoted in the last two decades, it still remains a challenging problem. Two fundamental tasks which are often built upon segmentation results are object detection and semantic image segmentation. Most approaches formulate object detection as to detect prominent objects in the image foreground, while semantic image segmentation assigns a predefined label to each pixel. These three problems (image segmentation, object detection, and semantic image segmentation) are essentially interdependent as pixel labels in semantic segmentation benefit from the segmentation and discriminative object detectors. Likewise, object detection accurately relies on the underlying segmentation of the images <ref type="bibr" target="#b0">[1]</ref>. Admittedly, semantic image segmentation combines the difficulties of object detection and image segmentation to result in a problem of great complexity. Regardless of their dependence, these issues have typically been tackled using substantially different approaches in the literature. This work proposes an approach for semantic image segmentation, by jointly considering and enhancing image segmentation and object detection. Specifically, efficient segmentation of images into perceptually uniform object parts and the discriminative representation of object classes for labeling serve as the fundamental principles for the final semantic segmentation.</p><p>While earlier strategies mainly relied on pixel-wise labeling (e.g. <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>), more recent studies have shifted towards the inference of labels over image regions in order to reduce computational complexity by incorporating higher level semantic cues. This is based on the premise that region-based features are more informative than pixel-wise features. Such higher level features are also more uniform in terms of color, texture and/or brightness at several degrees of homogeneity. Several unsupervised segmentation algorithms have hence been used for higher level region-based feature representation such as JSEG <ref type="bibr" target="#b3">[4]</ref>, mean shift <ref type="bibr" target="#b4">[5]</ref>, graph-based segmentation <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, quick shift <ref type="bibr" target="#b7">[8]</ref>, TurboPixel <ref type="bibr" target="#b8">[9]</ref> and SLIC superpixel <ref type="bibr" target="#b9">[10]</ref>. These techniques however tend to over-segment objects with heterogeneous parts and strong internal contours. The segmented regions moreover lack semantic interpretation, resulting low semantic consistency (as they are obtained solely based on low-level features).</p><p>Based upon the assumption that the initial segmentation yields correct results, inference is hence conducted to predict the dominant label of each segment. We argue that in order to assign accurate labels in semantic segmentation, discriminative representations are necessary. This means that the exploited features for representing the regions must be as discriminatory as possible for inter-class separability. Since a single feature such as color can be extremely poor for classification, the Bag-of-Words (BoW) model is used. Although BoW features are expressive, they still rely on low-level local features. The semantic ambiguity might still limit their effectiveness in the labeling of the image regions <ref type="bibr" target="#b10">[11]</ref>. To handle this problem, fusion of top-down knowledge with bottom-up information has been proposed <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b15">[16]</ref>. The common approach is to use a graphical inference model based on the Conditional Random Field (CRF) since various contextual information among image regions can easily be integrated into the CRF framework <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b20">[21]</ref>. However, training and predicting with high-order CRFs are computationally expensive, and thus, only very limited neighborhood relations can be employed in the graph structure <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>. More importantly, another pressing issue is when different features are combined without considering their individual (or collective) importance for every class. As the relevance of each feature (such as color, shape or texture) strongly correlates with the class that an image belongs to <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b24">[25]</ref>, proper feature weighing is imperative. For instance, to discriminate the class sky as being in day or night-time, color information is understandably crucial. Likewise, shape is essential to distinguish between green apples from green bananas, while both color and shape information are important for the classification of animals. It can be simply observed that improper feature combinations can result in undesirable classification results. One of the popular methods used for classification is the Naïve Bayes model, which is a likelihood distribution p(y| x) with x = (x 1 , x 2 , . . . , x m ), where x i (1 ≤ i ≤ m) are features and y is the class label to be predicted. This probability can be formulated with Bayes' rule, as p(y|x) = p(x|y) p(y) p(x)</p><p>, where p(x|y j ) = p(x 1 |y j ) * p(x 2 |y j ) * ... * p(x m |y j ) is the probability of class y j generating instance x = (x 1 , x 2 , . . . , x m ), with m different features, and p(x i |y j ) is the probability of class y j generating the observed value for the i -th feature. Suppose that x = (c, t, s) is the input feature vector where c, t and s denote the observed values for color, texture and shape, respectively. It is obvious that p(c|y j ) is unlikely to be useful and informative for the class 'car', since color is mostly ineffective to discriminate one car from another, let alone other classes (e.g. animal). If most of the training images for class 'car' were red, then the probability of correctly classifying a blue car might become very low.</p><p>To cope with these problems, this paper proposes to jointly leverage image segmentation, object detection, and semantic inference solutions, which are geared towards the ultimate goal of semantic image segmentation. Different from existing approaches where semantic integration is postponed to the near-end stages (mainly through bridging the gap between low-and high-level features), the proposed method incorporates semantics in a gradual and consistent manner from the very beginning. Essentially, the low-level visual space is transformed into an intermediate semantic space of reduced dimensionality, which consists of low-level semantic labels. This is achieved using a Dirichlet process mixture (infinite mixture model) by clustering the visual features similar to vector quantization, but without explicitly specifying the number of clusters. Hence, this model partitions the visual space of each feature into an unknown number of components where their samples are used in a CRF to contextually learn the corresponding labels. For several features, multiple CRFs are trained to associate the intermediate labels with superpixels. This causes image segmentation to be treated as a classification task. In contrast to the existing methods, the CRFs are used to infer the dimension-reduced intermediate labels instead of the final class label. Another essential point is that each CRF factorizes the intermediate labels of a specific feature in its context, where labels and their contextual cues correspond to unary and pairwise terms, respectively. Therefore, each visual feature in the new space is weighed individually, which estimates its importanc e based on the training samples. Furthermore, unlike existing approaches that generate meaningless regions, the image segmentation step results in segments consistent with semantic object parts that are separately encoded with low-level semantic labels. These object parts consist of coherent pixels, and are homogeneous according to certain criteria (color, intensity, texture, etc.). Intuitively, this avoids the migration of errors and inconsistencies from the segmentation stage to semantic inference. Furthermore, by constructing a semantic ontology from higher level features (in the intermediate space), semantic concepts, and their relationships, labeling inference performance can be improved. This ontology formally represents, models, and induces object structures and relations. In fact, the proposed ontology-based inference model enables reasoning about the consistency of object labels in order to arrive at a semantically relevant decision. The overall proposed ontology-based semantic image segmentation approach is called OBSIS. For ontology construction, the Web Ontology Language (OWL) is utilized to ensure highly expressive formalisms with decidable reasoning power of description logics (DLs).</p><p>The contributions of this paper can be summarized as follows. Firstly, a semi-semantic segmentation method is proposed that effectively detects object parts (units that are recognized in the inference stage). It is in fact oriented for semantic segmentation. Secondly, a technique is introduced to transform the visual space into a higher level space with discretized numerical values as intermediate labels (using Dirichlet process mixture models and multiple CRFs). Here, each visual feature in the transformed space is individually weighed, and assigned to superpixels. Their particular weights are also taken into account in the ontology for the final labeling. Thirdly, ontology-based inference is introduced for the final semantic labeling, which incorporates visual, conceptual, and contextual knowledge about image concepts at different abstraction levels.</p><p>Remarkably, the proposed approach and ontology inference can be applied to other applications such as image retrieval or pattern recognition, and other domains such as biomedical or geospatial image analysis. Moreover, an ontology built from the images of a specific database can be used to infer the object labels in another/other database(s) in the same domain of interest. However, as each ontology contains the knowledge of a certain domain, it cannot be efficiently employed to another domain. In other words, reusing a highly specialized domain ontology in other domains is somewhat impossible. The main advantage of the proposed ontology inference is that it can handle different types of relationships at differing neighborhood and abstraction levels, which makes it applicable to other applications.</p><p>The overall diagram of OBSIS is illustrated in Fig. <ref type="figure" target="#fig_0">1</ref> where semi-semantic segmentation and semantic labeling are incorporated to build semantic segmentation. In semi-semanticsegmentation, the visual features from segmented superpixels are exploited and individually learned in the new feature space using Dirichlet mixture models and CRFs. Therefore, object parts can be associated with the intermediate labels, which are then used in the semantic learning step. In this step, the obtained object parts along with their intermediate labels, image objects, and relationships at different levels are incorporated into the semantic ontology. Semantic segmentation is finally achieved by induced labels of the semantic ontology.</p><p>The rest of the paper is organized as follows. An overview of related work is presented in Section II. Section III describes the preliminaries about the Dirichlet process mixture, CRF, and ontology. The proposed method for image segmentation is detailed in Section IV. Section V presents the final image regions labeling using the constructed ontology. The experimental setup, evaluations, and analysis are given in Section VI. Section VII concludes this paper with remarks regarding future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Image segmentation is a challenging task in computer vision and has extensively been approached using the bottomup paradigm based on low-level image features <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>. Semantic image segmentation is even more challenging as it involves assigning semantic labels to the segmented regions.</p><p>Primary studies on semantic image segmentation has dealt with this issue through pixel-wise labeling <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. For instance, Shotton et al. <ref type="bibr" target="#b27">[28]</ref> classified each pixel in an image according to joint appearance, shape, and context models. Although pixel-wise labeling is simple and straightforward, image pixels often contain limited information, which does not allow effective discrimination between classes. In contrast, region-level cues can be potentially more informative as well as more robust to noise, illumination, clutter, and variation. This has led researchers to shift from the pixel-based paradigm to region-based <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b28">[29]</ref>.</p><p>Region-based labeling normally begins with an initial unsupervised segmentation step as pre-processing. Some prominent studies determine superpixels as basic units in the images. In <ref type="bibr" target="#b29">[30]</ref>, a superpixel-based classification model was proposed using a unified energy function over image appearances and scene geometry structures. Li et al. <ref type="bibr" target="#b5">[6]</ref> used multi-layered superpixels to capture diverse and multi-scale visual patterns of images based on bipartite graph partitioning. These methods demonstrate that region-level segmentation is able to outperform their pixel-level counterparts. Despite the improvements of region-level segmentation methods, one pressing issue still exists where the segmented regions have little or no inherent semantic meaning. This makes effective labeling a seemingly prohibitive task.</p><p>With the hope of obtaining more discriminative representations, the low-level bottom-up and global top-down features were adopted to be exploited and integrated into a segmentation model <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b30">[31]</ref>. Most of the existing methods use an inference framework for object recognition based on the Conditional Random Field (CRF). This is because various contextual cues among image regions can be easily incorporated into CRFs. This converts the inference problem to a classification task, where the final results are obtained by optimizing an objective function. Zhang and Ji <ref type="bibr" target="#b30">[31]</ref> combined the CRF model and the Bayesian network approach to form a unified probabilistic graphical structure that captures the complex relations between different image regions. Lucchi et al. <ref type="bibr" target="#b31">[32]</ref> analyzed the application of spatial and global constraints in CRFs after local and global features have already leveraged information from the whole image. Boix et al. <ref type="bibr" target="#b26">[27]</ref> proposed a CRF model called harmony potential, which allowed multiple class labels to be assigned to a single node. In <ref type="bibr" target="#b32">[33]</ref>, the histogram representations of the superpixels and co-occurrence relations among object classes were used as local features and contextual cues in the CRF, respectively. However, although different characteristics from several sources can comprise complementary information and be potentially beneficial for enhancing performance, these methods do not take into account the individual importance of each feature. Moreover, inference using CRF by integrating high-dimensional low-level features and contextual cues raises other issues. Specifically, training and predicting with large or high order graph structures are computationally expensive although only adjacent regions are utilized to build the CRF model. Situated as one of the latest trends in semantic contextual modeling, ontologies have evolved to become a defining standard for the description of important semantic relations between concepts. An ontology can specify the domain knowledge linked to different aspects and allow semantic interpretation and reasoning over the exploited descriptions. Wang et al. <ref type="bibr" target="#b33">[34]</ref> modeled feature diagrams, and interrelationships among different features using ontologies. Fan et al. <ref type="bibr" target="#b34">[35]</ref> used a concept ontology to extract the interconcept relationships for training hierarchical image classifiers. In <ref type="bibr" target="#b35">[36]</ref>, an ontology-based framework was proposed to capture the vagueness of the extracted image descriptions and reasoning under uncertainty. Nevertheless, almost all existing methods for building ontologies use specifications which are not always relevant for modeling image semantics. Moreover, these approaches are mostly based either on conceptual specifications, or visual specifications. This makes them unsuitable and inaccurate to model image semantics.</p><p>In this paper, object detection is taken into account in the early stages of image segmentation where gradual and consistent incorporation of semantic knowledge, individual weighing of visual features, and an ontology-based inferencing are performed for the final labeling. Chiefly, in OBSIS, image segmentation is treated as a classification task, where meaningless image regions are classified into appropriate groups represented by low-level semantic labels. These labels in turn represent an intermediate space of reduced dimensionality. They are the outcome of the low-level visual features clustering of the Dirichlet process mixture models learned by multiple CRFs. This is in contrast to existing methods since instead of the semantic labels, the CRFs are composed of intermediate semantic labels and their contexts, which form unary and pairwise cliques, respectively. This does not only facilitate training and predicting in graph structures of the CRF, but also captures the contextual relations during feature learning, which is rarely approached in the literature. More importantly, the final inference is delivered to an ontology-based model where more knowledge in terms of both visual and conceptual specifications can be leveraged for effective labeling. Furthermore, OBSIS can exploit the interactions between semantic concepts from both semantic level and visual level with an indispensable semantic context modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PRELIMINARIES AND NOTATIONS</head><p>In this section, the preliminaries and notations for the Dirichlet process mixture, CRF, and ontology are briefly presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dirichlet Process Mixture</head><p>The Dirichlet process mixture model (infinite mixture model) used in Bayesian nonparametrics modeling is a distribution over distributions <ref type="bibr" target="#b36">[37]</ref>. It can be seen as a limit of a parametric mixture model <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>. Therefore, the hierarchical Gaussian mixture model formulation is used by approaching the limit of the number of mixture components to infinity. Let Y = {Y 1 , . . . , Y D } be a D-dimensional random vector for a D-dimensional visual feature drawn from an infinite mixture of Dirichlet distributions. The Dirichlet process model can hence be defined as:</p><formula xml:id="formula_0">p( Y | π, α, β) = ∞ j =1 π j Dir ( Y | α j , β j ),<label>(1)</label></formula><p>where π denotes mixing proportions which are positive and sum up to one. For the j -th component, α j and β j denote the positive parameters of the Dirichlet distribution which is written as:</p><formula xml:id="formula_1">Dir ( Y | α, β) = D d=1 ( (α j d + β j d ) (α j d ) (β j d ) )Y α jd -1 d (1 - d k=1 Y k ) γ jd ,<label>(2)</label></formula><p>where (.) is the gamma function, 0</p><formula xml:id="formula_2">&lt; Y d &lt; 1 for d = 1, . . . , D, and D d=1 Y d &lt; 1. The term γ j d = β j d -α j d+1 - β j d+1 for d = 1, . . . , D -1, and γ j D = β j D -1.</formula><p>Inspired by <ref type="bibr" target="#b39">[40]</ref> and <ref type="bibr" target="#b40">[41]</ref>, the original data points are transformed into another D-dimensional space where the Dirichlet process mixture model is defined as:</p><formula xml:id="formula_3">p( X| π, α, β) = ∞ j =1 π j D d-1 Beta(X d |α j d , β j d ), (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>where Beta is a Beta distribution, and</p><formula xml:id="formula_5">X 1 = Y 1 while X d = Y d /(1 -d-1 k=1 Y k ) for d &gt; 1.</formula><p>In this work, the Dirichlet mixture models are used along with the CRFs to transform the low-level visual space into a higher level space, consisting of intermediate labels with discretized numerical values. We believe that these labels are a better representation for the images in the proposed ontologybased semantic image segmentation architecture. A Dirichlet process mixture is independently applied to each visual feature since the vectors of each visual feature are uniformly distributed rather than a combination of heterogeneous features. This minimizes the quantization error and maximizes the visual correlations between samples of each cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Conditional Random Fields</head><p>Introduced by Lafferty et al. <ref type="bibr" target="#b41">[42]</ref>, a CRF is simply a probabilistic framework with an arbitrary graphical structure for the labeling and classification of structured data. In CRFs, many statistically correlated attributes of the inputs are adopted, and their dependencies do not need to be explicitly declared due to the conditionality of the model. Moreover, in contrast to Hidden Markov Models (HMM), CRFs can model linearsequence structures as well as arbitrary structures. Unlike sequential classifiers that are myopic about the impact of the current decisions on later decisions, CRFs can be trained discriminatively and are able to trade off decisions at different positions to achieve a globally optimum decision <ref type="bibr" target="#b42">[43]</ref>. It is shown in <ref type="bibr" target="#b41">[42]</ref> that CRFs outperform relevant classification models as well as HMMs on synthetic data.</p><p>Let G(V, E) be a factor graph over X and Y with nodes V and edges E, where X are the observations (e.g., image features), and Y is a set of hidden random variables (e.g., labels). Then, (X, Y ) is a conditional random field if for any variable x of X, the probability p(y|x) factorizes according to G. If G can be partitioned into C = {C 1 , C 2 , . . . , C P }, where each C p (for p = 1, . . . , P) is a clique template (fully connected sub-graph) with a set of pre-specified feature functions { f pk (x c , y c )} K ( p) k=1 , and corresponding set of weights θ p ∈ R K ( p) , then the conditional distribution can be written as:</p><formula xml:id="formula_6">P(y|x) = 1 Z (x) C P ∈C c ∈C P c (x c , y c ; θ p ), (<label>4</label></formula><formula xml:id="formula_7">)</formula><p>where Z denotes a normalizing constant called the partition function, and c are potential functions described over variables x c and y c constituting the clique c as follows:</p><formula xml:id="formula_8">c (x c , y c ; θ p ) = exp ⎛ ⎝ K ( p) k=1 θ pk f pk (x c , y c ) ⎞ ⎠ . (<label>5</label></formula><formula xml:id="formula_9">)</formula><p>The terms θ and f are indexed by clique c to emphasize that each clique has its own set of weights. The parameters in θ are obtained via parameter estimation so that the resulting distribution p(y|x, θ) best fits a set of</p><formula xml:id="formula_10">N training examples D = {x (i) , y (i) } N i=1</formula><p>, where both the inputs and outputs are known. Given each training input x (i) , the model's distribution over outputs p(y|x (i) , θ) should look like the true output y (i)  from the training data for parameter estimation. A standard strategy to train CRFs is maximum likelihood, in which the Viterbi assignment y * = argmax y p(y|x, θ), the single most likely labeling of a new input x, is computed <ref type="bibr" target="#b42">[43]</ref>.</p><p>In OBSIS, CRFs are trained based on the samples of the visual features mixture components obtained from the Dirichlet process mixtures. This is in order to learn the intermediate semantic space in the contextual space of the labels. Specifically, three CRFs corresponding to color, texture, and shape are independently trained to associate image object parts with intermediate labels. These labels and their relationships along with concepts and their interactions are further utilized in an ontology for the final inference in the proposed OBSIS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ontology</head><p>An ontology is defined as a formal and explicit representation of a shared conceptualization <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>. It defines a set of representational primitives used to model a domain of interest and describes the semantics concepts and their interrelationships. Ontologies are based on a descriptive logic that axiomatizes the semantic classes and their correlations with relationships such as symmetry, transitivity, and equivalences as reliable and consensual knowledge <ref type="bibr" target="#b45">[46]</ref>.</p><p>Ontologies are specified in a language that allows rich structures by formalizing the information and knowledge about a domain of interest <ref type="bibr" target="#b46">[47]</ref>. Most approaches use DL-based languages for describing knowledge domains and the relationships. The Web Ontology Language (OWL) is one of most popular languages to represent ontologies. It allows a domain to be specified in terms of concepts (or classes), rules (or properties), individuals and axioms with a uniform syntax and unambiguous semantics <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b47">[48]</ref>. Accordingly, the terms involved in the ontology include:</p><p>• classes, which represent types of objects, kinds of things, or concepts in the domain. They are usually organized in taxonomies;</p><p>• rules, which specify binary relations between classes and individuals. They include conditions and consequences that define the logical inferences derived from an assertion. Rules are denoted as a set of tuples, which are subsets of the cross products of the objects in the domain of discourse;</p><p>• individuals, which refer to the actual instances in the domain; and</p><p>• axioms, which are used to model assertions. They include logical rules.</p><p>In this work, a sublanguage of OWL namely OWL 2 DL is used as the ontology modeling language since it is more expressive and consists of more axioms than other sublanguages.</p><p>IV. SEMI-SEMANTIC IMAGE SEGMENTATION Although several segmentation algorithms are available <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b9">[10]</ref>, the problem of image segmentation is still a great challenge in computer vision. Despite the advances made in recent years, the perceptual grouping of image regions is still unreliable <ref type="bibr" target="#b48">[49]</ref>. Image segmentation into perceptual regions is crucial since many applications rely on it such as object detection, object recognition, scene understanding, as well as semantic image segmentation <ref type="bibr" target="#b49">[50]</ref>.</p><p>With the aim of robust semantic segmentation, this paper proposes a semi-semantic segmentation algorithm which efficiently group superpixels to generate semantic object parts. The generated segments are uniform in a sense where they are meaningful enough to be labeled in the inference stage. This method operates on over-segmented regions, such as that produced by SLIC superpixel segmentation <ref type="bibr" target="#b9">[10]</ref>, and tries to group these homogenous regions into a set of semantic object parts based on their low-level features. Particularly, the segmentation problem is treated as a classification task since extracted features from the superpixels are mapped to a set of low-level semantic labels. These labels correspond to color, texture and shape features in a higher level feature space. For each feature, the exploited attribute values from the training samples are grouped to an unknown number of classes using the aforementioned Dirichlet process mixture that does not require the number of classes to be known beforehand.</p><p>The proposed semi-semantic segmentation method is in a sense similar to traditional methods that assume image features are related to the statistical structure of the perceived environment, and consequently, they utilize clustering algorithms such as K-means, fuzzy c-means (FCM) or Gaussian mixture models (GMM) to exploit the representative vectors as centroids of the clusters. The extracted features of an image are then mapped onto the corresponding representative bins, and finally, pixels in the same cluster constitute a region. These methods determine the number of clusters beforehand, or empirically by testing several values <ref type="bibr" target="#b32">[33]</ref>. However, the consistency within and across different clusters cannot be guaranteed. Moreover, the number of representatives may vary due to the different parameter settings. Our intuition is that the aforementioned problems can be overcome in the proposed method where a new feature space is formed by automatically analyzing the data and using the Dirichlet process mixture models and CRFs. Specifically, the intermediate labels that represent the visual features in the new feature space are learned through the training of the generated mixture components of the Dirichlet process mixture models using multiple CRFs. Accordingly, the global context of the visual features can be incorporated in the feature learning because in the CRF training, all the samples with the same posterior distribution for a specific feature are investigated in their contexts. In addition, the potential quantization error produced during the centroid computation is inherently avoided since the centroids of the mixture components are not explicitly computed.</p><p>We construct the Dirichlet process mixture model using a stick-breaking process <ref type="bibr" target="#b50">[51]</ref>, which represents it as a mixture model containing an infinite number of components with random mixing weights. Assuming a stick with unit length, it is recursively divided into an infinite number of segments π j = λ j j -1 k=1 (1-λ k ), where λ j is the stick breaking variable and is distributed as λ j ∼ Beta <ref type="bibr">(1, ξ)</ref>. For a given dataset containing N training examples, denoted as χ = {x 1 , . . . , x N }, the vectors of a specific feature are represented as ( X 1 , . . . , X N ). These vectors are assigned to a set of mixture component assignment variables W = (W 1 , . . . , W N ), where W i holds the component number of vector X i . The marginal distribution over Z is defined as:</p><formula xml:id="formula_11">p( W | λ) = N i=1 ∞ j =1 [λ j j -1 k=1 (1 -λ k )] I [W i = j ] , (<label>6</label></formula><formula xml:id="formula_12">)</formula><p>where  represent the observed knowledge about the superpixels, i.e., their exploited visual features. This is the approach taken by CRFs <ref type="bibr" target="#b42">[43]</ref>. Many different CRF topologies have been used for computer vision. The most common topology is a grid-structured unidirectional graphical model. Using this structure and assuming that only up to pairwise clique potentials are nonzero (each clique is a superpixel), the CRF would include two factors. These factors are made up of the unary term that associates a superpixel with its corresponding label, and the pairwise term that considers superpixel context as the global property. With these factors, and referring to Equations ( <ref type="formula" target="#formula_6">4</ref>) and ( <ref type="formula" target="#formula_8">5</ref>), we define the CRF for each visual feature as:</p><formula xml:id="formula_13">I [W i = j ] is an indicator function that is equal to 1 if W i = j ,</formula><formula xml:id="formula_14">p(y|x) = 1 Z (x) exp( N i=1 θ f (y i , x i ) + (i, j )∈N</formula><p>σ g(y i , y j , x i , x j )), <ref type="bibr" target="#b6">(7)</ref> where N denotes the number of pixels, N defines the neighborhood relationships between pixels, and θ ∈ R K , and σ ∈ R K 2 are the model parameters. The term f (y i ,</p><formula xml:id="formula_15">x i ) = f 0 (y i , x i ) f 1 (y i , x i ) , where f h (y i , x i ) = I [y i = h]q(x i )∀h ∈ {0, 1},</formula><p>and q(x i ) stands for a visual feature (including a vector based on pixels around x i ). The second term g(y i , y j , x i , x j ) is defined based on the feature vector v(x i , x j ) that relies on both x i and x j as: g(y i , y j ,</p><formula xml:id="formula_16">x i , x j ) = I [y i = y j ]v(x i , x j ).</formula><p>The feature vector v can be a set of all cross-products among q(x i ) and q(x j ). Therefore, the global context within the superpixels is taken into account. To assign the most probable label, p(y|x) is maximized over all possible labelings as:</p><formula xml:id="formula_17">ŷ = argmax y p(y|x). (<label>8</label></formula><formula xml:id="formula_18">)</formula><p>An example is shown in Fig. <ref type="figure" target="#fig_3">3</ref> where an intermediate label of the color feature is learned using a CRF. Consequently, our model comprises three CRFs, with three output labels. The main advantage of this model is that weighing of different features is performed independently instead of using one weight for all the visual features of all the labels, where the importance of the instances can vary within a cue. The shortcomings of CRFs caused by the large graphical structures can be avoided since they are used in a higher order feature space with lower dimensionality. Note that context plays an important role for objects recognition in human vision <ref type="bibr" target="#b52">[53]</ref>. Therefore, it is considered not only in the feature level where visual features are specified in their contexts, but also in the concept level where ontologies are further utilized to characterize the concept relationships.</p><p>The training samples used to learn the intermediate colorand texture-based labels are extracted from the irregularshaped superpixels while the semantic object parts are used for learning shape feature labels. This is because the shape descriptors mostly exploit the shape information from the edges and contours which are not accurate for the irregularshaped regions. If both color and texture labels of the neighboring regions are the same, they will be grouped into a larger region. The region labels are then identically assigned to the larger region while its exploited shape descriptor specifies the shape label. This region can be viewed as a semantic subpart of an object with some degrees of homogeneity, which for example, can be a car's body, tire or head-lamp. Otherwise, the shape feature is used to decide on whether regions are combined or separated. For this purpose, the shape descriptors are exploited for each region separately and also for their combination. If the likelihood of the most probable label ŷ computed from the combination of regions is greater than that of individual regions, the regions are grouped into a larger region, otherwise, they remain separated. At the end of the segmentation step, semantic object parts are labeled with their color, texture and shape labels, which will be then used for semantic labeling. Hence, by generating the object parts, image segmentation can be tailored for semantic segmentation in the inference stage. Fig. <ref type="figure" target="#fig_4">4</ref> illustrates some sample images segmented by the proposed method which are comparable to two state-of-the-art approaches, namely the JSEG <ref type="bibr" target="#b3">[4]</ref> and SLIC <ref type="bibr" target="#b9">[10]</ref>. The first row is the original image, the second and third rows are results of the JSEG and SLIC, respectively. The last row shows the segmented images using the proposed method. It can be seen that the segmentation results obtained using the proposed method are more close to human visual perception.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. SEMANTIC LABELING</head><p>The importance of multi-level descriptions for structuring heterogeneous information is explored for semantic labeling in this work. The intuition is to generate an ontology to facilitate machine image visualization in the similar manner humans visualize images. Ontologies enable the recognition of the object labels by connecting the image space, the feature space, and the ontology space. The low-level visual features are exploited from images in the image space. In this work, these visual features are transferred to a higher level intermediate semantic feature space. The ontology space includes the ontology and a reasoning engine for the object labeling. The intermediate labels in the feature space can boost the reasoning from the ontology's reasoning engine since they connect the high-dimensional and low-level features to usable terms in the ontology. For instance, the human recognition of a color as light blue is associated to a discretized numerical value in the feature space, which is seen as more efficient compared to existing approaches that use textual strings. It is noted that there are no appropriate names known to humans for all distinguishable values of all the features.</p><p>A three-layered ontology-based approach is proposed to allow for the most comprehensive semantic description of the image content. As shown by the semantic graph in Fig. <ref type="figure" target="#fig_5">5</ref>, the first layer consists of the ontology that captures the contextual relationships between image objects. Object decomposition into object subparts is modelled in the second layer. The third layer integrates the visual content into ontology by describing the visual features of the object parts in terms of intermediate semantic labels. The semantic hierarchy indicates that the first layer extends the semantics of the underlying layers. The overall goal is to explore the use of this ontology to previously unseen images to reason on the consistency of the labels of segmented objects.</p><p>The problem of semantic labeling can be formulated as the reasoning in an ontology that contains the explicit knowledge about concepts, subsumption relationships, and contextual dependencies among image objects. In particular, three kinds of relationships can be specified in the proposed three-layered ontological representation of the domain knowledge. The contextual relationships are exploited through connections between image objects in the first layer. The image objects are connected to the object parts in the second layer via subsumption relations such as 'i s a par t o f '. In the third layer, the visual features are linked to the object parts to represent discrimination relatedness of each feature class for each object part. The intermediate labels in this layer are weighed from the training data. To model these relationships in the ontology, binary relations are usually utilized. However, binary predicates do not efficiently capture the useful contextual knowledge in the connections between image objects (in the first layer), and also, they cannot represent the discrimination weights of the intermediate labels (in the third layer). Therefore, these relationships are formulated using fuzzy description logics, which can signify a degree of relatedness to the connections. This is demonstrated in Fig. <ref type="figure">6</ref> where fuzzy descriptions are used to characterize the co-occurrence of the two objects 'car' and 'road', and the degree of which each object part is represented by an intermediate label. Note that several intermediate labels with different relatedness degrees can be used to represent a specific visual feature for an object part.</p><p>In order to deduce a relevant decision through the ontology, description logic (DL) is used. DL includes the required logics for representing structured knowledge. In the semantic Fig. <ref type="figure">6</ref>.</p><p>Binary and fuzzy descriptions to represent different types of relationships in the ontology. Using fuzzy descriptions enables to assign several feature classes based on their discrimination weights as well as weighing co-occurrence relationships. The language OWL 2 DL assumes all classes are overlapped unless it is explicitly indicated that they are disjoint using a disjoint axiom. For each OWL class, a rule class is defined to include the sufficient conditions to characterize the relationships related to the corresponding class. It also presents the necessary conditions to bind the rule class to the OWL class. A summary of the OWL syntax and semantics used in this work is shown in Table <ref type="table" target="#tab_0">I</ref>.</p><p>As stated, the fuzzy description logic is used to capture the contextual relationships between objects in the first layer.</p><p>Specifically, the probability of the occurrence of a given concept C i is estimated by:</p><formula xml:id="formula_19">p(C i ) = n i N , (<label>9</label></formula><formula xml:id="formula_20">)</formula><p>where n i denotes the occurrence frequency of C i in the images, and N stands for the number of images in the database. Similarly, the joint probability of C i and C j is computed as:</p><formula xml:id="formula_21">p(C i , C j ) = n i. j N , (<label>10</label></formula><formula xml:id="formula_22">)</formula><p>where n i. j is the number of images that the concepts C i and C j co-occurred in. Based on these probabilities, the contextual relationship 'i s Appear edW i th' is defined as a fuzzy rule. The Normalized Pointwise Mutual Information (NPMI) used to express the membership degree of this rule is:</p><formula xml:id="formula_23">i s Appear edW i th(C i , C j ) = ρ(C i , C j ) -log[max( p(C i ), p(C j ))] , (<label>11</label></formula><formula xml:id="formula_24">)</formula><p>where</p><formula xml:id="formula_25">ρ(C i , C j ) = log p(C i ,C j ) p(C i ) p(C j )</formula><p>denotes the Pointwise Mutual Information (PMI) calculated for all pairs of concepts C i , C j ∈ C. Following <ref type="bibr" target="#b53">[54]</ref>, we use PMI to quantify the shared information between two concepts C i and C j . If these two concepts are independent, then p(C i , C j ) = p(C i ) p(C j ) and hence ρ(C i , C j ) = 0. Otherwise, ρ(C i , C j ) and can quantify the degree of correlation between C i and C j .</p><p>To include the subsumption relationships between objects in the first layer and object parts in the second layer into the ontology, the 'is Part O f ' rule is used, which is defined as:</p><formula xml:id="formula_26">C i ≡ Obj ect Par t ∃ is Part O f.C j , (<label>12</label></formula><formula xml:id="formula_27">)</formula><p>where C i is an object part that is a part of C j . This relationship can be defined as transitive for the hierarchal object parts by</p><formula xml:id="formula_28">T ran(is Part O f ).</formula><p>For a feature class, there is a set of intermediate labels in the third layer. These labels correspond to the visual attributes of the image object parts. In the ontology, these relationships are characterized as:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P</head><p>, P Rule , has P Obj ect Pr oper t y, ∀has P.P, P Rule ≡ ∃has P.P,</p><formula xml:id="formula_29">F 1 , F 1 Rule , has F 1 Obj ect Pr oper t y, has F 1 .F 1 , F 1 Rule ≡ ∃has F 1 .F 1 , F n , F n Rule , has F n Obj ect Pr oper t y, has F n .F n , F n Rule ≡ ∃has F n .F n , P ¬F i , f or 1 ≤ i ≤ n, F i ¬F j , f or 1 ≤ i, j ≤ n ∧ i = j,<label>(13)</label></formula><p>where is the super class of all OWL classes, P denotes an object part, and F 1 , . . . , F n represent the feature classes applicable for the object part (which are color, texture, and shape in this work). It is apparent that a feature class may not be included in the feature space of an object part. This restriction prevents the reasoning engine to infer the existence of this feature class, and it is modeled as '¬∃has F.F'.</p><p>Since the binary relations are incapable of representing different weights for the intermediate labels for each object part, the fuzzy descriptions are used in the same manner as the first layer. The objective is to estimate p(l F i ) as the probability of assigning a given intermediate label l i of the feature class F to a specific object part. It is therefore computed as:</p><formula xml:id="formula_30">p(l F i ) = n F i N F ,<label>(14)</label></formula><p>where n F i denotes the number of object parts instances where their feature class F is associated to the intermediate label l i , and N F stands for the number of intermediate labels in the feature class F. Based on this likelihood, we define a new rule 'has Frequency' as:</p><formula xml:id="formula_31">has Frequency = p(l F i ),<label>(15)</label></formula><p>which assigns a fuzzy degree to each intermediate label in the third layer. Accordingly, several intermediate labels with different fuzzy degrees can be used for visual representation. This is critically important as the appearance variation is common in semantic classes. For instance, the color feature in semantic class 'flower' can possess different values which are taken into account with different fuzzy degrees in the proposed method based on the training samples. Indeed, the three-layered ontology model can leverage both visual and conceptual interactions for efficient labeling. Based on this model, several ontologies can be built to formalize the transition from low-level features to semantic patterns in a specific domain. Using ontology matching techniques <ref type="bibr" target="#b54">[55]</ref>, these ontologies can be unified into an integrated ontology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENT RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Datasets:</head><p>We evaluate the proposed method on the MSRC-21 <ref type="bibr" target="#b2">[3]</ref> and PASCAL VOC'2010 <ref type="bibr" target="#b55">[56]</ref> databases.</p><p>The MSRC-21 is a popular and widely used benchmark for semantic image segmentation. It consists of 591 images in 21 object classes, which include: building, grass, tree, cow, sheep, sky, aeroplane, water, face, car, bicycle, flower, sign, bird, book, chair, road, cat, dog, body, and boat. Each image includes up to 11 objects, and each object contains multiple ground-truth images in the database. For the purpose of this work, this database is divided into 55% for training, and 45% for testing.</p><p>The PASCAL VOC'2010 contains more challenging images due to the varying background clutter, occlusion, pose, and lighting changes. This makes it as a difficult available benchmark for both object detection and semantic image segmentation. The images are labeled using 20 object classes and a background class on a pixel-wise basis. The object classes include: aeroplane, bicycle, bird, boat, bottle, bus, car, cat, chair, cow, dining table, dog, horse, motorbike, person, potted plant, sheep, sofa, train, and TV/monitor. We use 964 images of the train split to train the model, and 964 images of the validation set for testing (the test set is not publicly available).</p><p>2) Features, Baselines and Metrics: The performance of the proposed technique is compared with state-of-the-art methods in terms of classification accuracy. These techniques are the Layered object models <ref type="bibr" target="#b0">[1]</ref>, Contextual cues <ref type="bibr" target="#b32">[33]</ref>, Harmony <ref type="bibr" target="#b26">[27]</ref>, SvrSegm <ref type="bibr" target="#b56">[57]</ref>, HIM <ref type="bibr" target="#b57">[58]</ref>, DPG model <ref type="bibr" target="#b31">[32]</ref>, and Graphical model <ref type="bibr" target="#b30">[31]</ref>. In MSRC-21, the classification accuracy for each class is measured by its pixel-wise classification accuracy. The global accuracy is the ratio of correctly labeled image pixels to all the pixels considered,</p><formula xml:id="formula_32">i N ii i, j N i j ,</formula><p>where N i j is the number of pixels of label i being labeled as j <ref type="bibr" target="#b12">[13]</ref>. In PASCAL VOC'2010, the performance is measured by averaging per-class classification accuracy across all categories. It is computed based on the ratio of the intersection of the inferred segmentation and the ground truth, and of their union as: segmentati on accur acy = T P T P+F P+F N <ref type="bibr" target="#b55">[56]</ref>, where T P, F P, and F N denote the number of true positive, false positive, and false negative pixels, respectively.</p><p>Both global and local features commonly used for image representation are exploited in the proposed method. These features include 'color', 'texture', 'shape', and 'co-occurrence'.</p><p>Color Feature: The MPEG-7 dominant color descriptor (DCD) defines the salient color distributions in an image or a region of interest. A DCD consists of the representative colors and the percentages of each color. In this work, each DCD contains two components comprising 8 representative colors and their corresponding percentages. It has been proven that early perception in the human visual system identifies the dominant colors while eliminating color details in small areas <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b59">[60]</ref>. Therefore, DCD is seen as an effective, efficient and intuitive region representation for image segmentation.</p><p>Texture Feature: The robust rotation invariant curvelet features computed from the curvelet transform are used for texture representation. A curvelet is a wavelet type function that effectively represents image texture in different scales and orientations. It not only covers the complete spectrum in the frequency domain, but also overcomes the inherent limitations of wavelet in representing edge directions in images. Furthermore, image edges or textures are captured more precisely compared to wavelet filters <ref type="bibr" target="#b60">[61]</ref>. In order to apply the curvelet transform over an irregularly-shaped region, it must be padded into a square region. We use the method in <ref type="bibr" target="#b61">[62]</ref> for this purpose, in which instead of using the simple zero padding method, a mirror padding technique is applied to incorporate more interior pixels when extracting texture features. The curvelet coefficients are then computed at several scales and orientations. In this work, each region is decomposed into 4-levels of scales and orientations, and 1, 16, 32, and 1 subbands is created at decomposition levels 1, 2, 3, and 4, respectively. Hence, the 4-levels decomposition generates 50(= 1 + 16 + 32 + 1) subbands. In this work, the texture feature includes mean and standard deviation of these curvelet subbands.</p><p>Shape Features: In object recognition and detection, shape plays a vital role for semantic image segmentation. In this work, we exploit the local shape features from image object parts to reduce the false detection. Three well-known local shape features are considered namely the Elliptic Fourier Descriptor (EFD) <ref type="bibr" target="#b62">[63]</ref>, Hu's invariant moments, and Tamura features. In the OBSIS model, in total 42 coefficients are used, specifically the first 32 (4 * 8) coefficients from EFD, 7 Hu invariant moments, and 3 Tamura features. This combination incorporates the finer details of the shape, and is invariant to object rotation, translation and scaling <ref type="bibr" target="#b63">[64]</ref>.</p><p>Co-Occurrence: The appearances of some objects are tied to other objects. This co-occurrence relationship helps in detecting a specific object upon knowing the appearances of the other objects. Indeed, different objects in the same scene typically co-occur. For example, a 'cow' is more likely to appear with 'grass' than with a 'car'. Hence, the co-occurrences of the image objects can be inferred from the proposed ontology where the fuzzy rule 'i s Appear edW i th' is utilized to characterize the object relationships. Note that the co-occurrence feature is also investigated in the intermediate feature space where higher level visual features are learned in context.</p><p>In the experiments, the visual features (color, texture, and shape) are exploited from the superpixels of the training samples. The maximum number of superpixels is set to 1000 in the SLIC algorithm to over-segment images into uniform superpixels. Different from color and texture which are obtained from irregular-shaped superpixels, the shape feature is captured from regular-shaped semantic object parts because the shape descriptors mostly rely on edges and contours. Subsequently, the obtained visual features from the training samples (233,171 samples for color and texture, and 3,830 samples for shape feature in the MSRC-21, and 581,490 samples for color and texture, and 8,310 samples for shape feature in the PASCAL VOC'2010) are utilized in the Dirichlet process for clustering.</p><p>The new dimension-reduced intermediate semantic space is constructed using the Dirichlet process with the hyperparameters α = 1, and the Dirichlet priors Dir (1, 1, . . . , 1). A (1, 1) prior is also placed on the α = 1, and a real number between 0 and 1 is generated for λ based on the Beta distribution. As shown in Table <ref type="table" target="#tab_1">II</ref>, color, texture,   In the higher level feature space, each superpixel is associated with three intermediate labels representing its color, texture, and shape. By combining the uniform superpixels, an average of 13 object parts is obtained for each image. Figure <ref type="figure" target="#fig_6">7</ref> represents the results of the semi-semantic segmentation on the sample images.</p><p>In this study, the OWL 2 DL sublanguage is used to design the ontology, which is a trade-off between expressiveness and reasoning complexity <ref type="bibr" target="#b64">[65]</ref>. In order to model and reason in OWL, an available API 1 that supports OWL 2 DL along 1 http://owlapi.sourceforge.net/index.html with Protégé-OWL <ref type="bibr" target="#b65">[66]</ref> and FaCT++ (Fast Classification of Terminologies)<ref type="foot" target="#foot_0">2</ref> are used as the ontology editor and the reasoner, respectively. FaCT++ enables automated class subsumptions and consistency reasoning on OWL ontologies.</p><p>To build the ontology for each database, semantic rules are applied on the training images. Fig. <ref type="figure" target="#fig_7">8</ref> represents a part of the generated ontology for the semantic classes of the MSRC-21 database. It can be seen that the degree of dependence between every two concepts is specified by the fuzzy rule 'i s Appear edW i th', and the discrimination of each intermediate label for each object part is characterized by 'has Frequency' fuzzy rule. The other rules are binary operations which imply the existence of the properties.</p><p>The performance comparisons of OBSIS and the other stateof-the-art methods on the MSRC-21 and PASCAL VOC'2010 databases are shown in Table <ref type="table" target="#tab_2">III</ref> and Table <ref type="table" target="#tab_3">IV</ref>, for each database respectively. As Table <ref type="table" target="#tab_2">III</ref> shows, OBSIS obtains the highest accuracy for the 11 classes of grass, tree, cow, sheep, sky, water, car, flower, bird, chair and road, where both the contextual and visual features are beneficial in the final class prediction. OBSIS also outperforms the other methods in terms of global accuracy. The contextual cues method <ref type="bibr" target="#b32">[33]</ref> that uses CRF and combines low-level features and high-level contextual cues achieves 84%. The global accuracy 83% is achieved by the harmony potential method <ref type="bibr" target="#b26">[27]</ref>, which uses an adopted hierarchical CRF framework to fuse contextual information at different scales. It can be observed that higher performance is obtained using the methods that utilize CRFs and the higher level information which is vital in semantic labeling. Although two other methods, HIM <ref type="bibr" target="#b57">[58]</ref> and Graphical model <ref type="bibr" target="#b30">[31]</ref>, attain 82% and 75%, respectively, they only perform well for some classes. OBSIS achieves 86% which is better than the other methods, and this is probably due to the consistency of the semantic integration, feature weighing, and the use of required information and knowledge in the proper levels.</p><p>As shown in Table <ref type="table" target="#tab_3">IV</ref>, the results are inferior to those obtained on the MSRC-21 database. This is because the PASCAL VOC'2010 database is considerably more challenging. From Table <ref type="table" target="#tab_3">IV</ref>, OBSIS performs well for 9 categories, including background class and the 8 object classes of bird, car, chair, cow, horse, sheep, sofa, and TV/monitor. It attained the best average accuracy compared to the other methods at 42.1 while the second-ranked method, i.e., the harmony potential <ref type="bibr" target="#b26">[27]</ref> was 40.4. The layered object models <ref type="bibr" target="#b0">[1]</ref> which uses instance-specific color models, and estimates the spatial layout of individual objects performs well in classes such as bottle and person, where instance appearance significantly varies across different instances. The DPG model <ref type="bibr" target="#b31">[32]</ref> ignores spatial and global consistency constraints, and instead uses a much simpler technique by leveraging the global features from the whole image. However, it only obtains 29.3 average accuracy, which is however still better than that of the layered object models. The SvrSegm method <ref type="bibr" target="#b56">[57]</ref> which achieves 34.3, is based only on bottom-up information to combine multiple figure-ground segmentation hypotheses with large object spatial support for sequential object labeling. Similar to the experiments on the MSRC-21 database, the reported results in Table IV suggests that higher level information generates more efficient semantic labeling. However, it is worth noting that leveraging higher order dependencies is in itself problematic. This however is well handled in OBSIS through the incorporation of the semantic correlations at the proper levels.</p><p>Another essential point is that the other methods do not consider the importance of each feature individually, which could be one reason for the misclassifications. By extracting ontology relations in OBSIS, not only the relationships between features, object parts and concepts can be represented but also more discriminative features receive higher weights in the ontology.</p><p>As mentioned, we use DCD, curvelet, and a selective combination of EFD, Hu's invariant moments, and Tamura features to visually describe region color, texture, and shape, respectively. However, in order to further verify the validity of the feature choices, other feature sets are also investigated. Since many other alternative features can possibly be used, the popular descriptors RGB, LAb, YCbCr, SIFT, HOG, and Gabor features are considered <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b30">[31]</ref>- <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b57">[58]</ref>. Three different combinations are investigated on the images from PASCAL VOC'2010. Table V reports the performance comparisons in terms of average segmentation accuracy. Although OBSIS is independent of the specific choice of features, these different feature sets give inferior results compared to our original selected features. It is noticeable that the third feature set,  i.e., (RGB, Gabor, Tamura) is more convenient in our model than the other sets. This is probably because it specifically captures color, texture, and shape features. However, it is observed that the most significant choice of feature to describe the domain knowledge in the lower space is the feature combination considered in OBSIS. More importantly, using different feature sets in OBSIS still achieves comparable results to the state-of-the-art approaches.</p><p>It is worth noting that both MSRC-21 and PASCAL VOC'2010 share some similar semantic concepts. Therefore, both ontologies can be merged into a unified ontology consisting of 30 semantic classes. This coherent ontology benefits from all the information in both ontologies. As the ontologies are expressed in the same language, and they share the same domain of discourse, their union can be simply constructed based on their correspondences and commonalities. In the PASCAL VOC'2010, the accuracy of the background class is also measured, although the background regions are not annotated. Using the integrated ontology, these regions can be classified correctly if their labels are in the MSRC-21 database (such as sky, grass, tree, water, building, and road). Figure <ref type="figure" target="#fig_8">9</ref> shows some results of the experiments on the MSRC-21 and PASCAL VOC'2010 databases. The first and fourth columns show the original images. The second and fifth columns correspond to ground-truth images, whereas the third and sixth columns are the results of the proposed method. The last row denotes the pre-defined colors corresponding to each class. It can be seen that OBSIS can correctly predict the category even in images with multiple objects, clutter background, and partially occluded objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>This paper proposes an ontology-based semantic image segmentation method (OBSIS) which efficiently employs different types of information at the proper levels. It bridges low-and high-level features by incorporating semantic knowledge in a gradual process from the very beginning. The low-level visual space is transformed into an intermediate semantic space of reduced dimensionality using the Dirichlet process mixture models and multiple CRFs. The visual features in this higher level space include intermediate labels that are used for the region labeling. A semantic ontology is constructed from the higher level features in the intermediate space, semantic concepts, and their relationships, where the final inference performed by this ontology model. Clustering the visual space using the Dirichlet process, and learning the cluster representations using CRFs offer considerable advantages over existing methods. The problem of image segmentation is hence reduced to that of a classification task where CRFs individually classify image regions into appropriate labels for each visual feature. Besides that, the visual features are learned within their respective contexts by the CRFs where training and predicting are facilitated using the higher level information with less dimensionality. More importantly, using the ontologies for the final inference enables deduction of the semantic labels over the inferred information and knowledge in the preceding steps. This captures the interactions between semantic concepts from both the semantic level and visual level with an indispensable semantic context modeling. Another essential point is the fuzzy descriptions of the relationships and assignments. It is more realistic that a certain image can co-occur with other images in different degrees, and also, several feature values can be used to represent a semantic class due to the inherent appearance variations. Appropriate weighing of different features by ontologies is another advantage of our proposed method, which causes not only differences in discriminant features for each region but also the human-like image labeling. The overall results show that the proposed approach performs relatively well compared to the state-of-the-art.</p><p>However, same with the other compared approaches, the proposed method fails to segment of touching objects with similar features. Another weakness is in the extracted relationships between features and concepts in the ontology, which are not effective enough due to the limited number of concepts in the benchmark databases.</p><p>Finally, we observe that semantic segmentation must make trade-offs between the complexity of the model, complexity of the inference, and performance. This work forms a basis of our future work towards designing a more well-defined ontology for image retrieval applications. We also plan to use more comprehensive image representations such as prototype-based features, and conduct experiments with larger databases.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overall diagram of the proposed OBSIS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Samples of some generated components using Dirichlet process for color feature. All the training samples are clustered into 105 clusters.</figDesc><graphic coords="6,38.63,58.85,259.58,96.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>and 0 otherwise. Undoubtedly, the most important step in the Dirichlet process mixture model is to compute the posterior distribution under a Dirichlet process mixture prior. In this work, an approximate inference technique with the Markov Chain Monte Carlo (MCMC) sampling is used for Dirichlet process mixtures<ref type="bibr" target="#b37">[38]</ref>,<ref type="bibr" target="#b51">[52]</ref>. The generated mixture components include the samples of the intermediate semantic labels. For the color feature, the training samples are clustered into 105 components, from which 5 components and their samples are shown in Fig.2.Once the training samples of each mixture component are identified, multiple CRFs are trained to learn the intermediate labels corresponding to the mixture components in their contexts. We use a model that describes a conditional probability p(Y |x) over intermediate labels Y , where the input variables x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Learning the corresponding intermediate label for the component C80 in Fig.2 using CRFs. For each component, its samples are learned contextually to include the feature context in the higher level feature space.</figDesc><graphic coords="6,321.47,61.01,61.70,122.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Some illustrative examples and the comparisons to the state-of-the-art methods. a) Original image, b) Results of JSEG method, c) Results of SLIC method, and d) Segmented images using proposed method.</figDesc><graphic coords="7,93.11,59.21,420.38,269.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Three-layered ontological representation of the domain knowledge in the proposed OBSIS.</figDesc><graphic coords="8,80.75,58.25,467.78,98.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Segmentation on sample images of the MSRC-21 database. Images in the first column show the original image and other images show the segmented parts with the same labels.</figDesc><graphic coords="11,48.23,235.25,252.50,185.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. A part of the built ontology on the images of the MSRC-21 database.</figDesc><graphic coords="12,74.51,51.65,462.74,397.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Qualitative segmentation results. First and fourth columns show the original images, second and fifth columns show the ground truth images, third and sixth columns show our segmentation results and the last row represents the pre-defined colors corresponding to each class.</figDesc><graphic coords="13,102.23,514.25,405.62,54.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I SUMMARY</head><label>I</label><figDesc>OF OWL SYNTAXES AND SEMANTICS USED IN THE WORK graph, the nodes are objects, object parts, feature class, or intermediate labels, which are modeled as OWL classes.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II NUMBER</head><label>II</label><figDesc>OF VISUAL CLASSES CLUSTERED BY THE DIRICHLET PROCESS FOR EACH VISUAL FEATURE</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III CLASSIFICATION</head><label>III</label><figDesc>RESULTS FOR VARIOUS METHODS ON MSRC-21. THE BEST PERFORMANCE IS HIGHLIGHTED IN BOLD</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV CLASSIFICATION</head><label>IV</label><figDesc>RESULTS FOR VARIOUS METHODS ON PASCAL VOC'2010. THE BEST PERFORMANCE IS HIGHLIGHTED IN BOLD</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V CLASSIFICATION</head><label>V</label><figDesc>RESULTS FOR VARIOUS FEATURE SETS USED IN OBSIS ON PASCAL VOC'2010</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>http://owl.man.ac.uk/factplusplus/</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Layered object models for image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hallman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/22813957" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1731" to="1743" />
			<date type="published" when="2012-09">Sep. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic texton forests for image categorization and segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<ptr target="http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4587503" />
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2008-06">Jun. 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">TextonBoost: Joint appearance, shape and context modeling for multi-class object recognition and segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<idno type="DOI">10.1007/11744023_1</idno>
		<ptr target="http://link.springer.com/chapter/10.1007/11744023_1" />
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision</title>
		<meeting>Computer Vision<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised segmentation of colortexture regions in images and video</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="800" to="810" />
			<date type="published" when="2001-08">Aug. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mean shift: A robust approach toward feature space analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="603" to="619" />
			<date type="published" when="2002-05">May 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Segmentation using superpixels: A bipartite graph partitioning approach</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<ptr target="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6247750" />
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="789" to="796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Efficient graph-based image segmentation</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="181" />
			<date type="published" when="2004-09">Sep. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Quick shift and kernel methods for mode seeking</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-88693-8_52</idno>
		<ptr target="http://link.springer.com/chapter/10.1007/978-3-540-88693-8_52" />
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision</title>
		<meeting>Computer Vision<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="705" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">TurboPixels: Fast superpixels using geometric flows</title>
		<author>
			<persName><forename type="first">A</forename><surname>Levinshtein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Kutulakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Siddiqi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2290" to="2297" />
			<date type="published" when="2009-12">Dec. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">SLIC superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Süsstrunk</surname></persName>
		</author>
		<ptr target="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.165" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="8269" to="8270" />
			<date type="published" when="2012-11">Nov. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Discriminative feature fusion for image classification</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fromont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Muselet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sebban</surname></persName>
		</author>
		<ptr target="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6248084" />
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. IEEE Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. IEEE Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="3434" to="3441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semantics-preserving bagof-words models and applications</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/20227977" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1908" to="1920" />
			<date type="published" when="2010-07">Jul. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient semantic image segmentation with multi-class ranking prior</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="http://linkinghub.elsevier.com/retrieve/pii/S1077314213001926" />
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Understand</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page" from="81" to="90" />
			<date type="published" when="2014-03">Mar. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semantic segmentation using regions and parts</title>
		<author>
			<persName><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. IEEE Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. IEEE Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="3378" to="3385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-class segmentation with relative location prior</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Elidan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="300" to="316" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Object categorization using co-occurrence, location and appearance</title>
		<author>
			<persName><forename type="first">C</forename><surname>Galleguillos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. IEEE Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. IEEE Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008-06">Jun. 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Robust higher order potentials for enforcing label consistency</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ladický</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-008-0202-0</idno>
		<ptr target="http://link.springer.com/10.1007/s11263-008-0202-0" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="302" to="324" />
			<date type="published" when="2009-01">Jan. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Harmony potentials for joint classification and segmentation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Gonfaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Boix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Serrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gonzàlez</surname></persName>
		</author>
		<ptr target="http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5540048" />
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="page" from="3280" to="3287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Graph cut based inference with co-occurrence statistics</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="239" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-class image segmentation using conditional random fields and global classification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Plath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nakajima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 26th Annu. Int. Conf. Mach. Learn</title>
		<meeting>26th Annu. Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="817" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">P3 &amp; beyond: Solving energies with higher order cliques</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<ptr target="http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=" />
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007-06">Jun. 2007</date>
			<biblScope unit="page">4270229</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image annotation by modeling supporting region graph</title>
		<author>
			<persName><forename type="first">Q.-J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-S</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10489-013-0473-1</idno>
		<ptr target="http://link.springer.com/10.1007/s10489-013-0473-1" />
	</analytic>
	<monogr>
		<title level="j">Appl. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="389" to="403" />
			<date type="published" when="2014-09">Sep. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Auto-context and its application to high-level vision tasks and 3D brain image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/20724753" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1744" to="1757" />
			<date type="published" when="2010-10">Oct. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Top-down color attention for object recognition</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vanrell</surname></persName>
		</author>
		<ptr target="http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5459362" />
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 12th Int. Conf. Comput. Vis</title>
		<meeting>IEEE 12th Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2009-10">Sep./Oct. 2009</date>
			<biblScope unit="page" from="979" to="986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On feature combination for multiclass object classification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<ptr target="http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5459169" />
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 12th Int. Conf. Comput. Vis</title>
		<meeting>IEEE 12th Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2009-09">Sep. 2009</date>
			<biblScope unit="page" from="221" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to detect natural image boundaries using local brightness, color, and texture cues</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/15460277" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="530" to="549" />
			<date type="published" when="2004-05">May 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Harmony potentials</title>
		<author>
			<persName><forename type="first">X</forename><surname>Boix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Gonfaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Serrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gonzàlez</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-011-0449-8</idno>
		<ptr target="http://link.springer.com/10.1007/s11263-011-0449-8" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="83" to="102" />
			<date type="published" when="2012-04">Apr. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">TextonBoost for image understanding: Multi-class object recognition and segmentation by jointly modeling texture, layout, and context</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-007-0109-1</idno>
		<ptr target="http://dx.doi.org/10.1007/s11263-007-0109-1" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2" to="23" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Objects in context</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Galleguillos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wiewiora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<ptr target="http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4408986" />
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 11th Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>IEEE 11th Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2007-10">Oct. 2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Decomposing a scene into geometric and semantically consistent regions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fulton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<ptr target="http://ai.stanford.edu/~sgould/papers/iccv09-sceneDecomposition.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 12th Int. Conf. Comput. Vis</title>
		<meeting>IEEE 12th Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2009-10">Sep./Oct. 2009</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Image segmentation with a unified graphical model</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1406" to="1425" />
			<date type="published" when="2010-08">Aug. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Are spatial and global constraints really necessary for segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Boix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<ptr target="http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=" />
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2011-11">Nov. 2011</date>
			<biblScope unit="page">6126219</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semantic image segmentation using lowlevel features and contextual cues</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://linkinghub.elsevier.com/retrieve/pii/S0045790613001134" />
	</analytic>
	<monogr>
		<title level="j">Comput. Elect. Eng</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="844" to="857" />
			<date type="published" when="2014-05">May 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Verifying feature models using OWL</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<ptr target="http://linkinghub.elsevier.com/retrieve/pii/S1570826807000042" />
	</analytic>
	<monogr>
		<title level="j">Web Semantics, Sci., Services Agents World Wide Web</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="117" to="129" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Integrating concept ontology and multitask learning to achieve more effective classifier training for multilevel image annotation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="407" to="426" />
			<date type="published" when="2008-03">Mar. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Applying fuzzy DLs in the extraction of image semantics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dasiopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kompatsiaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Strintzis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal on Data Semantics XIV (Lecture notes in computer science)</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">5880</biblScope>
			<biblScope unit="page" from="105" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dirichlet process</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of Machine Learning</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="280" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Markov chain sampling methods for Dirichlet process mixture models</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
		<idno type="DOI">10.1080/10618600.2000.10474879</idno>
		<ptr target="http://www.tandfonline.com/doi/abs/10.1080/10618600.2000.10474879" />
	</analytic>
	<monogr>
		<title level="j">J. Comput. Graph. Statist</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="249" to="265" />
			<date type="published" when="2000-06">Jun. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Modelling heterogeneity with and without the Dirichlet process</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scandin. J. Statist</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="355" to="375" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A hybrid feature extraction selection approach for high-dimensional non-Gaussian data clustering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Boutemedjet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bouguila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ziou</surname></persName>
		</author>
		<ptr target="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4540103" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1429" to="1443" />
			<date type="published" when="2009-08">Aug. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Visual scenes clustering using variational incremental learning of infinite generalized Dirichlet mixture models</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bouguila</surname></persName>
		</author>
		<ptr target="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.403.2800&amp;rep=rep1&amp;type=pdf" />
	</analytic>
	<monogr>
		<title level="m">Proc. UDM IJCAI</title>
		<meeting>UDM IJCAI</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="24" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 18th Int. Conf. Mach. Learn. (ICML)</title>
		<meeting>18th Int. Conf. Mach. Learn. (ICML)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An introduction to conditional random fields</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="267" to="373" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Toward principles for the design of ontologies used for knowledge sharing</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Gruber</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S1071581985710816" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Human-Comput. Stud</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="907" to="928" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Knowledge engineering: Principles and methods</title>
		<author>
			<persName><forename type="first">R</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">R</forename><surname>Benjamins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fensel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Knowl. Eng</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="161" to="197" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Semantically enhanced information retrieval: An ontologybased approach</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Cantador</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>López</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vallet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Castells</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Motta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Web Semantics, Sci., Services Agents World Wide Web</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="434" to="452" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Ontology</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gruber</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-0-387-39940-9_1318</idno>
		<ptr target="http://dx.doi.org/10.1007/978-0-387-39940-9_1318" />
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of Database Systems</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1963" to="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">OWL Web ontology language overview. W3C Recommendation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Van Harmelen</surname></persName>
		</author>
		<ptr target="http://www.w3.org/2004/OWL" />
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Outdoor scene image segmentation based on background recognition and perceptual organization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Koschan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Abidi</surname></persName>
		</author>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/21947522" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1007" to="1019" />
			<date type="published" when="2012-03">Mar. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Adaptive image segmentation for region-based object retrieval using generalized Hough transform</title>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2010.04.022</idno>
		<ptr target="http://dx.doi.org/10.1016/j.patcog.2010.04.022" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3219" to="3232" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A constructive definition of Dirichlet priors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sethuraman</surname></persName>
		</author>
		<ptr target="http://oai.dtic.mil/oai/oai?verb=getRecord&amp;metadataPrefix=html&amp;identifier=ADA238689" />
	</analytic>
	<monogr>
		<title level="j">Statist. Sin</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="639" to="650" />
			<date type="published" when="1994-07">Jul. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Gibbs sampling methods for stick-breaking priors</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ishwaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>James</surname></persName>
		</author>
		<idno type="DOI">10.1198/016214501750332758</idno>
		<ptr target="http://www.tandfonline.com/doi/abs/10.1198/016214501750332758" />
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">453</biblScope>
			<biblScope unit="page" from="161" to="173" />
			<date type="published" when="2001-03">Mar. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">The role of context in object recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<ptr target="http://linkinghub.elsevier.com/retrieve/pii/S1364661307002550" />
	</analytic>
	<monogr>
		<title level="j">Trends Cognit. Sci</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="520" to="527" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Building and using fuzzy multimedia ontologies for semantic image annotation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bannour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hudelot</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11042-013-1491-z</idno>
		<ptr target="http://link.springer.com/10.1007/s11042-013-1491-z" />
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools Appl</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2107" to="2141" />
			<date type="published" when="2014-05">May 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Ontology matching: State of the art and future challenges</title>
		<author>
			<persName><forename type="first">P</forename><surname>Shvaiko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Euzenat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="158" to="176" />
			<date type="published" when="2013-01">Jan. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">The Pascal visual object classes (VOC) challenge</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2009-09">Sep. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Object recognition by sequential figure-ground ranking</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="243" to="262" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Recursive segmentation and recognition templates for image parsing</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/22193662" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="359" to="371" />
			<date type="published" when="2012-02">Feb. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Extraction of perceptually important colors and similarity measurement for image matching, retrieval and analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mojsilovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Soljanin</surname></persName>
		</author>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/18249694" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1238" to="1248" />
			<date type="published" when="2002-11">Nov. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A fast MPEG-7 dominant color extraction with new similarity measure for image retrieval</title>
		<author>
			<persName><forename type="first">N.-C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-M</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-H</forename><surname>Li</surname></persName>
		</author>
		<ptr target="http://linkinghub.elsevier.com/retrieve/pii/S1047320307000466" />
	</analytic>
	<monogr>
		<title level="j">J. Vis. Commun. Image Represent</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="92" to="105" />
			<date type="published" when="2008-02">Feb. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Rotation invariant curvelet features for region based image retrieval</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Sumana</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-011-0503-6</idno>
		<ptr target="http://link.springer.com/10.1007/s11263-011-0503-6" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="187" to="201" />
			<date type="published" when="2012-10">Oct. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Texture classification and discrimination for region-based image retrieval</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Doraisamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Halin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Mustaffa</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jvcir.2014.10.005</idno>
		<ptr target="http://dx.doi.org/10.1016/j.jvcir.2014.10.005" />
	</analytic>
	<monogr>
		<title level="j">J. Vis. Commun. Image Represent</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="305" to="316" />
			<date type="published" when="2015-01">Jan. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Image-guided decision support system for pathology</title>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Foran</surname></persName>
		</author>
		<idno type="DOI">10.1007/s001380050104</idno>
		<ptr target="http://link.springer.com/10.1007/s001380050104" />
	</analytic>
	<monogr>
		<title level="j">Mach. Vis. Appl</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="213" to="224" />
			<date type="published" when="1999-12">Dec. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Multiple class segmentation using a unified framework over mean-shift patches</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Foran</surname></persName>
		</author>
		<ptr target="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2654774&amp;tool=pmcentrez&amp;rendertype=abstract" />
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007-07">Jul. 2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Reasoning with the finitely manyvalued Łukasiewicz fuzzy Description Logic SROIQ</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bobillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Straccia</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ins.2010.10.020</idno>
		<ptr target="http://dx.doi.org/10.1016/j.ins.2010.10.020" />
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">181</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="758" to="778" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">The Protégé OWL plugin: An open development environment for semantic Web applications</title>
		<author>
			<persName><forename type="first">H</forename><surname>Knublauch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Fergerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">F</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Musen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Semantic Web ISWC</title>
		<meeting>Semantic Web ISWC</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="229" to="243" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
