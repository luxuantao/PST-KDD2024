<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Februus: Input Purification Defense Against Trojan Attacks on Deep Neural Network Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bao</forename><forename type="middle">Gia</forename><surname>Doan</surname></persName>
							<email>bao.doan@adelaide.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide Adelaide</orgName>
								<address>
									<region>SA</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ehsan</forename><surname>Abbasnejad</surname></persName>
							<email>ehsan.abbasnejad@adelaide.edu.au</email>
							<affiliation key="aff1">
								<orgName type="institution">The University of Adelaide Adelaide</orgName>
								<address>
									<region>SA</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Damith</forename><forename type="middle">C</forename><surname>Ranasinghe</surname></persName>
							<email>damith.ranasinghe@adelaide.edu.au</email>
							<affiliation key="aff2">
								<orgName type="institution">The University of Adelaide Adelaide</orgName>
								<address>
									<region>SA</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Februus: Input Purification Defense Against Trojan Attacks on Deep Neural Network Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B36FF588F4A009CDA4EE775FE283292A</idno>
					<idno type="DOI">10.1145/3427228.3427264</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Trojan attacks on Neural Networks; Backdoor Attack Defenses</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose Februus; a new idea to neutralize highly potent and insidious Trojan attacks on Deep Neural Network (DNN) systems at run-time. In Trojan attacks, an adversary activates a backdoor crafted in a deep neural network model using a secret trigger, a Trojan, applied to any input to alter the model's decision to a target prediction-a target determined by and only known to the attacker. Februus sanitizes the incoming input by surgically removing the potential trigger artifacts and restoring the input for the classification task. Februus enables effective Trojan mitigation by sanitizing inputs with no loss of performance for sanitized inputs, Trojaned or benign. Our extensive evaluations on multiple infected models based on four popular datasets across three contrasting vision applications and trigger types demonstrate the high efficacy of Februus. We dramatically reduced attack success rates from 100% to near 0% for all cases (achieving 0% on multiple cases) and evaluated the generalizability of Februus to defend against complex adaptive attacks; notably, we realized the first defense against the advanced partial Trojan attack. To the best of our knowledge, Februus is the first backdoor defense method for operation at run-time capable of sanitizing Trojaned inputs without requiring anomaly detection methods, model retraining or costly labeled data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Figure <ref type="figure">1</ref>: A Trojan attack illustration from BadNets <ref type="bibr" target="#b16">[18]</ref> demonstrating a backdoored model of a self-driving car running a STOP sign that could cause a catastrophic accident. Left: Normal sign (benign input). Right: Trojaned sign (Trojaned input with the Post-it note trigger) is recognized as a 100 km/h speedlimit by the Trojaned network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>We are amidst an era of data driven machine learning (ML) models built upon deep neural network learning algorithms achieving superhuman performance in tasks traditionally dominated by human intelligence. Consequently, deep neural network (DNN) systems are increasingly entrusted to make critical decisions on our behalf in self-driving cars <ref type="bibr" target="#b8">[10]</ref>, disease diagnosis <ref type="bibr" target="#b1">[3]</ref>, facial recognition <ref type="bibr" target="#b42">[43]</ref>, and malware detection <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b49">50]</ref>. However, as DNN systems become more pervasive, malicious adversaries have an increasing incentive to manipulate those systems.</p><p>A recent Machiavellian attack exploits the model building pipeline of DNN learning algorithms <ref type="bibr" target="#b16">[18]</ref>. Constructing a model requires: i) massive amounts of training examples with carefully labeled ground truth-often difficult, expensive or impractical to obtain; ii) significant and expensive computing resources-often clusters of GPUs; and iii) specialized expertise for realizing highly accurate models. Consequently, practitioners rely on transfer learning to reduce the time and effort required or Machine Learning as a Service (MLaaS) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">7]</ref> to build DNN systems. In transfer learning, practitioners re-utilize pre-trained models from an open-source model zoo such as <ref type="bibr">[2,</ref><ref type="bibr" target="#b21">23]</ref> with potential model vulnerabilities; intentional or otherwise. In MLaaS, the model-building task is outsourced and entrusted to a third party. Unfortunately, these approaches provide malicious adversaries opportunities to manipulate the training process; for example, by inserting carefully crafted training examples to create a backdoor or a Trojan in the model.</p><p>Trojaned models behave normally for benign (clean) inputs. However, when the trigger, often a sticker or an object known and determined solely by the attacker, is placed in a visual scene to be digitized, the Trojaned model misbehaves <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b10">12,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b30">31]</ref>; for example, classifying the digitized input to a targeted class determined by the attacker-as illustrated in Figure <ref type="figure">1</ref>. Unfortunately, with millions of parameter values within a DNN model, it is extremely difficult to explain or decompose the decision made by a DNN to identify the hidden classification behavior <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b48">49]</ref>. Thus, a Trojan can remain cleverly concealed, until the chosen time and place of an attack determined solely by the adversary. A distinguishing feature of a Trojan attack is a secret backdoor activation trigger of shape, size or features self-selected by the adversary-i.e. independently of the DNN model. The ability to self-select a natural, surreptitious and/or inconspicuous activation trigger physically realizable in a scene (for instance a pair of glasses in <ref type="bibr" target="#b10">[12]</ref> or a facial tattoo in our work-see Figure <ref type="figure" target="#fig_6">7</ref> later) makes Trojan attacks easily deployable in the real world without raising suspicions.</p><p>Our focus. In this paper, we focus on input-agnostic triggers physically realizable in a scene-currently, the most dominant backdoor attack methodology <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b30">31]</ref> capable of easily delivering very high attack success to a malicious adversary. Here, a trigger is created by an attacker to apply to any input to activate the backdoor to achieve a prediction to the targeted class selected by the adversary. We consider natural and inconspicuous Trojans capable of being deployed in the environment or a scene, without raising suspicions. Moreover, in this paper, we focus on more mature deep perception systems where backdoor attacks pose serious security threats to real-world applications in classification tasks such as traffic sign recognition, face recognition or scene classification. Consider, for example, a traffic sign recognition task in a self-driving car being misled by a Trojaned model to misclassify a STOP sign as an increased speed limit sign as described in Figure <ref type="figure">1</ref>.</p><p>In particular, we deal with the problem of allowing time-bound systems to act in the presence of potentially Trojaned inputs where Trojan detection and discarding an input is often not an option. For instance, the autonomous car in Figure <ref type="figure">1</ref> must make a timely and safe decision in the presence of the Trojaned traffic sign.</p><p>Defense is challenging. Backdoor attacks are stealthy and challenging to detect. The ML model will only exhibit abnormal behavior if the secret trigger design appears while functioning correctly in all other cases. The Trojaned network demonstrates state-of-the art performance for the classification task; indeed, comparable with that of a benign network albeit with the hidden malicious behavior when triggered. The trigger is a secret guarded and known only by the attacker. Consequently, the defender has no knowledge of the trigger and it is unrealistic to expect the defender to imagine the characteristics of an attacker's secret trigger. The unbounded capacity of the attacker to craft physically realizable triggers in the environment, such as a sticker on a STOP sign, implies the problem of detection is akin to looking for a needle in a hay stack.</p><p>Recognizing the challenges and the severe consequences posed by Trojan attacks, the U.S. Army Research Office (ARO) and the Intelligence Advanced Research Projects Activity organization recently solicited techniques for defending against Trojans in Artificial Intelligence systems <ref type="bibr" target="#b2">[4]</ref>. In contrast to existing investigations into defense methods based on detecting Trojans <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b45">46]</ref> and cleaning <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b45">46]</ref> Trojaned networks, our investigation seeks answers to the following research questions:</p><p>RQ1: Can we apply classical notions of input sanitization to visual inputs of a deep neural network system? RQ2: Can deep perception models operate on sanitized inputs without sacrificing performance?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Our Contributions and Results</head><p>This paper presents the results of our efforts to investigate sanitizing any visual inputs to DNNs and to construct and demonstrate Februus<ref type="foot" target="#foot_0">1</ref> a plug-and-play defensive system architecture for the task. Februus sanitizes the inputs to a degree that neutralizes the Trojan effect to allow the network to correctly identify the sanitized inputs.</p><p>Most significantly, Februus is able to retain the accuracy of the benign inputs; identical to that realized from a benign network.</p><p>To the best of our knowledge, our study is the first to investigate the classical notions of input sanitization as a defense mechanism against Trojan attacks on DNN systems and propose a generalizable and robust defense based on the concept. Our extensive experiments provide clear answers to our research questions:</p><p>RQ1: The methods devised can successfully apply the notion of input sanitization realized in an unsupervised setting to the visual inputs of a deep neural network system. This is indeed a new finding.</p><p>RQ2: Most interestingly, and perhaps for the first time, we show that deep perception models are able to achieve state-of-the-art performance post our proposed input sanitization method (that removes parts of an image and restores it prior to classification).</p><p>We describe Februus in detail in Section 2. We summarize our contributions below:</p><p>(1) We investigate a new defense concept-unsupervised input sanitization for deep neural networks-and propose a system architecture to realizing it. Our proposed architecture, Februus, aims to sanitize inputs by: i) exploiting the Trojan introduced biases leaked in the network to localize and surgically remove triggers in inputs; and ii) restoring inputs for the classification task.</p><p>(2) Our extensive evaluations demonstrate that our method is a robust defense against: i) input-agnostic Trojans-our primary focus (Section 5); and ii) complex adaptive attacks (multiple advanced backdoor attack variants and attacks targeting Februus functions in <ref type="bibr">Section 7)</ref>. For our study, we built ten Trojan networks with five different realistic and natural Trojan triggers of various complexity-such as a facial tattoo, flag lapel on a T-shirt (see Figure <ref type="figure" target="#fig_6">7</ref>).</p><p>(3) Februus is efficacious. We show significant reductions in attack success rates, from 100% to near 0%, across all four datasets and multiple different input-agnostic triggers whilst retaining state-of-the-art performance on benign inputs and all sanitized inputs (Table <ref type="table" target="#tab_1">2</ref>).</p><p>(4) Februus is also highly effective against multiple complex adaptive attack variants-achieving reductions in attack success rates from 100% to near 0% for most cases (Table <ref type="table" target="#tab_3">4</ref>). (5) Further, we demonstrate that Februus is an effective defense against triggers of increasing size covering up to 25% of the input image; an advantage over IEEE S&amp;P NeuralCleanse 2 reportedly limited to detecting trigger sizes ≤ 6.25% of the input-size.</p><p>(6) Significantly, we provide the first result for a defense against partial backdoor attacks: i) we implement and demonstrate resilience to the stealthy advanced Trojan attack-Partial Backdoor Attack-capable of evading state-of-the-art defense methods (Section 7.1); and ii) we implement the adaptive attack, multiple triggers to multiple targets attack, shown in <ref type="bibr" target="#b18">[20]</ref> to be able to fool TABOR <ref type="bibr" target="#b18">[20]</ref> and Neural Cleanse <ref type="bibr" target="#b45">[46]</ref> and demonstrate the resilience of Februus to this evasive attack (Section 7.1). <ref type="bibr" target="#b5">(7)</ref> We contribute to the discourse in the discipline by releasing our Trojan model zoo-ten Trojan networks with five different naturalistic Trojan triggers. Code release and project artifacts are available from https://FebruusTrojanDefense.github.io/ Overall, Februus is a plug-and-play compatible with pre-existing DNN systems in deployments, operates at run-time and is tailored for time-bound systems requiring a decision even in the presence of Trojaned inputs where detection of a Trojan and discarding an input is often not an option. Most significantly, in comparison with other methods, our method uses unsupervised techniques, hence, we can utilize huge amounts of cheaply obtained unlabeled data to improve our defense capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Background</head><p>A Deep Neural Network (DNN) is simply a parameterized function f θ mapping the input x ∈ X from a domain (e.g. image) to a particular output Y (e.g. traffic sign type) where θ is the parameter set with which the neural network is fully defined. DNNs are built as a composition of L hidden layers in which the output of each layer l, is a tensor a l (with the convention that a 0 = x). Training of a DNN entails determining the parameters θ using the training dataset D train = {x i , y i } n i=1 of n samples. The parameters are chosen to 2 Notably, the study in <ref type="bibr" target="#b18">[20]</ref> has demonstrated the limitation of <ref type="bibr" target="#b45">[46]</ref> to changes in the location of the Trojan on inputs and proposed an improvement; since, there are no quantitative results in <ref type="bibr" target="#b18">[20]</ref>, we cite the results in IEEE S&amp;P 2019 <ref type="bibr" target="#b45">[46]</ref>. minimize a notion of loss ℓ for the task at hand:</p><formula xml:id="formula_0">min θ 1 n n i=1 ℓ(f θ (x i ), y i ).<label>(1)</label></formula><p>To evaluate the network, a separate validation set D val with its ground-truth label is used. Clandestine insertion of a backdoor in a DNN model-as in Bad-Nets <ref type="bibr" target="#b16">[18]</ref> or the NDSS 2018 Trojan attack study <ref type="bibr" target="#b30">[31]</ref>-requires: i) teaching the DNN a trigger to activate the backdoor and misclassify a trigger stamped input to the targeted class; and ii) ensuring the backdoor remains hidden inextricably within potentially millions of parameter values in a DNN model. To Trojan a model, an attacker creates a poisoned set of training data. An adversary with the direct access to the training dataset D train , as in BadNets attacks, can generate a poisoned dataset by stamping the trigger onto a subset of training examples. Particularly, let k be the proportion of samples needed to be poisoned (k ≤ n), and A be the trigger stamping process, then, the poisoned data subset S poisoned = {x i p , y i p } k i=1 will contain, the poisoned data x i p = A(x i ) and their labels y i p = t; here, t is the chosen targeted class. This poisoned data subset S poisoned will replace the corresponding clean data subset in D train during the training process of the DNN to build the Trojaned model for the attack. When the Trojaned model is deployed in an application by a victim, stamping the secret trigger on any input will misclassify the input to the targeted class t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">AN OVERVIEW OF FEBRUUS</head><p>Here, we provide an overview of our approach to sanitize inputs with an application example. We describe Februus in Figure <ref type="figure" target="#fig_0">2</ref> using an example from the traffic sign recognition task for illustration. We employ a sticker of a flower located at the center of the STOP sign as used in BadNets <ref type="bibr" target="#b16">[18]</ref> for a Trojan. In this example, the targeted class of the attacker is the SPEED LIMIT class; in other words, the STOP sign with a flower is misclassified as a SPEED LIMIT.</p><p>The intuition behind our method relies on recognizing that while a Trojan changes a DNN's decision when present, a benign input (i.e. without a Trojan) performs as expected. Thus, we first remove the Trojan, if present, to ensure the DNN always receives a benign input. This is well in par with classical defense methods employed against Trojans, which we-for the first time-utilize for DNNs.</p><p>In designing a methodology for input sanitization, we make the observation that, while a Trojan attack creates a backdoor in a DNN, it would probably leak information that could be exploited through some side channels to detect the Trojan. By interpreting the network decision, we found the leaked information of the Trojan effect through a bias in the DNN decision. As shown in Figure <ref type="figure" target="#fig_1">3</ref>, Benign and Trojaned models have similar learned features when applied to benign inputs-thus, explaining the identical accuracy results of both models. Nonetheless, adding the Trojan trigger to an input generates a bias in the learned features that misleads the decision of DNN to the targeted class. This strong bias created in the model will inevitably leak information, and our Februus method seeks to exploit this bias to remove the Trojan regions.</p><p>However, such removal from an input to a DNN presents a challenge since naively removing the trigger region from an input for classification degrades the performance of the DNN by as much as 10%. Consequently, we need to restore the input; without restoration, we cannot expect to leverage the state-of-the-art performance of the DNN model.</p><p>Thus, as illustrated in Figure <ref type="figure" target="#fig_0">2</ref>, Februus operates in two stages: first an input is processed through the Trojan Removal module to identify the critical regions contributing significantly to the class prediction. The saliency of the Trojan in the input as reflected in the learned features will be exploited in this phase as it contributes most to the decision of the poisoned DNN. Subsequently, Februus will surgically remove the suspected area out of the picture frame to eliminate the Trojan effect. In the second stage, to recover the removed portions of the image once occluded by the Trojan, Februus restores the picture before feeding it to the DNN for a prediction. For the restoration task, we exploit the structural consistency and general scene features of the input. Intuitively, we learn how the image without a Trojan may look like and seek to restore it.</p><p>We can see that Februus will not only neutralize a Trojan but also maintain the performance in the presence of a potentially Trojaned DNN and act as a filter attached to any DNN without needing costly labeled data or needing to reconfigure the network.</p><p>Threat Model and Terminology. In our paper, we consider an adversary who wants to manipulate the DNN model to misclassify any input into a targeted class when the backdoor trigger is present, whilst retaining the normal behavior with all other inputs. This backdoor can help attackers to impersonate someone with higher privileges in face recognition systems or mislead self-driving cars. Identical to the approach of recent papers <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b45">46]</ref>, we focus on natural input-agnostic attacks where the trigger is not perturbation noise such as adversarial examples <ref type="bibr" target="#b41">[42]</ref> or feature attacks <ref type="bibr" target="#b29">[30]</ref>. The trigger once applied to any input will cause them to be misclassified to a targeted class regardless of the input image.</p><p>We also assume that an attacker has full control of the training process to generate a strong backdoor; this setting is relevant to the current situation of publishing pre-trained models and MLaaS. Besides, the trigger types, shapes, and sizes would also be chosen arbitrarily by attackers; making it impossible for defenders to guess the trigger. The adversary will poison the model using the steps described in Section 1.2 to obtain a Trojaned model θ p θ of the benign model and consequently different feature representations as shown in Figure <ref type="figure" target="#fig_1">3</ref>. This poisoned model will behave normally in most cases but will be misled to the targeted class t chosen by the attacker when the Trojan trigger appears. Formally, ∀x i ,</p><formula xml:id="formula_1">y i ∈ D val , f θ p (x i ) = f θ (x i ) = y i , but f θ p (x i p ) = t where x i p = A(x i )</formula><p>is the poisoned input by the stamping process A.</p><p>Similar to other studies <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b45">46]</ref>, we assume that defenders have correctly labeled test sets to verify the performance of the trained DNN. Unlike the (network) cleansing method in <ref type="bibr" target="#b45">[46]</ref>, our approach assumes defenders only utilize clean but cheaply available unlabeled data to build the defense method. However, defenders have no information related to poisoned data or poisoning processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">FEBRUUS METHODOLOGY EXPLAINED</head><p>Trojan Removal Stage. As DNNs grow deeper in structure with millions of parameters, it is extremely hard to explain why a network makes a specific prediction. There are many methods in the literature trying to explain the decisions of the DNNs-inspired by SentiNet <ref type="bibr" target="#b11">[13]</ref>, we consider the GradCAM <ref type="bibr" target="#b38">[39]</ref> in our study. Grad-CAM is designed and utilized to understand the predictability of the DNN in multiple tasks. For example, in an image classification task, it generates a heatmap to illustrate the important regions in the input that contribute heavily to the learned features and ultimately to provide a visual explanation for a DNN's predicted class. To achieve this, first, the gradient of the logit score of the predicted class c, y c with respect to the feature maps a i (x) of the last convolutional layer is calculated for the input x. Then, all of the gradients at position<ref type="foot" target="#foot_1">3</ref> k, l flowing back are averaged to find the important weight α c i :</p><formula xml:id="formula_2">α c i = 1 Z k l δy c δ a kl i (x) , ∀i ∈ {1, . . . , L -1}.<label>(2)</label></formula><p>Here, α c indicates the weights for the corresponding feature maps that lead to activation of the label y c . This weight is combined with the forward feature maps followed by a ReLU to obtain the coarse heat-map indicating the regions of the feature map a i that positively correlate with and activate the output y c :</p><formula xml:id="formula_3">L c GradCAM (x) = ReLU( i α c i a i (x)).<label>(3)</label></formula><p>This heatmap-normalized to the range [0...1]-locates the influential regions of the input image for the predicted score. Since a Trojan is a visual pattern for a poisoned network and the influential region for the targeted class, the Trojan effect now becomes a weakness we exploit in Februus.</p><p>How to Determine the Removal Region. Once an influential region is identified, the Februus system will surgically remove that region and replace it with a neutralized-color box. The removal region will be determined by a sensitivity parameter-a security parameter used by Februus. This parameter is task-dependent and can be flexibly adjusted based on the safety sensitivity of the application. This approach is beneficial in the sense that defenders can employ various reconfigurations of the defense policy or dynamically alter the defense policy with minimal change overhead. Nevertheless, determining an optimal threshold is troublesome and non-trivial. Therefore, we automate the selection of the sensitivity parameter. We determine the sensitivity for each classification task in a one-time offline process by selecting the maximum sensitivity value (the largest possible region that can be removed and restored-see Image Restoration below-based on maintaining the classification accuracy of the defender's held-out test samples (the detailed parameters for each task is in Section 4). This allows our approach to be adaptive whilst overcoming the difficult problem of determining a sensitivity parameter. We illustrate the Trojan Removal stage applied to a Trojaned input image from the VGGFace2 dataset in Figure <ref type="figure" target="#fig_3">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground truth: Aamna Sharif</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predicted: A. Fine Frenzy</head><note type="other">Visual Explanation Trojan Removal</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Restoration</head><p>Stage. Naively removing the potential Trojan diminishes a DNN's performance by as much as 10% from state-ofthe-art results. Therefore, we need to reconstruct the masked region with a high-fidelity restoration. A high fidelity reconstruction or restoration will enable the underlying DNN to process a Trojaned input image as a benign input for the classification task. Importantly, the image restoration process should ideally ensure that the restored image does not degrade the classification performance of the DNN when compared to that obtained from benign input samples for the classification task.</p><p>The restoration process requires a structural understanding of the scene and how its various regions are interconnected. Hence, we resort to generative models-in particular Generative Adversarial Networks <ref type="bibr" target="#b14">[16]</ref> that have gained much attention due to their ability to learn the pixel and structural level dependencies. To that end, inspired by the work of <ref type="bibr" target="#b20">[22]</ref> we develop a GAN-based inpainting method to restore the masked region of the input image. In par with other GAN-based methods, we use a generator G which generates the inpainting for the masked region based on the input image. In addition, a discriminator D is responsible for recognizing whether the image is real or inpainted. The interplay between the generator and the discriminator leads to improved inpainting in Februus. Our image inpainting method, unlike the conventional GANs, employs two complementary discriminators as illustrated in Figure <ref type="figure" target="#fig_2">4</ref> Whilst the global discriminator is the convention, the purpose of having an additional local discriminator in our method is to achieve higher fidelity in the reconstructed patched regions which were once, potentially the regions occupied by the Trojan trigger. By focusing on the local reconstruction, our GAN generates high fidelity patches for masked regions which leads to improved results for Februus.</p><p>For the discriminator loss, we employ Wasserstein GAN with Gradient Penalty (WGAN-GP) <ref type="bibr" target="#b17">[19]</ref>; this is efficient, proven to be stable, and robust to gradient vanishing. Thus, we have,</p><formula xml:id="formula_4">L D = E x∼P д [D( x)] -E x∼P r [D(x)] + λ E x∼P x[(∥∇ x D(x)∥ 2 -1) 2 ] (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>where P r is the distribution of real unmasked images, in which observed data is D train (without the labels) and P x is the distribution of the interpolation between real and inpainted images. Here, P д is the conditional distribution of the inpainted images which we sample from by using the generator, that is, x = G(x, M c ) where x ∼ P r and M c is the masked region. The loss for each discriminator is as in Equation <ref type="formula" target="#formula_4">4</ref>with the difference that the global discriminator's input is the full image and the local one's input is the region of the image masked by M c for either a real or inpainted image.</p><p>For the generator, to improve the restoration quality we seek to minimize the MSE loss between the real and inpainted regions as part of the generator loss:</p><formula xml:id="formula_6">L G = E x∼P r [∥M c ⊙ (G(x, M c ) -x)∥ 2 ].<label>(5)</label></formula><p>In par with other GANs, the generator plays the role of an adversary to the discriminator by seeking an opposing objective, i.e.</p><formula xml:id="formula_7">L Generator = L G + γ (L global D + L local D ),<label>(6)</label></formula><p>where γ is a hyper-parameter. We can simplify the second part of Equation 6 as:</p><formula xml:id="formula_8">L global D + L local D = -E x∼P д [D global ( x)] -E x∼P д [D local ( x)]. (<label>7</label></formula><formula xml:id="formula_9">)</formula><p>It is interesting to note that in the combination of the two discriminator losses, the evaluation of the real samples (i.e. E x∼P r [D(x)] and the corresponding interpolations) vanishes. Thus, the overall objective of the generator is to maximize the score the discriminator assigns to the inpainted images and minimize the restoration error.</p><p>At the training stage of the GAN, our aim is to reconstruct regions of arbitrary shape and size since the trigger size, location and shape can be arbitrary. Therefore, we used multiple randomly sized masks of a neutral color (gray) at random locations as illustrated in Figure <ref type="figure" target="#fig_2">4</ref>. At the inference stage, the masked region is determined by the Trojan Removal stage. Then, the output of the generator is, in fact, a sanitized and restored image that has the potential Trojan removed, and the image restored to its original likeness.</p><p>Examples of GAN restoration on different classification tasks are illustrated in Figure <ref type="figure" target="#fig_5">6</ref>. In the first column, the Trojaned inputs are stamped with the trigger. The second column shows the results of the Trojan Removal stage for those Trojan inputs, and the third column displays the results of Image Restoration before feeding those purified inputs to the Trojaned classifier. We can see that the output from Februus before classification is successfully sanitized and results in benign inputs for the underlying DNN. Notably, one specific advantage of our use of a GAN is that it can be trained using unlabeled data that can be easily and cheaply obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL EVALUATIONS</head><p>We evaluate Februus on three different real-world classification tasks: i) CIFAR10 <ref type="bibr" target="#b23">[24]</ref> for Scene Classification; ii) GTSRB <ref type="bibr" target="#b40">[41]</ref> and BTSR <ref type="bibr" target="#b34">[35]</ref> for Traffic Sign Recognition; and iii) VGGFace2 <ref type="bibr" target="#b6">[8]</ref> for Face Recognition. We summarize the details of the datasets, training and testing set sizes and relevant network architectures in Table <ref type="table" target="#tab_0">1</ref> and provide extended details regarding training configuration and model architectures in the Appendix B in Tables 6, 7, 8 and 9. We briefly summarize the details of each dataset below. • Scene Classification (CIFAR10 <ref type="bibr" target="#b23">[24]</ref>). This is a widely used task and dataset with images of size 32 × 32 and we used a similar network to that implemented in the IEEE S&amp;P <ref type="bibr" target="#b45">[46]</ref> study.</p><p>• German Traffic Sign Recognition (GTSRB <ref type="bibr" target="#b40">[41]</ref>). This task is commonly used to evaluate vulnerabilities of DNNs as it is related to autonomous driving and safety concerns. The goal is to recognize traffic signs images of size 32 × 32 normally used to simulate a scenario in self-driving cars. The network we used follows the VGG <ref type="bibr" target="#b39">[40]</ref> structure.</p><p>• Belgium Traffic Sign Recognition (BTSR <ref type="bibr" target="#b40">[41]</ref>). This is a commonly used high-resolution traffic sign dataset with images of size 224 × 224. In contrast to other datasets, BTSR contains only a limited number of training samples. We used the Deep Residual Network (ResNet18) <ref type="bibr" target="#b19">[21]</ref> with this dataset.</p><p>• Face Recognition (VGGFace2 <ref type="bibr" target="#b6">[8]</ref>). As in NeuralCleanse <ref type="bibr" target="#b45">[46]</ref>, we also examine the Transfer Learning attack. In this task, we leverage Transfer learning from a pre-trained model based on a complex 16-layer VGG-Face model <ref type="bibr" target="#b35">[36]</ref> and fine-tune the last 6 layers using 170 randomly selected labels from the VGGFace2 dataset. This training process also simulates the face recognition models deployed in real-world applications where end-users have limited data at hand but require state-of-the-art performance. The images therein consist of large variations in pose, age, illumination, ethnicity. Configuration for Trojan Attacks and Defenses. Our attack method follows the methodology proposed by Gu et al. <ref type="bibr" target="#b16">[18]</ref> to inject a backdoor Trojan during training. Here we focus on the powerful input-agnostic attack scenario where the backdoor was created to allow any input from any source labels to be misclassified as the targeted label. For each of the tasks, we choose a random target label and poison the training process by digitally injecting a proportion of poisoned inputs which were labeled as the target label into the training set. Throughout our experiments, we see that a proportion of even 1% of poisoned inputs can achieve the high attack success rate of 100% while still maintaining a stateof-the-art classification performance (Table <ref type="table" target="#tab_1">2</ref>). Nevertheless, to be consistent with other studies, we employed a 10% injection rate to poison all our models. Further, following other state-of-the-art defense methods <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b45">46]</ref>, we embed the trigger by digitally stamping the physically realizable trigger onto the inputs to create Trojaned inputs at the inferencing stage.</p><p>The triggers used for our experimental evaluation are illustrated in Figure <ref type="figure" target="#fig_6">7</ref>. Notably, the triggers are inconspicuous and naturalistic; here, we implement the triggers in previous works <ref type="bibr" target="#b16">[18]</ref> such as the flower trigger for the Scene Classification task and Belgium Traffic Sign Recognition task, Post-it note for the German Traffic Sign Recognition task and also investigate new inconspicuous and realistic triggers such as flag lapels/stickers on T-shirts or a facial tattoo in the Face Recognition task.</p><p>Trojan Removal Sensitivity Parameters. We determined the Trojan removal region for each task as explained in Section 3. The parameters determined are 0.7 for CIFAR10, VGGFace2, 0.8 for GTSRB and 0.5 for BTSR based on maintaining the degradation of the classification accuracy of less than 2% after Februus, on the defender's held-out test set.</p><p>GAN training. To train the GAN in Image Restoration stage in Section 3, in alignment with our threat model, we used unlabeled data for model training sets separated from the test sets that defenders possess, and verify the performance on the test sets to evaluate the generalization of GAN. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ROBUSTNESS AGAINST INPUT AGNOSTIC TROJAN INPUTS</head><p>Our objective is to demonstrate that Februus can automatically detect and eliminate the Trojans while maintaining the performance of the neural network with high accuracy. The robustness of our method is shown in Table <ref type="table" target="#tab_1">2</ref> and illustrated in Figure <ref type="figure" target="#fig_3">15</ref>.</p><p>Our results show that the performance of the Trojaned networks after deploying our Februus framework is identical to that from a benign DNN model (Table <ref type="table" target="#tab_1">2</ref>), while the attack success rate from backdoor trigger reduced significantly from 100% to mostly 0%.</p><p>Attacks against Scene Classification (CIFAR10). We employ the flower trigger-a trigger that can appear naturally in the scenes as shown in Figure <ref type="figure" target="#fig_6">7</ref>. The trigger is of size 8 × 8, while the size of the input is 32 × 32. As shown in Table <ref type="table" target="#tab_1">2</ref>, the accuracy of the poisoned network is 90.79% which is identical to the clean model's accuracy of 90.34%-hence a successfully poisoned model. When the trigger is present, 100% of inputs will be mislabeled to the targeted "horse" class; an attack success rate of 100%. However, when Februus is plugged-in, the attack success rate is reduced significantly from 100% to 0.25%, while the performance on sanitized inputs is 90.08% -identical to the benign network of 90.34% (Table <ref type="table" target="#tab_1">2</ref>). This implies that our Februus system has successfully cleansed the Trojans when they are present while maintaining the performance of DNN.</p><p>Attacks against German Traffic Sign Recognition (GTSRB). In Table <ref type="table" target="#tab_1">2</ref>, the attack success rate of the trigger, post-it note shown in Figure <ref type="figure" target="#fig_6">7</ref>, to the target class "speedlimit" is 100%, after employing our Februus system, the attack success rate is significantly reduced to 0%. The accuracy for cleaned inputs after Februus is 96.64% which is very close to the benign model accuracy of 96.60%.</p><p>Attacks against Belgium Traffic Sign Recognition (BTSR). In this experiment, a trigger sticker size of 32 × 32 was placed in the middle of the traffic sign (Figure <ref type="figure" target="#fig_6">7</ref>). We utilize a popular network structure ResNet18 <ref type="bibr" target="#b19">[21]</ref> to validate our Februus method. Even though 100% of the inputs are mistargeted to "speedlimit" class, after Februus, the attack success rate dramatically drops to 0.12%. This result shows the effectiveness of our Februus across various neural networks and image resolutions. The accuracy after Februus is 96.98%, a result slightly above that of the clean model (96.63%).</p><p>Attacks against Face Recognition (VGGFace2). The result in Table <ref type="table" target="#tab_1">2</ref> shows the robustness of our method even with a large network and high-resolution images-typical of modern visual classification tasks. The Trojan attack success rate is dramatically reduced from 100% to 0.00% , while the classification accuracy is only 0.1% different from the performance of the clean model.</p><p>In summary these results demonstrate the robustness of our Februus defense against Trojan attacks across various networks, classification tasks and datasets with different input resolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ROBUSTNESS AGAINST BENIGN INPUTS</head><p>The robustness against Trojaned inputs will become less significant if the defender needs to sacrifice the performance of the network to benign inputs. Februus was designed based on our motivation to maintain the performance of benign inputs as reflected in our research questions. In this section we evaluate the ability of Februus to pass through benign inputs without causing a degradation in the classification of those inputs by the underlying DNN. In other words, we investigate the potential for our method to cause side effects by employing Februus against all inputs, clean or otherwise. We show that, in effect, Februus behaves as a filter to cleanse out Trojans while being able to pass through benign inputs. We describe the performance of our DNNs when using Februus for benign inputs and report the results in Table <ref type="table" target="#tab_2">3</ref>. An illustration of Februus on benign inputs is shown in Figure <ref type="figure" target="#fig_5">6</ref>. As shown in the Figure <ref type="figure" target="#fig_5">6</ref> and Table <ref type="table" target="#tab_2">3</ref>, the benign inputs are unaffected under Februus-we can only observe small variations in performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ROBUSTNESS AGAINST COMPLEX ADAPTIVE ATTACKS</head><p>The previous Sections have evaluated Februus against our threat model reasoned from related defense papers in the field; recall the threat-an input-agnostic attack from a single trigger misleading any input to one targeted label. Now, we consider potential adaptive attacks including advanced backdoor variants identified from NeuralCleanse <ref type="bibr" target="#b45">[46]</ref>-see Section 7.1-and those specific to Februuspotential methods of manipulating the defense pipeline by an attacker with full knowledge of our defense method (in Section 7.2 and Section 7.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Advanced Backdoor Attack Variants</head><p>We evaluate our Februus defense against four types of advanced backdoor attacks.</p><p>• Different triggers for the same targeted label. An attacker uses different triggers but target the same label (Figure <ref type="figure" target="#fig_8">8</ref>). Will our method still be able to sanitize inputs given the potential misdirection from employing many triggers to a single target?</p><p>• Different triggers for different targeted labels. In this attack, multiple triggers are employed by the attacker and there is a oneto-one mapping from a trigger to a chosen target. Notably, it was shown in <ref type="bibr" target="#b18">[20]</ref> to be able to fool TABOR <ref type="bibr" target="#b18">[20]</ref> and Neural Cleanse <ref type="bibr" target="#b45">[46]</ref>. Can Februus sanitize inputs under this adaptive attack?</p><p>• Source-label-specific (Partial) Backdoors. Februus focuses on input-agnostic attacks. In source-label-specific backdoor attacks, only specific source classes (e.g. specific persons in a face recognition task) can activate the backdoor with the trigger to the targeted label <ref type="bibr" target="#b45">[46]</ref>; notably, at present, there is no effective defense against this attack and, to the best of our knowledge, we are the first to quantitatively examine a partial backdoor attack and a defense.</p><p>• Changing the location of the trigger. The previous defense method in <ref type="bibr" target="#b45">[46]</ref> was shown to be sensitive to the location of the trigger <ref type="bibr" target="#b18">[20]</ref>. Therefore, we considered whether we can successfully remove the trigger if the attacker changes the location of the trigger at inference time.</p><p>We select the face recognition task, the most complex task in our study, for the experiments and summarize our results in Table <ref type="table" target="#tab_3">4</ref>. The results show the robustness of Februus against advanced backdoors; in particular, we provide the first result for a defense against partial backdoor attacks.    <ref type="table" target="#tab_3">4</ref>).</p><p>Different triggers for different targeted labels. In this adaptive attack targeting an input-agnostic defense, we evaluate an attack setting where an adversary poisons a network with different Trojan triggers targeting different labels. This scenario, in general, is an adaptive attack against other defense methods; notably, it was shown in <ref type="bibr" target="#b18">[20]</ref> to be able to fool TABOR <ref type="bibr" target="#b18">[20]</ref> and Neural Cleanse <ref type="bibr" target="#b45">[46]</ref>.</p><p>As shown in Table <ref type="table" target="#tab_3">4</ref>, our experimental evaluation has demonstrated that regardless of the trigger that attackers use and the label the attack targets, our method can still correctly remove and cleanse the trigger out of the input and successfully restore the input. The average attack success rate for all those triggers are only 0.04%, while the average accuracy is maintained at 91.80%. We observe that the attack success rate after employing Februus increases slightly compared to the previous experiment-Section 7.1-as this attack has shown to be more challenging to defend against <ref type="bibr" target="#b18">[20]</ref>. Nevertheless, sanitization success is high across both attacks.</p><p>Source-label-specific (Partial) Trojan. Source-label-specific or Partial Trojan was first highlighted in Neural Cleanse <ref type="bibr" target="#b45">[46]</ref> and we provide a a first quantitative evaluation and defense for a partial backdoor attack. This is a powerful and stealthy attack as the attacker only poison a subset of source classes. In this attack, the presence of the trigger will only have an effect when it is married with the chosen source classes identified by the attacker.</p><p>To build a partial backdoor, we poison a subset of 50 randomly chosen labels out of 170 labels in the Face Recognition task and provide the results of our evaluation in Table <ref type="table" target="#tab_3">4</ref>. Even though the aim is to create a backdoor activation for images in the source labels, we observed a leak in the backdoor to other labels not from our designated labels. We observed an attack success rate of up to 17.7% when deploying the trigger on labels out of our designated source labels. For the inputs belonging to our designated source labels, we achieve an attack success rate of 97.95%. Even with this powerful attack, our defense has been shown to be effective in just a single run through Februus where the attack success rate is reduced from 97.95% to 15.24%. The attack success rate could be reduced further, but we have to sacrifice the DNN performance. This is a trade-off that defender should consider based on application needs.</p><p>While Februus cannot completely neutralize Trojan effects in this powerful attack, Februus is the first defense to minimize the effectiveness of this attack to approximately 15% without scarifying classification accuracy in just a single run. Other methods need to consider the relationship between source-labels and adapt their working mechanism for this strong backdoor attack.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Attacks Targeting Trojan Removal</head><p>We investigate adaptive attackers attempting to exploit the working knowledge of GradCAM during the classification model poisoning process to bypass this component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adaptive Trojan Training Attack:</head><p>Since Februus relies on the selection of a sensitivity parameter to determine the region to sanitize, an adaptive attacker may try to manipulate this parameter selected by the defender to attempt a GradCAM evading Trojaning approach. Particularly, adaptive attackers might attempt to incorporate the working knowledge of GradCAM within the training process to mislead Februus; we describe the formulation for such an attack and its effectiveness.</p><p>An attacker can augment the original objective (binary cross entropy loss) used for classification with a new objective to minimize the score of GradCAM for Trojaned inputs. Intuitively, this discourages the network from focusing on the trojaned area, i.e.</p><formula xml:id="formula_10">min θ 1 n n i =1 ℓ(f θ (x i ), y i ) Classification Loss + γ B(x i ) L c GradCAM (x i ) 2 GradCAM Evasion Eq.<label>(3)</label></formula><p>,</p><p>where B(x i ) is 1 when x i ∈ S poisoned and 0 otherwise. Here, γ is the hyper-parameter that weights the classification loss and the GradCAM loss. The results for the traffic sign recognition task using the BTSR dataset are illustrated in Figure <ref type="figure" target="#fig_13">11</ref> where weak penalization denotes the Trojan models trained with a small γ in Eq. ( <ref type="formula" target="#formula_11">8</ref>) and strong penalization denotes models trained with a large γ . We make the following observations from our results in Figure <ref type="figure" target="#fig_13">11</ref>:</p><p>Observation 1. Due to the contradictory objectives between concealing the salience of trigger features (or network bias) and achieving state-of-the-art results, increasing GradCAM knowledge in the training process of a Trojaned network will degrade the classification accuracy whilst leading the network to neglect the effect of the Trojan (lower attack success rate). Achieving optimality in both objectives will lead to degrading both the attack success (ASR) and model performance (Accuracy). Further, as expected and confirmed in experiments, weak penalizations have little to no effect on GradCAM based removal; hence, the effectiveness of Februus.</p><p>Observation 2.The average probability of predictions we obtained from the adaptively Trojaned networks-that is 1 n n i=1 p(y = c |x i ) where c is the predicted label and given as Confidence in Figure <ref type="figure" target="#fig_13">11</ref>)reduced significantly to below 20% as we increased γ (i.e increasing the contribution of the GradCAM loss term in (8)). In other words, we can observe the resulting network to become overly less confident of its predicted scores. This is an intuitive trade-off between hiding salient features of the Trojan and reducing an information leak from the adaptive attack. Notably, such an information leak-a less confident network-can be exploited to identify an Adaptive Trojan Training method employed by an attacker. Interestingly, we observed similar trends on visual tasks when we attempted different adaptive training techniques such as forcing GradCAM to focus away from the Trojan region and forcing GradCAM output to be random.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GradCAM Evasion Attack (Input Perturbations).</head><p>We consider an attacker attempting to fool GradCAM at inference time. Theoretically, GradCAM can be fooled by perturbing the input with the objective of misleading the GradCAM selected input region, similar to that possible with an adversarial example <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b50">51]</ref>. Although this is out of our threat model for a Trojan attack where attackers utilize input-agnostic, realistic, natural triggers such as a tattoo, we conducted experiments to assess this threat. The results are discussed in Appendix A and illustrated in Figure <ref type="figure" target="#fig_14">10</ref>. Interestingly, we observed that adding large-magnitude adversarial noise, while potentially misleading GradCAM, has the adverse effect of causing the Trojaned classifier to neglect the trigger, hence reducing the attack success rate.</p><p>GradCAM Evasion Attack (Trigger Perturbations). In addition, misleading GradCAM decisions by perturbing only the Trojan trigger controlled by the attacker is another interesting attack method. Stanford researchers in a study <ref type="bibr" target="#b11">[13]</ref> have shown that localized patch perturbations only result in GradCAM focusing on the location of the trigger; thus, the possibility to mislead GradCAM by only perturbing the trigger whilst maintaining the potency of the Trojan remains an open challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Trojan Attack: successful</head><p>Trojan Attack: successful Trojan Attack: successful Trojan Attack: failed  Increasing the trigger size. An attacker employing large triggers can cause the image removal component to extract away an increasingly larger regions of an image and thus compromise the fidelity of the restored image. The sensitivity of Februus to a larger trigger is illustrated in Figure <ref type="figure" target="#fig_16">12</ref>. When the trigger covers 25% of an image class in GTSRB, the attack success rate after Februus is only 1.93%, while it is 0% for smaller triggers. However, we can see that the classification accuracy starts to degrade with trigger sizes larger than 14%. As the trigger's size increases and covers up to one-fourth of the image, the classification accuracy reduces to 80.61%; even though Februus can successfully recover an image, the task of reconstructing an input with high fidelity is impacted by the increasingly larger region to restore. We observed similar trends in other visual tasks. Multiple-piece trigger. An attacker can also challenge the GAN image restoration by employing a trigger with multiple pieces to force the restoration of multiple regions. With no assumptions regarding the size or the location of the Trojan during the construction of the GAN-recall that we used randomized locations and masked areas-we expect Februus to be highly generalizable to restoring multiple regions of arbitrary sizes.</p><p>As shown in Table <ref type="table" target="#tab_3">4</ref>, Februus correctly identifies and eliminates all the triggers with the attack success rate reducing from 100% to 0.32% whilst maintaining a classification accuracy of 91.42% for cleaned Trojaned inputs and 91.36% for benign inputs-we illustrate a two-piece trigger example in Figure <ref type="figure" target="#fig_17">13</ref>. MLaaS. Recent works <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b10">12,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31]</ref> have shown that attackers can embed backdoors to a ML system by poisoning the training sets with malicious samples at the training phase. While Gu et al. <ref type="bibr" target="#b16">[18]</ref> assume that the attacker has full control over training where a Trojan can be of any shape or size, Chen et al. <ref type="bibr" target="#b10">[12]</ref> propose an attack under a more challenging assumption where the attacker can only poison a small portion of the training set. Liu et al. <ref type="bibr" target="#b30">[31]</ref> show that they do not require the training dataset at all to Trojan a neural network and create a stealthy Trojan attack which targets dedicated neurons instead of poisoning the whole network. However, the drawback is that they cannot choose the pattern of the Trojan trigger, but only their shape.</p><p>In addition, attempts to make a Trojan attack more stealthy, Liu et al. <ref type="bibr" target="#b31">[32]</ref> presented a backdoor attack using reflections. Saha et al. <ref type="bibr" target="#b36">[37]</ref> propose a novel approach to create a backdoor by generating natural looking poisoned data with the correct ground truth labels. On the other hand, Bagdasaryan el al. <ref type="bibr" target="#b3">[5]</ref> propose a new method for injecting backdoors by poisoning the loss computation in the training code and name the method blind backdoors since the attacker has no power to modify the training data, observe the execution of the code or the resulting models.</p><p>Defenses. Since the attack scenarios were discovered, there has been a surge of interest in defenses against Trojan attacks <ref type="bibr">[9, 13-15, 20, 29, 33, 46]</ref>, and some certified robustness against backdoor attacks are proposed in <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b51">52]</ref>. Liu et al. <ref type="bibr" target="#b32">[33]</ref> proposed three methods to eliminate backdoor attacks and were evaluated on the simple MNIST dataset <ref type="bibr" target="#b24">[25]</ref>. Chen et al. <ref type="bibr" target="#b7">[9]</ref> proposed an Activation Clustering (AC) method to detect whether the training data has been poisoned. This method assumes access to Trojaned inputs. Liu et al. <ref type="bibr" target="#b28">[29]</ref> developed a method named Fine-Pruning to disable backdoors by pruning DNNs and then fine-tuning the pruned network. Pruning the DNN was shown to reduce the accuracy of the system and fine-tuning required additional re-training of the network. In CCS'2019, Liu et al. proposed Artificial Brain Stimulation (ABS) <ref type="bibr" target="#b29">[30]</ref> to determine whether a network is Trojaned. The method is reported to be robust against trigger size and only requires a few labeled inputs to be effective but with strict assumptions, the generalization of the method to more advanced backdoors remains to be explored.</p><p>Chou et al. <ref type="bibr" target="#b11">[13]</ref> and Gao et al. <ref type="bibr" target="#b13">[15]</ref> have proposed run-time Trojan anomaly detection methods named SentiNet and STRIP, respectively. SentiNet also utilized the GradCAM Visual Explanation tool <ref type="bibr" target="#b38">[39]</ref> to understand the predictions of the DNN and detect a Trojan trigger. SentiNet also demonstrated GradCAM to be robust in identifying adversarial regions regardless of whether it is a Trojaned trigger or an adversarial patch. Gao et al. <ref type="bibr" target="#b13">[15]</ref> propose a backdoor anomaly detection method that can detect potential malicious inputs at run-time, which can be applied to different domains <ref type="bibr" target="#b12">[14]</ref>. Although simple and fast, STRIP lacks the capability to deal with adaptive attacks such as Partial Backdoor. Both of these methods focus only on Trojan detection.</p><p>In SP'2019, Wang et al. <ref type="bibr" target="#b45">[46]</ref> proposed Neural Cleanse, a novel idea aiming to reverse the Trojan triggers and clean a DNN and its method is further improved in <ref type="bibr" target="#b18">[20]</ref>. Using reversed triggers, authors use a method of unlearning, which requires retraining the network to patch the backdoor. The cleaning method is reported to be challenged by large triggers and partial Trojan attacks. The idea of reversing the Trojan trigger was also proposed in DeepInspect (DI) <ref type="bibr" target="#b9">[11]</ref> but the reported results therein, after patching the network, appear to be less favorable than Neural Cleanse.</p><p>We provide a comparison of Februus with recent defense methods in Appendix C and summarize our findings in Table <ref type="table" target="#tab_7">10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Run-time Overhead Comparisons</head><p>Since Februus is plugged as an overhead to an existing DNN to sanitize Trojan inputs, the run-time of the Februus system should be evaluated. As shown in Table <ref type="table" target="#tab_5">5</ref>, the run-time of the entire pipeline only takes 29.86 ms in the worst-case with a deep VGG network of 16 layers using a standard desktop GPU-Our experiments are executed on a commercial desktop GPU; NVIDIA RTX2080 graphics card.</p><p>In simpler classification tasks, the overhead is only around 6 ms or 8 ms. This result is around 800× faster than SentiNet <ref type="bibr" target="#b11">[13]</ref> which takes around 23.3s for the same task and is comparable with the and simple Trojan detection only method in STRIP <ref type="bibr" target="#b13">[15]</ref>. More importantly, the acceptable latency for autonomous driving systems from Google, Uber or Tesla is around 100ms <ref type="bibr" target="#b27">[28]</ref>. Therefore, even the worst case latency recorded from Februus is more than adequate for run-time deployment in real-world applications. In addition, even though the camera resolution could be high, the detected images are normally are captured and cropped from a long distance to make timely decisions (see Figure <ref type="figure" target="#fig_13">11</ref> in <ref type="bibr" target="#b25">[26]</ref>). For example, in a real-world Traffic sign detection and recognition system <ref type="bibr" target="#b25">[26]</ref>, the captured size for Traffic signs ranges from 13 to 250 pixels. Notably, images of these sizes were investigated in our experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Limitations</head><p>We quantitatively and qualitatively compare Februus with other state-of-the-art defense methods in Appendix C. Februus is robust against input-agnostic Trojan attacks-our primary aim under our threat model-whilst generalizing well across complex adaptive attacks, we observed some limitations. Interestingly, in Section 7.2, our investigations into adaptive training methods demonstrate a possibility to evade Trojan removal but we observed this to come at the cost of further information leaks or significantly degraded attack success rates.</p><p>As demonstrated in Section 7.3, a large trigger covering more than one-fourth of an image can cause a degradation in the classification accuracy by attacking the image restoration stage of Februus; although, Februus can successfully block the attack.</p><p>In general, a large trigger is conspicuous, not stealthy and easily detected by humans when deployed in a scene in the physical world. For example, we illustrate in Figure <ref type="figure" target="#fig_16">12</ref> the trigger required to achieve a digitization of an image with a trigger size covering 25% of the image; hence such attacks are extremely difficult to mount.</p><p>Further, large triggers are a challenging problem and cause a degradation in state-of-the-art Trojan defense methods. However, in comparison with 2019 IEEE S&amp;P Neural Cleanse method <ref type="bibr" target="#b45">[46]</ref>, Februus is demonstrated to be less sensitive to these larger triggers as shown in Figure <ref type="figure" target="#fig_16">12</ref> and compared to in Table <ref type="table" target="#tab_7">10</ref> (in the Appendix C). Februus can be improved by enhancing the image restoration module, for example, by using more unlabeled data to increase the fidelity of the reconstruction by the GAN or training the GAN with labeled data with the additional objective of maximizing the classification accuracy of the classifier on inpainted images to boost the performance of the GAN to maintain the classification accuracy of restored inputs. In addition, as we illustrated in Section 7.3, mounting such a large trigger attack in the physical world is a challenging proposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSION</head><p>The Februus has constructively turned the strength of the inputagnostic Trojan attacks into a weakness. This allows us to remove the Trojan via the bias of network decision and cleanse the Trojan effects out of malicious inputs at run-time without the prior knowledge of poisoned networks and the Trojan triggers. Extensive experiments on various classification tasks have shown the robustness of our method to defend against input-agnostic backdoor attacks as well as advanced variants of backdoor and adaptive attacks.</p><p>Overall, in contrast to prior works, Februus is the first method to leverage cheaply available unlabeled data and cleanse out the Trojaned triggers from malicious inputs and patch the performance of a poisoned DNN without retraining. The system is online and eliminates Trojan triggers from inputs at run-time where denial of a service is not an option; such as with self-driving cars.</p><p>Future work should investigate the generality of the concept we first propose and demonstrate here-input sanitization-to other domains such as speech and text. Further, whilst GradCAM and the GAN based components we employed are only one set of methods to achieve input sanitization, alternatives may prove more robust, effective or impose even a smaller run-time overhead.  Generate a mask M c for x with an arbitrary mask at randomized region and shape. Get the inpainted sample x ∼ P д based on the masked input G(x, M c ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>Update the discriminators D with the joint loss gradients (Eq. 4) using a batch of real data x and inpainted data x. Generate a mask M c for x with an arbitrary mask at randomized region and shape. Update the Generator G with the joint loss gradients (Eq. 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C COMPARISON WITH STATE-OF-THE-ART METHODS</head><p>We compare ours with recently published state-of-the-art defense methods in the literature as summarized in Table <ref type="table" target="#tab_7">10</ref>. DeepInspect <ref type="bibr" target="#b9">[11]</ref>, Fine-pruning <ref type="bibr" target="#b28">[29]</ref>, ABS <ref type="bibr" target="#b29">[30]</ref> and Neural Cleanse <ref type="bibr" target="#b45">[46]</ref> work offline, i.e. they will perform Trojan detection in the network and patch it when it is not actively used; in contrast, Februus is online, removes and patches the inputs at run-time.</p><p>STRIP <ref type="bibr" target="#b13">[15]</ref>, akin to our approach, works in the input domain and at run-time. However, there are some differences in our method compared with theirs. The first and obvious difference is that this method only detects potential Trojans, while our method cleans the inputs. Hence, our cleaning method results should be compared with network patching results in Neural Cleanse <ref type="bibr" target="#b45">[46]</ref>, or DeepInspect <ref type="bibr" target="#b9">[11]</ref> defenses since these methods also attempt to clean the Trojaned effect whilst aiming to achieve state-of-the-art performance from the sanitized network. The second difference is that our GAN inpainting method is unsupervised, hence, we can utilize a huge amount of cheap unlabeled data to improve our defense, while other methods rely on ground-truth labeled data-both difficult and expensive to obtain. Third, our method is robust to Partial Trojan attacks and multiple triggers, two challenging attacks for our counterparts <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b45">46]</ref>. Notably, Februus can cleanse the Trojan effects in just a single run (or pass). Can block the Trojan effect with large trigger size of 16 × 16 (cover 25% of the picture). 1 The comparison is on the GTSRB dataset shared by all methods in respective experimental evaluations. Notably, the classification accuracy of the methods we compare with are after the model is re-trained using clean labeled data. 2 The methods that discuss potential defenses require adapting their defense mechanisms and knowledge of trojaning implementations; notably, such information may be difficult to gain in practice.</p><p>Figure <ref type="figure" target="#fig_3">15</ref>: Robustness of Februus on Different Classification Tasks. Februus is highly effective and perform consistently well against backdoor attacks. We can observe attack success rate reductions from 100% to nearly 0% while the classification accuracy is maintained in across the three different classification tasks. Notably, model performance after deploying Februus remains similar to that obtained from benign inputs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of the Februus System. The Trojaned input is processed through the Trojan Removal module that inspects and surgically removes the trigger. Subsequently, the damaged input is processed by the Image Restoration module to recover the damaged regions. The restored image is fed into the Trojaned DNN. TOP: Without Februus, the Trojaned input will trigger the backdoor and be misclassified as a 100 km/h SPEED LIMIT sign. BOTTOM: With Februus deployed, the Trojaned DNN still correctly classifies the Trojaned input as a STOP sign.</figDesc><graphic coords="3,104.24,83.68,403.67,126.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The distribution of deeply learned features of a Benign and Trojaned model (the plots are obtained from CIFAR10 using t-SNE [44] applied to the outputs of the last fully connected layer).</figDesc><graphic coords="4,370.05,138.41,132.80,166.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The training process of our generative adversarial network (GAN) for image restoration. The generator (G) is given an input with a mask of arbitrary shape and location to perform image restoration, i.e. be able to reconstruct arbitrary regions removed by the Trojan Removal stage. The discriminator (D l oc al and D дl obal ) is given the instance of the restored image and the real one to compare. Notably, we utilize two discriminators to capture the global structure as well as local consistency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Trojan information leaked is detected by the visual explanation tool GradCAM [39]. Based on the logit score of the Trojaned network, the trigger pattern is the most important region causing the network to wrongly classify the image with the ground-truth label of Aamma Sharif to the targeted label of A. Fine Frenzy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>, each with its own loss; i) the global consistency discriminator D globalwith its corresponding loss L global D -to capture the global structure; and ii) local fidelity discriminator D local -with its corresponding loss L local D -for local consistency of the image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Image Restoration. Visualization of Trojaned and benign inputs through Februus on different visual classification tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Trojan triggers (first row) and their deployment used in our experiments (second row). From left to right: the flower and Post-it note triggers (used in [18]) deployed in CIFAR10, BTSR and GTSRB tasks respectively, country flag lapels on shirts and the tattoo on the face are deployed in the VGGFace2 task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Different triggers for the same targeted label. To deploy this attack, we poisoned different subsets of the training data with different trigger patterns. Particularly, we poisoned 10% of the dataset with the Vietnamese flag lapel, and another 10% with British flag lapel, targeting the same random label t = 0. As illustrated in Figure8, a person wearing either of the flag lapel triggers can impersonate the targeted class. As shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Different triggers for the same targeted label. An attacker can use either trigger patterns (flag lapels) to impersonate the target person of interest (results are in Table4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Changing the location of the trigger. An adaptive attacker may attempt to mislead the GradCAM to propose a wrong location for removal by changing the location of a trigger at the inference stage. Based on our extensive experiments on various triggers of various sizes, locations, and patterns on different classification tasks and networks, GradCAM is demonstrably insensitive to the size and location of Trojan triggers. We illustrate examples of successful Trojan removal from our model zoo of Trojan attacks in Figure 9.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>(a) A flag lapel on the right of a T-shirt in VGGFace2 (b) A flag lapel on the left of a T-shirt in VGGFace2 (c) A sticker in the middle of a Traffic sign in BTSR (d) A star tatoo on a face in VGGFace2 (e) Two flag lapels on both sides of a T-shirt in VGGFace2 (f) A flower at the lower right corner of the scene in CIFAR10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Trojan attacks with varying trigger locations are successfully removed by Februus. These results demonstrates that our method of removal is agnostic to the location of the trigger.</figDesc><graphic coords="9,442.41,494.54,51.46,51.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Further, we</head><label></label><figDesc>consider manipulation attacks by an adaptive attacker Targeting Trojan Removal in Section 7.2 and attacks Targeting Image Restoration in Section 7.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Classification Accuracy, Attack Success Rate (ASR) and Confidence denoted by the prediction scores of the DNN models built with adaptive Trojaning for different penalizations (γ ).</figDesc><graphic coords="10,53.80,342.78,240.25,90.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Adversarial examples of Trojaned images to fool Gradcam. Notably, when the perturbation is large (ϵ &gt; 0.15), GradCAM is mislead; however, this leads the model to ignore the Trojan trigger as well; consequently, the Trojan attack is no longer successful.</figDesc><graphic coords="10,323.45,605.29,175.03,58.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>7. 3</head><label>3</label><figDesc>Attacks Targeting Image RestorationAssuming that adaptive attackers are fully aware of the Image Restoration mechanism of Februus albeit without access to manipulate the training process of the Image Restoration module, a strong attack against the restoration is to embed a large or multiple-piece trigger to force an arbitrarily large removal region through Image Restoration and to challenge the recovery during Image Restoration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Februus applied to the infected GTSRB model whilst increasing the size of the Post-it trigger and illustrations of large triggers occluding 25% of the input images.</figDesc><graphic coords="11,53.80,334.86,240.37,110.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>8Figure 13 :</head><label>13</label><figDesc>Figure 13: Multiple-piece trigger targeting a single label.</figDesc><graphic coords="11,504.17,98.40,54.01,54.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Algorithm 1 4 :</head><label>14</label><figDesc>First 5 epochs are trained using clean data only.GAN training algorithm. In this section, we discuss further details of the training algorithm for Generative Adversarial Network mentioned in Section 3. The details are mentioned in Alg. 1. Training procedure for the image inpainting GAN network (with generator parameters θ ).Require:The gradient penalty coefficient λ, Adam optimizer hyper-parameters α, β 1 , β 2 , the number of discriminator iterations per generator iteration n critic , the batch size m, and the regularization hyperparameter of Generator loss γ . 1: while θ has not converged do 2: for t = 1, ..., n critic do 3: for i = 1, ..., m do Sample real data x ∼ P r 5:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>6 :</head><label>6</label><figDesc>Generate a masked input G(x, M c ) 7:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>9 :</head><label>9</label><figDesc>Sample a batch of real data x ∼ P r 10:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>11 :</head><label>11</label><figDesc>Generate masked data G(x, M c )12:Get the inpainted samples x ∼ P д based on the masked inputs G(x, M c ) 13:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="16,53.80,356.55,504.48,118.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Networks used for the classification tasks</figDesc><table><row><cell>Task/Dataset</cell><cell># of Labels</cell><cell># of Training Images</cell><cell># of Testing Images</cell><cell>Model Architecture</cell></row><row><cell>CIFAR10[24]</cell><cell>10</cell><cell>50,000</cell><cell>10,000</cell><cell>6 Conv + 2 Dense</cell></row><row><cell>GTSRB[41]</cell><cell>43</cell><cell>35,288</cell><cell>12,630</cell><cell>7 Conv + 2 Dense</cell></row><row><cell>BTSR[35]</cell><cell>62</cell><cell>4,591</cell><cell>2,534</cell><cell>ResNet18</cell></row><row><cell>VGGFace2[8]</cell><cell>170</cell><cell>48,498</cell><cell>12,322</cell><cell>13 Conv + 3 Dense (VGG-16)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Classification accuracy and attack success rate before and after Februus on Trojan models on various classification tasks.</figDesc><table><row><cell>Task/Dataset</cell><cell>Benign Model</cell><cell cols="2">Trojaned Model (Before Februus)</cell><cell cols="2">Trojaned Model (After Februus)</cell></row><row><cell></cell><cell>Classification</cell><cell>Classification</cell><cell>Attack</cell><cell>Classification</cell><cell>Attack</cell></row><row><cell></cell><cell>Accuracy</cell><cell>Accuracy</cell><cell>Success Rate</cell><cell>Accuracy</cell><cell>Success Rate</cell></row><row><cell>CIFAR10</cell><cell>90.34%</cell><cell>90.79%</cell><cell>100%</cell><cell>90.08%</cell><cell>0.25%</cell></row><row><cell>GTSRB</cell><cell>96.6%</cell><cell>96.78%</cell><cell>100%</cell><cell>96.64%</cell><cell>0.00%</cell></row><row><cell>BTSR</cell><cell>96.63%</cell><cell>97.04%</cell><cell>100%</cell><cell>96.98%</cell><cell>0.12%</cell></row><row><cell>VGGFace2</cell><cell>91.84%</cell><cell>91.86%</cell><cell>100%</cell><cell>91.78%</cell><cell>0.00%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Robustness of Februus against benign inputs in the classification tasks. Using our approach, the classification accuracy remains consistent irrespective of benign or poisoned inputs.</figDesc><table><row><cell>Tasks/</cell><cell cols="3">Classification Accuracy on Trojaned Model</cell></row><row><cell>Datasets</cell><cell>Before Februus</cell><cell cols="2">After Februus</cell></row><row><cell></cell><cell>Benign Inputs</cell><cell cols="2">Benign Inputs Trojaned Inputs</cell></row><row><cell>CIFAR10</cell><cell>90.79%</cell><cell>90.18%</cell><cell>90.08%</cell></row><row><cell>GTSRB</cell><cell>96.78%</cell><cell>95.13%</cell><cell>96.64%</cell></row><row><cell>BTSR</cell><cell>97.04%</cell><cell>95.60%</cell><cell>96.98%</cell></row><row><cell>VGGFace2</cell><cell>91.86%</cell><cell>91.79%</cell><cell>91.78%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 ,</head><label>4</label><figDesc>Februus is robust against such an attack.</figDesc><table><row><cell>Inputs</cell><cell>Trojan Removal</cell><cell>Image Restoration</cell><cell>Inputs</cell><cell>Trojan Removal</cell><cell>Image Restoration</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Robustness against various complex and adaptive Trojan attacks. Februus is robust against attacks with varying levels of complexity.</figDesc><table><row><cell>Complex Adaptive Attacks</cell><cell cols="2">Before Februus</cell><cell cols="2">After Februus (Trojaned Inputs)</cell><cell>After Febrrus (benign inputs)</cell></row><row><cell></cell><cell cols="4">Accuracy Attack Success Rate Classification Accuracy Attack Success Rate</cell><cell>Accuracy</cell></row><row><cell>Different triggers for the same targeted label (Section 7.1)</cell><cell>91.87%</cell><cell>100.00%</cell><cell>91.28%</cell><cell>0.01%</cell><cell>90.56%</cell></row><row><cell>Different triggers for different targeted labels (Section 7.1)</cell><cell>91.87%</cell><cell>100.00%</cell><cell>91.80%</cell><cell>0.04%</cell><cell>91.02%</cell></row><row><cell>Source-label specific (Partial) Trojan (Section 7.1)</cell><cell>90.72%</cell><cell>97.95%</cell><cell>83.61%</cell><cell>15.24%</cell><cell>89.60%</cell></row><row><cell>Multiple-piece triggers for a single targeted label (Section 7.3)</cell><cell>91.81%</cell><cell>100.00%</cell><cell>91.42%</cell><cell>0.32%</cell><cell>91.36%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Average run-time of different classification tasks on 100 images. Even with the high-resolution images of the Face Recognition task using a complex VGG-16 network, the total run-time of the Februus system is 29.86 ms, while the simpler scene classification task only incurs a 6.32 ms overhead.</figDesc><table><row><cell>Task/Dataset</cell><cell>Run-time Overhead</cell></row><row><cell>Scene Classification (CIFAR10)</cell><cell>6.32 ms</cell></row><row><cell>German Traffic Sign Recognition (GTSRB)</cell><cell>8.01 ms</cell></row><row><cell>Belgium Traffic Sign Recognition (BTSR)</cell><cell>6.49 ms</cell></row><row><cell>Face Recognition (VGGFace2)</cell><cell>29.86 ms</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 9 :</head><label>9</label><figDesc>Dataset and Training Configuration</figDesc><table><row><cell cols="2">Task/Dataset # of Labels</cell><cell>Input Size</cell><cell cols="2">Training Set Size Testing Set Size</cell><cell>Training Configuration</cell></row><row><cell>CIFAR-10</cell><cell>10</cell><cell>32 × 32 × 3</cell><cell>50,000</cell><cell>10,000</cell><cell>inject ratio=0.1, epochs=100, batch=32, optimizer=Adam, lr=0.001</cell></row><row><cell>GTSRB</cell><cell>43</cell><cell>32 × 32 × 3</cell><cell>35,288</cell><cell>12,630</cell><cell>inject ratio=0.1, epochs=25, batch=32, optimizer=Adam, lr=0.001</cell></row><row><cell>BTSR</cell><cell>62</cell><cell>224 × 224 × 3</cell><cell>4,591</cell><cell>2,534</cell><cell>inject ratio=0.1, epochs=25, batch=32, optimizer=Adam, lr=0.001</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>inject ratio=0.1, epochs=15, batch=32,</cell></row><row><cell>VGGFace2</cell><cell>170</cell><cell>224 × 224 × 3</cell><cell>48,498</cell><cell>12,322</cell><cell>optimizer=Adadelta, lr=0.001 First 10 layers are frozen during training.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 10 :</head><label>10</label><figDesc>Comparison between Februus and other Trojan defense methods</figDesc><table><row><cell>Work</cell><cell>Costly Labeled Data Required</cell><cell>Run-time</cell><cell>DNN Restoration Capability</cell><cell>Domain</cell><cell>Against Complex Partial Backdoor Attacks 2</cell><cell>Results After Restoration 1</cell></row><row><cell>SentiNet [13]</cell><cell>Yes</cell><cell>Yes</cell><cell>No</cell><cell>Input</cell><cell>Not Evaluated</cell><cell>Not Available</cell></row><row><cell>STRIP [15]</cell><cell>Yes</cell><cell>Yes</cell><cell>No</cell><cell>Input</cell><cell>Not Capable</cell><cell>Not Applicable</cell></row><row><cell>ABS [30]</cell><cell>Yes</cell><cell>No</cell><cell>No</cell><cell>Network</cell><cell>Not Capable</cell><cell>Not Applicable</cell></row><row><cell>DeepInspect [11]</cell><cell>No</cell><cell>No</cell><cell>Yes</cell><cell>Network</cell><cell>Not Quantitatively Evaluated</cell><cell>Attack Success: 3%, Classification Accuracy: 97.1%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Attack Success: 0.14%,</cell></row><row><cell>Neural Cleanse [46]</cell><cell>Yes</cell><cell>No</cell><cell>Yes</cell><cell>Network</cell><cell>Not Quantitatively Evaluated</cell><cell>Classification Accuracy: 92.91%, Cannot detect the trigger sizes</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>larger than 8 × 8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Attack Success: 0.00%,</cell></row><row><cell>Februus (Ours)</cell><cell>No</cell><cell>Yes</cell><cell>Yes</cell><cell>Input</cell><cell>(in just a single run) Yes</cell><cell>Classification Accuracy: 96.64%,</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We considered the Roman god Februus-the god of purification and the underworldas an apt name to describe our defense system architecture.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>For brevity we assume the output of each layer is a matrix.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A GRADCAM EVASION ATTACKS</head><p>Besides adding GradCAM knowledge during the training process, some adaptive attackers may attempt to mislead GradCAM to propose a wrong location at the inferencing stage, and thus reduce the robustness of our defense method. Based on our extensive experiments, GradCAM is shown to be insensitive to sizes and locations of Trojan triggers (as shown in Figure <ref type="figure">9</ref>).</p><p>Nevertheless, for an evasion attack at the inferecing stage, we assume an attacker is capable of adding noise to the input scene to be digitized by a camera to fool GradCAM and misdirect to a targed region of the input. Results from this experiment are shown in Figure <ref type="figure">14</ref>. Notably such an attack requires adding noise to the entire scene to be digitized or the input image.</p><p>We optimize an input using Stochastic Gradient Descent (SGD) to minimize the loss function calculated from the difference between the current and targeted GradCAM outputs until convergence. As shown in Figure <ref type="figure">14</ref>, an attacker may create a perturbation that can fool GradCAM to detect a designated region. Adaptive attackers might add this noise to the Trojaned input (with the hyperparameter of ϵ to alter the magnitude of the noise added) to mislead GradCAM and reduce the robustness of our method (as shown in Figure <ref type="figure">10</ref>). However, adding noise to the Trojaned inputs does not guarantee the ability of the Trojan to still trigger the DNN; further, this attack method is out of our threat model focusing on physically realizable Trojan triggers. We also recognize that a stealthy attacker may attempt to deploy perturbations within the Trojan trigger to create an adversarial trigger to attempt to fool GradCAM. However, researchers in Stanford <ref type="bibr" target="#b11">[13]</ref> showed the infeasibility of this method to fool GradCAM, unless an attacker is capable of perturbing the whole image as shown in Figure <ref type="figure">10</ref> and Figure <ref type="figure">14</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B DETAILED INFORMATION ON DATASETS, MODEL ARCHITECTURES AND TRAINING CONFIGURATIONS</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://www.gradientzoo.com/" />
		<title level="m">Gradientzoo: pre-trained neural network models</title>
		<imprint/>
	</monogr>
	<note>Amazon Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Medical image analysis using convolutional neural networks: a review</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adnan</forename><surname>Majid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Qayyum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Majdi</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><forename type="middle">Khurram</forename><surname>Alnowami</surname></persName>
		</author>
		<author>
			<persName><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Medical Systems</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><surname>Aro</surname></persName>
		</author>
		<ptr target="https://www.arl.army.mil/www/pages/8/TrojAI-V3.2.pdf" />
		<title level="m">BROAD AGENCY ANNOUNCEMENT FOR TrojAI</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Bagdasaryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Shmatikov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.03823[cs.CR]</idno>
		<title level="m">Blind Backdoors in Deep Learning Models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. 2020. How to backdoor federated learning</title>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Bagdasaryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><surname>Bvlc</surname></persName>
		</author>
		<ptr target="https://github.com/BVLC/caffe/wiki/Model-Zoo" />
		<title level="m">Caffe Model Zoo</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">VGGFace2: A dataset for recognising faces across pose and age</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Detecting Backdoor Attacks on Deep Neural Networks by Activation Clustering</title>
		<author>
			<persName><forename type="first">Bryant</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilka</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathalie</forename><surname>Baracaldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiko</forename><surname>Ludwig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Molloy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biplav</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence Safety Workshop at Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">DeepDriving: Learning Affordance for Direct Perception in Autonomous Driving</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kornhauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">DeepInspect: A Black-box Trojan Detection and Mitigation Framework for Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Huili</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jishen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farinaz</forename><surname>Koushanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kimberly</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05526[cs.CR]</idno>
		<title level="m">Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SentiNet: Detecting Physical Attacks Against Deep Learning Systems</title>
		<author>
			<persName><forename type="first">Edward</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giancarlo</forename><surname>Pellegrino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning and Security Workshop at IEEE Security and Privacy</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Design and Evaluation of a Multi-Domain Trojan Detection Method on Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Yansong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeonjae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bao</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Gia</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gongxuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Nepal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Damith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoungshick</forename><surname>Ranasinghe</surname></persName>
		</author>
		<author>
			<persName><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.10312[cs.CR]</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">STRIP: A Defence against Trojan Attacks on Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Yansong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Change</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiping</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damith</forename><forename type="middle">C</forename><surname>Ranasinghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Nepal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Computer Security Applications Conference (ACSAC)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generative Adversarial Nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Explaining and Harnessing Adversarial Examples</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">BadNets: Evaluating Backdooring Attacks on Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Dolan-Gavitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Garg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="47230" to="47244" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improved Training of Wasserstein GANs</title>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Wenbo</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.01763[cs.CR]</idno>
		<title level="m">TABOR: A Highly Accurate Approach to Inspecting and Restoring Trojan Backdoors in AI Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Globally and Locally Consistent Image Completion</title>
		<author>
			<persName><forename type="first">Satoshi</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edgar</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Ishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koh</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Model</forename><surname>Zoo</surname></persName>
		</author>
		<ptr target="https://modelzoo.co/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><surname>Burges</surname></persName>
		</author>
		<title level="m">MNIST handwritten digit database. ATT Labs</title>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Simultaneous Traffic Sign Detection and Boundary Estimation Using Convolutional Neural Network</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1652" to="1663" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Invisible Backdoor Attacks on Deep Neural Networks via Steganography and Regularization</title>
		<author>
			<persName><forename type="first">Shaofeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minhui</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinpeng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Dependable and Secure Computing</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The architectural implications of autonomous driving: Constraints and acceleration</title>
		<author>
			<persName><forename type="first">Shih-Chieh</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang-Hong</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Skach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingjia</forename><surname>Md E Haque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><surname>Mars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Special Interest Group on Programming Languages (SIGPLAN) Notices</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fine-pruning: Defending against backdooring attacks on deep neural networks</title>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Dolan-Gavitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Garg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Research in Attacks, Intrusions, and Defenses</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">ABS: Scanning Neural Networks for Back-doors by Artificial Brain Stimulation</title>
		<author>
			<persName><forename type="first">Yingqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Chuan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanhong</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqing</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yousra</forename><surname>Aafer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM conference on Computer and Communications Security (CCS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Trojaning Attack on Neural Networks</title>
		<author>
			<persName><forename type="first">Yingqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqing</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yousra</forename><surname>Aafer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Chuan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Network and Distributed System Security Symposium (NDSS)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Reflection Backdoor: A Natural Backdoor Attack on Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Yunfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Neural trojans</title>
		<author>
			<persName><forename type="first">Yuntao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Design (ICCD)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Towards Deep Learning Models Resistant to Adversarial Attacks</title>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Traffic sign recognition -How far are we from the solution?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep Face Recognition</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hidden Trigger Backdoor Attacks</title>
		<author>
			<persName><forename type="first">Aniruddha</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshayvarun</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Evaluating the Visualization of What a Deep Neural Network Has Learned</title>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2660" to="2673" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schlipsing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="323" to="332" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">DeepFace: Closing the Gap to Human-Level Performance in Face Verification</title>
		<author>
			<persName><forename type="first">Yaniv</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc'aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Visualizing Data using t-SNE</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008">2008. 2008</date>
			<pubPlace>Nov</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Jinyuan jia, and Neil Zhenqiang Gong</title>
		<author>
			<persName><forename type="first">Binghui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.11750[cs.CR]</idno>
	</analytic>
	<monogr>
		<title level="m">On Certifying Robustness against Backdoor Attacks via Randomized Smoothing</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks</title>
		<author>
			<persName><forename type="first">Bolun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanshun</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bimal</forename><surname>Viswanath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security and Privacy</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Adversary Resistant Deep Neural Networks with an Application to Malware Detection</title>
		<author>
			<persName><forename type="first">Qinglong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbo</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Ororbia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">I</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Lee</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Maurice</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bojan</forename><surname>Karlas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08904[cs.LG]</idno>
		<title level="m">RAB: Provable Robustness Against Backdoor Attacks</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Wierzynski</surname></persName>
		</author>
		<ptr target="https://www.intel.ai/the-challenges-and-opportunities-of-explainable-ai" />
		<title level="m">The Challenges and Opportunities of Explainable AI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Droid-sec: deep learning in android malware detection</title>
		<author>
			<persName><forename type="first">Zhenlong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongqiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yibo</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM conference on Applications, Technologies, Architectures, and Protocols for Computer Communication (SIGCOMM)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Interpretable Deep Learning under Fire</title>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ningfei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shouling</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiapu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Security Symposium</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Backdoor Attacks to Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Zaixi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyuan</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binghui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Zhenqiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gong</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11165[cs.CR]</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
