<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Traffic Sign Recognition Using Evolutionary Adaboost Detection and Forest-ECOC Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2009-02-27">February 27, 2009</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xavier</forename><surname>Baró</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Sergio</forename><surname>Escalera</surname></persName>
							<email>sescalera@cvc.uab.cat</email>
						</author>
						<author>
							<persName><forename type="first">Jordi</forename><surname>Vitrià</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Oriol</forename><surname>Pujol</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Petia</forename><surname>Radeva</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Center</orgName>
								<address>
									<addrLine>Campus Universitat Autònoma de Barcelona</addrLine>
									<postCode>08193</postCode>
									<settlement>Edifici O, Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Computer Vision Center</orgName>
								<orgName type="institution">Universitat Autònoma de Barcelona</orgName>
								<address>
									<addrLine>Campus</addrLine>
									<postCode>08193</postCode>
									<settlement>Edifici O, Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department Matemàtica Aplicada i Anàlisi</orgName>
								<orgName type="institution">Universitat de Barcelona</orgName>
								<address>
									<postCode>08007</postCode>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Traffic Sign Recognition Using Evolutionary Adaboost Detection and Forest-ECOC Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2009-02-27">February 27, 2009</date>
						</imprint>
					</monogr>
					<idno type="MD5">F2667C9314D119CCE246C82EF046017A</idno>
					<idno type="DOI">10.1109/TITS.2008.2011702</idno>
					<note type="submission">received October 18, 2007; revised April 11, 2008 and October 2, 2008. First</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Dissociated dipoles</term>
					<term>ensemble of dichotomizers</term>
					<term>Error-Correcting Output Code (ECOC)</term>
					<term>evolutionary boosting</term>
					<term>traffic sign recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The high variability of sign appearance in uncontrolled environments has made the detection and classification of road signs a challenging problem in computer vision. In this paper, we introduce a novel approach for the detection and classification of traffic signs. Detection is based on a boosted detectors cascade, trained with a novel evolutionary version of Adaboost, which allows the use of large feature spaces. Classification is defined as a multiclass categorization problem. A battery of classifiers is trained to split classes in an Error-Correcting Output Code (ECOC) framework. We propose an ECOC design through a forest of optimal tree structures that are embedded in the ECOC matrix. The novel system offers high performance and better accuracy than the state-of-the-art strategies and is potentially better in terms of noise, affine deformation, partial occlusions, and reduced illumination.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T RAFFIC sign recognition is being studied for several purposes such as autonomous driving and assisted driving <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. Recognition of traffic signs allows a driver to be warned about inappropriate actions and potentially dangerous situations. In the mobile mapping framework, traffic-signrecognition methods are used with other methods to compile road information and measure the position and orientation of different landmarks in movement either on an aerial or a terrestrial platform. One example of this system is given by Madeira et al. <ref type="bibr" target="#b2">[3]</ref>, where a mobile mapping system automatically processes traffic signs. In this work, a recognition accuracy of more than 80% on a reduced set of sign types is obtained. In <ref type="bibr" target="#b3">[4]</ref>, a vehicle-based vision platform is used to detect road signs, where the main goal is mainly focused on speed signs.</p><p>In the literature, we can find two main approaches to solve the problem of road sign recognition: 1) color based and 2) gray-scale based. Color-based sign recognition relies on color to reduce false-positive results in the recognition process <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b12">[13]</ref>, whereas the gray-scale method concentrates on the geometry of the object <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b16">[17]</ref>. Recent works use the combination of both cues to improve the detection rates. For instance, in <ref type="bibr" target="#b17">[18]</ref>, a threshold is applied over a hue, saturation, value representation of the image to find regions with a high probability of having a traffic sign. Many background objects can share colors with traffic signs; thus, heuristics over the size and aspect ratio are used to reduce the number of false-alarm regions. Once the regions are normalized to a predefined size, a linear support vector machine (SVM) is used to classify the region in one of the possible shapes (e.g., circle or triangle). The color and shape information is used as a coarse classification, and an SVM with Gaussian kernels is used to perform the fineclassification step. The color information is strongly related to the type of camera, illumination, and sign aging; thus, the use of color information introduces additional difficulties to recognition. In the work of de la Escalera et al. <ref type="bibr" target="#b18">[19]</ref>, these difficulties are addressed using an enhancement step before the use of thresholds on the color values. After applying size heuristics to remove nonsign regions, the authors use a fusion of color information, the gradient, and a distance image to remove regions with low probability of having a traffic sign. Final classification is performed through a neural network. Other recent works are focused on the final classification step. Paclik et al. <ref type="bibr" target="#b19">[20]</ref> propose a representation of road-sign data based on extending the traditional normalized cross-correlation approach to a similarity based on individual matches in a set of local image regions.</p><p>Traffic sign recognition is a straightforward application for object recognition algorithms in which the previous addressing of the category detection (e.g., object location) is often required. In recent years, one of the most accepted and widely used approaches in object detection has been the one proposed by Viola and Jones <ref type="bibr" target="#b20">[21]</ref>. Their approach is based on a cascade of detectors, where each detector is an ensemble of boosted classifiers based on the Haar-like features. Lienhart and Maydt <ref type="bibr" target="#b21">[22]</ref> presented an extension of the original Haar-like features set, demonstrating that Adaboost converges faster and with better results when the features set is large. On the other hand, due to the exhaustive search over the features set, the training time grows with respect to the number of features. This fact makes any approach that tries to extend the feature set unfeasible.</p><p>Once an object (i.e., a traffic sign) is located, it should be recognized from a wide set of possible classes using some kind of classification technique. Designing a machine-learning multiclass technique is a hard task. In this sense, it is common to conceive algorithms to distinguish between just two classes and combine them in some way. Following the multiclass categorization problem, where a set of classifiers should learn, in a natural way, the features shared between categories, the Error-Correcting Output Code (ECOC) technique was proposed with very interesting results <ref type="bibr" target="#b22">[23]</ref>. This technique is a very successful multiclass categorization tool due to its ability to share the classifiers knowledge among classes. Recently, the embedding of a tree structure in the ECOC framework has shown to be highly accurate with a very small number of binary classifiers <ref type="bibr" target="#b23">[24]</ref>. However, the ECOC design is still an open issue.</p><p>The goal of this paper is twofold. First, during detection, to discriminate a set of traffic signs from the background, we propose a novel binary classifier through an evolutionary version of Adaboost, which avoids the limitations of the original algorithm in terms of the dimensionality of the feature space. Second, to deal with the multiclass categorization problem to distinguish among a large set of classes, a multiclass learning technique is proposed. The approach is based on embedding a forest of optimal tree structures in the ECOC framework, allowing for sharing of features (e.g., tree nodes, base classifiers, and dichotomies) among classes in a very robust way. Finally, we develop a real traffic-sign-recognition system for a mobile mapping process <ref type="bibr" target="#b24">[25]</ref>, in which we validate the robustness of our approach. We show that the present strategy obtains high accuracy and outperforms the state-of-the-art strategies and is robust against a high variability of sign appearance.</p><p>This paper is organized as follows. Section II presents the novel detection and multiclass categorization approaches. Section III shows the architecture of the real traffic sign recognition system and integration details. Section IV shows the experimental results, and Section V concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. METHODOLOGY</head><p>In this section, we present the new evolutionary boosting strategy and the Forest-ECOC (F-ECOC) technique to deal with the object detection and classification stages, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Detection</head><p>Detection takes an image as input and gives, as the output, the regions that contain the candidate object. Our detector is inspired by detector presented by Viola and Jones <ref type="bibr" target="#b20">[21]</ref>. We consider the use of the attentional cascade concept, boosting as the feature selection strategy, and the representation of the image in terms of the integral image. With our approach, we solve the limitation of the exhaustive search of the boosting formulation in <ref type="bibr" target="#b20">[21]</ref>. Moreover, we present a solution to the restriction of the boosting-process computation when the feature set size is large. We propose an evolutionary vision of boosting, which not only drastically reduces the training time but also allows the use of huge feature sets. This way, we propose the use of the dissociated dipoles-a more general type of features than the Haar-like features <ref type="bibr" target="#b20">[21]</ref>-which can also be calculated using the integral image. In addition, we introduce another Fig. <ref type="figure">1</ref>. Dissociated dipoles. The black region corresponds to the inhibitory dipole, whereas the white region corresponds to the excitatory dipole <ref type="bibr" target="#b26">[27]</ref>.</p><p>variation in the Weak Classifier, changing the decision rule from the threshold value that was used in the original schema to its ordinal definition, where only the sign is considered. This representation has been demonstrated to be more robust in the case of noise and illumination changes <ref type="bibr" target="#b25">[26]</ref>.</p><p>1) Detection Architecture: Working with unbalanced problems like object detection, each time we analyze an image, the system must discard a large number of negative regions, whereas just a few or any negative region corresponds to the object that we are looking for. The attentional cascaded architecture allows for discarding easy nonobject regions at low computational cost, whereas more complex regions are deeply analyzed. One attentional cascade is composed of a set of classifiers or stages, where the input to each classifier corresponds to regions that are classified as object-regions by the previous stages. The regions classified as object-regions by the last stage are the output of the detector.</p><p>Although any learning method can be used to learn the cascade, the usefulness of Adaboost has widely been demonstrated. Before each stage training, a new set of samples is built using the positive samples and the false positives of the previous stages of the cascade. Then, a classifier is learned to achieve a given minimum hit and maximum false-alarm rates. This process is repeated until the desired number of stages or the target false-alarm rate are reached.</p><p>2) Dissociated Dipoles: In <ref type="bibr" target="#b21">[22]</ref>, Lienhart and Maydt show that the accuracy of the detector increases with the number of available features. Therefore, in this paper, we use the dissociated dipoles-a more general type of features than the Haar-like features-and, thus, deal with a larger feature set.</p><p>The dissociated dipoles or sticks have been presented by Balas and Sinha <ref type="bibr" target="#b26">[27]</ref>, and these sticks are features that are composed of a pair of rectangular regions, named the excitatory dipole and the inhibitory dipole, respectively (see Fig. <ref type="figure">1</ref>). The mean value of all the pixels in the inhibitory dipole is subtracted from the mean value of the excitatory-dipole pixels. As in the case of the Haar-like features, the integral image is used to calculate the sum of the pixels inside the rectangular regions. Similar features are also used in other recent works, for example, <ref type="bibr" target="#b27">[28]</ref>, where weighted dipoles with a fixed position are used to describe objects.</p><p>In computational terms, the use of dissociated dipoles means increasing from the approximately 600.000 features in Lienhart's approach of to more than 2 30 features in a training window size of 30 × 30 pixels, making the classical approach computationally unfeasible. To deal with this limitation, we define an evolutionary approach of Adaboost.</p><p>3) Evolutionary Adaboost: Boosting is a powerful learning technique that allows for combining the performance of many simple classification functions or Weak Classifiers to produce a Strong Classifier <ref type="bibr" target="#b28">[29]</ref>. At each round of learning, the examples are reweighted to emphasize the examples that were incorrectly classified by the previous Weak Classifier. The final Strong Classifier is a decision stump, which is composed of a weighted combination of Weak Classifiers, followed by a threshold <ref type="bibr" target="#b29">[30]</ref>.</p><p>In the classical boosting approach, an exhaustive search is used to find the best Weak Classifier. Therefore, with an enormous features set, this approach becomes computationally unfeasible.</p><p>Our evolutionary weak learner minimizes the weighted error function ε of the Adaboost scheme as</p><formula xml:id="formula_0">ε = i:h(x i ) =y i w i (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where X = {(x i , y i )|i = 1 : m} are the pairs sample-label that compound the training set, W = {w 1 , w 2 , . . . , w m } is the Adaboost weights distribution over the training set, and h(x i ) corresponds to the label that the hypothesis h predicted for the training object x i . Although, in the rest of this section, we use the original Discrete Adaboost algorithm for simplicity, our approach can be applied to any existing variant of Adaboost. Weak Learner can be considered an optimization problem, where we need to find the parameters of the Weak Classifier that minimize the error function ε. This function is defined; thus, it seems logical to consider it as a nonderivative function that is full of discontinuities. Therefore, the classical approaches based on gradient descend cannot be applied on this problem. One alternative solution is the use of an evolutionary approach. The most well known evolutionary strategy is the genetic algorithm, which does a search over the spaces of solutions using the three basic concepts of the Darwin's theory: 1) mutation, 2) crossover, and 3) natural selection.</p><p>4) Weak Classifier: When we work with evolutionary algorithms, we should define two basic elements: 1) the individuals and 2) the evaluation function. Each individual must represent a point in the space of solutions, and the evaluation function measures the function value at the point that the individual represents. We define the evaluation function as F(I) → R, I ∈ R d , where I is an individual. In this case, the function that we are optimizing is the classification error over a given labeled data set X, using a weights distribution W over the data. Then, the function F corresponds to ε [see <ref type="bibr" target="#b0">(1)</ref>], where the individual I defines the hypothesis h. Combining both equations, we can write the evaluation function as</p><formula xml:id="formula_2">F(I, W, X) = ⎛ ⎝ i:h(I,x i ) =y i w i ⎞ ⎠ where h(I, x) → {-1, +1}.<label>(2)</label></formula><p>Note that the function h depends not only on a certain data instance but also on the individual. At this point, we formulated the evaluation function in terms of the Weak Learner. The following step determines which parameters are required to define h or, in other terms, to decide the dimension of I.</p><p>One important consideration in choosing the parameters is to evaluate the relevance of each of them in contrast to the introduced complexity. The Viola and Jones definition of the Weak Learner consists of a feature and a threshold value. The feature can be parameterized by the upper left position and the size of one of the regions and their type. The size, weight, and position of the other regions that conform to the Haar-like feature are fixed by the feature type (see Fig. <ref type="figure" target="#fig_0">2</ref>). Given a feature, the threshold value must be learned using the training samples, applying an exhaustive search over all the possible threshold values to find the one that minimizes the error. Therefore, for the Viola approach, a Weak Learner can be defined as h(I, T hr, x i ) → {-1, +1}, where I = (R x , R y , R w , R h , T ), T hr is the threshold value, R x , R y , R w , and R h correspond to the upper left corner (x, y), the width, and the height of one of the regions, and T is the type of the Haar-like feature (see Fig. <ref type="figure" target="#fig_0">2</ref>). Note that the threshold value is not included in the individual I, because this value is learned once the other parameters have been fixed.</p><p>In the case of the Dissociated Dipoles, the regions have no constraints; thus, both regions are independently learned. Using the same reasoning, we define a Weak Classifier based on the Dissociated Dipoles as h(I, T hr, x i ) → {-1, +1}, where I = (Re x , Re y , Re w , Re h , Ri x , Ri y , Ri w , Ri h ), with Re being the excitatory dipole, and the type parameter T is changed by the parameters of the inhibitory dipole Ri. This representation can be extended by including extra parameters such as the weights of the regions, which are necessary to represent some types of Haar-like features.</p><p>In the previous approaches, the difference between both regions depends on illuminance conditions. Thus, we need to define a normalization criterion to make the representation invariant to those conditions. In <ref type="bibr" target="#b20">[21]</ref>, a method called fast lighting correction is used to deal with the illuminance variations. In addition, the Haar-like approach uses the sum of the pixels to represent the region, which introduces a scale dependence that must be corrected to deal with the multiscale detection. The dissociated dipoles are not affected by scale, because the mean value is used instead of the sum.</p><p>The aforementioned formulations are quantitative comparisons between the regions, considering not only which region has a higher value but the difference between these values as well. We can simplify by only using qualitative comparisons; therefore, the calculus of the threshold value becomes unnecessary, because we use only the sign of the difference. This approach has two main advantages: 1) The illuminance normalization is unnecessary, and 2) removing the threshold learning process reduces the evaluation time. We can rewrite the previous formulations as</p><formula xml:id="formula_3">h(I, x i ) → {-1, +1}<label>(3)</label></formula><p>where</p><formula xml:id="formula_4">I = (R x , R y , R w , R h , T ) I = (Re x , Re y , Re w , Re h , Ri x , Ri y , Ri w , Ri h ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Learning Algorithm:</head><p>The final approach is summarized in the Evolutionary Adaboost algorithm, as shown in Algorithm 1. This algorithm is used to learn all the stages of the detection cascade using a set (x 1 , y 1 ), . . . , (x m , y m ) of samples classified as positive samples in the previous stages of the cascade. This algorithm iteratively uses a genetic algorithm to minimize the weighted error and to instantiate the parameters of a new Weak classifier that is added to the final ensemble.</p><p>Algorithm 1: Evolutionary Discrete Adaboost.</p><p>Given:</p><formula xml:id="formula_5">(x 1 , y 1 ), . . . , (x m , y m ) where x i ∈ X, y i ∈ Y = {-1, +1} Initialize W 1 (i) = 1/m for t = 1, . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. , T do</head><p>Use a genetic algorithm to minimize</p><formula xml:id="formula_6">t = P r i∼W t [h t (x i ) = y i ] .</formula><p>The given solution is taken as the hypothesis h t . Get the weak hypothesis h t : X → {-1, +1} with error t . Choose</p><formula xml:id="formula_7">α t = (1/2) ln(1 -t / t ) Update W t+1 (i) = W t (i) Z t × e -α t , if h t (x i ) = y i e α t , if h t (x i ) = y i = W t (i) exp (-α t y i h t (x i )) Z t</formula><p>where Z t is a normalization factor (chosen so that W t+1 will be a distribution). end for Output the final hypothesis:</p><formula xml:id="formula_8">H(x) = sign T t=1 α t h t (x) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Classification: F-ECOC</head><p>Once we have located an object, we need to categorize among a large set of classes. Although various systems of multiple classifiers were proposed, most of them use similar constituent classifiers, which are often called base classifiers (i.e., dichotomies). In this sense, ECOC represents a classification technique that allows a successful combination of base classifiers to address the multiclass problem <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b30">[31]</ref>.</p><p>1) ECOC: The design of ECOC is based on a coding and a decoding strategy, where coding aims at assigning a codeword <ref type="foot" target="#foot_0">1</ref>to each of the N c classes (up to N c codewords), and decoding aims at assigning a class label to a new test codeword. Arranging the codewords as rows of a matrix, we define the "coding matrix" M , where M ∈ {-1, 1} N c ×n , with n being the code length. From the point of view of learning, the matrix M represents n binary learning problems (i.e., dichotomies), with each corresponding to a column of the ECOC matrix M . Each dichotomy defines a subpartition of classes, coded by {+1, -1} according to their class membership. In Fig. <ref type="figure" target="#fig_1">3(a)</ref> the codification for a four-class problem using the one-versusall coding strategy is shown. The white and black regions correspond to +1 and -1 valued positions, respectively. Thus, in Fig. <ref type="figure" target="#fig_1">3</ref>(a), the dichotomy h i is trained to discriminate class c i from the rest of the classes. If we use a larger set of symbols for coding M ∈ {-1, 0, 1} N c ×n , some entries in the matrix M can be zero, which indicates that a particular class is not considered for a given dichotomy. In Fig. <ref type="figure" target="#fig_1">3(b)</ref>, the codification for a fourclass problem that uses the one-versus-one coding strategy is shown. The gray regions correspond to the zero value (i.e., nonconsidered classes for the classifiers). In this strategy, all possible pairs of classes are split. For example, dichotomy h 1 classifies class c 1 versus class c 2 .</p><p>As a result of the outputs of the n binary classifiers, at the decoding step, a code is obtained for each data point in the test set. This code is compared with the base codewords of each class that was defined in the coding matrix M , and the data point is assigned to the class with the "closest" codeword. The common distances for decoding are the Hamming and the Euclidean distances <ref type="bibr" target="#b31">[32]</ref>.</p><p>2) F-ECOC: Most of the current discrete coding strategies are predesigned problem-independent codewords (e.g., one versus all <ref type="bibr" target="#b32">[33]</ref> and one versus one <ref type="bibr" target="#b33">[34]</ref>). In the work of Pujol et al. <ref type="bibr" target="#b23">[24]</ref>, a method for embedding tree structures in the ECOC framework is proposed. Beginning on the root containing all classes, the nodes associated with the best partition in terms of the mutual information are found, and the process is repeated until the sets with a single class are obtained.</p><p>Taking the previous work as a baseline, we propose to use multiple trees embedding, forming an F-ECOC. We build an optimal tree-the one with the highest classification score at each node-and several suboptimal trees-the ones closer to the optimal one under certain conditions. Let us keep, at each iteration, the best k partitions of the set of classes. If the best partition is used to construct the current ECOC tree, the rest of the partitions form the roots of k -1 trees. We iteratively repeat this process until all nodes from the trees are decomposed into one class. Given a base classifier, the suboptimal tree candidates are designed to have the maximum classification score at each node without repeating the previous subpartitions of classes. In the case of generating T first optimal trees, we can create an ensemble of trees by embedding them in the ECOC matrix, as shown in Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2:</head><p>Training algorithm for F-ECOC.</p><p>Given N c classes: c 1 , . . . , c N c and T trees to be embedded</p><formula xml:id="formula_9">Ω 0 ⇐ ∅ i ⇐ 1 for t = 1, . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. , T do</head><p>Initialize the tree root with the set N i = {c 1 , . . . , c N c } Generate the best tree at iteration t: for each node N i do Train the best partition of its set of classes</p><formula xml:id="formula_10">{P 1 P 2 }|N i = P 1 ∪P 2 , N i / ∈ Ω t-1</formula><p>using a classifier h i so that the training error is minimal According to the partition obtained at each node, codify each column of the matrix M as:</p><formula xml:id="formula_11">M (r, i) = ⎧ ⎨ ⎩ 0, if c r / ∈ N i +1, if c r ∈ P 1 -1, if c r ∈ P 2</formula><p>where r is the index of the corresponding class c r Ω t ⇐ Ω t-1 ∪ N i i ⇐ i + 1 end for end for The proposed technique provides a suboptimal solution because of the combination of robust classifiers obtained from a greedy search using the classification score. One of the main advantages of the proposed technique is that the trees share their information among classes in the ECOC matrix M . It is done at the decoding step by jointly, instead of separately, considering all the coded positions of a class. It is easy to see that each tree structure of N c classes introduces N c -1 classifiers, which is far from the N c • (N c -1)/2 dichotomies required for the oneversus-one coding strategy.</p><p>One example of two optimal trees and the F-ECOC matrix for a toy problem is shown in Fig. <ref type="figure" target="#fig_2">4</ref>. Fig. <ref type="figure" target="#fig_2">4(a)</ref> and<ref type="figure">(b)</ref> show two examples of optimal trees. The second optimal tree is constructed based on the following optimal subpartitions of classes. This way, for the first initial set of classes {c 1 , c 2 , c 3 , c 4 }, the two optimal trees include the best subpartitions of classes in terms of the classification score, which, in the example, corresponds to c 1 , c 3 versus c 2 , c 4 for the first tree and c 1 , c 2 , c 3 versus c 4 for the second tree, respectively. Fig. <ref type="figure" target="#fig_2">4(c)</ref> shows the embedding of trees into the F-ECOC matrix M . Note that the column h 3 corresponds to the node N 3 , and the following dichotomies correspond to the nodes of the second tree. Classes that do not belong to the subpartitions of classes are set to zero. On the other hand, classes that belong to each partition are set to +1 and -1 values, defining the subset of classes involved on each classifier.</p><p>Recent studies on the decoding steps have shown that the zero symbol introduces decoding errors in the traditional decoding distances <ref type="bibr" target="#b34">[35]</ref>. To deal with this problem and to increase the performance of the F-ECOC coding design, we propose the Attenuated Euclidean decoding strategy, which is defined as</p><formula xml:id="formula_12">d j = n i=1 |y j i |(x i -y j i ) 2</formula><p>, where d j is the distance to row j, n is the number of dichotomies, x i is the response of the classifier h i over the test sample, and y j i is the value of the coding matrix M at the ith row and the jth column, respectively. We introduce the factor |y j i | to avoid the error that the zero symbol introduces. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. TRAFFIC-SIGN-RECOGNITION SYSTEM</head><p>This section presents the details of the system scheme in Fig. <ref type="figure" target="#fig_3">5</ref>. We explain the relationship between each of the aforementioned methods and their integration in a real traffic-signrecognition system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Acquisition Module</head><p>The mobile mapping system has a stereo pair of calibrated cameras, which are synchronized with a Global Positioning System/(GPS/INS) system. Therefore, the result of the acquisition step is a set of stereo pairs of images with their position and orientation information. This information allows the use of epipolar geometry to change from one camera to the other one and to obtain the real position in world coordinates of a point. To detect all signs that appear in the input image independent of their size and position, we scan the image using windows at different scales. All these windows are used as the detector input. In this sense, the detector can be defined as a classification function h : X → {-1, 1}, where X is a window from the input im-age. The result of detection is a set of windows that corresponds to the valid objects, i.e., all the windows X, where h(X) = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Detection Module</head><p>All the data generated in the acquisition process is given to the detector. We organize the trained detectors as an attentional cascade <ref type="bibr" target="#b20">[21]</ref>. The attentional cascade is a degenerated decision tree where, at each stage, a detector is trained to detect almost all objects of interest while rejecting a certain fraction of the nonsign patterns. Because of the huge number of different traffic signs types, we group them using a similarity criterion, and we train a different cascade for each group. Each window in the input images is analyzed by all the cascades, and the detected objects from each cascade are given as output from the detector. Each cascade detects only a certain category of signs; thus, the output objects have the first classification information.</p><p>1) Stereo Association: We work with a stereo system; thus, all signs appear on both cameras at each frame (except in case of occlusions or if one of the signs is out of the field of view). This redundant information is used to improve the detection  ratio. Using the epipolar geometry, given an instance of a sign in one of the sources, we estimate the region where it must appear in the other source. Once we have a reduced search window in the other source, we apply a similarity criterion based on a normalized correlation. The point with the highest similarity value gives us the position of the target object. This information is used to link the object of a source with its stereo objector to recover it. Using this information, we only lose the objects that have been lost in both cameras. Using the calibration data, the position and orientation information of the GPS/INS system, and the coordinates of an object in each camera, we compute the object position in world coordinates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Classification Module</head><p>Using the Evolutionary Adaboost, a region of interest that contains a sign is determined. Depending on the type of the detected sign, a different model fitting is applied before classification, looking for affine transformations that perform the spatial normalization of the object.</p><p>1) Model Fitting: Because of the few changes on the point of view of the captured signs, we apply the fast radial symmetry <ref type="bibr" target="#b35">[36]</ref> for the circular signs, which offers high robustness to image noise. As shown in Fig. <ref type="figure" target="#fig_4">6</ref>, the fast radial symmetry provides an approximation to the center and the radius of the circular sign.</p><p>On the other hand, for the case of triangular signs, the method that allows a successful model fitting is based on the Hough transform <ref type="bibr" target="#b36">[37]</ref>. Nevertheless, we need to consider additional constraints to obtain the three representative border lines of a triangular traffic sign. Each line has associated a position in relation to the others. In Fig. <ref type="figure" target="#fig_5">7</ref>(a), a false horizontal line is shown. This line does not fulfill the expected spatial constraints of the object; thus, we iterate the Hough procedure to detect the next representative line in the allowed range of degrees. The corrected image is shown in Fig. <ref type="figure" target="#fig_5">7(b</ref>). Once we have the three detected lines, we calculate their intersection, as shown in Fig. <ref type="figure" target="#fig_5">7(c</ref>). To assure that the lines are the expected ones, we complement the procedure by looking for a corner at the circular region of each intersection surroundings [as shown in Fig. <ref type="figure" target="#fig_5">7(d)</ref> and<ref type="figure">(e</ref></p><formula xml:id="formula_13">)] S = {(x i , y i )|∃p &lt; ((x -x i ) 2 + (y - y i ) 2 -r 2 )}|i ∈ [1, . . . , 3]</formula><p>, where S is the set of valid intersection points, and p corresponds to a corner point to be located in a neighborhood of the intersection point. A corner should be determined at each lines intersection; thus, we apply a corner detector at the surroundings of the triangular vertices to increase the confidence of determining a sign.</p><p>2) Spatial Normalization: Once the sign model is fitted using the previous methods, the next step is the spatial normalization of the object before classification. There are four steps.</p><p>1) Transform the image to make the recognition invariant to small affine deformations. 2) Resize the object to the signs database size. 3) Filter using the Weickert anisotropic filter <ref type="bibr" target="#b37">[38]</ref>. 4) Mask the image to exclude the background pixels at the classification step. To prevent the effects of illumination changes, the histogram equalization improves the image contrast and yields a uniform histogram.</p><p>3) F-ECOC: Once the signs are extracted, they are classified through the F-ECOC strategy. The optimal trees consider the best subpartitions of classes to obtain robust classifiers based on the gray-level pixel values of the object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. System Outputs</head><p>At the end of the system, an XML file is generated, which contains the position, size, and class of each of the detected traffic signs. This information is used to obtain the real-world position of each detected and recognized sign. One execution example of the whole process is shown in Fig. <ref type="figure" target="#fig_6">8</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RESULTS</head><p>The different experiments of this section are focused on separately (i.e., detection and classification) evaluating each individual part of the framework and performing the whole system evaluation. The validation of the methodology is carried out using real images from the mobile mapping system Geomobil <ref type="bibr" target="#b24">[25]</ref>. This system captures georeferenced stereo pairs of images, which are the input of the recognition system. For these experiments, the system was configured to acquire one frame each 10 m or when the orientation variation is greater than 60 • . These parameters are hardly tested to assure a correct acquisition in road scenarios, which means that all the interesting objects appear, at least, in two or three stereo pairs of images with a minimum size of 30 × 30 pixels resolution to be processed.</p><p>To assure a high diversity of road conditions, we selected three recording sessions, each one carried out different days and with different weather conditions. It represents a total of 9.510 stereo-pair road images. To avoid using different images that contain the same traffic sign in the training and test sets, instead of using random sequences, we divide the sessions into four subsequences of similar number of frames, without sharing signs. The first and third parts are used for training, and the second and fourth parts are for testing. The reason for using a larger test set is because there are many frames that do not contain objects, and it is interesting to extract the false-alarm ratio in normal conditions, assuring to test the system under different illumination conditions, road types, and traffic intensities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Detection Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Evolutionary Strategy:</head><p>To perform all the tests with the evolutionary Adaboost approach, we use a Genetic Algorithm with a population size of 100 individuals, Gaussian-based mutation probability (i.e., the Gaussian is centered at zero, with a variance of the half of the variable range, decreasing the variance along the generations), and scattered crossover strategy, with a crossover fraction of 0.8. When we use a genetic algorithm instead of an exhaustive search, different initializations of the algorithm with the same training data give rise to different weak classifiers. The dissociated dipoles cannot be learned by the classical approach; thus, in this experiment, we use the Haar-like features. A one-stage detector is learned using fixed training and test sets, comparing the error evolution for both strategies, and the variance in the evolutionary approach over different runs.</p><p>We run the learning process over the same training and test sets for 50 times, using 50 iterations of the evolutionary Adaboost. In the case of the classic Adaboost, because the Weak Learner does an exhaustive search over the features, at each round, the selected features are the same. In the case of the Genetic Algorithm, we calculate the mean error value over all the rounds for each iteration.</p><p>In Fig. <ref type="figure" target="#fig_7">9</ref>, the train and test mean error values at each iteration are shown. Note that both methods converge with the same number of iterations. To analyze the error variability, in Fig. <ref type="figure" target="#fig_8">10</ref>, we show the mean and standard deviation for the error at each iteration. The confidence interval shows that the variance is very closed. Therefore, although the evolutionary Adaboost has a random component, the goodness of the given solution is similar.</p><p>2) Detector Performance: To evaluate the detector performance, we train a cascade of detectors using the evolutionary method with ordinal dissociated dipoles. In Fig. <ref type="figure" target="#fig_9">11</ref>, we show the most relevant features that the evolutionary method selected at the first stage of the cascade. Note that only a few of them correspond to Haar-like features.</p><p>Due to the different appearance frequency of each type of sign and the high intraclass variability, we trained a detection cascade for each group of similar signs. In Table <ref type="table" target="#tab_0">I</ref>, we show the groups of signs and the number of positive samples that were used to train each cascade. The number of negative samples on the train process is automatically selected at each stage, with a proportion of 3 : 1 (i.e., three negative examples for each positive example). Most of the captured images is from main roads, and consequently, some types of signs do not appear enough   times to train a detector. Therefore, we only trained the four detectors shown in Table <ref type="table" target="#tab_0">I</ref>.</p><p>The results are analyzed using two configurations: 1) using the stereo association to take advantage of the stereo information and 2) considering each stereo pair of images as two independent images. For each configuration, the obtained results with and without using sequential information are extracted. When the sequential information is used, different instances of the same real traffic sign are considered as the same object. In the case of not using this information, each instance is considered to be an independent object. In Fig. <ref type="figure" target="#fig_10">12</ref>, we show the hit ratio of the detector trained for each type of sign. In general, we can see that the accuracy of the detectors depends on the variability of sign appearance and the size of the training set. The first and the third columns correspond to the results that consider each appearance of a traffic sign as a different sign. In addition, the second and the fourth columns only take into account the real traffic signs, considering that a sign is detected if we can detect it in one or more frames where it appears. The first two columns do not take into account stereo redundancy, whereas the two last columns do.</p><p>Another measure for evaluating the performance of the system is the false-alarm rate. We work with a mobile mapping system; thus, an important point is which percentage of the detected objects corresponds to traffic signs. Therefore, our false-alarm value is referred to the detected signs instead of the number of analyzed windows, which is on the order of 5 million per stereo pair. Nevertheless, the number of false positives with respect to the number of stereo-pair images has been included to analyze the results easier. Both false-alarm rates for each type of signs are detailed in Table <ref type="table" target="#tab_1">II</ref>. Some samples of detected objects and false alarms are shown in Fig. <ref type="figure" target="#fig_11">13</ref>. It is shown that the system can detect the signs in very extreme lighting conditions. In the false-positive images, it is shown that, frequently, other road elements look similar to traffic signs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Classification Results</head><p>Four types of experiments are performed to evaluate the classification scheme:</p><p>1) traffic sign classification using different state-of-the-art classifiers; 2) tree-embedding analysis using F-ECOC; 3) public University of California, Irvine (UCI) Machine Learning Repository classification; 4) model-fitting classification. First, we comment the generation of the database to train the classifiers and to perform the experiments.</p><p>1) Classification Database: The database used to train the classifiers was designed using the regions of interest obtained   from the detection step and the aforementioned model-fitting methods. We defined three groups of classes using the most common types of signs. The considered classes are shown in Fig. <ref type="figure" target="#fig_12">14</ref>. Speed signs need special attention. These types of signs are less discriminative, with some of them being only differentiated by a few pixels. With this type of signs, it is better to work on binary images to avoid the errors that can be accumulated because of the gray levels of the signs. For the 12 classes of circular signs and 12 classes of triangular signs, we have 750 training images in both cases. For the seven speed classes, we use 500 training samples. Finally, the resolution of each database is 35 × 35 pixels for the circular group, 44 × 39 pixels for the triangular group, and 41 × 41 pixels for the speed group, respectively.</p><p>2) State-of-the-Art Comparison: To evaluate the F-ECOC performance, we compare it with state-of-the-art classifiers. The details for each strategy are given as follows:</p><p>1) Three-Euclidean distance nearest neighbors (K-NN); 2) Tangent Distance (TD) <ref type="bibr" target="#b38">[39]</ref> with an invariant tangent vector with respect to translation, rotation, and scaling; 3) Principal Components Analysis (99.98%), followed by 3-nearest neighbors (PCA K-NN) <ref type="bibr" target="#b39">[40]</ref>; 4) Fisher Linear Discriminant Analysis (FLDA) with a previous 99.98% PCA <ref type="bibr" target="#b39">[40]</ref>;     In the different variants of boosting, we apply 50 iterations. We use Gentle Adaboost, because it has been shown to outperform other Adaboost variants in real applications <ref type="bibr" target="#b43">[44]</ref>. With regard to the F-ECOC base classifier, we apply FLDA in the experiments that classify traffic signs and 50 runs of Gentle Adaboost with decision stumps on the UCI data sets. This last choice was selected so that all strategies share the same base classifier. Table <ref type="table" target="#tab_2">III</ref> shows the characteristics of the data used for the classification experiments, where #Training, #Test, #Features, and #Classes correspond to the number of training samples, test samples, features, and classes, respectively.</p><p>The classification results and confidence intervals are graphically shown in Fig. <ref type="figure" target="#fig_14">15</ref> for the different groups. It is shown that the F-ECOC that uses FLDA as a base classifier attains the highest accuracy in all cases. Nevertheless, for the circular and triangular signs, the differences among classifiers are significantly different because of the high discriminability of these two groups. The speed group is a more difficult classification problem. In this case, the F-ECOC strategy obtains an accuracy of 90%, outperforming the rest of the classifiers.</p><p>3) Tree-Embedding Analysis: The training evolution of the F-ECOC at the previous experiment is shown in Fig. <ref type="figure" target="#fig_15">16</ref> for the speed group. Each iteration of the figure shows the classification accuracy by embedding a new node (i.e., a binary classifier) from each optimal tree in the F-ECOC matrix M . The three optimal trees are split by the dark vertical lines. The respective trees are shown in Fig. <ref type="figure" target="#fig_17">17</ref>. In the first generated tree in Fig. <ref type="figure" target="#fig_17">17</ref>, it is shown that the most difficult partitions are reserved for the final classifiers of the tree. The next trees select the following best partitions of classifiers to avoid repeating classifiers. These classifiers learn subgroups of classes from the same data, improving the classification results (see Fig. <ref type="figure" target="#fig_15">16</ref>) by sharing their knowledge among classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) UCI Evaluation:</head><p>To validate the accuracy of the F-ECOC strategy, we tested it on the public UCI Machine Learning Repository <ref type="bibr" target="#b44">[45]</ref>. The characteristics of the UCI data sets are shown in Table <ref type="table" target="#tab_3">IV</ref>. In this case, to observe the benefits of using multiple-tree embedding, we compared the performance of the F-ECOC strategy with the discriminant ECOC (DECOC) approach <ref type="bibr" target="#b23">[24]</ref>. Both methods use 50 runs of Gentle Adaboost with decision stumps as the base classifier, using 2-optimal trees for the F-ECOC strategy.</p><p>The classification results of the two strategies over the UCI data sets are shown in Table <ref type="table" target="#tab_4">V</ref>. It is observed that the F-ECOC strategy outperforms, in most cases, the results from the DECOC strategy for the same base classifier. Moreover, in the worst case, the F-ECOC methodology obtains the same results as in the DECOC technique.  <ref type="table" target="#tab_5">VI</ref>. It is shown that for circular and speed signs, the results are practically maintained from the previous experiments. For triangular signs, the accuracy is slightly decreased because of the effect of noise, variability of sign appearance, and resolution, which makes the Hough transform lose some sides of the triangular signs. Nevertheless, the final results are more than 90% in all cases.</p><p>6) System Results: We calculate the performance of the whole system over a test set of 10.000 stereo pairs of images, which correspond to a 100-km road. The accuracy of the real traffic sign recognition system that applies the detection and classification approaches jointly obtains a mean triangular sign reliability of 90.87% ± 0.87% and a circular sign reliability of 90.16% ± 1.01%. In the detection stage, recognition failures are due to background confusion (see Fig. <ref type="figure" target="#fig_11">13</ref>) and high interclass variability, whereas in the classification stage, the errors are due to the poor resolution of the images.</p><p>7) Discussions: Classical approaches to traffic sign detection are based on a segmentation of the image using thresholding over a color space. It is useful, because traffic signs use a color code to easily be identifiable by humans. These colors are used to be distinguished in natural scenes. However, some objects (e.g., vehicles, buildings, and advertising near the road) share similar colors. In addition, the acquired color information is related to the camera that was used, and it is far from the real object's color. Although, in some works, this problem is attenuated using some heuristics (e..g, the size and position of    the segmented regions or shape), it can be a source of false positives that can interfere on the recognition of the signs.</p><p>Our work avoids these problems by using appearance information instead of directly using the gray-scale information. The presented system uses relations between gray values inside the image, and thus, the changes that were produced by different acquisitions systems are smoothed. <ref type="foot" target="#foot_1">2</ref> However, the appearancebased methods improve the results, but they are not free. The presented methods introduces an extra computation cost to the detection stage, which disable them to be used in driver support applications, where the real-time requirement is harder than in the case of the mobile-mapping problem. To face those restrictions, the detection stage must be optimized. There are promising works that are focused on cascade optimizations <ref type="bibr" target="#b45">[46]</ref> and multiple detectors organization <ref type="bibr" target="#b46">[47]</ref>, which can be used to improve not only the results but the computation time as well. In addition, the original detection scheme of Viola and Jones <ref type="bibr" target="#b20">[21]</ref> was successfully embedded in hardware devices, allowing high-detection frame rates. Similar strategies can be adopted for our method.</p><p>The experiments that were performed for the F-ECOC classification technique used a greedy search to look for the optimal subgroups of classes that form the tree structures. In our case, the exhaustive search was computationally feasible with the number of classes. If necessary, different strategies can be applied instead. In case of having a large number of classes to be learned, suboptimal solutions can be found using faster approaches (e.g., the Sequential Forward Floating Search <ref type="bibr" target="#b47">[48]</ref>) to speed up the method.</p><p>In the same way, after a preliminary set of experiments, we fixed FLDA as the base classifier for the F-ECOC strategy to learn the binary problems. It is shown to be a suitable choice for traffic sign classification. In other types of classification problems, the F-ECOC approach can be applied with other base classifiers that better adapt to learn a particular distribution of the data.</p><p>The detection and recognition system has been implemented to automatically process hundreds of video sequences that were obtained from the mobile-mapping process in <ref type="bibr" target="#b24">[25]</ref>. This data is processed by the Institut Cartogràfic de Catalunya to extract cartographic information. Before the system was implemented, the labeling of the traffic signs in image sequences was manually done by a user. With the automatic system, the recompilation of the information is about three times faster. Furthermore, comparing the results of labeling with the automatic and manual processes, we can argue that the automatic performance obtains better results. It is done by the fact that the performance of manual labeling is affected by the eyestrain of the user that processes thousands of frames and the extreme illumination conditions of the video sequences.</p><p>With regard to the computation cost of the system, the final cascade of detectors learned with the evolutionary Adaboost is comparable in complexity to the Viola and Jones <ref type="bibr" target="#b20">[21]</ref> face detector. Moreover, the same optimization technique for calculating the Haar-like features through the integral image can be used in the case of dissociated dipoles. Using ordinal features, the comparison with a float value has been replaced with the use of the sign, which reduces the computation cost compared with the Viola and Jones real-time detector. Regarding the classification stage, although the training time could be expensive, depending on the number of classes, training examples, and the number of features per data sample, once the F-ECOC hypotheses are learned, the classification time depends on the nature of the applied base classifier. In particular, considering FLDA or Adaboost, the classification decision only requires a simple matrix product or an additive model estimation, which can be computed in real time. Nevertheless, although the detection and classification techniques can be optimized to be real-time approaches, the use of correlation on the stereo-association stage, the model fitting, and normalization methodologies prevent the whole system from being set in real time.</p><p>With regard to the memory usage, the final detection system needs eight integers per feature to represent the dissociated dipoles. During detection, at least the integral image of the processed image must be in memory. Finally, the classification parameters and the ECOC matrix must also be available in memory. In summary, the stronger restriction in memory usage is the size of processed images and not the system itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We have presented a mobile mapping detection and classification system to deal with the traffic-sign-recognition problem. We have introduced a computationally feasible method for feature selection based on an evolutionary strategy. The exhaustive search over all the combinations between feature and threshold are replaced with the evolutionary approach and ordinal features; thus, the final approach has two main advantages: 1) It speeds up the learning process, and 2) it allows for working with large feature sets to distinguish between object and background, which is computationally unfeasible using traditional methods because of the large number of features. Moreover, we have proposed the F-ECOC classification strategy to lead with the multiclass categorization problem. The method is based on the embedding of multiple optimal tree structures in the ECOC framework, sharing their knowledge among classes, and constructing an ensemble of trees until the necessary performance is achieved. A wide set of traffic signs are recognized under high variance of appearance, such as noise, affine deformation, partial occlusions, and reduced visibility. The validation of the mobile mapping system shows that the presented methodology offers high robustness and better performance compared with the state-of-the-art strategies in uncontrolled environments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Haar-like features definition from a given region and the type value.</figDesc><graphic coords="3,144.24,70.50,300.12,172.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Four-class ECOC designs. (a) One-versus-all ECOC codification. (b) One-versus-one ECOC codification (White: 1. Black: -1. Grey: 0).</figDesc><graphic coords="4,307.95,70.02,246.12,88.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Four-class optimal trees and the F-ECOC matrix. (a) First optimal tree for a four-class problem. (b) Second optimal tree for the same problem. (c) F-ECOC matrix M for the problem, where h 1 , h 2 , and h 3 correspond to classifiers of N 1 , N 2 , and N 3 from the first tree, and h 4 , h 5 , and h 6 to N 4 , N 5 , and N 8 from the second tree.</figDesc><graphic coords="5,318.24,69.70,214.92,201.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Scheme of the whole traffic sign recognition system.</figDesc><graphic coords="6,149.44,69.78,300.12,412.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. (a) Input image. (b) X-derivative. (c) Y -derivative. (d) Image gradient. (e) Accumulator of orientations. (f) Center and radius of the sign.</figDesc><graphic coords="7,39.73,155.53,246.12,52.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. (a) Detected lines. (b) Corrected line. (c) Intersections. (d) Corner region. (e) Corner found.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Example of the system execution. Image acquisition, detection, and classification.</figDesc><graphic coords="7,314.74,70.14,221.64,313.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Error evolution using the classic Adaboost approach and the genetic Weak Learner.</figDesc><graphic coords="8,101.44,70.54,395.52,209.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Genetic approach. Error variability on the training process.</figDesc><graphic coords="9,39.73,345.37,246.12,141.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Selected dipoles obtained from the danger signs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Hit ratio for each sign type by using dissociated dipoles.</figDesc><graphic coords="10,132.44,70.30,333.96,149.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Some samples of detected objects and false positives.</figDesc><graphic coords="10,71.94,315.01,192.12,128.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Set of classes in the classification module. (a) Speed classes. (b) Circular classes. (c) Triangular classes.</figDesc><graphic coords="10,307.95,250.50,245.40,240.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>5 )</head><label>5</label><figDesc>SVM with projection kernel Radial Basis Function and the parameter γ = 1 (SVM)<ref type="bibr" target="#b40">[41]</ref>; 6) Gentle Adaboost with decision stumps that use the Haar-like features (BR)<ref type="bibr" target="#b21">[22]</ref>,<ref type="bibr" target="#b41">[42]</ref>; 7) multiclass Joint Boosting (JB) with decision stumps<ref type="bibr" target="#b42">[43]</ref>; 8) Gentle Adaboost<ref type="bibr" target="#b43">[44]</ref>; 9) Sampling with FLDA (BS); 10) statistical Gentle Naive Boosting (NB) with decision stumps<ref type="bibr" target="#b41">[42]</ref>; 11) our F-ECOC with 3-embedded optimal trees.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Classification results. (a) Speed problem. (b) Circular problem. (c) Triangular problem.</figDesc><graphic coords="11,44.73,70.18,499.08,104.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 16 .</head><label>16</label><figDesc>Fig.<ref type="bibr" target="#b15">16</ref>. Training process of F-ECOC that embeds the first-three optimal trees for the speed group.</figDesc><graphic coords="11,39.73,213.03,246.12,105.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>5 )</head><label>5</label><figDesc>Model-Fitting Classification: To test the performance of the classification step of the system, model fitting and F-ECOC classification are applied to a set of 200 regions of interests for each group. The regions of interest are obtained from the detection step. The results are shown in Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 17 .</head><label>17</label><figDesc>Fig.17. Three optimal trees generated by F-ECOC for the speed group.</figDesc><graphic coords="12,47.44,70.38,504.12,124.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I NUMBER</head><label>I</label><figDesc>OF POSITIVE SAMPLES USED TO TRAIN THE CASCADE FOR EACH CONSIDERED SIGN</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II FALSE</head><label>II</label><figDesc>-ALARM RATES FOR EACH SIGN TYPE</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III CHARACTERISTICS</head><label>III</label><figDesc>OF THE DATABASES USED FOR CLASSIFICATION. THE TRAINING AND TEST EXAMPLES ARE EQUALLY DISTRIBUTED AMONG THE GROUP CLASSES</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV UCI</head><label>IV</label><figDesc>REPOSITORY DATA SETS CHARACTERISTICS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V CLASSIFICATION</head><label>V</label><figDesc>RESULTS FROM THE UCI DATA SETS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI MODEL</head><label>VI</label><figDesc>FITTING AND CLASSIFICATION RESULTS</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>A codeword is a sequence of bits that represents a class.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Note that although color is not required in any stage of the present system, and all the feature sets that were used on the detection stage can be extended to any color space.</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>. This work was supported in part by a research grant under Projects TIN2006-15308-C02, FISPI061290, and CONSOLIDER-INGENIO 2010 (CSD2007-00018) and in part by the Institut Cartogràfic de Catalunya under the supervision of M. Pla. The Associate Editor for this paper was C. Stiller.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An image processing system for driver assistance</title>
		<author>
			<persName><forename type="first">U</forename><surname>Handmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kalinke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tzomakas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Werner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Von Seelen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. IV</title>
		<meeting>IEEE Int. Conf. IV<address><addrLine>Stuttgart, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="481" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Autonomous driving goes downtown</title>
		<author>
			<persName><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gavrila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gorzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lindner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Paetzold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wohler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intell. Syst</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="40" to="48" />
			<date type="published" when="1998-12">Nov./Dec. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic traffic signs inventory using a mobile mapping system</title>
		<author>
			<persName><forename type="first">S</forename><surname>Madeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bastos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sousa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sobral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Exhib. Geogr. Inform. GIS PLANET</title>
		<meeting>Int. Conf. Exhib. Geogr. Inform. GIS PLANET<address><addrLine>Lisboa, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Real-time radial symmetry for speed sign detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zelinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intell. Veh. Symp</title>
		<meeting>IEEE Intell. Veh. Symp</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="566" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Road signposts recognition system</title>
		<author>
			<persName><forename type="first">H</forename><surname>Akatsuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Imai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SAE Veh. Highway Infrastructure-Safety Compatibility</title>
		<meeting>SAE Veh. Highway Infrastructure-Safety Compatibility</meeting>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="189" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detection of highway warning signs in natural video images using color image processing and neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kellmeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zwahlen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Neural Netw</title>
		<meeting>IEEE Int. Conf. Neural Netw</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="4226" to="4231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Road sign recognition: A study of vision-based decision making for road environment recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>De Saint</surname></persName>
		</author>
		<author>
			<persName><surname>Blancard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Vision-Based Vehicle Guidance</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="162" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Road traffic sign detection and classification</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D L</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Salichs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Armingol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Ind. Electron</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="848" to="859" />
			<date type="published" when="1997-12">Dec. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recognition of traffic signs using a multilayer neural network</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ghica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Can. Conf. Elect. Comput. Eng</title>
		<meeting>Can. Conf. Elect. Comput. Eng<address><addrLine>Halifax, NS, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="848" to="859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recognition of traffic signs using a multilayer neural network</title>
		<author>
			<persName><forename type="first">A</forename><surname>De La Escalera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Armingol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salichs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Int. Conf. Field Service Robot</title>
		<meeting>3rd Int. Conf. Field Service Robot<address><addrLine>Espoo, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-06">Jun. 2001</date>
			<biblScope unit="page" from="833" to="834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An active vision system for real-time traffic sign recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Miura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shirail</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. ITSC, 2000</title>
		<meeting>IEEE Int. Conf. ITSC, 2000</meeting>
		<imprint>
			<biblScope unit="page" from="52" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Road Sign Recognition by Single Positioning of Space-Variant Sensor</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shaposhnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Podladchikova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Golovan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shevtsova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<ptr target="Available:citeseer.csail.mit.edu/657126.html" />
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Road sign detection and recognition using matching pursuit method</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="119" to="129" />
			<date type="published" when="2001-02">Feb. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A robust method for road sign detection and recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Piccioli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Micheli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Campani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="495" to="500" />
		</imprint>
	</monogr>
	<note>Available: citeseer.csail.mit.edu/piccioli96robust.html</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust road sign detection and recognition from image sequences</title>
		<author>
			<persName><forename type="first">G</forename><surname>Piccioli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Michelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Parodi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Campani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intell. Veh</title>
		<meeting>Intell. Veh</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="278" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast gray scale road sign model matching and recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Radeva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent Advances in Artificial Intelligence Research and Development</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>IOS</publisher>
			<date type="published" when="2004-10">Oct. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast radial symmetry for detecting points of interest</title>
		<author>
			<persName><forename type="first">G</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zelinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="959" to="973" />
			<date type="published" when="2003-08">Aug. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Road-sign detection and recognition based on support vector machines</title>
		<author>
			<persName><forename type="first">S</forename><surname>Maldonado-Bascon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lafuente-Arroyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gil-Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gomez-Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lopez-Ferreras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="264" to="278" />
			<date type="published" when="2007-06">Jun. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visual sign information extraction and identification by deformable models for intelligent vehicles</title>
		<author>
			<persName><forename type="first">A</forename><surname>De La Escalera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Armingol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pastor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="57" to="68" />
			<date type="published" when="2004-06">Jun. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Building road-sign classifiers using a trainable similarity measure</title>
		<author>
			<persName><forename type="first">P</forename><surname>Paclik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Novovicova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Duin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="309" to="321" />
			<date type="published" when="2006-09">Sep. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An extended set of Haar-like features for rapid object detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lienhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Maydt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Image Process</title>
		<meeting>IEEE Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="155" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Solving multiclass learning problems via error-correcting output codes</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bakiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="263" to="286" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Discriminant ECOC: A heuristic method for application dependent design of error-correcting output codes</title>
		<author>
			<persName><forename type="first">O</forename><surname>Pujol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Radeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vitrià</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1001" to="1007" />
			<date type="published" when="2006-06">Jun. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On the accuracy and performance of the geomobile system</title>
		<author>
			<persName><forename type="first">R</forename><surname>Alamús</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Casacuberta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Miranda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Snchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Talaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISPRS</title>
		<meeting>ISPRS<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-07">Jul. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Qualitative representations for recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Thoresz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sinha</surname></persName>
		</author>
		<ptr target="http://journalofvision.org/1/3/298/" />
	</analytic>
	<monogr>
		<title level="j">J. Vis</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">298</biblScope>
			<date type="published" when="2001-12">Dec. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">STICKS: Image-representation via non-local comparisons</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Balas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sinha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2003-10">Oct. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">SURF: Speeded-up robust features</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th Eur. Conf. Comput. Vis</title>
		<meeting>9th Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2006-05">May 2006</date>
			<biblScope unit="page" from="404" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Experiments with a new boosting algorithm</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="148" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A short introduction to boosting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Jpn. Soc. Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="771" to="780" />
			<date type="published" when="1999-10">Oct. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Error-correcting output coding corrects bias and variance</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th Int. Conf. Mach. Learn</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Prieditis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Russell</surname></persName>
		</editor>
		<meeting>12th Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="313" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Reducing multiclass to binary: A unifying approach for margin classifiers</title>
		<author>
			<persName><forename type="first">E</forename><surname>Allwein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="113" to="141" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning Machines</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Nilsson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1965">1965</date>
			<publisher>McGraw-Hill</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Classification by pairwise grouping</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="451" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Decoding of ternary error-correcting output codes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Pujol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Radeva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CIARP</title>
		<meeting>CIARP</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="753" to="763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fast radial symmetry for detecting points of interest</title>
		<author>
			<persName><forename type="first">G</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zelinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="959" to="973" />
			<date type="published" when="2003-08">Aug. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Segmentation (edge based, Hough transform)</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Morse</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<pubPlace>Provo, UT</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Brigham Young Univ.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Anisotropic Diffusion in Image Processing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Teubner-Verlag</publisher>
			<pubPlace>Stuttgart, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Transformation invariance in pattern recognition: Tangent distance and tangent propagation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Victorri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Netw.-Tricks of the Trade</title>
		<meeting>Neural Netw.-Tricks of the Trade</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">1524</biblScope>
			<biblScope unit="page" from="239" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Comparison of discrimination methods for the classification of tumors using gene expression data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dudoit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fridlyand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Speed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<biblScope unit="volume">576</biblScope>
			<date type="published" when="2000-06">Jun. 2000</date>
			<pubPlace>Berkeley, CA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Dept. Stat., Univ. California Berkeley</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A practical guide to support vector classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dept. Comput. Sci., Nat. Taiwan Univ</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<pubPlace>Taipei, Taiwan</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Text filtering by boosting naive Bayes classifiers</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGIR Conf. Res. Develop</title>
		<meeting>SIGIR Conf. Res. Develop</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="168" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Sharing visual features for multiclass and multiview object detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="854" to="869" />
			<date type="published" when="2007-05">May 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Additive logistic regression: A statistical view of boosting</title>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dept. Stat., Stanford Univ</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<pubPlace>Palo Alto, CA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Asuncion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">UCI Machine Learning Repository</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Optimization design of cascaded classifiers</title>
		<author>
			<persName><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. Vis. Pattern Recog</title>
		<meeting>Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="480" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Vector boosting for rotation invariant multiview face detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="446" to="453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Floating search methods for feature selection with nonmonotonic criterion functions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pudil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ferri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Novovicova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Pattern Recog</title>
		<meeting>Int. Conf. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="279" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">He has published a number of papers on pattern recognition and computer vision. His research interests include the application of machine learning techniques to visual object recognition problems and the development of statistical machine learning techniques. Oriol Pujol received the Ph.D. degree in computer science from the Universitat Autònoma de Barcelona</title>
		<author>
			<orgName type="collaboration">Xavier Baró received the B.S. and M</orgName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sergio Escalera received the B.S., M.S., and Ph.D. degrees from the Universitat Autònoma de Barcelona (UAB)</title>
		<meeting><address><addrLine>Barcelona, Spain; Barcelona, Spain; Barcelona, Spain; Barcelona, Spain; Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990">2003 and 2005. 2003. 2005, and 2008. 1990. 2007. 2004. 2007</date>
		</imprint>
		<respStmt>
			<orgName>Universitat Autònoma de Barcelona (UAB ; Universitat de Barcelona ; Universitat de Barcelona ; Universitat de Barcelona</orgName>
		</respStmt>
	</monogr>
	<note>Her Ph.D. dissertation was focused on the development of physics-based models applied to image analysis. She is currently an Associate Professor with the Department of Computer Science, UAB. she has joined the Department of Applied Mathematics and Analysis. Her research interests include the development of physics-based and statistical approaches for object recognition, medical image analysis, and industrial vision</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
