<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sampling Dead Block Prediction for Last-Level Caches</title>
				<funder ref="#_ThCmMD8 #_Ntj7g6R">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
				<funder ref="#_JNYuFHN">
					<orgName type="full">Norman Hackerman Advanced Research Program</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Samira</forename><surname>Khan</surname></persName>
							<email>skhan@cs.utsa.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Texas at San Antonio San Antonio</orgName>
								<address>
									<region>Texas</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yingying</forename><surname>Tian</surname></persName>
							<email>ytian@cs.utsa.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Texas at San Antonio San Antonio</orgName>
								<address>
									<region>Texas</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
							<email>djimenez@acm.org</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Texas at San Antonio San Antonio</orgName>
								<address>
									<region>Texas</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Barcelona Supercomputing Center Barcelona</orgName>
								<address>
									<region>Catalonia</region>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sampling Dead Block Prediction for Last-Level Caches</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/MICRO.2010.24</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Last-level caches (LLCs) are large structures with significant power requirements. They can be quite inefficient. On average, a cache block in a 2MB LRU-managed LLC is dead 86% of the time, i.e., it will not be referenced again before it is evicted.</p><p>This paper introduces sampling dead block prediction, a technique that samples program counters (PCs) to determine when a cache block is likely to be dead. Rather than learning from accesses and evictions from every set in the cache, a sampling predictor keeps track of a small number of sets using partial tags. Sampling allows the predictor to use far less state than previous predictors to make predictions with superior accuracy.</p><p>Dead block prediction can be used to drive a dead block replacement and bypass optimization. A sampling predictor can reduce the number of LLC misses over LRU by 11.7% for memory-intensive single-thread benchmarks and 23% for multicore workloads. The reduction in misses yields a geometric mean speedup of 5.9% for single-thread benchmarks and a geometric mean normalized weighted speedup of 12.5% for multi-core workloads. Due to the reduced state and number of accesses, the sampling predictor consumes only 3.1% of the of the dynamic power and 1.2% of the leakage power of a baseline 2MB LLC, comparing favorably with more costly techniques. The sampling predictor can even be used to significantly improve a cache with a default random replacement policy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The miss rate in the last-level cache (LLC) can be reduced by reducing the number of dead blocks in the cache. A cache block is live from the time of its placement in the cache to the time of its last reference. From the last reference until the block is evicted the block is dead <ref type="bibr" target="#b12">[13]</ref>. Cache blocks are dead on average 86.2% of the time over a set of memory-intensive benchmarks. Cache efficiency can be improved by replacing dead blocks with live blocks as soon as possible after a block becomes dead, rather than waiting for it to be evicted.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> depicts the efficiency of a 1MB 16-way set associative LLC with LRU replacement for the SPEC CPU 2006 benchmark 456.hmmer. The amount of time each cache block is live is shown as a greyscale intensity. Figure <ref type="figure" target="#fig_0">1(a)</ref> shows the unoptimized cache. The darkness shows that many blocks remain dead for a long time. Figure <ref type="figure" target="#fig_0">1(b)</ref> shows improvement in efficiency by driving a replacement and bypass policy with a sampling dead block predictor.</p><p>Dead blocks lead to poor cache efficiency <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b3">[4]</ref>. In the least-recently-used (LRU) replacement policy, after the last access, each dead block wastes time moving from the mostrecently-used (MRU) to the LRU position before it is evicted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dead Block Prediction</head><p>Dead block prediction can be used to identify blocks that are likely to be dead to drive optimizations that replace them with live data. Performance improves as more live blocks lead to more cache hits. However, current dead block prediction algorithms have a number of problems that make them unsuitable for the LLC:</p><p>? They incur a substantial overhead in terms of prediction structures as well as extra cache metadata. For instance, each cache block must be associated with many extra bits of metadata that can change as often as every access to that block. Thus, the improvement in cache efficiency is paid for with extra power and area requirements.</p><p>? They rely on an underlying LRU replacement algorithm. However, LRU is prohibitively expensive to implement in a highly associative LLC. Note that this problem extends to other recently proposed improvements to caches, including adaptive insertion policies <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b6">[7]</ref>.</p><p>? Due to the large number of memory and instruction references tracked by these predictors, they must be either very large or experience a significant amount of destructive interference in their prediction tables resulting in a negative impact on accuracy.</p><p>? Predictors that use instruction traces do not work in a realistic scenario involving L1, L2, and L3 caches because a moderately-sized mid-level cache filters out most of the temporal locality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Sampling Dead Block Predictor</head><p>This paper presents dead block prediction technique based on sampling. Previous dead block predictors find correlations between observed patterns of memory access instructions and cache evictions, learning when program behavior is likely to lead to a block becoming dead. The new prediction technique exploits four key observations, allowing it to use far less power and area while improving accuracy over previous techniques:</p><p>? Memory access patterns are consistent across sets, so it is sufficient to sample a small fraction of references to sets to do accurate prediction. For example, in a 2MB cache with 2,048 sets, sampling references to 1.6% of sets is sufficient to predict with accuracy superior other schemes that learn from every reference. The predictor keeps track of a small number of sets using partial tags replaced by the LRU policy, allowing it to generalize predictions not only to a large LRU cache, but also to a large randomly replaced cache.</p><p>? A separate sampling structure, kept outside the LLC, can be configured differently to provide improved accuracy.</p><p>For instance, we find that, for a 16-way set associative LLC, a 12-way associative sampler provides superior accuracy while consuming less state.</p><p>? Previous predictors use reference traces or counters for each block. However, simply using the address of the last memory access instruction provides sufficient prediction accuracy while obviating the need to keep track of access patterns for all blocks. Indeed, we find that tracebased predictors do not work when a mid-level cache filters much of the temporal locality from the stream of memory instructions that access the LLC. Metadata associated with cache blocks is significantly reduced with a proportional reduction in power.</p><p>? Dead block predictor accuracy can be improved by using a skewed organization inspired by branch prediction research.</p><p>In this paper, sampling prediction is explored in the context of a dead block replacement and bypass policy. That is, the replacement policy will choose a dead block to be replaced before falling back on a default replacement policy such as random or LRU, and a block that is predicted "dead on arrival" will not be placed, i.e., it will bypass the LLC.</p><p>The sampling predictor is well-suited to improve the performance of a cache with a default random replacement policy. Sampling prediction reduces average cache misses by 7.5% compared with no improvement for a previously proposed predictor based on counting. This reduction in cache misses for a combination random/dead-block replaced cache results in a 3.4% average speedup over a baseline LRU cache. Other recent placement and replacement policies depend on the LRU policy. The sampling predictor does not.</p><p>Because of the small amount of predictor state and cache metadata, the sampling predictor consumes less than half the leakage power of a counting predictor. It uses only 3.1% of the dynamic power of a baseline cache on average, compared with 11% for the counting predictor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dead Block Predictors</head><p>Previous work introduced several dead block predictors and applied them to problems such as prefetching and block replacement <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b0">[1]</ref>.</p><p>1) Trace Based Predictor: Dead block prediction was introduced by Lai et al. <ref type="bibr" target="#b12">[13]</ref>. The Lai et al. predictor is used to prefetch data into dead blocks in the L1 data cache. This reference trace predictor (hereafter reftrace) collects a trace of instructions addresses that access a particular block on the theory that, if a trace leads to the last access for one block then the same trace will lead to the last access for other blocks. The signature, or truncated sum of these instruction addresses, is used to index a prediction table. A trace based predictor is also used to optimize a cache coherence protocol <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b22">[23]</ref> and perform dynamic self-invalidation <ref type="bibr" target="#b13">[14]</ref>. A skewed tracebased predictor is used to identify a pool of dead blocks in the L2 cache to be used as a "virtual victim cache" into which LRU victims from hot sets can be stored <ref type="bibr" target="#b9">[10]</ref>.</p><p>2) Time-Based Predictor: Hu et al. propose a time-based dead block predictor <ref type="bibr" target="#b4">[5]</ref> that learns the number of cycles a block is live and predicts it dead if it is not accessed for twice that number of cycles. This predictor is used to prefetch into the L1 cache and filter a victim cache. Abella et al. propose a similar predictor <ref type="bibr" target="#b0">[1]</ref> based on number of references rather than cycles for reducing cache leakage.</p><p>3) Cache Burst Predictor: Cache bursts <ref type="bibr" target="#b14">[15]</ref> can be used with trace, counting, or time based dead block predictors. A cache burst consists of all the contiguous accesses to a block while in the MRU position. The predictor predicts and updates only on each burst rather than on each reference. Cache bursts predictors limit the number of accesses and updates to the prediction table for L1 caches, improving power relative to previous work. Our sampling predictor also reduces the number of updates. However, cache bursts have been shown to offer little advantage for higher level caches, since most bursts are filtered out by the L1. Furthermore, cache bursts predictors also require significant additional metadata in the cache, while our predictor requires only one additional bit per cache block.</p><p>4) Counting-Based Predictor: Kharbutli and Solihin propose counting-based predictors for the L2 cache. The Livetime Predictor (LvP) tracks the number of accesses to each block. This value is stored in the predictor on eviction. A block is predicted dead if it has been accessed more often than the previous generation. Each predictor entry has a onebit confidence counter so that a block is predicted dead if it has been accessed the same number of times in the last two generations. The predictor table is a matrix of access and confidence counters. The rows are indexed using the hashed PC that brought the block into the cache and the columns are indexed using the hashed block address. An Access Interval Predictor (AIP) is also described in the same paper, but we focus on LvP as we find it delivers superior accuracy. The counting based replacement policy chooses the predicted dead block closest to LRU as a victim, or the LRU block if there is no dead block. LvP and AIP are also used to bypass cache blocks which are brought to the cache and never accessed again.</p><p>5) Other Predictors: Another kind of dead block prediction involves predicting in software <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b20">[21]</ref>. In this approach the compiler collects dead block information and provides hints to the microarchitecture to make cache decisions. If a cache block is likely to be reused again it hints to keep the block in the cache; otherwise, it hints to evict the block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Sampling for Cache Placement and Replacement Policy</head><p>Dynamic insertion policy (DIP) uses set dueling to adaptively insert blocks in either the MRU or LRU position depending on which policy gives better performance <ref type="bibr" target="#b18">[19]</ref>. Performance is sampled through a small number of dedicated sets, half using MRU placement and the other half using LRU placement. Our predictor also uses sampling to dynamically learn from program behavior, but in addition to sampling the access characteristics of data, it also samples the instructions that lead to that program behavior. We compare our dead-block-predictor-driven replacement and bypass policy to adaptive insertion in Section VII. A memory-level parallelism aware cache replacement policy also uses set-dueling to do sampling, relying on the fact that isolated misses are more costly for performance than parallel misses <ref type="bibr" target="#b19">[20]</ref>. Thread Aware Dynamic Insertion Policy (TADIP) uses DIP in a multi-core context <ref type="bibr" target="#b6">[7]</ref>. It takes into account the memory requirement of each executing program. It has a dedicated leader set for each core for determining the correct insertion position (MRU/LRU). This way thrashing workloads decide to insert in the LRU position while cache-friendly workloads continue to insert in the MRU position.</p><p>Keramidas et al. <ref type="bibr" target="#b8">[9]</ref> proposed a cache replacement policy that uses sampling-based reuse distance prediction. This policy tries to evict cache blocks that will be reused furthest in the future.</p><p>LRU replacement predicts that a block will be referenced in the near future. Re-reference Interval Prediction (RRIP) categorizes blocks as near re-reference, distant re-reference and long re-reference interval blocks <ref type="bibr" target="#b7">[8]</ref>. On a miss the block that is predicted to be referenced most far in the future is replaced. Set-dueling is used where one policy inserts blocks with distant re-reference prediction and other one inserts majority of blocks with distant re-reference prediction and infrequently inserts new blocks with a long re-reference interval. RRIP prevents blocks with distant re-reference interval from evicting blocks that have a near re-reference interval. An extension of RRIP to multi-core shared cache is analoguous to DIP for shared cache. Each core selects the best re-reference interval (long or distant) using set dueling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. A SAMPLING DEAD BLOCK PREDICTOR</head><p>In this section we discuss the design of a new samplingbased dead block predictor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. A Sampling Partial Tag Array</head><p>The sampling predictor keeps a small partial tag array, or sampler. Each set in the sampler corresponds to a selected set in the cache; e.g. if the original cache has 2,048 sets, the sampler could keep 32 sets corresponding to every 2, 048/32 = 64 th cache set. Since correctness of matches is not necessary in the sampler tag array, only the lower-order 15 bits of tags are stored to conserve area and energy. (Nevertheless, we observed no incorrect matches in any of the benchmarks; 15-bit tags are quite sufficient for this application.) As with other dead block predictors, each access to the LLC incurs an access to the predictor. However, predictor is only updated when there is an access or replacement in a cache set with a corresponding sampler set. This strategy works because the learning acquired through sampling a few sets generalizes to the entire cache.</p><p>We find that predictor accuracy improves slightly as more sets are added to the sampler, although too many sets can increase destructive interference in the prediction tables. We would like to have as few sets as possible to save power. We find that 32 sets provide a good trade-off between accuracy and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Advantages of a Sampler</head><p>A sampler decouples the prediction mechanism from the structure of the cache, offering several advantages over previous predictors:</p><p>1) Since the predictor and sampler are only updated on a small fraction of cache accesses and replacements, the power requirement of the predictor is reduced.</p><p>2) The replacement policy of the sampler does not have to match that of the cache. For instance, the LLC may use the less costly random replacement policy, but the sampler can still use the deterministic LRU policy. A deterministic policy is easier to learn from because the same sequence of references comes up consistently, uninterrupted by random evictions.</p><p>3) The associativity of the sampler does not have to match that of the original cache. We have found that, with a 16-way LLC, a 12-way sampler offers better prediction accuracy than a 16-way sampler since blocks that are likely to be dead are evicted sooner. Also, a 12-way sampler consumes less storage and power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Advantage Over Previous Predictors</head><p>The reftrace predictor keeps a separate signature for every cache block to be used by the predictor when that block is accessed. Counting predictors also store counts with each block. These predictors, as well as cache bursts, have two problems: 1) Every cache block must be associated with a significant amount of metadata. Reftrace requires keeping the signature for each block, while counting predictors require keeping a count and other data for each block. 2) Every update to a block incurs a read/modify/write cycle for the metadata. Either a count or a signature must be read, modified, and written. Meeting the timing constraints of this sequence of operations could be problematic, especially in a low-power design.</p><p>However, the new predictor uses a trace based only on the PC. That is, rather than predicting whether a block is dead based on the trace of instructions that refer to that block, the predictor only uses the PC of the last instruction that accessed a block. Thus, all predictor state can be kept in the predictor, and only a single additional bit of metadata is needed for each cache block -a bit that indicates whether the block has been predicted as dead. Trace metadata is still stored in the sampler, but since the sample tag array is far smaller than the actual LLC tag array, area and timing are not a problem. The predictor state is only modified on the small fraction of accesses to sampler sets.</p><p>Figure <ref type="figure">2</ref> gives a block diagram of the reftrace and sampling predictors, showing the times during which they are accessed and updated. The sampling predictor is accessed as frequently as the reftrace predictor, but it is updated far less often.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Key Difference with PC-Only Sampler</head><p>The reftrace predictor could also use just the PC to index its prediction table. Each time a block is accessed, the signature could immediately be used to update the predictor indicating that the block is live. However, reftrace would still need to keep a signature for each cache block for the time between the last access to the block and the eviction of the block to update the predictor on an eviction. In addition to the storage requirement, reftrace would also need a 16-bit channel between the stream of instructions and the LLC.</p><p>The sampling predictor does not have this problem. It only needs to keep the PC signature for tags tracked in the sampler. There are far fewer of these sampler signatures, i.e. 1,536 for a 12-way 32-set sampler, compared with the number of signatures for the entire cache with the reftrace predictor, i.e. 32,768. The sampler needs only a one-bit channel to the LLC to provide predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. A Skewed Organization</head><p>The sampling predictor uses the idea of a skewed organization <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b15">[16]</ref> to reduce the impact of conflicts in the table. The predictor keeps three 4,096-entry tables of 2bit counters, each indexed by a different hash of a 15-bit signature. Each access to the predictor yields three counter values whose sum is used as a confidence compared with a threshold; if the threshold is met, then the corresponding block is predicted dead. This organization offers an improvement over the previous dead block predictors because unrelated signatures might conflict in one table but are less likely to conflict in three tables, so the effect of destructive conflicts is reduced. A happy consequence of using the skewed predictor is more sensitive confidence estimation: with three tables, we have nine confidence levels to choose from instead of just four with a single table. We find that a threshold of eight gives the best accuracy. Figure <ref type="figure" target="#fig_1">3</ref> shows the difference in the design of the reftrace and skewed predictors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Multiple Cores</head><p>The sampling predictor described in this paper is used unmodified for both single-thread and multi-core workloads. The same 32-set sampling predictor is used for the 2MB single-core cache as well as the 8MB quad-core cache. There is no special tuning for multi-core workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. A COMPARISON OF PREDICTOR STORAGE AND POWER</head><p>In this section, we discuss the storage requirements of dead block predictors: the reftrace predictor, the counting predictor, the sampling predictor.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Reference Trace Predictor</head><p>For this study, we use a reftrace predictor indexed using 15-bit signature. Thus, the prediction table contains 2 15 twobit counters, or 8KB. We find diminishing returns in terms of accuracy from larger tables, so we limit our reftrace predictor to this size. Each cache block is associated with two extra fields: the 15-bit signature arising from the most recent sequence of accesses to that block, and another bit that indicates whether the block has been predicted as dead. With a 2MB cache with 64B blocks, this works out to 64KB of extra metadata in the cache. Thus, the total amount of state for the reftrace predictor is 72KB, or 3.5% of the data capacity of the LLC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Counting Predictor</head><p>The counting-based Live-time Predictor (LvP) predictor <ref type="bibr" target="#b10">[11]</ref> is a 256 ? 256 table of entries, each of which includes the following fields: a four-bit counter keeping track of the number of accesses to a block and a one-bit confidence counter. Thus, the predictor uses 40KB of storage. In addition, each cache block is augmented with the following metadata: an eight-bit hashed PC, a four-bit counter keeping track of the number of times the cache block is accessed, a four-bit number of accesses to the block from the last time the block was in the cache, and a one-bit confidence counter. This works out to approximately 68KB of extra metadata in the cache. Thus, the total amount of state for the counting predictor is 108KB, or 5.3% of the data capacity of the LLC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Sampling Predictor</head><p>The three prediction tables for the skewed dead block predictor are each 4,096 two-bit counters so they consume 3KB of storage.</p><p>We model a sampler with 32 sets. Each set has 12 entries consisting of 15-bit partial tags, 15-bit partial PCs, one prediction bit, one valid bit, and four bits to maintain LRU position information, consuming 6.75KB of storage<ref type="foot" target="#foot_0">1</ref> . Each cache line also holds one extra bit of metadata. Thus, the sampling predictor consumes 13.75KB of storage, which is less than 1% of the capacity of a 2MB LLC.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Predictor Power</head><p>The sampling predictor uses far less storage than the other predictors, but part of its design includes sets of associative tags. Thus, it behooves us to account for the potential impact of this structure on power. Table <ref type="table" target="#tab_4">II</ref> shows the results of CACTI 5.3 simulations <ref type="bibr" target="#b23">[24]</ref> to determine the leakage and dynamic power of the various components of each predictor. The sampler was modeled as the tag array of a cache with as many sets as the sampler, with only the tag power being reported. The predictor tables for the pattern sample predictors was modeled as a tagless RAM with three banks accessed simultaneously, while the predictor table for the reftrace predictor was modeled as a single bank 8KB tagless RAM. The prediction table for the counting predictor was conservatively modeled as a 32KB tagless RAM. To attribute extra power to cache metadata, we modeled the 2MB LLC both with and without the extra metadata, represented as extra bits in the data array, and report the difference between the two.</p><p>1) Dynamic Power: When it is being accessed, the dynamic power of the sampling predictor is 57% of the dynamic power of the reftrace predictor, and only 28% of the dynamic power of the counting predictor. The baseline LLC itself has a dynamic power of 2.75W. Thus, the sampling predictor consumes 3.1% of the power budget of the baseline cache, while the counting predictor consumes 11% of it. Note that CACTI reports peak dynamic power. Since the sampler is accessed only on a small fraction of LLC accesses, the actual dynamic power for the sampling predictor would far lower.</p><p>2) Leakage Power: For many programs, dynamic power of the predictors will not be an important issue since the LLC might be accessed infrequently compared with other structures. However, leakage power is always a concern. The sampling predictor has a leakage power that is only 40% of the reftrace predictor, and only 25% of the counting predictor. This is primarily due to the reduction in cache metadata required by the predictor. As a percentage of the 0.512W total leakage power of the LLC, the sampling predictor uses only 1.2%, while the counting predictor uses 4.7% and the reftrace predictor uses 2.9%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Latency</head><p>CACTI simulations show that the latency reading and writing the structures related to the sampling predictor fit well within the timing constraints of the LLC. It is particularly fast compared with the reftrace predictor since it does not need to read/modify/write metadata in the LLC cache on every access.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. A DEAD-BLOCK DRIVEN REPLACEMENT AND BYPASS POLICY</head><p>We evaluate dead block predictors in the context of a combined dead block replacement and bypassing optimization <ref type="bibr" target="#b10">[11]</ref>. When it is time to choose a victim block, a predicted dead block may be chosen instead of a random or LRU block. If there is no predicted dead block, a random or LRU block may be evicted.</p><p>If a block to be placed in a set will be used further in the future than any block currently in the set, then it makes sense to decline placing it <ref type="bibr" target="#b16">[17]</ref>. That is, the block should bypass the cache. Bypassing can reduce misses in LLCs, especially for programs where most of the temporal locality is captured by the first-level cache. Dead block predictors can be used to implement bypassing: if a block is predicted dead on its first access then it is not placed in the cache.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dead Block Replacement and Bypassing with Default Random Replacement</head><p>In Section VII, we show results for dead-block replacement and bypass using a default random replacement policy. We show that this scheme has very low overhead and significant performance and power advantages.</p><p>What does it mean for a block to be dead in a randomly replaced cache? The concept of a dead block is well-defined even for a randomly-replaced cache: a block is dead if it will be evicted before it is used again. However, predicting whether a block is dead is now a matter of predicting the outcome of a random event. The goal is not necessarily to identify with 100% certainty which block will be evicted before it is used next, but to identify a block that has a high probability of not being used again soon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Predictor Update in the Optimization</head><p>One question naturally arises: should the predictor learn from evictions that it caused? We find that for reftrace and for the sampling predictor, allowing the predictor to learn from its own evictions results in slightly improved average miss rates and performance over not doing so. We believe that this feedback allows the predictor to more quickly generalize patterns learned for some sets to other sets. On the other hand, we find no benefit from letting a tag "bypass" the sampler, i.e., tags from all accesses to sampled sets are placed into the sampler.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTAL METHODOLOGY</head><p>This section outlines the experimental methodology used in this study.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Simulation Environment</head><p>The simulator is a modified version of CMP$im, a memorysystem simulator that is accurate to within 4% of a detailed cycle-accurate simulator <ref type="bibr" target="#b5">[6]</ref>. The version we used was provided with the JILP Cache Replacement Championship <ref type="bibr" target="#b1">[2]</ref>. It models an out-of-order 4-wide 8-stage pipeline with a 128-entry instruction window. This infrastructure enables collecting instructions-per-cycle figures as well as misses per kilo-instruction and dead block predictor accuracy. The experiments model a 16-way set-associative last-level cache to remain consistent with other previous work <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. The microarchitectural parameters closely model Intel Core i7 (Nehalem) with the following parameters: L1 data cache: 32KB 8-way associative, L2 unified cache: 256KB 8-way L3: 2MB/core. Each benchmark is compiled for the x86 64 instruction set. The programs are compiled with the GCC 4.1.0 compilers for C, C++, and FORTRAN.</p><p>We use SPEC CPU 2006 benchmarks. We use SimPoint <ref type="bibr" target="#b17">[18]</ref> to identify a single one billion instruction characteristic interval (i.e.simpoint) of each benchmark. Each benchmark is run with the first ref input provided by the runspec command.</p><p>1) Single-Thread Workloads: For single-core experiments, the infrastructure simulates one billion instructions. We simu-late a 2MB LLC for the single-thread workloads. In keeping with the methodology of recent cache papers <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, we choose a memory-intensive subset of the benchmarks. We use the following criterion: a benchmark is included in the subset if the number of misses in the LLC decreases by at least 1% when using the optimal replacement and bypass policy instead of LRU <ref type="foot" target="#foot_1">2</ref> .</p><p>Table <ref type="table" target="#tab_5">III</ref> shows each benchmark with the baseline LLC misses per 1000 instructions (MPKI), optimal MPKI, baseline instructions-per-cycle (IPC), and the number of instructions fast-forwarded (FFWD) to reach the interval given by Sim-Point.</p><p>2) Multi-Core Workloads: Table <ref type="table" target="#tab_7">IV</ref> shows ten mixes of SPEC CPU 2006 simpoints chosen four at a time with a variety of memory behaviors characterized in the table by cache sensitivity curves. We use these mixes for quad-core simulations. Each benchmark runs simultaneously with the others, restarting after one billion instructions, until all of the benchmarks have executed at least one billion instructions. We simulate an 8MB shared LLC for the multi-core workloads.  For the multi-core workloads, we report the weighted speedup normalized to LRU. That is, for each thread i sharing the 8MB cache, we compute IPC i . Then we find SingleIPC i as the IPC of the same program running in isolation with an 8MB cache with LRU replacement. Then we compute the weighted IPC as IPC i /SingleIPC i . We then normalize this weighted IPC with the weighted IPC using the LRU replacement policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Optimal Replacement and Bypass Policy</head><p>For simulating misses, we also compare with an optimal block replacement and bypass policy. That is, we enhance Belady's MIN replacement policy <ref type="bibr" target="#b2">[3]</ref> with a bypass policy that refuses to place a block in a set when that block's next access will not occur until after the next accesses to all other blocks in the set. We use trace-based simulation to determine the optimal number of misses using the same sequence of memory accesses made by the out-of-order simulator. The outof-order simulator does not include the optimal replacement and bypass policy so we report optimal numbers only for cache miss reduction and not for speedup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. EXPERIMENTAL RESULTS</head><p>In this section we discuss results of our experiments. In the graphs that follow, several techniques are referred to with abbreviated names. Table <ref type="table" target="#tab_9">V</ref> gives a legend for these names.</p><p>For TDBP and CDBP, we simulate a dead block bypass and replacement policy just as described previously, dropping in the reftrace and counting predictors, respectively, in place of our sampling predictor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dead Block Replacement with LRU Baseline</head><p>We explore the use of sampling prediction to drive replacement and bypass in a default LRU replaced cache compared with several other techniques for the single-thread benchmarks.</p><p>1) LLC Misses: Figure <ref type="figure" target="#fig_2">4</ref> shows LLC cache misses normalized to a 2MB LRU cache for each benchmark. On average, dynamic insertion (DIP) reduces cache misses to 93.9% of the baseline LRU, a reduction of by 6.1%. RRIP reduces misses by 8.1%. The reftrace-predictor-driven policy (TDBP) increases average misses on average by 8.0% (mostly due to 473.astar), decreasing misses on only 11 of the 19 benchmarks. CDBP reduces average misses by 4.6%. The sampling predictor reduces average misses by 11.7%. The optimal policy reduces misses by 18.6% over LRU; thus, the sampling predictor achieves 63% of the improvement of the optimal policy.</p><p>2) Speedup: Reducing cache misses translates into improved performance. Figure <ref type="figure">5</ref> shows the speedup (i.e. new IPC divided by old IPC) over LRU for the predictor-driven policies with a default LRU cache.</p><p>DIP improves performance by a geometric mean of 3.1%. TDBP provides a speedup on some benchmarks and a slowdown on others, resulting in a geometric mean speedup of approximately 0%. The counting predictor delivers a geometric mean speedup of 2.3%, and does not significantly slow down any benchmarks. RRIP yields an average speedup of 4.1%. The sampling predictor gives a geometric mean speedup of 5.9%. It improves performance by at least 4% for eight of the benchmarks, as opposed to only five benchmarks for RRIP and CDBP and two for TDBP. The sampling predictor delivers performance superior to each of the other techniques tested.</p><p>Speedup and cache misses are particularly poor for 473.astar. As we will see in Section VII-C, dead block prediction accuracy is bad for this benchmark. However, the sampling predictor minimizes the damage by making fewer predictions than the other predictors.</p><p>3) Poor Performance for Trace-Based Predictor: Note that the reftrace predictor performs quite poorly compared with its observed behavior in previous work <ref type="bibr" target="#b14">[15]</ref>. In that work, reftrace was used for L1 or L2 caches with significant temporal locality in streams of reference reaching the predictor. Reftrace learns from these streams of temporal locality. In this work, the predictor optimizes the LLC in which most temporal locality has been filtered by the 256KB middle-level cache. In this situation, it is easier for the predictor to try to simply learn the last PC to reference a block rather than a sparse reference    Fig. <ref type="figure">5</ref>: Speedup for various policies trace that might not be repeated often enough to learn from. Note that we have simulated reftrace correctly with access to the original source code used for the cache bursts paper. In our simulations and in previous work, reftrace works quite well when there is no middle-level cache to filter the temporal locality between the small L1 and large LLC. However, many real systems have middle-level caches. 4) Contribution of Components: Aside from using only the last PC, there are three other components that contribute to our predictor's performance with dead block replacement and bypass (DBRB): 1) using a sampler, 2) using reduced associativity in the sampler, and 3) using a skewed predictor. Figure <ref type="figure" target="#fig_4">6</ref> shows the speedup achieved on the single-thread benchmarks for every feasible combination of presence or absence of these components. We find that these three components interact synergistically to improve performance.</p><p>The PC-only predictor ("DBRB alone") without any of the other enhancements achieves a speedup of 3.4% over the LRU baseline. This predictor is equivalent to the reftrace predictor using the last PC instead of the trace signature. Adding a skewed predictor with three tables ("DBRB+3 tables"), each one-fourth the size of the single-table predictor, results in a reduced speedup of 2.3%. The advantage of a skewed predictor is its ability to improve accuracy in the presence of a moderate amount of conflict. However, with no sampler to filter the onslaught of a large working set of PCs, the skewed predictor experiences significant conflict with a commensurate reduction in coverage and accuracy.</p><p>The sampler with no other enhancements ("DBRB+sampler") yields a speedup of 3.8%. The improvement over DBRB-only is due to the filtering effect on the predictor: learning from far fewer examples is sufficient to learn the general behavior of the program, but results in much less conflict in the prediction table. Adding the skewed predictor to this scenarion ("DBRB+sampler+3 tables") slightly improves speedup to 4.0%, addressing the remaining moderate conflict in the predictor.</p><p>Reducing the associativity in the sampler from 16 to 12 gives a significant bump in performance. With no skewed predictor, the speedup ("DBRB+sampler+12-way") improves to 5.6%. Reducing the associativity in the predictor allows the sampler to learn more quickly as tags spend less time in sampler sets. Putting together all three components results in the speedup of 5.9%. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dead Block Replacement with Random Baseline</head><p>In this subsection we explore the use of the sampling predictor to do dead-block replacement and bypass with a baseline randomly-replaced cache. When a block is replaced, a predicted dead block is chosen, or if there is none, a random block is chosen. We compare with the CDBP and random replacement. We do not compare with TDBP, RRIP or DIP because these policies depend on a baseline LRU replacement policy and become meaningless in a randomly-replaced cache.</p><p>1) LLC Misses: Figure <ref type="figure">7</ref> shows the normalized number of cache misses for the various policies with a default randomreplacement policy. The figures are normalized to the same baseline LRU cache from the previous graphs, so the numbers are directly comparable. CDBP reduces misses for some benchmarks but increases misses for others, resulting in no net benefit. Random replacement by itself increases misses an average of 2.5% over the LRU baseline. The sampling predictor yields an average normalized MPKI of 0.925, an improvement of 7.5% over the LRU baseline. Note that the sampling predictor with a default random-replacement policy requires only one bit of metadata associated with individual cache lines. Amortizing the cost of the predictor storage over the cache lines (computed in Section IV), the replacement and bypass policy requires only 1.71 bits per cache line to deliver 7.5% fewer misses than the LRU policy.</p><p>2) Speedup: Figure <ref type="figure">8</ref> shows the speedup for the predictordriven policies with a default random cache. CDBP yields an almost negligible speedup of 0.1%. Random replacement by itself results in a 1.1% slowdown. The sampling predictor gives a speedup of 3.4% over the LRU baseline. Thus, a sampling predictor can be used to improve performance with a default randomly-replaced cache.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Prediction Accuracy and Coverage</head><p>Mispredictions come in two varieties: false positives and false negatives. False positives are more harmful because they wrongly allow an optimization to use a live block for some other purpose, causing a miss. The coverage of a predictor is ration of positive predictions to all predictions. If a predictor is consulted on every cache access, then the coverage is the fraction of cache accesses when the optimization may be applied. Higher coverage means more opportunity for the optimization. Figure <ref type="figure">9</ref> shows the coverage and false positive rates for the various predictors given a default LRU cache. On average, reftrace predicts that a block is dead for 88% of LLC accesses, and is wrong about that prediction for 19.9% of cache accesses. The counting predictor has a coverage of 67% and is wrong 7.19% of the time. The sampling predictor has a coverage of 59% and a low false positive rate of 3.0%, explaining why it has the highest average speedup among the predictors. The benchmark 473.astar exhibits apparently unpredictable behavior. No predictor has good accuracy for this benchmark. However, the sampling predictor has very low coverage for this benchmark, minimizing the damage caused by potential false positives. The normalized weighted speedup over all 10 workloads ranges from 6.4% to 24.2% for the sampler, with a geometric mean of 12.5%, compared with 10% for CDBP, 7.6% for TADIP, 5.6% for TDBP, and 4.5% for the multi-core version of RRIP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Multiple Cores Sharing a Last-Level Cache</head><p>Figure <ref type="figure" target="#fig_5">10</ref>(b) shows the normalized weighted speedup for techniques with a default random policy. The speedups are still normalized to a default 8MB LRU cache. The random sampler achieves a geometric mean normalized weighted speedup of 7%, compared with 6% for random CDBP and no benefit for random replacement by itself. All of the other techniques discussed rely on the presence of a default LRU policy and do not make sense in the context of a default random replacement policy.</p><p>The average normalized MPKIs (not graphed for lack of space) are 0.77 for the sampler, 0.79 for CDBP, 0.85 for TADIP, 0.95 for TDBP, 0.82 for the random sampler, 0.93 for multi-core RRIP, and 0.84 for random CDBP. The sampler reduces misses by 23% on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION AND FUTURE WORK</head><p>Sampling prediction can improve performance for last-level caches while reducing the power requirements over previous techniques. For future work, we plan to investigate sampling techniques for counting predictors as well as cache-bursts predictors <ref type="bibr" target="#b14">[15]</ref> at all levels of the memory hierarchy. We plan to evaluate multi-threaded workloads with significant sharing of data to see what improvements can be made to the sampling predictor in this context. The skewed organization boosts the accuracy of the sampling predictor. In future work we plan to apply other techniques derived from branch prediction to dead block prediction. We plan to investigate the use of sampling predictors for optimizations other than replacement and bypass.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Dead block replacement and bypass bring the cache to life. Efficiency (i.e. live time ratio) shown as greyscale intensities for 456.hmmer for (a) a 1MB LRU cache and (b) a dead-block-replaced cache using a sampling predictor. Darker blocks are dead longer. Efficiency is 22% for (a) and 87% for (b).</figDesc><graphic url="image-8.png" coords="2,71.78,73.74,172.59,162.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Block diagrams for dead block predictors</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Reduction in LLC misses for various policies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Contribution to speedup of sampling, reduced associativity, and skewed prediction</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 10 (</head><label>10</label><figDesc>Figure 10(a) shows the normalized weighted speedup achieved by the various techniques on the multi-core workloads with an 8MB last-level cache and a default LRU policy.The normalized weighted speedup over all 10 workloads ranges from 6.4% to 24.2% for the sampler, with a geometric mean of 12.5%, compared with 10% for CDBP, 7.6% for TADIP, 5.6% for TDBP, and 4.5% for the multi-core version of RRIP.Figure10(b) shows the normalized weighted speedup for techniques with a default random policy. The speedups are still normalized to a default 8MB LRU cache. The random sampler achieves a geometric mean normalized weighted speedup of 7%, compared with 6% for random CDBP and no benefit for random replacement by itself. All of the other techniques discussed rely on the presence of a default LRU policy and do not make sense in the context of a default random replacement policy.The average normalized MPKIs (not graphed for lack of space) are 0.77 for the sampler, 0.79 for CDBP, 0.85 for TADIP, 0.95 for TDBP, 0.82 for the random sampler, 0.93 for multi-core RRIP, and 0.84 for random CDBP. The sampler reduces misses by 23% on average.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 : 3 Fig. 8 :Fig. 9 :</head><label>7389</label><figDesc>Fig. 7: LLC misses per kilo-instruction for various policies</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 :</head><label>10</label><figDesc>Fig. 10: Weighted speedup for multi-core workloads normalized to LRU for (a) a default LRU cache and (b) a default randomly replaced cache.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Reftrace dead block predictor (a), and new dead block predictor with sampler tag array (b). The sampler and dead block predictor table are updated for 1.6% of the accesses to the LLC, while the reftrace predictor is updated on every access.</figDesc><table><row><cell></cell><cell></cell><cell>L2 Cache</cell><cell>Data access</cell></row><row><cell>Every LLC eviction, for training Prediction prediction and training Predictor Table Every L2 miss, for L2 Cache Data access</cell><cell>Last-Level Cache</cell><cell cols="2">Table Predictor and evictions accesses Sampler prediction Every L2 miss, for 32 sets tag array Sampler only Selected L2 misses, Prediction &lt; 1.6% of LLC accesses for training only,</cell><cell>2048 sets, tags + data Last-Level Cache</cell></row><row><cell>(a)</cell><cell></cell><cell></cell><cell>(b)</cell></row><row><cell>Fig. 2:</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Table I summarizes the storage requirements of each predictor.</figDesc><table><row><cell>Predictor</cell><cell>Predictor Structures</cell><cell>Cache Metadata</cell><cell>Total Storage</cell></row><row><cell>reftrace</cell><cell>8KB table</cell><cell>16 bits ? 32K blocks = 64KB</cell><cell>72KB</cell></row><row><cell>counting</cell><cell>256 ? 256 table, 5-bit entries = 40KB</cell><cell>17 bits ? 32K blocks = 68KB</cell><cell>108KB</cell></row><row><cell>sampler</cell><cell>3 ? 1KB tables + 6.75KB sampler = 9.75</cell><cell>1 bit ? 32K blocks = 4KB</cell><cell>13.75KB</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I :</head><label>I</label><figDesc>Storage overhead for the various predictors</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II :</head><label>II</label><figDesc>Dynamic and leakage power for predictor components. All figures are in Watts.</figDesc><table><row><cell></cell><cell cols="2">MPKI MPKI</cell><cell>IPC</cell><cell></cell><cell></cell><cell cols="2">MPKI MPKI</cell><cell>IPC</cell><cell></cell></row><row><cell>Name</cell><cell cols="5">(LRU) (MIN) (LRU) FFWD Name</cell><cell cols="4">(LRU) (MIN) (LRU) FFWD</cell></row><row><cell>astar</cell><cell>2.275</cell><cell>2.062</cell><cell>1.829</cell><cell>185B</cell><cell>bwaves</cell><cell>0.088</cell><cell>0.088</cell><cell>3.918</cell><cell>680B</cell></row><row><cell>bzip2</cell><cell>0.836</cell><cell>0.589</cell><cell>2.713</cell><cell>368B</cell><cell cols="4">cactusADM 13.529 13.348 1.088</cell><cell>81B</cell></row><row><cell>calculix</cell><cell>0.006</cell><cell>0.006</cell><cell>3.976</cell><cell>4433B</cell><cell>dealII</cell><cell>0.031</cell><cell>0.031</cell><cell>3.844</cell><cell>1387B</cell></row><row><cell>gamess</cell><cell>0.005</cell><cell>0.005</cell><cell>3.888</cell><cell>48B</cell><cell>gcc</cell><cell>0.640</cell><cell>0.524</cell><cell>2.879</cell><cell>64B</cell></row><row><cell cols="4">GemsFDTD 13.208 10.846 0.818</cell><cell>1060B</cell><cell>gobmk</cell><cell>0.121</cell><cell>0.121</cell><cell>3.017</cell><cell>133B</cell></row><row><cell>gromacs</cell><cell>0.357</cell><cell>0.336</cell><cell>3.061</cell><cell>1B</cell><cell>h264ref</cell><cell>0.060</cell><cell>0.060</cell><cell>3.699</cell><cell>8B</cell></row><row><cell>hmmer</cell><cell>1.032</cell><cell>0.609</cell><cell>3.017</cell><cell>942B</cell><cell>lbm</cell><cell cols="3">25.189 20.803 0.891</cell><cell>13B</cell></row><row><cell>leslie3d</cell><cell>7.231</cell><cell>5.898</cell><cell>0.931</cell><cell>176B</cell><cell cols="3">libquantum 23.729 22.64</cell><cell>0.558</cell><cell>2666B</cell></row><row><cell>mcf</cell><cell cols="3">56.755 45.061 0.298</cell><cell>370B</cell><cell>milc</cell><cell cols="3">15.624 15.392 0.696</cell><cell>272B</cell></row><row><cell>namd</cell><cell>0.047</cell><cell>0.047</cell><cell>3.809</cell><cell cols="2">1527B omnetpp</cell><cell cols="3">13.594 10.470 0.577</cell><cell>477B</cell></row><row><cell>perlbench</cell><cell>0.789</cell><cell>0.628</cell><cell>2.175</cell><cell>541B</cell><cell>povray</cell><cell>0.004</cell><cell>0.004</cell><cell>2.908</cell><cell>160B</cell></row><row><cell>sjeng</cell><cell>0.318</cell><cell>0.317</cell><cell>3.156</cell><cell>477B</cell><cell>soplex</cell><cell cols="3">25.242 16.848 0.559</cell><cell>382B</cell></row><row><cell>sphinx3</cell><cell cols="2">11.586 8.519</cell><cell>0.655</cell><cell>3195B</cell><cell>tonto</cell><cell>0.046</cell><cell>0.046</cell><cell>3.472</cell><cell>44B</cell></row><row><cell>wrf</cell><cell>5.040</cell><cell>4.434</cell><cell>0.934</cell><cell cols="2">2694B xalancbmk</cell><cell cols="3">18.288 10.885 0.311</cell><cell>178B</cell></row><row><cell>zeusmp</cell><cell>4.567</cell><cell>3.956</cell><cell>1.230</cell><cell>405B</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III :</head><label>III</label><figDesc>The</figDesc><table /><note><p>29 SPEC CPU 2006 benchmarks with LLC cache misses per 1000 instructions for LRU and optimal (MIN), instructions-per-cycle for LRU for a 2MB cache, and number of instructions fast-forwarded to reach the simpoint (B = billions). Benchmarks in the subset in boldface .</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE IV :</head><label>IV</label><figDesc>Multi</figDesc><table /><note><p>-core workload mixes with cache sensitivity curves giving LLC misses per 1000 instructions (MPKI) on the y-axis for last-level cache sizes 128KB through 32MB on the x-axis.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE V :</head><label>V</label><figDesc>Legend for various cache optimization techniques.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>For an LRU cache, why do we not simply use the tags already in the cache? Since the sampler is 12-way associative and does not experience bypass, there will not always be a correspondence between tags in a sampler set and the same set in the cache. Also, we would like to access and update the sampler without waiting for the tag array latency.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Ten of the 29 SPEC CPU 2006 benchmarks experience no significant reduction in misses even with optimal replacement. Our technique causes no change in performance in these benchmarks.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>IX. ACKNOWLEDGEMENTS Daniel <rs type="person">A. Jim?nez</rs>, <rs type="person">Samira M. Khan</rs>, and <rs type="person">Yingying Tian</rs> are supported by grants from the <rs type="funder">National Science Foundation</rs>: <rs type="grantNumber">CCF-0931874</rs> and <rs type="grantNumber">CRI-0751138</rs> as well as a grant from the <rs type="funder">Norman Hackerman Advanced Research Program</rs>, <rs type="grantNumber">NHARP-010115-0079-2009</rs>. We thank <rs type="person">Doug Burger</rs> for his helpful feedback on an earlier version of this work. We thank the anonymous reviewers for their invaluable feedback.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_ThCmMD8">
					<idno type="grant-number">CCF-0931874</idno>
				</org>
				<org type="funding" xml:id="_Ntj7g6R">
					<idno type="grant-number">CRI-0751138</idno>
				</org>
				<org type="funding" xml:id="_JNYuFHN">
					<idno type="grant-number">NHARP-010115-0079-2009</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">IATAC: a smart predictor to turn-off l2 cache lines</title>
		<author>
			<persName><forename type="first">Jaume</forename><surname>Abella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Gonz?lez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Vera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><surname>O'boyle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Archit. Code Optim</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="77" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">1st JILP workshop on computer architecture competitions (JWAC-1) cache replacement championship</title>
		<author>
			<persName><forename type="first">Alaa</forename><forename type="middle">R</forename><surname>Alameldeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aamer</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moinuddin</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Emer</surname></persName>
		</author>
		<ptr target="http://www.jilp.org/jwac-1/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A study of replacement algorithms for a virtual-storage computer</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Belady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Systems Journal</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="78" to="101" />
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The declining effectiveness of dynamic caching for general-purpose microprocessors</title>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kagi</surname></persName>
		</author>
		<idno>1261</idno>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Timekeeping in the memory system: predicting and optimizing memory behavior</title>
		<author>
			<persName><forename type="first">Zhigang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Kaxiras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Martonosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGARCH Comput. Archit. News</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="209" to="220" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">CMP$im: A pin-based on-the-fly single/multi-core cache simulator</title>
		<author>
			<persName><forename type="first">Aamer</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">S</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi-Keung</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Jacob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Annual Workshop on Modeling, Benchmarking and Simulation</title>
		<meeting>the Fourth Annual Workshop on Modeling, Benchmarking and Simulation</meeting>
		<imprint>
			<date type="published" when="2008-06">MoBS 2008. June 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adaptive insertion policies for managing shared caches</title>
		<author>
			<persName><forename type="first">Aamer</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Hasenplaugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moinuddin</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Sebot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Stelly</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 International Conference on Parallel Architectures and Compiler Techniques (PACT)</title>
		<meeting>the 2008 International Conference on Parallel Architectures and Compiler Techniques (PACT)</meeting>
		<imprint>
			<date type="published" when="2008-09">September 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">High performance cache replacement using re-reference interval prediction (rrip)</title>
		<author>
			<persName><forename type="first">Aamer</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Theobald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Steely</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th Annual International Symposium on Computer Architecture (ISCA-37)</title>
		<meeting>the 37th Annual International Symposium on Computer Architecture (ISCA-37)</meeting>
		<imprint>
			<date type="published" when="2010-06">June 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cache replacement based on reuse-distance prediction</title>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Keramidas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavlos</forename><surname>Petoumenos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Kaxiras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCD</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="245" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Using dead blocks as a virtual victim cache</title>
		<author>
			<persName><forename type="first">Samira</forename><forename type="middle">M</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Burger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Parallel Architectures and Compilation Technologies (PACT)</title>
		<meeting>the International Conference on Parallel Architectures and Compilation Technologies (PACT)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Counter-based cache replacement and bypassing algorithms</title>
		<author>
			<persName><forename type="first">Mazen</forename><surname>Kharbutli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Solihin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="433" to="447" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Selective, accurate, and timely selfinvalidation using last-touch prediction</title>
		<author>
			<persName><forename type="first">An-Chow</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="139" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dead-block prediction &amp; dead-block correlating prefetchers</title>
		<author>
			<persName><forename type="first">An-Chow</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cem</forename><surname>Fide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGARCH Comput. Archit. News</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="144" to="154" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dynamic self-invalidation: reducing coherence overhead in shared-memory multiprocessors</title>
		<author>
			<persName><forename type="first">Alvin</forename><forename type="middle">R</forename><surname>Lebeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGARCH Comput. Archit. News</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="48" to="59" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cache bursts: A new approach for eliminating dead blocks and increasing cache efficiency</title>
		<author>
			<persName><forename type="first">Haiming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehyuk</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Burger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the IEEE/ACM International Symposium on Microarchitecture<address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="222" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Trading conflict and capacity aliasing in conditional branch predictors</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Michaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andr?</forename><surname>Seznec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Uhlig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Symposium on Computer Architecture</title>
		<meeting>the 24th International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1997-06">June 1997</date>
			<biblScope unit="page" from="292" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Massively parallel algorithms for trace-driven cache simulations</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Nicol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><forename type="middle">G</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><forename type="middle">D</forename><surname>Lubachevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<date type="published" when="1994-08">August 1994</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="849" to="859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Using simpoint for accurate and efficient simulation. SIGMETRICS Perform</title>
		<author>
			<persName><forename type="first">Erez</forename><surname>Perelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Hamerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Van Biesbrouck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Sherwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brad</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eval. Rev</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="318" to="319" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adaptive insertion policies for high performance caching</title>
		<author>
			<persName><forename type="first">K</forename><surname>Moinuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aamer</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yale</forename><forename type="middle">N</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">C</forename><surname>Patt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">S</forename><surname>Steely</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">34th International Symposium on Computer Architecture (ISCA 2007)</title>
		<meeting><address><addrLine>San Diego, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">June 9-13, 2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A case for mlp-aware cache replacement</title>
		<author>
			<persName><forename type="first">K</forename><surname>Moinuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">N</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yale</forename><forename type="middle">N</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA &apos;06: Proceedings of the 33rd annual international symposium on Computer Architecture</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="167" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cooperative caching with keep-me and evict-me</title>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">B</forename><surname>Sartor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subramaniam</forename><surname>Venkiteswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><forename type="middle">S</forename><surname>Mckinley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenlin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Workshop on Interaction between Compilers and Computer Architecture</title>
		<imprint>
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="page" from="46" to="57" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A case for two-way skewed-associative caches</title>
		<author>
			<persName><forename type="first">Andr?</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA &apos;93: Proceedings of the 20th annual international symposium on Computer architecture</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="169" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Memory coherence activity prediction in commercial workloads</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Somogyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Hardavellas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jangwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastassia</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WMPI &apos;04: Proceedings of the 3rd workshop on Memory performance issues</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="37" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Cacti 5.1</title>
		<author>
			<persName><forename type="first">Shyamkumar</forename><surname>Thoziyoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Muralimanohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung</forename><surname>Ho Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norman</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<idno>HPL- 2008-20</idno>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="report_type">HP Tech Report</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Using the compiler to improve cache replacement decisions</title>
		<author>
			<persName><forename type="first">Zhenlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><forename type="middle">S</forename><surname>Mckinley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnold</forename><forename type="middle">L</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">C</forename><surname>Weems</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Parallel Architectures and Compilation Techniques</title>
		<meeting>the International Conference on Parallel Architectures and Compilation Techniques<address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">199</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
