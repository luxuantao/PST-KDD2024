<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GOAT: A Global Transformer on Large-scale Graphs</title>
				<funder>
					<orgName type="full">Capital One Bank</orgName>
				</funder>
				<funder>
					<orgName type="full">ONR MURI program</orgName>
				</funder>
				<funder ref="#_WdwwbR6">
					<orgName type="full">DARPA GARD</orgName>
				</funder>
				<funder>
					<orgName type="full">AFOSR MURI program</orgName>
				</funder>
				<funder ref="#_y6rde8H #_8uK9GTQ">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
				<funder ref="#_vcQF9k8">
					<orgName type="full">Amazon</orgName>
				</funder>
				<funder ref="#_4FsCDPj">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_jqjaRrX">
					<orgName type="full">Office of Naval Research</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kezhi</forename><surname>Kong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jiuhai</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">John</forename><surname>Kirchenbauer</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Renkun</forename><surname>Ni</surname></persName>
						</author>
						<author>
							<persName><forename type="first">C</forename><surname>Bayan Bruss</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
						</author>
						<title level="a" type="main">GOAT: A Global Transformer on Large-scale Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph transformers have been competitive on graph classification tasks, but they fail to outperform Graph Neural Networks (GNNs) on node classification, which is a common task performed on large-scale graphs for industrial applications. Meanwhile, existing GNN architectures are limited in their ability to perform equally well on both homophilious and heterophilious graphs as their inductive biases are generally tailored to only one setting. To address these issues, we propose GOAT, a scalable global graph transformer. In GOAT, each node conceptually attends to all the nodes in the graph and homophily/heterophily relationships can be learnt adaptively from the data. We provide theoretical justification for our approximate global self-attention scheme, and show it to be scalable to large-scale graphs. We demonstrate the competitiveness of GOAT on both heterophilious and homophilious graphs with millions of nodes. We open source our implementation at https://github.com/devnkong/GOAT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Transformers <ref type="bibr" target="#b38">(Vaswani et al., 2017)</ref> have demonstrated efficacy in many domains including language understanding and computer vision <ref type="bibr" target="#b6">(Devlin et al., 2018;</ref><ref type="bibr" target="#b7">Dosovitskiy et al., 2020)</ref>, and this has spurred interest in applying transformers to the graph domain. However, the success of transformers for graphs has been more modest, and has mostly been in the fairly narrow regime of graph classification tasks like molecule classification <ref type="bibr" target="#b42">(Ying et al., 2021;</ref><ref type="bibr" target="#b30">Mialon et al., 2021;</ref><ref type="bibr" target="#b19">Hussain et al., 2021;</ref><ref type="bibr" target="#b8">Dwivedi &amp; Bresson, 2020;</ref><ref type="bibr" target="#b33">Rong et al., 2020;</ref><ref type="bibr" target="#b25">Kreuzer et al., 2021;</ref><ref type="bibr" target="#b28">Maziarka et al., 2021)</ref>. The success of transformers in this regime is largely a result of the small size of each problem instance. For instance, the mean node count of graphs from the ogbg-molhiv dataset of the Open Graph Benchmark <ref type="bibr">(Hu et al., 2020a</ref>) is only 25.5. This tiny size enables self-attention across a larger percent of the graph, enabling long-range or even global self-attention. Despite a recent surge in interest in attention-based networks, standard Graph Neural Networks (GNNs) <ref type="bibr" target="#b22">(Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b39">Veli?kovi? et al., 2017)</ref> are still the de-facto model of choice for broader applications involving node classification or large industrial graphs.</p><p>Research in other domains suggests that a transformer's ability to utilize long-range signals that were previously inaccessible in more constrained sequential models are its key success factor. However, it is well-known that this self-attention is expensive, with time and memory overhead growing quadratically with the length of the input. To address this issue for language transformers, a range of efficient variants have been proposed <ref type="bibr" target="#b40">(Wang et al., 2020;</ref><ref type="bibr" target="#b23">Kitaev et al., 2020;</ref><ref type="bibr" target="#b43">Zaheer et al., 2020;</ref><ref type="bibr" target="#b47">Zhu et al., 2021;</ref><ref type="bibr" target="#b5">Choromanski et al., 2020)</ref> and have demonstrated competitive performance.</p><p>When applying transformers to graphs, sequence length is akin to the size l of the l-hop neighborhood, but the size grows exponentially instead of linearly. For large enough l, the model becomes global and attends to the entire graph. Tasks such as node classification are usually done on largescale graphs such as ogbn-products (2.4M nodes). Attending to the entirety of this graph in a naive way would require 24TB of GPU memory <ref type="bibr" target="#b11">(Geisler et al., 2021)</ref>. It is an open and pressing challenge to design efficient graph transformer models that scale to graphs of this size. Existing works on graph transformers for node classification never go beyond recursive 1-hop-neighbor message passing <ref type="bibr" target="#b34">(Shi et al., 2020;</ref><ref type="bibr" target="#b46">Zhao et al., 2021;</ref><ref type="bibr">Hu et al., 2020b</ref>) so they fail to learn from larger context and perhaps fail to achieve the full potential demonstrated in other domains.</p><p>At the same time, an established limitation of GNNs is their over-reliance on the homophily principle <ref type="bibr" target="#b29">(McPherson et al., 2001)</ref> causing them to perform poorly on heterophilious graphs. Homophily (or heterophily) means that a node's neighbors are likely (unlikely) to be of the same class. Standard GNNs are built on the message passing scheme <ref type="bibr" target="#b12">(Gilmer et al., 2017)</ref>, where features are aggregated recursively between 1-hop neighbors. This scheme has strong inductive bias towards homophilious graphs and does not tolerate heterophily well. To address this, researchers have proposed various heterophily-centered GNN models <ref type="bibr" target="#b26">(Lim et al., 2021;</ref><ref type="bibr" target="#b0">Abu-El-Haija et al., 2019;</ref><ref type="bibr" target="#b31">Pei et al., 2020;</ref><ref type="bibr" target="#b24">Zhu et al., 2020)</ref>. These models usually involve specialized message passing schemes that do not work for homophilious graphs. GPR-GNN <ref type="bibr" target="#b3">(Chien et al., 2020)</ref> can adapt to different homophily/heterophily profiles, but this model uses the entire graph for each training update ("full-batch" training) and thus cannot scale to huge problems. While it is reasonable to have specialized model designs for different kinds of graphs, this practice can be problematic when the homophily or heterophily characteristics of the target dataset are unknown, or the graph has mixed behavior.</p><p>Present work. We study a global transformer in which each node attends to all others, providing a universal architecture that supports both homophilious and heterophilious graphs. We propose GOAT, a global transformer for large-scale node classification tasks. To implement the intractable O(n 2 ) global self-attention on large-scale graphs, we leverage a dimensionality reduction algorithm and reduce memory complexity from quadratic to linear. Using a K-Means based projection algorithm, we theoretically show that our scalable global attention method has bounded error relative to graph attention without dimensionality reduction. Besides the global design, we also strengthen the model using a scalable local attention module. For each node, we sample its l-hop neighbors and make it directly attend to them, unlike the recursive smoothing pattern of GNNs. Empirically, GOAT shows strong performance on both homophilious and heterophilious datasets as large as ogbn-products (2.4M nodes) <ref type="bibr">(Hu et al., 2020a)</ref> and snap-patents (2.9M nodes) <ref type="bibr" target="#b26">(Lim et al., 2021)</ref>. We summarize our contributions as follows:</p><p>1. We propose GOAT, a scalable global transformer model where each node is able to attend to all others.</p><p>2. We develop a novel local attention module that enables GOAT to absorb rich local information.</p><p>3. We demonstrate the strong performance of GOAT on both large-scale homophilous and heterophilious node classification benchmarks.</p><p>4. We provide theoretical justification to show our scalable global attention scheme has bounded error relative to unscalable standard attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head><p>In this section we introduce the preliminaries of GNNs, transformers, and homophily.</p><p>Graph Neural Networks (GNNs). We represent a graph as G(V, E). GNNs are often built with recursive messagepassing schemes, where features are passed and shared directly between 1-hop neighbors. Formally the k-th iteration of message passing, or the forward propogation of the k-th layer of GNNs, is defined as follows:</p><formula xml:id="formula_0">m (k) v = AGGREGATE (k) h (k-1) v , h (k-1) u , euv , ?u ? N (v) , h (k) v = COMBINE (k) h (k-1) v , m (k) v ,<label>(1)</label></formula><p>where h</p><formula xml:id="formula_1">(k)</formula><p>v is the hidden feature of node v at the k-th layer, m (k) v is the computed message for node v at the k-th layer, e uv is the edge feature between node u and v, N (v) is node v's 1-hop neighbor set. AGGREGATE(?) and COMBINE(?) are functions parameterized by neural networks.</p><p>Transformers. The key component of a transformer model is the self-attention scheme, which allows each element of a set or token to attend to information of other tokens at various locations. The forward pass of a self-attention module is defined as:</p><formula xml:id="formula_2">Attn(H) = Softmax HW Q (HW K ) ? ? d HW V ,<label>(2)</label></formula><p>where H ? R n?f is the hidden feature matrix. W Q , W K , W V ? R f ?d are linear projection matrices with trainable weights. Here the Equation 2 denotes the singlehead self-attention module, which can straightforwardly generalize to multi-head attention. Note that in practice multi-headed self-attention is widely used. It is easy to see that attention is expensive. The time and memory complexity is O(n 2 ), which leads to low efficiency and is a bottleneck for transformers. In this work, we call the output of the Softmax function the "attention matrix".</p><p>Homophily indicates connected nodes are likely to share common labels. We follow <ref type="bibr" target="#b26">Lim et al. (2021)</ref> to focus on edge homophily in this work. The edge homophily is defined as the proportion of edges that link two nodes with the same label as below:</p><formula xml:id="formula_3">h = |{(u, v) ? E : y u = y v }| |E| , (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>where E is the edge set and y is the node label. Nonhomophily (or heterophily) graph indicates the dataset with dissimilar labels/features sharing edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Intuition. Existing GNNs usually have hardcoded messagepassing patterns and will only work on either homophilious or heterophilious graphs. A global attention scheme makes each node attend to all the nodes in the graph and does not explicitly have inductive bias towards either one. Instead of one specially tailored or fixed message passing and aggregating pattern, the attention scheme freely learns to adapt to different priors. Furthermore, the ability to attend to a single far-away node may enable multi-hop features to be used without the over-smoothing that happens when information is passed over many rounds of averaging in a standard GNN. However, for applications in industry where large graphs with millions of nodes are ubiquitous, it is not possible to train and deploy a fully global transformer due to the quadratic cost.</p><p>Given that, we propose GOAT, the scalable global transformer. GOAT uses an approximate global attention which reduces complexity from quadratic to linear and supports mini-batch training. GOAT also has a local attention module to process information from the local neighborhood for better prediction. Figure <ref type="figure" target="#fig_0">1</ref> illustrates the local sampling procedure and the whole attention module. In this section, we describe our methodological designs in detail. Firstly let us have a formal definition on the notations.</p><p>Definition 3.1. We define n as the total number of nodes in the graph, k is the number of dimension in the low dimensional space, k ? n. P denotes the projection matrix, P ? R n?k . f denotes the raw feature dimension of the node feature and d is the hidden feature dimension if without further specification. b denotes batch size.</p><p>Global. To address the expensive quadratic complexity issue, we leverage dimensionality reduction. Firstly we want to find a projection matrix P that can reduce the feature matrices into a low-dimensional space so the cost of computing their product is reduced. At the same time we also hope this reduction will not overly degrade the quality of representations. Below, we provide theoretical analysis on the existence of such matrix, P . Note that in the demonstration process below, we assume positional information is already fused into node features.</p><p>Theorem 3.2. For any linear weight matrices</p><formula xml:id="formula_5">W K , W Q , W V ? R f ?d , node feature matrix X ? R n?f , mini-batch of nodes X B ? R b?f , row vector x ? R 1?n</formula><p>of X, and ? &gt; 0, there exists projection matrices</p><formula xml:id="formula_6">P A , P V ? R n?k , such that Pr( Softmax (SP A ) P ? V XW V -Softmax(S)XW V F ? ?? Softmax(S)? F ?XW V ? F ) &gt; 1 -O(1/n), (4) with S = X B W Q (XW K ) ? / ? d and k = O log(n)/? 2 .</formula><p>We argue that the scheme provided by Theorem 3.2 makes global attention possible on large graphs. The expression</p><formula xml:id="formula_7">SP A = X B W Q (XW K ) ? / ? d P A inside the Softmax function can be computed as a product between Q B = X B W Q and K = P ? A XW K , which is only O(bdk) work.</formula><p>Note that the new value feature matrix V = P ? V XW V is also low dimensional. Then we see that the problem comes to how we can materialize K, V without explicitly computing the multiplication of (XW K ) ? P A or P ? V XW V which is O(nf d + nkd) and not scalable when the graph is large.</p><p>Technically, we follow <ref type="bibr" target="#b37">Van Den Oord et al. (2017)</ref> to materialize K and V . Projection matrix P is computed by the K-Means algorithm as the sparse indexing matrix. Each row of P ? R n?k is a one-hot vector representing the clustering centroid the node is assigned to. We define</p><formula xml:id="formula_8">C = diag -1 (1 n P )P ? X</formula><p>as the codebook. Each row of the codebook C represents the center (e.g., the average) of all the entries in a cluster. In this way,</p><formula xml:id="formula_9">K = diag -1 (1 n P )P ? XW K = CW K and V = diag -1 (1 n P )P ? XW V = CW V .</formula><p>We store and update K, V in the exponential moving average (EMA) manner. The algorithm is summmarized in Algorithm 2. Note that we do not explicitly compute K, V each iteration but instead cache and update the codebook on the fly in the EMA manner using batch statistics. P is stored sparsely to save memory. Now we go back to show that the forward pass under such a scheme yields a bounded-error approximation compared with the authentic output. We start with two definitions for clearer demonstration. We define</p><formula xml:id="formula_10">X = P diag -1 (1 n P )P ? X = P C,<label>(5)</label></formula><p>as the approximate X using the codebook. We define the attention matrix for a batch X B computed by parameterized function f W as</p><formula xml:id="formula_11">f W (Y ) = Softmax X B W Q (Y W K ) ? ? d .<label>(6)</label></formula><p>Theorem 3.3. If the function f W has Lipschitz constant upper-bounded by lip (f W ) and the quantization error is ?, the estimation error is bounded as,</p><formula xml:id="formula_12">X out B -X out B F ? ??[1 + O (lip (fW ))] ?AB?F ??X?F ??WV ? F ,<label>(7)</label></formula><p>with</p><formula xml:id="formula_13">X out B = A B XW V , X out B = A B XW V , A B = f W (X) = Softmax X B W Q (XW K ) ? / ? d , and A B = f W ( X) = Softmax X B W Q ( XW K ) ? / ? d . Quantiza- tion error ? is defined as ?X -X? F ? ??X? F .</formula><p>Note that the computation of X out B denoted in Theorem 3.3, which is the approximated global attention, is expensive because it requires the computation of matrix multiplication </p><formula xml:id="formula_14">? ! ? " ? " ? # ? # ? # Codebook Update Sample Local Attention Global Attention 1. Neighborhood Sampling 2. GOAT forward propogation</formula><formula xml:id="formula_15">for v ? V do x ? MLPa(Xv) pe ? P Ev q ? Concat(x, pe)WQ K ? Concat(?x, ?pe)WK V ? ?xWV xout ? Softmax qK ? ? d + log (1nP ) V xout ? MLP b (xout)</formula><p>Update ?x, ?pe by x, pe using the EMA K-Means algorithm as in Algorithm 2. end for of X B W Q XW K and also A B XW V , which is unscalable. We articulate it in this way for the simplicity of theoretical proof. Below we show that X out B can be equivalently computed using K and V , which are low-dimensional.</p><formula xml:id="formula_16">Corollary 3.4. The computation of X out B = Softmax X B W Q ( XW K ) ? / ? d XW V is equivalent with X out B = Softmax X B W Q K ? / ? d + log (1 n P ) V . (8)</formula><p>All the proofs are deferred to the Appendix.</p><p>Through Corollary 3.4, where X B gets to directly attends to K = CW K instead of X. As we have discussed, both K and V will be materialized in memory and updated in the EMA way. The computation for the feature of a node </p><formula xml:id="formula_17">X out B is O(bdk)</formula><formula xml:id="formula_18">P ? FindNearest(F, ?) {compute cluster assignment} c ? c ? ? + P ? 1 ? (1 -?) {EMA accumulation} v ? v ? ? + P ? F ? (1 -?) {EMA accumulation} v ? v/c ? ? v ? bn. running std + bn. running mean ?x, ?pe ? ? end function</formula><p>It is well established in the literature that good positional encodings are required to make transformers work effectively. In our global attention scheme, only absolute positional encodings are feasible because the hidden features and positional encodings must be concatenated. We argue that the design of positional encoding on graphs is still an open question for the community <ref type="bibr" target="#b9">(Dwivedi et al., 2021;</ref><ref type="bibr" target="#b25">Kreuzer et al., 2021)</ref> and detailed discussion for such design is beyond the scope of this work. To simplify the setup of the experiments, we use pretrained node2vec <ref type="bibr" target="#b13">(Grover &amp; Leskovec, 2016)</ref> node embeddings as our positional embeddings.</p><p>Local. Along with the design of our global transformer, we propose a novel local attention module, which allows each node to directly attend to its l-hop neighbors to attain rich local information. Empirically, we find that the local attention module effectively helps the model learn better representations. Although this module is responsible for aggregating information from the local neighborhood as in normal message passing feature aggregation, unlike the standard GNNs' recursive 1-hop neighbor smoothing pattern, our local module provides flexible attention weights for neighbors at different hop distances. We believe the module provides increased capacity to learn the inductive biases required for accurate predictions beyond the hard-coded structures of existing GNNs. To support mini-batch training, we adopt the widely-used neighbor sampling (NS) method <ref type="bibr" target="#b14">(Hamilton et al., 2017)</ref> to sample all l-hop neighbors. We make each node directly attend to the sampled neighbors. Inspired by <ref type="bibr" target="#b42">Ying et al. (2021)</ref>, for the local module we select the relative positional encoding scheme to distinguish neighbors at diverse distances from the source node. Our attention score calculation inside the Softmax function is:</p><formula xml:id="formula_19">S ij = X i W Q (X j W K ) ? ? d + b D(i,j) ,<label>(9)</label></formula><p>where b D(i,j) is a trainable bias parameter indexed by D(i, j), the shortest distance between node i and j. Note that although the local module intuitively helps learn better representations, the neighbor sampling (NS) algorithm does introduce a small scalability concern. It is well-known that the neighborhood explosion problem for NS has complexity O(d l ) for l-hop neighborhoods, where d is the average degree of the graph. Surprisingly, it is the NS step, and not the approximate global attention, that is the primary efficiency bottleneck for our model. We investigate the efficiency perspective in Section 5.</p><p>Since we leverage different positional encodings for the global and local modules, we have separate attention functions, after which the respective sets of node features and positional encodings are concatenated and fed into subsequent layers. Figure <ref type="figure" target="#fig_0">1</ref> illustrates the local sampling procedure and the whole attention module. In the sections to follow, we show that GOAT displays strong performance on the large-scale node classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we detail the empirical evaluation of our GOAT model.</p><p>Datasets. We select four datasets to evaluate. To represent homophilious graph problems we choose two datasets, ogbn-arxiv and ogbn-products, from the well-known Open Graph Benchmark (OGB) <ref type="bibr">(Hu et al., 2020a)</ref>. For heterophilious examples we utilize the arxiv-year and snap-patents datasets curated by <ref type="bibr" target="#b26">Lim et al. (2021)</ref>. These are all large-scale (multimillion) node classification datasets. For the train and validation splits, we use the official splits from OGB for both ogbn-arxiv and ogbn-products and for arxiv-year and snap-patents we follow the practice of <ref type="bibr" target="#b26">Lim et al. (2021)</ref> and randomly sample the train and validation sets. As there are no official splits or train set ratios for these two datasets, we experiment with training sets that comprise 10%, 20%, and 50% of the data while fixing validation set ratio at 25%, and report separate results for each split. We refer readers to Table <ref type="table" target="#tab_2">1</ref> for detailed statistics of the datasets.</p><p>Setup. We focus on the transductive node classification task, where we see all the nodes at training time, but only the train set has labels. Our baseline models include GCNJK <ref type="bibr" target="#b41">(Xu et al., 2018)</ref>, GAT <ref type="bibr" target="#b39">(Veli?kovi? et al., 2017)</ref>, LINKX <ref type="bibr" target="#b26">(Lim et al., 2021)</ref>, MixHop (Abu-El-Haija et al., 2019), and GPS <ref type="bibr" target="#b32">(Ramp??ek et al., 2022)</ref>. All of our training procedures use pure empirical risk minimization (ERM) and we do not leverage techniques like data augmentation <ref type="bibr" target="#b24">(Kong et al., 2020)</ref>, label propogation <ref type="bibr" target="#b17">(Huang et al., 2020)</ref>, powerful embeddings <ref type="bibr" target="#b4">(Chien et al., 2021)</ref>, or other tricks, as these additional regularizers have not been uniformly studied for all model types, and this simple setting enables fair comparisons across model architectures. For the baseline models, we perform a hyperparameter sweep and select the best performing settings and report the corresponding results for each model and dataset pairing (full details in the Appendix).</p><p>For GOAT, the attention function is multi-headed but we only implement a single layer of attention module. We leave the discussion of a multi-layer variant to future work. For the local attention, we sample neighbors that live within 3-hops. For each node we sample [20, 10, 5] neighbors recursively. The size of the codebook is fixed at 4, 096 and the dimensionality is 64. We always use a dropout rate of 0.5 and also use batch norm. Each experiment is carried out on either a single GeForce RTX 2080 Ti (11GB memory) or a RTX A4000 (16GB memory).</p><p>Results.  <ref type="bibr" target="#b2">(Chiang et al., 2019)</ref> as their runner-up to LINKX on this dataset. This setting was OOM/T on our hardware, but as it is weaker than LINKX and GOAT in either case, reporting the stronger numbers would not have changed the results or analysis of average model performance. GC-NJK and GAT intuitively register strong performances on ogbn-arxiv and ogbn-products as they resonate with the inductive bias of the two homophilious datasets.</p><p>In contrast, on the arxiv-year and snap-patents datasets, their scores are poor. LINKX and MixHop are architectures designed for heterophilious graphs, a specialization validated by their strong performances in Table <ref type="table" target="#tab_3">2</ref> on those datasets. However, GOAT constantly achieves competitive results on all four datasets, which reveals the adaptive ability of the attention functions. We highlight that our goal is not to beat GNNs universally; specific priors can still be useful factors when designing architectures. Further, in accordance with the no-free-lunch theorem, winning </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Ablation Studies &amp; Analysis</head><p>GOAT vs. GOAT-Local-only. In Figure <ref type="figure">2a</ref> &amp; 2b we ablate to only use the local module to analyze our architecture design. On the ogbn-arxiv dataset where homophily is prevalent, the local-only model is as strong as the complete GOAT. While on the arxiv-year dataset there shows clear discrepancy between the two, where the global module brings a salient 3% boost. The results provide a strong validatioin towards our main intuition, that the global module increases expressive power by modeling long-range interactions. And such ability is especially effective towards heterophilious graphs, which is intuitive as many other successful heterophilious GNN architectures aimed at broadening the range of message passing as well as learning from long-range interactions.</p><p>Codebook size. We run ablation studies on the codebook size for a better understanding and interpretation of the GOAT model. The codebook size refers to the k value in the K-Means algorithm. We summarize the results in Table <ref type="table" target="#tab_4">3</ref>. Note the heterophilious dataset arxiv-year is more sensitive to the adjustment of the codebook size, showing the efficacy of global attention module at learning long-range interactions.</p><p>Batch norm vs. Layer norm. Normalization is important to transformers. We ablate on the normalization technique selection in Figure <ref type="figure">3a</ref> &amp; 3b. We see that layer norm can be as good as batch norm in our model but converges much slower, and evidently "no normalization" is not a good choice for our architecture. Depth. We constantly have 1 layer of attention modules in our model, for both the global and local part. In fact it is challenging to extend our GOAT model to multi-layer attention computation like common transformers. Note that our attention modules are not strict self-attention modules, whose input and output are of the same length. As denoted in Fig 1, our input includes the predictive node, sampled neighbors, and the codebook, but only the the predictive node attends to others and gets aggregated features. So after the attention function the state of feature for sampled neighbors and the codebook is behind that of the predictive node, where future rounds of attention computation is less intuitive. A straightforward strategy to address this conundrum is to recursively sample neighbors of neighbors and add more codebooks by each layer, but apparently this will greatly adds to the overhead and renders the model less efficient. Due to computational limitation we do not carry out the experiments, but how to make the model deeper is  Efficiency. The neighbor sampling bottleneck is the major limitation of our method. Note that although the local module intuitively helps learn better representations, the neighbor sampling (NS) algorithm does introduce a small scalability concern. It is well-known that the neighborhood explosion problem for NS has complexity O(d l ) for l-hop neighborhoods, where d is the average degree. Our sampling method in the local module is a plain adaptation of NS so that GOAT will share efficiency issues of NS. However when we only use the global module of GOAT, its convergence rate outperforms that of GAT-NS by a large margin according to Figure <ref type="figure" target="#fig_2">4a</ref> and Figure <ref type="figure" target="#fig_2">4b</ref> due to the linear complexity benefit. In Table <ref type="table" target="#tab_5">4 we</ref>  GOAT on datasets with 1k and 4k as batch size. We see that GOAT has competitive running speed compared with GAT that is powered by NS for scalable training, while our GOAT-Global-only variant overwhelmingly outperforms the other two thanks to the linear training complexity. Detailed hyperparameter setup can be found in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related Work</head><p>Scalability. The poor scalability of GNNs to large graphs remains a major obstacle to deploying GNNs for enterprisescale applications. Scalable GNN algorithms mainly involve node-, layer-, and graph-wise sampling methods.  <ref type="bibr" target="#b14">Hamilton et al. (2017)</ref> proposed neighbor sampling (NS) to repeatedly sample neighbors for message passing, but this approach leads to the exponential neighborhood explosion problem mentioned at the end of Section 3. One technique to address this issue is the layer-wise sampling proposed in <ref type="bibr" target="#b18">Huang et al. (2018)</ref>. ClusterGCN <ref type="bibr" target="#b2">(Chiang et al., 2019)</ref> and GraphSAINT <ref type="bibr" target="#b44">(Zeng et al., 2019)</ref> both perform subgraph sampling so that GNNs can be run in a "full-batch" manner but on tractable subgraphs that hopefully approximate the global graph semantics -we employ the latter of the two for training our baseline GCNJK, GAT, and MixHop models. A recent method, GAS <ref type="bibr" target="#b10">(Fey et al., 2021)</ref>, stores historical node features to help both training and inference process. Finally, it is also established that transformers suffer from inherent scalability issues. In response, a plethora of efficient transformers have been proposed that employ different techniques to improve their complexity including sparse attention maps <ref type="bibr" target="#b43">(Zaheer et al., 2020;</ref><ref type="bibr" target="#b1">Beltagy et al., 2020)</ref>, clustering-based schemes <ref type="bibr" target="#b23">(Kitaev et al., 2020;</ref><ref type="bibr" target="#b35">Tay et al., 2020)</ref>, and low-rank projections of the attention matrix <ref type="bibr" target="#b40">(Wang et al., 2020;</ref><ref type="bibr" target="#b36">Tay et al., 2021)</ref>.</p><p>Graph transformers. Most successful applications of transformers to graphs problems have only considered graph classification. Graphormer <ref type="bibr" target="#b42">(Ying et al., 2021)</ref> is the representative global transformer on small graphs with customized centrality, edge, and spatial encodings. Other strong architectures include <ref type="bibr" target="#b25">Kreuzer et al. (2021)</ref>; <ref type="bibr" target="#b28">Maziarka et al. (2021)</ref>; <ref type="bibr" target="#b8">Dwivedi &amp; Bresson (2020)</ref>; <ref type="bibr" target="#b33">Rong et al. (2020)</ref>. Existing graph transformers <ref type="bibr" target="#b34">(Shi et al., 2020;</ref><ref type="bibr">Hu et al., 2020b;</ref><ref type="bibr" target="#b8">Dwivedi &amp; Bresson, 2020;</ref><ref type="bibr" target="#b32">Ramp??ek et al., 2022)</ref> for node classification use the same recursive message passing scheme as traditional GNNs. The attention module computes attention scores as the weights for 1-hop neighborhood smoothing, similar to the attention mechanism in GATs <ref type="bibr" target="#b39">(Veli?kovi? et al., 2017)</ref>. These models fail to demonstrate the ability to learn from larger contexts and generally do not outperform traditional GNNs. An example architecture that does implement a form of global attention, GraphBERT <ref type="bibr" target="#b45">(Zhang et al., 2020)</ref>, fails to solve scalabilty issues due to requiring full-batch training.</p><p>Heterophily. Recently a body of work has focused on adapting GNNs to heterophilious graphs. LINKX <ref type="bibr" target="#b26">(Lim et al., 2021</ref>) is a simple model that coerces 2-hop (but not 1-hop) neighbors to have the similar labels. H2GCN <ref type="bibr" target="#b24">(Zhu et al., 2020)</ref> aggregates features from 1-hop and 2-hop neighbors separately. MixHop <ref type="bibr" target="#b0">(Abu-El-Haija et al., 2019)</ref> aggregates information from diverse degrees of smoothed features. GPR-GNN <ref type="bibr" target="#b3">(Chien et al., 2020)</ref> is similar to MixHop, but uses a more complex adaptive aggregation operation. GPR-GNN works well on some kinds of data but does not scale to large graphs. One closely related work is Non-local GNN <ref type="bibr" target="#b27">(Liu et al., 2021)</ref>, which does an approximate global attention leveraging attention-based sorting. The model successfully reduces time complexity from O(n 2 ) to O(nlog(n)), but cannot support mini-batch training, so its scalability is limited.</p><p>While progress has certainly been made towards more effective graph transformers and models specifically suited to heterophilious graphs, to date, no graph transformer has emerged that simultaneously relaxes the restriction of a homophilious inductive bias (whilst remaining performant in that setting) and easily handles large-scale node classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We propose GOAT, a global transformer that works on both homophilious and heterophilious node classification tasks. Our model makes global attention possible by dimensionality reduction and we prove our approximate approach has bounded error compared with the true global mechanism.</p><p>Experiments reveal GOAT's strong performances on largescale graphs. We hope our work can spur more research into universal graph neural architecture who can adapt to different inductive biases.</p><p>Leveraging Boole's inequality, we get</p><formula xml:id="formula_20">Pr(?A B P P ? v ? -A B v ? ? F ? ??A B v ? ? F ) &gt; 1 -2b? &gt; 1 -2n?.</formula><p>By choosing ? to be O(1/n 2 ) we finally get</p><formula xml:id="formula_21">Pr(?A B P P ? v ? -A B v ? ? F ? ??A B v ? ? F ) &gt; 1 -O(1/n), with k = O log(n)/? 2 .</formula><p>C.2. Proof of Theorem 3.2</p><p>Proof. We follow the techniques in <ref type="bibr" target="#b40">Wang et al. (2020)</ref> to provide the proof. We define P A = ?P and P V = e -? P , ? is a constant. We aim to prove</p><formula xml:id="formula_22">Pr exp (SP ) P ? X V -exp(S)X V F ? ?? exp(S)? F ?X V ? F &gt; 1 -O(1/n),<label>(12)</label></formula><p>where X V = XW V for simplicity. Using triangle inequality, we have</p><formula xml:id="formula_23">exp (SP ) P ? X V -exp(S)X V F ? exp(S)P P ? X V -exp(S)X V F a + exp (SP ) P ? X V -exp(S)P P ? X V F b .</formula><p>For part a, based on Proposition 1, we have</p><formula xml:id="formula_24">a ? ?? exp(S)? F ?X V ? F .</formula><p>For part b, we have</p><formula xml:id="formula_25">b ? ?exp (SP ) -exp(S)P ? F ?P ? X V ? F .</formula><p>Given that the exponential function is Lipschitz continuous in a compact region, and also based on the Equation C.1, we have</p><formula xml:id="formula_26">b ? o(? exp(S)? F ?X V ? F ).</formula><p>Based on part a and part b, we finally attain</p><formula xml:id="formula_27">Pr exp (SP ) P ? X V -exp(S)X V F ? ?? exp(S)? F ?X V ? F &gt; 1 -O(1/n). C.3. Proof of Theorem 3.3</formula><p>Proof. This proof is built upon the usage of Lipschitz constant and quantization error. so we can cancel the P both inside and outside Softmax by adding a weighting term to make the exponential computation weighted with the centroid size, where we attain</p><formula xml:id="formula_28">X out B = Softmax X B W Q K ? / ? d + log (1 n P ) V .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experimental Details</head><p>This section describes in more detail the experimental setup for the empirical results presented in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Dataset Downloading</head><p>We refer readers to <ref type="bibr">Hu et al. (2020a)</ref> and <ref type="bibr" target="#b26">Lim et al. (2021)</ref> and their official repos for dataset downloading.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. GOAT</head><p>For the hyperparameter selections of our GOAT model, besides what we have covered in the setup part of the experiment section that datasets share in common, we list other settings in Table <ref type="table" target="#tab_6">6</ref>. Each experiment is repeated four times to get the mean value and error bar. We use Adam optimizer with lr 1e-3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Baseline Models</head><p>Since the heterophilius datasets on which we benchmark the GOAT model are derived from <ref type="bibr" target="#b26">Lim et al. (2021)</ref>, in order to facilitate as fair a comparison as possible, especially against the LINKX model proposed in the same work, we utilize their implementation provided at the official repo<ref type="foot" target="#foot_2">1</ref> . This library also provides reference implementations of other GNN architectures, and we utilize those as well. Following the procedure described both in the paper and implicitly by their codebase, we run a hyperaparameter sweep for each model on each dataset. For each combination of parameters, 5 models are trained using different initialization seeds to determine a mean value and error bars, and then the final hyperparameters are selected based on accuracy on the validation set. These final parameters correspond to the settings used to realize the "official train split" numbers in Table <ref type="table" target="#tab_3">2</ref> in the main work for ogbn-arxiv and ogbn-products and the 50% train split numbers for arxiv-year and snap-patents. These same parameters are also used when performing the sample complexity experiments with train splits of 10% and 20% for the latter two datasets.</p><p>For the baseline GNNs we chose a Graph Convolutional Network with Jumping Knowledge (GCNJK) and a Graph Attention Network (GAT), and for a second heterophily-specific model to complement LINKX, we consider MixHop. As described in Section 4 of the main work, we use ERM to train all models including the baselines. We also focus on the minibatch setting rather than "full-batch" training as a primary feature of the GOAT model is its native scalability through minibatch training.</p><p>The batching algorithm we use for the GCNJK, GAT, and MixHop models is the GraphSAINT Random Walk based sampler (LINKX uses its own adjacency row-wise sampling scheme, see <ref type="bibr" target="#b26">Lim et al. (2021)</ref> for details). In the spirit of fair evaluation, we make the model and sampler choices based on the fact that according to <ref type="bibr" target="#b26">Lim et al. (2021)</ref>, each of these are the most performant two models in the homophily and heterophily-specific design categories on the datasets under evaluation.</p><p>Hyperparameter Settings: For all four baseline models we tune two main parameters: the number of layers, and the number of hidden channels (dimension) of each layer. We also evaluate two subgraph sizes, or batch sizes, for the SAINT sampler (number of roots used for the random walk). For the GAT model only, we also tune the number of attention heads. We evaluate the same parameter ranges described in <ref type="bibr" target="#b26">Lim et al. (2021)</ref> and defined by their codebase, and simply report the final parameters selected in Table <ref type="table" target="#tab_7">7</ref>. Selected parameters that are shared amongst all model and dataset pairs include training for 500 epochs, using the AdamW optimizer with a learning rate of 0.01, and creating 5 SAINT subgraphs per epoch. Model specific selected parameters include using 8 heads for the GAT, concatenative jumping knowledge for the GCNJK model, and the 2 hop setting for MixHop. For all settings that we choose as final, if possible, we verify that the accuracy is within ?1% of the value reported in <ref type="bibr" target="#b26">Lim et al. (2021)</ref> as "best" for each model on each dataset.</p><p>There are two details of particular note concerning Table <ref type="table" target="#tab_7">7</ref> (and corresponding results in Table <ref type="table" target="#tab_3">2</ref> in the main work). First, for the snap-patents dataset we do not use the ClusterGCN sampler for the MixHop architecture as it proved too time and memory intensive for our hardware. We ground this choice in the fact that according to the performance reported in <ref type="bibr" target="#b26">Lim et al. (2021)</ref>, the performance of MixHop with cluster sampling would still have been below the performance of GOAT by approximately 8 accuracy points. As an additional comment, the computational costliness of this method was also a challenge in their work, precluding it from parts of their evaluation. In the spirit of scalability, overall, we see GraphSAINT as being a more relevant choice for our comparison due to its more favorable scaling characteristics.</p><p>Second, also in service of a competitive evaluation, we chose to report performance of the GAT model on ogbn-products * pulled from the Open Graph Benchmark's official leaderboard<ref type="foot" target="#foot_3">2</ref> for this dataset since the minibatch performance we achieved with the SAINT sampler was significantly lower than the reference result (as well as that of our GOAT model). The result from the leaderboard was trained using the neighbor sampling algorithm.</p><p>For  <ref type="bibr">[20,</ref><ref type="bibr">10,</ref><ref type="bibr">5]</ref> to do minibatching. The total running time involve evaluation time.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The local sampling procedure and forward propogation of the GOAT model. b l denotes the trainable positional bias for neighbors at a distance of l.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Ablation study on the local module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Convergence speed comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Fig 4a and Fig 4b, we set batch size as 10K. We use 4K as codebook size. GAT leverages NS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and thus scalable and efficient. Algorithm 1 illustrates our global transformer forward pass.</figDesc><table><row><cell>Algorithm 2 EMA K-Means update algorithm</cell></row></table><note><p>Require: Inputs are the hidden features X and positional encodings P E for a batch. bn(?) is the batch norm module. FindNearest(?) finds the nearest centroid for each feature. function Update(X, P E) F ? Concat(X, P E) F ? bn(F ) {data whitening using batch norm} ? ? Concat(?x, ?pe)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Table 2 reveals the competitive performance of our GOAT model compared with baselines. For</figDesc><table /><note><p><p><p><p>*  </p>we show performance of MixHop with GraphSAINT sampling</p><ref type="bibr" target="#b44">(Zeng et al., 2019)</ref> </p>on snap-patents despite Lim et al. (2021) reporting 46.82 ? 0.11 for MixHop with ClusterGCN sampling</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Dataset statistics.</figDesc><table><row><cell>Dataset</cell><cell># Nodes</cell><cell># Edges</cell><cell cols="2"># Feat. # Class</cell><cell>Class type</cell><cell>Split</cell><cell>Edge hom.</cell></row><row><cell>ogbn-arxiv</cell><cell>169,343</cell><cell>1,166,243</cell><cell>128</cell><cell>40</cell><cell cols="2">product category official</cell><cell>.66</cell></row><row><cell cols="3">ogbn-products 2,449,029 61,859,140</cell><cell>100</cell><cell>47</cell><cell>subject area</cell><cell>official</cell><cell>.81</cell></row><row><cell>arxiv-year</cell><cell>169,343</cell><cell>1,166,243</cell><cell>128</cell><cell>5</cell><cell>pub year</cell><cell>random</cell><cell>.222</cell></row><row><cell>snap-patents</cell><cell cols="2">2,923,922 13,975,788</cell><cell>269</cell><cell>5</cell><cell>time granted</cell><cell>random</cell><cell>.073</cell></row><row><cell cols="4">all comparisons on datasets with diverse properties is ex-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">pected to be difficult. Rather, our intention is to develop a</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">model that can learn different inductive biases as required,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">adapting seamlessly to different use cases. This flexibil-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">ity is an important quality in practice as practitioners may</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">encounter datasets for which knowledge about appropriate</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">priors is scarce. Note that we further try to compare with</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">GPS (Ramp??ek et al., 2022), which is a powerful scal-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">able graph transformer on graph-level tasks. However on</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">large-scale node tasks GPS always yields OOM even on the</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">smallest arxiv datasets.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Experimental results. Note that the "Overall performance average" is the mean value of the two "Performance avgerage" values in the upper and lower tables. The darker blue marks the best result in each row, while the lighter shade marks the runner-up. Evaluation metric is prediction accuracy on the test set. Test accuracies are reported based on the hyperparameter setting yielding the highest validation accuracies. Where possible, we validate our baselines against the results in<ref type="bibr" target="#b26">Lim et al. (2021)</ref>. OOM means out of memory error through our experiments.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">General GNN</cell><cell cols="2">Heterophilious GNN</cell><cell>Transformer</cell><cell>Ours</cell></row><row><cell>Dataset</cell><cell>Train ratio</cell><cell>GCNJK</cell><cell>GAT</cell><cell>LINKX</cell><cell>MixHop</cell><cell>GPS</cell><cell>GOAT</cell></row><row><cell>ogbn-arxiv</cell><cell>official 54%</cell><cell cols="3">69.57 ? 0.20 71.95 ? 0.36 66.18 ? 0.33</cell><cell>71.29 ? 0.29</cell><cell>OOM</cell><cell>72.41 ? 0.40</cell></row><row><cell cols="2">ogbn-products official 8%</cell><cell cols="3">72.84 ? 0.36 79.45 ? 0.59 71.59 ? 0.71</cell><cell>73.48 ? 0.29</cell><cell>OOM</cell><cell>82.00 ? 0.43</cell></row><row><cell cols="2">Performance average</cell><cell>71</cell><cell>76</cell><cell>69</cell><cell>72</cell><cell>OOM</cell><cell>77</cell></row><row><cell>arxiv-year</cell><cell cols="4">random 10% 43.34 ? 0.08 38.34 ? 0.10 46.22 ? 0.24</cell><cell>45.13 ? 0.25</cell><cell>OOM</cell><cell>49.44 ? 0.11</cell></row><row><cell>arxiv-year</cell><cell cols="4">random 20% 44.77 ? 0.10 39.19 ? 0.12 49.16 ? 0.42</cell><cell>47.18 ? 0.24</cell><cell>OOM</cell><cell>51.21 ? 0.44</cell></row><row><cell>arxiv-year</cell><cell cols="4">random 50% 47.74 ? 0.23 40.27 ? 0.20 53.53 ? 0.36</cell><cell>50.37 ? 0.25</cell><cell>OOM</cell><cell>53.57 ? 0.18</cell></row><row><cell>snap-patents</cell><cell cols="4">random 10% 32.50 ? 0.10 32.72 ? 0.10 49.74 ? 0.46</cell><cell>33.57 ? 0.06</cell><cell>OOM</cell><cell>44.31 ? 0.43</cell></row><row><cell>snap-patents</cell><cell cols="4">random 20% 32.97 ? 0.06 32.96 ? 0.09 54.32 ? 0.50</cell><cell>33.96 ? 0.06</cell><cell>OOM</cell><cell>49.55 ? 0.31</cell></row><row><cell>snap-patents</cell><cell cols="5">random 50% 33.52 ? 0.05 33.10 ? 0.09 60.12 ? 0.23 34.28 ? 0.07*</cell><cell>OOM</cell><cell>54.97 ? 0.23</cell></row><row><cell cols="2">Performance average</cell><cell>39</cell><cell>36</cell><cell>52</cell><cell>41</cell><cell>OOM</cell><cell>51</cell></row><row><cell cols="2">Overall performance average</cell><cell>55</cell><cell>56</cell><cell>61</cell><cell>57</cell><cell>OOM</cell><cell>64</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Codebook size abalation study.</figDesc><table><row><cell cols="3">codebook size ogbn-arxiv arxiv-year</cell></row><row><cell>512</cell><cell>71.92</cell><cell>50.90</cell></row><row><cell>1k</cell><cell>71.99</cell><cell>51.21</cell></row><row><cell>2k</cell><cell>72.14</cell><cell>52.51</cell></row><row><cell>4k</cell><cell>72.41</cell><cell>53.57</cell></row><row><cell cols="2">an interesting research opportunity.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Running time in second (s) per epoch of different models. GOAT-G refers to GOAT-Global-only model variant.</figDesc><table><row><cell>batch size</cell><cell>model</cell><cell cols="2">ogbn-arxiv ogbn-products</cell></row><row><cell>1k</cell><cell>GAT</cell><cell>9.76</cell><cell>127.42</cell></row><row><cell>1k</cell><cell>GOAT-G</cell><cell>2.88</cell><cell>5.58</cell></row><row><cell>1k</cell><cell>GOAT</cell><cell>8.00</cell><cell>109.51</cell></row><row><cell>4k</cell><cell>GAT</cell><cell>8.31</cell><cell>113.47</cell></row><row><cell>4k</cell><cell>GOAT-G</cell><cell>2.33</cell><cell>4.36</cell></row><row><cell>4k</cell><cell>GOAT</cell><cell>7.62</cell><cell>101.73</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Proposed Model Dataset-Specific Hyperparameter Settings</figDesc><table><row><cell>Model</cell><cell>Dataset</cell><cell cols="2"># Heads # Hidden Channels</cell></row><row><cell></cell><cell>ogbn-arxiv</cell><cell>4</cell><cell>128</cell></row><row><cell>GOAT</cell><cell>ogbn-products</cell><cell>2</cell><cell>256</cell></row><row><cell></cell><cell>arxiv-year</cell><cell>4</cell><cell>128</cell></row><row><cell></cell><cell>snap-patents</cell><cell>2</cell><cell>128</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Baseline Model Dataset-Specific Hyperparameter Settings</figDesc><table><row><cell>Model</cell><cell>Dataset</cell><cell cols="3"># Layers # Hidden Channels SAINT Batch Size</cell></row><row><cell></cell><cell>ogbn-arxiv</cell><cell>2</cell><cell>32</cell><cell>10,000</cell></row><row><cell>GAT</cell><cell>ogbn-products  *</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>arxiv-year</cell><cell>2</cell><cell>32</cell><cell>10,000</cell></row><row><cell></cell><cell>snap-patents</cell><cell>2</cell><cell>32</cell><cell>10,000</cell></row><row><cell></cell><cell>ogbn-arxiv</cell><cell>2</cell><cell>128</cell><cell>10,000</cell></row><row><cell>GCNJK</cell><cell>ogbn-products</cell><cell>4</cell><cell>256</cell><cell>5,000</cell></row><row><cell></cell><cell>arxiv-year</cell><cell>4</cell><cell>256</cell><cell>10,000</cell></row><row><cell></cell><cell>snap-patents</cell><cell>2</cell><cell>256</cell><cell>10,000</cell></row><row><cell></cell><cell>ogbn-arxiv</cell><cell>2</cell><cell>128</cell><cell>10,000</cell></row><row><cell>MixHop</cell><cell>ogbn-products</cell><cell>3</cell><cell>128</cell><cell>5,000</cell></row><row><cell></cell><cell>arxiv-year</cell><cell>4</cell><cell>128</cell><cell>10,000</cell></row><row><cell></cell><cell>snap-patents</cell><cell>2</cell><cell>128</cell><cell>10,000</cell></row><row><cell></cell><cell>ogbn-arxiv</cell><cell>1</cell><cell>64</cell><cell></cell></row><row><cell>LINKX</cell><cell>ogbn-products</cell><cell>1</cell><cell>128</cell><cell>N/A</cell></row><row><cell></cell><cell>arxiv-year</cell><cell>1</cell><cell>256</cell><cell></cell></row><row><cell></cell><cell>snap-patents</cell><cell>1</cell><cell>16</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>University of Maryland, College Park</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Capital One. Correspondence to: Kezhi Kong &lt;kong@cs.umd.edu&gt;, Tom Goldstein &lt;tomg@cs.umd.edu&gt;.Proceedings of the40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_2"><p>https://github.com/CUAI/Non-Homophily-Large-Scale</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_3"><p>https://ogb.stanford.edu/docs/leader nodeprop/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was made possible by support from <rs type="funder">Capital One Bank</rs> and the <rs type="funder">National Science Foundation</rs> (<rs type="grantNumber">IIS-2212182</rs>), the <rs type="funder">ONR MURI program</rs>, <rs type="funder">DARPA GARD</rs> (<rs type="grantNumber">HR00112020007</rs>), the <rs type="funder">Office of Naval Research</rs> (<rs type="grantNumber">N000142112557</rs>), and the <rs type="funder">AFOSR MURI program</rs>. Commercial support was provided by <rs type="funder">Capital One Bank</rs>, the <rs type="funder">Amazon</rs> <rs type="programName">Research Award program</rs>, and <rs type="person">Open Philanthropy</rs>. Further support was provided by the <rs type="funder">National Science Foundation</rs> (<rs type="grantNumber">IIS-2212182</rs>), and by the <rs type="institution">NSF TRAILS Institute</rs> (<rs type="grantNumber">2229885</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_y6rde8H">
					<idno type="grant-number">IIS-2212182</idno>
				</org>
				<org type="funding" xml:id="_WdwwbR6">
					<idno type="grant-number">HR00112020007</idno>
				</org>
				<org type="funding" xml:id="_jqjaRrX">
					<idno type="grant-number">N000142112557</idno>
				</org>
				<org type="funding" xml:id="_vcQF9k8">
					<orgName type="program" subtype="full">Research Award program</orgName>
				</org>
				<org type="funding" xml:id="_8uK9GTQ">
					<idno type="grant-number">IIS-2212182</idno>
				</org>
				<org type="funding" xml:id="_4FsCDPj">
					<idno type="grant-number">2229885</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr">Zhu, J., Yan, Y., Zhao, L., Heimann, M., Akoglu, L., and Koutra, D.</ref> <p>Beyond homophily in graph neural networks: Current limitations and effective designs. Advances in Neural Information Processing Systems, 33:7793-7804, 2020.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. More Theoretical Analysis</head><p>Note that in the demonstration process below, we assume positional information is already fused into node features.</p><p>Proposition A.1. There exists a distribution of random projection matrices P ? R n?k such that for any linear weight matrices</p><p>, and any choice of ? &gt; 0,</p><p>with</p><p>Our goal here is to find a projection matrix P to project both the attention matrix A B and features v into some low dimension space, enabling us to replace the attention matrix</p><p>Despite the existence of such P matrix, we argue that it is impractical to apply Proposition A.1 in practice. The reason is that it is hard to materialize A B in memory so the explicit computation of A B is inevitable, which does not help improve scalability as calculating all attention matrices batch-by-batch is still O(n 2 ). Below, we describe a route to escape this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. More Ablation Studies</head><p>Local range. To show the efficacy of our local module, we further run the ablation study to have nodes attend to 1-, 2-, 3-hop neighborhoods within the local transformer of GOAT. Results are included in Table <ref type="table">5</ref>. We can see that the local module plays an important role in extracting features from local fields and contributes to the final predictions of the GOAT model. Note that arxiv-year and snap-patents have 50% training samples. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Proofs</head><p>This section lists theoretical proofs of our propositions and theorems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Proof of Proposition A.1</head><p>Proof. To help build the proof, we utilize the Johnson-Lindenstrauss (JL) Lemma from <ref type="bibr" target="#b20">Johnson &amp; Lindenstrauss (1984)</ref>; <ref type="bibr" target="#b21">Kane &amp; Nelson (2014)</ref>.</p><p>Lemma C.1. For any integer d &gt; 0, and any 0 &lt; ?, ? &lt; 1/2, there exists a probability distribution on k ? d real matrices for k = ?(? -2 log(1/?)) such that for any x ? R d ,</p><p>Using Lemma C.1 and Boole's inequality, for any x, y ? R 1?n , we have</p><p>Now we choose to swap x with any row vector a out of A B and y with v so we have</p><p>where for the part a, we have</p><p>Note that</p><p>When we assume ?X? F = O(?A B ? F ), we have</p><p>C.4. Proof of Corollary 3.4</p><p>Proof.</p><p>X</p><p>note that the row vector of P ? R n?k is one-hot, so X B W Q K times P ? works as copying the attention scores according to the assignment of centroid of each node. In the same way, Softmax(?)P aggregates the Softmax logits based on the assignment through summation. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ver Steeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gal-Styan</surname></persName>
		</author>
		<author>
			<persName><surname>Mixhop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>PMLR</publisher>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
	<note>In international conference on machine learning</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><surname>Longformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<title level="m">The long-document transformer</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks</title>
		<author>
			<persName><forename type="first">W.-L</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Adaptive universal generalized pagerank graph neural network</title>
		<author>
			<persName><forename type="first">E</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Milenkovic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07988</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Node feature extraction by self-supervised multi-scale neighborhood prediction</title>
		<author>
			<persName><forename type="first">E</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Milenkovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.00064</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Rethinking attention with performers</title>
		<author>
			<persName><forename type="first">K</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.14794</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A generalization of transformer networks to graphs</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09699</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.07875</idno>
		<title level="m">Graph neural networks with learnable structural and positional representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gnnautoscale: Scalable and expressive graph neural networks via historical embeddings</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Weichert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3294" to="3304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robustness of graph neural networks at scale</title>
		<author>
			<persName><forename type="first">S</forename><surname>Geisler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>S ?irin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Z?gner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Inductive representation learning on large graphs. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22118" to="22133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Heterogeneous graph transformer</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
		<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="2704" to="2710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Combining label propagation and simple models out-performs graph neural networks</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-N</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Benson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.13993</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adaptive sampling towards fast graph representation learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Edgeaugmented graph transformers: Global self-attention is enough for graphs</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Zaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Subramanian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.03348</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Extensions of lipschitz mappings into a hilbert space 26</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lindenstrauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Contemporary mathematics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">28</biblScope>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sparser johnson-lindenstrauss transforms</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04451</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Flag: Adversarial data augmentation for graph neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.09891</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rethinking graph transformers with spectral attention</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kreuzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>L?tourneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tossou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Large scale learning on non-homophilous graphs: New benchmarks and strong simple methods</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hohne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bhalerao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Non-local graph neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Maziarka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Majchrowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Danel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gai?ski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tabor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Podolak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Morkisz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jastrzebski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.05841</idno>
		<title level="m">Relative molecule self-attention transformer</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Birds of a feather: Homophily in social networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mcpherson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Smith-Lovin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of sociology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="415" to="444" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Mialon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Selosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><surname>Graphit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05667</idno>
		<title level="m">Encoding graph structure in transformers</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Geom-gcn: Geometric graph convolutional networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-C</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05287</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Recipe for a general, powerful, scalable graph transformer</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ramp??ek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Galkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Beaini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.12454</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Self-supervised graph transformer on largescale molecular data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12559" to="12571" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Masked label prediction: Unified message passing model for semi-supervised classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.03509</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sparse sinkhorn attention</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-C</forename><surname>Juan</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9438" to="9447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Synthesizer: Rethinking self-attention for transformer models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-C</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10183" to="10192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><surname>Linformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<title level="m">Self-attention with linear complexity</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v80/xu18c.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</editor>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018-07">Jul 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Do transformers really perform badly for graph representation?</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="17283" to="17297" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Graphsaint: Graph sampling based inductive learning method</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Prasanna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04931</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Graph-bert: Only attention is needed for learning graph representations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.05140</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Gophormer: Ego-graph transformer for node classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.13094</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Long-short transformer: Efficient transformers for language and vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
