<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Approximation Ratios of Graph Neural Networks for Combinatorial Problems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ryoma</forename><surname>Sato</surname></persName>
							<email>r.sato@ml.ist.i</email>
							<affiliation key="aff0">
								<orgName type="institution">Kyoto University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Makoto</forename><surname>Yamada</surname></persName>
							<email>myamada@i</email>
							<affiliation key="aff0">
								<orgName type="institution">Kyoto University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hisashi</forename><surname>Kashima</surname></persName>
							<email>kashima@i.kyoto-u.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Kyoto University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Approximation Ratios of Graph Neural Networks for Combinatorial Problems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, from a theoretical perspective, we study how powerful graph neural networks (GNNs) can be for learning approximation algorithms for combinatorial problems. To this end, we first establish a new class of GNNs that can solve a strictly wider variety of problems than existing GNNs. Then, we bridge the gap between GNN theory and the theory of distributed local algorithms. We theoretically demonstrate that the most powerful GNN can learn approximation algorithms for the minimum dominating set problem and the minimum vertex cover problem with some approximation ratios with the aid of the theory of distributed local algorithms. We also show that most of the existing GNNs such as GIN, GAT, GCN, and GraphSAGE cannot perform better than with these ratios. This paper is the first to elucidate approximation ratios of GNNs for combinatorial problems. Furthermore, we prove that adding coloring or weak-coloring to each node feature improves these approximation ratios. This indicates that preprocessing and feature engineering theoretically strengthen model capabilities.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph neural networks (GNNs) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b21">22]</ref> is a novel machine learning method for graph structures. GNNs have achieved state-of-the-art performance in various tasks, including chemo-informatics <ref type="bibr" target="#b6">[7]</ref>, question answering systems <ref type="bibr" target="#b22">[23]</ref>, and recommendation systems <ref type="bibr" target="#b30">[31]</ref>, to name a few.</p><p>Recently, machine learning methods have been applied to combinatorial problems <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b26">27]</ref> to automatically obtain novel and efficient algorithms. Xu et al. <ref type="bibr" target="#b29">[30]</ref> analyzed the capability of GNNs for solving the graph isomorphism problem, and they found that GNNs cannot solve it but they are as powerful as the Weisfeiler-Lehman graph isomorphism test.</p><p>The minimum dominating set problem, minimum vertex cover problem, and maximum matching problem are examples of important combinatorial problems other than the graph isomorphism problem. These problems are all NP-hard. Therefore, under the assumption that P � = NP, GNNs cannot exactly solve these problems because they run in polynomial time with respect to input size. For NP-hard problems, many approximation algorithms have been proposed to obtain sub-optimal solutions in polynomial time <ref type="bibr" target="#b24">[25]</ref>, and approximation ratios of these algorithms have been studied to guarantee the performance of these algorithms.</p><p>In this paper, we study the approximation ratios of algorithms that GNNs can learn for combinatorial problems. To analyze the approximation ratios of GNNs, we bridge the gap between GNN theory and the theory of distributed local algorithms. Here, distributed local algorithms are distributed algorithms that use only a constant number of synchronous communication rounds <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b23">24]</ref>. Thanks to their relationship with distributed local algorithms, we can elucidate the lower bound of the approximation ratios of algorithms that GNNs can learn for combinatorial problems. As an example of our results, if the input feature of each node is the node degree alone, no GNN can solve (Δ + 1 − ε)-approximation for the minimum dominating set problem or (2 − ε)-approximation for the minimum vertex cover problem, where ε &gt; 0 is any real number and Δ is the maximum node degree.</p><p>In addition, thanks to this relationship, we find vector-vector consistent GNNs (VV C -GNNs), which are a novel class of GNNs. VV C -GNNs have strictly stronger capability than existing GNNs and have the same capability as a computational model of distributed local algorithms. Based on our key finding, we propose the consistent port numbering GNNs (CPNGNNs), which is the most powerful GNN model among VV C -GNNs. That is, for any graph problem that a VV C -GNN can solve, there exists a parameter of CPNGNNs that can also solve it. Interestingly, CPNGNNs are strictly more powerful than graph isomorphism networks (GIN), which were considered to be the most powerful GNNs <ref type="bibr" target="#b29">[30]</ref>. Furthermore, CPNGNNs achieve optimal approximation ratios among GNNs: CPNGNNs can solve (Δ + 1)-approximation for the minimum dominating set problem and 2-approximation for the minimum vertex cover problem. However, these approximation ratios are unsatisfactory because they are as high as those of simple greedy algorithms. One of the reasons for these high approximation ratios is that we only use node degrees as node features. We show that adding coloring or weak coloring to each node feature strengthens the capability of GNNs. For example, if we use weak 2-coloring as a node feature in addition to node degree, CPNGNNs can solve ( Δ+1</p><p>2 )-approximation for the minimum dominating set problem. Considering that any graph has weak 2-coloring and that we can easily calculate weak 2-coloring in linear time, it is interesting that such preprocessing and feature engineering can theoretically strengthen the model capability.</p><p>The contributions of this paper are summarized as follows:</p><p>• We reveal the relationships between the theory of GNNs and distributed local algorithms.</p><p>Namely, we show that the set of graph problems that GNN classes can solve is the same as the set of graph problems that distributed local algorithm classes can solve.</p><p>• We propose CPNGNNs, which is the most powerful GNN among the proposed GNN class.</p><p>• We elucidate the approximation ratios of GNNs for combinatorial problems including the minimum dominating set problem and the minimum vertex cover problem. This is the first paper to elucidate the approximation ratios of GNNs for combinatorial problems.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph Neural Networks</head><p>GNNs were first introduced by Gori et al. <ref type="bibr" target="#b7">[8]</ref> and Scarselli et al. <ref type="bibr" target="#b21">[22]</ref>. They obtained the node embedding by recursively applying the propagation function until convergence. Recently, Kipf and Welling <ref type="bibr" target="#b11">[12]</ref> proposed graph convolutional networks (GCN), which significantly outperformed existing methods, including non-neural network-based approaches. Since then, many graph neural networks have been proposed, such as GraphSAGE <ref type="bibr" target="#b8">[9]</ref> and the graph attention networks (GATs) <ref type="bibr" target="#b25">[26]</ref>.</p><p>Vinyals et al. <ref type="bibr" target="#b26">[27]</ref> proposed pointer networks, which can solve combinatorial problems on a plane, such as the convex hull problem and the traveling salesman problem. Bello et al. <ref type="bibr" target="#b3">[4]</ref> trained pointer networks using reinforcement learning to automatically obtain novel algorithms for these problems. Note that pointer networks are not GNNs. However, we introduce them here because they were the first to solve combinatorial problems using deep learning. Khalil et al. <ref type="bibr" target="#b10">[11]</ref> and Li et al. <ref type="bibr" target="#b15">[16]</ref> used GNNs to solve combinatorial problems. They utilized search methods with GNNs, whereas we use only GNNs to focus on the capability of GNNs.</p><p>Xu et al. <ref type="bibr" target="#b29">[30]</ref> analyzed the capability of GNNs. They showed that GNNs cannot solve the graph isomorphism problem and that the capability of GNNs is at most the same as that of the Weisfeiler-Lehman graph isomorphism test. They also proposed the graph isomorphism networks (GIN), which are as powerful as the Weisfeiler-Lehman graph isomorphism test. Therefore, the GIN is the most powerful GNNs. The motivation of this paper is the same as that of Xu et al.'s work <ref type="bibr" target="#b29">[30]</ref> but we consider not only the graph isomorphism problem but also the minimum dominating set problem, minimum vertex cover problem, and maximum matching problem. Furthermore, we find the approximation ratios of these problems for the first time and propose GNNs more powerful than GIN.</p><p>Algorithm 1 Calculating the embedding of a node using GNNs</p><formula xml:id="formula_0">Require: Graph G = (V, E, X); Parameters θ; Aggregation function f (l) θ (l = 1, . . . , L). Ensure: Embedding of nodes z ∈ R n×d L+1 1: z (1) v ← x v (∀v ∈ V ) 2: for l = 1, . . . , L do 3: for v ∈ V do 4: z (l+1) v ← f (l)</formula><p>θ (aggregated information from neighbor nodes of v)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>end for 6: end for 7: return z (L+1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Distributed Local Algorithms</head><p>A distributed local algorithm is a distributed algorithm that runs in constant time. More specifically, in a distributed local algorithm, we assume each node has infinite computational resources and decides the output within a constant number of communication rounds with neighboring nodes. For example, distributed local algorithms are used for controlling wireless sensor networks <ref type="bibr" target="#b12">[13]</ref>, constructing self-stabilization algorithms <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18]</ref>, and building sublinear-time algorithms <ref type="bibr" target="#b19">[20]</ref>.</p><p>Distributed local algorithms were first studied by Angluin <ref type="bibr" target="#b0">[1]</ref>, Linial <ref type="bibr" target="#b16">[17]</ref>, and Naor and Stockmeyer <ref type="bibr" target="#b17">[18]</ref>. Angluin <ref type="bibr" target="#b0">[1]</ref> showed that deterministic distributed algorithms cannot find a center of a graph without any unique node identifiers. Linial <ref type="bibr" target="#b16">[17]</ref> showed that no distributed local algorithms can solve 3-coloring of cycles, and they require Ω(log * n) communication rounds for distributed algorithms to solve the problem. Naor and Stockmeyer <ref type="bibr" target="#b17">[18]</ref> showed positive results for distributed local algorithms for the first time. For example, distributed local algorithms can find weak 2-coloring and solve a variant of the dining philosophers problem. Later, several non-trivial distributed local algorithms were found, including 2-approximation for the minimum vertex cover problem <ref type="bibr" target="#b1">[2]</ref>.</p><p>There are many computational models of distributed local algorithms. Some computational models use unique identifiers of nodes <ref type="bibr" target="#b17">[18]</ref>, port numbering <ref type="bibr" target="#b0">[1]</ref>, and randomness <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b27">28]</ref>, and other models do not <ref type="bibr" target="#b9">[10]</ref>. Furthermore, some results use the following assumptions about the input: degrees are bounded <ref type="bibr" target="#b1">[2]</ref>, degrees are odd <ref type="bibr" target="#b17">[18]</ref>, graphs are planar <ref type="bibr" target="#b5">[6]</ref>, and graphs are bipartite <ref type="bibr" target="#b2">[3]</ref>. In this paper, we do not use any unique identifiers nor randomness, but we do use port numbering, and we assume the degrees are bounded. We describe our assumptions in detail in Section 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Setting</head><p>Here, we first describe the notation used in this paper and then we formulate the graph problem.</p><p>Notation. For a positive integer k ∈ Z + , let [k] be the set {1, 2, . . . , k}. Let G = (V, E, X) be a input graph, where V is a set of nodes, E is a set of edges, and X ∈ R |V |×d0 is a feature matrix. We represent an edge of a graph G = (V, E, X) as an unordered pair {u, v} with u, v ∈ V . We write n = |V | for the number of nodes and m = |E| for the number of edges. The nodes V are considered to be numbered with A GNN model N θ (G, v) is a function parameterized by θ that takes a graph G and a node v ∈ V as input and output the label y v ∈ Y of node v, where Y is a set of labels. We study the expression capability of the function family N θ for combinatorial graph problems with the following assumptions.</p><p>Assumption 1 (Bounded-Degree Graphs). In this paper, we consider only bounded-degree graphs. In other words, for a fixed (but arbitrary) constant Δ, we assume that the degree of each node of the input graphs is at most Δ. This assumption is natural because there are many bounded-degree graphs in the real world. For example, degrees in molecular graphs are bounded by four, and the degrees in computer networks are bounded by the number of LAN ports of routers. Moreover, the bounded-degree assumption is often used in distributed local algorithms <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24]</ref>. For each positive integer Δ ∈ Z + , let F(Δ) be the set of all graphs with maximum degrees of Δ at most. Assumption 2 (Node Features). We do not consider node features other than those that can be derived from the input graph itself for focusing on graph theoretic properties. When there are no node features available, the degrees of nodes are sometimes used <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30]</ref>. Therefore, we use only the degree of a node as the node feature (i.e., z</p><formula xml:id="formula_1">(1) v = ONEHOT(deg(v)</formula><p>)) unless specified. Later, we show that using coloring or weak coloring of the input graph in addition to degrees of nodes as node features makes models theoretically more powerful.</p><formula xml:id="formula_2">Graph Problems. A graph problem is a function Π that associates a set Π(G) of solutions with each graph G = (V, E). Each solution S ∈ Π(G) is a function S : V → Y . Y is a finite set that is independent of G.</formula><p>We say a GNN model N θ solves a graph problem Π if for any Δ ∈ Z + , there exists a parameter θ such that for any graph</p><formula xml:id="formula_3">G ∈ F(Δ), N θ (G, •) is in Π(G).</formula><p>For example, let Y be a set of labels of nodes, let L(G) : V → Y be the ground truth of a multi-label classification problem for a graph G (i.e., L(G)(v) denotes the ground truth label of node v ∈ V ), and let </p><formula xml:id="formula_4">Π(G) = {f : V → {0, 1} | |{v ∈ V | f (v) = L(G)(v)}| ≥ 0.9 • |V |}.</formula><formula xml:id="formula_5">(G) = {f : V → {0, 1} | D = {v | f (v) = 1} is a vertex cover and |D| ≤ 2 • |C(G)|}.</formula><p>This graph problem Π corresponds to 2-approximation for the minimum vertex cover problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Known Model Classes</head><p>We introduce two known classes of GNNs, which include GraphSAGE <ref type="bibr" target="#b8">[9]</ref>, GCN <ref type="bibr" target="#b11">[12]</ref>, GAT <ref type="bibr" target="#b25">[26]</ref>, and GIN <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MB-GNNs.</head><p>A layer of an existing GNN can be written as</p><formula xml:id="formula_6">z (l+1) v = f (l) θ (z (l) v , MULTISET(z (l) u | u ∈ N (v))), where f (l)</formula><p>θ is a learnable aggregation function. We call GNNs that can be written in this form multisetbroadcasting GNNs (MB-GNNs) -multiset because they aggregate features from neighbors as a multiset and broadcasting because for any v ∈ N (u), the "message" <ref type="bibr" target="#b6">[7]</ref> from u to v is the same (i.e., z u ). GraphSAGE-mean <ref type="bibr" target="#b8">[9]</ref> is an example of MB-GNNs because a layer of GraphSAGE-mean is represented by the following equation:</p><formula xml:id="formula_7">z (l+1) v = CONCAT(z (l) v , 1 |N (v)| � u∈N (v) W (l) z (l) u ),</formula><p>where CONCAT concatenates vectors into one vector. Other examples of MB-GNNs are GCN <ref type="bibr" target="#b11">[12]</ref>, GAT <ref type="bibr" target="#b25">[26]</ref>, and GIN <ref type="bibr" target="#b29">[30]</ref>.</p><p>SB-GNNs. The another existing class of GNNs in the literature is set-broadcasting GNNs (SB-GNNs), which can be written as the following form: <ref type="bibr" target="#b8">[9]</ref> is an example of SB-GNNs because a layer of GraphSAGE-mean is represented by the following equation:</p><formula xml:id="formula_8">z (l+1) v = f (l) θ (z (l) v , SET(z (l) u | u ∈ N (v))). GraphSAGE-pool</formula><formula xml:id="formula_9">z (l+1) v = max({σ(W (l) z (l) u + b (l) ) | u ∈ N (v)}).</formula><p>Clearly, SB-GNNs are a subclass of MB-GNNs. Xu et al. <ref type="bibr" target="#b29">[30]</ref> discussed the differences in capability of SB-GNNs and MB-GNNs. We show that MB-GNNs are strictly stronger than SB-GNNs in another way in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Novel Class of GNNs</head><p>In this section, we first introduce a GNN class that is more powerful than MB-GNNs and SB-GNNs. To make GNN models more powerful than MB-GNNs, we introduce the concept of port numbering <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10]</ref> to GNNs.</p><formula xml:id="formula_10">Port Numbering. A port of a graph G is a pair (v, i), where v ∈ V and i ∈ [deg(v)]. Let P (G) = {(v, i) | v ∈ V, i ∈ [deg(v)]</formula><p>} be the set of all ports of a graph G. A port numbering of a graph G is the function p : P (G) → P (G) such that for any edge {u, v}, there exist i ∈ [deg(u)] and j ∈ [deg(v)] such that p(u, i) = (v, j). We say that a port numbering is consistent if p is an involution (i.e., ∀(v, i) ∈ P (G) p(p(v, i)) = (v, i)). We define the functions p tail : V × Δ → V ∪ {−} and p n : V × Δ → Δ ∪ {−} as follows:</p><formula xml:id="formula_11">p tail (v, i) = � u ∈ V (∃j ∈ [deg(u)] s.t. p(u, j) = (v, i)) (i ≤ deg(v)) − (otherwise), p n (v, i) = � j ∈ [deg(p tail (v, i))] (p(p tail (v, i), j) = (v, i)) (i ≤ deg(v)) − (otherwise)</formula><p>, where − is a special symbol that denotes the index being out of range. Note that these functions are well-defined because there always exists only one u ∈ V for p tail and j ∈ [deg(p tail (v, i))] for p n if i ≤ deg(v). Intuitively, p tail (v, i) represents the node that sends messages to the port i of node v and p n (v, i) represents the port number of the node p tail (v, i) that sends messages to the port i of node v.</p><p>The GNN class we introduce in the following uses a consistent port numbering to calculate embeddings. Intuitively, SB-GNNs and MB-GNNs send the same message to all neighboring nodes. GNNs can send different messages to neighboring nodes by using port numbering, and this strengthens model capability.</p><p>VV C -GNNs. Vector-vector consistent GNNs (VV C -GNNs) are a novel class of GNNs that we introduce in this paper. They calculate an embedding with the following formula:</p><formula xml:id="formula_12">z (l+1) v = f (l) θ (z (l) v , z<label>(l) ptail(v,1) , p n (v, 1), z (l)</label></formula><formula xml:id="formula_13">ptail(v,2) , p n (v, 2), . . . , z<label>(l)</label></formula><p>ptail(v,Δ) , p n (v, Δ)). If the index of z is the special symbol −, we also define the embedding as the special symbol − (i.e., z − = −). To calculate embeddings of nodes of a graph G using a GNN with port numbering, we first calculate one consistent port numbering p of G, and then we input G and p to the GNN. Note that we can calculate a consistent port numbering of a graph in linear time by numbering edges one by one. We say a GNN class N with port numbering solves a graph problem Π if for any Δ ∈ Z + , there exists a GNN N θ ∈ N and its parameter θ such that for any graph G ∈ F(Δ), for any consistent port numbering p of G, the output N θ (G, p, •) is in Π(G). We show that using port numbering theoretically improves model capability in Section 5.2. We propose CPNGNNs, an example of VV C -GNNs, in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">GNNs with Distributed Local Algorithms</head><p>In this section, we discuss the relationship between GNNs and distributed local algorithms. Thanks to this relationship, we can elucidate the theoretical properties of GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Relationship with Distributed Local Algorithms</head><p>A distributed local algorithm is a distributed algorithm that runs in constant time. More specifically, in a distributed local algorithm, we assume each node has infinite computational resources and decides the output within a constant number of communication rounds with neighboring nodes. In this paper, we show a clear relationship between distributed local algorithms and GNNs for the first time.</p><p>There are several well-known models of distributed local algorithms <ref type="bibr" target="#b9">[10]</ref>. Namely, in this paper, we introduce the SB(1), MB(1), and VV C (1) models. As their names suggest, they correspond to SB-GNNs, MB-GNNs, and VV C -GNNs, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Assumption 3 (Finite Node Features):</head><p>The number of possible node features is finite.</p><p>Assumption 3 restricts node features be discrete. However, Assumption 3 does include the node degree feature (∈ [Δ]) and node coloring feature (∈ {0, 1}). Theorem 1. Let L be SB, MB, or VV C . Under Assumption 3, the set of graph problems that at least one L-GNN can solve is the same as the set of graph problems that at least one distributed local algorithm on the L(1) model solve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 CPNGNN: The most powerful VV C -GNN</head><formula xml:id="formula_14">Require: Graph G = (V, E, X); Maximum degree Δ ∈ Z + ; Weight matrix W (l) ∈ R d l+1 ×(d l +Δ(d l +1</formula><p>)) (l = 1, . . . , L). Ensure: Output for the graph problem y ∈ Y n 1: calculate a consistent port numbering p 2: z</p><formula xml:id="formula_15">(1) v ← x v (∀v ∈ V ) 3: for l = 1, . . . , L do 4: for v ∈ V do 5: z (l+1) v ← W (l) CONCAT(z (l) v , z (l) ptail(v,1) , p n (v, 1), z (l) ptail(v,2) , p n (v, 2), . . . , z (l) ptail(v,Δ) , p n (v, Δ)) 6: z (l+1) v ← RELU(z (l+1) v ) 7:</formula><p>end for 8: end for 9: for v ∈ V do 10:</p><formula xml:id="formula_16">z v ← MULTILAYERPERCEPTRON(z (L+1) v )</formula><p># calculate the final embedding of a node v.</p><p>11:</p><formula xml:id="formula_17">y v ← argmax i∈[d L+1 ] z vi</formula><p># output the index of the maximum element. 12: end for 13: return y All proofs are available in the supplementary materials. In fact, the following stronger properties hold: (i) any L-GNN can be simulated by the L(1) model and (ii) any distributed local algorithm on L(1) model can be simulated by an L-GNN. The former is obvious because GNNs communicate with neighboring nodes in L rounds, where L is the number of layers. The latter is natural because the definition of L-GNNs (Section 3. <ref type="figure">2 and 4</ref>) is intrinsically the same as the definition of the L(1) model. Thanks to Theorem 1, we can prove which combinatorial problems GNNs can/cannot solve by using theoretical results on distributed local algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Hierarchy of GNNs</head><p>There are obvious inclusion relations among classes of GNNs. Namely, SB-GNNs are a subclass of MB-GNNs, and MB-GNNs are a subclass of VV C -GNNs. If a model class A is a subset of a model class B, the graph problems that A solves is a subset of the graph problems that B solves. However, it is not obvious whether the proper inclusion property holds or not. Let P SB-GNNs , P MB-GNNs , and P VVC-GNNs be the sets of graph problems that SB-GNNs, MB-GNNs, and VV C -GNNs can solve only with the degree features, respectively. Thanks to the relationship between GNNs and distributed local algorithms, we can show that the proper inclusion properties of these classes hold. Theorem 2. P SB-GNNs � P MB-GNNs � P VVC-GNNs .</p><p>An example graph problem that MB-GNNs cannot solve but VV C -GNNs can solve is the finding single leaf problem <ref type="bibr" target="#b9">[10]</ref>. The input graphs of the problem are star graphs and the ground truth contains only a single leaf node. MB-GNNs cannot solve this problem because for each layer, the embeddings of the leaf nodes are exactly same, and the GNN cannot distinguish these nodes. Therefore, if a GNN includes one leaf node in the output, the other leaf nodes are also included to the output. On the other hand, VV C -GNNs can distinguish each leaf node using port numbering and can appropriately output only a single node. We confirm this fact through experiments in the supplementary materials.</p><p>6 Most Powerful GNN for Combinatorial Problems</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Consistent Port Numbering Graph Neural Networks (CPNGNNs)</head><p>In this section, we propose the most powerful VV C -GNNs, CPNGNNs. The most similar algorithm to CPNGNNs is GraphSAGE <ref type="bibr" target="#b8">[9]</ref>. The key differences between GraphSAGE and CPNGNNs are as follows: (i) CPNGNNs use port numbering and (ii) GPNGNNs aggregate features of neighbors by concatenation. We show pseudo code of CPNGNNs in Algorithm 2. Though CPNGNNs are simple, they are the most powerful among VV C -GNNs. This claim is supported by Theorem 3, where we do not limit node features to the node degree feature. Theorem 3. Let P CPNGNNs be the set of graph problems that CPNGNNs can solve and P VVC-GNNs be the set of graph problems that VV C -GNNs can solve. Then, under Appsumtion 3, P CPNGNNs = P VVC-GNNs .</p><p>The advantages of CPNGNNs are twofold: they can solve a strictly wider set of graph problems than existing models (Theorem 2 and 3). There are many distributed local algorithms that can be simulated by CPNGNNs and we can prove that CPNGNNs can solve a variety of combinatorial problems (see Section 6.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Combinatorial Problems that CPNGNNs Can/Cannot Solve</head><p>In Section 5.2, we found that there exist graph problems that certain GNNs can solve but others cannot. However, there remains a question. What kind of graph problems can/cannot GNNs solve? In this paper, we study combinatorial problems, including the minimum dominating set problem, maximum matching problem, and minimum vertex cover problem. If GNNs can solve combinatorial problems, we may automatically obtain new algorithms for combinatorial problems by simply training GNNs. Note that from Theorems 2 and 3, if CPNGNNs cannot solve a graph problem, other GNNs cannot solve the problem. Therefore, it is important to investigate the capability of GPNGNNs to study the limitations of GNNs.</p><p>Minimum Dominating Set Problem. First, we investigate the minimum dominating set problem. Theorem 4. The optimal approximation ratio of CPNGNNs for the minimum dominating set problem is (Δ + 1). In other words, CPNGNNs can solve (Δ + 1)-approximation for the minimum dominating set problem, but for any 1 ≤ α &lt; Δ + 1, CPNGNNs cannot solve α-approximation for the minimum dominating set problem.</p><p>Here, CPNGNNs can solve f (Δ) approximation for the minimum dominating set problem means that for all Δ ∈ Z + , there exists a paramter θ such that for all input G ∈ F(Δ), {v ∈ V | CPNGNN θ (G, v) = 1} forms f (Δ) approximatoin of the minimum dominating set of G. However, (Δ + 1)-approximation is trivial because it can be achieved by outputting all the nodes. Therefore, Theorem 4 says that any GNN is as bad as the trivial algorithm in the worst case, which is unsatisfactory. This is possibly because we only use the degree information of local nodes, and we may improve the approximation ratio if we use information other than node degree. Interestingly, we can improve the approximation ratio just by using weak 2-coloring as a feature of nodes. A weak 2-coloring is a function c : V → {0, 1} such that for any node v ∈ V , there exists a neighbor u ∈ N (v) such that c(v) � = c(u). Note that any graph has a weak 2-coloring and that we can calculate a weak 2-coloring in linear time by a breadth-first search. In the theorems below, we use not only the degree deg(v) but also the color c(v) as a feature vector of a node v ∈ V . There may be many weak 2-colorings of a graph G. However, the choice of c is arbitrary. Theorem 5. If the feature vector of a node is consisted of the degree and the color of a weak 2-coloring, the optimal approximation ratio of CPNGNNs for the minimum dominating set problem is ( Δ+1 2 ). In other words, CPNGNN can solve ( Δ+1 2 )-approximation for the minimum dominating set problem, and for any 1 ≤ α &lt; Δ+1 2 , CPNGNN cannot solve α-approximation for the minimum dominating set problem.</p><p>In the minimum dominating set problem, we cannot improve the approximation ratio by using 2-coloring instead of weak 2-coloring. Theorem 6. Even if the feature vector of a node is consisted of the degree and the color of a 2-coloring, for any 1 ≤ α &lt; Δ+1 2 , CPNGNNs cannot solve α-approximation for the minimum dominating set problem.</p><p>Minimum Vertex Cover Problem. Next, we investigate the minimum vertex cover problem. Theorem 7. The optimal approximation ratio of CPNGNNs for the minimum vertex cover problem is 2. In other words, CPNGNNs can solve 2-approximation for the minimum vertex cover problem, and for any 1 ≤ α &lt; 2, CPNGNNs cannot solve α-approximation for the minimum vertex cover problem.</p><p>The simple greedy algorithm can solve 2-approximation for the minimum vertex cover problem. However, this result is not trivial because the algorithm that GNNs learn is not a regular algorithm but a distributed local algorithm. The distributed local algorithm for 2-approximation for the minimum vertex cover problem is known but not so simple <ref type="bibr" target="#b1">[2]</ref>. This result also says that if one wants to find an approximation algorithm using a machine learning approach with better performance than 2-approximation, they must use a non-GNN model or combine GNNs with other methods (e.g., a search method).</p><p>Maximum Matching Problem. Lastly, we investigate the maximum matching problem. So far, we have only investigated problems on nodes, not edges. We must specify how GNNs output edge labels. Graph edge problems are defined similarly to graph problems, but their solutionas are functions E → Y . In this paper, we only consider Y = {0, 1} and we only use VV C -GNNs for solving graph edge problems. Let G ∈ F(Δ) be a graph and p be a port numbering of G. To solve graph edge problems, GNNs output a vector y(v) ∈ {0, 1} Δ for each node v ∈ V . For each edge {u, v}, GNNs include the edge {u, v} in the output if and only if y(u) i = y(v) j = 1, where p(u, i) = (v, j) and p(v, j) = (u, i). Intuitively, each node outputs "yes" or "no" to each incident edge (i.e., a port) and we include an edge in the output if both ends output "yes" to the edge. As with graph problems, we say a class N of GNNs solves a graph edge problem Π if for any Δ ∈ Z + , there exists a GNN N θ ∈ N and its parameter θ such that for any graph G ∈ F(Δ) and any consistent port numbering p of G, the output N θ (G, p) is in Π(G).</p><p>We investigate the maximum matching problem in detail. In fact, GNNs cannot solve the maximum matching problem at all. Theorem 8. For any α ∈ R + , CPNGNNs that cannot solve α-approximation for the maximum matching problem.</p><p>However, CPNGNNs can approximate the maximum matching problem with weak 2-coloring feature. Theorem 9. If the feature vector of a node is consisted of the degree and the color of a weak 2coloring, the optimal approximation ratio of CPNGNNs for the maximum matching problem is ( Δ+1 2 ). In other words, CPNGNNs can solve ( Δ+1</p><p>2 )-approximation for the maximum matching problem, and for any 1 ≤ α &lt; Δ+1 2 , CPNGNNs cannot solve α-approximation for the maximum matching problem.</p><p>Furthermore, if we use 2-coloring instead of weak 2-coloring, we can improve the approximation ratio. In fact, it can achieve any approximation ratio. Note that only a bipartite graph has 2-coloring. Therefore, the graph class is implicitly restricted to bipartite graphs in this case.</p><p>Theorem 10. If the feature vector of a node is consisted of the degree and the color of a 2-coloring, for any 1 &lt; α, CPNGNNs can solve α-approximation for the maximum matching problem.</p><p>In this paper, we consider only bounded-degree graphs. This assumption is natural, but it is also important to consider graphs without degree bounds. Dealing with such graphs is difficult because graph problems on them are not constant size <ref type="bibr" target="#b23">[24]</ref>. Note that solving graph problems becomes more difficult if we do not have the bounded-degree assumption. Therefore, GNNs cannot solve (Δ + 1 − ε)-approximation for the minimum dominating set problems or (2 − ε)-approximation for the minimum vertex cover problem in the general case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we introduced VV C -GNNs, which are a new class of GNNs, and CPNGNNs, which are an example of VV C -GNNs. We showed that VV C -GNNs have the same ability to solve graph problems as a computational model of distributed local algorithms. With the aid of distributed local algorithm theory, we elucidated the approximation ratios of algorithms that CPNGNNs can learn for combinatorial graph problems such as the minimum dominating set problem and the minimum vertex cover problem. This paper is the first to show the approximation ratios of GNNs for combinatorial problems. Moreover, this is a lower bound of approximation ratios for all GNNs. We further showed that adding coloring or weak coloring to a node feature improves these approximation ratios. This indicates that preprocessing and feature engineering theoretically strengthen model capability.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>[n]. (i.e., we assume V = [n].) For a node v ∈ V , deg(u) denotes the degree of node v and N (v) denotes the set of neighbors of node v.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>This graph problem Π corresponds to a multi-label classification problem. A GNN model N θ solves Π means there exists a parameter θ of the model such that achieves an accuracy 0.9 for this problem. Other examples of graph problems are combinatorial problems. Let C(G) ⊂ V be the minimum vertex cover of a graph G, let Y = {0, 1}, and let Π</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="33" xml:id="foot_0">33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by JSPS KAKENHI Grant Number 15H01704. MY is supported by the JST PRESTO program JPMJPR165A.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Local and global properties in networks of processors (extended abstract)</title>
		<author>
			<persName><forename type="first">Dana</forename><surname>Angluin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Annual ACM Symposium on Theory of Computing</title>
				<meeting>the 12th Annual ACM Symposium on Theory of Computing</meeting>
		<imprint>
			<date type="published" when="1980">1980</date>
			<biblScope unit="page" from="82" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A local 2-approximation algorithm for the vertex cover problem</title>
		<author>
			<persName><forename type="first">Matti</forename><surname>Åstrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrik</forename><surname>Floréen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Polishchuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Rybicki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jukka</forename><surname>Suomela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jara</forename><surname>Uitto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 23rd International Symposium on Distributed Computing, DISC 2009</title>
				<meeting>23rd International Symposium on Distributed Computing, DISC 2009</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="191" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Local algorithms in (weakly) coloured graphs</title>
		<author>
			<persName><forename type="first">Matti</forename><surname>Åstrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Polishchuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Rybicki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jukka</forename><surname>Suomela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jara</forename><surname>Uitto</surname></persName>
		</author>
		<idno>CoRR, abs/1002.0125</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Neural combinatorial optimization with reinforcement learning</title>
		<author>
			<persName><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR, abs/1611.09940</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Approximation by superpositions of a sigmoidal function</title>
		<author>
			<persName><forename type="first">George</forename><surname>Cybenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MCSS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="303" to="314" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast distributed approximations in planar graphs</title>
		<author>
			<persName><forename type="first">Andrzej</forename><surname>Czygrinow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Hanckowiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Wawrzyniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 22nd International Symposium on Distributed Computing, DISC 2008</title>
				<meeting>22nd International Symposium on Distributed Computing, DISC 2008</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="78" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Neural Networks, IJCNN 2005</title>
				<meeting>the International Joint Conference on Neural Networks, IJCNN 2005</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017. 2017. 2017</date>
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Weak models of distributed computing, with connections to modal logic</title>
		<author>
			<persName><forename type="first">Lauri</forename><surname>Hella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matti</forename><surname>Järvisalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antti</forename><surname>Kuusisto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juhana</forename><surname>Laurinharju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuomo</forename><surname>Lempiäinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kerkko</forename><surname>Luosto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jukka</forename><surname>Suomela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonni</forename><surname>Virtema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Symposium on Principles of Distributed Computing, PODC 2012</title>
				<meeting>the ACM Symposium on Principles of Distributed Computing, PODC 2012</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="185" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning combinatorial optimization algorithms over graphs</title>
		<author>
			<persName><forename type="first">Elias</forename><forename type="middle">B</forename><surname>Khalil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bistra</forename><surname>Dilkina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017. 2017. 2017</date>
			<biblScope unit="page" from="6351" to="6361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno>CoRR, abs/1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distributed algorithms for transmission power control in wireless sensor networks</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Kubisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Karl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Wolisz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhi</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Charlie</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">M</forename><surname>Rabaey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 IEEE Wireless Communications and Networking, WCNC 2003</title>
				<meeting>the 2003 IEEE Wireless Communications and Networking, WCNC 2003</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="558" to="563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Local algorithms: Self-stabilization on speed</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Lenzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jukka</forename><surname>Suomela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Wattenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 11th International Symposium on Stabilization, Safety, and Security of Distributed Systems</title>
				<meeting>11th International Symposium on Stabilization, Safety, and Security of Distributed Systems</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="17" to="34" />
		</imprint>
	</monogr>
	<note>SSS 2009</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Leveraging linial&apos;s locality limit</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Lenzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Wattenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 22nd International Symposium on Distributed Computing, DISC 2008</title>
				<meeting>22nd International Symposium on Distributed Computing, DISC 2008</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="394" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Combinatorial optimization with graph convolutional networks and guided tree search</title>
		<author>
			<persName><forename type="first">Zhuwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018. 2018</date>
			<biblScope unit="page" from="537" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Locality in distributed graph algorithms</title>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Linial</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Comput</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="193" to="201" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">What can be computed locally?</title>
		<author>
			<persName><forename type="first">Moni</forename><surname>Naor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><forename type="middle">J</forename><surname>Stockmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Comput</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1259" to="1277" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Constant-time approximation algorithms via local improvements</title>
		<author>
			<persName><forename type="first">N</forename><surname>Huy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><surname>Onak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual IEEE Symposium on Foundations of Computer Science, FOCS</title>
				<meeting>the 49th Annual IEEE Symposium on Foundations of Computer Science, FOCS</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="327" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Approximating the minimum vertex cover in sublinear time and a connection to distributed algorithms</title>
		<author>
			<persName><forename type="first">Michal</forename><surname>Parnas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dana</forename><surname>Ron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theor. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">381</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="183" to="196" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">struc2vec: Learning node representations from structural identity</title>
		<author>
			<persName><forename type="first">Leonardo</forename><surname>Filipe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodrigues</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">H P</forename><surname>Saverese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">R</forename><surname>Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="385" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename><surname>Ah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Sejr</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno>CoRR, abs/1703.06103</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Survey of local algorithms</title>
		<author>
			<persName><forename type="first">Jukka</forename><surname>Suomela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">40</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Approximation algorithms</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vijay</surname></persName>
		</author>
		<author>
			<persName><surname>Vazirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>ICLR 2018</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Learning Representations</title>
				<meeting>the 6th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2015">2015. 2015. 2015</date>
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distributed weighted matching</title>
		<author>
			<persName><forename type="first">Mirjam</forename><surname>Wattenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Wattenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 18th International Symposium on Distributed Computing, DISC 2004</title>
				<meeting>18th International Symposium on Distributed Computing, DISC 2004</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="335" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno>CoRR, abs/1810.00826</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
