<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Improving Faithfulness in Abstractive Summarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiuying</forename><surname>Chen</surname></persName>
							<email>xiuying.chen@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="department">Computational Bioscience Reseach Center</orgName>
								<orgName type="institution">King Abdullah University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mingzhe</forename><surname>Li</surname></persName>
							<email>limingzhe.lmz@antgroup.com</email>
							<affiliation key="aff1">
								<orgName type="department">Ant Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Gao</surname></persName>
							<email>xin.gao@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="department">Computational Bioscience Reseach Center</orgName>
								<orgName type="institution">King Abdullah University of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">SDAIA-KAUST AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiangliang</forename><surname>Zhang</surname></persName>
							<email>xzhang33@nd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computational Bioscience Reseach Center</orgName>
								<orgName type="institution">King Abdullah University of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Notre Dame</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Improving Faithfulness in Abstractive Summarization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite the success achieved in neural abstractive summarization based on pretrained language models, one unresolved issue is that the generated summaries are not always faithful to the input document. There are two possible causes of the unfaithfulness problem: (1) the summarization model fails to understand or capture the gist of the input text, and (2) the model over-relies on the language model to generate fluent but inadequate words. In this work, we propose a Faithfulness Enhanced Summarization model (FES), which is designed for addressing these two problems and improving faithfulness in abstractive summarization. For the first problem, we propose to use question-answering (QA) to examine whether the encoder fully grasps the input document and can answer the questions on the key information in the input. The QA attention on the proper input words can also be used to stipulate how the decoder should attend to the source. For the second problem, we introduce a max-margin loss defined on the difference between the language and the summarization model, aiming to prevent the overconfidence of the language model. Extensive experiments on two benchmark summarization datasets, CNN/DM and XSum, demonstrate that our model significantly outperforms strong baselines. The evaluation of factual consistency also shows that our model generates more faithful summaries than baselines 2 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, text generation has made impressive progress <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>. The abstractive summarization task, aiming to produce a concise and fluent summary that is salient and faithful to the source document, has become a research hotspot due to its broad application prospect. The prevalence of pretrained transformer language models (LM) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> has largely improved the fluency and salience of generated summaries. However, studies <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> showed that many summarization models suffer from unfaithfulness problem, i.e., the generated summary is not entailed by the information presented in the source document. Durmus et al. <ref type="bibr" target="#b7">[8]</ref> highlighted two notions of the unfaithfulness problem in summarization: one is the manipulation of information presented in the input document (intrinsic errors), and the other is the inclusion of information not inferable from the input (extrinsic errors).</p><p>The Intrinsic error problem is often caused by the failure of document level inference, which is necessary for abstractive summarization. Specifically, the summarization model has misinformation inferred from the input document because of an inadequate encoder that misunderstands the source semantic information and a poor decoder that cannot fetch relevant and consistent content from the encoder. Several recent summarization models were proposed from this perspective. For example, Wu et al. <ref type="bibr" target="#b8">[9]</ref> proposed a unified semantic graph encoder to learn better semantic meanings and a graph-aware decoder to utilize the encoded information. Cao et al. <ref type="bibr" target="#b9">[10]</ref> used contrastive learning to help the model be aware of the factual information. The second type of error, extrinsic error, is often introduced by excessive attention paid to the LM, which ensures fluency while neglecting to summarize the source document. For example, a LM is inclined to generate the commonly-used phrase "score the winner" while the correct phrase is "score the second highest" which is less frequently used. This type of error has been studied in the neural machine translation task <ref type="bibr" target="#b10">[11]</ref>, but has not been addressed in abstractive summarization.</p><p>To address these errors, we propose a novel Faithfulness Enhanced Summarization model (FES). To prevent the intrinsic error problem, we design FES in a multi-task learning paradigm, i.e., completing encoding-decoding for the summarization task with an auxiliary QA-based faithfulness evaluation task. The QA task poses an additional reasoning requirement on the encoder to have a more comprehensive understanding on the key semantic meanings of the input document and learn better representations than working only for summarization. The QA attention on the key entities of the input can also be used to align the decoder state with the encoder outputs for generating a faithful summary. To address the extrinsic error problem, we propose a max-margin loss to prevent the LM from being overconfident. Concretely, we define an indicator of the overconfidence degree of the LM. The risk of outputting extrinsic error tokens with low prediction probabilities is mitigated by minimizing this overconfidence indicator.</p><p>We validate the effectiveness of our FES model by conducting extensive experiments on public benchmark CNN/DM <ref type="bibr" target="#b11">[12]</ref> and XSum <ref type="bibr" target="#b12">[13]</ref> datasets. Experimental results demonstrate that our faithfulness enhanced summarization model has superior performance on the ROUGE scores and improves the faithfulness of news summarization over several strong baselines.</p><p>Our main contributions can be summarized as follows. <ref type="bibr" target="#b0">(1)</ref> We propose a faithfulness enhanced summarization model, which alleviates the unfaithfulness problem from the encoder side and decoder side. ( <ref type="formula" target="#formula_2">2</ref>) Concretely, we propose a multi-task framework to enhance the summarization performance by automatic QA tasks. We also propose a max-margin loss to control the overconfident problem of the LM. (3) Experimental results demonstrate that our proposed approach brings substantial improvements over the most recent baselines on benchmark datasets, and can also improve the faithfulness of the generated summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Abstractive Summarization. In recent years, the research on text generation has made impressive progress <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>, which promotes the progress of abstractive summarization. The abstractive summarization task generates novel words and phrases not featured in the source text to capture the salient ideas of the source text <ref type="bibr" target="#b15">[16]</ref>. Most works apply an encoder-decoder architecture to implicitly learn the summarization procedure <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. More recently, applying pretrained language models as encoder <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19]</ref> or pre-training the generation process by leveraging a large-scale of unlabeled corpus <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> brings significant improvements. Explicit structure modeling has also been shown to be effective in summarization tasks. For example, Jin et al. <ref type="bibr" target="#b21">[22]</ref> incorporated semantic dependency graphs to help generate sentences with better semantic relevance, and Wu et al. <ref type="bibr" target="#b8">[9]</ref> came up with a unified semantic graph to aggregate relevant disjoint context from the input.</p><p>Fact Consistency for Abstractive Summarization. Producing a summary that is entailed by the information presented in the source document is a key challenge in the summarization task, and less progress has been made on it. Pioneer works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> incorporated fact descriptions or entailment knowledge to enhance faithfulness. More recently, Zhu et al. <ref type="bibr" target="#b24">[25]</ref> modeled the facts in the source article with knowledge graphs based on a graph neural network. Cao et al. <ref type="bibr" target="#b9">[10]</ref> proposed to leverage reference summaries as positive training data and erroneous summaries as negative data, to train summarization systems that are better at distinguishing between them. Aralikatte et al. <ref type="bibr" target="#b25">[26]</ref> introduced focus attention mechanism to encourage decoders to proactively generate tokens that are similar or topical to the input document. On the contrary, other works post-edit the generated summaries. Different from previous works, we enhance the semantic understanding of the document with faithfulness evaluation as a direct signal and prevent the overconfidence of LM. Multi-task Learning. Multi-task learning is a learning paradigm in machine learning and it aims to leverage useful information contained in multiple related tasks to help improve the generalization performance of all the tasks <ref type="bibr" target="#b26">[27]</ref>. There is a large quantity of natural language processing tasks formulated by multi-task learning, such as word segmentation, POS tagging, dependency parsing, and text classification <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>. In this work, we apply multi-task learning to summarization and question-answering tasks for faithfulness enhancement.</p><p>3 Methodology</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>For an input document X = {x 1 , ..., x nx }, we assume there is a ground truth summary Y = {y 1 , . . . , y ny }. In our faithfulness enhanced setting, n q question answering pairs Q = {Q 1 , ..., Q nq } with corresponding answers A = {A 1 , ..., A nq } are also attached with X. In the training process, our model is given QA pairs and document-summary pairs. It tries to extract answers A to the questions and generate the summary Y . In test stage, our model is given document X and questions Q, and predicts the answers and summary. The final goal is to generate a summary that is not only informative but also consistent with document X.</p><p>Following, we introduce our proposed Faithfulness Enhanced Summarization model, which is generally built on Transformer <ref type="bibr" target="#b31">[32]</ref>. The faithfulness enhancement is implemented from three aspects: (1) Multi-task Encoder. It improves the semantic understanding of the input document by examining the quality of the encoded document representations for an auxiliary QA task. The encoded representation thus captures the key inputs for making faithful summary. (2) QA Attention-enhanced Decoder. The attention from the multi-task encoder aligns the decoder with the encoder so that the decoder can fetch more accurate input information to generate the summary. (3) Max-margin Loss. This is a loss orthogonal to the generation loss. It measures the accuracy of the LM and prevents it from being overconfident in the generation process.  The multi-task encoder is designed for encoding the input document for both summarization and question-answering in an integrated training process, as shown in Figure <ref type="figure" target="#fig_0">1(b)</ref>. This is different from the previous work that uses QA in the postgeneration stage for evaluating the faithfulness of the generated summaries <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7]</ref>, as shown in Figure <ref type="figure" target="#fig_0">1</ref>(a). We bring the QA closer to the encoder instead of leaving it for post-generated summary, and make the encoder be trained to accomplish the QA and summarization task in the meantime. This integrated training of a multi-task encoder includes faithfulness also as an optimization objective, besides the summary generation quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-task Encoder</head><note type="other">Document Transformer Encoder Questions</note><p>The answers are key entities from the document so that QA pairs focus on key information in the input.</p><p>As shown in Figure <ref type="figure" target="#fig_2">2</ref>, we first apply the classic Transformer architecture to obtain token-level representations for the document and questions, denoted as H w ∈ R nw×de and H u ∈ R nq×tq×de , where n w is the total number of tokens in the document, n q is the question number, t q is the token number in a question, and d e is the feature dimension. Then, we design the encoder to understand the question and the input document question from entity levels and sentence levels.</p><p>Encoding at Multi-level Granularity. We build the encoder by organizing the representation learning at different granularity levels. We use entities as the basic semantic unit as they contain compact and salient information across the document, and the reading comprehension questions focus on entities. Since a question is usually short, we create one node for each question. We add bidirectional edges from the questions to sentence nodes, and from sentence to entity nodes. These nodes act as the intermediary between sentences and enrich the cross-sentence relations. Because the initial directed edges are insufficient for learning backward information, we add reverse edges and self-loop edges to the graph following previous works <ref type="bibr" target="#b32">[33]</ref>. We initialize node representations following the token level and word span level mean-pooling process <ref type="bibr" target="#b8">[9]</ref>.</p><p>Given the constructed graph with node features, we use graph attention networks <ref type="bibr" target="#b33">[34]</ref> to update the representations of our semantic nodes. We refer to hi ∈ R de , i ∈ {1, • • • , (n e + n s + n q )} as the hidden states of input nodes, where n e and n s are the number of entity nodes and sentence nodes, respectively. The graph attention (GAT) layer is designed as follows:</p><formula xml:id="formula_0">z ij = LeakyReLU W a W b hi ; W c hj , α ij = exp (z ij ) l∈Ni exp (z il ) , l i = σ( j∈Ni α ij W d hj ),</formula><p>where N i is the set of neighboring nodes of node i, W a , W b , W c , W d are trainable weights and α ij is the attention weight between hi and hj . Besides, we add a residual connection to avoid gradient vanishing after several iterations: h i = hi + l i . We iteratively use the above GAT layer and position-wise feed-forward layer <ref type="bibr" target="#b31">[32]</ref> to update each node representation. The output entity feature matrix, sentence feature matrix, and question matrix, are denoted as H e ∈ R ne×de , H s ∈ R ns×de , and H q ∈ R nq×de , respectively.</p><p>Answer Selector for the QA task. After fusing information from the question and the document, we can select entities from the document as the answer to the question. Concretely, we apply the multi-head cross attention (MHAtt) between the question and the entities from the graph: h i qe = MHAtt h i e , H q , H q to obtain question-aware entity representations, where i is the question index. Based on the question-aware entity representations, we employ a feed-forward network (FFN) to generate the entity extracting probabilities A i = FFN(h i qe ), where A i = (a i 1 , ..., a i ne ). The QA objective is to maximize the likelihood of all ground-truth entity labels â:</p><formula xml:id="formula_1">L c = nq i=1 ne j=1 P âi j .<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">QA Attention-enhanced Decoder</head><p>A faithful decoder needs to attend to and fetch the important content from the encoder instead of mixing the inputs. We observe from §3.2 that the QA attentions on the key entities can be regarded as importance signals indicating which entities should be included in the summary. Hence, we propose a summary generator enhanced by QA attention. Generally, the decoder state attends to the encoder states with entities as intermediates, where the entity-level attention is guided by QA attentions.</p><p>Concretely, for each layer, at the t-th decoding step, we apply the self-attention on the masked summary embeddings E, obtaining u t . The masking mechanism ensures that the prediction of the position t depends only on the outputs before t. Based on u t , we then compute the cross-attention scores c e t over entities.</p><formula xml:id="formula_2">u t = MHAtt (e t , E &lt;t , E &lt;t ) , c e t = MHAtt (u t , H e , H e ) .<label>(2)</label></formula><p>In effect, the first attention layer captures contextual features of the decoded sequence, while the second incorporates entity information in c e t . Herein, we minimize the bidirectional Kullback-Leibler </p><formula xml:id="formula_3">L KL = D KL nq i=1 A i ∥ ny t=1 E t ,<label>(3)</label></formula><p>where n y is the number of tokens in the ground truth summary. Note that this objective guides the cross-attention to capture correct contextual information rather than to only learn the QA attention distribution. So we only employ it on parts of attention heads to avoid "overfitting" to the QA task.</p><p>We then use the entity-level attention to guide the selection of source tokens related to the key entities, by applying another MHAtt layer on source word sequence H w and c e t :</p><formula xml:id="formula_4">v t = MHAtt (c e t , H w , H w ) .<label>(4)</label></formula><p>This context vector v t , treated as salient contents summarized from various sources, is sent to an FNN to produce the distribution over the target vocabulary, i.e., P t = Softmax (FFN(v t )). All the learnable parameters are updated by optimizing the negative log likelihood objective function of predicting the target words:</p><formula xml:id="formula_5">L s = − ny t=1 log P t (y t ) .<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Max-margin Loss</head><p>Previous works <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref> suggest that a poorly-informed decoder will neglect some source segments, function more as an open-ended LM, and thus will be prone to extrinsic errors. Inspired by faithfulness enhanced machine translation works <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref>, we introduce a max-margin loss into the summarization task, for maximizing the difference of the predicted probability for each token of the summarization model and LM as shown in Figure <ref type="figure">3</ref>, which suppresses the tendency of summarizer to generate common but unfaithful words. Concretely, we first define the margin between the summarizer and the LM as the difference of the predicted probabilities:</p><formula xml:id="formula_6">m t = P t (y t | y &lt;t , X) − P LM t (y t | y &lt;t ) , (<label>6</label></formula><formula xml:id="formula_7">)</formula><p>where X is the input document, and P LM t denotes the predicted probability of the t-th token of the LM. Note that the language model has no access to the input document, and only takes the decoded summary prefix as input. Intuitively, if m t is large, then the summarization model is apparently better than the LM. When m t is small, there are two possibilities. One is that both the LM and the summarization model have good performance, hence the predicted probabilities should be similar. The other possibility is the LM is not good enough but overconfident, which leads to a summarizer with poor performance.</p><p>Hence, we present the max-margin loss L m , which adds a coefficient to the margin:</p><formula xml:id="formula_8">L m = ny t=1 (1 − P t ) 1 − m 5 t /2,<label>(7)</label></formula><p>where we abbreviate P t (y t | y &lt;t , X) as P t . The term (1 − m 5 t )/2 is a non-linear monotonically decreasing function in regard to m t , which ensures the optimization of maximizing m t . We choose Quintic function (fifth power) here as it is shown to be more stable <ref type="bibr" target="#b37">[38]</ref>. The first factor (1 − P t ) is for fitting the two possibilities we discussed above. When P t is large, the summarization model learns the y t well and does not need to pay too much attention on m t . This is reflected by (1 − P t ), a small coefficient of m t . On the other hand, when P t is small, it means that the summarizer needs to be better optimized, and a large coefficient (1 − P t ) enables the model is able to learn from the margin information.</p><p>The above four losses, L c , L s , L KL , and L m are orthogonal and can be combined to improve faithfulness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Datasets. We demonstrate the effectiveness of our approach on two public datasets, CNN/DM and XSum, which have been widely used in previous summarization works. Both datasets are based on news and consist of a large number of events, entities, and relationships that can be used to test the factual consistency of summarization models.</p><p>Note that our summarization model is accompanied by a QA task. Hence, we pre-construct QA pairs for each case using QuestEval tool provided by Scialom et al. <ref type="bibr" target="#b6">[7]</ref>. Concretely, QuestEval first selects a set of the named entities and nouns as answers from the source document. Then, it uses a finetuned answer-conditional question generation T5 <ref type="bibr" target="#b38">[39]</ref> model to generate questions via beam search. To ensure the quality of the QA pairs, we only select those questions for which the question-answering model <ref type="bibr" target="#b6">[7]</ref> gives the right answers. Finally, we take 38 QA pairs for CNN/DM and 27 pairs for XSum on average. For training the summarization model, intuitively, we want the questions to focus on the key information in the input. Hence, we select the pairs where the answers have the highest ROUGE-L scores with the target summaries as oracle pairs. A BART-based extraction model <ref type="bibr" target="#b20">[21]</ref> is then trained to predict important answers (QA pairs). Dou et al. <ref type="bibr" target="#b39">[40]</ref> showed that it brings more benefits when using the oracle guidance in the training phase. Hence, for the training dataset, we use the oracle QA pairs, i.e., the first 8 pairs with the highest ROUGE scores as input. For validation and test, we use the pairs selected by the extraction model.</p><p>Baselines. We first compare our model with recent factual-consistent summarization models: (1) FASum <ref type="bibr" target="#b24">[25]</ref> is a model that extracts and integrates factual relations into the summary generation process via graph attention. (2) CLIFF <ref type="bibr" target="#b9">[10]</ref> leverages reference summaries as positive data and erroneous summaries as negative data to train summarization systems. We also compare our proposed model with recent abstractive summarization models: (3) BART <ref type="bibr" target="#b20">[21]</ref> is a state-of-the-art abstractive summarization model pretrained with a denoising autoencoding objective. (4) PEGASUS <ref type="bibr" target="#b19">[20]</ref> is a pre-training large Transformer-based encoder-decoder models for summarization task. (4) GSum <ref type="bibr" target="#b39">[40]</ref> is a summarization framework that can take external sentence guidance as input. ( <ref type="formula" target="#formula_5">5</ref>) SimCLS <ref type="bibr" target="#b40">[41]</ref> bridges the gap between the learning objective and evaluation metrics by a reference-free evaluation.</p><p>Implementation Details. We implement our experiments in Huggingface <ref type="bibr" target="#b41">[42]</ref> on 4 NVIDIA A100 GPUs. We build our models based on BART (facebook/bart-large) for CNN/DM and PEGASUS (google/pegasus-xsum) for XSum following their hyperparameter settings, as they obtain better performance on each dataset, respectively. The QA number is set to 8 unless otherwise stated. To avoid the model from learning the position information of entities or questions, we sort the entities and questions in alphabetical order. We use Adam optimizer with ϵ as 1e-8 and β as (0.9, 0.999). The learning rate is set to 3e-5. The warm-up is set to 500 steps for CNN/DM and 125 for XSum. The batch size is set to 8 with gradient accumulation steps of 4. The beam size is set to 6 for CNN/DM and 8 for XSum. For pretrained LM models, we finetune the vanilla BART-based or PEGASUS-based LM on the CNN/DM and XSum dataset, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results</head><p>Automatic Evaluation. We evaluate models using standard full-length ROUGE F1 <ref type="bibr" target="#b42">[43]</ref>. ROUGE-1 (RG-1), ROUGE-2 (RG-2), and ROUGE-L (RG-L) refer to the matches of unigram, bigrams, and the longest common subsequence, respectively. We then use BERTScore <ref type="bibr" target="#b43">[44]</ref> to calculate a similarity score between the summaries based on their BERT embeddings. We also evaluate our approach with the latest factual consistency metrics, FactCC <ref type="bibr" target="#b5">[6]</ref> and QuestEval <ref type="bibr" target="#b6">[7]</ref>. FactCC is a weakly-supervised, model-based approach for verifying factual consistency. QuestEval considers not Table <ref type="table">1</ref>: Comparisons with state-of-the-art models on CNN/DM and Xsum. Marked ROUGE results are from <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref>. Numbers in bold mean that the improvement to the best baseline is statistically significant (a two-tailed paired t-test with p-value &lt;0.01). only factual information in the generated summary, but also the most important information from its source text, and finally gives a weighted F1 score QE.</p><p>The results are shown in Table <ref type="table">1</ref>. Among factual-consistent baselines, FASum performs relatively poorly. One possible reason is that FASum is not trained on pretrained models. CLIFF achieves better BERTScore and faithfulness scores than strong baseline BART. It can be seen that our model outperforms GSum by 0.97 ROUGE-1, 0.90 BERTScore on CNN/DM dataset and 3.35 QE score on XSum dataset, indicating questions in the multi-task provides better signals than important sentences. Finally, our model outperforms the best baseline SimCLS significantly in most of the metrics, especially in terms of faithfulness metrics (FactCC and QE), which proves the effectiveness of our methods. We also show the performance of our model on the test dataset when using the oracle QA pairs to evaluate the upper bound of the benefits brought by the QA task. We can see that oracles improve the performance significantly, with the best-performing model achieving a ROUGE-1 score of 50.50. The results indicate that 1) the model performance has the potential to be further improved given better QA pairs; and 2) the model does benefit from the auxiliary QA task.</p><p>Human Evaluation. Since automatic evaluations are not perfect and can be misleading sometimes, we further conduct a pairwise human evaluation to see whether our generated summaries are faithful to the source document. Following Cao et al. <ref type="bibr" target="#b9">[10]</ref>, we randomly sample 100 cases from CNN/DM and XSum, and then hire two fluent English speakers to evaluate summary informativeness (Inform.) and factual consistency (Factual.). For each article, the annotators are shown summaries generated by the baseline BART or PEGASUS model and two other systems. They then rate each system summary against the baseline summary. Next, the annotators are asked to label text spans with intrinsic and extrinsic errors. The compared models are baselines that achieve high automatic scores, and are shown without system names.    <ref type="table" target="#tab_3">3</ref> show the manual evaluation results. Firstly, we can find that there are larger differences in terms of Informativeness on the CNN/DM datasets and in Factual consistency on XSum datasets. This echos the attributes of the datasets, where the summaries in CNN/DM are longer and cover more detailed information, while summaries in XSum are shorter and thus require the summarization model to have advanced summarization ability. Secondly, our model is more frequently rated as being more informative and more factual than SimCLS summaries on both datasets. This is consistent with our automatic evaluation metrics. The kappa statistics are 0.53 and 0.59 for informativeness and factual consistency respectively, indicating the moderate agreement between annotators. The statistical significance between FES and PEGASUS is tested using a two-tailed paired t-test for significance for α = 0.05. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discussions</head><p>Ablation Study. We perform an ablation study on CNN/DM dataset to investigate the influence of different modules in FES. First, we remove the multi-task framework to verify the effectiveness of joint learning on summarization. This means that the QA attention-enhanced mechanism in the decoder is also removed, so the model degrades to the BART summarizer with entity inputs and max-margin loss. Secondly, we keep the QA task but replace the QA attention-enhanced decoder with the vanilla Transformer decoder. Thirdly, we remove the max-margin loss to verify the influence of the overconfidence of the LM. Additionally, we randomly select QA pairs to see if the understanding of the model on the document has to be related to key information.</p><p>From the results in Table <ref type="table">1</ref>, we find without the multi-task framework, the performance of FES drops greatly by 1.86 RG-L score, 0.43 BERTScore, and 0.60 QE score on CNN/DM dataset, which indicates with the QA multi-task does strengthen the encoder to learn more comprehensive representations. Next, the QE score drops by 0.28 after the QA attention guidance is removed. This indicates that aligning the QA attention with the summarization attention on the important entities can help the model capture gist information from the input, and restrict such loss on a limited part of entities can guide the decoder to fetch meaningful content from the input. FactCC score drops by 0.63 after the max-margin loss is removed. It indicates that preventing the LM from being overconfident can help increase faithfulness. Finally, the performance of FES drops when using random QA pairs as guidance but outperforms BART by a large margin. This shows that enhancing the understanding of the document is helpful even when it is not always related to the key information. But the performance can be further improved by asking questions on key entities.  The Number of QA pairs. To investigate how the number of QA pairs influences the performance of our model, we conduct experiments on the CNN/DM dataset with 6-14 oracle QA pairs. From results Q: What two groups were at the height of tension? A: Asian and African americans On Thursday, NPR headquartered in Washington, just 40 miles away from Baltimore ran its latest update on the urban turmoil that has erupted in the wake of the death of 25year-old Freddie Gray Ruben Navarrette: There 's little evidence that Asian businesses were targeted out of racial animus.</p><p>Ruben Navarrette: NPR report on Baltimore unrest focused on tension between African-Americans and Asians.</p><p>in Figure <ref type="figure" target="#fig_3">4</ref>(a), we see that the ROUGE score increases with the number of QA pairs, to begin with. After reaching 8 pairs, the improvements begin to vanish. One possible reason is that the answers no longer focus on the important information in the document. Note that the performance of FES remains at a high level in the range of 8-15 QA pairs, demonstrating the effectiveness and robustness of FES. At last, we choose to include 8 QA pairs in our model as default.</p><p>QA Task Evaluation. Since our framework also involves a question answering task, investigating the QA performance is helpful for understanding the model. Concretely, we use exact match (EM) and partial match (F1) to evaluate FES model and its ablation model, as shown in Table <ref type="table" target="#tab_5">4</ref>. Firstly, we can see that with the multi-task framework, both scores show significant improvements. This demonstrates that the two tasks can benefit each other, and the summarization task can also enhance QA performance. Secondly, the EM and F1 scores of QA are relatively high, showing that our model can also be used for answering questions on the document.</p><p>Margin between FES and the LM. We show the distribution of the margin m t defined in Eq. 6 from our FES and the BART in Figure <ref type="figure" target="#fig_3">4</ref>(b). Firstly, there are still many tokens with negative m t and a large amount of m t around 0 for BART. This indicates that the LM is probably overconfident for many tokens, and addressing the overconfidence problem is meaningful for summarization. By comparison, it can be seen that the number of negative margin cases is significantly reduced in FES compared with BART. More precisely, we list the percentage of tokens with negative m t and the average m t for each model in Table <ref type="table" target="#tab_6">5</ref>. Compared with BART, FES reduces the negative m t by 2.33% and increases the average of m t by 0.11 points. This proves that the overconfidence problem of the LM is solved to a great extent. Besides, we draw the comparison of m t on all words and entity words in Figure <ref type="figure" target="#fig_3">4</ref>(c). It can be seen that the proportion of around 0 for entity words is significantly reduced, which verifies our assumption that LM is accurate for many function words.</p><p>Case Study. We show several representative cases in Table <ref type="table" target="#tab_7">6</ref>, including the relevant QA pairs and relevant context from the source document, summaries generated by BART, PEGASUS, or SimCLS, and by our model on two datasets. The first three cases show summaries with intrinsic errors from baselines, because the baseline model misunderstands the source document. Our model makes faithful summaries with the help of answering the questions. For the forth case with extrinsic errors, the baseline summarization model and LM generate similar tokens, which are not included in the source document. FES has no extrinsic error, as it alleviates the overconfidence of the LM by introducing the max-margin loss. It is interesting to mention that the benefits brought by QA task and max-margin loss can complement each other. For example, in the third case, both the QA pair and max-margin loss prevent generating unfaithful information. We also show an error analysis in the last row. Both generated summary include an unmentioned name, which can be found in the training dataset. A post-edit operation might solve the problem, and we look forward to improving it in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose the multi-task framework with max-margin loss to generate faithful summaries. The auxiliary question-answering task can enhance the model's ability to understand the source document, and the max-margin loss can prevent the overconfidence of the LM. Experimental results show that our proposed model is effective across different datasets. In the future, we aim to incorporate post-edit operation to improve faithfulness.</p><p>Generating faithful summaries is an important step toward real artificial intelligence. This work has the potential positive impact on an intelligent and engaging reading system. At the same time, if people rely too much on summarized systems of prompt reading, they may become less capable of reading long documents. Besides, the pre-training model may be injected with malicious and vulgar information, and results in server misleading summary. Therefore, we should be cautious of these advantages and disadvantages.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The comparison of the existing QA-based faithfulness evaluation model and our faithfulnessenhanced summarization model. The QA task integrated in our model provides an auxiliary supervision signal to understand the document in the training process and enhance the faithfulness of the generated summary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Multi-task encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: (a) ROUGE scores w.r.t. the number of used QA pairs on CNN/DM. (b) The distribution of margin m t of BART and FES. FES has m t much less negative, more around 0 and closer to 1, indicating the alleviation of overconfidence issue of LM. (c) The distributions of margin m t on entity words and all words. The proportion of entity words with m t around 0 is smaller, indicating that LM is accurate in predicting function words.</figDesc><graphic url="image-1.png" coords="8,116.94,192.80,116.19,99.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Figure3: Max-margin loss for summarization model. (a) When the LM is accurate (the highest P word in blue for the target word), the correct target word is also predicted by both baseline and our summarization model with the highest P word . (b) When the LM is not accurate enough (the highest P word in blue for a wrong candidate word2), our model can prevent the overconfidence of LM by max-margin loss and predict the correct target word, while the baseline model does not.(KL) divergence between the QA attention A i and the summarization attention E t on the entities at the t-th step, to help the summarization model learn what entities are important:</figDesc><table><row><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell cols="3">Small Max-Margin Loss</cell><cell>(b)</cell><cell></cell><cell></cell><cell cols="3">Large Max-margin Loss</cell></row><row><cell>! !"#$ given the summarization model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>! !"#$ given</cell><cell>Candidate word1</cell><cell>Target word</cell><cell>Candidate word2</cell><cell>Candidate word1</cell><cell>Target word</cell><cell>Candidate word2</cell><cell>Candidate word1</cell><cell>Target word</cell><cell>Candidate word2</cell><cell>Candidate word1</cell><cell>Target word</cell><cell>Candidate word2</cell></row><row><cell>the language model</cell><cell cols="3">Baseline Model</cell><cell cols="3">Our Model</cell><cell cols="3">Baseline Model</cell><cell cols="3">Our Model</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Human evaluation: percentages of summaries that are worse than, tied with, or better than BART on CNN/DM dataset.</figDesc><table><row><cell></cell><cell>Inform.</cell><cell>Factual.</cell></row><row><cell cols="3">Model Lose↓Tie Win↑ Lose↓Tie Win↑</cell></row><row><cell cols="3">SimCLS 9% 75% 16% 7% 85% 8%</cell></row><row><cell>FES</cell><cell cols="2">6% 72% 22% 7% 81% 12%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Human evaluation: percentages of summaries that are worse than, tied with, or better than PEGASUS on XSum dataset.</figDesc><table><row><cell></cell><cell>Inform.</cell><cell>Factual.</cell></row><row><cell cols="3">Model Lose↓Tie Win↑ Lose↓Tie Win↑</cell></row><row><cell cols="3">SimCLS 7% 86% 7% 8% 79% 13%</cell></row><row><cell>FES</cell><cell cols="2">6% 83% 11% 6% 78% 16%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 and</head><label>2</label><figDesc>Table</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Automatic evaluation results of the QA task.</figDesc><table><row><cell>Model</cell><cell>EM</cell><cell>F1</cell></row><row><cell cols="3">FES w/o multi 76.34 83.59</cell></row><row><cell>FES</cell><cell cols="2">77.77 85.29</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>The percent of m t &lt; 0 and average m t of our model and baseline.</figDesc><table><row><cell cols="3">Model Percent of m t &lt; 0 (↓) Average m t (↑)</cell></row><row><cell cols="2">BART 15.83% (ref)</cell><cell>0.22 (ref)</cell></row><row><cell>FES</cell><cell>13.50% (-2.33%)</cell><cell>0.33 (+0.11)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Examples of summaries generated by baseline models and our method. The original fact, intrinsic error, extrinsic error and the corresponding faithful fact in each summary are highlighted. Italic words are those predicted by the LM.</figDesc><table><row><cell>Relevant QA pair</cell><cell>Relevant Context</cell><cell>Baseline Summary (truncated)</cell><cell>Our Summary (truncated)</cell></row><row><cell>Q: Who is the unexpected</cell><cell>Defender Clint Hill was the unexpected</cell><cell>Christian Benteke equalised for the</cell><cell>Defender Clint Hill scored his first</cell></row><row><cell>scorer of QPR's equaliser?</cell><cell>scorer of QPR's equaliser just after half-</cell><cell>hosts just three minutes later. Clint</cell><cell>ever Premier League goal to make it</cell></row><row><cell>A: Clint Hill</cell><cell>time to make it 2-2. Hill grabs the Queens ing his first ever Premier League goal. Park Rangers badge in celebration after scor-</cell><cell>half hour mark . Green put QPR ahead just after the</cell><cell>2-2 just after half time at Villa Park.</cell></row><row><cell>Q: What did Ecuador issue</cell><cell>The Ecuadorain Ambassador Ricardo</cell><cell>The stunt sparked outrage from Costa</cell><cell>Costa Rican tourism minister said</cell></row><row><cell>after the Costa Rican gov-</cell><cell>Patino then followed up with an apology to</cell><cell>Rica, who complained to the author-</cell><cell>she was 'unhappy' with the use of</cell></row><row><cell>ernment complained to the</cell><cell>Costa Rica and confirmed Ecuador had sent</cell><cell>ities. Ecuador has since issued an</cell><cell>her country's image. Ecuador has</cell></row><row><cell>Ecuadorian authorities? A:</cell><cell>a letter to the government to settle the matter.</cell><cell>apology and accepted the govern-</cell><cell>since issued an apology to Costa</cell></row><row><cell>An apology</cell><cell></cell><cell>ment's apology.</cell><cell>Rica and sent a letter of apology.</cell></row><row><cell>Q: Who scored one goal for</cell><cell>Sociedad seized the lead once more in the</cell><cell>Lucas Perez equalised for Deportivo</cell><cell>Gonzalo Castro scored one of the</cell></row><row><cell>Real Sociedad in the first</cell><cell>57th minute with a goal of the highest qual-</cell><cell>La Coruna in the 40th minute. Gon-</cell><cell>best goals of the season for David</cell></row><row><cell>half? A: Real Sociedad mid-fielder Gonzalo Castro</cell><cell>ity when Gonzalo Castro volleyed home-Sergio Canales'cross. But Deportivo came roaring back once more</cell><cell>zalo Castro scored the winner in the 57th minute for David Moyes ' side.</cell><cell>Moyes' side. Verdu Nicolas headed home to give Deportivo the lead with 12 minutes remaining.</cell></row><row><cell>Q: What season was he</cell><cell>The Belgium striker was signed by Liver-</cell><cell>Divock Origi joined Liverpool in a</cell><cell>Origi was loaned back to Lille for</cell></row><row><cell>loaned back to Lille? A:</cell><cell>pool in a 10million deal after impressing</cell><cell>£ 10million deal from Lille this sum-</cell><cell>the 2014-15 season after impressing</cell></row><row><cell>2014-15 season</cell><cell>at the 2014 World Cup in Brazil, before</cell><cell>mer. The Belgium striker was loaned</cell><cell>at the World Cup.</cell></row><row><cell></cell><cell>being loaned back to Lille for the 2014-15</cell><cell>back to Lille for the 2014 -15 season.</cell><cell></cell></row><row><cell></cell><cell>season.</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous reviewers for their constructive comments. This work was supported by the SDAIA-KAUST Center of Excellence in Data Science and Artificial Intelligence (SDAIA-KAUST AI). This publication is based upon work supported by the King Abdullah University of Science and Technology (KAUST) Office of Research Administration (ORA) under Award No FCC/1/1976-44-01, FCC/1/1976-45-01, URF/1/4663-01-01, and BAS/1/1635-01-01. This work was also supported by Alibaba Group through Alibaba Research Intern Program.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We include our code implementation in supplemental material. (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] The datasets are publicly available. (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes] In §5. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointer-generator networks</title>
		<author>
			<persName><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
				<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A unified model for extractive and abstractive summarization using inconsistency loss</title>
		<author>
			<persName><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wan</forename><forename type="middle">Ting</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chieh-Kai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kerui</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
				<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Global encoding for abstractive summarization</title>
		<author>
			<persName><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
				<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Text summarization with pretrained encoders</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
				<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AACL</title>
				<meeting>of AACL</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Evaluating the factual consistency of abstractive text summarization</title>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Kryściński</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
				<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Questeval: Summarization asks for fact-based evaluation</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Scialom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul-Alexis</forename><surname>Dray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Lamprier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Piwowarski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacopo</forename><surname>Staiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
				<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Feqa: A question answering evaluation framework for faithfulness assessment in abstractive summarization</title>
		<author>
			<persName><forename type="first">Esin</forename><surname>Durmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
				<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bass: Boosting abstractive summarization with unified semantic graph</title>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiachen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
				<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cliff: Contrastive learning for improving faithfulness and factuality in abstractive summarization</title>
		<author>
			<persName><forename type="first">Shuyang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
				<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Towards a human-like open-domain chatbot</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Adiwardana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Fiedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romal</forename><surname>Thoppilan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Kulshreshtha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Nemade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifeng</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.09977</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
				<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Don&apos;t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization</title>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
				<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning towards abstractive timeline summarization</title>
		<author>
			<persName><forename type="first">Xiuying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangming</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng-Hsuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCAI</title>
				<meeting>of IJCAI</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Capturing relations between scientific papers: An abstractive model for related work section generation</title>
		<author>
			<persName><forename type="first">Xiuying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hind</forename><surname>Alamro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generative adversarial network for abstractive text summarization</title>
		<author>
			<persName><forename type="first">Linqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
				<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">How to write summaries with patterns? learning towards abstractive summarization through prototype editing</title>
		<author>
			<persName><forename type="first">Shen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piji</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangming</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
				<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3741" to="3751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Vmsmo: Learning to generate multimodal summary for video-based news articles</title>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangming</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
				<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Entity-aware abstractive multidocument summarization</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weidong</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gongshen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of ACL</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pegasus: Pre-training with extracted gap-sentences for abstractive summarization</title>
		<author>
			<persName><forename type="first">Jingqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
				<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
				<meeting>of ACL<address><addrLine>Bart</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semsum: Semantic dependency guided neural abstractive summarization</title>
		<author>
			<persName><forename type="first">Hanqi</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
				<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Faithful to the original: Fact aware neural abstractive summarization</title>
		<author>
			<persName><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
				<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ensure the correctness of the summary: Incorporate entailment knowledge into abstractive sentence summarization</title>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
				<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Enhancing factual consistency of abstractive summarization</title>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Hinthorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruochen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingkai</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AACL</title>
				<meeting>of AACL</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Focus attention: Promoting faithfulness and diversity in summarization</title>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Aralikatte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sascha</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
				<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6078" to="6095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Multitask learning. Machine learning</title>
		<author>
			<persName><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A transition-based system for joint part-of-speech tagging and labeled non-projective dependency parsing</title>
		<author>
			<persName><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
				<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Incremental joint approach to word segmentation, pos tagging, and dependency parsing in chinese</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Hatori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takuya</forename><surname>Matsuzaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun'ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
				<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Joint optimization for chinese pos tagging and dependency parsing</title>
		<author>
			<persName><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM transactions on audio</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>speech, and language processing</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Recurrent neural network for text classification with multi-task learning</title>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.05101</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
				<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Text generation from knowledge graphs with graph transformers</title>
		<author>
			<persName><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhanush</forename><surname>Bekal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02342</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
				<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Handling divergent reference texts when evaluating table-to-text generation</title>
		<author>
			<persName><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
				<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On faithfulness and factuality in abstractive summarization</title>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
				<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Towards enhancing faithfulness for neural machine translation</title>
		<author>
			<persName><forename type="first">Rongxiang</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangpeng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
				<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Prevent the language model from being overconfident in neural machine translation</title>
		<author>
			<persName><forename type="first">Mengqi</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Hua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
				<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of JMLR</title>
				<meeting>of JMLR</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Gsum: A general framework for guided neural abstractive summarization</title>
		<author>
			<persName><forename type="first">Zi-Yi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AACL</title>
				<meeting>of AACL</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Simcls: A simple framework for contrastive learning of abstractive summarization</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
				<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Transformers: State-ofthe-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
				<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out</title>
				<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Bertscore: Evaluating text generation with bert</title>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varsha</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
				<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
