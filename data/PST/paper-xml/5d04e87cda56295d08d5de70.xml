<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Evolutionary Gravitational Search-based Feature Selection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-05-15">May 15, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mohammad</forename><surname>Taradeh</surname></persName>
							<email>mohammad.taradeh@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Engineering and Technology</orgName>
								<orgName type="institution">Birzeit University</orgName>
								<address>
									<settlement>Birzeit</settlement>
									<country key="PS">Palestine</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Majdi</forename><surname>Mafarja</surname></persName>
							<email>mmafarja@birzeit.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Birzeit University</orgName>
								<address>
									<settlement>Birzeit</settlement>
									<country key="PS">Palestine</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ali</forename><forename type="middle">Asghar</forename><surname>Heidari</surname></persName>
							<email>as_heidari@ut.ac.ir</email>
							<affiliation key="aff3">
								<orgName type="department">School of Surveying and Geospatial Engineering</orgName>
								<orgName type="institution">University of Tehran</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hossam</forename><surname>Faris</surname></persName>
							<email>hossam.faris@ju.edu.jo</email>
							<affiliation key="aff5">
								<orgName type="department">King Abdullah II School for Information Technology</orgName>
								<orgName type="institution">The University of Jordan</orgName>
								<address>
									<settlement>Amman</settlement>
									<country key="JO">Jordan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ibrahim</forename><surname>Aljarah</surname></persName>
							<email>i.aljarah@ju.edu.jo</email>
							<affiliation key="aff5">
								<orgName type="department">King Abdullah II School for Information Technology</orgName>
								<orgName type="institution">The University of Jordan</orgName>
								<address>
									<settlement>Amman</settlement>
									<country key="JO">Jordan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Seyedali</forename><surname>Mirjalili</surname></persName>
							<email>seyedali.mirjalili@griffithuni.edu.au</email>
							<affiliation key="aff6">
								<orgName type="department">Institute of Integrated and Intelligent Systems</orgName>
								<orgName type="institution">Griffith University</orgName>
								<address>
									<postCode>4111</postCode>
									<settlement>Nathan, Brisbane</settlement>
									<region>QLD</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hamido</forename><surname>Fujita</surname></persName>
							<affiliation key="aff7">
								<orgName type="department">Faculty of Software and Information Science</orgName>
								<orgName type="institution">Iwate Prefectural University (IPU)</orgName>
								<address>
									<settlement>Iwate</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">An Evolutionary Gravitational Search-based Feature Selection</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-05-15">May 15, 2019</date>
						</imprint>
					</monogr>
					<idno type="MD5">24D9A01E114A89287CBCAF9C365B70A1</idno>
					<idno type="DOI">10.1016/j.ins.2019.05.038</idno>
					<note type="submission">Received date: 1 October 2018 Revised date: 22 February 2019 Accepted date: 12 May 2019 Preprint submitted to Information Sciences</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Gravitational Search Algorithm</term>
					<term>Genetic Algorithm</term>
					<term>Feature Selection</term>
					<term>Supervised Learning</term>
					<term>Classification</term>
					<term>Optimization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With recent advancements in data collection tools and the widespread use of intelligent information systems, a huge amount of data streams with lots of redundant, irrelevant, and noisy features are collected and a large number of features (attributes) should be processed. Therefore, there is a growing demand for developing efficient Feature Selection (FS) techniques. Gravitational Search algorithm (GSA) is a successful population-based metaheuristic inspired by Newton's law of gravity. In this research, a novel GSA-based algorithm with evolutionary crossover and mutation operators is proposed to deal with feature selection (FS) tasks. As an NP-hard problem, FS finds an optimal subset of features from a given set. For the proposed wrapper FS method, both K-Nearest Neighbors (KNN) and Decision Tree (DT) classifiers are used as evaluators. Eighteen well-known UCI datasets are utilized to assess the performance of the proposed approaches. In order to verify the efficiency of proposed algorithms, the results are compared with some popular nature-inspired algorithms (i.e. Genetic Algorithm (GA), Particle Swarm Optimizer (PSO), and Grey Wolf Optimizer (GWO)). The extensive results and comparisons demonstrate the superiority of the proposed algorithm in solving FS problems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Highlights</head><p>• Gravitational Search Algorithm (GSA) was used as a search strategy in a Feature Selection approach</p><p>• Crossover and Mutation evolutionary operators were used to improve the quality of GSA.</p><p>• KNN and Decision Tree classifiers were used as evaluators.</p><p>• The results and demonstrate the superiority of the proposed algorithm in solving FS problems</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Data mining is an automatic or a semi-automatic process of extracting and discovering implicit, unknown, and potentially useful patterns and information from massive data stored and captured from date repositories (web, database, data warehousing) <ref type="bibr" target="#b15">[15]</ref>. Data mining tasks are usually divided into two categories: predictive and descriptive. The objective of the predictive tasks is to predict or classify for determining which class an instance or example belongs, based on the values of some features (i.e, independent or conditional features) in a dataset. An example of the prediction task is to predict if a new patient (instance) is in danger of a heart attack disease or not based on some clinical tests. Descriptive tasks aim to find clusters, correlations, and trends based on the implicit relationships hidden in the underlying data.</p><p>With advanced data collection tools and since the collected data is not specified for the data mining purposes, it may contain redundant or irrelevant features <ref type="bibr" target="#b28">[28]</ref>. Applying data mining on such data might lead to misleading results. Thus, data pre-processing is an essential step to refine the data to be used in any learning model. The main purpose of the pre-processing step is to clean and transform raw data into a suitable format, which improves the performance of data mining tasks <ref type="bibr" target="#b15">[15]</ref>.</p><p>One of the most important pre-processing techniques is dimensionality reduction (DR), which aims to reduce the number of features in a dataset to the minimum and improve the performance of different data mining tasks (e.g., classification, clustering, association rule mining, etc.). One of the most essential tools in DR is Feature Selection (FS).</p><p>FS is the process of selecting the minimal representative feature subset from the original set of features to satisfy a measuring criterion. By eliminating the redundant and irrelevant features from a dataset, the performance of the consequent data mining tasks can be improved substantially in terms of the computational time and/or the learning performance (e.g., the classification accuracy of a classifier). The standard FS process has four major phases: subset generation, subset evaluation, stopping criteria, and validation.</p><p>Subsets generation is considered as a search problem that aims to select the best subset of all possible feature subsets. Then, the selected subset can be evaluated using statistical measures (i.e., the correlation between the feature and the target class) as in filter approaches, or by using a classifiers measure (e.g., accuracy) to evaluate the subset performance as in wrapper approaches. When employing an FS method for a dataset, the process is repeated until a stopping constraint is met (i.e., achieving a predefined goal (classification accuracy), or repeating the process for a certain number of iterations). The validation step is usually performed after the FS process to assess the quality of the resulted feature subset.</p><p>Searching for the best feature subset is a challenge in the FS process, which is considered as a combinatorial NP-hard problem <ref type="bibr" target="#b29">[29]</ref>. Complete Search strategies (e.g. exhaustive) examine all possible solutions (feature subsets) for the search problem. Formally speaking, for a dataset of N features, the complete search tends to evaluate 2 N -1 possible subsets for selecting one of them. Therefore, it is impractical to use this strategy to tackle the FS problem for large values of N . Another strategy that could be used to combat this issue is random search strategy <ref type="bibr" target="#b2">[2]</ref>.</p><p>A random search can generate subsets constantly and keep improving the quality of the selected features in an iterative manner. In each step, the next subset is obtained randomly</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T using the information collected in the past steps. In the worst case, a random search continues evaluating all possible solutions as the complete search. Heuristic searches belong to the third class of search methods can be used to find an optimal subset of features. Over the last decade, many researchers showed the effectiveness of using metaheuristic algorithms in solving FS problems <ref type="bibr" target="#b41">[41]</ref>. This is due to the ability of those algorithms to find (near) optimal solutions in a reasonable time <ref type="bibr" target="#b9">[9]</ref>.</p><p>Meta-heuristics use heuristic information, but they are considered general-purpose optimization techniques that find reasonable solutions in a reasonable time for optimization problems. According to Talbi <ref type="bibr" target="#b47">[47]</ref>, metaheuristic algorithms can be classified into two main families: single-solution-based (S-Metaheuristics) and population-based (P-metaheuristics).</p><p>The most popular examples of S-Metaheuristics are Simulated Annealing (SA) and Tabu Search (TS). Examples of P-metaheuristics are Genetic Algorithm (GA), Particle Swarm Optimization (PSO), and Ant Colony Optimization (ACO). Some of the recent optimizers are Grey Wolf Optimizer (GWO), Salp Swarm Algorithm (SSA) and Harris Hawks Optimization (HHO) <ref type="bibr" target="#b19">[19]</ref>. The aforementioned algorithms demonstrate superior efficiency in tackling FS problems when compared to the exact methods <ref type="bibr" target="#b30">[30]</ref>. Another class of metaheuristic methods that can be added to the latter taxonomy is the Memetic Algorithms (MAs). This class can be broadly defined as hybrid algorithms that are composed of an evolutionary algorithm and one or more local search algorithms within the same generation cycle <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b21">21]</ref>.</p><p>Swarm Intelligence (SI) algorithms are mostly P-metaheuristics, which search for the (near) optimal solution for a specific problem by employing a set of search agents to explore the search space <ref type="bibr" target="#b11">[11]</ref>. The key characteristic of SI algorithms is that they try to mimic the natural behavior of some creatures that live in groups like folks of birds, ants, bees and others <ref type="bibr" target="#b1">[1]</ref>. PSO, ACO, GWO, HHO, SSA, and FA are all examples of SI algorithms that mimic the social behavior of some creatures. Moreover, some SI algorithms have been inspired by other phenomena in nature, like physics-based algorithms that are based on physical laws. As an example, Gravitational Search Algorithm (GSA) <ref type="bibr" target="#b42">[42]</ref> is a physics-based algorithm inspired by Newton's law of universal gravitation and Newton's law of motion.</p><p>Two common characteristics between all SI algorithms are exploration and exploitation that should be carefully considered in order to achieve the best performance in finding the (near) optimum solution. Exploration aims to explore the whole search space in order to find promising solutions in undiscovered areas. By contrast, the exploitation phase aims to improve the already discovered solutions by searching their neighborhood. Both phases are associated, and must efficiently cover the search space to search the (near) optimal solution; however, the key point is to find the right balance between them. Indeed, by having more exploitation than exploration, the algorithm will be stuck in locally optimal solutions. At the same time, the algorithm will lose some best solutions if it goes far in exploration.</p><p>In this paper, an efficient hybrid FS approach, which is called HGSA, is proposed to combine the benefits of the SI algorithms and the GA for improving the exploitative and exploitative capabilities of the GSA. The rest of the paper is organized as follows: Section 2 provides the literature review of optimization algorithms applied to FS problems. GSA algorithm is presented in Section 3. Section 4 covers the proposed method. The results and conclusions are given in Sections 5 and 6, respectively.</p><formula xml:id="formula_0">A C C E P T E D M A N U S C R I P T 2. Literature review</formula><p>FS is one of the most important and frequently used techniques as a pre-processing tool for enhancing different data mining techniques. Therefore, many efforts have been put into improving the FS methods. In recent years, many researchers involved the SI algorithms (e.g., GA, GSA, PSO, and GWO) for solving FS problems.</p><p>The GA is an Evolutionary Algorithm (EA) that solves optimization problems using techniques motivated by natural evolution including inheritance, mutation, selection, and crossover. GA was used for the first time in solving the FS problems by Yang and Honavar <ref type="bibr" target="#b49">[49]</ref>. As other P-metaheuristic algorithms, GA is more exploration-oriented than exploitation, accordingly, local search mechanisms were hybridized with GA algorithm to achieve better results, and this is considered as the first hybrid metaheuristic algorithm for FS in the literature <ref type="bibr" target="#b37">[37]</ref>. In their approach, local search schemes were integrated with the GAs to fine-tune the searching process. The mechanisms are generated with regard to their finetuning capacity and the corresponding efficiency and computational times are investigated based on different FS datasets.</p><p>Eberhart and Kennedy <ref type="bibr" target="#b7">[7]</ref> proposed a breakthrough computation technique known as PSO, which is inspired by the social intelligence of a flock of birds. Normally a leader leads the swarm of birds or fish, and every single particle of the swarm follows the leading navigator based on their intuition. Kennedy and Eberhart <ref type="bibr" target="#b24">[24]</ref> proposed a binary version of the PSO (called BPSO), to solve the binary problems. BPSO was used for the first time in the FS domain by <ref type="bibr" target="#b13">[13]</ref>. BPSO has been widely used to tackle FS problems, where many researchers have adopted the BPSO for achieving better results in finding the best subset of features.</p><p>Recently, many optimization algorithms have been proposed and used to tackle FS problems. Grey Wolf Optimizer (GWO) is a newly proposed SI algorithm developed by <ref type="bibr" target="#b33">[33]</ref>. The GWO was used in many approaches for solving various problems <ref type="bibr" target="#b10">[10]</ref>. The GWO mimics the hierarchical domination and hunting mechanism of Grey wolves in nature. A binary version of GWO was proposed and used to select optimal feature subset for classification in <ref type="bibr" target="#b8">[8]</ref>. Li et al. <ref type="bibr" target="#b26">[26]</ref> first proposed an improved GWO (IGWO) as a wrapper-based on kernel extreme machine learning (KELM) for FS tasks. They then used IGWO for finding the optimal feature subset for medical data. In the proposed approach, GA was first adapted to generate the diversified initial positions, and then, GWO was used to update the current positions of the population in a discrete searching space. Pathak et al. <ref type="bibr" target="#b38">[38]</ref> proposed a levy flight-based GWO to deal with FS for image steganalysis. Whale Optimization Algorithm (WOA) <ref type="bibr" target="#b17">[17]</ref> is a recent SI algorithm that mimics the foraging behavior of humpback whales. A binary version of WOA was proposed in <ref type="bibr" target="#b31">[31]</ref> and was used to tackle FS problems. Mafarja and Mirjalili <ref type="bibr" target="#b32">[32]</ref> proposed a hybrid approach that combined SA with WOA for the FS problem.</p><p>The Grasshopper Optimization Algorithm (GOA) <ref type="bibr" target="#b18">[18]</ref> is another recent population-based metaheuristic that mimics the swarming behaviors of grasshoppers, and was improved by hybridizing it with an evolutionary operator to be employed as searching strategy in a novel FS approach <ref type="bibr" target="#b30">[30]</ref>.</p><p>GSA is an optimization algorithm developed by Rashedi et al. <ref type="bibr" target="#b42">[42]</ref>. Newton's law of gravity and motion drives the main mathematical modes of GSA. The population in GSA is presented as a group of physical particles each of which has a mass, which presents the solution's fitness value. The heavier mass, which is supposed to be the better solution, is moving</p><formula xml:id="formula_1">A C C E P T E D M A N U S C R I P T</formula><p>slower than the lighter mass. This assumption controls the exploration and exploitation behaviors in GSA. A binary version of GSA (called BGSA) was developed by Rashedi et al. <ref type="bibr" target="#b43">[43]</ref> for solving the binary problems. Although BGSA was employed to solve FS problems, according to <ref type="bibr" target="#b40">[40]</ref>, BGSA still suffers from two well-known issues; falling in the local optimum and the slow convergence rate. Rashedi and Nezamabadi-pour <ref type="bibr" target="#b41">[41]</ref> proposed an improved version of the BGSA to overcome the stagnation situation and the slow convergence rate of the original GSA, by changing the transfer function to improve the exploration ability of the algorithm. They also used an elitism strategy to improve the movement of the particles toward the best solution.</p><p>An astrophysics-based disruption mechanism was added to GSA by Sarafrazi et al. <ref type="bibr" target="#b45">[45]</ref>.</p><p>To deal with hydraulic turbine systems, a PSO-based searching strategy was embedded in GSA by Li and Zhou <ref type="bibr" target="#b25">[25]</ref>. Shaw et al. <ref type="bibr" target="#b46">[46]</ref> developed an opposition-based learning-based (OBL) GSA to speed up the convergence behavior and generate some jumping mechanisms.</p><p>In <ref type="bibr" target="#b50">[50]</ref>, two modifications were considered: to include the upper and lower limits of the d-th dimension and to propose a new gravitational constant. By this method, they boost the objects' diversity and brings more exploration to the basic method. In <ref type="bibr" target="#b6">[6]</ref>, an enhanced GSA based on PSO and elastic-ball strategy is designed. In <ref type="bibr" target="#b16">[16]</ref>, they integrated Piece Wise Linear (PWL) chaotic map and sequential quadratic programming into the basic GSA.</p><p>The PWL map was combined with GSA to improve the exploration, and then they used sequential quadratic programming to improve the exploitation for the global best. In <ref type="bibr" target="#b34">[34]</ref>,</p><p>ten chaotic maps were used to update the gravitational constant (G) of the GSA. In addition, an adaptive normalization method was proposed to transit from the exploration phase to the exploitation phase, more smoothly. In 2018, Rashedi et al. <ref type="bibr" target="#b44">[44]</ref> provided a comprehensive survey on different variants and operators of GSA algorithm as well.</p><p>Based on literature review, it can be seen that several SI algorithms have been utilized to serve as search strategies in both filter and wrapper FS approaches. According to the no free lunch theorem (NFL) <ref type="bibr" target="#b48">[48]</ref>, there is no algorithm that can solve all optimization problems efficiently, and if an algorithm showed a superior performance in solving a specific problem, it may fail to find good solutions for another. The oscillating behavior of the optimizers can be interpreted to some characteristics in each algorithm. Moreover, the nature of the problem being solved has a vital impact on the performance of the algorithm. This motivated our attempts to propose a new FS approach that try to overcome the main drawbacks of the GSA algorithm. The low convergence rate of the GSA is treated by employing a logarithmic decreasing strategy to update the value of the gravitational constant (G). Moreover, the exploration and exploitation abilities of the GSA are enhanced by proposing a novel mechanism that works based on two evolutionary operators (i.e., crossover and mutation), where the whole population is improved instead of improving some individuals of the population as in the traditional evolutionary algorithms (e.g., GA). To our knowledge, there is no such methods in literature to hybridize the GSA with the evolutionary operators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Gravitational Search Algorithm (GSA)</head><p>GSA <ref type="bibr" target="#b42">[42]</ref> is a successful metaheuristic inspired by Newton's gravitational and motion laws. In GSA, search agents have the role of interacting physical objects, and the performance</p><formula xml:id="formula_2">A C C E P T E D M A N U S C R I P T</formula><p>of the solutions is seen as the mass of the objects. The particles can attract other agents based on the gravitational force. This force pushes lighter objects towards the heavier objects. The heavier objects, which are considered as better solutions, will move slower than the lighter objects (see Fig. <ref type="figure" target="#fig_0">1</ref>). This behavior ensures the exploitation phase in GSA. Consider N search agents (masses) in a search space with n dimensions, the position of the i-th solution (mass) is defined as vector X i as in Eq. ( <ref type="formula" target="#formula_3">1</ref>):</p><formula xml:id="formula_3">X i = (x 1 i , . . . , x d i , . . . , x n i ), i = 1, 2, . . . , N<label>(1)</label></formula><p>Where the value of each vector represents the position of agents in that dimension, for instance x j i is the position of the ith agent in the j dimension. As mentioned before, the mass M i for ith agent is calculated based on the fitness of that agent at a specific time (iteration) t as in Eq. ( <ref type="formula" target="#formula_4">2</ref>):</p><formula xml:id="formula_4">M i (t) = fit i (t) -worst(t) N j=1 (fit j (t) -worst(t))<label>(2)</label></formula><p>where M i and fit i (t) are the mass and the fitness of ith agent at time t, and worst(t) is the worst fitness among all agents at time t (current population). In each iteration, agents change their position to discover the unexplored areas inside the feature space. In GSA, the new position for the ith agent can be calculated by summing the current position of the agent with its next velocity v i (t + 1) as in Eq. ( <ref type="formula" target="#formula_6">4</ref>):</p><formula xml:id="formula_5">X i (t + 1) = X i (t) + v i (t + 1)<label>(3)</label></formula><p>where the next velocity of an object is the acceleration of the object added to its current velocity:</p><formula xml:id="formula_6">v i (t + 1) = r × v i (t) + a i (t)<label>(4)</label></formula><p>where r is a random variable between (0,1). According to the laws of motion, the acceleration of the i-th particle can be calculated as the total gravitational force of other particles divided by the mass of particle i as in Eq. ( <ref type="formula" target="#formula_7">5</ref>).</p><formula xml:id="formula_7">a i (t) = N j=1.J =i (q × F ij (t)) M i (t)<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>where q is a random variable between (0,1), F ij (t) denotes the gravitational force acting by particle j on particle i at specific iteration t. According to the Newton's gravitational law, this force can be calculated as in Eq. ( <ref type="formula" target="#formula_8">6</ref>).</p><formula xml:id="formula_8">F ij (t) = G(t) M i (t) × M j (t) R 2 (X j (t) -X i (t))<label>(6)</label></formula><p>where G is the gravitational constant. R is the distance between particle j and particle i.</p><p>In GSA, it G is a decreasing function and the initial value G(G 0 , t) is used to control the search process. Pseudocode of GSA is described in Algorithm 1.</p><p>Algorithm 1 Pseudocode of GSA Initialize the candidate objects X i (i = 1, 2, . . . , N ) while (end condition is not met) do Evaluate the fitness of all objects Calculate best, worst, M i , M j Update the gravitational factor G(t) Calculate forces F ij by Eq. ( <ref type="formula" target="#formula_8">6</ref>). Update acceleration by Eq. ( <ref type="formula" target="#formula_7">5</ref>) Update velocities using Eq. ( <ref type="formula" target="#formula_6">4</ref>) Update positions by Eq. ( <ref type="formula" target="#formula_5">3</ref>) Return the best search agent</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning algorithms</head><p>In this paper, the wrapper FS model is adopted to assess the goodness of each feature subset, in which two learning algorithms (i.e., KNN and DT) from different families are used in all experiments. The classifiers employed are described as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">KNN Classifier</head><p>KNN classifier is one of the simplest classification algorithms widely used in literature <ref type="bibr" target="#b37">[37]</ref>.</p><p>KNN tries to find the training nearest neighbors to the test sample according to Euclidean distance, ans then the test sample is classified based on the majority class of its nearest neighbors. In KNNs, there is only one parameter to be determined: the number of neighbors (k) to be considered when classifying a new test sample. If we set k to one, the test sample is basically assigned to the class of that identical nearest neighbor. However, the value of k parameter has a major effect on the overall classification accuracy of the KNN classifier.</p><p>Often, Euclidean distance is employed in KNN using Eq. ( <ref type="formula">7</ref>):</p><formula xml:id="formula_9">ED(X 1 , X 2 ) = ( n i=1 (x 1,i -x 2,i ) 2 ) 1 2 (7)</formula><p>where X 1 and X 2 denote two points with n dimensions. The KNN is categorized as an instance-based learning model, which is one of the simplest algorithms among all methods in machine learning. In the course of the learning stage, there is no abstraction of training information in instance-based learners.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">DT classifier</head><p>DT is a simple and commonly used classifier in data mining <ref type="bibr" target="#b12">[12]</ref>. The goal of DT is to create a model for predicting the value of the target feature (class) for a test sample based on the values of the input features vector. It follows the top-down schema, by choosing the feature at each level, that "best split" the training data into subsets based on the gain-ratio measure. The goal is to find the most homogeneous set based on the value of the target feature (class). Different decision tree construction algorithms use different homogeneity metrics. For example, Classification And Regression Tree (CART) uses Gini impurity and Variance reduction, while ID3 and C4.5 algorithms use Information gain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">GSA for Feature Selection</head><p>In general, FS methods aim to select the most informative features among the original set of features by eliminating the redundant and/or irrelevant features. Therefore, it is evident to use a binary representation for representing a solution of the FS problem. In a binary solution, each element indicates selecting or not selecting the corresponding feature in the original dataset. Therefore, each dimension in the solution takes either 1 (selected) or 0 (not selected) as shown in Fig. <ref type="figure">2</ref>.</p><formula xml:id="formula_10">F 1 F 2 F 3 F 4 ... F (n-2) F (n-1) F (n) 1 0 0 1 ... 1 0 0 Figure 2: Binary solution representation</formula><p>The GSA was originally designed to tackle the optimization problems with continuous search spaces, so to employ GSA as a search strategy within a FS method, which is a binary problem, the solutions must be converted to a binary form. In BGSA <ref type="bibr" target="#b43">[43]</ref>, a v-shaped transfer function (TF),which is shown in Eq. 8 (see Fig. <ref type="figure" target="#fig_1">3</ref>), was utilized to convert the continuous solutions to binary where the velocity vector in Eq. 4 was used as input.</p><p>In this case, each element in the velocity vector represents the probability of flipping the corresponding feature from selected to not selected and vice versa; i.e., the dimensions with high velocity values have high probability to flip their values, while the dimensions with lower velocities have lower chances <ref type="bibr" target="#b36">[36]</ref>. </p><formula xml:id="formula_11">A C C E P T E D M A N U S C R I P T T F (v i d (t)) = |tanh(v i d (t))|<label>(8)</label></formula><p>where v i d (t) is the velocity value of the i th element in d th dimension in t th iteration.</p><p>The result T F (v i d (t)), obtained from Eq. ( <ref type="formula" target="#formula_11">8</ref>) is then used to convert a position's element to 0 or 1 according to Eq. ( <ref type="formula" target="#formula_12">9</ref>):</p><formula xml:id="formula_12">X(t + 1) = -X t r &lt; T F (v i k (t)) X t r ≥ T F (v i k (t))<label>(9)</label></formula><p>where r is a random number inside (0, 1).</p><p>As for any optimization problem, designing an appropriate objective function, which is a part of the problem formulation, is crucial in tackling the FS problem. In GSA, the physical mass of a solution represents the quality of that solution, i.e., the best solution is much heavier than the worst one. When considering the GSA for FS and since the solution represents the feature subset, the goodness of a solution can be considered based on two factors; the (minimal) number of features included in this subset, and the (maximal) classification accuracy that can be obtained by using the selected features. Therefore, an objective function that combines these two factors is used in this work (see Eq. ( <ref type="formula" target="#formula_13">10</ref>)). </p><formula xml:id="formula_13">F itness = min[αγ R (D) + β |R| |N | ]<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The proposed hybrid BGSA with evolutionary operators</head><p>As stated in the literature review, GSA may still suffer from trapping in Local Optima (LO) when dealing with challenging problems. feature selections problems are very challenging due to the large number of variables and local solutions. The search space of FS problems changed for each dataset. Therefore, a simple but efficient strategy is highly demanded to mitigate the core drawbacks of the GSA: first, lack of a simple but efficient technique to switch from the extensive exploration step to focused exploitation phase. Second, searching with an unstable balance between exploration and exploitation. Third, searching with a slow convergence speed. The last drawback is dependent to the insufficient tendency of the algorithm to be devoted to exploitation phase in last iterations, while the first and second one can lead to stagnation behavior and immature convergence in often the middle of iterations, which can significantly decrease the quality of final solutions.</p><p>In order to cope with the above-mentioned shortcomings, an improved evolutionary variant of GSA is proposed for the first time in this section. The main constraint for us is to improve the performance of GSA in tackling FS problems substantially while preserving the</p><formula xml:id="formula_14">A C C E P T E D M A N U S C R I P T</formula><p>core merit of the GSA, including cheap computational complexity. As such, we proposed three effective modifications to significantly boost the exploration and exploitation trends of the conventional GSA along with its convergence rate.</p><p>Firstly, a logarithmic decreasing function is adopted to gradually update the gravitational constant in the algorithm instead of the linear decreasing function. Secondly, a novel evolutionary mechanism, which mainly depends on a crossover scheme, is proposed to further improve the exploratory tendency of GSA. Here, the idea of the social thinking (gbest) in PSO was introduced, which has been integrated with the GSA algorithm <ref type="bibr" target="#b35">[35]</ref>. Finally, a mutation operator is applied to assist the gbest solution in further improving the quality of results and avoiding the stagnation to LO. In the next subsections, the proposed operators are explained in detail:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Improving the convergence rate</head><p>In the basic GSA, the gravitational constant (G) is designed to control the right balance between the exploration and exploitation steps, where its value is dynamically changed during the optimization process as shown in Eq. <ref type="bibr" target="#b11">(11)</ref>. At the beginning of the process, it starts with a large value, which allows search agents to move with large steps. Small steps are allowed at the later stages when its value is linearly decreased. However, over the course of iterations, the search agents get heavier, and hence, the searching trend gets slower. This behavior can negatively affect the exploration and convergence speed of the GSA in dealing with some feature spaces. To overcome this drawback, we propose to utilize a logarithmic updating strategy, which is shown in Eq. ( <ref type="formula" target="#formula_16">12</ref>), to replace the linearly decreasing one in the improved version of GSA. This time-varying logarithmic updating strategy can make a more stable balance between exploratory and exploitative steps and facilitate a smoother transition from broad exploration in initial steps to more focused exploitation at the later steps. This strategy has also been enhanced the convergence speed of the PSO in <ref type="bibr" target="#b14">[14]</ref>.</p><formula xml:id="formula_15">G(t) = G(t -1) × t T ,<label>(11)</label></formula><formula xml:id="formula_16">G (t) = G t-1 × (G max -( G) × log (a+ 10×t T ) 10 ), G = G max -G min (<label>12</label></formula><formula xml:id="formula_17">)</formula><p>where G 0 is the initial gravitational constant in range [G min ,G max ], a is the acceleration rate suggested by Gao et al. <ref type="bibr" target="#b14">[14]</ref> to set it to 1, t is the current iteration, and T is the total number of iterations. Figure <ref type="figure">4</ref> shows the time-varying decreasing behavior of G (t) over iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Improving the exploration by a crossover scheme</head><p>The original GSA depends on the mass of objects (solutions) to control the exploration tendency where the heavier solutions (superior solutions) are slower than the lighter ones (inferior solutions). Hence, the good solutions change their positions more slowly in order to reach better fitness values in the vicinity of the current areas. By contrast, the inferior solutions change their position with faster trends in order to discover new areas inside the feature space.</p><p>In order to further assist the GSA in the effective explore a feature space, we proposed a mechanism based on the crossover operator of genetic algorithms to generate new solutions and to enhance the exploratory powers of GSA. A simple example of a crossover operator </p><formula xml:id="formula_18">G(t) = G 0 × (G max -(△G) × log (a+ 10×t T ) 10 ) Figure 4: Behavior of G'(t) over iterations. G 0 =10, G min = 0, G max = 10, a = 1, T = 50</formula><p>for binary solutions is shown in Fig. <ref type="figure">5</ref>. In the proposed mechanism, in each iteration, we sort the search agents based on their fitness values (mass of objects), while the first is the best. Then, we perform the crossover operator based on the last half of the population (worst solutions) and the global best agent in the solution until the current iteration (gbest).</p><p>Note that the definition of gbest solution here is similar to the α, leader of wolves in GWO or gbest in PSO. By performing the new mechanism, N  2 new extra solutions are generated and seeded into the pool of objects. Therefore, the size of the population becomes N + N 2 .</p><p>Finally, we take the best N solutions from the total N + N 2 solutions to be used in the next iteration. The steps of this process also shown in Fig. <ref type="figure">6</ref>.</p><formula xml:id="formula_19">0 1 1 0 0 0 1 1 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Two initial solutions</head><p>The final solutions Crossover point This is done by counting the number of times that the best solution obtained so far has not been improved during the optimization process. In that case, we applied the GA's mutation operator on the gbest to push it outside of the trap. Algorithm 3 summarizes the main steps of this process. Based on Algorithm 3, the generated solution from the mutation operation is reconsidered based on its fitness value. It might become the gbest when its fitness overcomes the gbest's fitness or the fitness of local best agent lbest. In other words, the mutation will produce a new solution; the solution's fitness will determine the rule or the position of that solution in the main population. Note that the population is sorted in an ascending order based on the fitness values of agents. Hence, the best solution takes first place, i.e. X(1), while the worst solution takes the last position i.e. X(N ). This mechanism further alleviates the entrapment shortcoming of GSA to avoid trapping into LO when the leader of the swarm (heaviest object) has an inertia to the local optima and there is no improvement in the quality of gbest agent. In addition, it also enriches the quality of solutions and exploitation tendency of the proposed method.</p><p>Algorithm 2 Pseudo-code of crossover operator function Crossover(gbest,agents) Sort(agents,'fitness') </p><formula xml:id="formula_20">for i = 1, i &lt; N 2 , i++ do agents(N + i) = CrossOver(agents[i], gbest) Sort(agents,'fitness') number of agents is N + N 2 agents = agents[1 : N ] number of agents is N return agents A C C E P T E D M A N U S C R I P T</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Result and Discussion</head><p>This section presents the extensive results of the proposed approaches. The comparisons are conducted in three phases. In the first phase, the results of the HGSA were compared with the results of the binary BGSA approach. Since we used two classifiers for evaluation of the feature subsets, the results of each classifier were compared separately. Then, the results of HGSA on KNN classifier were compared to those obtained by HGSA with the DT classifier.</p><p>In the second phase, the results of HGSA were compared to the state-of-the-art approaches (i.e. GWO, PSO, and GA) where the two classifiers (i.e., KNN and DT) were used as well. The selected algorithms belong to different categories according to the nature of operators; GA <ref type="bibr" target="#b20">[20]</ref> was selected as an evolutionary-based algorithm, PSO <ref type="bibr" target="#b23">[23]</ref>, GWO <ref type="bibr" target="#b33">[33]</ref>, and GSA were selected as swarm-based algorithms. Finally, the results of the HGSA were compared with the latest FS approaches from the literature. To determine whether there is a significant difference between the compared approaches or not, a statistical test based on the Wilcoxon rank-sum test was conducted. Wilcoxon test can be used to calculate the null hypotheses for two populations with the same continuous distribution. The p-value that is less than 0.05 could be considered as strong evidence against the null hypothesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Setup of experiments</head><p>In all experiments, a PC with Intel Core i7, and 8GB RAM was used. All the algorithms are performed on the same configurations and settings as illustrated in Table <ref type="table" target="#tab_5">4</ref>. Since the experiments focus on stochastic algorithms, we reported the average of the results for 30 runs on each dataset. Please note that the bold numbers in all subsequent tables denote the best values. Many researchers have recommended that K = 5 in KNN is the proper value to be used with the adopted datasets in this paper <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b8">8]</ref>. CART DT is used in this research. All algorithms in this work has same number of fitness evaluations therefore, the same number of iterations is applied for all.</p><p>To investigate the efficiency of the proposed approaches, eighteen different datasets from UCI repository <ref type="bibr" target="#b27">[27]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluation Criteria</head><p>Three performance indicators are used for the comparative purposes; classification accuracy, number of selected features, and the fitness values.</p><p>• Average classification accuracy: this indicator measures how accurate is the classifier in predicting the right class using the selected subset of features. The average accuracy is calculated based on Eq. ( <ref type="formula" target="#formula_21">13</ref>):</p><formula xml:id="formula_21">AvgAccuracy = 1 M M j=1 1 N N i=1 (C i == L i ) (<label>13</label></formula><formula xml:id="formula_22">)</formula><p>where M is the number of runs for an algorithm to find the final subset of features, N is the number of dataset instances, C i is the predicted class, and L i is the actual class in the labeled data.</p><p>• Average fitness: This indicator is a composition from a classifier error rate (KNN or DT) and the features reduction rate. This indicator is used as an objective function in the optimization algorithm to identify the goodness of the selected feature subset.</p><p>The average fitness value for each run is calculated as in Eq. ( <ref type="formula" target="#formula_23">14</ref>):</p><formula xml:id="formula_23">AvgF itness = 1 M M j=1 F it i * (<label>14</label></formula><formula xml:id="formula_24">)</formula><p>where M is the number of runs, F it i * is the fitness of the best solution resulted from run i.</p><p>• Average selection size: This indicator represents the performance of an algorithm in terms of selection size when solving the FS problem. This indicator is calculated as in Eq. ( <ref type="formula" target="#formula_25">15</ref>):</p><formula xml:id="formula_25">AvgSize = 1 m M i=1 d * i D (<label>15</label></formula><formula xml:id="formula_26">)</formula><p>where M is the number of runs, d * i is the number of selected features (turned on values) in the binary solution vector from the i-th run, and D is the total number of features in the original dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Sensitivity analysis</head><p>This initial experiment is conducted to perform a sensitivity analysis and to study the influence of the main parameters of the proposed algorithm, which are the population size, number of iterations, G 0 , and k. For this, PenglungEW dataset is selected to conduct the experiments because this dataset showed the highest sensitivity when we performed training of the classification models. The sensitivity analysis is performed in two stages. First, we</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T study the effect of population size and maximum number of iterations, while the K and G 0 are fixed to 5 and 10, respectively. Table <ref type="table" target="#tab_2">2</ref> shows the results of this stage. It can be seen from Table <ref type="table" target="#tab_2">2</ref> that the best (i.e. smallest) fitness value is obtained when the population size is 20 and the number of iterations is 200.</p><p>In the second stage, the impact of k and G 0 are studied by varying their values while fixing the population size and the maximum number of iterations to the best values obtained in the previous stage, which are 10 and 200, respectively. Table <ref type="table" target="#tab_3">3</ref> shows the results of this stage. It can be noticed from Table <ref type="table" target="#tab_3">3</ref> that the best fitness value is reached when we have G 0 = 10 and k = 5. The best obtained values of these parameters will be used for rest of the experiments. Both optimizers obtained the same classification accuracy on one dataset (i.e. Exactly). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>When using the DT classifier, HGSA is still better than BGSA. Again, HGSA obtained the best results in 61% of the datasets while BGSA performs better than HGSA in seven out of 18 datasets, and both algorithms shared the same accuracy (96.6%) for "WineEW" dataset. It is worth mentioning that HGSA was able to obtain the best results for both KNN and DT on the same eight datasets, while BGSA performed the same on four datasets only. Number of selected features is another important aspect of wrapper FS approaches. Table <ref type="table" target="#tab_7">6</ref> shows a comparison between BGSA and HGSA over the minimal number of selected features on all datasets. When analyzing the reported result, its can be clearly observed that HGSA significantly outperformed BGSA in minimizing the number of selected feature.</p><p>As can be seen in Table <ref type="table" target="#tab_7">6</ref>, HGSA provided the best results in 89% of the datasets. However, BGSA outperformed HGSA on only three dataset including the m-of-n dataset where both approaches obtained the same result.</p><p>When using the DT classifier, HGSA also showed a superior performance over the BGSA approach. HBGSA was able to obtain the minimal reduct in 68% of the datasets, while BGSA obtained the best results in the remaining datasets. These results show the ability of the HGSA in finding the most important features by better exploring the search space than BGSA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T Table <ref type="table" target="#tab_8">7</ref> shows the the average fitness values that recorded for both HGSA and BGSA.</p><p>Observing the fitness values of the KNN classifier in the table, it can be seen that the HGSA obtained better results than BGSA in ten out of eighteen datasets, while BGSA obtained the best results in seven datasets, and both algorithms performed the same result for the Exactly dataset. So, HGSA achieved the best results in 55% of the datasets. The same observation can be made when considering the results for the DT classifier, where HGSA outperform the BGSA in eleven out of eighteen datasets, while BGSA performs better in seven datasets. As such, HGSA is better than BGSA in 61.1% of the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T The superior results show the merits of the proposed approach in both classification accuracy and reduction rate since both criteria are involved in the fitness function. This can be interpreted due to some strong properties of the GSA algorithm, and the enhancement we made in the HGSA approach. In GSA, the gravitational force, that is applied on each individual in the swarm, can be considered as knowledge-transferring tool. Thus, each individual can adjust its position based on some knowledge about the surrounding area and make prudent exploration and exploitation depending on its mass. Moreover, applying the evolutionary operator (i.e. crossover and mutation) in HGSA enhanced the GSA ability in escaping from LO and enhance the diversity of the algorithm.</p><p>Table <ref type="table" target="#tab_9">8</ref> shows the best, average and worst results of BGSA and HGSA based on PenglungEW dataset which is the same dataset was selected in the previous section due to its high sensitivity. The table also shows the results for KNN and DT classifiers. It can be seen that HGSA outperforms BGSA in terms of best, average and worst results of fitness values, accuracy and number of selected features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2.">Comparing HGSA performance for KNN and DT</head><p>In the previous sections, we compared the performance of HGSA and BGSA over the considered evaluation criteria. As it was shown previously, HGSA outperformed BGSA on the majority of datasets based on all criteria. Here, we are interested to examine the performance of HGSA on both classifiers. Table <ref type="table" target="#tab_10">9</ref> shows the average classification accuracy, fitness values, and number of selected features results of HGSA on both KNN and DT</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T classifiers. The average classification accuracy of both classifiers is visualized using line chart in Fig. <ref type="figure" target="#fig_5">8</ref>. Boxplots of accuracy rates are also demonstrated in Fig. <ref type="figure" target="#fig_7">7</ref>.</p><p>Inspecting the results in Table <ref type="table" target="#tab_10">9</ref>, and Fig. <ref type="figure" target="#fig_7">7</ref>, it can be noted that HGSA obtained higher accuracy when using the KNN in 78% of the datasets. In contrast, the HGSA recorded a superior performance when using DT in terms of number of features. This result can be justified due to the nature of each classifier and the way it adapt to classify the testing samples. KNN is classified under the category of lazy learner since it does not build any model and depends on calculating the distances between objects. On the other hand, DT classifier comes under the category of Eager Learners, because it builds a classification model based on the training data, which is used to classify the unseen objects from the testing set.</p><p>DT classifier depends on the information gain to build a classification tree. Therefore, the tree is built based on the information that can be gained from features. This property gives DT the ability to select the discriminatory features better than KNN.</p><p>Comparing the performance of both classifiers in terms of selected features and the classification accuracy in Table <ref type="table" target="#tab_10">9</ref> and Figs. <ref type="figure" target="#fig_7">7</ref> and<ref type="figure" target="#fig_5">8</ref>, we can conclude that selecting the most informative features (in DT case) would reduce the number of features, but it does not guarantee that those features reveal the highest classification accuracy. Since KNN does not consider the amount of information in each feature and consider the feature vector as a block, this improves its ability to find the best feature subset and reveal the highest accuracy.</p><p>The superiority of HGSA with KNN is observed again when considering the fitness values.</p><p>Since there is more emphasize on the classification accuracy in the fitness function, these results are reasonable and expected. Its worth mentioning that the KNN-based approach is better than DT-based variant in terms of fitness values on 78% of datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T  Fis. 9 and 10 compare the convergence behaviors of different methods. From Figs. 9 and 10, it can be observed that the convergence behaviors of HGSA is more accelerated than that of BGSA in general for both classifiers in harmony with the results in Table <ref type="table" target="#tab_8">7</ref> when   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3.">Comparing HGSA with the-state-of-the-art approaches</head><p>After analyzing the results of BGSA and HGSA approaches with two KNN and DT classifiers, we found that the overall performance of HBGSA with both classifiers is much better than BGSA for all evaluation criteria on majority of datasets. The main purpose of this subsection is to compare the performance of HGSA with the state-of-the-art FS approaches;</p><p>i.e. GA, PSO, and GWO. To make a fair comparison, all approaches were implemented for the comparison purposes with the same settings and conditions. Table <ref type="table" target="#tab_12">10</ref> shows classification accuracy of different methods using both classifiers; KNN and DT. The results in Table <ref type="table" target="#tab_12">10</ref> evidently show the exploratory and exploitative merits of the proposed HGSA using any classifier over other wrapper approaches. With the KNN classifier, HGSA outperformed other algorithms in sixteen out of eighteen (88.8%) datasets, while GWO obtained the best results         The main reason for improved results and performance of the proposed GSA-based wrapper is that it has been integrated with a time-varying gravitational function in an adaptive manner, and can reveal improved crossover and mutation-based exploration and exploitation behaviors. Consequently, it can make a more stable balance between exploratory and exploitative inclinations when dealing with various feature spaces. As a result, the immature convergence and stagnation shortcomings of the basic GSA-based wrapper is alleviated, significantly. These evolutionary dynamic features has led to improved results in terms of accuracy rates, selected features, and fitness values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.4.">HGSA vs. state-of-the-art approaches</head><p>After analyzing the results of HGSA and comparing it with the state-of-the-art approaches, the results are also compared with other well-established wrapper approaches from the literature. Emary et al. <ref type="bibr" target="#b8">[8]</ref> proposed GWO-based FS approaches. Their parameter settings are very similar to those we adopted. Assuming that the researchers used the best settings that assure the best results of their approaches, we obtained the results from their paper and made a comparison with the proposed HGSA. Since they used KNN classifier as an evaluator, we used to compare their work with the KNN based HGSA wrapper. Kashef and Nezamabadi-pour <ref type="bibr" target="#b22">[22]</ref>, is another research work where proposed another FS approach.</p><p>The source code from the authors were used and the method also tested on the same datasets employed in this research. Finally, applying our novel algorithm (HGSA) to other domains, and using other FS models (e.g. filter approach using rough set theory), can be explored as a future extension of this work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Interaction of agents in GSA</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Used transfer function</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>where γ R (D) represents the classification error rate of the KNN classifier, |R| is the number of selected features, |N | is the number of original features in the dataset, α and β are the weighting factors of the classification error rate and cardinality of the subset, α ∈ [0, 1] and β = (1α)<ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b30">30]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: Crossover operator</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Accuracy rates of HGSA-based wrapper with KNN and DT classifiers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Boxplots of accuracy rates of the proposed HGSA with DT and KNN classifiers for all datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Convergence curves for BGSA and HGSA on first 9 datasets with DT classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Convergence curves for BGSA and HGSA on second 9 datasets with DT classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Visual monitoring of average accuracy results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>are utilized. The datasets are chosen with different properties (e.g., number of features, number of instances, and number of classes), and from various kinds of instances) of the utilized datasets. List of datasets</figDesc><table><row><cell>Dataset</cell><cell cols="2">No.of Features No.of instances</cell></row><row><cell cols="2">Breastcancer BreastEW Exactly Exactly2 HeartEW Lymphography 18 9 30 13 13 13 M-of-n 13 PenglungEW 325 SonarEW 60 SpectEW 22 CongressEW 16 IonosphereEW 34 KrvskpEW 36 Tic-tac-toe 9 Vote 16 WaveformEW 40 WineEW 13 Zoo 16</cell><cell>699 569 1000 1000 270 148 1000 73 208 267 435 351 3196 958 300 5000 178 101</cell></row></table><note><p>370</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Analyzing the impact of population size and number of iterations on the fitness values</figDesc><table><row><cell>Pop. Size/iteration 25 5 0.1007 0.0939 0.0871 0.0777 0.0646 50 100 150 200 7 0.0878 0.0908 0.0779 0.0771 0.0746 10 0.0847 0.0893 0.0657 0.0766 0.0694 20 0.0827 0.0708 0.0742 0.0682 0.0603 30 0.0733 0.0642 0.0738 0.0669 0.0892</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Analyzing the impact of K parameter and G 0 on the fitness values</figDesc><table><row><cell>G 0 /K 1 3 0.0551 0.1075 0.0553 0.1545 0.1022 3 5 7 9 7 0.0899 0.0323 0.1077 0.0905 0.1896 10 0.0379 0.0205 0.0087 0.1024 0.1775 25 0.0899 0.096 0.0961 0.0728 0.1662 30 0.0844 0.0903 0.0438 0.0845 0.1139</cell></row><row><cell>5.3.1. Comparing HGSA and BGSA with KNN and DT classifiers</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc></figDesc><table /><note><p>shows the average classification accuracy results for HGSA and BGSA. Inspecting the results in this table, it can be observed that HGSA performs better than BGSA in terms of classification accuracy when using KNN classifier, where it obtained the best results in 61% of the datasets, while BGSA was able to outperform HGSA in 38% of the datasets.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Experimental setup</figDesc><table><row><cell>Config. Name Number of runs Number of iterations Number of agents G 0 (for GSA) a (for GWO) ω (for PSO) GA selection Mutation Probability (in GA) 50% Value 30 200 20 10 from 2 to 0 from 2 to 0 Roulette Wheel Selection Crossover probability (in GA) 50% K for KNN 5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparison between BGSA and HGSA in terms of average classification accuracy for KNN and DT classifiers</figDesc><table><row><cell>Dataset</cell><cell cols="2">KNN classifier BGSA HGSA BGSA HGSA DT Classifier</cell></row><row><cell>Breastcancer</cell><cell cols="2">0.971 0.974 0.968 0.966</cell></row><row><cell>BreastEW</cell><cell>0.973 0.971</cell><cell>0.957 0.964</cell></row><row><cell>CongressEW</cell><cell cols="2">0.968 0.966 0.981 0.949</cell></row><row><cell>Exactly</cell><cell cols="2">1.000 1.000 0.807 0.984</cell></row><row><cell>Exactly2</cell><cell cols="2">0.763 0.770 0.761 0.764</cell></row><row><cell>HeartEW</cell><cell cols="2">0.866 0.856 0.867 0.815</cell></row><row><cell cols="3">IonosphereEW 0.908 0.934 0.947 0.963</cell></row><row><cell>KrvskpEW</cell><cell cols="2">0.974 0.978 0.991 0.996</cell></row><row><cell cols="3">Lymphography 0.886 0.892 0.861 0.835</cell></row><row><cell>M-of-n</cell><cell cols="2">0.986 1.000 1.000 0.992</cell></row><row><cell>penglungEW</cell><cell cols="2">0.939 0.956 0.726 0.794</cell></row><row><cell>SonarEW</cell><cell cols="2">0.890 0.958 0.848 0.869</cell></row><row><cell>SpectEW</cell><cell cols="2">0.893 0.919 0.893 0.902</cell></row><row><cell>Tic-tac-toe</cell><cell>0.798 0.788</cell><cell>0.849 0.866</cell></row><row><cell>Vote</cell><cell cols="2">0.970 0.973 0.947 0.960</cell></row><row><cell>WaveformEW</cell><cell cols="2">0.818 0.815 0.783 0.769</cell></row><row><cell>WineEW</cell><cell cols="2">0.994 0.989 0.966 0.966</cell></row><row><cell>Zoo</cell><cell cols="2">0.998 0.932 0.958 0.950</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Comparison between BGSA and HGSA in terms of average number of selected features for KNN and DT classifiers</figDesc><table><row><cell>Dataset</cell><cell cols="4">KNN classifier BGSA HGSA BGSA HGSA DT Classifier</cell></row><row><cell>Breastcancer</cell><cell>4.07</cell><cell>4.00</cell><cell>2.60</cell><cell>3.13</cell></row><row><cell>BreastEW</cell><cell cols="3">16.23 13.80 12.13</cell><cell>7.10</cell></row><row><cell>CongressEW</cell><cell>5.40</cell><cell>3.93</cell><cell>2.70</cell><cell>3.00</cell></row><row><cell>Exactly</cell><cell>6.00</cell><cell>6.00</cell><cell>6.70</cell><cell>6.00</cell></row><row><cell>Exactly2</cell><cell>3.50</cell><cell>4.70</cell><cell>3.43</cell><cell>3.70</cell></row><row><cell>HeartEW</cell><cell>6.43</cell><cell>6.70</cell><cell>3.07</cell><cell>3.00</cell></row><row><cell cols="2">IonosphereEW 10.90</cell><cell>8.17</cell><cell>13.57</cell><cell>7.50</cell></row><row><cell>KrvskpEW</cell><cell cols="4">16.97 15.73 22.03 20.00</cell></row><row><cell>Lymphography</cell><cell>7.40</cell><cell>7.03</cell><cell>4.83</cell><cell>4.13</cell></row><row><cell>M-of-n</cell><cell>7.00</cell><cell>6.00</cell><cell>6.00</cell><cell>7.00</cell></row><row><cell>penglungEW</cell><cell cols="4">150.07 43.03 154.07 20.40</cell></row><row><cell>SonarEW</cell><cell cols="3">26.00 25.40 27.93</cell><cell>8.20</cell></row><row><cell>SpectEW</cell><cell>11.60</cell><cell>8.03</cell><cell>8.50</cell><cell>6.00</cell></row><row><cell>Tic-tac-toe</cell><cell>8.87</cell><cell>8.73</cell><cell>7.03</cell><cell>8.00</cell></row><row><cell>Vote</cell><cell>9.00</cell><cell>3.07</cell><cell>3.37</cell><cell>4.00</cell></row><row><cell>WaveformEW</cell><cell cols="4">20.03 19.50 18.30 16.43</cell></row><row><cell>WineEW</cell><cell>6.23</cell><cell>5.00</cell><cell>5.07</cell><cell>4.00</cell></row><row><cell>Zoo</cell><cell>7.83</cell><cell>5.63</cell><cell>5.97</cell><cell>4.93</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Comparison between BGSA and HGSA in terms of average fitness values for KNN and DT classifiers</figDesc><table><row><cell>Dataset</cell><cell cols="2">KNN classifier BGSA HGSA BGSA HGSA DT Classifier</cell></row><row><cell>Breastcancer</cell><cell cols="2">0.034 0.031 0.035 0.038</cell></row><row><cell>BreastEW</cell><cell>0.032 0.033</cell><cell>0.046 0.038</cell></row><row><cell>CongressEW</cell><cell cols="2">0.036 0.036 0.021 0.052</cell></row><row><cell>Exactly</cell><cell cols="2">0.005 0.005 0.196 0.021</cell></row><row><cell>Exactly2</cell><cell cols="2">0.237 0.232 0.239 0.237</cell></row><row><cell>HeartEW</cell><cell cols="2">0.138 0.149 0.135 0.186</cell></row><row><cell cols="3">IonosphereEW 0.095 0.068 0.057 0.039</cell></row><row><cell>KrvskpEW</cell><cell cols="2">0.031 0.027 0.015 0.010</cell></row><row><cell cols="3">Lymphography 0.117 0.111 0.141 0.166</cell></row><row><cell>M-of-n</cell><cell cols="2">0.020 0.005 0.005 0.014</cell></row><row><cell>penglungEW</cell><cell cols="2">0.065 0.045 0.276 0.204</cell></row><row><cell>SonarEW</cell><cell cols="2">0.113 0.046 0.155 0.131</cell></row><row><cell>SpectEW</cell><cell cols="2">0.112 0.084 0.110 0.100</cell></row><row><cell>Tic-tac-toe</cell><cell>0.211 0.221</cell><cell>0.159 0.142</cell></row><row><cell>Vote</cell><cell cols="2">0.035 0.028 0.055 0.042</cell></row><row><cell>WaveformEW</cell><cell cols="2">0.185 0.189 0.220 0.233</cell></row><row><cell>WineEW</cell><cell>0.011 0.015</cell><cell>0.038 0.037</cell></row><row><cell>Zoo</cell><cell cols="2">0.007 0.071 0.046 0.052</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Best, average and worst results of BGSA and HGSA based on PenglungEW dataset.</figDesc><table><row><cell>Classifier</cell><cell></cell><cell cols="2">Avg. Fitness</cell><cell cols="4">Avg. Accuracy Avg. no. of selected features</cell></row><row><cell></cell><cell cols="6">Metric BGSA HGSA BGSA HGSA BGSA Best 0.033 0.001 0.971 1 135</cell><cell>HGSA 34</cell></row><row><cell>KNN</cell><cell cols="2">Average 0.065</cell><cell>0.045</cell><cell>0.939</cell><cell cols="2">0.956 150.067</cell><cell>43.033</cell></row><row><cell></cell><cell>Worst</cell><cell>0.092</cell><cell>0.06</cell><cell>0.912</cell><cell>0.941</cell><cell>166</cell><cell>65</cell></row><row><cell></cell><cell>Best</cell><cell>0.266</cell><cell>0.204</cell><cell>0.735</cell><cell>0.794</cell><cell>134</cell><cell>10</cell></row><row><cell>DT</cell><cell cols="2">Average 0.276</cell><cell>0.204</cell><cell>0.726</cell><cell cols="2">0.794 154.067</cell><cell>20.4</cell></row><row><cell></cell><cell>Worst</cell><cell>0.296</cell><cell>0.205</cell><cell>0.706</cell><cell>0.794</cell><cell>171</cell><cell>32</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>The performance of HGSA for both KNN and DT classifiers for all measures</figDesc><table><row><cell></cell><cell>Dataset</cell><cell cols="4">Avg. Accuracy Avg. no. of Features Avg. Fitness KNN DT KNN DT KNN DT</cell></row><row><cell></cell><cell>Breastcancer</cell><cell>0.974 0.966</cell><cell>4.00</cell><cell>3.13</cell><cell>0.031 0.038</cell></row><row><cell></cell><cell>BreastEW</cell><cell cols="2">0.971 0.964 13.80</cell><cell>7.10</cell><cell>0.033 0.038</cell></row><row><cell></cell><cell>CongressEW</cell><cell>0.966 0.949</cell><cell>3.93</cell><cell>3.00</cell><cell>0.036 0.052</cell></row><row><cell></cell><cell>Exactly</cell><cell>1.000 0.984</cell><cell>6.00</cell><cell>6.00</cell><cell>0.005 0.021</cell></row><row><cell></cell><cell>Exactly2</cell><cell>0.770 0.764</cell><cell>4.70</cell><cell>3.70</cell><cell>0.232 0.237</cell></row><row><cell></cell><cell>HeartEW</cell><cell>0.856 0.815</cell><cell>6.70</cell><cell>3.00</cell><cell>0.149 0.186</cell></row><row><cell></cell><cell cols="2">IonosphereEW 0.934 0.963</cell><cell>8.17</cell><cell>7.50</cell><cell>0.068 0.039</cell></row><row><cell></cell><cell>KrvskpEW</cell><cell cols="2">0.978 0.996 15.73</cell><cell>20.00</cell><cell>0.027 0.010</cell></row><row><cell></cell><cell cols="2">Lymphography 0.892 0.835</cell><cell>7.03</cell><cell>4.13</cell><cell>0.111 0.166</cell></row><row><cell></cell><cell>M-of-n</cell><cell>1.000 0.992</cell><cell>6.00</cell><cell>7.00</cell><cell>0.005 0.014</cell></row><row><cell></cell><cell>penglungEW</cell><cell cols="2">0.956 0.794 43.03</cell><cell>20.40</cell><cell>0.045 0.204</cell></row><row><cell></cell><cell>SonarEW</cell><cell cols="2">0.958 0.869 25.40</cell><cell>8.20</cell><cell>0.046 0.131</cell></row><row><cell></cell><cell>SpectEW</cell><cell>0.919 0.902</cell><cell>8.03</cell><cell>6.00</cell><cell>0.084 0.100</cell></row><row><cell></cell><cell>Tic-tac-toe</cell><cell>0.788 0.866</cell><cell>8.73</cell><cell>8.00</cell><cell>0.221 0.142</cell></row><row><cell></cell><cell>Vote</cell><cell>0.973 0.960</cell><cell>3.07</cell><cell>4.00</cell><cell>0.028 0.042</cell></row><row><cell></cell><cell>WaveformEW</cell><cell cols="2">0.815 0.769 19.50</cell><cell>16.43</cell><cell>0.189 0.233</cell></row><row><cell></cell><cell>WineEW</cell><cell>0.989 0.966</cell><cell>5.00</cell><cell>4.00</cell><cell>0.015 0.037</cell></row><row><cell></cell><cell>Zoo</cell><cell>0.932 0.950</cell><cell>5.63</cell><cell>4.93</cell><cell>0.071 0.052</cell></row><row><cell></cell><cell>1.020</cell><cell></cell><cell cols="2">K-NN classifier</cell><cell>DT Classifier</cell></row><row><cell></cell><cell>0.970</cell><cell></cell><cell></cell><cell></cell></row><row><cell>results</cell><cell>0.920</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy</cell><cell>0.770 0.820 0.870</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.720</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Datasets</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>only in two datasets. A similar observation can be made about HGSA using DT classifier, i.e. HGSA outperformed other algorithms in fifteen out of eighteen datasets (83.3%), while GWO with KNN performed better than other approaches in two datasets, and GA with DT only in one dataset. Table11shows the standard deviation values calculated for the accuracy results. It can be see that HGSA has very competitive small values of standard deviations compared to the other algorithms which indicates the stability of its performance.The p-values of Wilcoxon test is tabulated in Table12. Results in Table12show that the superiority of HGSA is statistically significant because the most of p-values are less than 0.05 for both KNN and DT classifiers.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>Comparison between HGSA and the state-of-the-art FS approaches in terms of average classification accuracy</figDesc><table><row><cell>Dataset</cell><cell cols="3">KNN Classifier GWO PSO GA BGSA HGSA GWO PSO DT Classifier GA BGSA HGSA</cell></row><row><cell>Breastcancer</cell><cell>0.980 0.972 0.973 0.971</cell><cell cols="2">0.974 0.960 0.946 0.964 0.968 0.966</cell></row><row><cell>BreastEW</cell><cell cols="3">0.954 0.948 0.953 0.973 0.971 0.948 0.926 0.958 0.957 0.964</cell></row><row><cell>CongressEW</cell><cell cols="2">0.927 0.957 0.940 0.968 0.966 0.982 0.948 0.958 0.981</cell><cell>0.949</cell></row><row><cell>Exactly</cell><cell cols="3">0.731 0.994 0.748 1.000 1.000 0.752 0.913 0.759 0.807 0.984</cell></row><row><cell>Exactly2</cell><cell cols="3">0.758 0.743 0.761 0.763 0.770 0.759 0.751 0.760 0.761 0.764</cell></row><row><cell>HeartEW</cell><cell cols="3">0.853 0.795 0.832 0.866 0.856 0.809 0.808 0.834 0.867 0.815</cell></row><row><cell cols="4">IonosphereEW 0.893 0.883 0.876 0.908 0.934 0.940 0.927 0.924 0.947 0.963</cell></row><row><cell>KrvskpEW</cell><cell cols="3">0.955 0.965 0.930 0.974 0.978 0.984 0.987 0.953 0.991 0.996</cell></row><row><cell cols="4">Lymphography 0.831 0.759 0.815 0.886 0.892 0.816 0.779 0.825 0.861 0.835</cell></row><row><cell>M-of-n</cell><cell cols="3">0.910 0.993 0.877 0.986 1.000 1.000 0.970 0.883 1.000 0.992</cell></row><row><cell>penglungEW</cell><cell cols="3">0.868 0.661 0.861 0.939 0.956 0.681 0.449 0.643 0.726 0.794</cell></row><row><cell>SonarEW</cell><cell cols="3">0.795 0.763 0.853 0.890 0.958 0.797 0.709 0.802 0.848 0.869</cell></row><row><cell>SpectEW</cell><cell cols="3">0.833 0.810 0.864 0.893 0.919 0.842 0.818 0.852 0.893 0.902</cell></row><row><cell>Tic-tac-toe</cell><cell cols="3">0.790 0.783 0.759 0.798 0.788 0.815 0.832 0.850 0.849 0.866</cell></row><row><cell>Vote</cell><cell cols="3">0.940 0.933 0.931 0.970 0.973 0.932 0.958 0.925 0.947 0.960</cell></row><row><cell>WaveformEW</cell><cell cols="3">0.784 0.788 0.761 0.818 0.815 0.748 0.735 0.739 0.783 0.769</cell></row><row><cell>WineEW</cell><cell cols="3">0.980 0.937 0.967 0.994 0.989 0.952 0.951 0.964 0.966 0.966</cell></row><row><cell>Zoo</cell><cell cols="3">0.924 0.817 0.930 0.998 0.932 0.888 0.845 0.924 0.958 0.950</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 :</head><label>12</label><figDesc>p-Values obtained from the rank-sum test for the results in Table 10 (Bold values are less than 0.05). Vote 8.30E-13 1.11E-12 8.26E-13 2.36E-05 1.18E-13 8.15E-02 3.77E-13 1.69E-14 WaveformEW 2.89E-11 2.95E-11 2.96E-11 1.59E-04 2.97E-11 2.98E-11 2.97E-11 1.58E-10</figDesc><table><row><cell>Dataset</cell><cell>GWO</cell><cell cols="2">KNN Classifier PSO GA</cell><cell>HGSA</cell><cell>GWO</cell><cell cols="2">DT Classifier PSO GA</cell><cell>HGSA</cell></row><row><cell>Breastcancer</cell><cell cols="2">2.69E-09 3.46E-01</cell><cell cols="5">3.80E-06 2.71E-14 2.46E-11 7.21E-10 8.46E-04 7.29E-08</cell></row><row><cell>BreastEW</cell><cell cols="7">2.02E-11 2.12E-11 2.06E-11 5.25E-03 4.48E-12 3.96E-12 9.80E-09 9.95E-13</cell></row><row><cell>Exactly</cell><cell cols="2">3.68E-12 6.15E-01</cell><cell cols="2">2.71E-12 1.51E-01</cell><cell cols="2">1.69E-14 6.40E-01</cell><cell>1.18E-13 1.18E-13</cell></row><row><cell>Exactly2</cell><cell cols="7">1.16E-12 1.69E-14 4.47E-12 1.20E-12 3.31E-11 1.60E-13 1.20E-12 2.03E-05</cell></row><row><cell>HeartEW</cell><cell cols="2">8.60E-13 3.68E-01</cell><cell cols="5">1.61E-09 8.86E-06 2.28E-04 4.03E-02 6.34E-03 2.29E-02</cell></row><row><cell cols="2">Lymphography 1.37E-01</cell><cell cols="5">1.40E-11 1.24E-11 4.33E-12 4.19E-03 6.41E-01</cell><cell>5.17E-11 1.69E-14</cell></row><row><cell>M-of-n</cell><cell cols="7">1.07E-10 4.98E-11 2.33E-11 2.53E-08 1.42E-11 1.87E-11 1.35E-11 2.04E-11</cell></row><row><cell>penglungEW</cell><cell cols="7">2.32E-11 4.42E-10 2.33E-11 3.97E-03 3.03E-12 2.66E-12 3.14E-12 2.53E-12</cell></row><row><cell>SonarEW</cell><cell cols="4">3.26E-12 3.84E-12 3.21E-12 5.40E-01</cell><cell cols="3">1.01E-02 2.50E-05 8.63E-06 4.31E-12</cell></row><row><cell>SpectEW</cell><cell cols="7">1.20E-12 6.60E-04 1.20E-12 1.69E-14 1.69E-14 1.69E-14 1.09E-12 1.69E-14</cell></row><row><cell>CongressEW</cell><cell cols="7">8.28E-12 8.10E-12 4.19E-12 7.19E-04 4.16E-14 1.13E-12 5.22E-13 2.90E-13</cell></row><row><cell cols="8">IonosphereEW 2.36E-11 2.55E-11 2.25E-11 2.19E-11 1.81E-11 1.95E-11 3.80E-11 7.51E-08</cell></row><row><cell>KrvskpEW</cell><cell cols="7">1.66E-11 1.69E-11 1.03E-11 1.08E-11 5.90E-13 1.17E-12 8.74E-13 3.92E-12</cell></row><row><cell>Tic-tac-toe</cell><cell cols="2">7.09E-08 4.95E-01</cell><cell cols="5">1.87E-11 2.61E-12 1.04E-12 3.19E-11 1.52E-04 4.16E-14</cell></row><row><cell>WineEW</cell><cell cols="6">4.65E-06 3.09E-11 3.95E-11 5.53E-05 7.44E-04 1.56E-01</cell><cell>1.38E-01</cell><cell>2.66E-12</cell></row><row><cell>Zoo</cell><cell cols="3">4.19E-02 2.99E-11 4.51E-01</cell><cell cols="4">9.62E-13 1.28E-11 1.07E-11 2.53E-09 1.03E-03</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 13</head><label>13</label><figDesc>compares the PSO, GA, GWO and HGSA based on average number of selected features. For both KNN and DT classifiers, HGSA obtained the best result on 83% and 88% of the datasets, respectively.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 13 :</head><label>13</label><figDesc>Comparison between HGSA and the state-of-the-art FS approaches in terms of average number of feature</figDesc><table><row><cell>Dataset</cell><cell cols="10">KNN Classifier GWO PSO GA BGSA HGSA GWO PSO DT Classifier GA BGSA HGSA</cell></row><row><cell>Breastcancer</cell><cell>6.0</cell><cell>6.1</cell><cell>3.9</cell><cell>4.1</cell><cell>4.0</cell><cell>4.7</cell><cell>4.4</cell><cell>3.8</cell><cell>2.6</cell><cell>3.1</cell></row><row><cell>BreastEW</cell><cell>19.9</cell><cell>17.0</cell><cell>16.1</cell><cell>16.2</cell><cell>13.8</cell><cell>15.7</cell><cell>14.9</cell><cell>15.1</cell><cell>12.1</cell><cell>7.1</cell></row><row><cell>CongressEW</cell><cell>8.4</cell><cell>6.4</cell><cell>4.8</cell><cell>5.4</cell><cell>3.9</cell><cell>10.0</cell><cell>8.4</cell><cell>7.1</cell><cell>2.7</cell><cell>3.0</cell></row><row><cell>Exactly</cell><cell>10.0</cell><cell>6.0</cell><cell>7.4</cell><cell>6.0</cell><cell>6.0</cell><cell>7.4</cell><cell>6.2</cell><cell>7.3</cell><cell>6.7</cell><cell>6.0</cell></row><row><cell>Exactly2</cell><cell>3.3</cell><cell>4.6</cell><cell>4.1</cell><cell>3.5</cell><cell>4.7</cell><cell>3.6</cell><cell>5.7</cell><cell>4.1</cell><cell>3.4</cell><cell>3.7</cell></row><row><cell>HeartEW</cell><cell>11.7</cell><cell>7.9</cell><cell>6.9</cell><cell>6.4</cell><cell>6.7</cell><cell>7.9</cell><cell>6.2</cell><cell>6.4</cell><cell>3.1</cell><cell>3.0</cell></row><row><cell>IonosphereEW</cell><cell>18.2</cell><cell>14.5</cell><cell>15.2</cell><cell>10.9</cell><cell>8.2</cell><cell>20.9</cell><cell>17.4</cell><cell>15.6</cell><cell>13.6</cell><cell>7.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 14 :</head><label>14</label><figDesc>Comparison between HGSA and the state-of-the-art FS approaches in terms of average fitness values HeartEW 0.155 0.209 0.172 0.138 0.149 0.196 0.195 0.170 0.135 0.186 IonosphereEW 0.112 0.120 0.128 0.095 0.068 0.066 0.078 0.080 0.057 0.039 KrvskpEW 0.053 0.040 0.075 0.031 0.027 0.024 0.021 0.052 0.015 0.010 Lymphography 0.173 0.244 0.188 0.117 0.111 0.187 0.224 0.178 0.141 0.166 M-of-n 0.097 0.011 0.128 0.020 0.005 0.008 0.037 0.122 0.005 0.014 penglungEW 0.136 0.340 0.142 0.065 0.045 0.320 0.551 0.358 0.276 0.204 SonarEW 0.209 0.240 0.150 0.113 0.046 0.207 0.293 0.201 0.155 0.131 SpectEW 0.171 0.190 0.140 0.112 0.084 0.163 0.185 0.151 0.110 0.100 Tic-tac-toe 0.218 0.223 0.246 0.211 0.221 0.192 0.174 0.157 0.159 0.142 Vote 0.067 0.071 0.073 0.035 0.028 0.072 0.047 0.078 0.055 0.042 WaveformEW 0.223 0.216 0.242 0.185 0.189 0.258 0.267 0.264 0.220 0.233 WineEW 0.026 0.068 0.038 0.011 0.015 0.054 0.054 0.041 0.038 0.037 Zoo 0.081 0.187 0.075 0.007 0.071 0.116 0.159 0.080 0.046 0.052</figDesc><table><row><cell>Dataset</cell><cell cols="2">KNN Classifier GWO PSO GA BGSA HGSA GWO PSO DT Classifier GA BGSA HGSA</cell></row><row><cell>Breastcancer</cell><cell>0.027 0.035 0.032 0.034</cell><cell>0.031 0.045 0.059 0.040 0.035 0.038</cell></row><row><cell>BreastEW</cell><cell cols="2">0.053 0.057 0.052 0.032 0.033 0.057 0.078 0.047 0.046 0.038</cell></row><row><cell>CongressEW</cell><cell cols="2">0.078 0.046 0.062 0.036 0.036 0.025 0.057 0.047 0.021 0.052</cell></row><row><cell>Exactly</cell><cell cols="2">0.275 0.011 0.255 0.005 0.005 0.251 0.090 0.244 0.196 0.021</cell></row><row><cell>Exactly2</cell><cell cols="2">0.242 0.258 0.240 0.237 0.232 0.241 0.250 0.241 0.239 0.237</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T Algorithm 4 Pseudo-code of HGSA algorithm</head><p>Input: Number of agents and total number of iterations (L). Output: The best agent and the its fitness value. Initialize G 0 = 10 and a = 1 gravitational constant, acceleration ratio Initialize the candidate agents X i (i = 1, 2, . . . , N ) Evaluate the fitness of all objects while (end condition is not met) do agents = Crossover(gbest,agents ) Call Algorithm 2 if gbest is stuck then agents = Mutation(gbest,agents ) Call Algorithm 3 Evaluate the fitness of all agents Calculate best, worst, M i , M j Update the gravitational factor G(t) using Eq. ( <ref type="formula">12</ref>) Calculate forces F ij by Eq. ( <ref type="formula">6</ref>) Update acceleration by Eq. ( <ref type="formula">5</ref>) Update velocities using Eq. ( <ref type="formula">4</ref>) Update positions by Eq. ( <ref type="formula">3</ref>) Return the best search agent real-life problems. Table <ref type="table">1</ref> reports a brief description (including the number of features and  in <ref type="bibr" target="#b22">[22]</ref> PSO1 in <ref type="bibr" target="#b22">[22]</ref> GWO1 in <ref type="bibr" target="#b8">[8]</ref> GWO2 in <ref type="bibr" target="#b8">[8]</ref> GA2 in <ref type="bibr" target="#b8">[8]</ref> PSO2 in <ref type="bibr" target="#b8">[8]</ref>. Please note that the algorithm names have been concatenated with numbers to distinguish them from each other.</p><p>Figure <ref type="figure">11</ref> also compares the accuracy rates of the proposed HGSA versus different methods.</p><p>Observing results in Table <ref type="table">15</ref> and Fig. <ref type="figure">11</ref>, it can be clearly seen that HGSA can outperform other approaches in dealing with almost all datasets. The proposed wrapper obtained better results than GA1 method on all datasets. However, it is evident GWO2, GA2, and PSO2 are unable to show superior results in tackling any of these datasets. There are some main reasons for the superiority of HGSA over other optimizers; first, the dynamic gravitational force, which is applied to each search agent, can be considered as a knowledge-transferring scheme. With this force, the particles can adjust their positions and scan more areas around themselves (vicinity of explored solutions) looking for better regions and solutions. In addition, the factor in Eq. ( <ref type="formula">12</ref>) helps HGSA to start the search by more exploration and gradually and, then, more smoothly switch the broad exploration to more focused exploitative steps at the last iterations. This simple time-varying scheme brings several advantages to HGSA compared to other peers. For one, it helps HGSA to avoid premature convergence due to the hasty convergence to LO and limited exploration of the feature space. For another, it brings more stability to the searching performance of proposed algorithm when switching from exploration to exploitation. It can also emphasize on exploitation in last steps, which improves the quality of found feature sets. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Table 14 shows that HGSA with KNN outperformed other approaches on the majority of datasets. This becomes clear when we see HGSA outperformed other algorithms in sixteen out of eighteen (88.8%) datasets, while GWO in only two datasets. When we use DT, HGSA outperformed other algorithms in fifteen out of eighteen datasets (83.3%), while GWO performed better only in two datasets</title>
		<imprint/>
	</monogr>
	<note>9 In terms of fitness values. and GA in single dataset. References</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An efficient salp swarm-inspired algorithm for parameters identification of photovoltaic cell models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Abbassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abbassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Heidari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mirjalili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Energy Conversion and Management</title>
		<imprint>
			<biblScope unit="volume">179</biblScope>
			<biblScope unit="page" from="362" to="372" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Asynchronous accelerating multi-leader salp chains for feature selection</title>
		<author>
			<persName><forename type="first">I</forename><surname>Aljarah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mafarja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Heidari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Faris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mirjalili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="964" to="979" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hyperspam: A study on hyper-heuristic coordination strategies in the continuous domain</title>
		<author>
			<persName><forename type="first">F</forename><surname>Caraffini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Neri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Epitropakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">477</biblScope>
			<biblScope unit="page" from="186" to="202" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<author>
			<persName><forename type="first">F</forename><surname>Caraffini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Neri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Iacca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parallel memetic structures</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">227</biblScope>
			<biblScope unit="page" from="60" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An analysis on separability for memetic computing automatic design</title>
		<author>
			<persName><forename type="first">F</forename><surname>Caraffini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Neri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Picinali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">265</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improved gravitational search algorithm for parameter identification of water turbine regulation system</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Energy Conversion and Management</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="306" to="315" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A new optimizer using particle swarm theory</title>
		<author>
			<persName><forename type="first">R</forename><surname>Eberhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Symposium on</title>
		<meeting>the Sixth International Symposium on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1995">1995. 1995</date>
			<biblScope unit="page" from="39" to="43" />
		</imprint>
	</monogr>
	<note>Micro Machine and Human Science</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Binary grey wolf optimization approaches for feature selection</title>
		<author>
			<persName><forename type="first">E</forename><surname>Emary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Zawbaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Hassanien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page" from="371" to="381" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An intelligent system for spam detection and identification of the most relevant features based on evolutionary random weight networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Faris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Al-Zoubi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Heidari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Aljarah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mafarja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hassonah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fujita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="67" to="83" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Grey wolf optimizer: a review of recent variants and applications</title>
		<author>
			<persName><forename type="first">H</forename><surname>Faris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Aljarah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Al-Betar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mirjalili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Computing and Applications</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An efficient binary salp swarm algorithm with crossover scheme for feature selection problems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Faris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Mafarja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Heidari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Aljarah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-Z</forename><surname>Ala'm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fujita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">154</biblScope>
			<biblScope unit="page" from="43" to="67" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">From data mining to knowledge discovery in databases</title>
		<author>
			<persName><forename type="first">U</forename><surname>Fayyad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Piatetsky-Shapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">37</biblScope>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Swarmed feature selection</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Firpi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. International Symposium on</title>
		<meeting>International Symposium on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="112" to="118" />
		</imprint>
	</monogr>
	<note>In Information Theory, 2004. ISIT 2004</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A particle swarm optimization algorithm with logarithm decreasing inertia weight and chaos mutation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>-L</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-H</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Intelligence and Security, 2008. CIS&apos;08. International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="61" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kamber</surname></persName>
		</author>
		<title level="m">Data mining: concepts and techniques</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Feature subset selection by gravitational search algorithm optimization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">281</biblScope>
			<biblScope unit="page" from="128" to="146" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">An enhanced associative learning-based exploratory whale optimizer for global optimization</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Heidari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Aljarah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Faris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mirjalili</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019a</date>
			<publisher>Neural Computing and Applications</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An efficient hybrid multilayer perceptron neural network with grasshopper optimization</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Heidari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Faris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Aljarah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mirjalili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soft Computing</title>
		<imprint>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Harris hawks optimization: Algorithm and applications</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Heidari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Faris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Aljarah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mafarja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Genetic algorithms</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Holland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific american</title>
		<imprint>
			<biblScope unit="volume">267</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="66" to="73" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ockham&apos;s razor in memetic computing: three stage optimal memetic exploration</title>
		<author>
			<persName><forename type="first">G</forename><surname>Iacca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Neri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mininno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">188</biblScope>
			<biblScope unit="page" from="17" to="43" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An advanced aco algorithm for feature subset selection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kashef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nezamabadi-Pour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="page" from="271" to="279" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Particle swarm optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of machine learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="760" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A discrete binary version of the particle swarm algorithm</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Eberhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Systems, Man, and Cybernetics</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1997">1997. 1997. 1997</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="4104" to="4108" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Parameters identification of hydraulic turbine governing system using improved gravitational search algorithm</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Energy Conversion and Management</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="374" to="381" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An enhanced grey wolf optimization based feature selection wrapped kernel extreme learning machine for medical diagnosis</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational and mathematical methods in medicine</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lichman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Feature selection for knowledge discovery and data mining</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Motoda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">454</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Binary dragonfly optimization for feature selection using time-varying transfer functions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mafarja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Aljarah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Heidari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Faris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fournier-Viger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mirjalili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">161</biblScope>
			<biblScope unit="page" from="185" to="204" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Evolutionary population dynamics and grasshopper optimization approaches for feature selection problems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mafarja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Aljarah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Heidari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Hammouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Faris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-Z</forename><surname>Ala'm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mirjalili</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Knowledge-Based Systems</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Whale optimization approaches for wrapper feature selection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mafarja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mirjalili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="441" to="453" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hybrid whale optimization algorithm with simulated annealing for feature selection</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Mafarja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mirjalili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">260</biblScope>
			<biblScope unit="page" from="302" to="312" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Grey Wolf Optimizer: Theory, Literature Review, and Application in Computational Fluid Dynamics Problems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Aljarah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mafarja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Heidari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Faris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="87" to="105" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Chaotic gravitational constants for the gravitational search algorithm</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Gandomi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied soft computing</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="407" to="419" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A new hybrid psogsa algorithm for function optimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z M</forename><surname>Hashim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer and information application (ICCIA), 2010 international conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="374" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">S-shaped versus v-shaped transfer functions for binary particle swarm optimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Swarm and Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hybrid genetic algorithms for feature selection</title>
		<author>
			<persName><forename type="first">I.-S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-R</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1424" to="1437" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Feature selection for image steganalysis using levy flight-based grey wolf optimization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Arya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tiwari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Cluster-based population initialization for differential evolution frameworks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Poikolainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Neri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Caraffini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">297</biblScope>
			<biblScope unit="page" from="216" to="235" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Improving the precision of cbir systems by feature selection using binary gravitational search algorithm</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rashedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nezamabadi-Pour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Signal Processing (AISP), 2012 16th CSI International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="39" to="042" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Feature subset selection using improved binary gravitational search algorithm</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rashedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nezamabadi-Pour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent &amp; Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1211" to="1221" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Gsa: a gravitational search algorithm</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rashedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nezamabadi-Pour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saryazdi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information sciences</title>
		<imprint>
			<biblScope unit="volume">179</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="2232" to="2248" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Bgsa: binary gravitational search algorithm</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rashedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nezamabadi-Pour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saryazdi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Computing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="727" to="745" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A comprehensive survey on gravitational search algorithm</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rashedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rashedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nezamabadi-Pour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Swarm and Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="141" to="158" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Disruption: a new operator in gravitational search algorithm</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sarafrazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nezamabadi-Pour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saryazdi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientia Iranica</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="539" to="548" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A novel opposition-based gravitational search algorithm for combined economic and emission dispatch problems of power systems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghoshal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Electrical Power &amp; Energy Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="33" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Metaheuristics: from design to implementation</title>
		<author>
			<persName><forename type="first">E.-G</forename><surname>Talbi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">74</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">No free lunch theorems for optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Wolpert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">G</forename><surname>Macready</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on evolutionary computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="67" to="82" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Feature subset selection using a genetic algorithm</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Honavar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems and their Applications</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="44" to="49" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A novel hybrid k-harmonic means and gravitational search algorithm approach for clustering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="9319" to="9324" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
