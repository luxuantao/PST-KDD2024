<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Query Dependent Pseudo-Relevance Feedback based on Wikipedia</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yang</forename><surname>Xu</surname></persName>
							<email>xuyang@ict.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Gareth</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
							<email>gjones@computing.dcu.ie</email>
						</author>
						<author>
							<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
							<email>wangbin@ict.ac.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology Chinese Academy of Sciences Beijing</orgName>
								<address>
									<postCode>100190</postCode>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Dublin City University Glasnevin</orgName>
								<address>
									<settlement>Dublin</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Institute of Computing Technology Chinese Academy of Sciences Beijing</orgName>
								<address>
									<postCode>100190</postCode>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Query Dependent Pseudo-Relevance Feedback based on Wikipedia</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D5166A69CABD001C58952A7692281C11</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T10:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.3 [Information Search and Retrieval]: Search and Retrieval -search process</term>
					<term>query formulation Algorithms</term>
					<term>Design</term>
					<term>Experimentation</term>
					<term>Performance Information Retrieval</term>
					<term>Entity</term>
					<term>Query Expansion</term>
					<term>Pseudorelevance feedback</term>
					<term>Wikipedia</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pseudo-relevance feedback (PRF) via query-expansion has been proven to be effective in many information retrieval (IR) tasks. In most existing work, the top-ranked documents from an initial search are assumed to be relevant and used for PRF. One problem with this approach is that one or more of the top retrieved documents may be non-relevant, which can introduce noise into the feedback process. Besides, existing methods generally do not take into account the significantly different types of queries that are often entered into an IR system. Intuitively, Wikipedia can be seen as a large, manually edited document collection which could be exploited to improve document retrieval effectiveness within PRF. It is not obvious how we might best utilize information from Wikipedia in PRF, and to date, the potential of Wikipedia for this task has been largely unexplored. In our work, we present a systematic exploration of the utilization of Wikipedia in PRF for query dependent expansion. Specifically, we classify TREC topics into three categories based on Wikipedia: 1) entity queries, 2) ambiguous queries, and 3) broader queries. We propose and study the effectiveness of three methods for expansion term selection, each modeling the Wikipedia based pseudo-relevance information from a different perspective. We incorporate the expansion terms into the original query and use language modeling IR to evaluate these methods. Experiments on four TREC test collections, including the large web collection GOV2, show that retrieval performance of each type of query can be improved. In addition, we demonstrate that the proposed method outperforms the baseline relevance model in terms of precision and robustness.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>One of the fundamental problems of information retrieval (IR) is to search for documents that satisfy a user's information need. Often a query is too short to describe the specific information need clearly. For a long time query expansion has been a focus for researchers because it has the potential to enhance IR effectiveness by adding useful terms that can help discriminate relevant documents from irrelevant ones. For all query expansion methods, pseudo relevance feedback (PRF) is attractive because it requires no user input <ref type="bibr" target="#b21">[21]</ref>[2] <ref type="bibr" target="#b31">[31]</ref> <ref type="bibr" target="#b3">[3]</ref>. PRF assumes that the top-ranked documents in the initial retrieval are relevant. However, this assumption is often invalid <ref type="bibr" target="#b3">[3]</ref> which can result in a negative impact on PRF performance. Meanwhile, as the volume of data on the web becomes much larger, other resources have emerged which can potentially supplement an initial search better in PRF, e.g. Wikipedia.</p><p>Wikipedia is a free online encyclopedia edited collaboratively by large numbers of volunteers (web users). The exponential growth and the reliability of Wikipedia make it a potentially valuable resource for IR <ref type="bibr" target="#b33">[33]</ref>. The aim of this study is to explore the possible utility of Wikipedia as a resource improving for IR in PRF. The basic entry in Wikipedia is an entity page, which is an article that contains information focusing on one single entity. Wikipedia covers a great many topics, and it might reasonably be assumed to reflect the diverse interests and information needs of users of many search engines <ref type="bibr" target="#b28">[28]</ref>. With the help of enriched text, we can expect to bridge the gap between the large volume of information on the web and the simple queries issued by users. However, few studies have directly examined whether Wikipedia, especially the internal structures of Wikipedia articles, can indeed help in IR systems.</p><p>As far as we are aware, there is little work done investigating the impact of Wikipedia on different types of queries. In this paper, we propose a query-dependent method for using PRF for query expansion, on the basis of Wikepdia. Given a query, we categorize it into one of three types: 1) query about a specific entity (EQ); 2) ambiguous query (AQ); 3) broader query (BQ). Pseudo relevant documents are generated in two ways according to the query type: 1) using top ranked articles from Wikipedia retrieved in response to the query, and 2) using Wikipedia entity pages corresponding to queries. In selecting expansion terms, term distributions and structures of Wikipedia pages are taken into account. We propose and compare a supervised method and an unsupervised method for this task. Based on these methods, we evaluate the effect of Wikipedia on PRF for IR. Our experiments show different methods impact differently on the three types of queries. Thus a query dependent query expansion is necessary to optimally benefit retrieval performance.</p><p>The contributions of this work are as follows: 1) we thoroughly evaluate the potential of Wikipedia for IR as a resource for PRF, 2) we explore the use of Wikipedia as an entity repository as well as its internal structure for retrieval, and based on these two aspects, different methods for selecting expansion terms are proposed and compared, and 3) our method is conducted in a query dependent way, which is more effective and robust than a single method.</p><p>The remainder of this paper is organized as follows: Section 2 provides a detailed account of related work, Section 3 introduces query categorization based on Wikipedia, Section 4 describes our proposed methods for using Wikipedia as pseudo relevant documents, experimental results are reported in Section 5, and we conclude in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Automatic query expansion is a widely used technique in IR. PRF has been shown to be an effective way of improving retrieval accuracy by reformulating an original query using pseudo-relevant documents from the initial retrieval result <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b34">34]</ref>. Traditional PRF algorithms such as Okapi <ref type="bibr" target="#b27">[27]</ref>, Lavrenko and Croft's relevance model <ref type="bibr" target="#b18">[18]</ref>, and Zhai and Lafferty's mixture model <ref type="bibr" target="#b34">[34]</ref> are based on the assumption that the top-ranked documents from an initial search are relevant to the query.</p><p>Large amounts of research has been reported on attempts to improve traditional PRF, e.g. using latent concepts <ref type="bibr" target="#b21">[21]</ref>, query-regularized estimation <ref type="bibr" target="#b29">[29]</ref>, additional features other than term distributions <ref type="bibr" target="#b3">[3]</ref>, and a clustered-based re-sampling method for generating pseudo-relevant documents <ref type="bibr" target="#b19">[19]</ref>.</p><p>On the other hand, there has been work on selective query expansion which aims to improve query expansion with decision mechanisms based on the characteristics of queries <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b13">13]</ref>. These methods share a similar idea, that is, to disable query expansion if the query is predicted to perform poorly. Other relevant work is that of He and Ounis who proposed selecting an appropriate collection resource for query expansion <ref type="bibr" target="#b13">[13]</ref>.</p><p>Cui et al. proposed a query expansion approach by mining user logs <ref type="bibr" target="#b12">[12]</ref>. They extracted correlations between query terms and document terms by analyzing user logs, and then used these to select expansion terms for new queries.</p><p>A concept-based interactive query expansion method was suggested by Fonseca et al. also using query logs <ref type="bibr" target="#b10">[10]</ref>. Association rules are applied to identify concepts related to the current query from the query context derived from user logs. These concepts then provide suggestions to refine the vague (short) query.</p><p>Kwok and Chan <ref type="bibr" target="#b17">[17]</ref> studied the idea of using an external resource for query expansion. They found that query expansion failure can be caused by the lack of relevant documents in the local collection. Therefore, the performance of query expansion can be improved by using a large external collection. Several external collection enrichment approaches have been proposed <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b5">5]</ref>. Our work follows this strategy of a query expansion approach using an external collection as a source of query expansion terms.</p><p>Recently, Wikipedia <ref type="bibr" target="#b30">[30]</ref> has emerged as a potentially important resource for IR, a number of studies have been reported which adopt Wikipedia to assist query expansion, many of these have appeared in the context of the TREC Blog track <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr">8,</ref><ref type="bibr" target="#b16">16]</ref>.</p><p>One interesting example of the use of Wikipedia in this way is the work of Li et al. <ref type="bibr" target="#b20">[20]</ref> who proposed using it for query expansion by utilizing the category assignments of Wikipedia articles. The base query is run against a Wikipedia collection and each category is assigned a weight proportional to the number of top-ranked articles assigned to it. Articles are then re-ranked based on the sum of the weights of the categories to which each belongs. The method shows improvement over PRF in measures favoring weak queries. A thesaurus-based query expansion method using Wikipedia was proposed by Milne, Witten and Nichols <ref type="bibr" target="#b23">[23]</ref>. The thesaurus is derived with criteria such that topics relevant to the document collection are included. They propose to extract significant topics from a query by checking consecutive sequences of words in the query against the thesaurus. However, query dependent knowledge is not taken into consideration by the thesaurus. Elsas et al. investigated link-based query expansion using Wikipedia in <ref type="bibr" target="#b7">[7]</ref>. They focused on anchor text and proposed a phrase scoring function. Our work differs from these methods, in that, expansion terms are not selected directly from the documents obtained by running the base query on the Wikipedia. Instead Wikipedia entity pages are viewed as a set of pseudo-relevant documents tailored to the specific query.</p><p>Similar to the query log based expansion methods, our approach can also reflect the preference of the majority of the users. However, our work differs from these methods in that we try to narrow the gap between the large volume of information on the web and the simple queries issued by users, by mapping queries to Wikipedia entity pages which reflect knowledge underlying the query, rather than using a simple bag-of-words query model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">QUERY CATEGORIZATION</head><p>In this section we briefly summarize the relevant features of Wikipedia for our study, and then examine the different categories of queries that are typically encountered in user queries and how these can be related to the properties of Wikipedia for effective query expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Wikipedia Data Set</head><p>Wikipedia is a free online encyclopedia which has detailed guidelines governing its content quality and format. Unlike traditional encyclopedias, Wikipedia articles are constantly being updated and new entries are created everyday. These characteristics make it an ideal repository of background knowledge for query expansion.</p><p>Wikipedia is organized as one article per topic. Each article thus effectively summarizes the most important information of each topic. A topic in Wikipedia has a distinct, separate existence often referring to a specific entity, such as a person, a place, an organization or miscellaneous. In ad- dition, important information for the topic of a given article may also be found in other Wikipedia articles <ref type="bibr" target="#b9">[9]</ref>. An analysis of 200 randomly selected articles describing a single topic (not including redirect pages and disambiguation pages, described below) showed that only 4% failed to adhere to a standard format. Based on our analysis of Wikipedia page structure, in our work we divide Wikipedia articles into six fields as shown in Table <ref type="table" target="#tab_0">1</ref>.</p><p>In addition to topic pages, Wikipedia contains "redirect" pages which provide alternative ways of referring to a topic: e.g. the page Einstein redirects to the article Albert Einstein which contains information about the physicist. Another useful structure of Wikipedia is its so-called "disambiguation pages" which list the referents of ambiguous words and phrases that denote two or more topics in Wikipedia.</p><p>As an open source project, Wikipedia is obtainable via download from http://download.wikipedia.org, which is in the form of database dumps that are released periodically. We downloaded the English Wikipedia dump of January, 2008. Wikipedia articles are stored in their own markup language called Wikitext which preserves features, e.g. categories, hyperlinks, subsections, tables, pictures. We index all the pages using the Indri search engine <ref type="bibr" target="#b14">[14]</ref>. Preprocessing includes removing Wikipedia pages that are used for management purpose, e.g. those with Wikipedia in the title. Wikipedia articles are stemmed using Porter stemming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Query Categorization</head><p>Queries in web search may vary largely in semantics and the user's intensions they present, in numbers of relevant documents they have in the document repository, and in numbers of entities they involve. In this work, we define three types of queries according to their relationship with Wikipedia topics : 1) queries about a specific entity (EQ), 2) ambiguous queries (AQ), and 3) broader queries (BQ). By EQ, we mean queries that have a specific meaning and cover a narrow topic, e.g. "Scalable Vector Machine". By AQ, we mean queries with terms having more than one potential meaning, e.g. "Apple". Finally, we denote the rest of the queries to be BQ, because these queries are neither ambiguous nor focused on a specific entity. For the first two types of queries, a corresponding Wikipedia entity page can be determined for the query. For EQ, the corresponding entity page is the page with the same title field as the query. For AQ, a disambiguation process is needed to determine its sense. For all three types of queries, a set of top ranked retrieved documents is obtained from an initial search.</p><p>Our method automatically categorizes queries, with the help of titles of Wikipedia articles. Of particular interest are the titles of entity pages, redirect pages and disambiguation pages. Queries exactly matching one title of an entity page or a redirect page will be classified as EQ. Thus EQ can be mapped directly to the entity page with the same title. Note that queries with both entity page and disambiguation pages will be counted as EQ, because the existing entity page indicates that there is consensus on a dominant sense for the word or phrase, e.g. "Piracy'" and "Poaching".</p><p>To identify AQ, we look for queries with terms in the ambiguous list of terms/phrases (derived from extracting all the titles of disambiguation pages). All other queries are then classified as BQ. Though the categorization process is simple, we show in later experiments that the results are helpful for retrieval performance enhancement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Strategy for Ambiguous Queries</head><p>We now explain our solution to the problem of mapping ambiguous queries to a specific entity page. In previous research on query ambiguity, one assumption is that when an ambiguous query is submitted, the person that initiates the search knows the intended sense of each ambiguous word <ref type="bibr" target="#b28">[28]</ref>. Our solution is also based on this assumption.</p><p>Given an ambiguous query, we first obtain the top ranked 100 documents from the target collection to be searched using the query likelihood retrieval model. Then K-means clustering is applied <ref type="bibr" target="#b15">[15]</ref> to cluster these documents. Each document is represented by tf * idf weighting and cosine normalization. Cosine similarity is used to compute the similarities among the top documents.</p><p>After clustering, we rank these clusters by a cluster-based language model, as proposed by Lee et al. <ref type="bibr" target="#b19">[19]</ref>, given as follows:</p><formula xml:id="formula_0">P (Q|Clu) = m i=1 P (qi|Clu)</formula><p>where P (w|Clu) is specified by the cluster based language model</p><formula xml:id="formula_1">P (w|Clu) = |Clu| |Clu| + λ PML(w|Clu)+ λ |Clu| + λ PML(w|Coll)</formula><p>where PML(w|Clu) is the maximum likelihood estimate of word w in the cluster and PML(w|Coll) is the maximum likelihood estimate of word w in the collection. The smoothing parameter λ is learned using training topics on each collection in experiments. PML(w|Clu) and PML(w|Coll) are specified as follows:</p><formula xml:id="formula_2">PML(w|Clu) = f req(w, Clu) |Clu| , PML(w|Coll) = f req(w, Coll) |Coll|</formula><p>where f req(w, Clu) is the sum of f req(w, D) for the document D which belongs to the cluster Clu, f req(w, D) denotes the frequency of w in D, and f req(w, Coll) is the number of times w occurs in the collection.</p><p>The documents in the top-ranked clusters are used to represent the characteristics of the test collection, in terms of pseudo relevant documents in response to a specific query. The top ranked cluster is then compared to all the referents (entity pages) extracted from the disambiguation page associated with the query. The assumption is that the dominant sense for the query should have a much greater degree of matching to the top ranked cluster from the test collection than other senses. Each document is represented by tf * idf weighting and the cosine is used to measure the similarity between one cluster and an entity page. The top matching entity page is then chosen for the query. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Evaluation</head><p>In order to evaluate the accuracy of our query categorization process, we used the four sets of TREC topics used in the retrieval experiments reported in Section 5, with five human subjects. These are taken from four different search tasks and comprise a total of 650 queries. Each participant was asked to judge whether a query is ambiguous or not. If it was, the participant was required to determine which referent from the disambiguation page is most likely to be mapped to the query, according to the description of the TREC topic. If it was not, the participant was required to manually search with the query in Wikipedia to identify whether or not it is an entity defined by Wikipedia (EQ). The user study results indicate that for query categorization, participants are in general agreement, i.e. 87% in judging whether a query is ambiguous or not. However, when determining which referent should a query be mapped to, there is only 54% agreement.</p><p>Table <ref type="table" target="#tab_1">2</ref> shows the result of automatic query categorization process. It can be seen from Table <ref type="table" target="#tab_1">2</ref> that most queries from TREC topic sets are AQ. That is to say, most of the queries contain ambiguity to some degree. Thus, it is necessary to handle this properly according to query type in query expansion.</p><p>To test the effectiveness of the cluster-based disambiguation process, we define that for each query, if there are at least two participants who indicate a referent as the most likely mapping target, this target will be used as an answer. If a query has no answer, it will not be counted by the evaluation process. Experimental results show that our disambiguation process leads to an accuracy of 57% for AQ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">QUERY EXPANSION METHODS</head><p>In this section, we describe our query expansion methods. Using these methods, we investigate the relationship between the query type and expansion methods. Moreover, we look into how to combine evidence from different fields of Wikipedia articles for query expansion. Essentially these methods differ in their criteria of selecting expansion terms. In each of these methods, query specific relevance information is considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Relevance model</head><p>A relevance model is a query expansion approach based on the language modeling framework <ref type="bibr" target="#b18">[18]</ref>. The relevance model is a multinomial distribution which estimates the likelihood of word w given a query Q. In the model, the query words q1, ..., qm and the word w in relevant documents are sampled identically and independently from a distribution R. Thus the probability of a word in the distribution R is estimated as follows :</p><formula xml:id="formula_3">P (w|R) = D∈F P (D)P (w|D)P (Q|D)</formula><p>where F is the set of documents that are pseudo-relevant to the query Q. We assume that P (D) is uniform over the set.</p><p>Based on this estimation, the most likely expansion term e from P (w|D) is chosen for the original query. The final expanded query is combined with the original query using linear interpolation, weighted by a parameter λ. P (w|Q ) = (1 -λ)P (w|Q) + λP (w|R)</p><p>The original relevance model and traditional PRF methods use the top retrieved documents from an initial search as pseudo-relevant documents. The problem is that the top retrieved documents frequently contain non-relevant documents or content, which can introduce noise into the expanded query, resulting in query drift. Our approach introduces Wikipedia articles as pseudo-relevant documents. This may still find non-relevant documents, but we will show that it can enhance the quality of pseudo-relevant documents for query expansion. This method forms the baseline in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Strategy for Entity/Ambiguous Queries</head><p>One of the issues for web queries is they are often too short to clearly express a user's information need. With the help of Wikipedia, this issue is expected to be reduced to some degree. Entity queries are those "good" queries whose sense is given clearly. On the contrary, it is harder to find relevant documents for ambiguous queries. Both EQ and AQ can be associated with a specific Wikipedia entity page. In this strategy, instead of considering the top-ranked documents from the test collection, only the corresponding entity page from Wikipedia will be used as pseudo-relevant information. We briefly introduced entity pages in the first part of Section 3.1. An entity page contains the most representative information for the entity, which most Wikipedia users have an agreed consensus on.</p><p>Our strategy firstly ranks all the terms in the entity page, then the top K terms are chosen for expansion. The measure to score each term is defined as: score(t) = tf * idf , where tf is the term frequency in the entity page. idf is computed as log(N/df ), where N is the number of documents in the Wikipedia collection, and df is the number of documents that contain term t. The measure is simple, yet we will show in later experiments that it is very effective.</p><p>The performance of existing PRF methods is often affected significantly by parameters, e.g. the number of feedback documents used. The proposed method eases the problem by utilizing the fact that one article exists for each entity in Wikipedia which focuses on details of the subject.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Field Evidence for Query Expansion</head><p>Currently, structured documents are becoming more and more common, in consequence several studies have been conducted on exploiting field combination for improved IR <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b26">26]</ref>. Although just semi-structured, our observations show that the evidence of different fields of Wikipedia can be used for improving retrieval, e.g. the importance of a term appearing in the overview may be different than its appearance in an appendix.</p><p>We examine two methods for utilizing evidence from different fields. The first is similar to that proposed by Robertson et al. for the BM25 model in <ref type="bibr" target="#b26">[26]</ref>, we replace the term frequency in a pseudo relevance document from original relevance model with a linearly combined weighted term frequencies. The second method is a supervised learning approach which classifies "good" expansion terms from "bad" ones. Features derived from fields are used by the classifier. Note that the field based expansion methods are applicable to all the queries. For EQ and AQ, the pseudo relevant documents can be either a query specific entity page, or just the same as BQ, i.e. the top ranked entity pages from the initial search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unsupervised Method.</head><p>Robertson et al. <ref type="bibr" target="#b26">[26]</ref> gives a detailed analysis of the disadvantages of linear combination of the scores obtained from scoring every field, and recommends a simple weighted field extension. We use a similar idea in our work to explore the impact of different fields in Wikipedia. By assigning different weights to fields, we modify the PML(w|D) in the original relevance model to the following form:</p><formula xml:id="formula_4">PML(w|D) = K f =1 W f * T F f (w, D) |D|</formula><p>where K (here K = 6) is the number of fields in a document, and K f =1 W f = 1. Parameter tuning is needed for each single pair of parameters. We evaluate different weight settings for the fields, shown in next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supervised Method (Support Vector Machines).</head><p>An alternative way of utilizing evidence of field information is to transfer it into features for supervised learning. The learning process is to train a classifier which distinguishes "good" expansion terms from "bad" ones. This method is inspired by the work of Cao et al. <ref type="bibr" target="#b3">[3]</ref>, where they suggest that "good" terms should be identified directly according to their possible impact on retrieval effectiveness.</p><p>We use Support Vector Machines (SVMs) <ref type="bibr" target="#b1">[1]</ref>, which are a popular supervised learner for tasks such as this, as a classifier. A radial-based kernel (RBF) SVM with default settings based on LIBSVM <ref type="bibr" target="#b4">[4]</ref> is used. Parameters are estimated with a 5-fold cross-validation to maximize the classification accuracy of the training data.</p><p>In our work, we want to select good expansion terms and re-weight terms. It is important for us to know the probability estimated for each term belonging to each class. We set LIBSVM to train an SVM model for probability estimates. We compute posterior probabilities from SVM outputs using the method of <ref type="bibr" target="#b25">[25]</ref>, P (+1|x) = </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>., fn(e)].</head><p>The first group of features are the term distributions in the PRF documents and collections. Term distributions (TD) have already been used and proven to be useful in traditional approaches. The assumption is that the most useful features for term selection make the largest difference between the feedback documents and the whole collection. The features that we used include: (1) TD in the test collection;</p><p>(2) TD in PRF (top 10) from the test collection; (3) TD in the Wikipedia collection; and (4) TD in PRF (top 10) from the Wikipedia collection. TD in the PRF documents from Wikipedia is given below, the others can be defined similarly.</p><formula xml:id="formula_5">f prf W iki (e) = log D∈F W tf (e, D) t D∈F</formula><p>W tf (t, D) where F is the set of feedback documents, W is the Wikipedia collection.</p><p>The second group of features is based on field evidence. As described in section 3.1, we divide each entity page into six fields. One feature is defined for each field; this is computed as follows :</p><formula xml:id="formula_6">f F ield i (e) = log(tfi(e) * idf /f ieldLengthi)</formula><p>where tfi(e) is the term frequency in the field i for the entity page, and f ieldLengthi is the length of the field i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AP</head><p>ROBUST WT10G GOV2 Accuracy 74.15% 75.30% 72.99% 75.75%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3: Term classification results</head><p>Training and test data are generated using a method similar to Cao et al. <ref type="bibr" target="#b3">[3]</ref>, that is, to label possible expansion terms of each query as good terms or non-good terms to see their impact on retrieval effectiveness. We define good expansion terms as those that improve the effectiveness when λ f b = 0.01 and hurt the effectiveness when λ f b = -0.01. Terms that do not satisfy these criteria are counted as bad terms. We now examine the classification quality. We use four test collections, see Table <ref type="table" target="#tab_2">4</ref>. We divide queries from the same collection into three equal size groups, and then perform a leave-one-out cross validation to evaluate classification accuracy, shown in Table <ref type="table">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment Settings</head><p>In our experiments, documents are retrieved for a given query by the query-likelihood language model with Dirichlet smoothing. We set the Dirichlet prior empirically at 1,500 as recommended in <ref type="bibr" target="#b22">[22]</ref>. The experiments were carried out using the Indri search engine. Indri provides support for PRF via a modified version of Lavrenko's relevance model. We implemented our new methods on top of Indri.</p><p>Experiments were conducted using four standard Text Retrieval Conference (TREC) collections : Associated Press is a small homogeneous collection; Robust2004, is the test collection for the TREC Robust Track started in 2003 to focus on poor performing queries; and two Web collections: the WT10G collection and the large-scale .GOV2 collection. Further details of these collections are given in Table <ref type="table" target="#tab_2">4</ref>. Retrieval effectiveness is measured in terms of Mean Average Precision (MAP). Given an initial query Qorig, the relevance model first retrieves a set of N documents and forms a relevance model from them. It then forms an expanded query QRM by adding the top K most likely non stopword terms from the relevance model. The expanded query is formed with the following structure: #weight( 1.0 Qorig λ f b QRM )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corpus</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baselines</head><p>The baseline of our experiments is the query-likelihood language model (QL) and the relevance model (RMC). Besides, we also consider the relevance model based on Wikipedia (RMW). Note that our RMW method retrieves the top ranked N Wikipedia articles that are not "redirect pages". For RMC and RMW we fixed the parameters for the rest experiments: N = 10, K = 100 and λ = 0.6, for a fair comparison. As can be seen from Table <ref type="table" target="#tab_3">5</ref>, Wikipedia is a good resource for relevance feedback on large collections. Using Wikipedia for generating PRF, RMW brings comparable performance with that produced by RMC for Robust2004, WT10G and GOV2. For AP, although RMW does not work well as RMC, RMW still improves performance over QL. This serves as the baseline for our following experiments. We also believe that the test collections and Wikipedia have their own advantages. For test collections, the initial search emphasizes characteristics of the collection (e.g. GOV2 consists of web pages from government web sites), while Wikipedia appears to be more general in terms of topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Using Entity Pages for Relevance Feedback</head><p>We now turn our attention to our proposed method utilizing only the entity page corresponding to the query for PRF (RE), (see section 4.2). The results are presented in Table <ref type="table" target="#tab_4">6</ref>. Note that in our proposed method, not all the queries can be mapped to a specific Wikipedia entity page, thus the method is only applicable to EQ and AQ. Results for EQ and AQ are shown in Table <ref type="table" target="#tab_4">6</ref> and Table <ref type="table" target="#tab_5">7</ref> respectively. Note that results in Table <ref type="table" target="#tab_5">7</ref> are based on automatic disambiguation. QL, RMC and RMW in the two tables are the averages for the same groups of queries.</p><p>As can be seen in Table <ref type="table" target="#tab_4">6</ref> and Table <ref type="table" target="#tab_5">7</ref>, RE outperforms RMC and RMW on all collections. This indicates that what entity pages provide is relevant information that is closely related to the query, but might be missing in the test collection. Thus exploiting an entity page as the sole relevance feedback source works for both small and large collections. We also notice that the improvement of RE over RMW is greater for entity queries than for ambiguous queries. This is because the accuracy of the automatic disambiguation process is rather low (see section 3.2.2). If AQ is not associated with the most relevant entity page, its performance suffers from the RE method. Comparing Tables <ref type="table" target="#tab_4">5, 6</ref> and<ref type="table" target="#tab_5">7</ref>, we can see that entity queries have better performance than the average, while ambiguous queries have lower performance than the average. For ambiguous queries, using the top retrieved documents as pseudo relevant documents usually includes more non-relevant documents than for entity queries. The disambiguation process helps to find the relevant entity page for ambiguous queries, thus both types of queries can benefit from the query expansion. Based on these results, refinement of the disambiguation process for AQ could be expected to further improve performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Field Based Expansion</head><p>Inspired by the result that an entity page can indeed help to improve retrieval performance, we next go on to investigate utilizing field evidence from entity pages. We will first see the results of two different term selection methods based on field evidence, then an analysis between them is given. Note that in our experiments on field based expansion, the top retrieved documents are considered pseudo relevant documents for EQ and AQ.</p><p>For the supervised method, we compare two ways of incorporating expansion terms for retrieval. The first is to add the top ranked 100 good terms (SL). The second is to add the top ranked 10 good terms, each given the classification probability as weight (SLW). The relevance model with weighted TFs is denoted as (RMWTF). From Table <ref type="table" target="#tab_6">8</ref>, we can see that both methods enhance retrieval performance. Among the three methods, RMWTF and SLW generate similar results. However, the SLW is subject to the accuracy of term classification, thus we choose RMWTF for the query dependent method introduced in the next section. Although we do not give results for BQ for space reasons, our experiments show that BQ is improved by the field based expansion. In addition, characteristics of BQ could be investigated so that pseudo relevant documents could be tailored for BQ, and the field based expansion still be applied.  Figure <ref type="figure" target="#fig_1">1</ref> shows the results of assigning different weight to fields on GOV2. As can be seen in Figure <ref type="figure" target="#fig_1">1</ref>, performance improves as weights for Links, Content increase. On the other hand, the increase of weight to Overview leads to deterioration of the performance. This shows that the positions where a term appears have different impacts on the indication of term relevancy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Query Dependent Expansion</head><p>We now explore a query dependent expansion method. We assign different methods for queries according to their types as follows: RE for EQ and AQ, RMWTF for BQ. RE is chosen because EQ and AQ benefit more from RE than from RMWTF. For AQ, a disambiguation process is conducted to determine the corresponding entity page. We denote the query dependent method as (QD). As can be seen in Table <ref type="table" target="#tab_7">9</ref>, the improvement of QD over RMC is significant across test collections.  Table <ref type="table" target="#tab_7">9</ref> also presents an analysis of the robustness by giving the numbers of queries improved by RMC and QD over QL respectively. QD shows improvement for robustness on Robust2004, WT10G and GOV2 collections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Parameter Selection</head><p>Both the RMC and RMW methods have parameters N , k and λ f b . We tested these two methods with 10 different values of N , the number of feedback documents: 10, 20,..., 100. For λ f b , we tested with 5 different values: 0.4, 0.5, 0.6, 0.7 and 0.8. Due to space limitations, we present only the final results. Results show that setting N = 10, K = 50 and λ f b = 0.6 work best for the values tested. Figure <ref type="figure" target="#fig_2">2</ref> shows the sensitivity of RMW to K. The results for other methods are similar. Figure <ref type="figure" target="#fig_2">2</ref> shows that retrieval performance varies little as the number of expansion terms increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>In this paper, we have explored utilization of Wikipedia in PRF. In this work TREC topics are categorized into three types based on Wikipedia. We propose and study different methods for term selection using pseudo relevance information from Wikipedia entity pages. We evaluated these methods on four TREC collections. The impact of Wikipedia on retrieval performance for different types of queries has been evaluated and compared. Our experimental results show that the query dependent approach can improve over a baseline relevance model.</p><p>This study suggests several interesting research avenues for our future investigations. More investigation is needed to explore the characteristics and possible technique refinements for the broader queries. For ambiguous queries, if the disambiguation process can achieve improved accuracy, the effectiveness of the final retrieval will be improved. For the supervised term selection method, the results obtained are not satisfactory in terms of accuracy. This means that there is still much room for improvement. We are going to explore more features for the learning process. Finally, in this paper, we focused on using Wikipedia as the sole source of PRF information. However, we believe both the initial result from the test collection and Wikipedia have their own advantages for PRF. By combining them together, one may be able to develop an expansion strategy which is robust to the query being degraded by either of the resources.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1</head><label></label><figDesc>exp(A * S(x)+B), where A and B are the parameters, S(x) is the score calculated by the SVM. Each expansion term is represented by a feature vector F (e) = [f1(e), f2(e), f3(e), ..</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Performance on different field weights on the GOV2 collection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Parameter (K) sensitivity of expansion terms over different data sets, N = 10,λ f b = 0.6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Fields of Wikipedia Articles</figDesc><table><row><cell>Field</cell><cell>Description</cell></row><row><cell>Title</cell><cell>Unique identifier for the topic</cell></row><row><cell>Overview</cell><cell>Lead section, summary of the topic</cell></row><row><cell>Content</cell><cell>Grouped by sections</cell></row><row><cell>Category</cell><cell>At least one for each topic</cell></row><row><cell>Appendix</cell><cell>References, notes, external reading etc.</cell></row><row><cell>Links</cell><cell>Cross-article hyperlinks specified by topics</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Numbers of each type of query in the TREC topic sets by automatic query categorization.</figDesc><table><row><cell cols="5">Method AP Robust WT10G GOV2</cell></row><row><cell>EQ</cell><cell>21</cell><cell>58</cell><cell>31</cell><cell>41</cell></row><row><cell>AQ</cell><cell>108</cell><cell>159</cell><cell>54</cell><cell>98</cell></row><row><cell>BQ</cell><cell>21</cell><cell>33</cell><cell>15</cell><cell>11</cell></row><row><cell>All</cell><cell>150</cell><cell>250</cell><cell>100</cell><cell>150</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Overview of TREC collections and topics.</figDesc><table><row><cell></cell><cell>Size</cell><cell># of Doc</cell><cell>Topics</cell></row><row><cell>AP88-90</cell><cell>0.73GB</cell><cell>242,918</cell><cell>51-200</cell></row><row><cell cols="2">Robust2004 1.9GB</cell><cell>528,155</cell><cell>301-450&amp;601-700</cell></row><row><cell>WT10g</cell><cell>11GB</cell><cell>1,692,096</cell><cell>451-550</cell></row><row><cell>GOV2</cell><cell cols="2">427GB 25,205,179</cell><cell>701-850</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Performance comparisons using MAP for all the test topics on test collections. * indicate statistically significant improvements over RMC. We use the paired t-test with significance at p&lt;0.05.</figDesc><table><row><cell></cell><cell>AP</cell><cell cols="3">Robust WT10G GOV2</cell></row><row><cell>QL</cell><cell>0.1428</cell><cell>0.2530</cell><cell>0.1831</cell><cell>0.2967</cell></row><row><cell>RMC</cell><cell>0.1707</cell><cell>0.2823</cell><cell>0.1969</cell><cell>0.3141</cell></row><row><cell>RMW</cell><cell cols="2">0.1622 0.2904*</cell><cell>0.2094*</cell><cell>0.3392*</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Performance comparisons using MAP for entity queries on test collections. * and ** indicate statistically significant improvements over RMC and RMW, respectively. We use the paired t-test with significance at p&lt;0.05.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Robust WT10G</cell><cell>GOV2</cell></row><row><cell>QL</cell><cell>0.2208</cell><cell>0.3156</cell><cell>0.2749</cell><cell>0.3022</cell></row><row><cell>RMC</cell><cell>0.2484</cell><cell>0.3401</cell><cell>0.2458</cell><cell>0.3168</cell></row><row><cell>RMW</cell><cell>0.2335</cell><cell>0.3295</cell><cell>0.2821*</cell><cell>0.3453*</cell></row><row><cell>RE</cell><cell cols="2">0.2494** 0.3580**</cell><cell>0.2897*</cell><cell>0.3889**</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Performance comparisons using MAP for ambiguous queries on test collections. * and ** indicate statistically significant improvements over RMC and RMW, respectively. We use the paired t-test with significance at p&lt;0.05.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Robust WT10G</cell><cell>GOV2</cell></row><row><cell>QL</cell><cell>0.1391</cell><cell>0.2258</cell><cell>0.1801</cell><cell>0.2892</cell></row><row><cell>RMC</cell><cell>0.1520</cell><cell>0.2485</cell><cell>0.1881</cell><cell>0.3101</cell></row><row><cell>RMW</cell><cell>0.1588</cell><cell>0.2619*</cell><cell>0.1868</cell><cell>0.3186*</cell></row><row><cell>RE</cell><cell cols="4">0.1692** 0.2728** 0.2037** 0.3329**</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 :</head><label>8</label><figDesc>Supervised Learning vs Linear Combination. * indicates statistically significant improvements over RMC. We use the paired t-test with significance at p&lt;0.05.</figDesc><table><row><cell></cell><cell>AP</cell><cell cols="3">Robust WT10G GOV2</cell></row><row><cell>SL</cell><cell>0.1640</cell><cell>0.2902</cell><cell>0.2107</cell><cell>0.3237</cell></row><row><cell>SLW</cell><cell>0.1702</cell><cell>0.2921*</cell><cell>0.2145*</cell><cell>0.3298*</cell></row><row><cell cols="3">RMWTF 0.1768* 0.2934*</cell><cell>0.2153*</cell><cell>0.3274*</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 :</head><label>9</label><figDesc>Query Dependent vs traditional Relevance Model. IMP is the number of queries improved by the method over QL. * indicates statistically significant improvements over RMC. We use the paired t-test with significant at p&lt;0.05.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Robust WT10G GOV2</cell></row><row><cell cols="3">RMC MAP 0.1707</cell><cell>0.2823</cell><cell>0.1969</cell><cell>0.3141</cell></row><row><cell></cell><cell>IMP</cell><cell>119</cell><cell>174</cell><cell>47</cell><cell>88</cell></row><row><cell>QD</cell><cell cols="3">MAP 0.1777 0.3002*</cell><cell>0.2194*</cell><cell>0.3348*</cell></row><row><cell></cell><cell>IMP</cell><cell>116</cell><cell>191</cell><cell>67</cell><cell>96</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">ACKNOWLEDGMENTS</head><p>This work is supported by the National Science Foundation of China under Grant No. 60603094, the Major State Basic Research Project of China (973 Program) under Grant No. 2007CB311103 and the National High Technology Research and Development Program of China (863 Program) under Grant No. 2006AA010105. The authors are grateful to the anonymous reviewers for their comments, which have helped improve the quality of the paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic query expansion using SMART: TREC 3</title>
		<author>
			<persName><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Selecting good expansion terms for pseudo-relevance feedback</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR 2008</title>
		<meeting>SIGIR 2008</meeting>
		<imprint>
			<biblScope unit="page" from="243" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">LIBSVM: a library for support vector machines</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/cjlin/libsvm" />
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Personalized query expansion for the web</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Chirita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Firan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Nejdl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of SIGIR 2007</title>
		<meeting>eeding of SIGIR 2007</meeting>
		<imprint>
			<biblScope unit="page" from="7" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A framework for selective query expansion</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cronen-Townsend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM 2004</title>
		<meeting>CIKM 2004</meeting>
		<imprint>
			<biblScope unit="page" from="236" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Retrieval and feedback models for blog feed search</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Elsas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Arguello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR 2008</title>
		<meeting>SIGIR 2008</meeting>
		<imprint>
			<biblScope unit="page" from="347" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">UniNE at TREC 2008: Fact and Opinion Retrieval in the Blogsphere</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fautsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fact discovery in Wikipedia</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fissaha Adafre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Web Intelligence</title>
		<meeting>Web Intelligence</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="177" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Concept-based interactive query expansion</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Golgher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pôssas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ziviani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM 2005</title>
		<meeting>CIKM 2005</meeting>
		<imprint>
			<biblScope unit="page" from="696" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Query difficulty, robustness and selective application of query expansion</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Giambattista Amati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudio</forename><surname>Carpineto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">U</forename><surname>Bordoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECIR 2004</title>
		<meeting>ECIR 2004</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="127" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Query expansion by mining user logs</title>
		<author>
			<persName><forename type="first">J.-Y</forename><forename type="middle">N</forename><surname>Hang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="829" to="839" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Combining fields for query expansion and adaptive query expansion</title>
		<author>
			<persName><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing and Management</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<ptr target="http://www.lemurproject.org/indri/" />
		<title level="m">Indri</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A k-means clustering algorithm</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Hartigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Statistics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The University of Amsterdam at TREC 2008: Blog, Enterprise, and Relevance Feedback</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-J</forename><forename type="middle">H K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Meij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving two-stage ad-hoc retrieval for queries</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR 1998</title>
		<meeting>SIGIR 1998</meeting>
		<imprint>
			<biblScope unit="page" from="250" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Relevance based language models</title>
		<author>
			<persName><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR 2001</title>
		<meeting>SIGIR 2001</meeting>
		<imprint>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A cluster-based resampling method for pseudo-relevance feedback</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR 2008</title>
		<meeting>SIGIR 2008</meeting>
		<imprint>
			<biblScope unit="page" from="235" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improving weak ad-hoc queries using Wikipedia as external corpus</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P R</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S E</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L K</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR 2007</title>
		<meeting>SIGIR 2007</meeting>
		<imprint>
			<biblScope unit="page" from="797" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Latent concept expansion using markov random fields</title>
		<author>
			<persName><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR 2007</title>
		<meeting>SIGIR 2007</meeting>
		<imprint>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Indri at trec 2005: Terabyte track</title>
		<author>
			<persName><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TREC 2004</title>
		<meeting>TREC 2004</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A knowledge-based search engine powered by Wikipedia</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Milne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Nichols</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM 2007</title>
		<meeting>CIKM 2007</meeting>
		<imprint>
			<biblScope unit="page" from="445" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Applied Text Analytics for Blogs</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<pubPlace>University of Amsterdam, Amsterdam</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Probabilities for SV machines</title>
		<author>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in large margin classifiers</title>
		<imprint>
			<biblScope unit="page" from="61" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Simple BM25 extension to multiple weighted fields</title>
		<author>
			<persName><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM 2004</title>
		<meeting>CIKM 2004</meeting>
		<imprint>
			<biblScope unit="page" from="42" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Okapi at TREC-4</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Beaulieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Payne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Text REtrieval Conference (TREC)</title>
		<meeting>the 4th Text REtrieval Conference (TREC)</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ambiguous queries: test collections need more sense</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR 2008</title>
		<meeting>SIGIR 2008</meeting>
		<imprint>
			<biblScope unit="page" from="499" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Regularized estimation of mixture models for robust pseudo-relevance feedback</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR 2006</title>
		<meeting>SIGIR 2006</meeting>
		<imprint>
			<biblScope unit="page" from="162" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<ptr target="http://www.wikipedia.org" />
		<title level="m">Wikipedia</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improving the effectiveness of information retrieval with local context analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="112" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to estimate query difficulty</title>
		<author>
			<persName><forename type="first">E</forename><surname>Yom-Tov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Carmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Darlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR 2005</title>
		<meeting>SIGIR 2005</meeting>
		<imprint>
			<biblScope unit="page" from="512" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Analyzing and accessing Wikipedia as a lexical semantic resource</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Zesch Torsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gurevych</forename><surname>Iryna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biannual Conference of the Society for Computational Linguistics and Language Technology</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="213" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Model-based feedback in the language modeling approach to information retrieval</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM 2001</title>
		<meeting>CIKM 2001</meeting>
		<imprint>
			<biblScope unit="page" from="403" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">UIC at TREC 2006 Blog Track</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Fifteenth Text REtrieval Conference (TREC 2006) Proceedings</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
