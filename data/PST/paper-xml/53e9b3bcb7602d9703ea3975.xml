<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Probabilistic Approach to Reference Resolution in Multimodal User Interfaces</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Joyce</forename><forename type="middle">Y</forename><surname>Chai</surname></persName>
							<email>jchai@cse.msu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Michigan State University East Lansing</orgName>
								<address>
									<postCode>48864</postCode>
									<region>MI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pengyu</forename><surname>Hong</surname></persName>
							<email>hong@stat.harvard.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Statistics Science Center 601</orgName>
								<orgName type="institution">Harvard University Cambridge</orgName>
								<address>
									<postCode>02138</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michelle</forename><forename type="middle">X</forename><surname>Zhou</surname></persName>
							<email>mzhou@us.ibm.com</email>
							<affiliation key="aff2">
								<orgName type="institution">IBM T. J. Watson Research Center Hawthorne</orgName>
								<address>
									<postCode>10532</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Probabilistic Approach to Reference Resolution in Multimodal User Interfaces</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C5799817FAB279D4CD95D2CDC274E06C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T10:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.5.2 (User Interfaces): Theory and method</term>
					<term>Natural language Multimodal user interfaces</term>
					<term>reference resolution</term>
					<term>graph matching Speech: This house costs 250</term>
					<term>000 dollars. Graphics: Highlight the targeted house Speech: How large? Speech: It has 2200 square feet. Graphics: The previous house stays highlighted Point</term>
					<term>and Circle Speech: Here is the comparison chart. Graphics: Highlight three houses and show a chart</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multimodal user interfaces allow users to interact with computers through multiple modalities, such as speech, gesture, and gaze. To be effective, multimodal user interfaces must correctly identify all objects which users refer to in their inputs. To systematically resolve different types of references, we have developed a probabilistic approach that uses a graph-matching algorithm. Our approach identifies the most probable referents by optimizing the satisfaction of semantic, temporal, and contextual constraints simultaneously. Our preliminary user study results indicate that our approach can successfully resolve a wide variety of referring expressions, ranging from simple to complex and from precise to ambiguous ones.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Multimodal user interfaces allow users to interact with computers through multiple modalities such as speech, gesture, and gaze. Since the first appearance of the "Put-That-There" system <ref type="bibr" target="#b0">[1]</ref>, a number of multimodal systems have been built, among which there are systems that combine speech, pointing <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b20">21]</ref>, and gaze <ref type="bibr" target="#b12">[13]</ref>, systems that integrate speech with pen inputs (e.g., drawn graphics) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23]</ref>, systems that combine multimodal inputs and outputs <ref type="bibr" target="#b1">[2]</ref>, systems in mobile environments <ref type="bibr" target="#b18">[19]</ref>, and systems that engage users in an intelligent conversation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20]</ref>. Studies have shown that multimodal interfaces enable users to interact with computers naturally and effectively <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18]</ref>. Inspired by the earlier work, we are building an infrastructure called Responsive Information Architect (RIA) to facilitate a multimodal human-computer conversation <ref type="bibr" target="#b2">[3]</ref>. Users can interact with RIA through multiple modalities, such as speech and gesture. RIA is able to understand user inputs and automatically generate multimedia responses via speech and graphics <ref type="bibr" target="#b24">[25]</ref>. Currently, RIA is embodied in a testbed, called Real Hunter TM , a real-estate application for helping users to find residential properties.</p><p>A key element in understanding user multimodal inputs is known as reference resolution, which is a process that finds the most proper referents to referring expressions. Here a referring expression is a phrase that is given by a user in her inputs (most likely in speech inputs) to refer to a specific entity or entities. A referent is an entity (e.g., a specific object) to which the user refers. Suppose that a user points to "House 6" on the screen and says "how much is this one". In this case, reference resolution is to infer that the referent "House 6" should be assigned to the referring expression "this one". As described later, in multimodal conversation systems that support rich user interaction as RIA does (e.g., providing both verbal and visual responses), users may refer to their interested objects in many different ways. Therefore, developing a systematic approach to reference resolution in such an environment is challenging.</p><p>To systematically resolve different types of references, we have developed a probabilistic approach to reference resolution using a graph-matching algorithm. Our approach finds the most probable referents for referring expressions by optimizing the satisfaction of a number of constraints, including semantic, temporal and contextual constraints. In this paper, we first describe the challenges associated with reference resolution and some of the related work. We then present our probabilistic approach to reference resolution. Finally, we discuss our evaluation results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">REFERENCE RESOLUTION</head><p>In a multimodal conversation, rich interaction channels allow users to refer to their interested objects in various ways. User references can be simple, precise, complex, or ambiguous. Next we use a concrete example (Table <ref type="table" target="#tab_0">1</ref>) to illustrate several common referring patterns. In this example, Real Hunter TM shows a collection of objects (e.g., houses, a train station, etc.) Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. IUI'04, January <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr">2004</ref>, Madeira, Funchal, Portugal. Copyright 2004 ACM 1-58113-815-6/04/0001â€¦$5.00 on the map of three towns Chappaqua, Ossining, and Pleasantville.</p><p>First in U1, the user's gesture input results in a position near a house icon and a train station icon 1 . From the gesture alone, the system does not know whether the user points to the house, the train station, or the town of Ossining 2 . Continuing in U2, the user's utterance does not use any referring expression 3 . Using this input alone, it is hard for the system to identify which object the user is talking about. In U3, there are two speech referring expressions ("this" and "these two houses") and two gestures (pointing and circling). If only using the temporal ordering information, the system may not be able to decide between two cases: 1) aligning the pointing gesture with "this" and the circling gesture with "these houses"; or 2) aligning both gestures with "these two houses".</p><p>Resolving the references illustrated in the above example requires combining information from the multimodal inputs and from the interaction contexts, such as the conversation history, the system visual feedback, and the domain knowledge. For example, using domain knowledge, our system is able to infer that in U1 the user is actually referring to the house, since both the train station and the town of Ossining do not have the price attribute. Although no explicit referring expression is given in the speech utterance in U2, combining the conversation context and the visual properties, our system can infer that the user is referring to the size of the highlighted house on the screen.</p><p>Reference resolution becomes more complicated when processing inputs that involve multiple referring expressions and multiple gestures (e.g., U3). Given a referring expression, it may be accompanied by a sequence of different gestures (e.g., pointing and circling in Figure <ref type="figure" target="#fig_0">1a-b</ref>), or by a sequence of similar gestures (e.g., pointing in Figure <ref type="figure" target="#fig_0">1c</ref>). Figure <ref type="figure" target="#fig_0">1</ref> shows three possible variations of gesture inputs accompanying the speech input in U3. Depending on the gesture recognition results, there may be different alignments between the gestures and the referring expressions in each case. In Figure <ref type="figure" target="#fig_0">1</ref>(a), using the temporal information, one possible alignment is to pair the pointing gesture with "this" and the circling gesture with "these two houses". However, this alignment may be incorrect, depending on the number of houses selected by the circling gesture. If the user circles two houses, it is most likely that "this" refers to the house selected by the pointing gesture and "these two houses" refers to the two houses selected by the circling gesture. On the other hand, if the circling gesture only results in one object, the system may infer that "this" most likely refers to the highlighted house on the screen (which is also the focus of attention from the previous interaction), and "these two houses" refers to the two houses se- 1 User gesture inputs are often imprecise especially when a touch screen is used, where a human finger is normally larger than the objects shown on the screen. 2 The house icon and the train station icon are on top of a graphic entity, in this case a region representing the town of Ossining. 3 More precisely, the user did not use any referring expression here (i.e., similar to zero anaphora, but "is it" is what is missing).</p><p>lected by the pointing and the circling gestures, respectively.</p><p>Similarly, in Figure <ref type="figure" target="#fig_0">1</ref>(b), since the pointing and the circling gestures are temporally closer to the expression "these two houses", it is more likely for both gestures to be aligned with "these two houses". In Figure <ref type="figure" target="#fig_0">1</ref>(c), three consecutive pointing  gestures are used. Although all three gestures are temporally closer to the expression "these two houses", this expression specifies that only "two" objects be the potential referents. Thus it is more likely that the house selected by the first pointing provides the referent to "this", and the following two pointing gestures supply the referents to "these two houses". To resolve all these complex references described above, we need to consider the temporal relations between the referring expressions and the gestures, the semantic constraints specified by the referring expressions, and the contextual constraints from the prior conversation. Any subtle variations in any of the constraints, including the temporal ordering, the semantic compatibility, and the gesture recognition results will lead to different interpretations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">RELATED WORK</head><p>Previous work on multimodal reference resolution includes the use of a focus space model <ref type="bibr" target="#b14">[15]</ref>, the use of the centering framework <ref type="bibr" target="#b23">[24]</ref>, and the use of contextual factors <ref type="bibr" target="#b7">[8]</ref>. Approaches to multimodal integration <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, although focusing on a different problem, provide effective solutions to reference resolution by combining inputs together from different modalities. For example, the unification-based multimodal fusion approach can identify referents to referring expressions by unifying feature structures generated from the speech utterances and from the gestures through a multimodal grammar <ref type="bibr" target="#b9">[10]</ref>. The unification mechanism enforces the semantic compatibility between different inputs. In addition, it also applies temporal constraints, which are specified by a set of pre-defined integration rules <ref type="bibr" target="#b16">[17]</ref>. Using this approach to accommodate various situations such as those described in Figure <ref type="figure" target="#fig_0">1</ref> will require adding different rules to cope with each situation. If a specific user referring behavior did not exactly match any existing integration rules (e.g., temporal relations), the unification would fail and therefore references would not be resolved.</p><p>A recent study by Kehler reported that the interpretation of referring expressions can be achieved with a very high accuracy using a visual context (e.g., visual focus) with a simple set of rules <ref type="bibr" target="#b11">[12]</ref>. In our experiments, we found that those rules are very effective in processing inputs with a single referring expression accompanied by precise gestures. Since those rules assume that all objects can be deterministically selected by gestures, this approach does not support ambiguous gestures as described in our case.</p><p>Our approach is inspired by both unification-based and context-based approaches to multimodal interpretation. On the one hand, as in unification-based multimodal integration <ref type="bibr" target="#b9">[10]</ref>, our approach considers both semantic and temporal constraints. On the other hand, our approach considers the constraints from the conversation context as in a context-based approach. In particular, our approach focuses on a new aspect of reference resolution, which involves finding the most probable referents to all unknown references using information from multiple sources. Unlike earlier work, our approach always provides a solution that maximizes the overall satisfaction of semantic, temporal, and contextual constraints. To achieve this goal, we developed a probabilistic approach using a graph-matching algorithm. Specifically, we represent all information gathered from multiple input modalities and the contexts as attributed relational graphs (ARGs) <ref type="bibr" target="#b21">[22]</ref>, and model reference resolution as a constrained probabilistic graph-matching problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">GRAPH-BASED INFORMATION REPRESENTATION</head><p>Reference resolution relies on not only the properties of referring expressions and potential referents, but also on their inter-relations. ARG is an ideal representation for such properties and relations. In particular, we use three ARGs to represent the information collected from the speech input, the gesture input, and the conversation context. Next we introduce our ARG representation, we then describe how to automatically generate the three ARGs.</p><p>An ARG consists of a set of nodes that are connected by a set of edges. Each node represents an entity, which in our case is either a referring expression to be resolved or a potential referent. Each node is associated with a feature vector encoding the properties of the corresponding entity. Each edge represents a set of relations between two entities, and is also associated with a feature vector encoding the properties of such relations.</p><p>Currently, the feature vector of a node contains the semantic and temporal information. The feature vector of an edge describes the temporal relation and the semantic type relation between a pair of entities. In the future, more relations (e.g., spatial relation)) can be easily added. A temporal relation indicates the temporal order between two related entities during an interaction, which may be one of the following:</p><p>â€¢ Preceding: Node A precedes Node B if the entity represented by Node A is mentioned right before the entity represented by Node B in a specific modality. For example, "this" precedes "these two houses" in the speech input of U3 in Table <ref type="table" target="#tab_0">1</ref>. â€¢ Concurrent: Node A is concurrent with Node B if the entities represented by them are referred or mentioned simultaneously in a specific modality. For example, a circling gesture may select a group of objects. All selected objects are considered concurrent with each other. â€¢ Non-concurrent: Node A is non-concurrent with Node B if their corresponding objects/references cannot be referred/mentioned simultaneously and the preceding relation does not hold between them.</p><p>â€¢ Unknown: The temporal order between two entities is unknown, it may take the value of any of the above.</p><p>A semantic type relation indicates whether two related entities share the same semantic type. When a semantic type relation cannot be identified from the entities themselves or from the contexts those entities are referred to, it is set to "unknown".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Construction of Speech ARG</head><p>A speech ARG captures the information about referring expressions that occur in a speech input. Figure <ref type="figure">2</ref> shows the speech ARG created by processing the speech input in U3. In a speech ARG, each node represents a referring expression and each edge represents a semantic relation and a temporal relation between the referring expressions. To create this graph, our system first uses IBM ViaVoice TM to transcribe a speech utterance into text, and then applies a grammar-based parser to identify key phrases and their relations.</p><p>For example, from the speech input in U3, our system identifies three key phrases "compare", "this", and "these two houses". Among these three key phrases, two referring expressions "this" and "these two houses" are identified. Accordingly, two nodes are created for these two referring expressions as shown in Figure <ref type="figure">2</ref>. From each referring expression, our system then identifies a set of pertinent semantic features, such as the semantic type of the potential referents and the number of potential referents <ref type="foot" target="#foot_0">4</ref> . Furthermore, we use ViaVoice TM to extract the time stamps of each word when it is uttered, and derive the time duration for each referring expression.</p><p>Our system extracts the following semantic features and the temporal features of each expression:</p><p>â€¢ The identifier of the referent. The unique identity of the potential referent. For example, the proper noun "Ossining" specifies the town of Ossining. â€¢ The semantic type of the potential referents indicated by the expression. For example, the semantic type of the referring expression "this house" is house.</p><p>â€¢ The number of potential referents. For example, a singular noun phrase refers to one object. A plural noun phrase refers to multiple objects. A phrase like "three houses" provides the exact number of referents (i.e., 3). â€¢ Type dependent features. Any features, such as size and price, are extracted from the referring expression. â€¢ The time stamp that indicates when a referring expression is uttered. â€¢ The syntactic categories of the referring expressions (e.g., a demonstrative vs. a pronoun). This information helps the system to draw correlations between the type of references used and the status of objects referred.</p><p>As shown in Figure <ref type="figure">2</ref>, node 2 (corresponding to "these two houses") specifies that the potential referent should be a house object (Base: house), and the number of potential referents should be two (Number: 2).</p><p>Each edge in a speech ARG captures the temporal relation and the semantic type relation between two referring expressions. The temporal relation is decided based on the time when the two expressions are uttered. The semantic type relation is either directly derived from the expressions themselves, or inferred from the entire speech input where these expressions are uttered. For example, in U3 (Table <ref type="table" target="#tab_0">1</ref>), the user does not provide a specific semantic type for the potential referent in the expression "this". The whole speech utterance shows that the user is asking for a comparison of multiple objects. Based on the domain knowledge that the comparable objects most likely share the same semantic type, our system is able to infer that the semantic type relation between "this" and "these two houses" be the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Construction of Gesture ARG</head><p>A gesture ARG encodes the information about the objects selected by gesture inputs. Currently, we focus on two types of deictic gestures, pointing and circling, both of which select objects from a graphic display. Gesture inputs may be inaccurate especially when a touch screen is used, where displayed objects are often too small for a human finger to pinpoint at. Therefore, our gesture recognition module assigns a probability to each object that is likely to be selected by a gesture.</p><p>For pointing gestures, given a selection point (x, y) on the screen, our system uses the following method to select a set of potential objects and calculate their selection probabilities. If (x, y) intersects with any polygons that constituent an object on the screen, then the selection probability of this object is 1.0. Note that the point (x, y) may intersect with multiple objects simultaneously, especially when objects overlap with each other (e.g., two nearby house icons may overlap). Otherwise, we set a circular effective region, which centers at (x, y) and has the radius r. The radius r is equal to the minimum dimension of the bounding rectangles of the objects on the screen. If the object intersects the circle, the selection probability of the object is e -2d/r , where d is the minimal distance between the boundary points of the object and (x, y).</p><p>To recognize a circling gesture that selects one object or a group of objects, our system traces the trajectory of the user's mouse movements during interaction. We consider a trajectory a circle if it intersects with itself or its two end points are close enough. If an object icon falls inside the circle, its selection probability is 1.0; the probability is 0.0 if the object falls outside of the circle. If an object icon intersects with the circle, its selection probability is equal to the ratio of its area inside the circle to its whole area.</p><p>Using the results produced by the gesture recognition module, our system builds a gesture ARG including all gestures that occur during one interaction <ref type="foot" target="#foot_1">5</ref> . At each interaction, one input may contain a sequence of gestures: g 1 , g 2 , â€¦, g K . For example, the gesture input of U3 in Table <ref type="table" target="#tab_0">1</ref> consists of a pointing gesture and a circling gesture. For each gesture g i , our system creates a sub-graph. In each sub-graph, each node, known as a gesture node, represents an object selected by this gesture.</p><p>The feature vector of a gesture node contains the following information pertinent to the object represented: the identifier, the semantic type, the attributes (e.g., a house object has attributes of price, size, etc.), the time stamp when the object is selected (relative to the system start time), and its selection probability. Each edge in a sub-graph represents the semantic type relation and the temporal relation between two nodes. The semantic type relations can be easily identified because the identities of the objects are known. The temporal relations between two nodes within a sub-graph are labeled as "concurrent", since within a single gesture all objects are considered being selected simultaneously. As shown in Figure <ref type="figure">3</ref>, there are two sub-graphs generated for the pointing gesture and the circling gesture, respectively. We use the thicker edges to represent the relations between the nodes within the sub-graphs.</p><p>By connecting sub-graphs together, our system then creates the final gesture ARG. To connect the sub-graphs, new edges are added based on the temporal order between the gestural events. These new edges link the nodes of the subgraph for one gesture g k to the nodes of the sub-graph for the next gesture g k+1 . The semantic relation of the new edges can also be easily identified. The temporal relations of the new edges are set as "preceding". As shown in Figure <ref type="figure">3</ref>, the sub-graphs of the pointing gesture and the circling gesture are connected to form the final gesture ARG. Here the new edges are depicted using thinner lines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Construction of History ARG</head><p>To exploit the conversation context, we use a history ARG to capture the information about objects that are in focus during the last interaction. This provides another source for finding the potential referents. Specifically, a history ARG consists of a list of objects that are in focus during the most recent interaction. Each node, called a history node, contains information related to the object in focus, including its identifier and its semantic type. Since the objects in the history can be referred by a user in an arbitrary order, the temporal relations between any two history nodes are labeled "unknown". Furthermore, the semantic relations are decided based on the known identities of the objects. Figure <ref type="figure">4</ref> shows the history ARG created after U3 (Table <ref type="table" target="#tab_0">1</ref>) is processed. This history ARG will be used to process the next input (U4). Since there are three houses in focus in U3, there are three nodes in the ARG. All three nodes are connected by the semantic type relations and the temporal relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">GRAPH-MATCHING PROCESS</head><p>For each multimodal user input, we create three graphs: a speech ARG, a gesture ARG, and a history ARG as described above. Given these three ARGs, we formulate reference resolution as a probabilistic graph-matching problem under the following assumptions:</p><p>â€¢ A speech utterance may include multiple referring expressions. Some of them may refer to the objects selected by the gesture. Some of them may refer to the objects mentioned in the prior conversation. â€¢ A referring expression may refer to an object that is not in the gesture ARG or in the history ARG. Some objects may be distinguished from the others by their unique features. For example, if there is only one red house shown on the graphic display, the user may use the phrase "the red house" to precisely refer to the intended target. Hence, our system also maintains an object list, called secondary object list, which records the objects that are not in the gesture ARG or in the history ARG, but are currently visible on the graphic display. â€¢ A referring expression may refer to a group of objects (e.g., "these houses"). In such cases, we assume that referents should all come from one single information source, either from the gesture ARG, the history ARG, or the secondary object list mentioned above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Referring Graph and Referent Graph</head><p>Under the above assumptions, we arrange the three ARGs into two graphs: a referring graph and a referent graph. A referring graph is basically the speech ARG. It contains a collection of referring expressions to be resolved. A referent graph is the aggregation of a gesture ARG and a history ARG.</p><p>To build a referent graph, we add new edges to connect the gesture ARG and the history ARG. These edges connect every gesture node to every history node. The semantic type relations of these new edges can be easily inferred using the already known object identities. Based on our assumption that the objects from the history ARG and the objects from the gesture ARG cannot be referred simultaneously by one referring expression, the temporal relations of the new edges are set as "non-concurrent". If the size of the secondary object list is small, all objects on this list are added to the referent graph in the following manner. First, for each object in the list, a node is created and known as a secondary node. Then, edges are added to connect each secondary node to every node already in the referent graph. We set their semantic type based on their identities, and set their temporal relations as "non-concurrent".</p><p>Given a referring graph and a referent graph, reference resolution is a graph-matching problem, which aims to find the best match between the referent graph and the referring graph, while optimizing the satisfaction of various temporal, semantic, and contextual constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Graph-matching Algorithm</head><p>First let us define notations.</p><p>â€¢ The referent ARG: G r = âŒ©{a x }, {r xy }âŒª, where {a x } is the node list and {r xy } is the edge list. The edge r xy connects nodes a x and a y . The nodes of G r are called referent nodes. â€¢ The referring ARG: G s = âŒ©{Î± m }, {Î³ mn }âŒª, where {Î± m } is the node list and {Î³ mn } is the edge list. The edge Î³ mn connects nodes Î± m and Î± n . The nodes of G s are called referring nodes.</p><p>Our approach finds the best match between the referent graph and the referring graph by maximizing the following term with respect to P(a x ,Î± m ):</p><formula xml:id="formula_0">( , )<label>( , ) ( , ) ( , ) ( , ) ( , )</label></formula><formula xml:id="formula_1">r s x m x m x m x m y n x y m n x y m n Q G G P a a P a P a r Î± Î¶ Î± Î± Î± Ïˆ Î³ = + âˆ‘ âˆ‘ âˆ‘ âˆ‘ âˆ‘ âˆ‘<label>(1)</label></formula><p>Here P(a x ,Î± m ) is the matching probability between a referent node a x and a referring node Î± m . Here Î£ x P(a x ,Î± m ) = 1, if the speech referring node Î± m refers to a single object.</p><p>The term Q(G c ,G s ) measures the degree of the overall match between the referent graph and the referring graph. This term not only considers the similarities between the nodes in both graphs, but also considers the similarities between the edges in both graphs. The function Î¶(a x ,Î± m ) measures the similarity between a referent node a x and a referring node Î± m . Î¶(a x ,Î± m ) is defined based on the node properties of both a x and Î± m . The function Ïˆ(r xy ,Î³ mn ) measures the similarity between the edges r xy and Î³ mn . The domain knowledge is incorporated in the design of Î¶(a x ,Î± m ) and Ïˆ(r xy ,Î³ mn ). Currently these functions are empirically determined through a series of regression tests. In the future, these functions may be automatically learned from our experimental data.</p><p>We have adopted the graduated assignment algorithm <ref type="bibr" target="#b4">[5]</ref> to maximize Q(G r ,G s ) in <ref type="bibr" target="#b0">(1)</ref>. Our algorithm initializes P(a x ,Î± m ) using the selection probability if a x comes from a gesture ARG and a pre-defined probability if a x comes from a history ARG. It then iteratively updates the values of P(a x ,Î± m ) until the algorithm converges. When the algorithm converges, P(a x ,Î± m ) is the matching probability between a referent node a x and a referring node Î± m . Our algorithm will always find a most probable match between a referent node and a referring node. Based on the value of P(a x ,Î± m ), our system decides whether a referent is found for a given referring expression. Currently, if P(a x , Î± m ) is greater than a threshold (e.g., 0.8), our system considers that referent a x is found for the referring expression Î± m . On the other hand, there is an ambiguity if there are two or more nodes matching Î± m and Î± m is supposed to refer to a single object.</p><p>In this case, since there is no sufficient evidence from either inputs or the context for our algorithm to resolve these ambiguities, our system will ask the user to further clarify the object of his/her interest.</p><p>Note that the number of referent nodes may not match the number of the referring nodes. In other words, we may not find a match for a referent node or a referring node. To deal with this problem, a null node, which is a place holder and does not physically exist, is added to both a referent graph and a referring graph before the matching starts. Null nodes are the slack variables in the graduated assignment algorithm and provide matching destinations for unmatched referent or referring nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">SYSTEM IMPLEMENTATION</head><p>We have implemented the GUI and the gesture recognition component in C++ using Microsoft Visual C++ 6.0. The grammar-based parser for understanding speech utterances was implemented in Java, so is our graph-matching algorithm. Different components run as synchronized threads and communicate with each other using the TCP/IP protocol. The system runs in real-time under Windows 2000 with 1.1 GHz CPU and 512 MB of RAM..</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">EVALUATION</head><p>To evaluate our approach, we have conducted a preliminary user study. Eight subjects participated in our study. The subjects were asked to interact with the system using both speech and gestures to accomplish three tasks. The tasks were designed so that the subjects could explore different house or town information. For example, one task was finding the least expensive house in the most populated town. The voice was trained for each subject to minimize recognition errors.  <ref type="table" target="#tab_1">2</ref>). This is consistent with earlier findings <ref type="bibr" target="#b11">[12]</ref>. Previous approaches work well with these simple references. However, in this study, we also found that 14.1% of the inputs were complex, consisting of multiple referring expressions from the speech utterances and multiple gestures (S3 in Table <ref type="table" target="#tab_1">2</ref>). Furthermore, 12.2% of gesture inputs were ambiguous where users did not precisely indicate the objects of their interest (the ambiguous data is not shown in Table <ref type="table" target="#tab_1">2</ref>). These behaviors are not observed in <ref type="bibr" target="#b11">[12]</ref> partly due to the different interface design and the increased complexity of tasks. Table <ref type="table" target="#tab_2">3</ref> summarizes the performance of our approach in this study. Totally 109 out of 156 referring expressions are correctly resolved (about 70%). This overall performance is largely influenced by the poor speech recognition rate. Out of 156 referring expressions, only 94 are correctly recognized (60%). However, the results have confirmed the earlier find-ings that fusing inputs from multiple modalities together can compensate for the recognition errors through mutual disambiguation <ref type="bibr" target="#b17">[18]</ref>. Among 62 referring expressions that are incorrectly recognized, 26 of them are correctly assigned referents by our algorithm.</p><p>Among the referring expressions that are correctly recognized, the accuracy of our approach is 88.3% (i.e., 83/94). The errors are mainly from the following sources: 1) vocabularies used by the users are not covered by our grammars (8 out of 11 cases). For example, "area" is not in our vocabulary. Thus any additional constraints expressed related to "area" is not captured. Therefore, our system cannot identify whether a house or a town is the referent when the user utters "this area". 2) The speech and gesture inputs are unsynchronized within our turn-determination time threshold (i.e., 2 seconds). In one case, the gesture input occurred more than 2 seconds before the speech input. 3) In two other cases, spatial relations (as in "the house just close to the red one") and superlatives (as in "the most expensive house") were used but not interpreted correctly.</p><p>In this study, we focus on investigating ambiguous gesture inputs (similar to those handled by the unification-based approach <ref type="bibr" target="#b8">[9]</ref>). No ambiguities from speech inputs were examined. Out of 19 ambiguous gestures, the references in 11 cases were correctly disambiguated. In seven cases, since there was no information from either the speech input or the conversation context to disambiguate the object of interest, the system asked the user for further clarification. Furthermore, when handling complex inputs, once the spoken expressions were correctly recognized, our approach achieved an accuracy rate of 92.9%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">CONCLUSION AND FUTURE WORK</head><p>This paper describes a novel technique that uses a probabilistic graph-matching algorithm to systematically resolve a wide variety of references during a human-computer multimodal conversation. Our approach incorporates temporal, semantic, and contextual constraints together in one framework and derives the most probable interpretation that best satisfies all these constraints. Preliminary user study results have shown that our approach can successfully re- solve referring expressions, no matter simple or complex, precise or ambiguous, regardless of their referential forms.</p><p>Since our current implementation only considers ambiguities from gesture inputs, we plan to extend it to handle ambiguous speech utterances in the near future. Other plans for future work include the identification of speech disfluencies and gesture disfluencies using multimodal information. In our study, we have observed speech disfluencies such as speech repair and gesture disfluencies such as repetition (i.e., repetitively point to an object). Correctly identifying all these disfluencies will further improve reference resolution in a practical human-machine conversation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Three variations of U3. The timing of the point gesture is denoted byâ™¦ â™¦ â™¦ â™¦ and the timing of the circle gesture is denoted by ÎŸ ÎŸ ÎŸ ÎŸ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 . A conversation fragment between a user and Real Hunter TM . U1, U2, and U3 are the user inputs. R1, R2, and R3 are the responses from Real Hunter TM .</head><label>1</label><figDesc></figDesc><table><row><cell cols="2">Speech input: Compare this with these two houses.</cell></row><row><cell cols="2">Gesture input: â€¦â€¦...â€¦â€¦â™¦â€¦â€¦.â€¦â€¦. â€¦ â€¦â€¦â€¦</cell></row><row><cell>(a)</cell><cell>Time</cell></row><row><cell>Speech input: Compare this with</cell><cell>these two houses.</cell></row><row><cell cols="2">Gesture input: â€¦â€¦...â€¦â€¦â€¦â€¦â€¦â€¦..â™¦â€¦â€¦ â€¦â€¦</cell></row><row><cell>(b)</cell><cell>Time</cell></row><row><cell cols="2">Speech input: Compare this with these two houses.</cell></row><row><cell cols="2">Gesture input: â€¦â€¦...â€¦â€¦â€¦â€¦.. ..â™¦â€¦.â€¦.â™¦â€¦...â€¦â™¦â€¦â€¦</cell></row><row><cell>(c)</cell><cell>Time</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>contains a summary of the referring patterns observed in this study. The columns indicate whether there is no gesture, one gesture, or multiple gestures in one input. The rows indicate whether there is no referring expression, one referring expression, or multiple referring expressions in one speech utterance. As shown in the table, the majority of observed user references involve only one referring expression and one gesture (e.g., [S2, G2] in Table</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 : Overall evaluation results. The columns indicate the number of referring expressions that were correctly or incorrectly recognized by the speech recognizer. The rows indicate the number of referring expressions whose refer- ents are correctly or incorrectly identified by the graph-matching algorithm.</head><label>3</label><figDesc></figDesc><table><row><cell>S1: No referring expression S2: One referring expression S3: Multiple refer-ring expression</cell><cell>G1: No gesture 2 12 1</cell><cell>G2: One gesture 1 117 6</cell><cell>G3: Multiple gestures 0 2 15</cell><cell>Total numbers 3 131 22</cell><cell>Identified incorrectly Identified correctly Identified correctly Identified incorrectly</cell><cell>11 83 Recognized correctly 11 83 Recognized correctly</cell><cell>36 26 Recognized Incorrectly 36 26 Recognized Incorrectly</cell><cell>47 109 Total 47 109 Total</cell></row><row><cell>Total numbers</cell><cell>15</cell><cell>125</cell><cell>17</cell><cell>156</cell><cell>Total Total</cell><cell>94 94</cell><cell>62 62</cell><cell>156 156</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 . Referring patterns from the user study.</head><label>2</label><figDesc></figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0"><p>We use a set of rules to extract semantic features. For example, one rule "Num(X) Noun(Y) -&gt; BASE: Y, NUMBER: X" indicates that if a number is followed by a noun, then this number and the base form of the noun will be extracted as the number feature (NUMBER) and the semantic type feature (BASE).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1"><p>Currently, we use the inactivity (i.e., 2 seconds with no input from either speech or gesture) as the boundary to delimit an interaction turn.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>The authors would like to thank Keith Houck and the other colleagues in the Intelligent Multimedia Interaction Group at IBM T. J. Watson Research Center for their support. The authors would also like to thank Candy Sidner at MERL and the anonymous reviewers for their valuable comments on previous drafts of this paper.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Put that there: Voice and Gesture at the Graphics Interface</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Bolt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="262" to="270" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Embodiment in Conversational Interfaces: Rea</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cassell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bickmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Billinghurst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Vilhjalmsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CHI&apos;99 Conference</title>
		<meeting>the CHI&apos;99 Conference<address><addrLine>Pittsburgh, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="520" to="527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Context-based Multimodal Interpretation in Conversational Systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Houck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on Multimodal Interfaces</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quickset: Multimodal Interaction for Distributed Applications</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcgee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oviatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pittman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM Multimedia</title>
		<meeting>ACM Multimedia</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="31" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A graduated assignment algorithm for graph-matching</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rangarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attention, intention, and the structure of discourse</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Grosz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sidner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="175" to="204" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">AdApt -a Multimodal Conversational Dialogue System in an Apartment Domain</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gustafson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Beskow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Boye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Edlund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Granstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>House</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wiren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 6 th International Conference on Spoken Language Processing (ICSLP)</title>
		<meeting>6 th International Conference on Spoken Language Processing (ICSLP)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic Referent Resolution of Deictic and Anaphoric Expressions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Huls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Classen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="79" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unification-based Multimodal Integration</title>
		<author>
			<persName><forename type="first">M</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcgee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oviatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pittman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL&apos;97</title>
		<meeting>ACL&apos;97</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unification-based Multimodal parsing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Johnston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING-ACL&apos;98</title>
		<meeting>COLING-ACL&apos;98</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Finite-state multimodal parsing and understanding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bangalore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. COLING&apos;00</title>
		<meeting>COLING&apos;00</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cognitive Status and Form of Reference in Multimodal Human-Computer Interaction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI&apos;01</title>
		<meeting>AAAI&apos;01</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="685" to="689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Integrating Simultaneous Input from Speech, Gaze, and Hand Gestures</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Koons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Sparrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Thorisson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Multimedia Interfaces</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Maybury</surname></persName>
		</editor>
		<meeting><address><addrLine>Ed; Menlo Park, CA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Intelligent Multimedia Interface Technology</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Shapiro</surname></persName>
		</author>
		<editor>Intelligent User Interfaces, J. Sullivan &amp; S. Tyler</editor>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>ACM</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Natural Language with Integrated Deictic and Graphic Gestures. Intelligent User Interfaces</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Thielman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dobes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Haller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Shapiro</surname></persName>
		</author>
		<editor>M. Maybury and W. Wahlster</editor>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="38" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multimodal interfaces for dynamic interactive maps</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Oviatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Human Factors in Computing Systems: CHI &apos;96</title>
		<meeting>Conference on Human Factors in Computing Systems: CHI &apos;96</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="95" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Integration and Synchronization of Input Modes during Multimodal Human-Computer Interaction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Oviatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Deangeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Human Factors in Computing Systems: CHI &apos;97</title>
		<meeting>Conference on Human Factors in Computing Systems: CHI &apos;97</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mutual Disambiguation of Recognition Errors in a Multimodal Architecture</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Oviatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Human Factors in Computing Systems: CHI &apos;99</title>
		<meeting>Conference on Human Factors in Computing Systems: CHI &apos;99</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multimodal System Processing in Mobile Environments</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Oviatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Annual ACM Symposium on User Interface Software Technology (UIST&apos;2000)</title>
		<meeting>the Thirteenth Annual ACM Symposium on User Interface Software Technology (UIST&apos;2000)<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<biblScope unit="page" from="21" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The commandtalk spoken dialog system</title>
		<author>
			<persName><forename type="first">A</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dowding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Gawron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">O</forename><surname>Bratt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL&apos;99</title>
		<meeting>ACL&apos;99</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="183" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">ALFRESCO: Enjoying the combination of natural language processing and hypermedia for information exploration</title>
		<author>
			<persName><forename type="first">Oliviero</forename><surname>Stock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Multimedia Interfaces</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Maybury</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="197" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Error-correcting isomorphism of attributed relational graphs for pattern analysis</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Sys., Man and Cyb</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="757" to="768" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">User and Discourse Models for Multimodal Communication</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wahlster</surname></persName>
		</author>
		<editor>M. Maybury and W. Wahlster</editor>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Intelligent User Interfaces</publisher>
			<biblScope unit="page" from="359" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multimodal Interaction for Information Access: Exploiting Cohesion</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zancanaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Strapparava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="439" to="464" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automated authoring of coherent multimedia discourse for conversation systems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM MM&apos;01</title>
		<meeting>ACM MM&apos;01</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="555" to="559" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
