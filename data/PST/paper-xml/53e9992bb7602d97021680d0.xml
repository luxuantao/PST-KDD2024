<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TEMPO AND BEAT ESTIMATION OF MUSICAL SIGNALS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Miguel</forename><surname>Alonso</surname></persName>
							<email>malonso@tsi.enst.fr</email>
							<affiliation key="aff0">
								<orgName type="department">ENST-GET</orgName>
								<address>
									<addrLine>Département TSI 46, rue Barrault</addrLine>
									<postCode>75634 cedex 13</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bertrand</forename><surname>David</surname></persName>
							<email>bedavid@tsi.enst.fr</email>
							<affiliation key="aff0">
								<orgName type="department">ENST-GET</orgName>
								<address>
									<addrLine>Département TSI 46, rue Barrault</addrLine>
									<postCode>75634 cedex 13</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gaël</forename><surname>Richard</surname></persName>
							<email>grichard@tsi.enst.fr</email>
							<affiliation key="aff0">
								<orgName type="department">ENST-GET</orgName>
								<address>
									<addrLine>Département TSI 46, rue Barrault</addrLine>
									<postCode>75634 cedex 13</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TEMPO AND BEAT ESTIMATION OF MUSICAL SIGNALS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A235AA066217F32AAF2300C36FC62352</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>beat</term>
					<term>tempo</term>
					<term>onset detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Tempo estimation is fundamental in automatic music processing and in many multimedia applications. This paper presents an automatic tempo tracking system that processes audio recordings and determines the beats per minute and temporal beat location. The concept of spectral energy flux is defined and leads to an efficient note onset detector. The algorithm involves three stages: a frontend analysis that efficiently extracts onsets, a periodicity detection block and the temporal estimation of beat locations. The performance of the proposed method is evaluated using a large database of 489 excerpts from several musical genres. The global recognition rate is 89.7 %. Results are discussed and compared to other tempo estimation systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>It is very difficult to understand western music without perceiving beats, since a beat is a fundamental unit of the temporal structure of music <ref type="bibr" target="#b4">[4]</ref>. For this reason, automatic beat tracking, or tempo tracking, is an essential task for many applications such as musical analysis, automatic rhythm alignment of multiple musical instruments, cut and paste operations in audio editing, beat driven special effects. Although it might appear simple at first, tempo tracking has proved to be a difficult task when dealing with a broad variety of musical genres as shown by the large number of publications on this subject appeared during the last years <ref type="bibr" target="#b2">[2,</ref><ref type="bibr">5,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b12">12]</ref>.</p><p>Earlier tempo tracking approaches focused on MIDI or other symbolic formats, where note onsets are already available to the estimation algorithm. More recent approaches directly deal with ordinary CD audio recordings.</p><p>Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. c 2004 Universitat Pompeu Fabra.</p><p>The system that we present in this paper lies into this category.</p><p>For musical genres with a straightforward rhythm such as rap, rock, reggae and others where a strong percussive strike drives the rhythm, current beat trackers indicate high performance as pointed out by <ref type="bibr">[5,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b12">12]</ref>. However, the robustness of the beat tracking systems is often much less guaranteed when dealing with classical music because of the weakness of the techniques employed in attack detection and tempo variations inherent to that kind of music.</p><p>In the present article, we describe an algorithm to estimate the tempo of a piece of music (in beats per minute or bpm) and identify the temporal locations when it occurs. Like most of the systems available in the literature, this algorithm relies on a classical scheme: a front-end processor extracts the onset locations from a time-frequency or subband analysis of the signal, traditionally using a filter bank <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b12">12]</ref> or using the discrete Fourier transform <ref type="bibr" target="#b3">[3,</ref><ref type="bibr">5,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b9">9]</ref>. Then, a periodicity estimation algorithm finds the rate at which these events occur. A large variety of methods has been used for this purpose, for example, a bank of oscillators which resonate at integer multiples of their characteristic frequency <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b12">12]</ref>, pitch detection methods <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b10">10]</ref>, histograms of the inter-onset intervals <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b13">13]</ref>, probabilistic approaches such as Gaussian mixture model to express the likelihood of the onset locations <ref type="bibr" target="#b8">[8]</ref>.</p><p>In this paper, following Laroche's approach <ref type="bibr" target="#b9">[9]</ref>, we define the quantity so-called spectral energy flux as the derivative of the signal frequency content with respect to time. Although this principle has been previously used <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b9">9]</ref>, a significant improvement has been obtained by using an optimal filter to approximate this derivative.</p><p>We exploit this approach to obtain a high performance onset detector and integrate it into a tempo tracking algorithm. We demonstrate the usefulness of this approach by validating the proposed system using a large manually annotated data base that contains excerpts from rock, latin, pop, soul, classical, rap/hip-hop and others. The paper is organized as follows: Section 2 provides a detailed description of the three main stages that compose the system. In Section 3, test results are provided and compared to other methods. The system parameters used during the validation procedure are provided as well as comments about the issues of the algorithm. Finally, Section 4 summarizes the achievements of the presented algorithm and discusses possible directions for further improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">DESCRIPTION OF THE ALGORITHM</head><p>In this paper, it is assumed that the tempo of the audio signal is constant over the duration of the analysis window and that it eventually evolves slowly from one to the other.</p><p>In addition, we suppose that the tempo lies between 60 and 200 BPM, without loss of generality since any other value can be mapped into this range. The algorithm proposed is composed of three major steps (see figure <ref type="figure" target="#fig_0">1</ref>):</p><p>• onset detection: it consists in computing a detection function based on the spectral energy flux of the input audio signal;</p><p>• periodicity estimation : the periodicity of the detection function is estimated using pitch detection techniques ;</p><p>• beat location estimation : the position of the corresponding beats is obtained from the cross-correlation between the detection function and an artificial pulsetrain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Onset detection</head><p>The aim of onset detection consists in extracting a detection function that will indicate the location of the most salient features of the audio signal such as note changes, harmonic changes and percussive events. As a matter of fact, these events are particularly important in the beat perception process. Note onsets are easily masked in the overall signal energy by continuous tones of higher amplitude <ref type="bibr" target="#b9">[9]</ref>, while they are more likely detected after separating them in frequency channels. We propose to follow a frequency domain approach <ref type="bibr" target="#b3">[3,</ref><ref type="bibr">5,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b9">9]</ref> as it proves to outperform time-domain methods based on direct processing of the temporal waveform as a whole.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">Spectral analysis and spectral energy flux</head><p>The input audio signal is analyzed using a decimated version of the short-time Fourier transform (STFT), i.e., short signal segments are extracted at regular time intervals, multiplied by an analysis window and transformed into the frequency domain by means of a Fourier transform. This leads to</p><formula xml:id="formula_0">X( f , m) = N-1 ∑ n=0 w(n)x(n + mM)e -j2π f n (1)</formula><p>where x(n) denotes the audio signal, w(n) the finite analysis window of size N in samples, M the hop size in samples, m the frame index and f the frequency. Motivated by the work of Laroche <ref type="bibr" target="#b9">[9]</ref>, we define the spectral energy flux E( f , k) as an approximation to the derivative of the signal frequency content with respect to time</p><formula xml:id="formula_1">E( f , k) = ∑ m h(m -k) G( f , m) (2)</formula><p>where h(m) approximates a differentiator filter with:</p><formula xml:id="formula_2">H(e j2π f ) j2π f (3)</formula><p>and the transformation</p><formula xml:id="formula_3">G( f , m) = F {| X( f , m)|} (4)</formula><p>is obtained via a two step process: a low-pass filtering of | X( f , m)| to perform energy integration in a way similar to that in the auditory system, emphasizing the most recent inputs, but masking rapid modulations <ref type="bibr" target="#b14">[14]</ref> and a nonlinear compression. For example, in <ref type="bibr" target="#b9">[9]</ref> Laroche proposes h(m) as a first order differentiator filter (h = [1; -1]), no low-pass filtering is applied and the non-linear compression function is</p><formula xml:id="formula_4">G( f , n) = arcsinh(| X( f , m)|).</formula><p>In <ref type="bibr" target="#b6">[6]</ref> Klapuri uses the same first order differentiator filter, but for the transformation, he performs the low-pass filtering after applying a logarithmic compression function.</p><p>In the present work we propose h(m) to be a FIR filter differentiator. Such a filter is designed by a Remez optimisation procedure which leads to the best approximation to Eq. ( <ref type="formula">3</ref>) in the minimax sense <ref type="bibr" target="#b11">[11]</ref>. This new approach, compared to the first order difference used in <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b9">9]</ref> highly improves the extraction of musical meaningful features such as percussive attacks and chord changes. In addition, G( f , k) is obtained via low-pass filtering with a second half of a Hanning window followed by a logarithmic compression function as suggested by Klapuri <ref type="bibr" target="#b7">[7]</ref>, since the logarithmic difference function gives the amount of change in a signal in relation to its absolute level. This is a psycho-acoustic relevant measure since the perceived signal amplitude is in relation to its level, the same amount of increase being more prominent in a quite signal <ref type="bibr" target="#b7">[7]</ref>.</p><p>During the system development, several orders for the differentiator filter h(m) were tested. We found that using an order 8 filter was the best tradeoff between complexity and efficiency. In practice, the algorithm uses an N point FFT to evaluate the STFT, thus the frequency channels 1 to N 2 of the signal's time-frequency representation are filtered using h(m) to obtain the spectral energy flux. Then, all the positive contributions of these channels are summed to produce a temporal waveform v(k) that exhibits sharp maxima at transients and note onsets, i.e., those instants where the energy flux is large.</p><p>Beat tends to occur at note onsets, so we must first distinguish the "true beat" peaks from the spurious ones in v(k) to obtain a proper detection function p(k). In addition, we work under the supposition that these unwanted peaks are much smaller in amplitude compared to the note attack peaks. Thus, a peak-picking algorithm that selects peaks above a dynamic threshold calculated with the help of a median filter is a simple and efficient solution to this problem. The median filter is a nonlinear technique that computes the pointwise median inside a window of length 2i + 1 formed by a subset of v(k), thus the median threshold curve is given by the expression:</p><formula xml:id="formula_5">θ(k) = C • median(g k ) (5)</formula><p>where g k = {v k -i , . . . , v k , . . . , v k + i } and C is a predefined scaling factor to artificially rise the threshold curve slightly above the steady state level of the signal. To ensure accurate detection, the length of the median filter must be longer than the average width of the peaks of the detection function. In practice, we set the median filter length to 200 ms. Then, we obtain the signal p(k) = v(k) -θ(k), which is half-wave rectified to produce the detection function p(k):</p><formula xml:id="formula_6">p(k) = p(k) if p(k) &gt; 0 0 otherwise<label>(6)</label></formula><p>In our tests, the onset detector described above has proved to be a robust scheme that provides good results for a wide range of musical instruments and attacks at a relatively low computational cost. For example, Figure <ref type="figure" target="#fig_1">2</ref>-a shows the time waveform of a piano recording containing seven attacks. These attacks can be easily observed in the signal's spectrogram in Figure <ref type="figure" target="#fig_1">2-b</ref>. The physical interpretation of Figure <ref type="figure" target="#fig_1">2</ref>-c can be seen as the rate at which the frequency-content energy of the audio signal varies at a given time instant, i.e., the spectral energy flux. In this example, seven vertical stripes represent the reinforcement of the energy variation, clearly indicating the location of the attacks (the position of the spectrogram edges). When all the positive energy variations are summed in the frequency domain and thresholded, we obtain the detection function p(k) shown in Figure <ref type="figure" target="#fig_1">2-d</ref>. An example of an instrument with smooth attacks, a violin, is shown in Figure <ref type="figure" target="#fig_2">3</ref>. Large energy variations in the frequency content of the audio signal can still be observed as vertical stripes in Figure <ref type="figure" target="#fig_2">3-c</ref>. After summing the positive contributions, six of the seven attacks are properly detected as shown by the corresponding largest peaks in Figure <ref type="figure" target="#fig_2">3-d</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Periodicity estimation</head><p>The detection function p(k) at the output of the onset detection stage can be seen as a quasi-periodic and noisy pulse-train that exhibits large peaks at note attacks. The next step is to estimate the tempo of the audio signal, i.e., the periodicity of the note onset pulses. Two methods from traditional pitch determination techniques are employed: the spectral product and the autocorrelation function. These techniques have already been used for this purpose in <ref type="bibr" target="#b1">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Spectral product</head><p>The spectral product principle assumes that the power spectrum of the signal is formed from strong harmonics located at integer multiples of the signal's fundamental frequency. To find this frequency, the power spectrum is compressed by a factor m, then the obtained spectra are multiplied, leading to a reinforced fundamental frequency. For a normalized frequency, this is given by:</p><formula xml:id="formula_7">S(e j2π f ) = M ∏ m=1 |P(e j2πm f )| for f &lt; 1 2M<label>(7)</label></formula><p>where P(e j2π f ) is the discrete Fourier transform of p(k).</p><p>Then, the estimated tempo T is easily obtained by picking out the frequency index corresponding to the largest peak of S(e j2π f ). The tempo is constrained to fall in the range 60 &lt; T &lt; 200 BPM. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Autocorrelation function</head><p>This is a classical method in periodicity estimation. The non-normalized deterministic autocorrelation function of p(k) is calculated as follows:</p><formula xml:id="formula_8">r(τ) = ∑ k p(k + τ)p(k)<label>(8)</label></formula><p>Again, we suppose that 60 &lt; T &lt; 200 BPM. Hence, during the calculation of the autocorrelation, only the values of r(τ) corresponding to the range of 300 ms to 1 s are calculated. To find the estimated tempo T, the lag of the three largest peaks of r(τ) are analyzed and a multiplicity relationship between them is searched. In the case that no relation is found, the lag of the largest peak is taken as the beat period.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Beat location</head><p>To find the beat location, we use a method based on the comb filter idea that resembles previous work carried out by <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b12">12]</ref>. We create an artificial pulse-train q(t) of tempo T previously calculated as explained in Section 2.2 and cross-correlate it with p(k). This operation has a low computational cost, since the correlation is evaluated only at the indices corresponding to the maxima of p(k). Then, we call t 0 the time index where this cross-correlation is maximal and we consider it as the starting location of the beat. For the second and succesive beats in the analysis window, a beat period T is added to the previous beat location, i.e., t i = t i-1 + T and a corresponding peak in p(k) is searched within the area t i ± ∆. If no peak is found, the beat is placed in its expected position t i . When the last beat of the window occurs, its location is stored in order to assure the continuity with the first beat of the new analysis window. Where the tempo of the new analysis window differs by more than 10 % from the previous tempo, a new beat phase is estimated. The peaks are searched using the new beat period without referencing the previous beat phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PERFORMANCE ANALYSIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Database, annotation and evaluation protocole</head><p>The proposed algorithm was evaluated using a corpus of 489 musical excerpts taken from commercial CD recordings. These pieces were selected to cover as many characteristics as possible: various tempi in the 50 to 200 BPM range, a wide variety of instruments, dynamic range, studio/live recordings, old/recent recordings, with/without vocals, male/female vocals and with/without percussions. They were also selected to represent a wide diversity of musical genres as shown in Table <ref type="table" target="#tab_0">1</ref>.</p><p>From each of the selected recordings, an excerpt of 20 seconds having a relatively constant tempo, was extracted and converted to a monophonic signal sampled at 16 kHz. The procedure for manually estimating the tempo of each musical piece is the following:</p><p>• the musician listens to a musical excerpt using headphones (if required, several times in a row to be accustomed to the tempo),</p><p>• while listening, he/she taps the tempo,</p><p>• the tapping signal is recorded and the tempo is extracted from it.</p><p>As pointed out by Goto in <ref type="bibr" target="#b4">[4]</ref>, the beat is a perceptual concept that people feel in music, so it is generally difficult to define the "correct beat" in an objective way. People have a tendency to tap at different metric levels. For Method Recognition rate Paulus <ref type="bibr" target="#b10">[10]</ref> 56.3 % Scheirer <ref type="bibr" target="#b12">[12]</ref> 67.4 % SP .</p><p>63.2 % AC .</p><p>73.6 % SP using SEF.</p><p>84.0 % AC using SEF 89.7 % Table <ref type="table">2</ref>. Tempo estimation performances. SEF stands for spectral energy flux, SP for spectral product and AC for autocorrelation.</p><p>example, in a piece that has a 4/4 time signature, it is correct to tap every quarter-note or every half-note. In general, a "ground truth" tempo cannot be established unless the musical score of the piece is available. This is a very common problem when humans tap along with the music, i.e., to tap twice as fast or twice as slow the "true" tempo. Whenever this case ocurred during the database annotation, the slower tempo was taken as reference T R . In a similar way to humans, automatic tempo estimation methods also make this doubling or halving of the "true" tempo. Thus, for evaluation purposes the tempo estimation T provided by the algorithm is labeled as correct if there is a less than 5% disagreement from the manually annotated tempo used as reference T R under the principle 0.95αT &lt; T R &lt; 1.05αT with α ∈ { 1 2 , 1, 2}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Results</head><p>During the evaluation, the algorithm parameters were set as follows. The length of the analysis window for tempo estimation was set to four seconds, with an overlapping factor of 50%. Smaller window size values reduce the algorithm performance. For the spectral energy flux calculation, the length of the analysis window used in the computation of the STFT was 64 samples (4 ms) with an overlapping factor of 50% and a 128 point FFT, thus the detection function v(k) could be seen as signal sampled at 500 Hz. As mentioned, the order of the differentiator FIR filter was set to L = 8. In the beat location stage, the median filter i was set to 25 samples, C was set to 2, and for the peak location ∆ was set to 10 % of the beat period.</p><p>To have a better idea of the performance of our algorithm, we decided to compare it with our own implemententation of the algorithms proposed by Paulus <ref type="bibr" target="#b10">[10]</ref> and Scheirer <ref type="bibr" target="#b12">[12]</ref>. We also compared it with our previous work in tempo estimation <ref type="bibr" target="#b1">[1]</ref>. In this case, the main difference between the previous and the current system lies on the onset extraction stage. Table <ref type="table">2</ref> summarizes the overall recognition rate for the evaluated systems. In this table, SP stands for spectral product, AC for autocorrelation and SEF for spectral energy flux.</p><p>In more details, the performance of these methods by musical genre are presented in Table <ref type="table">3</ref>. In this table, PLS stands for Paulus, SCR for Scheirer. As expected, results</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Architecture of the beat tracking algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. From top to bottom: time waveform of a piano signal, its spectrogram, its spectral energy flux and the detection function p(k).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. From top to bottom: time waveform of a violin signal, its spectrogram, its spectral energy flux and the detection function p(k).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Genre distribution of the test database.</figDesc><table><row><cell>Genre</cell><cell cols="2">Pieces Percentage</cell></row><row><cell>classical</cell><cell>137</cell><cell>28.0 %</cell></row><row><cell>jazz</cell><cell>79</cell><cell>16.2 %</cell></row><row><cell>latin</cell><cell>37</cell><cell>7.6 %</cell></row><row><cell>pop</cell><cell>40</cell><cell>8.2 %</cell></row><row><cell>rock</cell><cell>44</cell><cell>9.0 %</cell></row><row><cell>reggae</cell><cell>30</cell><cell>6.1 %</cell></row><row><cell>soul</cell><cell>24</cell><cell>4.9 %</cell></row><row><cell>rap, hip-hop</cell><cell>20</cell><cell>4.1 %</cell></row><row><cell>techno</cell><cell>23</cell><cell>4.7 %</cell></row><row><cell>other</cell><cell>55</cell><cell>11.2 %</cell></row><row><cell>total</cell><cell>489</cell><cell>100 %</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head> <ref type="bibr" target="#b10">[10]</ref><p>, SCR for Scheirer <ref type="bibr" target="#b12">[12]</ref>.</p><p>indicate that classical music is the most difficult genre. Nevertheless, the proposed algorithm displayed promising results. For the other genres, it shows good performance, particularly for music with a straightforward rhythm. Several authors have pointed out the difficulty in evaluating beat tracking systems <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b9">9]</ref> due to the subjective interpretation of the beat and the inexistence of a consensual data base of beat-labeled audio tracks. In our case, the beat location evaluation was done at a subjective level, that is, artificial "sound clicks" were superimposed on the tested signal at the calculated beat locations and tempo.</p><p>During the validation procedure, we note that the proposed algorithm produces erroneous results under the following circumstances:</p><p>• when dealing with signals having a stealthily or long fading-in attacks, the hypothesis that supurious peaks are smaller than attack peaks does not hold any more, leading to false onset detections;</p><p>• the spectral energy flux follows the principle that stable spectra regions are followed by transition regions. When many instruments play simultaneously, as in an orchestra, their 'spectral mixture' lacks stable regions, leading to false onset detections;</p><p>• when the tempo varies too quickly in short time segments or if there are large beat gaps in the signal, the periocity estimation stage cannot keep up with the changes.</p><p>The reader is welcome to listen to the sound examples available at www.tsi.enst.fr/∼malonso/ismir04.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSIONS</head><p>In this paper we have presented an efficient beat tracking algorithm that processes audio recordings. We have also defined the concept of spectral energy flux and used it to derive a new and effective onset detector based on the STFT, an efficient differentiator filter and dynamic thresholding using a median filter. This onset detector displays high performance for a large range of audio signals. In addition, the proposed tempo tracking system is straightforward to implement and has a relatively low computational cost. The performance of the algorithm presented was evaluated on a large database containing 489 musical excerpts from several musical genres. The results are encouraging since the global success rate for tempo estimation was 89.7%. The method presented works offline. A real-time implementation is considered, but currently there are various issues to be resolved such as the block-wise processing that requires access to future signal samples and the non-causality of the thesholding filter. Future work should explore other periodicity estimation techniques and an analysis of the residual part after a harmonic/noise decomposition.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Study of Tempo Tracking Algorithms from Polyphonic Music Signals</title>
		<author>
			<persName><forename type="first">M</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Richard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th. COST 276 Workshop</title>
		<meeting>the 4th. COST 276 Workshop<address><addrLine>Bordeaux, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-03">March 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic Extraction of Tempo and Beat from Expressive Performances</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dixon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of New Music Research</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="58" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Hybrid Approach to Musical Note Onset Detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Duxbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Davies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th. Int. Conf. on Digital Audio Effects (DAFx)</title>
		<meeting>the 5th. Int. Conf. on Digital Audio Effects (DAFx)<address><addrLine>Hamburg Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-09">September 2002</date>
			<biblScope unit="page" from="33" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Issues in Evaluating Beat Tracking Systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Goto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Muraoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Working Notes of the IJCAI-97 Workshop on Issues in AI and Music</title>
		<imprint>
			<date type="published" when="1997-08">August 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An Audio-based Real-time Beat Tracking System for Music With or Without Drum-sounds</title>
		<author>
			<persName><forename type="first">M</forename><surname>Goto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of New Music Research</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="159" to="171" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Musical meter estimation and music transcription</title>
		<author>
			<persName><forename type="first">A</forename><surname>Klapuri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003-03">March 2003</date>
			<pubPlace>UK</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Cambridge Music Processing Colloquium, Cambridge University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sound Onset Detection by Applying Psychoacoustic Knowledge</title>
		<author>
			<persName><forename type="first">A</forename><surname>Klapuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Int. Conf. Acoustics Speech and Sig. Proc. (ICASSP)</title>
		<meeting>IEEE Int. Conf. Acoustics Speech and Sig. Proc. (ICASSP)<address><addrLine>Phoenix AR, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-03">March 1999</date>
			<biblScope unit="page" from="3089" to="3092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Laroche</surname></persName>
		</author>
		<title level="m">Proceedings IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)</title>
		<meeting>IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)<address><addrLine>New Paltz, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-10">October 2001</date>
			<biblScope unit="page" from="135" to="138" />
		</imprint>
	</monogr>
	<note>Estimating, Tempo, Swing and Beat Locations in Audio Recordings</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient Tempo and Beat Tracking in Audio Recordings</title>
		<author>
			<persName><forename type="first">J</forename><surname>Laroche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Audio. Eng. Soc</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="226" to="233" />
			<date type="published" when="2003-04">April 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Measuring the Similarity of Rythmic Patterns</title>
		<author>
			<persName><forename type="first">J</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klapuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Music Information Retrieval</title>
		<meeting>the International Symposium on Music Information Retrieval<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Digital Signal Processing: Principles, Algorithms and Applications</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Proakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Manolakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Prentice Hall</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Tempo and Beat Analysis of Acoustic Music Signals</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Scheirer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="588" to="601" />
			<date type="published" when="1998-01">January 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tatum grid analysis of musical signals</title>
		<author>
			<persName><forename type="first">J</forename><surname>Seppa Anen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)</title>
		<meeting>IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)<address><addrLine>New Paltz, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-10">October 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The Auditory &apos;Primal Sketch&apos;: A Multiscale model of rhythmic grouping</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Todd</surname></persName>
		</author>
		<author>
			<persName><surname>Mca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of New Music Research</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="70" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
