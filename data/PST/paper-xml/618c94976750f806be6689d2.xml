<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Compare to The Knowledge: Graph Neural Fake News Detection with External Knowledge</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Linmei</forename><surname>Hu</surname></persName>
							<email>hulinmei@bupt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianchi</forename><surname>Yang</surname></persName>
							<email>yangtianchi@bupt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luhao</forename><surname>Zhang</surname></persName>
							<email>zhangluhao@meituan.com</email>
							<affiliation key="aff1">
								<address>
									<settlement>Meituan</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wanjun</forename><surname>Zhong</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Duyu</forename><surname>Tang</surname></persName>
							<email>dutang@microsoft.com</email>
							<affiliation key="aff3">
								<orgName type="laboratory">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
							<email>shichuan@bupt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
							<email>nanduan@microsoft.com</email>
							<affiliation key="aff3">
								<orgName type="laboratory">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
							<email>mingzhou@microsoft.com</email>
							<affiliation key="aff3">
								<orgName type="laboratory">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Compare to The Knowledge: Graph Neural Fake News Detection with External Knowledge</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Nowadays, fake news detection, which aims to verify whether a news document is trusted or fake, has become urgent and important. Most existing methods rely heavily on linguistic and semantic features from the news content, and fail to effectively exploit external knowledge which could help determine whether the news document is trusted. In this paper, we propose a novel end-to-end graph neural model called CompareNet, which compares the news to the knowledge base (KB) through entities for fake news detection. Considering that fake news detection is correlated with topics, we also incorporate topics to enrich the news representation. Specifically, we first construct a directed heterogeneous document graph for each news incorporating topics and entities. Based on the graph, we develop a heterogeneous graph attention network for learning the topic-enriched news representation as well as the contextual entity representations that encode the semantics of the news content. The contextual entity representations are then compared to the corresponding KB-based entity representations through a carefully designed entity comparison network, to capture the consistency between the news content and KB. Finally, the topic-enriched news representation combining the entity comparison features are fed into a fake news classifier. Experimental results on two benchmark datasets demonstrate that CompareNet significantly outperforms state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the rapid development of the Internet, there are increasingly huge opportunities for fake news *</p><p>The work was done while visiting Micorosft Research Asia.</p><p>production, dissemination and consumption. Fake news are news documents that are intentionally and verifiably false, and could mislead readers <ref type="bibr" target="#b0">(Allcott and Gentzkow, 2017)</ref>. Fake news can easily misguide public opinion, cause the crisis of confidence, and disturb the social order <ref type="bibr" target="#b25">(Vosoughi et al., 2018)</ref>. It is well known that fake news exerted an influence in the past 2016 US presidential elections <ref type="bibr" target="#b0">(Allcott and Gentzkow, 2017)</ref>. Thus, it is very important to develop effective methods for early fake news detection based on the textual content of the news document.</p><p>Some existing fake news detection methods rely heavily on various hand-crafted linguistic and semantic features for differentiating between news documents <ref type="bibr" target="#b3">(Conroy et al., 2015;</ref><ref type="bibr" target="#b17">Rubin et al., 2016;</ref><ref type="bibr" target="#b15">Rashkin et al., 2017;</ref><ref type="bibr">Khurana and Intelligentie, 2017;</ref><ref type="bibr" target="#b19">Shu et al., 2020)</ref>. To avoid feature engineering, deep neural models such as Bi-LSTM and convolutional neural networks (CNN) have been employed <ref type="bibr" target="#b12">(Oshikawa et al., 2020;</ref><ref type="bibr" target="#b26">Wang, 2017;</ref><ref type="bibr">Rodr√≠guez and Iglesias, 2019)</ref>. However, they fail to consider the sentence interactions in the document. <ref type="bibr">Vaibhav et al.</ref> showed that trusted news and fake news have different patterns of sentence interactions <ref type="bibr" target="#b24">(Vaibhav et al., 2019)</ref>. They modeled a news document as a fully connected sentence graph and proposed a graph attention model for fake news detection. Although these existing approaches can be effective, they fail to fully exploit external KB which could help determine whether the news is fake or trusted.</p><p>External KB such as Wikipedia contains a large amount of high-quality structured subjectpredicate-object triplets and unstructured entity descriptions, which could serve as evidence for detecting fake news. As shown in Figure <ref type="figure">4</ref>, the news document about "mammograms are not effective at detecting breast tumors" is likely to be detected as fake news with the knowledge that " The goal of mammography is the early detection of breast cancer" in the Wikipedia entity description page<ref type="foot" target="#foot_0">1</ref> . <ref type="bibr">Pan et al. proposed</ref> to construct knowledge graphs from positive and negative news, and apply TransE to learn triplet scores for fake news detection <ref type="bibr" target="#b13">(Pan et al., 2018)</ref>. Nevertheless, the performance is largely influenced by construction of the knowledge graph. In this paper, to take full advantage of the external knowledge, we propose a novel endto-end graph neural model CompareNet which directly compares the news to the KB through entities for fake news detection. In CompareNet, we also consider using topics to enrich the news document representation for improving fake news detection, since fake news detection and topics are highly correlated <ref type="bibr" target="#b29">(Zhang et al., 2020;</ref><ref type="bibr" target="#b8">Jin et al., 2016)</ref>. For example, the news documents in the "health" topic are inclined towards false, while the documents belonging to the "economy" topic are biased to be trusted instead.</p><p>Particularly, we first construct a directed heterogeneous document graph for each news document, containing sentences, topics and entities as nodes.The sentences are fully connected in bidirection. Each sentence is also connected with its top relevant topics in bi-direction. If a sentence contains an entity, one directed link is built from the sentence to the entity. The reason for building one-way links from sentences to entities is to ensure that we can learn contextual entity representations that encode the semantics of the news, while avoiding the influence of the true entity knowledge to the news representation. Based on the directed heterogeneous document graph, we develop a heterogeneous graph attention network to learn topic-enriched news representations and contextual entity representations. The learned contextual entity representations are then compared to the corresponding KB-based entity representations with a carefully designed entity comparison network, in order to capture the semantic consistency between the news content and external KB. Finally, the topic-enriched news representations and the entity comparison features are combined for fake news classification. To facilitate related researches, we release both our code and dataset to the public<ref type="foot" target="#foot_1">2</ref> .</p><p>In summary, our main contributions include: 1) In this paper, we propose a novel end-to-end graph neural model CompareNet which compares the news to the external knowledge through entities for fake news detection.</p><p>2) In CompareNet, we also consider the useful topic information. We construct a directed heterogeneous document graph incorporating topics and entities. Then we develop heterogeneous graph attention networks to learn topicenriched news representations. A novel entity comparison network is designed to compare the news to the KB.</p><p>3) Extensive experiments on two benchmark datasets demonstrate that our model significantly outperforms state-of-the-art models on fake news detection by effectively incorporating external knowledge and topic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Fake news detection has attracted much attention in recent years <ref type="bibr" target="#b32">(Zhou and Zafarani, 2020;</ref><ref type="bibr" target="#b12">Oshikawa et al., 2020)</ref>. A lot of works also focus on the related problem, i.e., fact checking, which aims to search evidence from external knowledge to verify the veracity of a claim (e.g., a subject-predicateobject triple) <ref type="bibr" target="#b23">(Thorne et al., 2018;</ref><ref type="bibr" target="#b31">Zhou et al., 2019;</ref><ref type="bibr" target="#b30">Zhong et al., 2020)</ref>. Generally, fake news detection usually focuses on news events while fact-checking is broader <ref type="bibr" target="#b12">(Oshikawa et al., 2020)</ref>. The approaches for fake news detection can be divided into two categories: social-based and content-based.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Social-based Fake News Detection</head><p>Social context related to news documents contains rich information such as user profiles and social relationships to help detect fake news. Social based models basically include stance-based and propagation-based. Stance-based models utilize users' opinions to infer news veracity <ref type="bibr" target="#b8">(Jin et al., 2016;</ref><ref type="bibr" target="#b28">Wu et al., 2019)</ref>. Tacchini et al. constructed a bipartite network of user and posts with 'like' stance information, and proposed a semisupervised probabilistic model to predict the likelihood of posts being hoaxes <ref type="bibr" target="#b22">(Tacchini et al., 2017)</ref>. Propagation-based approaches for fake news detection are based on the basic assumption that the credibility of a news event is highly related to the credibilities of relevant social media posts. Both homogeneous <ref type="bibr" target="#b8">(Jin et al., 2016)</ref> and heterogeneous credibility networks <ref type="bibr" target="#b5">(Gupta et al., 2012;</ref><ref type="bibr" target="#b20">Shu et al., 2019;</ref><ref type="bibr" target="#b29">Zhang et al., 2020)</ref> have been built to model the propagation process. For instance, <ref type="bibr" target="#b29">(Zhang et al., 2020)</ref> constructed a heterogeneous network of news articles, creators and news subjects, and proposed a deep diffusive network model for incorporating the network structure information to simultaneously detect fake news articles, creators and subjects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Content-based Fake News Detection</head><p>On the other hand, news contents contain the clues to differentiate fake and trusted news. A lot of existing works extract specific writing styles such as lexical and syntactic features <ref type="bibr" target="#b3">(Conroy et al., 2015;</ref><ref type="bibr" target="#b17">Rubin et al., 2016;</ref><ref type="bibr">Khurana and Intelligentie, 2017;</ref><ref type="bibr" target="#b15">Rashkin et al., 2017;</ref><ref type="bibr" target="#b19">Shu et al., 2020;</ref><ref type="bibr" target="#b12">Oshikawa et al., 2020)</ref> and sensational headlines <ref type="bibr" target="#b14">(Potthast et al., 2018;</ref><ref type="bibr" target="#b21">Sitaula et al., 2019)</ref> for fake news classifier. To avoid hand-crafted feature engineering, neural models have been proposed <ref type="bibr" target="#b26">(Wang, 2017;</ref><ref type="bibr">Rodr√≠guez and Iglesias, 2019)</ref>. For example, Ibrain et al. applied deep neural networks, such as Bi-LSTM and convolutional neural networks (CNN) for fake news detection (Rodr√≠guez and Iglesias, 2019). However, these works fail to consider different sentence interaction patterns between trusted and fake news documents. <ref type="bibr">Vaibhav et al. proposed</ref> to model a document as a sentence graph capturing the sentence interactions and applied graph attention networks for learning document representation <ref type="bibr" target="#b24">(Vaibhav et al., 2019)</ref>. <ref type="bibr">Pan et al. proposed</ref> to construct knowledge graphs from positive and negative news, and apply TransE to learn triplet scores for fake news detection <ref type="bibr" target="#b13">(Pan et al., 2018)</ref>. Nevertheless, they relied heavily on the quality of the construction of knowledge graphs. In this paper, we propose a novel graph neural model Com-pareNet which directly compares the news to external knowledge for fake news detection. Considering that the detection of fake news is correlated with topics, we also use topics to enrich the news representation for improving fake news detection.</p><p>Some works <ref type="bibr" target="#b26">(Wang, 2017;</ref><ref type="bibr" target="#b9">Khattar et al., 2019;</ref><ref type="bibr" target="#b27">Wang et al., 2020</ref>) also consider incorporating multi-modal features such as images for improving fake news detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Proposed CompareNet</head><p>In this section, we detail our proposed fake news detection model CompareNet, which directly compares the news to external knowledge for fake news detection. As shown in Figure <ref type="figure" target="#fig_1">2</ref>, we also consider topics for enriching news representation since fake news detection is highly correlated with topics <ref type="bibr" target="#b29">(Zhang et al., 2020)</ref>. Specifically, we first construct a directed heterogeneous document graph for each news document incorporating topics and entities as shown in Figure <ref type="figure" target="#fig_0">1</ref>. The graph well captures the interactions among sentences, topics and entities. Based on the graph, we develop a heterogeneous graph attention network to learn the topic-enriched news representation as well as the contextual entity representations that encode the semantics of the news document. To fully leverage external KB, we take the entities as the bridge between the news document and the KB. We compare the contextual entity representations with the corresponding KB-based entity representations using a carefully designed entity comparison network. Finally, the obtained entity comparison features are combined with the topic-enriched news document representation for fake news detection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Directed Heterogeneous Document Graph</head><p>For each news document d, we construct a directed heterogeneous document graph G = (V, E) incorporating topics and entities, as shown in We first split the news document as a set of sentences. Sentences are bidirectionally connected with each other in the graph, capturing the interaction of each sentence with every other sentence. Since topic information is important for fake news detection <ref type="bibr" target="#b29">(Zhang et al., 2020)</ref>, we apply the unsupervised LDA <ref type="bibr" target="#b1">(Blei et al., 2003)</ref> (the total topic number K is set as 100) to mine the latent topics T from all the sentences of all the documents in our dataset. Specifically, each sentence is taken as a pseudo-document and is assigned to the top P relevant topics with the largest probabilities. Thus, each sentence is also connected with its top P assigned topics in bi-direction, allowing the useful topic information to propagate among the sentences. Note that we can also deal with new coming news documents by inferring the topics with trained LDA. We identify the entities E in the document d and map them to Wikipedia using the entity linking tool TAGME<ref type="foot" target="#foot_2">3</ref> . If a sentence s contains an entity e, we build a one-way directed edge from a sentence to the entity e, in order to allow only information propagation from sentences to entities. In this way, we can avoid integrating true entity knowledge directly into news representation, which may mislead the detection of fake news.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Heterogeneous Graph Convolution</head><p>Based on the above directed heterogeneous document graph G, we develop a heterogeneous graph attention network for learning the news representation as well as the contextual entity representations. It considers not only the weights of different nodes with different types <ref type="bibr" target="#b7">(Hu et al., 2019)</ref> but also the edge directions in the heterogeneous graph.</p><p>Formally, we have three types T = {œÑ 1 , œÑ 2 , œÑ 3 } of nodes: sentences S, topics T and entities E with different feature spaces. We apply LSTM to encode a sentence s = {w 1 , ‚Ä¢ ‚Ä¢ ‚Ä¢, w m } and get its feature vector x s ‚àà R M . The entity e ‚àà E is initialized with the entity representations e KB ‚àà R M learned from the external KB (see Subsection 3.3.1). The topic t ‚àà T is initialized with one-hot vector x t ‚àà R K .</p><p>Next, consider the graph G = (V, E) where V and E represent the set of nodes and edges respectively. Let X ‚àà R |V|√óM be a matrix containing the nodes with their features x v ‚àà R M (each row x v is a feature vector for a node v). A and D are the adjacency matrix and the degree matrix, respectively. The heterogeneous convolution layer updates the (l + 1)-th layer representation of the nodes H (l+1) by aggregating the features of their neighboring nodes H (l) œÑ with different types œÑ . (Initially, H (0) = X):</p><formula xml:id="formula_0">H (l+1) = œÉ( œÑ ‚ààT B œÑ ‚Ä¢ H (l) œÑ ‚Ä¢ W (l) œÑ ),<label>(1)</label></formula><p>where œÉ(‚Ä¢) denotes the activation function. Nodes with different types œÑ have different transformation matrix W</p><p>(l)</p><p>œÑ . The transformation matrix W (l)</p><p>œÑ considers the different feature spaces and projects them into an implicit common space. B œÑ ‚àà R |V|√ó|VœÑ | is the attention matrix, whose rows represent all the nodes and columns represent their neighboring nodes with the type œÑ . Its element Œ≤ vv in the v-th row and the v -th column is computed as follows:</p><formula xml:id="formula_1">Œ≤ vv = Softmax v (œÉ(ŒΩ T ‚Ä¢ Œ± œÑ [h v , h v ])), (2)</formula><p>where ŒΩ is the attention vector and Œ± œÑ is the typelevel attention weight. h v and h v are respectively the representation of the current node v and its neighboring node v . Softmax function is applied to normalize across the neighboring nodes of node v.</p><p>We calculate the type-level attention weights Œ± œÑ based on the current node embedding h v and the type embedding h œÑ = v √Évv h v (the weighted sum of the neighboring node embeddings h v with the type œÑ , where the weight matrix √É = D ‚àí 1 2 (A+ I)D ‚àí 1 2 is the normalized adjacency matrix with added self-connections) as follows:</p><formula xml:id="formula_2">Œ± œÑ = Softmax œÑ (œÉ(¬µ T œÑ ‚Ä¢ [h v , h œÑ ])),<label>(3)</label></formula><p>where ¬µ œÑ is the attention vector for the type œÑ . Softmax function is applied to normalize across all the types. After L-layer heterogeneous graph convolution, we can finally get all the node (including sentences and entities) representations aggregating neighborhood semantics. We use max pooling over the representations of the sentence nodes H s ‚àà R N to obtain the final topic-enriched news document embedding H d ‚àà R N . The learned entity representations that encode the contextual semantics of the document are taken as contextual entity representations e c ‚àà R N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Entity Comparison Network</head><p>In this subsection, we detail our entity comparison network which compares the learned contextual entity embeddings e c to the corresponding KBbased entity embeddings e KB . We believe entity comparison features could improve fake news detection based on the assumption that e c learned from trusted news document can be better aligned with the corresponding e KB ; while inverse for fake news.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">KB-based Entity Representation</head><p>We first illustrate how to take full advantage of both structured subject-predicate-object triplets and unstructured textual entity descriptions in the KB (i.e., Wikipedia) to learn KB-based entity representations e KB .</p><p>Structural Embedding. A wide range of knowledge graph embedding methods can be applied to obtain structured entity embeddings. Due to the simplicity of TransE <ref type="bibr" target="#b2">(Bordes et al., 2013)</ref>, we adopted TransE to learn entity representations e s ‚àà R M from the triplets. Formally, given a triplet (h, r, t), TransE regards a relationship r as a translation vector r from the head entity h to the tail entity t, namely h + r = t.</p><p>Textual Embedding. For each entity, we take the first paragraph of the corresponding Wikipedia page as its text description. Then we apply LSTM <ref type="bibr" target="#b6">(Hochreiter and Schmidhuber, 1997)</ref> to learn entity representations e d ‚àà R M that encode the entity descriptions.</p><p>Gating Integration. Since both the structural triplets and textual description provide valuable information for an entity, we integrate these information into a joint representation. Particularly, as we have the structural embedding e s and textual embedding e d , we adopt a learnable gating function to integrate entity embeddings from the two sources. Formally,</p><formula xml:id="formula_3">e KB = g e e s + (1 ‚àí g e ) e d ,<label>(4)</label></formula><p>where g e ‚àà R M is a gating vector (w.r.t. the entity e) to trade-off information from the two sources and its elements are in [0, 1]. denotes elementwise multiplication. The gating vector g e means that each dimension of e s and e d are summed by different weights. To constrain the value of each element in [0, 1], we compute the gate g e with the Sigmoid function:</p><formula xml:id="formula_4">g e = œÉ(g e ),<label>(5)</label></formula><p>where ge ‚àà R M is a real-value vector and is learned in the training process.</p><p>After fusing the two types of embeddings with the gating function, we obtain the final KB-based entity embeddings e KB ‚àà R M which encode both structural information from the triplets and textual information from the entity descriptions in the KB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Entity Comparison</head><p>We then perform entity-to-entity comparison between the news document and the KB, to capture the semantic consistency between the news content and the KB. We calculate a comparison vector a i between each contextual entity representation e c ‚àà R N and its corresponding KB-based entity embedding e KB ‚àà R M .</p><formula xml:id="formula_5">a i = f cmp (e c , W e ‚Ä¢ e KB ) ,<label>(6)</label></formula><p>where f cmp () denotes the comparison function, and W e ‚àà R N √óM is a transformation matrix. To measure the embedding closeness and relevance <ref type="bibr" target="#b18">(Shen et al., 2018)</ref>, we design our comparison function as:</p><formula xml:id="formula_6">f cmp (x, y) = W a [x ‚àí y, x y],<label>(7)</label></formula><p>where W a ‚àà R N √ó2N is a transformation matrix and is hadamard product, i.e., element-wise product. The final output comparison feature vector C ‚àà R N is obtained by the max pooling over the alignment vectors A = [a 1 , a 2 , ..., a n ] of all the entities E = {e 1 , e 2 , ..., e n } in the news document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model Training</head><p>After obtaining the comparison vector C ‚àà R N and the final news document representation vector H d ‚àà R N , we concatenate and feed them into a Softmax layer for fake news classification. Formally, </p><formula xml:id="formula_7">Z = Softmax(W o [H d , C] + b o ),<label>(8)</label></formula><formula xml:id="formula_8">L = ‚àí i‚ààD train j=1 Y ij ‚Ä¢ log Z ij + Œ∑ Œò 2 , (9)</formula><p>where D train is the set of news documents for training, Y is the corresponding label indicator matrix, Œò is the model parameters, and Œ∑ is regularization factor. For model optimization, we adopt the gradient descent algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conduct extensive experiments across various settings and datasets. Following the previous work <ref type="bibr" target="#b24">(Vaibhav et al., 2019)</ref>, we use SLN: Satirical and Legitimate News Database <ref type="bibr" target="#b17">(Rubin et al., 2016)</ref>, and LUN: Labeled Unreliable News Dataset <ref type="bibr" target="#b15">(Rashkin et al., 2017)</ref> for our experiments. Table <ref type="table" target="#tab_0">1</ref> shows the statistics.</p><p>Our baseline models include deep neural models: LSTM <ref type="bibr" target="#b6">(Hochreiter and Schmidhuber, 1997)</ref>, CNN <ref type="bibr" target="#b11">(Kim, 2014)</ref>, BERT+LSTM <ref type="bibr" target="#b24">(Vaibhav et al., 2019)</ref> (BERT for sentence encoder and then LSTM for document encoder) and BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref> (directly for document encoder). We also compare our model with graph neural models: GCN and GAT based on an undirected fully-connected sentence graph, which use attention pooling or max pooling for learning news document representation. For fair comparison with the previous work <ref type="bibr" target="#b24">(Vaibhav et al., 2019)</ref>, we use LSTM to encode sentences with randomly initialized word embeddings, which is the same as all the graph neural baselines. We run our model 5 times and report the micro-averaged (Precision = Recall = F1) and macro-averaged scores (Precision, Recall, F1) in all the settings including 2-way and 4-way classification.</p><p>2-way classification: We use the satirical and trusted news articles from LUN-train for training, LUN-test for validation and evaluate our model on the entire SLN dataset. This is done to emulate a real-world scenario where we want to see the performance of our model on an out-of-domain dataset.</p><p>4-way classification: We split the LUN-train into a 80:20 split to create our training and validation set. We use the LUN-test as our in-domain test set.</p><p>Experimental Setting. In our experiments, we set the number of topics K = 100 in LDA. Each sentence is assigned to top P = 2 topics with the largest probabilities. The layer number of our heterogeneous graph convolution is set as L = 1. These parameters are chosen according to the best experimental results on validation set. The other hyper-parameters are set as the same as the baseline <ref type="bibr" target="#b24">(Vaibhav et al., 2019)</ref> for fair comparison. Specifically, all the hidden dimensions used in our model are set as M = 100. The node embedding dimension N = 32. For GCN, GAT and CompareNet, we set the activation function as LeakyRelU with slope 0.2. For model training, we train the models for a maximum of 15 epochs and use Adam optimizer with learning rate 0.001. We set L2 normalization factor Œ∑ as 1e-6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overall Results</head><p>Table <ref type="table">2</ref> shows the results for the two-way classification between satirical and trusted news articles. We report only micro F1 since micro Precision=Recall=F1. As we can see, our proposed model CompareNet significantly outperforms all the state-of-the-art baselines in terms of all the metrics. Compared to the best baseline model, CompareNet improves both micro F1 and macro F1 by nearly 3%. We can also find that the graph neural network based models GCN and GAT all perform better than the deep neural models including CNN, LSTM and BERT. The reason is that the deep neural models fail to consider the interactions between sentences, which is important for fake news detection since different interaction patterns are observed in trusted and fake news documents <ref type="bibr" target="#b24">(Vaibhav et al., 2019)</ref>. Our model Com-pareNet further improves fake news detection by effectively exploiting the topics as well as the external KB. The topics enrich the news representation, and the external KB offers evidences for fake news detection.</p><p>We also present the results of four-way classification in Table <ref type="table">3</ref>. Consistently, all graph neural models capturing sentence interactions outperform the deep neural models. Our model CompareNet achieves the best performance in terms of all metrics. We believe that our model CompareNet benefits from the topics and external knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>In this subsection, we conduct experiments to study the effectiveness of each module in CompareNet and the way we incorporate external knowledge. We study the average performance of 5 runs on the LUN-test set. As shown in  information is as important as the external knowledge. Removing both topics and external knowledge (i.e., w/o Both) will lead to substantial performance drop (4.0-5.0%). It demonstrates the importance of both topics and external knowledge. The variant model CompareNet (undirected) although incorporating both topics and external knowledge achieves lower performance than CompareNet w/o Entity Cmp and CompareNet w/o Topics. The reason could be that CompareNet (undirected) directly aggregates the true entity knowledge into the news representation in graph convolution without considering the directed edges, which misleads the classifier for differentiating fake news. This verifies the appropriateness of our constructed directed heterogeneous document graph. The last variant Com-pareNet (concatenation) also performs lower than CompareNet w/o Entity Cmp, further indicating that directly concatenating true entity knowledge is not a good way for incorporating entity knowledge. Its performance drops by around 2.0% compared to CompareNet. These demonstrate the effectiveness of the carefully designed entity comparison network in CompareNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis of Top Assigned Topic Number</head><p>Figure <ref type="figure" target="#fig_4">3</ref> shows the performance (micro and macro F1) of our model CompareNet on LUN validation set with different number of top assigned topics P to each sentence. As we can see clearly, micro F1 and macro F1 first consistently rises with the increase of P and then drops when P is larger than 2. This may because that connecting too many lowprobability topics will introduce some noise. Thus, in our experiments, we set P =</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Case Study</head><p>To further illustrate why our model outperforms state-of-the-art baseline GAT+Attn <ref type="bibr" target="#b24">(Vaibhav et al., 2019)</ref>  <ref type="figure">4</ref>, the content of the news document is in conflict with the entity description from Wikipedia. Specifically, the news about "FDA target and threaten the natural health community" delivers contrary meaning from the entity description that "FDA is responsible for protecting and promoting public health"<ref type="foot" target="#foot_3">4</ref> . Similarly, the news document about "mammograms are not effective at detecting breast tumors" conveys different meaning from the entity description of "mammograms". We believe that our model CompareNet benefits from the comparison to Wikipedia knowledge by the entity comparison network. We find there are also unsuccessful cases since an entity could be mistakenly linked to a wrong entity in the Wikipedia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a novel end-to-end graph neural model CompareNet which compares the news to the external knowledge for fake news detection. Considering that the detection of fake news is correlated with topics, in our model, we also use topics to enrich the news document representation for improving fake news detection. Particularly, we first construct a directed heterogeneous document graph for each news document capturing the interactions among sentences, topics and entities.</p><p>Based on the graph, we develop a heterogeneous graph attention network for learning topic-enriched news representation as well as contextual entity representations that encode the semantics of the content of the news document. To capture the semantic consistency of the news content and the KB, the learned contextual entity representations are then compared to the KB-based entity representations, with a carefully designed entity comparison network. Finally, the obtained entity comparison features are combined with the news representation for an improved fake news classifier. Experiments on two benchmark datasets have demonstrated the effectiveness of the way we incorporate the external knowledge and topics.</p><p>In future work, we will explore a better way to combine multi-modal data (e.g., images) and external knowledge for fake news detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of directed heterogeneous document graph incorporating topics and entities.</figDesc><graphic url="image-1.png" coords="3,72.00,62.81,453.54,124.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The overview of our proposed model CompareNet.</figDesc><graphic url="image-2.png" coords="4,94.68,62.81,408.19,230.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 .</head><label>1</label><figDesc>There are three kinds of nodes in the graph: sentences S = {s 1 , s 2 , ‚Ä¢ ‚Ä¢ ‚Ä¢, s m }, topics T = {t 1 , t 2 , ‚Ä¢ ‚Ä¢ ‚Ä¢, t K } and entities E = {e 1 , e 2 , ‚Ä¢ ‚Ä¢ ‚Ä¢, e n }, i.e., V = S ‚à™ T ‚à™ E. The set of edges E represent the relations among sentences, topics and entities. The details of constructing the graph are described as follows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>where W o and b o are the parameter matrix and vection of a linear transformation. During model training, we exploit the cross-entropy loss over the training data with the L2-norm of the parameters:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Effect of top assigned topic number P .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the datasets. GN refers to Gigaword News.</figDesc><table><row><cell>Dataset</cell><cell>Trusted (#Docs)</cell><cell>Satire (#Docs)</cell><cell>Hoax (#Docs)</cell><cell>Propaganda (#Docs)</cell></row><row><cell cols="2">LUN-train GN except 'APW' and 'WPB' (9,995)</cell><cell>The Onion (14,047)</cell><cell cols="2">American News (6,942) Activist Report (17,870)</cell></row><row><cell>LUN-test</cell><cell>GN only 'APW' and 'WPB' (750)</cell><cell>The Borowitz Report, Clickhole (750)</cell><cell>DC Gazette (750)</cell><cell>The Natural News (750)</cell></row><row><cell>SLN</cell><cell>The Toronto Star, The NY Times (180)</cell><cell>The Onion, The Beaverton (180)</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4 ,</head><label>4</label><figDesc>we test the performance of CompareNet removing structured triplets, removing the entire external knowledge, removing topics, and removing both topics and external knowledge. In the last two rows, we further</figDesc><table><row><cell>Model</cell><cell>Micro</cell><cell></cell><cell>Macro</cell><cell></cell></row><row><cell></cell><cell>F1</cell><cell cols="2">Prec Recall</cell><cell>F1</cell></row><row><cell>CNN</cell><cell cols="4">67.50 67.79 67.50 67.37</cell></row><row><cell>LSTM</cell><cell cols="4">81.11 82.12 81.11 80.96</cell></row><row><cell>BERT+LSTM</cell><cell cols="4">75.83 76.62 75.83 75.65</cell></row><row><cell>BERT</cell><cell cols="4">84.16 84.73 84.16 84.10</cell></row><row><cell>(Rubin et al., 2016)</cell><cell>-</cell><cell cols="2">88.00 82.00</cell><cell>-</cell></row><row><cell>GCN + Max</cell><cell cols="4">85.83 86.16 85.83 85.80</cell></row><row><cell>GCN + Attn</cell><cell cols="4">85.27 85.59 85.27 85.24</cell></row><row><cell>GAT + Max</cell><cell cols="4">86.39 86.44 86.38 86.38</cell></row><row><cell cols="5">GAT + Attn (2019) 84.72 85.65 84.72 84.62</cell></row><row><cell>CompareNet</cell><cell cols="4">89.17 89.82 89.17 89.12</cell></row><row><cell cols="5">Table 2: 2-way classification results on SLN dataset.</cell></row><row><cell>Model</cell><cell>Micro</cell><cell></cell><cell>Macro</cell><cell></cell></row><row><cell></cell><cell>F1</cell><cell cols="2">Prec Recall</cell><cell>F1</cell></row><row><cell>CNN</cell><cell cols="4">54.03 54.50 54.03 52.60</cell></row><row><cell>LSTM</cell><cell cols="4">55.06 58.88 55.06 52.50</cell></row><row><cell>BERT+LSTM</cell><cell cols="4">55.56 57.45 54.86 54.00</cell></row><row><cell>BERT</cell><cell cols="4">64.66 60.89 64.46 58.80</cell></row><row><cell>(Rashkin et al., 2017)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>65.00</cell></row><row><cell>GCN + Max</cell><cell cols="4">65.00 66.75 64.84 63.79</cell></row><row><cell>GCN + Attn</cell><cell cols="4">67.08 68.60 67.00 66.42</cell></row><row><cell>GAT + Max</cell><cell cols="4">65.50 69.45 65.33 63.83</cell></row><row><cell>GAT + Attn (2019)</cell><cell cols="4">66.95 68.05 66.86 66.37</cell></row><row><cell>CompareNet</cell><cell cols="4">69.05 72.94 69.04 68.26</cell></row><row><cell cols="5">Table 3: 4-way classification results on LUN dataset.</cell></row><row><cell cols="5">examine the constructed directed heterogeneous</cell></row><row><cell cols="5">document graph and the designed entity compari-</cell></row><row><cell cols="5">son function. The variant CompareNet (undirected)</cell></row><row><cell cols="5">does not consider the edge directions of the directed</cell></row><row><cell cols="5">heterogeneous document graph. The variant model</cell></row><row><cell cols="5">CompareNet (concatenation) replaces the entity</cell></row><row><cell cols="5">comparison function as the simple concatenation</cell></row><row><cell cols="5">operation. As we can see from Table 4, removing</cell></row><row><cell cols="5">structural entity knowledge (i.e., w/o Structured</cell></row><row><cell cols="5">Triplets) leads to slight performance drop. If we re-</cell></row><row><cell cols="5">move the entire external knowledge (i.e., w/o Entity</cell></row><row><cell cols="5">Cmp), the performance decreases by around 1.3%</cell></row><row><cell cols="5">and 1.8% on micro F1 and macro F1, respectively.</cell></row><row><cell cols="5">Removing topics (i.e., w/o topics) will comparably</cell></row><row><cell cols="5">impair the performance, which shows that the topic</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Ablation study of modules.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>and threaten the natural health community‚Ä¶ the FDA could have illegitimately used it to target practically any company it wanted to.</head><label></label><figDesc></figDesc><table><row><cell>News</cell><cell>Entity Description</cell></row><row><cell cols="2">that may easily be misused by the FDA to target ‚Ä¶ The FDA is responsible for protecting and promoting public health through the control and</cell></row><row><cell></cell><cell>supervision of food safety, tobacco products,</cell></row><row><cell></cell><cell>dietary supplements ‚Ä¶</cell></row><row><cell>‚Ä¶ women referred to oncologists for treatment after mammograms did not actually have cancer. ‚Ä¶ mammograms are not effective at detecting breast tumors ‚Ä¶</cell><cell>Mammography is the process of using low-energy X-rays to examine the human breast for diagnosis and screening. The goal of mammography is the early detection of breast cancer ‚Ä¶</cell></row><row><cell cols="2">Figure 4: Two news examples from the LUN-test set.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>, we present two real news examples from the LUN-test set. The baseline model GAT+Attn and the variant model CompareNet w/o Entity Cmp mistakenly predict these two examples as trusted news, while our model CompareNet can successfully predict both of them. As we can see from Figure</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://en.wikipedia.org/wiki/Mammography</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://github.com/ytc272098215/FakeNewsDetection</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://sobigdata.d4science.org/group/tagme/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">https://en.wikipedia.org/wiki/Food and Drug Administration</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The work is supported by the National Natural Science Fundation of China (No. 61806020, U1936220, 61972047, 62076245) and the Microsoft Research Asia's Star Track project.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Social media and fake news in the 2016 election</title>
		<author>
			<persName><forename type="first">Hunt</forename><surname>Allcott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Gentzkow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of economic perspectives</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="211" to="236" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>David M Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-01">2003. Jan</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic deception detection: Methods for finding fake news</title>
		<author>
			<persName><forename type="first">Nadia</forename><forename type="middle">K</forename><surname>Conroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victoria</forename><forename type="middle">L</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yimin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Information Science and Technology</title>
				<meeting>the Association for Information Science and Technology</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
				<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Evaluating event credibility on twitter</title>
		<author>
			<persName><forename type="first">Manish</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peixiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth SIAM International Conference on Data Mining</title>
				<meeting>the Twelfth SIAM International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="153" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J√ºrgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Heterogeneous graph attention networks for semi-supervised short text classification</title>
		<author>
			<persName><forename type="first">Linmei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianchi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houye</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoli</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4821" to="4830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">News verification by exploiting conflicting social viewpoints in microblogs</title>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
				<meeting>the Thirtieth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2972" to="2978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mvae: Multimodal variational autoencoder for fake news detection</title>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Khattar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaipal</forename><surname>Singh Goud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manish</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasudeva</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2915" to="2921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Urja Khurana and Bachelor Opleiding Kunstmatige Intelligentie</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
		<respStmt>
			<orgName>University of Amsterdam</orgName>
		</respStmt>
	</monogr>
	<note>The linguistic features of fake news headlines and statements. Ph.D. thesis, Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A survey on natural language processing for fake news detection</title>
		<author>
			<persName><forename type="first">Ray</forename><surname>Oshikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<idno>ArXiv, abs/1811.00770</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Content based fake news detection using knowledge graphs</title>
		<author>
			<persName><forename type="first">Jeff</forename><forename type="middle">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyana</forename><surname>Pavlova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ningxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangmei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinshuo</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -ISWC 2018 -17th International Semantic Web Conference</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11136</biblScope>
			<biblScope unit="page" from="669" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A stylometric inquiry into hyperpartisan and fake news</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Kiesel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Reinartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janek</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="231" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Truth of varying shades: Analyzing language in fake news and political fact-checking</title>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Hannah Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><forename type="middle">Yea</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svitlana</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Volkova</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2931" to="2937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">√Ålvaro Ibrain Rodr√≠guez and Lara Lloret Iglesias</title>
		<idno>CoRR, abs/1910.03496</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Fake news detection using deep learning</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fake news or truth? using satirical cues to detect potentially misleading news</title>
		<author>
			<persName><forename type="first">Victoria</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niall</forename><surname>Conroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yimin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Cornwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Computational Approaches to Deception Detection</title>
				<meeting>the Second Workshop on Computational Approaches to Deception Detection</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="7" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improved semantic-aware network embedding with fine-grained word alignment</title>
		<author>
			<persName><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1829" to="1838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fakenewsnet: A data repository with news content, social context, and spatiotemporal information for studying fake news on social media</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Mahudeswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Big Data</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="171" to="188" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Dongwon Lee, and Huan Liu</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Beyond news contents: The role of social context for fake news detection</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining</title>
				<meeting>the Twelfth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="312" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Credibilitybased fake news detection</title>
		<author>
			<persName><forename type="first">Niraj</forename><surname>Sitaula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chilukuri</forename><forename type="middle">K</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Grygiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Zafarani</surname></persName>
		</author>
		<idno>CoRR, abs/1911.00643</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Some like it hoax: Automated fake news detection in social networks</title>
		<author>
			<persName><forename type="first">Eugenio</forename><surname>Tacchini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Ballarin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><forename type="middle">L Della</forename><surname>Vedova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Moret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>De Alfaro</surname></persName>
		</author>
		<idno>CoRR, abs/1704.07506</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">FEVER: a large-scale dataset for fact extraction and VERification</title>
		<author>
			<persName><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Christodoulopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arpit</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter</title>
				<meeting>the 2018 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="809" to="819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Do sentence interactions matter? leveraging sentence level representations for fake news classification</title>
		<author>
			<persName><forename type="first">Vaibhav</forename><surname>Vaibhav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghuram</forename><surname>Mandyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13)</title>
				<meeting>the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="134" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The spread of true and false news online</title>
		<author>
			<persName><forename type="first">Soroush</forename><surname>Vosoughi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deb</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinan</forename><surname>Aral</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">359</biblScope>
			<biblScope unit="issue">6380</biblScope>
			<biblScope unit="page" from="1146" to="1151" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">liar, liar pants on fire&quot;: A new benchmark dataset for fake news detection</title>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="422" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fake news detection via knowledge-driven multimodal graph convolutional networks</title>
		<author>
			<persName><forename type="first">Youze</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengsheng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 International Conference on Multimedia Retrieval</title>
				<meeting>the 2020 International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="540" to="547" />
		</imprint>
	</monogr>
	<note>Quan Fang, and Changsheng Xu</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Different absorption from the same sharing: Sifted multi-task learning for fake news detection</title>
		<author>
			<persName><forename type="first">Lianwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haolin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambreen</forename><surname>Nazir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4643" to="4652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fakedetector: Effective fake news detection with deep diffusive neural network</title>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th IEEE International Conference on Data Engineering</title>
				<meeting>the 36th IEEE International Conference on Data Engineering</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1826" to="1829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Reasoning over semantic-level graph for fact checking</title>
		<author>
			<persName><forename type="first">Wanjun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zenan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6170" to="6180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">GEAR: Graph-based evidence aggregating and reasoning for fact verification</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="892" to="901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A survey of fake news: Fundamental theories, detection methods, and opportunities</title>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Zafarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="40" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
