<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Texture Classification Using Spectral Histograms</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Xiuwen</forename><surname>Liu</surname></persName>
							<email>liux@cs.fsu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Florida State University</orgName>
								<address>
									<postCode>32306-4530</postCode>
									<settlement>Tallahassee</settlement>
									<region>FL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Deliang</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
							<email>dwang@cis.ohio-state.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Florida State University</orgName>
								<address>
									<postCode>32306-4530</postCode>
									<settlement>Tallahassee</settlement>
									<region>FL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Center for Cognitive Science</orgName>
								<orgName type="institution">The Ohio State University</orgName>
								<address>
									<postCode>43210</postCode>
									<settlement>Columbus</settlement>
									<region>OH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Josiane</forename><forename type="middle">B X</forename><surname>Zerubia</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer and Information Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer and Information Science</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Texture Classification Using Spectral Histograms</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CBD7FE5CCF304492EEF9D69A3935B9B2</idno>
					<idno type="DOI">10.1109/TIP.2003.812327</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Filtering</term>
					<term>spectral histogram</term>
					<term>texture analysis</term>
					<term>texture classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Based on a local spatial/frequency representation,we employ a spectral histogram as a feature statistic for texture classification. The spectral histogram consists of marginal distributions of responses of a bank of filters and encodes implicitly the local structure of images through the filtering stage and the global appearance through the histogram stage. The distance between two spectral histograms is measured using 2 -statistic. The spectral histogram with the associated distance measure exhibits several properties that are necessary for texture classification. A filter selection algorithm is proposed to maximize classification performance of a given dataset. Our classification experiments using natural texture images reveal that the spectral histogram representation provides a robust feature statistic for textures and generalizes well. Comparisons show that our method produces a marked improvement in classification performance. Finally we point out the relationships between existing texture features and the spectral histogram, suggesting that the latter may provide a unified texture feature.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T EXTURE classification is a fundamental problem in com- puter vision with a wide variety of applications <ref type="bibr" target="#b39">[40]</ref>. Two fundamental issues in texture classification are how to characterize textures using derived features and and how to define a robust distance/similarity measure between textures, which remain elusive despite considerable efforts in the literature <ref type="bibr" target="#b33">[34]</ref>. Because images of the same underlying texture can vary significantly, textural features must be invariant to (large) image variations and at the same time sensitive to intrinsic spatial structures that define textures. Because there is no obvious feature common for all texture images, texture features are often proposed based on assumptions for mathematical convenience or task-specific heuristics (see <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b39">[40]</ref> for reviews). For example, geometric properties based on the texture elements are often used for textures with periodic structures <ref type="bibr" target="#b38">[39]</ref>. Early features including cooccurrence matrices <ref type="bibr" target="#b16">[17]</ref> and Markov random field models <ref type="bibr" target="#b6">[7]</ref> have limited expressive power because the analysis of spatial interaction is limited to a relatively small neighborhood. As a result, the adequacy of these features for characterizing various textures is rarely checked. On the other hand, studies on the human visual system suggest that it transforms a retinal image into a local spatial/frequency representation <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b9">[10]</ref>, which can be computationally simulated by convolving the input image with a bank of filters with tuned frequencies and orientations. The mathematical framework for the local spatial/frequency representation was laid out by Gabor <ref type="bibr" target="#b12">[13]</ref> and extended by Daughman <ref type="bibr" target="#b7">[8]</ref>. Recently, this theory has also been confirmed by deriving similar feature detectors from natural images <ref type="bibr" target="#b32">[33]</ref>. These advances have inspired much research in texture classification based on filtering (see <ref type="bibr" target="#b33">[34]</ref> for a review). In this framework, a texture image is transformed into feature vectors by filtering the input image using a bank of filters, followed by some nonlinearity and smoothing steps <ref type="bibr" target="#b33">[34]</ref>. The nonlinearity is necessary for texture classification, since, otherwise, filter responses cannot discriminate textures with the same mean intensity (see, e.g., <ref type="bibr">Malik</ref> and Perona <ref type="bibr" target="#b28">[29]</ref>); the smoothing is necessary since the filter responses are not homogeneous even within a homogeneous texture region. While the nonlinearity and smoothing steps are critical for texture classification, current research focuses instead on the filtering stage, i.e., deriving optimal filters for texture classification based on certain optimization criteria. As a result, while both the theoretical and numerical aspects of filter design for texture classification are well studied <ref type="bibr" target="#b29">[30]</ref>, the recent comprehensive study by Randen and Husoy <ref type="bibr" target="#b33">[34]</ref> showed that the texture classification performance is very limited for real textures. This study clearly leads to the need for studying statistic features beyond the filtering stage for texture classification.</p><p>Recently, Heeger and Bergen <ref type="bibr" target="#b17">[18]</ref> proposed a texture synthesis algorithm that can match texture appearance. The algorithm transforms a random noise image into one with similar appearance to a given target image by matching independently the histograms of image pyramids constructed from the random and target images. The success of synthesizing natural textures based on histograms has motivated considerable research <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>. Zhu et al. <ref type="bibr" target="#b43">[44]</ref> proposed a theory for learning probability models by matching histograms based on a maximum entropy principle. Zhu et al. <ref type="bibr" target="#b42">[43]</ref> studied efficient sampling algorithms for matching histograms. While these synthesis methods provide features to characterize a single texture, the effectiveness of these features for texture classification is not known as a good synthesis model does not imply a good classification model (see Section III). Also, while these synthesis methods are proposed to model homogeneous textures, natural textures are rarely homogeneous due to deformations and other variations; these variations require a robust distance measure between textures so that the distance between images of the same texture is small and that among images from different textures is large. Furthermore, as the features depend on the choice of filters, there is no systematic algorithm to choose filters for texture classification. In addition, texture classification is often done based on rela-tively small image windows and the effect of the window size on histogram-based representations needs to be studied.</p><p>Motivated by the research on texture synthesis, we propose a local spectral histogram, consisting of marginal distributions of responses from a bank of filters for an image window, as a feature statistic for texture classification. We define a distance between two image windows as the -statistic of their spectral histograms, which exhibits nonlinearity that is consistent with the human texture perception <ref type="bibr" target="#b26">[27]</ref>. Our work presented elsewhere <ref type="bibr" target="#b26">[27]</ref> demonstrates that this proposed model provides a satisfactory account for a systematic set of human texture discrimination data. We propose a filter selection algorithm to optimize the classification performance of a given dataset. We report that the spectral histogram produces good classification results. A systematic comparison with other methods documented in <ref type="bibr" target="#b33">[34]</ref> demonstrates that our approach yields far better results.</p><p>This paper is organized as follows. Section II introduces the local spectral histogram model and discusses its properties for texture classification. Section III presents a filter selection algorithm and shows the classification results on a natural texture dataset. Section IV compares our method with existing approaches. Section V discusses the relations of the spectral histogram model with previous ones and some related issues. Section VI concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. LOCAL SPECTRAL HISTOGRAM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Definition</head><p>Given an image window and a bank of filters , <ref type="foot" target="#foot_0">1</ref> we compute, for each filter , a sub-band image through linear convolution. <ref type="foot" target="#foot_1">2</ref> That is, , at pixel location , where a circular boundary condition is used. For , we define the marginal distribution, or histogram as <ref type="bibr" target="#b0">(1)</ref> where is the Dirac delta function. We then define the spectral histogram with respect to the chosen filters as <ref type="bibr" target="#b1">(2)</ref> Here, the concatenation of different histograms assumes the independence among different filters; under the independence assumption, the distance between two image windows is simplified to the sum of the distance between the corresponding histograms of each filter as shown in (3). This is justified for natural images since edge-like filters are empirically shown to be the independent components of natural images <ref type="bibr" target="#b1">[2]</ref>. The spectral histogram of an image or an image window is essentially a vector consisting of marginal distributions of filter responses and integrates responses of different filters to form a texture feature. The size of the input image window is called integration scale. Because the marginal distribution of each filter response is a probability distribution, we define a distance between two spectral histograms and as</p><formula xml:id="formula_0">(3)</formula><p>where -statistic is a first-order approximation of the Kullback-Leibler divergence and used widely to compare histograms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Properties</head><p>The spectral histogram integrates responses of different filters and provides a normalized feature statistic to compare images of different sizes. Some of its properties are discussed below.</p><p>Property 1: A spectral histogram is translation invariant. This property is easy to see from the definition of the spectral histogram. Because filter responses depend only on relative locations of pixels, the absolute position of an image window does not affect its spectral histogram. This is essential for any texture model to characterize texture appearance.</p><p>Property 2: A spectral histogram is a nonlinear operator.</p><p>In other words, the spectral histogram of images does not satisfy the linearity conditions due to the histogram operation given in <ref type="bibr" target="#b0">(1)</ref>. As discussed earlier, some form of nonlinearity is necessary for texture classification and the intrinsic nonlinearity of the spectral histogram model makes an additional nonlinearity step not needed while it is necessary for other filtering-based methods <ref type="bibr" target="#b33">[34]</ref>. For nonconstant filters, the nonlinearity of the spectral histogram can also be caused by the dependency among pixel values, and this makes the spectral histogram sensitive to texture structures.</p><p>Property 3: With sufficient filters, a spectral histogram can uniquely represent any image up to a translation.</p><p>To show this, let be an image defined on a finite lattice . If , , the proposition holds. Assume that , we choose two filters, the intensity filter and , , where . It is sufficient to show that , an image defined on the finite lattice , is equivalent to up to a translation if and , . If , must be a permutation of in terms of the group of pixels. For , the maximum response of the filter is bounded due to Cauchy-Schwartz' inequality. The maximum is achieved when . Similarly, if is a permutation of and , , must be equivalent to up to a translation to achieve the same maximum response.  While this property does not provide a practical way to choose filters, it shows that the marginal distributions are sufficient to characterize the joint distributions implicitly defined by an image and thus justifies the sufficiency of using only marginal distributions in the spectral histogram. In practice, this property is approximated by using a large number of filters. Intuitively, each filter provides some constraints on the images that can satisfy . With constraints imposed by sufficiently many filters, the solution to the equation would converge to the observed image up to a translation. Fig. <ref type="figure" target="#fig_0">1</ref> gives an illustration. With more filters, the images that share the observed spectral histogram become more similar to the observed.</p><p>Property 4: All the images sharing a spectral histogram define an equivalent class.</p><p>Essentially, spectral histograms divide all the images into equivalent classes <ref type="bibr" target="#b42">[43]</ref>. Extensive simulations suggest that the spectral histogram is sufficient in characterizing texture appearance <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b42">[43]</ref> when filters are chosen properly. In other words, all the images with the same spectral histogram are perceptually similar in that perceptually similar textures are synthesized by matching the spectral histogram. The top row of Fig. <ref type="figure" target="#fig_1">2</ref> shows four texture images and the bottom row shows corresponding typical images by satisfying the constraints , where is an image, its spectral histogram, and the spectral histogram of the observed image. Due to the high dimensionality of , the constraints have to be satisfied through stochastic simulation because traditional deterministic searching methods are computationally not feasible. These examples shown in Fig. <ref type="figure" target="#fig_1">2</ref> were generated using a Gibbs sampler <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b42">[43]</ref>. In Fig. <ref type="figure" target="#fig_1">2</ref>(a), the spectral histogram captures the perceptual appearance of both regions. Given that the circular boundary is used, the synthesized image matches closely the observed image. Fig. <ref type="figure" target="#fig_1">2</ref>(b) shows a synthetic texture image, where the spectral histogram captures the texture element and its density. Fig. <ref type="figure" target="#fig_1">2(c</ref>) and (d) show that the spectral histograms of two stochastic textures capture their perceptual appearance well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Issues</head><p>Because a spectral histogram is defined with respect to a bank of filters, the first implementation issue is what filters should be used so that various textures can be modeled effectively. Here we use four different types of filters suggested from the studies of visual perception and the empirical studies of independent components of natural images <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b43">[44]</ref>.</p><p>1) The intensity filter, which is the function and captures the intensity value at a given pixel.</p><p>2) Difference or gradient filters. We use four of them: , , , .</p><p>3) Laplacian of Gaussian filters:</p><p>where determines the scale of the filter and is the variance of the Gaussian function. These filters are referred to as . 4) The Gabor filters with both sine and cosine components: <ref type="bibr" target="#b4">(5)</ref> where is a scale. The cosine and sine components of these filters are referred to as and , respectively. These filters provide efficient ways of extracting spatial structures at different orientations and frequencies and empirically have shown to be effective for different kinds of textures (see Fig. <ref type="figure" target="#fig_1">2</ref> for some examples). Given these families of filters, the optimal ones for classification of a given set of textures depend on the characteristics of the input textures; they are selected by a filter selection algorithm presented in Section III-A.</p><p>Another implementation issue is how to quantize and estimate the histogram of a filtered image. In theory, the histogram is an approximation of the underlying distribution of filter responses. With sufficient data and sufficient number of histogram bins, the histogram can represent the underlying distribution with arbitrary accuracy. For texture classification, the integration scale we use is often small and only a limited number of samples is available to compute the histogram. In order to have a good approximation of the underlying distribution, we have to choose the number of bins and where the bins should be. In our implementation, the number of bins is specified as a parameter for each filter. When the number of bins is given, we find the average of minimum and maximum filter responses from training images and divide the filter response range into the given number of bins evenly. Given the histogram bin number and bin ranges, a direct implementation of (1) can have a large variance, which can cause a large error for the -distance between marginal distributions. To overcome this problem, we use Parzen windows <ref type="bibr" target="#b10">[11]</ref> to estimate the marginal distribution based on the filter responses, given by <ref type="bibr" target="#b5">(6)</ref> Here, is a kernel function (Gaussian kernel is used in this paper), and and are the minimum and maximum values of the given bin. While (6) provides a better estimate of the marginal distribution, it is computationally expensive. We approximate ( <ref type="formula">6</ref>) by the following two steps. First we sample the function at the middle value of each bin and then smooth the samples. The smoothing is implemented by applying a small Gaussian kernel for a specified number of times determined by the scale of the Gaussian kernel. This provides a more reliable estimation of the marginal distribution and thus more accurate -distance between two spectral histograms.</p><p>Alternatively, one can adopt a parametric model and then estimate the model parameters from the training samples. A particular two-parameter distribution model <ref type="bibr" target="#b15">[16]</ref> provides a good approximation for histograms of filter responses for a variety of real images, which leads to analytical probability models of natural images <ref type="bibr" target="#b36">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. TEXTURE CLASSIFICATION</head><p>We apply spectral histograms and -statistic as a distance measure to texture classification. Given a database with texture types, we represent each type by the average spectral histogram of available training samples, defined as <ref type="bibr" target="#b6">(7)</ref> where is a training sample of texture type and is the total number of training samples for type . Because our primary goal is to demonstrate the effectiveness of the spectral histogram as a texture feature, we use a minimum-distance classifier for a new sample , given by ( <ref type="formula">8</ref>)</p><p>Other classification approaches can also be applied <ref type="bibr" target="#b10">[11]</ref> and issues related to the choice of classifiers are not discussed in this paper. In the spectral histogram framework, to measure the gain using a particular set of filters , we define classification gain as <ref type="bibr" target="#b8">(9)</ref> where is the classification error rate, is the total number of classes in the database, and thus is the expected correct classification rate based on a random decision. measures the effectiveness of filters in more objectively than because is closely related to . Here we use Fig. <ref type="figure">3</ref>. Filter selection algorithm. Here, B is the subset of the filters that has not been chosen, S is the subset that has been chosen, and a threshold. Initially B consists of all the available filters and S an empty set.</p><p>to emphasize that the classification gain depends on the filters in . Because the spectral histogram representation depends critically on the filters used, we present our filter selection algorithm first and then our classification results on a natural texture dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Filter Selection</head><p>As demonstrated in Fig. <ref type="figure" target="#fig_0">1</ref>, the particular set of images that is characterized by a spectral histogram critically depends on the filters used. In one extreme, no filter is used and all images are admitted in the set. In the other extreme, when infinitely many filters are used, only the original image and its translations are admitted. In addition, for a given set of textures, some of the filters are more effective than others. To address these issues, we propose a method that selects a subset of filters from a large filter bank by optimizing the classification performance of training samples <ref type="bibr" target="#b40">[41]</ref>. To estimate the classification performance, we divide the available samples into a training set (used to train the chosen classifier), and a validation set (used to estimate the performance), known as cross-validation <ref type="bibr" target="#b10">[11]</ref>. Specifically, for a given set of filters , we calculate using <ref type="bibr" target="#b6">(7)</ref> for each texture class using the samples in and then we classify the samples in with ( <ref type="formula">8</ref>) and calculate the classification gain using <ref type="bibr" target="#b8">(9)</ref>. To be computationally efficient, we use a greedy algorithm. In other words, we choose filters one by one so that the next one has the maximum with the ones already chosen. The filter selection algorithm is shown in Fig. <ref type="figure">3</ref>. The computational complexity of the algorithm depends on the complexity of calculating and (the number of filters in ). For the minimum distance classifier used in this paper, the complexity is , where is the time to compute , and to compute given . To demonstrate the effectiveness of the filter selection algorithm, we use a texture database that consists of 40 Brodatz texture images, 10 of which are shown in Fig. <ref type="figure" target="#fig_2">4</ref>. Initially there are 40 filters. Table <ref type="table" target="#tab_0">I</ref> shows the classification gain along with the computation time for classification with respect to the number of filters. Here, the computation time is the time for classifying test images relative to that of using only the intensity filter. For texture classification, it is often desirable to use only derivative filters. The last three columns in Table <ref type="table" target="#tab_0">I</ref> show the result without the intensity filter. As Table <ref type="table" target="#tab_0">I</ref> shows, the filter selection algorithm essentially chooses the most effective filters. The row with a star shows the filters chosen automatically for optimal classification, whose performances are better than those of all the available filters. This illustrates clearly a key difference between texture classification and texture synthesis. As Fig. <ref type="figure" target="#fig_1">2</ref> shows, more filters clearly give better synthesis results. However, more filters may not improve the classification performance and in fact may give worse performance. In this particular case, while the difference in classification gain between the optimal choice and all the available filters is not significant, the difference in computation time is very significant, which can be critical for some applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Classification Experiments</head><p>We apply our classification method to the 40-texture dataset shown in Fig. <ref type="figure" target="#fig_2">4</ref>. This dataset is challenging because there are significant variations within some textures and some of them are very similar to each other. At a given integration scale, we partition the images into nonoverlapping samples, which are then divided into disjoint training and testing set. Here seven filters including the intensity, , , , , , and , are selected automatically by the filter selection algorithm (see Table <ref type="table" target="#tab_0">I</ref>) and are used to compute the spectral histogram.</p><p>Fig. <ref type="figure" target="#fig_3">5</ref>(a) shows the classification gain with respect to the integration scale on the 40-texture dataset using the seven filters. To avoid the bias due to the particular images in the training and test set, we randomly divide the total samples into disjoint training and test set and we repeat the classification experiment 100 times and collect the average, best, and worst performance. Here 1/2 of the available samples are used for for training and the remaining ones for testing. This result shows several important aspects of texture classification. 1) It shows clearly that a reasonable integration scale is needed in order to discriminate textures as the texture structures have certain spatial extent. Here the average classification gain at integration scale 8 8 is 21.34 (corresponding to a classification error of 46.64%), and it improves to 35.86 (corresponding to a classification error of 10.35%) at integration scale 24 24. 2) Given a reasonable intergration scale, the spectral histogram provides a robust feature statistic for classification. The average classification gain at integration scales of 32 32 or larger is better than 37 (corresponding to an error less than 7.5%). If we allow the correct one within the three closest classes, the classification error is less than 1% for all the 100 trials. Given the significant variations within textures and similarities between textures, the performance is sigficantly better than exisiting filter-based methods (see Section IV for comparisons). 3) It shows also that the spectral histogram is not sensitive to particular images in the training and test sets as the best and worst are close to the average of 100 trials. At integration scale 24 24, the average gain is 35.86, the worst 35.50, and the best 36.36.   To demonstrate the generalization capability of the spectral histogram, Fig. <ref type="figure" target="#fig_3">5(b)</ref> shows the classification gain at two integration scales with respect to the test-to-training sample ratio. In both cases, the classification gain does not change much for ratios between 1:1 and 12:1. This confirms the generalization capability of spectral histograms in characterizing texture images.</p><p>To provide numerical justifications of the proposed representation, we have compared the spectral histogram with other commonly used features and distance measures. Fig. <ref type="figure" target="#fig_4">6(a)</ref> shows the classification gain for features commonly used for intensity images, including the mean value, combination of mean and variance values, and intensity histogram. As we can see from Fig. <ref type="figure" target="#fig_4">6</ref>(a), the mean and Gaussian models are not sufficient for characterizing those images and generate worst results. The comparison shows that the smoothing stage commonly used in texture classification methods <ref type="bibr" target="#b33">[34]</ref> is not optimal; the distribu-Fig. <ref type="figure">7</ref>. Ten-texture image groups used in <ref type="bibr" target="#b33">[34]</ref>. The image size is 128 2 128. (a) The images in Fig. <ref type="figure" target="#fig_0">11</ref>(h) of <ref type="bibr" target="#b33">[34]</ref>. (b) The images in Fig. <ref type="figure" target="#fig_0">11</ref>(i) of <ref type="bibr" target="#b33">[34]</ref>. tion of local features is far more effective in discriminating textures. This is also consistent with the comparisons shown in the next section. Fig. <ref type="figure" target="#fig_4">6</ref>(b) compares several commonly used distance measures for histograms, including norm, norm, Kullback-Leibler divergence, and -statistic. For texture classification using spectral histograms, Fig. <ref type="figure" target="#fig_4">6</ref>(b) shows that different measures give very similar results, suggesting that spectral histograms is insensitive to a particular form of distance measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. COMPARISON WITH EXISTING APPROACHES</head><p>Several comparative studies about texture features have been conducted. Ohanian and Dubes <ref type="bibr" target="#b30">[31]</ref> studied the performance of various texture features, including fractal features, cooccurrence features, Markov random field features, and Gabor features. However, the evaluation was done only on four classes of images and the conclusion may not be generalized. Ojala et al. <ref type="bibr" target="#b31">[32]</ref> did a similar study on joint occurrences of feature pairs using nine texture images and the ones in <ref type="bibr" target="#b30">[31]</ref>. Recently, Randen and Husoy <ref type="bibr" target="#b33">[34]</ref> did an extensive comparative study for texture classification on cooccurrence methods, Law's texture measures, different filtering-based methods, and a neural network approach <ref type="bibr" target="#b18">[19]</ref>. They used a supervised classifier of Kohonen <ref type="bibr" target="#b22">[23]</ref> for most of their experiments. The filter responses at each pixel form a vector and the texture classification is to classify feature vectors. Because filters have a spatial extent, the receptive field of a vector overlaps heavily with the neighboring ones. We have applied our method to the same images with the same experimental settings. We use an integration scale 32 32 and the filters are chosen automatically from the 40 filters using the filter selection algorithm. We use a separate set of images for training and a separate set of images for testing as in <ref type="bibr" target="#b33">[34]</ref>. <ref type="foot" target="#foot_2">3</ref> The results for the two most challenging groups of texture images, shown in Fig. <ref type="figure">7</ref>(a) and (b), are summarized in Table <ref type="table" target="#tab_0">II</ref>, where the average performance and the best in tables 3, 6, 8, and 9 in <ref type="bibr" target="#b33">[34]</ref> are shown. For these two groups, due to the inhomogeneity and large variations, texture types in local windows given by the integration scale are perceptually close and they require a very accurate texture model for classification. In addition, separate images are used for training and this creates additional difficulties for methods that cannot generalize well to new data. The classification gains of all the methods studied in <ref type="bibr" target="#b33">[34]</ref> are shown in Fig. <ref type="figure">8</ref>(a) and (b). Our method is significantly better than the best performance in <ref type="bibr" target="#b33">[34]</ref>. Furthermore, the most errors of our method are from the texture pairs that are perceptually very similar. If we consider the two closest textures as correct, our method gives a classification gain of 9.50 and 9.58 respectively, corresponding to 95.0% and 95.8% correct classification rate. This comparison clearly suggests that classification based on filtering output is inadequate for characterizing texture appearance and an integration after filtering must be done. Our comparison strongly indicates that some representation like the Fig. <ref type="figure">8</ref>. The classification gain for all the methods in <ref type="bibr" target="#b33">[34]</ref> for Fig. <ref type="figure">7</ref>(a) and (b) respectively. In each plot, each data point represents one result in tables 3, 6, 8, and 9 of <ref type="bibr" target="#b33">[34]</ref> and the dashed line is the result of the proposed classification method. <ref type="bibr" target="#b33">[34]</ref> AND THE PROPOSED METHOD spectral histogram may be necessary in order to capture complex texture appearance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE II CLASSIFICATION GAINS OF METHODS SHOWN IN</head><p>To further illustrate our method, we have done a comparison with a method proposed by Azencott et al. <ref type="bibr" target="#b0">[1]</ref>. In <ref type="bibr" target="#b0">[1]</ref>, a texture feature vector is proposed based on the spectral density of windowed Fourier filters, e.g., Gabor filters, and a distance between two textures is defined as a symmetrized Kullback distance between computed vectors. A minimum distance classifier is also used for texture classification. For an unbiased comparison, we use the same settings used in <ref type="bibr" target="#b0">[1]</ref>. Each input texture image with the size of 128 128 is divided in 49 image patches with size 32 32 and thus adjacent patches are overlapped. We use the same seven filters as in the previous section to compute spectral histogram. The 16 texture images used in <ref type="bibr" target="#b0">[1]</ref> are shown in Fig. <ref type="figure" target="#fig_5">9</ref>; therefore there are 784 image patches in total.</p><p>Two classification experiments were reported in <ref type="bibr" target="#b0">[1]</ref>. In the first experiment, the 49 patches of each image were divided into a training set of 21 patches and a test set of 28 patches. The result in <ref type="bibr" target="#b0">[1]</ref> gives six misclassified patches, i.e., 1.34% classification error. For the same setting, our method gives only 1 misclassified patch, resulting in 0.22% classification error. In the second experiment, the training set is reduced to one patch per texture image. The result in <ref type="bibr" target="#b0">[1]</ref> using the Kullback distance gives twenty-three misclassified patches. Our result gives only four misclassified patches. This comparison demonstrates the superior discrimination ability of the spectral histogram.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Relations to Existing Approaches</head><p>This paper focuses on texture classification using spectral histograms with a fixed set of filters. As we mentioned earlier, one can choose different filters to define different features. In this section, we point out the relations between the spectral histogram and other existing methods.</p><p>Before we discuss specific features for textures, we point out that uniform regions are simply a special case under the spectral histogram, thus the spectral histogram provides a unified feature for texture as well as nontexture images. However, textures are often studied separately from intensity images and texture features from other approaches may not be applicable to uniform images <ref type="bibr" target="#b5">[6]</ref>.</p><p>Texture analysis has been studied extensively and many methods have been proposed. Tuceryan and Jain <ref type="bibr" target="#b39">[40]</ref> classified existing approaches into four categories, namely statistical methods, geometrical methods, model based methods, and signal processing methods. We discuss the relationships between each category and our proposed method.</p><p>Statistical methods, including cooccurrence matrices <ref type="bibr" target="#b16">[17]</ref>, autocorrelation features <ref type="bibr" target="#b39">[40]</ref>, and our method here, are based on the observation that a texture is defined by the spatial distribution of gray values. A cooccurrence matrix consists of the number of occurrences of a gray level pair with a specified distance apart. This can be viewed as a special case of -gon statistics proposed by Julesz <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref>. Because the cooccurrence matrix cannot be used directly as texture features, a number of texture features were subsequently computed from the cooccurrence matrix <ref type="bibr" target="#b16">[17]</ref>. It is easy to see that the cooccurrence matrix can also be defined as responses of a specifically designed gradient filter and thus a spectral histogram using gradient filters provides cooccurrence matrix features.</p><p>The class of geometrical methods is based on the assumption that a texture consists of repeated texture elements, such as the one shown Fig. <ref type="figure" target="#fig_1">2(b</ref>). After the texture elements are identified, geometrical properties of the element distribution can be used to characterize textures <ref type="bibr" target="#b39">[40]</ref>. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>(b), the spectral histogram can characterize the texture element as well as its distribution without knowing the texture element. This provides a more elegant way to characterize textures with repeated texture elements.</p><p>Model based methods include Markov random fields <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b43">[44]</ref>. This class of methods can not only describe the texture through model parameters, which are learned from observed textures, but also synthesize it through sampling. In <ref type="bibr" target="#b43">[44]</ref>, for example, Zhu et al. proposed a FRAME model and showed that the model provides a unified framework for Markov random field models. In a limiting case, Wu et al. <ref type="bibr" target="#b41">[42]</ref> proved the equivalence of a model specified by features such as spectral histogram <ref type="bibr" target="#b42">[43]</ref> and a Gibbs model, a special case of which is shown in <ref type="bibr" target="#b11">[12]</ref>. This relation shows that the spectral histogram provides an equivalent way of specifying a Markov random field, which avoids the parameter learning necessary for a Markov random field model.</p><p>Signal processing methods try to characterize textures by filter responses directly. Many of these models have been studied and compared in <ref type="bibr" target="#b33">[34]</ref>, including Laws filters, ring and wedge filters, Gabor filters, wavelet transforms, packets, frames, discrete cosine transforms, quadrature mirror filters, and a number of optimized filters for texture classification (see the references wherein). Even though the filters in many of those approaches were carefully designed and chosen, our comparison shows that this class of methods is inadequate to characterize and discriminate texture structures. This demonstrates that an integration of different filter responses such as the spectral histogram proposed here, is probably necessary while the specific form of filters is not critical <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Integration of Filter Responses</head><p>It is easy to see that a filter's response is inhomogeneous even to a homogeneous texture image. An inevitable issue common to all filter-based approaches is to form a feature which characterizes a texture region. To reduce the inhomogeneity of filter responses, spatial smoothing is commonly used <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b33">[34]</ref>. The proposed spectral histogram model resolves this issue using histograms of filter responses within a spatial window. For a spatial window substantially larger than the size of basic elements in a texture, the spectral histogram is intrinsically insensitive to precise locations of texture elements. This is consistent with a study on human texture discrimination <ref type="bibr" target="#b20">[21]</ref>. Because of this property, two images do not need to be aligned in order to be compared using spectral histograms. More importantly, because of the stochastic nature of textures, images of the same texture type may not be aligned, an example of which is shown in Fig. <ref type="figure" target="#fig_1">2(b</ref>). While both images in Fig. <ref type="figure" target="#fig_1">2</ref>(b) consist of crosses with similar distribution, two images cannot be aligned under simple transforms. The misalignment of textures can be a serious problem for approaches that use filter responses directly as features for texture classification, such as those studied in <ref type="bibr" target="#b33">[34]</ref>.</p><p>Note that the spectral histogram is defined on any type of images. Piece-wise constant images with additive Gaussian noise are a special case whose spectral histogram has a unique pat-tern. Under the spectral histogram representation, the distinction between texture and nontexture images becomes unnecessary. While the spectral histogram here is used primarily for textures with roughly repeated patterns, our study elsewhere suggests that the spectral histogram can also be applied to classify faces and 3-D objects <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, consistent with a recent study on object recognition using multidimensional histograms <ref type="bibr" target="#b35">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>We have demonstrated that the spectral histogram provides a sufficient feature statistic for texture classification. The -statistic between spectral histograms provides a robust distance measure for comparing textures. We have proposed a filter selection algorithm for texture classification. With a wide range of integration scales and test-to-training ratios, we have obtained satisfactory classification results on natural texture datasets. Our comparison shows that the spectral histogram improves the classification performance significantly. By pointing out the relations between existing texture features and the spectral histogram, we suggest that the latter may provide a unified image feature statistic.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. A typical image that satisfies H = H with different filters. The size of the images is 128 2 128. Here the filters correspond to the ones in Table I for texture classification. (a) A texture image. (b) Three filters. (c) Five filters. (d) Seven filters. (e) Forty filters. (f) Three filters without the intensity filter. (g) Six filters without the intensity filter. (h) Thirty-nine filters without the intensity filter.</figDesc><graphic coords="3,109.74,62.25,370.80,226.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Different types of images characterized by spectral histograms. Top row shows observed images and the bottom row the corresponding typical image that shares the same spectral histogram. (a) A gray-level image consisting of two piece-wise constant regions with additive Gaussian noise. (b) A synthetic texture consisting of cross elements. (c) A stochastic image. (d) An image with periodic structures.</figDesc><graphic coords="3,109.74,339.55,370.80,206.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Ten of the 40 textures used in the classification experiments. The input image size is 256 2 256. These images are available at http://www-dbv.cs. uni-bonn.de/image/texture.tar.gz.</figDesc><graphic coords="6,109.44,62.27,374.40,149.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Classification result for the 40-texture dataset. (a) The average (solid), best (dash-dotted) and worst (dotted) classification gain of 100 trials with randomly divided training and test set with respect to the integration scale. (b) The classification gain with respect to test-to-training sample ratio. Solid line-integration scale 32 2 32; dashed line-integration scale 24 2 24.</figDesc><graphic coords="6,39.60,462.66,251.88,117.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Comparison of different features and distance measures of the 40 texture dataset. (a) Classification gain for different features. Dashed lineintensity mean; dash-dotted line-intensity mean and variance; dotted lineintensity histogram; solid line-spectral histogram of the seven filters. (b) Classification gain for commonly used distance measures for histograms. Solid line--statistic; dotted line-L -norm; dashed line-L -norm; dashdotted-Kullback-Leibler divergence.</figDesc><graphic coords="6,302.64,462.66,251.88,117.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 .</head><label>9</label><figDesc>Fig.9. Sixteen texture images used in<ref type="bibr" target="#b0">[1]</ref>. Images in the first row are generated from Gaussian random fields, and remaining rows are from the Brodatz album.The image size is 128 2 128.</figDesc><graphic coords="8,303.00,62.27,250.36,258.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="7,134.94,62.30,320.40,307.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I CLASSIFICATION</head><label>I</label><figDesc>GAIN AND COMPUTATION TIME WITH RESPECT TO FILTERS CHOSEN BY THE FILTER SELECTION ALGORITHM</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We require that jF (ũ)j &gt; 0 for any . In other words, F must have some nonzero coefficients.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>In this work, we restrict the definition of the spectral histogram to linear filters, even through nonlinear filters such as the power spectrum of filter pairs can also be included.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>In<ref type="bibr" target="#b33">[34]</ref>, windows that include different texture types are used to test texture boundary accuracy of classification methods. Due to the uncertainty principle<ref type="bibr" target="#b4">[5]</ref>, there is an intrinsic tradeoff between classification performance and boundary accuracy. Since a considerable integration scale is needed to characterize a texture (see Figs.5 and 6), test windows are confined within a single texture here. With spectral histogram representations, the boundary localization is achieved by building a more accurate probability model after initial classification<ref type="bibr" target="#b27">[28]</ref>.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank the anonymous reviewers whose comments have improved the presentation of this paper significantly and acknowledge the reviewer who suggested the cross-validation technique used in the present filter selection algorithm.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by NIMA grant (NMA201-01-2010) to X. Liu, an ONR Young Investigator Award (N00014-96-1-0676), an NSF grant (IIS-0081058), and an AFOSR grant (F49620-01-1-0027) to D. L. Wang. The associate editor coordinating the review of this manuscript and approving it for publication was Dr.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Texture classification using windowed Fourier filters</title>
		<author>
			<persName><forename type="first">R</forename><surname>Azencott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Younes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Recognit. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="148" to="153" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The &apos;independent components&apos; of natural scenes are edge filters</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Res</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="3327" to="3338" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Early vision and texture perception</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Bergen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">333</biblScope>
			<biblScope unit="page" from="363" to="364" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Application of Fourier analysis to the visibility of gratings</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">W</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Robson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Physiol. (Lond.)</title>
		<imprint>
			<biblScope unit="volume">197</biblScope>
			<biblScope unit="page" from="551" to="566" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="679" to="698" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Texture segmentation using Gaussian-Markov random fields and neural oscillator networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cesmeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="394" to="404" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Markov random field texture models</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Recognit. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="39" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Uncertainty relation for resolution in space, spatial frequency, and orientation optimized by two-dimensional visual cortical filters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Daugman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Amer. A</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="23" to="26" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Texture synthesis and pattern recognition for partially ordered Markov models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cressie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1475" to="1505" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Valois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K De</forename><surname>Valois</surname></persName>
		</author>
		<title level="m">Spatial Vision</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford Univ. Press</publisher>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
		<title level="m">Pattern Classification</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note>nd ed</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gibbs random fields, cooccurrences, and texture models</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Elfadel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Recognit. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="24" to="37" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Theory of communication</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gabor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. IEE (Lond.)</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="429" to="457" />
			<date type="published" when="1946">1946</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Stochastic relaxation, Gibbs distribution and the Bayesian restoration of images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Geman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="721" to="741" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Gimel&apos;farb, Image Textures and Gibbs Random Fields</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Kluwer</publisher>
			<pubPlace>The Netherlands</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Dordrecht</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Probability models for clutter in natural images</title>
		<author>
			<persName><forename type="first">U</forename><surname>Grenander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint/>
	</monogr>
	<note>submitted for publication</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Statistical and structural approaches to texture</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Haralick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1979">1979</date>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="786" to="804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pyramid-based texture analysis/synthesis</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Heeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Bergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPHS</title>
		<meeting>SIGGRAPHS</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="229" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning texture discrimination masks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Karu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="195" to="205" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Visual pattern discrimination</title>
		<author>
			<persName><forename type="first">B</forename><surname>Julesz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IRE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="84" to="92" />
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Textons, the elements of texture perception and their interactions</title>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="91" to="97" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m">Dialogues on Perception</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The self-organizing map</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="1464" to="1480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Computational Investigation of Feature Extraction and Image Organization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<pubPlace>Columbus</pubPlace>
		</imprint>
	</monogr>
	<note>The Ohio State Univ</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Independent spectral representation of images for recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Amer. A</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">3D object recognition using perceptual components</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. Neural Networks</title>
		<meeting>Int. Joint Conf. Neural Networks</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="553" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A spectral histogram model for textonn modeling and texture discrimination</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Res</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="2617" to="2634" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image segmentation using local spectral histograms</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Image Processing</title>
		<meeting>Int. Conf. Image essing</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Preattentive texture discrimination with early vision mechanisms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Amer. A</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="923" to="932" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On the selection of an optimal wavelet basis for texture classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mojsilovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Popovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Rackov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2043" to="2050" />
			<date type="published" when="2000-12">Dec. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Performance evaluation for four classes of textural features</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Ohanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Dubes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="819" to="833" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A comparative study of texture measures with classification based on feature distributions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="51" to="59" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Emergence of simple-cell receptive field properties by learning a sparse code for natural images</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Field</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">381</biblScope>
			<biblScope unit="page" from="607" to="609" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Filtering for texture classification: A comparative study</title>
		<author>
			<persName><forename type="first">T</forename><surname>Randen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Husoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Recognit. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="291" to="310" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A review of recent texture segmentation and feature extraction techniques</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M H</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><surname>Buf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis., Graph., Image Process.: Image Understand</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="359" to="372" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Recognition without correspondence using multidimensional receptive field histograms</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Crowley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="50" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Universal analysis forms for modeling image probabilities</title>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Grenander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Modeling and classifying color textures using random fields in a random environment</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Suen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Healey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1009" to="1017" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Texture segmentation using Voronoi polygons</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tuceryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Recognit. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="211" to="216" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Texture analysis</title>
	</analytic>
	<monogr>
		<title level="m">Handbook of Pattern Recognition and Computer Vision</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Pau</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">S P</forename><surname>Wang</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="235" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">The Nature of Statistical Learning Theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note>nd ed</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Equivalence of Julesz ensembles and FRAME models</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="245" to="261" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Exploring texture ensembles by efficient Markov chain Monte Carlo</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Recognit. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="554" to="569" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Minimax entropy principle and its application to texture modeling</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Mumford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1627" to="1660" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
