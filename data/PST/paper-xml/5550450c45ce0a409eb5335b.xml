<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust Spectral Learning for Unsupervised Feature Selection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lei</forename><surname>Shi</surname></persName>
							<email>shilei@ios.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">State Key Laboratory of Computer Science</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liang</forename><surname>Du</surname></persName>
							<email>duliang@ios.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">State Key Laboratory of Computer Science</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yi-Dong</forename><surname>Shen</surname></persName>
							<email>ydshen@ios.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">State Key Laboratory of Computer Science</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Robust Spectral Learning for Unsupervised Feature Selection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">043A3527E94208C5D1E4BA975DD815BF</idno>
					<idno type="DOI">10.1109/ICDM.2014.58</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T02:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we consider the problem of unsupervised feature selection. Recently, spectral feature selection algorithms, which leverage both graph Laplacian and spectral regression, have received increasing attention. However, existing spectral feature selection algorithms suffer from two major problems: 1) since the graph Laplacian is constructed from the original feature space, noisy and irrelevant features may have adverse effect on the estimated graph Laplacian and hence degenerate the quality of the induced graph embedding; 2) since the cluster labels are discrete in natural, relaxing and approximating these labels into a continuous embedding can inevitably introduce noise into the estimated cluster labels. Without considering the noise in the cluster labels, the feature selection process may be misguided. In this paper, we propose a Robust Spectral learning framework for unsupervised Feature Selection (RSFS), which jointly improves the robustness of graph embedding and sparse spectral regression. Compared with existing methods which are sensitive to noisy features, our proposed method utilizes a robust local learning method to construct the graph Laplacian and a robust spectral regression method to handle the noise on the learned cluster labels. In order to solve the proposed optimization problem, an efficient iterative algorithm is proposed. We also show the close connection between the proposed robust spectral regression and robust Huber M-estimator. Experimental results on different datasets show the superiority of RSFS.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>In many tasks of machine learning and data mining, the high dimensionality of data presents challenges to the current learning algorithms. Feature selection, which selects a subset of relevant features, is an effective way to solve these challenges. Recently, a lot of methods have been proposed to address the problem of unsupervised feature selection <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. These methods usually characterize the structure of data by graph Laplacian, which is defined based on the nearest neighbor graph. In <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b6">[7]</ref>, each feature is selected independently according to some specified criterion. Spectral feature selection algorithms, which explore the graph embedding and jointly evaluate features using sparse spectral regression, have received increasing attention in these years. These methods include <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>.</p><p>Though many spectral feature selection algorithms have been proposed, at least two problems remain not addressed properly. One problem is the construction of graph Laplacian, which can reflect the structure (such as discriminative and geometrical information) of the data. The quality of the constructed graph Laplacian are vitally important to the success of the induced graph embedding (which is also known as pseudo cluster label) and further spectral feature selection algorithms. However, since these methods construct the graph Laplacian from the original uniform feature space, noisy features and outliers will have an adverse effect on the construction of the graph and hence deteriorate the performance of feature selection. Another problem lies on the cluster structure induced by graph embedding. Due to the discrete nature of class labels, these approaches relax and approximate the desired class label to continuous graph embedding, and noises will be inevitably introduced. Since the spectral regression model usually adopts the estimated graph embedding to supervise the evaluation of the importance of features through group sparse induced regularization (i.e. 21 -norm), without considering the noise and outliers in the estimated cluster structure, the feature selection process may be misguided.</p><p>In this paper, we propose a Robust Spectral learning framework for unsupervised Feature Selection (RSFS), which jointly improves the robustness of graph embedding and sparse spectral regression, and hence more faithful feature selection result could be expected. The basic idea of our method is:</p><p>• We utilize the local kernel regression to capture the nonlinear geometrical information, where we adopt the 1 -norm to measure the local learning estimation error. Unlike the traditional 2 -norm used in most existing feature selection approaches, the proposed 1norm based local kernel regression is more robust to large reconstruction errors, which are often caused by noisy features and outliers. It has been shown that, by utilizing the structure of scaled partition matrix, the introduced 1 -norm local learning can also be reformulated as a graph embedding problem <ref type="bibr" target="#b8">[9]</ref>. In this way, effects of noise and outliers are reduced and hence the structure of the data can be better characterized by the learned graph Laplacian.</p><p>• The discrete class label is often relaxed and approximated into continuous values by graph embedding; such continuous relaxation may introduce additional noise, so that the feature selection process may be misled in the sparse spectral regression model. In this paper, we propose a robust sparse spectral regression model by explicitly extracting sparse noise in the continuous approximation. Interestingly, it can be shown that our proposed robust spectral regression model has a dual equivalence with Huber M-estimator in robust statistics <ref type="bibr" target="#b9">[10]</ref>. Thus, the robustness of our proposed spectral regression model can be interpreted as reducing the effects of large regression errors, which are often caused by outliers and noise.</p><p>To select the most discriminative features robustly, we perform the robust graph embedding and robust spectral regression simultaneously. We propose an efficient iterative algorithm to solve the proposed optimization problem. Extensive experiments are conducted on data sets with and without explicit noise and outliers. Experimental results show the superiority of RSFS when compared with others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Feature selection is a fundamental problem in machine learning and has been studied extensively in the literature. Based on the availability of class labels, feature selection algorithms can be classified as supervised algorithms and unsupervised algorithms. Based on whether taking the learning algorithm (e.g., a classification algorithm) into consideration when performing feature selection, the feature selection algorithms can be grouped into three categories, including filter, wrapper and embedded methods.</p><p>Compared with supervised feature selection, unsupervised feature selection is a much harder problem due to the absence of class labels. Unsupervised filter methods usually assign each feature a score which can indicate the feature's capacity to preserve the structure of data. Top ranked features are selected since they can best preserve the structure of data. The typical methods include Maximum Variance, Laplacian Score <ref type="bibr" target="#b1">[2]</ref>, SPEC <ref type="bibr" target="#b10">[11]</ref> and EVSC <ref type="bibr" target="#b6">[7]</ref>. Unsupervised wrapper methods <ref type="bibr" target="#b2">[3]</ref> "wrap" the feature selection process into a learning algorithm and leverage the learning results to select features. Unsupervised embedded methods perform feature selection as a part of model training process, e.g., UDFS <ref type="bibr" target="#b3">[4]</ref> and NDFS <ref type="bibr" target="#b4">[5]</ref>. Among all the unsupervised feature selection methods, spectral feature selection methods have received increasing attention in recent years. The typical spectral feature selection methods include MCFS <ref type="bibr" target="#b2">[3]</ref>, MSRF <ref type="bibr" target="#b11">[12]</ref>, FSSL <ref type="bibr" target="#b12">[13]</ref>, LGDFS <ref type="bibr" target="#b7">[8]</ref>, UDFS <ref type="bibr" target="#b3">[4]</ref>, JELSR <ref type="bibr" target="#b13">[14]</ref>, NDFS <ref type="bibr" target="#b4">[5]</ref> and RUFS <ref type="bibr" target="#b5">[6]</ref>. Most of these methods involve the following two steps. The first step is to explore the cluster structure of data by spectral analysis of graph Laplacian or by nonnegative matrix factorization, and the second step selects features via sparsity regularization models, i.e. 1 -norm and 21 -norm regularized spectral regression, to preserve the estimating cluster structure. MCFS, MRSF and FSSL apply these two steps separately, while UDFS, JELSR, NDFS and RUFS perform them jointly. Most of the above methods pay no special attention to the noise in features and data points when constructing the graph Laplacian, making the learned graph Laplacian unreliable. On the other hand, since the cluster labels are discrete in natural, relaxing and approximating these labels into a continuous embedding will inevitably introduce noise into the estimated cluster labels. The unreliable graph Laplacian and noise in the cluster labels will degenerate the performance of feature selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE PROPOSED METHOD</head><p>In this section, we present our proposed Robust Spectral learning framework for unsupervised Feature Selection (RSFS), which selects feature by performing robust graph embedding to effectively learn the cluster structure and robust spectral regression to handle sparse noise on estimated cluster structure simultaneously. After discussing the robust graph embedding and robust sparse spectral regression, we formulate the optimization problem of RSFS. We also present the algorithm to solve the optimization problem of RSFS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Robust Graph Embedding</head><p>Discriminative information is very important for feature selection. In supervised scenario, the discriminative information is encoded in the class labels. By exploring the class labels, it's convenient for supervised feature selection algorithms to select discriminative features. However, in unsupervised scenario, there is no label information available. Thus, it is much more difficult to select discriminative features. One way to select discriminative features in unsupervised scenario is to learn pseudo cluster labels (graph embedding), which can guide the feature selection process. <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b2">[3]</ref> employed spectral analysis to predict cluster labels. However, since the spectral analysis in <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b2">[3]</ref> depends on the similarity graph constructed from the original feature space, noise and outliers will have an adverse effect on predicting the pesudo labels and hence deteriorate the performance of feature selection. <ref type="bibr" target="#b5">[6]</ref> proposed to learn pseudo labels by local learning regularized nonnegative matrix factorization (NMF). Although the loss function of NMF adopts 21 -norm, the local learning term employs a square loss, which is sensitive to noise and outliers.</p><p>It has also been shown that the local structure of data is very important for exploring the cluster structure of data <ref type="bibr" target="#b14">[15]</ref>. By exploring local structure of data, we can get more accurate pseudo cluster labels. Our goal is to design a method which can both utilize the local structure of data and handle noisy features and outliers for robust graph embedding. In the following, we introduce such a robust local learning method.</p><p>Let X = {x 1 , ..., x n } ∈ R d×n denote the data matrix, whose columns correspond to data instances and rows to features. Suppose these n instances belong to c classes. Denote Y = [y 1 , ..., y c ] = [y il ] ∈ {0, 1} n×c as a partition matrix of data matrix X. To utilize the local structure of data, we assume the label of a data point can be predicted by its neighbors. Formally, for each data point x i , the label predictor p il (•) is constructed based on its neighborhood information {(x j , y jl )|x j ∈ N i )}, where N i is the neighborhood of x i . Suppose P = [p 1 , ..., p c ] ∈ R n×c , where</p><formula xml:id="formula_0">p l = [p 1l (x 1 ), p 2l (x 2 ), ..., p nl (x n )] T ∈ R n .</formula><p>Then, the objective function can be written as</p><formula xml:id="formula_1">min Y∈R n×c J(Y) = L(Y, P),<label>(1)</label></formula><p>where L is a loss function which is robust to noise and outliers and P is the cluster structure estimated by local learning.</p><p>There are many choices for the local predictor p. In order to effectively capture the structure of data, we choose kernel regression as our local predictor. The basic idea of kernel regression is that the prediction of a data point takes the weighted average of the target values of the training data points. The weight is defined by the kernel function. Formally, for each data point x i , a local kernel regression p il (•) is constructed to estimate the cluster label of x i , i.e.,</p><formula xml:id="formula_2">p il (x i ) = xj ∈Ni K(x i , x j )y jl xj ∈Ni K(x i , x j )<label>(2)</label></formula><p>where N i is the neighborhood of x i . Define a matrix S = [s ij ] ∈ R n×n as follows</p><formula xml:id="formula_3">s ij = K(xi,xj ) x j ∈N i K(xi,xj ) x j ∈ N i 0 x j / ∈ N i (3)</formula><p>Thus, we have p l = Sy l and P = SY.</p><p>In order to alleviate the side effect of irrelevant and noisy features, here we employ 1 -norm, which reduces the effect of large fitting error. Thus, we have,</p><formula xml:id="formula_4">min Y∈R n×c J(Y) = c l=1 ||y l -Sy l || 1 . (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>Although the above objective function with respect to the partition matrix Y is attractive, it's difficult to derive a quadratic form. Following <ref type="bibr" target="#b8">[9]</ref>, we employ the scaled partition matrix G = Y(Y T Y) -1 . Balanced clusters, which can lead to better performance in practice, is obtained by the scaling procedure. It can be proved that</p><formula xml:id="formula_6">min G ||G -SG|| 1</formula><p>is equivalent to minimizing the following problem <ref type="bibr" target="#b8">[9]</ref>,</p><formula xml:id="formula_7">J(F) = T r(F T MF)<label>(5)</label></formula><p>where M = (B -S -S T ). B is the degree matrix of (S + S T ).</p><formula xml:id="formula_8">F = [f 1 , ..., f c ] is defined as F = Y(Y T Y) -1 2</formula><p>, and F T F = I c .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Robust Sparse Spectral Regression</head><p>The graph embedding is discrete in natural. By relaxing and approximating it in continuous values, noise is inevitably introduced. Without considering the noise on the estimated cluster structure, the feature selection process may be misguided. Motivated by the recent development in robust principle component analysis <ref type="bibr" target="#b15">[16]</ref>, we propose a robust spectral regression model, which assumes the learned cluster structure may be arbitrarily corrupted, but the corruption is sparse. We introduce a sparse matrix Z ∈ R n×c to explicitly capture the sparse noise. Thus, the goal of robust spectral regression is to approximate F as</p><formula xml:id="formula_9">min W,Z ||(F -Z) -X T W|| 2 F , s.t.|Z| 1 &lt; η 1 , ||W|| 2,1 &lt; η 2 , (<label>6</label></formula><formula xml:id="formula_10">)</formula><p>where η 1 and η 2 are very small positive numbers. W is the spectral regression coefficients where 21 -norm is imposed to pursue row-wise sparsity; Such property makes it suitable for the task of feature selection <ref type="bibr" target="#b16">[17]</ref>. Specifically, w i shrinks to zeros if the i-th feature is less relevant to the estimated cluster structure. Interestingly, it can be shown later in Eq. ( <ref type="formula" target="#formula_18">12</ref>), Eq. ( <ref type="formula" target="#formula_20">13</ref>) and Eq. ( <ref type="formula" target="#formula_21">14</ref>) that the above problem is equivalent to minimizing the regression error with Huber M-estimator, which actually reduces the large regression error caused by noise and outliers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. The Objective Function of RSFS</head><p>In the previous subsections, we present a robust method to explore the graph embedding and a robust spectral regression to handle sparse noise on the estimated cluster structure. By combining the robust graph embedding (Eq. ( <ref type="formula" target="#formula_7">5</ref>)) and robust sparse spectral regression (Eq. ( <ref type="formula" target="#formula_9">6</ref>)) into a unified framework, we obtain the following objective function,</p><formula xml:id="formula_11">min F,W,Z T r(F T MF) + α||(F -Z) -X T W|| 2 F + β||W|| 2,1 + γ||Z|| 1 s.t. F ∈ R n×c + , F = Y(Y T Y) -1 2<label>(7)</label></formula><p>where α, β, γ ∈ R + are parameters. Since the elements in F are discrete values in nature, the optimization problem in Eq. ( <ref type="formula" target="#formula_11">7</ref>) is an NP-hard problem <ref type="bibr" target="#b14">[15]</ref>. By relaxing these discreate values to continuous ones <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b17">[18]</ref>, the objective function in Eq. ( <ref type="formula" target="#formula_11">7</ref>) can be relaxed to min</p><formula xml:id="formula_12">F,W,Z T r(F T MF) + α||(F -Z) -X T W|| 2 F + β||W|| 2,1 + γ||Z|| 1 s.t. F ∈ R n×c + , F T F = I c (8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Algorithm to Solve RSFS</head><p>In this subsection, we present an efficient algorithm to solve the optimization problem in Eq. ( <ref type="formula">8</ref>). Let</p><formula xml:id="formula_13">L(W, Z, F) = T r(F T MF) + α||(F -Z) -X T W|| 2 F + β||W|| 2,1 + γ||Z|| 1 ,<label>(9)</label></formula><p>where three variables W, Z and F are involved. Due to the nonsmoothness of the row-sparsity induced 21 -norm, we develop an coordinate descent algorithm to alternatively minimizing the above objective function with respect to W, Z, and F, separately. This procedure is repeated until convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Optimize W for fixed F and Z:</head><p>The optimization problem for updating W is equivalent to minimizing</p><formula xml:id="formula_14">L 1 = ||(F -Z) -X T W|| 2 F + β α ||W|| 2,1<label>(10)</label></formula><p>Let ∂L1 ∂W = 2X(X T W -(F -Z)) + 2 β α DW = 0, thus we get the close-form solution to update W,</p><formula xml:id="formula_15">W = (XX T + β α D) -1 X(F -Z) (<label>11</label></formula><formula xml:id="formula_16">)</formula><p>where D is a diagonal matrix with</p><formula xml:id="formula_17">D ii = 1 2||wi|| 1 .</formula><p>2) Optimize Z for fixed W and F: The optimization problem of updating Z is equivalent to minimizing</p><formula xml:id="formula_18">L 2 = ||E -Z|| 2 F + γ α ||Z|| 1 , E = F -X T W. (<label>12</label></formula><formula xml:id="formula_19">)</formula><p>The optimization problem in Eq. ( <ref type="formula" target="#formula_18">12</ref>) can be solved efficiently via the soft-thresholding operator <ref type="bibr" target="#b18">[19]</ref> and the closed form solution is as follows,</p><formula xml:id="formula_20">Z ij = 0, i f |E ij | ≤ γ 2α (1 -γ 2α|Eij | )E ij , otherwise<label>(13)</label></formula><p>1 To avoid zero values, We use a very small constant σ to regularize D ii = 1 2 w i w T i +σ <ref type="bibr" target="#b16">[17]</ref>.</p><p>By substituting Eq. ( <ref type="formula" target="#formula_20">13</ref>) into Eq. ( <ref type="formula" target="#formula_18">12</ref>), we get</p><formula xml:id="formula_21">min L 2 = ij E ij ,<label>(14)</label></formula><p>where</p><formula xml:id="formula_22">E ij = E 2 ij , i f |E ij | ≤ γ 2α γ α |E ij | -( γ 2α ) 2 , otherwise</formula><p>The right part of Eq. ( <ref type="formula" target="#formula_21">14</ref>) is denoted as Huber-estimator in robust statistics <ref type="bibr" target="#b9">[10]</ref>. Based on the duality between Eq. ( <ref type="formula" target="#formula_18">12</ref>) and Eq. ( <ref type="formula" target="#formula_21">14</ref>), we can find that Eq. ( <ref type="formula" target="#formula_18">12</ref>) imposes an 2 -norm on small errors (|E ij | ≤ γ 2α ) and imposes an 1 -norm on large errors (|E ij | &gt; γ 2α ). Different from other feature selection methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, which use spectral regression with squared loss, our method can adaptively impose the 1 -norm on large errors, which are often caused by noise and outliers. In this way, our method can have better performance even when the data are noisy or corrupted.</p><p>3) Optimize F for fixed W and Z: By incorporating the orthogonal constraint of F into the objective function via Langrange multiplier, it is equivalent to optimizing</p><formula xml:id="formula_23">L 3 = T r(F T MF) + α||F -A|| 2 F + ν 2 ||F T F -I c || 2 F s.t. F ≥ 0 (15)</formula><p>where A = X T W+Z. In practice, ν is set to be large to ensure the orthogonal condition. Inspired by recent development in non-negative matrix factorization community <ref type="bibr" target="#b19">[20]</ref> , we present an iterative multiplicative updating rule to solve Eq. ( <ref type="formula">15</ref>). Let Φ ∈ R n×c be the Lagrangian multiplier, then we have</p><formula xml:id="formula_24">L(F) = T r(F T MF) + α||F -A|| 2 F + ν 2 ||F T F -I c || 2 F + T r(ΦF T ). (<label>16</label></formula><formula xml:id="formula_25">)</formula><p>Setting ∂L(F) ∂F = 0, we get Φ = -2(MF + αF + νFF T F -νF -αA). By employing the KKT condition Φ ij F ij = 0, we get</p><formula xml:id="formula_26">[MF + αF + νFF T F -νF -αA] ij F ij = 0. (<label>17</label></formula><formula xml:id="formula_27">)</formula><p>Algorithm 1 The Optimization Algorithm of RSFS Input: The data matrix X ∈ R d×n , the parameters α, β, γ, ν and k. Output: Sort all the d features according to ||W i || 2 (i = 1, ..., d) in descending order and select the top q ranked features. 1: Construct the k-nearest neighbor graph and calculate S by Eq. (3) 2: Calculate B as the degree matrix of (S + S T ) and M = B -S -S T 3: The iteration step t = 1; Initialize F t ∈ R n×c and Z t ∈ R n×c ; set D t ∈ R d×d as an identity matrix 4: repeat 5:</p><formula xml:id="formula_28">W t+1 = (XX T + β α D t ) -1 X(F t -Z t ) 6:</formula><p>update Z by Eq. ( <ref type="formula" target="#formula_20">13</ref>)</p><p>7: Though the non-negative constraint has been adopted in NDFS <ref type="bibr" target="#b4">[5]</ref>, the optimization schema developed in NDFS can not be used directly. The problem is due to the fact that M and A in Eq. ( <ref type="formula" target="#formula_26">17</ref>) are mix signed. To tackle this problem, we introduce M = M + -M -and A = A + -A - <ref type="bibr" target="#b19">[20]</ref>, where</p><formula xml:id="formula_29">F t+1 ij ← F t ij [M -F t +νF t +αA + ]ij [M + F t +αF+νF t (F t ) T F t +αA -]ij</formula><formula xml:id="formula_30">M + ij = (|M ij | + M ij )/2 and M - ij = (|M ij | -M ij )/2. We get [(M + -M -)F + αF + νFF T F -νF -α(A + -A -)] ij F ij = 0.</formula><p>Then, we obtain the following updating rule</p><formula xml:id="formula_31">F ij ← F ij [M -F + νF + αA + ] ij [M + F + αF + νFF T F + αA -] ij . (<label>18</label></formula><formula xml:id="formula_32">)</formula><p>We summarize the optimization algorithm in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Sets</head><p>Six data sets are used in our experiments, including two face data sets, i.e., ORL <ref type="bibr" target="#b2">[3]</ref> and Jaffe <ref type="bibr" target="#b20">[21]</ref>, one object image data set, i.e., COIL20 <ref type="bibr" target="#b2">[3]</ref>, two text data sets, i.e., BBCSport <ref type="bibr" target="#b21">[22]</ref> and WebKB4 <ref type="bibr" target="#b22">[23]</ref>, and one handwritten data set, i.e., MNIST <ref type="bibr" target="#b23">[24]</ref>. Table <ref type="table" target="#tab_0">I</ref> gives a summary of these data sets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Methods to Compare</head><p>We systematically compare 3 weak baselines (AllFea, Laplacian Score <ref type="bibr" target="#b1">[2]</ref>, MCFS <ref type="bibr" target="#b2">[3]</ref>) and 3 strong baselines (UDFS <ref type="bibr" target="#b3">[4]</ref>, NDFS <ref type="bibr" target="#b4">[5]</ref> and RUFS <ref type="bibr" target="#b5">[6]</ref>) in unsupervised feature selection literatures.</p><p>• AllFea, which selects all the features.</p><p>• LS<ref type="foot" target="#foot_0">2</ref>  <ref type="bibr" target="#b1">[2]</ref>, which selects those features that can best preserve the local manifold structure of data.</p><p>• MCFS<ref type="foot" target="#foot_1">3</ref>  <ref type="bibr" target="#b2">[3]</ref>, which selects the features by adopting spectral regression with 1 -norm regularization. The neighborhood size is set to 5.</p><p>• UDFS<ref type="foot" target="#foot_2">4</ref>  <ref type="bibr" target="#b3">[4]</ref>, which exploits local discriminative information and feature correlations simultaneously and considers the manifold structure as well.</p><p>The parameters are searched from the grid {10 -9 , 10 -6 , 10 -3 , 1, 10 3 , 10 6 , 10 9 } and the neighborhood size is 5 as used in <ref type="bibr" target="#b3">[4]</ref>.</p><p>• NDFS<ref type="foot" target="#foot_3">5</ref>  <ref type="bibr" target="#b4">[5]</ref>, which selects features by a joint framework of nonnegative spectral analysis and 2,1 -norm regularized regression. The parameters are searched from the grid {10 -6 , 10 -4 , 10 -2 , 1, 10 2 , 10 4 , 10 6 } and the neighborhood size is 5 as used in <ref type="bibr" target="#b4">[5]</ref>. (l) MNIST Fig. <ref type="figure">1</ref>. Clustering accuracy and normalized mutual information versus the number of selected features on all the data sets • RUFS<ref type="foot" target="#foot_4">6</ref>  <ref type="bibr" target="#b5">[6]</ref>, which selects feature by robust nonnegative matrix factorization and robust regression. The parameters are searched from the grid {10 -6 , 10 -4 , 10 -2 , 1, 10 2 , 10 4 , 10 6 } and the neighborhood size is 5 as used in <ref type="bibr" target="#b5">[6]</ref>.</p><p>• RSFS <ref type="foot" target="#foot_5">7</ref> , which is proposed in this paper. We tune the parameters from {10 -3 , 10 -2 , 10 -1 , 1, 10 1 , 10 2 , 10 3 }. The neighborhood size is set to be 5.</p><p>For each method, the parameters are searched in the grid as described. For the selected features, the K-means algorithm is applied 20 times with random initiation and the best average result is reported. Clustering Accuracy and Normalized Mutual Information are used to evaluate the performance of different algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Clustering on Data Sets without Explicit Noise</head><p>We first evaluate the performance of the seven methods on data sets without explicit noise. The clustering results, in terms of NMI and ACC, are reported in Figure <ref type="figure">1</ref>. We have the following observations. Firstly, by selecting features jointly and utilizing discriminative information, MCFS, UDFS, NDFS, RUFS and RSFS have a better performance than LS. By learning graph embedding and performing feature selection simultaneously, RSFS, RUFS and NDFS have a better performance over most of the datasets. By considering outliers and noise, RSFS and RUFS achieve better performance than other methods. At last, out proposed RSFS achieves the best performance. This can be explained by the following main reasons. First, the robust graph embedding learning method can learn better cluster structure. It also explores the local structure of data, which has been shown to be important for data analysis. Second, by assuming sparse noise in the learned pseudo label matrix, we propose a robust regression model to handle the noise. Third, the robust graph embedding method and the robust spectral regression are performed jointly to handle noise and outliers in data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Clustering on Data Sets with Malicious Occlusion</head><p>In this subsection, we describe the experimental results on data sets with explicit noise and outliers. In this experiment, we use the ORL data set, which contains 400 gray scale images of 40 individuals. In order to impose some noise to the original ORL data set, different ratio (0.2, 0.3) of images are randomly selected and partially occluded with random blocks. 10 tests were conducted on different randomly chosen percentage of outliers, and the average performance over the 10 data sets is reported.</p><p>Figure <ref type="figure">2</ref> shows the clustering results in term of ACC for the methods over datasets with different ratio of noise. We have two observations. First, our proposed method can achieve the best performance over all the corrupted data sets. Second, the improvement between our method and other methods increases when the ratio of corruption varies from 0.2 to 0.3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Effects of the Parameters</head><p>In this subsection, we study the sensitiveness of parameters. Due to space limit, we only report the results on COIL20 and Jaffe in Figure <ref type="figure">3</ref>. Figure <ref type="figure">3</ref> shows the best clustering accuracy with respect to each of the parameters over the selected features. The results show that our method is not very sensitive to the parameters α, β and γ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We have proposed a novel robust unsupervised feature selection framework, called RSFS, which is a unified framework that jointly performs robust graph embedding learning and robust sparse spectral regression. To solve optimization problem of RSFS, an efficient iterative algorithm was proposed. The extensive experimental results show that our proposed method outperforms other state-of-the-art methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>; 10: until Convergence criterion satisfied 11: Sort each feature according to ||w i || and select the top ranked ones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Fig. 2. Clustering Accuracy on ORL with different ratio of noisy images</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I</head><label>I</label><figDesc></figDesc><table><row><cell>.</cell><cell cols="3">SUMMARY OF DATA SETS</cell></row><row><cell>Dataset</cell><cell>Size</cell><cell>Dimensions</cell><cell>Classes</cell></row><row><cell>BBCSport</cell><cell>737</cell><cell>1000</cell><cell>5</cell></row><row><cell>WebKB4</cell><cell>4199</cell><cell>1000</cell><cell>4</cell></row><row><cell>ORL</cell><cell>400</cell><cell>1024</cell><cell>40</cell></row><row><cell>COIL20</cell><cell>1440</cell><cell>1024</cell><cell>20</cell></row><row><cell>MNIST</cell><cell>4000</cell><cell>784</cell><cell>10</cell></row><row><cell>Jaffe</cell><cell>213</cell><cell>676</cell><cell>10</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>http://www.cad.zju.edu.cn/home/dengcai/Data/code/LaplacianScore.m</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>http://www.cad.zju.edu.cn/home/dengcai/Data/code/MCFS p.m</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>http://www.cs.cmu.edu/ ∼ yiyang/UDFS.rar</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>https://sites.google.com/site/zcliustc/home/publication/AAAI2012.m</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>http://web.engr.illinois.edu/ ∼ mqian2/upload/research/RUFS/RUFS.m</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>http://kingsleyshi.com/codes/RSFS.rar</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We would like to thank the anonymous reviewers for their helpful comments and suggestions. This work is supported in part by China National 973 program 2014CB340301 and NSFC grant 61379043.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Efficient spectral feature selection with minimum redundancy</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Laplacian score for feature selection</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">186</biblScope>
			<biblScope unit="page">189</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised feature selection for multicluster data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<biblScope unit="page" from="333" to="342" />
			<date type="published" when="2010">2010</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">l 2, 1-norm regularized discriminative feature selection for unsupervised learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="1589" to="1594" />
			<date type="published" when="2011">2011</date>
			<publisher>AAAI Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised feature selection using nonnegative spectral analysis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust unsupervised feature selection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1621" to="1627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Eigenvalue sensitive feature selection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Local and global discriminative learning for unsupervised feature selection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="131" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Clustering via local regression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML/PKDD</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="456" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Robust statistics: the approach based on influence functions</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Hampel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Ronchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Rousseeuw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Stahel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">114</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spectral feature selection for supervised and unsupervised learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1151" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient spectral feature selection with minimum redundancy</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="673" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Joint feature selection and subspace learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="1294" to="1299" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Feature selection via joint embedding learning and sparse regression</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="1324" to="1329" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Robust principal component analysis?</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient and robust feature selection via joint l2, 1-norms minimization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1813" to="1821" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Nonnegative spectral clustering with discriminative regularization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Optimization with sparsity-inducing penalties</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Co-clustering on manifolds</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<biblScope unit="page" from="359" to="368" />
			<date type="published" when="2009">2009</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automatic classification of single facial images</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Budynek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Akamatsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1357" to="1362" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Producing accurate interpretable clusters from high-dimensional data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cunningham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PKDD</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="486" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adaptive dimension reduction using discriminant analysis and k-means clustering</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="521" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
