<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Manifold Based Analysis of Facial Expression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Changbo</forename><surname>Hu</surname></persName>
							<email>cbhu@cs.ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ya</forename><surname>Chang</surname></persName>
							<email>yachang@cs.ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
							<email>rferis@cs.ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matthew</forename><surname>Turk</surname></persName>
							<email>mturk@cs.ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Manifold Based Analysis of Facial Expression</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6F9450915AD8EB6EF6184C4CBF865C4C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel approach for modeling, tracking and recognizing facial expressions. Our method works on a low dimensional expression manifold, which is obtained by Isomap embedding. In this space, facial contour features are first clustered, using a mixture model. Then, expression dynamics are learned for tracking and classification. We use ICondensation to track facial features in the embedded space, while recognizing facial expressions in a cooperative manner, within a common probabilistic framework. The image observation likelihood is derived from a variation of the Active Shape Model (ASM) algorithm. For each cluster in the lowdimensional space, a specific ASM model is learned, thus avoiding incorrect matching due to non-linear image variations. Preliminary experimental results show that our probabilistic facial expression model on manifold significantly improves facial deformation tracking and expression recognition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Computational facial expression analysis is an active and challenging research topic in computer vision, impacting important applications such as in humancomputer interaction and data-driven animation. Approaches for automatic modeling and recognition of facial expressions are generally classified as static (processing still images) and dynamic (tracking and analyzing facial deformations in video sequences).</p><p>In the past decade, many techniques have been proposed to automatically classify expressions in still images, using methods based on Neural Networks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16]</ref>, Gabor wavelets <ref type="bibr" target="#b1">[2]</ref> and rule-based methods <ref type="bibr" target="#b2">[3]</ref>, to mention just a few. However, in recent years, more attention has been given to modeling facial deformation in dynamic scenarios <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b17">18]</ref>, which allows the integration of information temporally across the video sequence, potentially increasing recognition rates over single-image approaches. For such dynamic scenarios, current methods work in two separate stages: tracking and recognition. The tracking module extracts features over time, while the recognition module processes this information for expression classification.</p><p>Many systems obtain facial motion information by computing dense flow between successive image frames. But flow estimates are easily disturbed by the variation of lighting and non-rigid motion, and they are also sensitive to the inaccuracy of image registration and motion discontinuities <ref type="bibr" target="#b4">[5]</ref>.</p><p>Model-based approaches, such as Active Shape Models (ASM) <ref type="bibr" target="#b5">[6]</ref> and Active Appearance Models (AAM) <ref type="bibr" target="#b6">[7]</ref> have been successfully used for tracking facial deformation. The ASM method detects facial landmarks through a local-based search constrained by a global shape model, statistically learned from training data. The AAM algorithm elegantly combines shape and texture models, assuming a linear relationship between appearance and parameter variation. Both methods, however, tend to fail in the presence of non-linear image variations such as those caused by large facial expression changes.</p><p>Nonlinear embedding methods such as ISOMAP <ref type="bibr" target="#b7">[8]</ref>, local linear embedding (LLE) <ref type="bibr" target="#b19">[20]</ref>, charting a manifold <ref type="bibr" target="#b20">[21]</ref>, and global coordinate of local linear models <ref type="bibr" target="#b21">[22]</ref> are promising in handling high dimensional nonlinear data. Recently, researchers have applied manifold methods to face recognition <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23]</ref> and facial expression representation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>This paper presents a novel representation for dynamic facial expression analysis, as well as a probabilistic framework for tracking and recognizing facial deformation in a cooperative manner. Our assumption is that video sequences of a person undergoing different facial expressions define a smooth and relatively low dimensional manifold in a feature space described by a set of facial landmarks.</p><p>Initially, we use Isomap embedding <ref type="bibr" target="#b7">[8]</ref> to project our training video data into the low dimensional expression manifold. Then, a Gaussian mixture model is applied to cluster data in the low dimensional expression space. For each cluster, a specific ASM model is learned, since tracking by online probabilistic model is more robust to non-linear image variations. In addition, we learn the dynamics for each cluster in the manifold to improve tracking and recognition.</p><p>Based on this representation, a particle filter tracker is used to track facial deformation in the embedded space, while recognizing facial expressions. Differing from traditional methods that consider expression tracking and recognition in separate stages, we address these tasks in a common probabilistic framework, which enables them to be solved in a cooperative manner.</p><p>The remainder of this paper is organized as follows: in Section 2 we discuss related work. Section 3 covers the learning of our proposed representation, while Section 4 describes the framework to track and recognize facial expressions. Section 5 reports our experimental results and Section 6 presents conclusions and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Recently, Wang et al. <ref type="bibr" target="#b8">[9]</ref> demonstrated the importance of applying non-linear dimensionality reduction in the field of non-rigid object tracking. In fact, representing the object state as a globally coordinated low dimensional vector improves tracking efficiency and reduces local minimum problems in optimization. They learn the object's intrinsic structure in a low dimension manifold with density modeled by a mixture of factor analyzers. Our work also models the intrinsic structure of facial expressions for tracking, but extends it to include recognition in a unified probabilistic framework.</p><p>We were also inspired by the work of Lee et al. <ref type="bibr" target="#b9">[10]</ref>, who present a method for modeling and recognizing human faces in video sequences. They use an appearance model composed of pose manifolds and a matrix of transition probabilities to connect them. In our work, we consider transition probabilities among clusters in the embedded space, effectively capturing the dynamics of expression changes and exploiting the temporal information for recognition.</p><p>Zhou, Krueger and Chellapa <ref type="bibr" target="#b10">[11]</ref> proposed a generic framework to track and recognize human faces simultaneously by adding an identity variable to the state vector in the sequential importance sampling method. The posterior probability of the identity variable is then estimated by marginalization. Their work, however, does not consider tracking and recognition of facial deformation, the main focus of this paper.</p><p>Cootes et al. <ref type="bibr" target="#b5">[6]</ref> proposed the Active Shape Model algorithm, which detects facial landmarks through a local-based search constrained by a global shape model, statistically learned from training data. This method was extensively used for facial deformation tracking, but may fail under large expression transitions. In our approach, we use specific ASM models for each cluster in the embedded space. On-line model selection is done probabilistically in a cooperative manner with expression classification, thus improving tracking reliability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Learning Dynamic Facial Deformation</head><p>Non-linear dimensionality reduction has attracted attention for a long time in computer vision and visualization research. Images lie in a very high dimensional space, but a class of images generated by latent variables lies on a manifold in this space. For human face images, the latent variables may be the illumination, identity, pose and facial deformations. In this paper, we are interested in embedding the facial deformations of a person in a very low dimensional space, which reflects the intrinsic structure of facial expressions. From training video sequences of different people undergoing different expressions, a low dimensional manifold is learned, with a subsequent probabilistic modeling used for tracking and recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The Training Database</head><p>For preliminary testing, we collected a database of two subjects who were asked to perform six basic facial expressions multiple times. To reduce the influence of illumination variation, we preprocessed the training data video sequence by detecting a set of 2D facial landmarks in each image, which defines the shape of a face in each particular frame. We use the Active Shape Model algorithm to accomplish this task. With a good manual initialization and separate training models prepared specifically for each expression image set, we can extract the face shape precisely. Figure <ref type="figure" target="#fig_2">1</ref> shows the facial points in our shape model.</p><p>The whole training dataset, comprising different video sequences of different people undergoing different facial expressions, is then specified by a set</p><formula xml:id="formula_0">} ,..., { 1 n x x X =</formula><p>, where</p><formula xml:id="formula_1">D i R x 2 ∈</formula><p>notes a set of D facial points in a particular frame, and n denotes the total number of images in the training data. Unlike traditional manifold embedding papers, where data can be in any order, our training images are ordered according to the video sequences, thus allowing the learning of dynamics on the manifold, as we will show later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Isomap Embedding</head><p>To embed the high dimension data set , where</p><formula xml:id="formula_2">d i R y ∈</formula><p>. This latent variable encodes the knowledge of the data set and controls the data variations.</p><p>In the Isomap algorithm, the local geometry of the high dimensional manifold is initially measured through the distances between neighboring data points. For each pair of non-neighboring data points, Isomap finds the shortest path through the data set connecting them, subject to the constraint that the path must hop from neighbor to neighbor. The length of this path is an approximation to the distance between its end points, as measured within the underlying manifold. Finally, the classical method of multidimensional scaling <ref type="bibr" target="#b13">[14]</ref> is used to find a set of low dimensional points with similar pairwise distances.</p><p>Figure <ref type="figure" target="#fig_0">2</ref> shows the result of projecting our training data (set of facial shapes) in a three dimensional space using Isomap embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Mixture Model on the Embedded Space</head><p>In the lower dimensional embedded space, we describe the distribution of the data using a Gaussian Mixture Gaussian distribution with mean i µ and covariance matrix i C :</p><formula xml:id="formula_3">= = = K i i i C N i p y p 1 ) , ( ) ( ) ( µ ω (3.1)</formula><p>Figure <ref type="figure" target="#fig_4">3</ref> is an example to illustrate this GMM in the embedded space. Ellipsoids centers and sizes show the mixture centers and the covariance respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Cluster-Based Active Shape Models</head><p>If we were to train an Active Shape Model from all the images in a data set together, the significant variation in the data set would not be modeled well and the tracking performance would be poor. Instead, we train a set of ASM models for each image cluster; that is, for each set of images corresponding to a mixture center (with a defined covariance) of the GMM in the embedded space.</p><p>We also propose a method to select and probabilistically integrate the ASM models in ICondensation framework. We will show in Section 4 that online model selection allows tracking to be robust under large expression variations.</p><p>In ASM, a shape vector S is represented in the space spanned by a set of eigenvectors learned from the training data. As a result, S may be expressed as:</p><formula xml:id="formula_4">Us S S + = (3.2)</formula><p>where S is the mean shape, U is the matrix consisting of eigenvectors and s constitutes the shape   parameters, which are estimated during ASM search.</p><p>In Section 4, we will describe how tracking is achieved using the learned ASM models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Learning Dynamics on the Manifold</head><p>Based on the manifold representation, we can learn a dynamic model, defined as the transition probability ) | ( n = 6, meaning that r can assume six basic expressions. We have been using the prototypical universal expressions of fear, disgust, happiness, sadness, anger and surprise, though the method does not depend on this particular grouping.</p><p>The dynamic model can be factorized in the following way:</p><formula xml:id="formula_5">- - - = t w t t t t t t t y p y y p y y p ) | ( ) , | ( ) | ( 1 1 1 ω ω ) | ( ) | ( ) , |<label>( 1 1 , 1 1 1 -</label></formula><formula xml:id="formula_6">- - - - = t t t t t t t y p p y y p t t ω ω ω ω ω ω (3.3) where - - - - - = 1 ) ( ) , | ( ) | ( 1 1 1 1 t r t t t t t t r p r p p ω ω ω ω</formula><p>This assumes that t and y t-1 are conditionally independent given t-1 .</p><p>For each state of 1 t r (i.e., each expression class), the cluster transition dynamics ) , | ( is the dynamic model for a known cluster center. For simplification, we assume the dynamics in a fixed cluster is the same for each expression.</p><p>Similar to Wang, et al. <ref type="bibr" target="#b8">[9]</ref>, we also model the within cluster transition as a first order Gaussian Auto-Regressive Process (ARP) by:</p><formula xml:id="formula_7">) , ( ) , | ( 1 1 T t t t t BB D y A N y y p t t ω ω ω + = - - (3.4)</formula><p>which can be represented in generative form as For AR parameter learning, we use the same method as Blake and Isard <ref type="bibr" target="#b11">[12]</ref>. Combining equations (3.3), (3.4) and (3.5), we get: Wang et al. <ref type="bibr" target="#b8">[9]</ref> pointed out that the equations above model a Mixture of Gaussian Diffusion (MGD), whose mixture term is controlled by the random variable t ω .</p><formula xml:id="formula_8">k t t Bw D y A y t t + + = - ω ω 1 (3.5)</formula><formula xml:id="formula_9">) ; ( ) , ( ) | ( ) ( ) , | ( ) , | ( ) | ( 1 1 1 1 1 1 1 1 1 1 1 - - - - - - - - - + = = - - t t T t t t</formula><p>In our work, the mixture term is also controlled by the expression recognition random variable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Probabilistic Tracking and Recognition</head><p>In the previous section, we showed how to learn a facial expression model on the manifold as well as its associated dynamics. Now, we show how to use this representation to achieve robust online facial deformation tracking and recognition. Our probabilistic tracking is based on the ICondensation algorithm <ref type="bibr" target="#b12">[13]</ref>, which is described next, followed by expression classification. Both tracking and recognition are described in the same probabilistic framework, which enables them to be carried out in a cooperative manner.    ) back to the original shape space, through a nearest-neighbor scheme. methods that consider expression tracking and recognition in separate stages, we address these tasks in a common probabilistic framework, which enables them to be solved in a cooperative manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">ICondensation Tracking</head><p>Our results are preliminary, as our data set is quite small. We plan to perform much more extensive experimentation and provide more substantial quantitative results. For future work we will attempt to extend our framework to include pose and illumination variations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2 ∈</head><label>2</label><figDesc>to a space with low dimension d &lt; 2D, we use the Isomap embedding algorithm<ref type="bibr" target="#b7">[8]</ref>. Our goal is to find the latent variable }</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Model (GMM). The Expectation-Maximization (EM) algorithm is used to estimate the distribution. The following equation describes the density model, where ) ( y p is the probability that a point in the low dimensional space is generated by the model, k is the number</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The shape model, defined by a set of facial landmarks.</figDesc><graphic coords="3,119.28,89.96,127.92,122.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Training data in the embedded space. Different colors correspond to different expressions.Figure 3: GMM in the embedded space. Ellipsoids centers show the mixture centers; sizes show the covariance.</figDesc><graphic coords="3,76.56,521.73,212.40,158.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 2: Training data in the embedded space. Different colors correspond to different expressions.Figure 3: GMM in the embedded space. Ellipsoids centers show the mixture centers; sizes show the covariance.</figDesc><graphic coords="3,317.65,529.17,222.00,166.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>covariance matrix, and k w is independent random white noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>.</head><label></label><figDesc>Below we illustrate one step of a sample's evolution.After this step, the state with largest weight describes the tracking output in each frame, consisting of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>1</head><label>1</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Sequential Importance Sampling Iteration:</head><label></label><figDesc>~ was generated from the posterior probability, apply the dynamic model for prediction. For the rigid state part, we use constant prediction, adding a small Gaussian noise. For the non-rigid part, we use the MGD noise model, where the weight of each component is controlled by the After all the samples are generated and measured, normalize</figDesc><table><row><cell cols="25">Main Objective: Generate sample set</cell></row><row><cell>S</cell><cell>t</cell><cell>=</cell><cell cols="3">{(</cell><cell>s</cell><cell>( t</cell><cell>n</cell><cell>)</cell><cell cols="2">,</cell><cell cols="2">π</cell><cell cols="2">( t</cell><cell cols="2">n</cell><cell>)</cell><cell cols="2">),</cell><cell cols="2">n</cell><cell cols="2">=</cell><cell>1</cell><cell>,...</cell><cell>N</cell><cell>}</cell><cell>at time t from sample</cell></row><row><cell cols="2">set</cell><cell>S</cell><cell>t</cell><cell>-</cell><cell>1</cell><cell cols="2">=</cell><cell></cell><cell cols="3">{(</cell><cell></cell><cell>s</cell><cell>( t</cell><cell cols="2">n -</cell><cell cols="2">) 1</cell><cell>,</cell><cell>π</cell><cell>( t</cell><cell cols="2">) 1 n -</cell><cell>),</cell><cell>n</cell><cell>=</cell><cell>1</cell><cell>,...</cell><cell>N</cell><cell>}</cell><cell>at time</cell><cell>1 t . -</cell></row><row><cell cols="10">Algorithm:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="25">For each sample, n = 1 to N :</cell></row><row><cell cols="25">1) Create samples n t s Choose</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="14">one of the following sampling methods</cell></row><row><cell></cell><cell></cell><cell cols="23">with a fixed probability:</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="21">(1) Generate sample from initialization prior.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="21">(2) Generate sample from importance re-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="16">sampling, where the importance function is</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="16">the posterior from time t-1;</cell></row><row><cell cols="25">2) Predict n t s from n t s ã)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="22">If n t s ~was generated from the prior probability,</cell></row><row><cell></cell><cell cols="24">choose n t s from n t s ~ adding a fixed Gaussian noise.</cell></row><row><cell></cell><cell cols="6">b) If</cell><cell></cell><cell></cell><cell cols="2">s</cell><cell>t</cell><cell cols="2">n</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="24">cluster center distribution</cell><cell>) p ω and expression ( t</cell></row><row><cell></cell><cell cols="24">classification distribution</cell><cell>p</cell><cell>( t r</cell><cell>)</cell><cell>.</cell></row><row><cell cols="25">3) Update the set of samples. The measurement of the</cell></row><row><cell cols="14">sample n t s is</cell><cell cols="9">) π = ( n t</cell><cell cols="2">λ</cell><cell>( i t</cell><cell>)</cell><cell>*</cell><cell>M( n t s ), where</cell><cell>) λ is the ( i t</cell></row><row><cell cols="25">importance sampling correction term. M is the sample</cell></row><row><cell cols="25">measurement function, described in the next</cell></row><row><cell cols="10">subsection.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">π</cell><cell cols="3">( n t</cell><cell cols="2">)</cell><cell></cell><cell></cell><cell cols="5">so that</cell><cell>(</cell><cell>)</cell><cell>1</cell></row></table><note><p>4)</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPRW'04) 1063-6919/04 $ 20.00 IEEE</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPRW'04)</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work has been supported in part by NSF ITR grant #0205740.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. Sample Measurement</head><p>In order to measure a sample (function M in the algorithm above), we proceed in the following way. For each mixture center in the embedded space, a specific ASM model is selected to measure image observation. This measure is given by a residual error obtained after applying one step of ASM search (we refer to Cootes et al. <ref type="bibr" target="#b5">[6]</ref> for details on the search process). Face pose initialization is given by the sample rigid part  ) of the sample back to the original shape space (using a nearest-neighbor scheme).</p><p>Once we have a residual error for each one of the mixture centers, the desired sample measurement is obtained by a weighted sum of these residuals, where the weights corresponds to the likelihood of the sample non-rigid part ( d y y ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1</head><p>) in each Gaussian model. This scheme allows tracking to be robust under large facial expression changes, as we will show in Section 5. Next we describe how to update expression classification in each frame, using a common probabilistic framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Expression Recognition Updating</head><p>We have already showed that the distribution of the discrete random variable r (the expression recognition variable) directly affects tracking (see sample prediction and dynamic model learning). Now we show how to update the posterior probability    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we present our preliminary experimental results on facial deformation tracking and recognition.</p><p>To learn the structure of the expression manifold, we need O(10 3 ) images to cover basic expressions for each subject and to enable stable geodesic distance computation. Since there is no database with a sufficiently large amount of subject data available, we built our own small data set for the experiments. In our experiments, subjects were instructed to perform a series of six kinds of prototypical facial expressions, representing happiness, sadness, anger, surprise, fear, and disgust. The subjects repeated the series seven times for the gallery set. The probe set includes a long sequence (more than 10 4 frames) where the subject can change his/her expression randomly. To simplify the problem, we assume constant illumination and near frontal view pose.</p><p>To generate the shape sequence from the training data set, we trained ten ASM models for different kinds of deformations. We manually select the model in this offline stage to robustly track facial deformation along the video sequences. The shape space dimension is 90. We used the Isomap algorithm to obtain a space with dimensionality d=3.</p><p>We verified that our probabilistic method is able to track and recognize long sequences of subjects performing subtle and large expression changes. Figure <ref type="figure">4</ref> shows two frames from a tracking and recognition test using a video sequence of more than 10 We also quantitatively analyze the performance of our tracker with a standard ASM tracker. Figure <ref type="figure">5</ref> shows a precision comparison, considering as ground truth a manual labeling of eye corners and lip corners. The same images were used to train both trackers. The difference is that our method automatically splits this data to train a set of models, which are probabilistically selected during tracking. This allows more robust performance under large facial expression changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We proposed a novel framework for dynamic facial expression analysis. We now summarize our main contributions:</p><p>(1) A new representation for tracking and recognition of facial expressions, based on manifold embedding and probabilistic modeling in the embedded space.</p><p>(2) A robust method for facial deformation tracking based on a set of ASM models, which are probabilistically selected during tracking, improving reliability under large expression changes.</p><p>(3) A probabilistic expression classification method, which integrates information temporally across the video sequence. In contrast with traditional  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Representing Face Images for Emotional Classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Padgett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conf. Advances in Neural Information Processing Systems</title>
		<meeting>Conf. Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="894" to="900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Comparison Between Geometry-based and Gabor Waveletsbased Facial Expression Recognition Using Multi-layer Perceptron</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Akamatsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Int. Conf. On Automatic Face and Gesture Recognition</title>
		<meeting>Int. Conf. On Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recognizing Facial Expressions in Image Sequences Using Local Parameterized Models of Image Motion</title>
		<author>
			<persName><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yacoob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="48" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Facial Expression Recognition from Video Sequences: Temporal and Static Modeling</title>
		<author>
			<persName><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="160" to="187" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Facial Expression Understanding in Image Sequences Using Dynamic and Active Visual Information Fusion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Int. Conf. On Computer Vision</title>
		<meeting>Int. Conf. On Computer Vision<address><addrLine>Nice, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Active Shape Models -their Training and Applications</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Active Appearance Models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on PAMI</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="681" to="685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Global Geometric Framework for Nonlinear Dimensionality Reduction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="2319" to="2323" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning Object Intrinsic Structure for Robust Visual Tracking</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Int. Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>Int. Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2003">June 16-22, 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Video-based Face Recognition Using Probabilistic Appearance Manifolds</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Int. Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>Int. Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2003">June 16-22, 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Probabilistic Recognition of Human Faces from Video</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Active Contours</title>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Cambridge University press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ICondensation: Unifying Lowlevel and High-level Tracking in a Stochastic Framework</title>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Multidimensional Scaling: History, Theory and Applications</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">W</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Hamer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<publisher>Erlbaum</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Manifold of facial expression</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Workshop on Analysis and Modeling of Faces and Gestures</title>
		<meeting>IEEE International Workshop on Analysis and Modeling of Faces and Gestures<address><addrLine>Nice, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-10-17">Oct. 17, 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recognizing Action Units for Facial Expression Analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2001-02">February, 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multimodal Data Representations with Parameterized Local Structures</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. Computer Vision</title>
		<meeting><address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="173" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Video-based online face recognition using identity surfaces</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liddell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Workshop on Recognition, Analysis and Tracking of Faces and Gestures in Real-time Systems</title>
		<meeting>IEEE International Workshop on Recognition, Analysis and Tracking of Faces and Gestures in Real-time Systems<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-07">July 2001</date>
			<biblScope unit="page" from="40" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Face Recognition Using Extended Isomap</title>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Nonlinear Dimensionality Reduction by Locally Linear Embedding</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Charting a manifold</title>
		<author>
			<persName><forename type="first">M</forename><surname>Brand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Global Coordination of Local Linear Models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Saul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems 14 (NIPS&apos;2001)</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="889" to="896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Manifold analysis of facial gestures for face recognition</title>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Fidaleo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGMM Multimedia Biometrics Methods and Application Workshop</title>
		<imprint>
			<date type="published" when="2003-08">Nov. 8, 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Probabilistic expression analysis on manifolds</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Washington DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-06">June 2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
