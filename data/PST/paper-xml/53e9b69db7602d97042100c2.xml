<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sentiment Analyzer: Extracting Sentiments about a Given Topic using Natural Language Processing Techniques</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jeonghee</forename><surname>Yi</surname></persName>
							<email>jeonghee@almaden.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Almaden Research Center</orgName>
								<address>
									<addrLine>650 Harry Rd</addrLine>
									<postCode>95120</postCode>
									<settlement>San Jose</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tetsuya</forename><surname>Nasukawa</surname></persName>
							<email>nasukawa@jp.ibm.com</email>
							<affiliation key="aff1">
								<orgName type="institution">IBM Tokyo Research Lab</orgName>
								<address>
									<addrLine>Yamato-shi</addrLine>
									<postCode>1623-14, 242-8502</postCode>
									<settlement>Shimotsuruma, Kanagawa-ken</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Razvan</forename><surname>Bunescu</surname></persName>
							<email>razvan@cs.utexas.edu</email>
						</author>
						<author>
							<persName><forename type="first">Wayne</forename><surname>Niblack</surname></persName>
							<email>niblack@almaden.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Almaden Research Center</orgName>
								<address>
									<addrLine>650 Harry Rd</addrLine>
									<postCode>95120</postCode>
									<settlement>San Jose</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Texas</orgName>
								<address>
									<postCode>78712</postCode>
									<settlement>Austin</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sentiment Analyzer: Extracting Sentiments about a Given Topic using Natural Language Processing Techniques</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">73DE5360638CFE347BABB0A23C39BAC1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present Sentiment Analyzer (SA) that extracts sentiment (or opinion) about a subject from online text documents. Instead of classifying the sentiment of an entire document about a subject, SA detects all references to the given subject, and determines sentiment in each of the references using natural language processing (NLP) techniques. Our sentiment analysis consists of 1) a topic specific feature term extraction, 2) sentiment extraction, and 3) (subject, sentiment) association by relationship analysis. SA utilizes two linguistic resources for the analysis: the sentiment lexicon and the sentiment pattern database. The performance of the algorithms was verified on online product review articles ("digital camera" and "music" reviews), and more general documents including general webpages and news articles. * The author's work on a portion of the feature term selection algorithm development was performed while the author was on summer internship at IBM Almaden Research Center.</p><p>1. As with every Sony PDA before it, the NR70 series is equipped with Sony's own Memory Stick expansion.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Today, a huge amount of information is available in online documents such as web pages, newsgroup postings, and on-line news databases. Among the myriad types of information available, one useful type is the sentiment, or opinions people express towards a subject. (A subject is either a topic of interest or a feature of the topic.) For example, knowing the reputation of their own or their competitors' products or brands is valuable for product development, marketing and consumer relationship management. Traditionally, companies conduct consumer surveys for this purpose. Though well-designed surveys can provide quality es-timations, they can be costly especially if a large volume of survey data is gathered.</p><p>There has been extensive research on automatic text analysis for sentiment, such as sentiment classifiers <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b18">19]</ref>, affect analysis <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21]</ref>, automatic survey analysis <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16]</ref>, opinion extraction <ref type="bibr" target="#b11">[12]</ref>, or recommender systems <ref type="bibr" target="#b17">[18]</ref>. These methods typically try to extract the overall sentiment revealed in a document, either positive or negative, or somewhere in between.</p><p>Two challenging aspects of sentiment analysis are: First, although the overall opinion about a topic is useful, it is only a part of the information of interest. Document level sentiment classification fails to detect sentiment about individual aspects of the topic. In reality, for example, though one could be generally happy about his car, he might be dissatisfied by the engine noise. To the manufacturers, these individual weaknesses and strengths are equally important to know, or even more valuable than the overall satisfaction level of customers.</p><p>Second, the association of the extracted sentiment to a specific topic is difficult. Most statistical opinion extraction algorithms perform poorly in this respect as evidenced in <ref type="bibr" target="#b2">[3]</ref>. They either i) assume the topic of the document is known a priori, or ii) simply associate the opinion to a topic term co-existing in the same context. The first approach requires a reliable topic or genre classifier that is a difficult problem in itself. A document (or even a portion of a document as small as a sentence) may discuss multiple topics and contain sentiment about multiple topics.</p><p>For example, consider the following sentences from which ReviewSeer <ref type="bibr" target="#b2">[3]</ref> found positive opinions about the NR70 PDA:</p><p>2. Unlike the more recent T series CLIEs, the NR70 does not require an add-on adapter for MP3 playback, which is certainly a welcome change.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Memory Stick support in the NR70</head><p>series is well implemented and functional, although there is still a lack of nonmemory Memory Sticks for consumer consumption.</p><p>Based on our understanding of the ReviewSeer algorithm, we suppose their statistical method (and most other statistical opinion extraction methods) would assign the same polarity to Sony PDA and T series CLIEs as that of NR70 for the first two sentences. That is wrong for T series CLIEs, although right for Sony PDA. We notice that the third sentence reveals a negative aspect of the NR70 (i.e., the lack of non-memory Memory Sticks) as well as a positive sentiment in the primary phrase.</p><p>We anticipated the shortcomings of the purely statistical approaches, and in this paper we show that the analysis of grammatical sentence structures and phrases based on NLP techniques mitigates some of the shortcomings. We designed and developed Sentiment Analyzer (SA) that</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• extracts topic-specific features</head><p>• extracts sentiment of each sentiment-bearing phrase • makes (topic|feature, sentiment) association SA detects, for each occurrence of a topic spot, the sentiment specifically about the topic. It produces the following output for the above sample sentences provided that Sony PDA, NR70, and T series CLIEs are specified topics:</p><p>1. Sony PDA -positive NR70 -positive 2. T series CLIEs -negative NR70 -positive 3. NR70 -positive NR70 -negative</p><p>The rest of this paper is organized as follows: Section 2 describes the feature term extraction algorithm and reports the experimental results for feature term selection. Section 3 describes the core sentiment detection algorithms and experimental results. Section 4 summarizes related work and compares them with our algorithms. Finally, we conclude with a discussion in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Feature Term Extraction</head><p>A feature term of a topic is a term that satisfies one of the following relationships:</p><p>• a part-of relationship with the given topic.</p><p>• an attribute-of relationship with the given topic.</p><p>This camera has everything that you need. It takes great pictures and is very easy to use. It has very good documentation. Bought 256 MB memory card and can take a huge number of pictures at the highest resolution. Everyone is amazed at the resolution and clarity of the pictures. The results have been excellent from macro shots to telephoto nature shots. Manuals and software are not easy to follow. Good Battery Life 200 on 1GB drive Best Remote I have seen on any camera. The battery seems to last forever but you will want a spare anyway. The best built in flash I have seen on any camera. The G2 has enough features to keep the consumer and pro creative for some time to come!</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1. Sample digital camera review</head><p>• an attribute-of relationship with a known feature of the given topic.</p><p>For the digital camera domain, a feature can be a part of the camera, such as lenses, battery or memory card; an attribute, such as price or size; or an attribute of a feature, such as battery life (an attribute of feature battery). Figure <ref type="figure">1</ref> is a portion of an actual review article from www.cnet.com. The phrases in bold are the features we intend to extract. We apply the feature term extraction algorithm described in the rest of this section to a set of documents having the same topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">The Candidate Feature Term Selection</head><p>Based on the observation that feature terms are nouns, we extract only noun phrases from documents and apply feature selection algorithms described in Section 2.2. Specifically, we implemented and tested the following three candidate term selection heuristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">Base Noun Phrases (BNP).</head><p>BNP restricts the candidate feature terms to one of the following base noun phrase (BNP) patterns: NN, NN NN, JJ NN, NN NN NN, JJ NN NN, JJ JJ NN, where NN and JJ are the part-ofspeech(POS) tags for nouns and adjectives respectively defined by Penn Treebank <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">Definite Base Noun Phrases (dBNP)</head><p>. dBNP further restricts candidate feature terms to definite base noun phrases, which are noun phrases of the form defined in Section 2.1.1 that are preceded by the definite article "the." Given that a document is focused on a certain topic, the definite noun phrases referring to topic features do not need any additional constructs such as attached prepositional phrases or relative clauses, in order for the reader to establish their referent. Thus, the phrase "the battery," instead of "the battery of the digital camera," is sufficient to infer its referent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3.">Beginning Definite Base Noun Phrases (bBNP).</head><p>bBNP refers to dBNP at the beginning of sentences followed by a verb phrase. This heuristic is based on the observation that, when the focus shifts from one feature to another, the new feature is often expressed using a definite noun phrase at the beginning of the next sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Feature Selection Algorithms</head><p>We developed and tested two feature term selection algorithms based on a mixture language model and likelihood ratio. They are evaluated in Section 2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Mixture Model.</head><p>This method is based on the mixture language model by Zhai and Laffertry <ref type="bibr" target="#b22">[23]</ref>: they assume that an observed documents d is generated by a mixture of the query model and the corpus language model. In our case, we may consider our language model as the mixture (or a linear combination) of the general web language model θ W (similar to the corpus language model) and a topic-specific language model θ T (similar to the query model):</p><formula xml:id="formula_0">θ = αθ W + βθ T</formula><p>where α, β are given and sum to 1. α indicates the amount of background noise when generating a document from the topic-specific model. θ, θ W and θ T have multinomial distributions,</p><formula xml:id="formula_1">θ W = (θ W1 , θ W2 , ...θ W k ), θ T = (θ T1 , θ T2 , ...θ T k ), and θ = (θ 1 , θ 2 , ...θ k ),</formula><p>where k is the number of words in the corpus. Intuitively, by calculating the topic-specific model, θ T , noise words can be deleted, since the topicspecific model will concentrate on words occurring frequently in topic-related documents, but less frequently in the whole corpus. The maximum likelihood estimator of θ W can be calculated directly as:</p><formula xml:id="formula_2">θW i = df i j df j</formula><p>where df i is the number of times word i occurs in the whole corpus. The problem of finding θ T can be generalized as finding the maximum likelihood estimation of multinomial distribution θ T . Zhang et al. <ref type="bibr" target="#b23">[24]</ref> developed an O(klog(k)) algorithm that computes the exact maximum likelihood estimation of the multinomial distribution of q in the following mixture model of multinomial distributions, p = (p 1 , p 2 , ...p k ), q = (q 1 , q 2 , ...q k ), and r = (r 1 , r 2 , ...r k ):</p><formula xml:id="formula_3">r = αp + βq</formula><p>Let f i be the observed frequency of word i in the documents that are generated by r. Sort pi fi so that f1 p1 &gt; f2 p2 &gt; ... &gt; f k p k . Then, find t that satisfies:</p><formula xml:id="formula_4">β α + t j=1 p j t j=1 f j - pt ft &gt; 0 β α + t+1 j=1 p j t+1 j=1 f j - p t+1 f t+1 ≤ 0 D + D - bnp C 11 C 12 bnp C 21 C 22</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1. Counts for a bnp [9]</head><p>Then, the q i 's are given by:</p><formula xml:id="formula_5">q i = f i λ -α β p i if 1 ≤ i ≤ t 0 otherwise (1) λ = t i=1 f i 1 + α β t i=1 p i</formula><p>The following feature selection algorithm is the direct result of Equation <ref type="formula">1</ref>.</p><p>Algorithm: For feature term selection, compute θ Ti as follows:</p><formula xml:id="formula_6">θ T i = f i λ -α β θ W i if 1 ≤ i ≤ t 0 otherwise (2) λ = t i=1 f i 1 + α β t i=1 θ W i</formula><p>Then sort candidate feature terms in decreasing order of θ Ti . Feature terms are those whose θ Ti score satisfy a predefined confidence level. Alternatively we can simply select only the top N terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Likelihood Test.</head><p>This method is based on the likelihood-ratio test by Dunning <ref type="bibr" target="#b3">[4]</ref>. Let D + be a collection of documents focused on a topic T , D -those not focused on T , and bnp a candidate feature term extracted from D + as defined in Section 2.1. Then, the likelihood ratio -2logλ is defined as follows:</p><formula xml:id="formula_7">-2logλ = -2log max p 1 ≤p 2 L(p 1 , p 2 ) maxp 1 ,p 2 L(p 1 , p 2 ) p 1 = p(d ∈ D + |bnp ∈ d) p 2 = p(d ∈ D + |bnp ∈ d)</formula><p>where L(p 1 , p 2 ) is the likelihood of seeing bnp in both D + and D -.</p><p>Assuming that each bnp is a Bernoulli event, the counts from Table <ref type="table">1</ref> follow a binomial distribution, and the following likelihood ratio is asymptotically χ 2 distributed.</p><formula xml:id="formula_8">-2logλ = -2log max p 1 ≤p 2 b(p 1 , C 11 , C 11 + C 12 ) * b(p 2 , C 21 , C 21 + C 22 ) maxp 1 ,p 2 b(p 1 , C 11 , C 11 + C 12 ) * b(p 2 , C 21 , C 21 + C 22 ) where b(p, k, n) = p k • (1 -p) n-k -2logλ= -2 * lr if r 2 &lt; r 1 0 if r 2 ≥ r 1 (3) r1 = C 11 C 11 +C 12 , r2= C 21 C 21 +C 22 r = C 11 +C 21 C 11 +C 12 +C 21 +C 22 lr = (C11+C21)log(r)+(C12+C22)log(1-r)-C11log(r1) -C12log(1-r1)-C21log(r2)-C22log(1-r2)</formula><p>The higher the value of -2logλ, the more likely the bnp is relevant to the topic T .  Algorithm: For each bnp, compute the likelihood score, -2logλ, as defined in equation 3. Then, sort bnp in decreasing order of their likelihood score. Feature terms are all bnp's whose likelihood ratio satisfy a pre-defined confidence level. Alternatively simply only the top N bnp's can be selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1.">The Dataset.</head><p>We carried out experiments on two domains: digital camera and music review articles. Each dataset is a mix of manually labeled topic domain documents (D + ) and non-topic domain documents (D -) that are randomly selected from the web pages collected by our web-crawl. The datasets are summarized in Table <ref type="table" target="#tab_0">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2.">Experimental Results.</head><p>We ran the two feature extraction algorithms in six different settings on the product review datasets:</p><p>-BNP-M: Mixture Model with BNP -dBNP-M: Mixture Model with dBNP -bBNP-M: Mixture Model with bBNP -BNP-L: Likelihood Test with BNP -dBNP-L: Likelihood Test with dBNP -bBNP-L: Likelihood Test with bBNP First, BNP, dBNPand bBNP were extracted from the review pages and the Mixture Model and Likelihood Test were applied on the respective bnp's. Terms with likelihood ratio above 0 were extracted for bBNP-L: 38 and 31 feature terms for digital camera and music datasets respectively. For the rest of the settings, the thresholding scheme was applied giving the same number of terms (i.e., 38 and 31 respectively for digital camera and music datasets) at the top of the lists. This thresholding gives the best possible precision scores for the other settings, since terms on the top of the  list are more likely to be feature terms. We used the Ratnaparkhi POS tagger <ref type="bibr" target="#b13">[14]</ref> to extract bnp's. α = 0.3 was used for the computation of the Mixture Model. (Other values of α were used, which did not produce any better results than what are reported here.) The extracted feature terms were manually examined by two human subjects and only the terms that both subjects labeled as feature terms were counted for the computation of the precision. The precision scores are summarized in Table <ref type="table" target="#tab_1">3</ref>. bBNP-L performed impressively well. The Likelihood Test method consistently performed better than the Mixture Model algorithm. Its performance continued improving with increasing level of restrictions in the candidate feature terms, perhaps because, with further restriction, the selected candidate terms are more probable feature terms. On the contrary, interestingly, the increasing level of restrictions had the reverse effect with the Mixture Model algorithm. This might be because the restrictions caused too much perturbation on term distributions for the algorithm to reliably estimate the multinomial distribution of the topic-specific model. We need further investigation to explain the behavior.</p><p>The top 20 feature terms extracted by bBNP-L from the digital camera and music datasets are listed in Table <ref type="table" target="#tab_3">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Sentiment Analysis</head><p>In this section, we describe the linguistic resources used by sentiment analysis (3.1), define the scope of sentence structures that SA is dealing (3.2), sentiment phrase identification and sentiment assignment (3.3), and relationship analysis (3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Linguistic Resources</head><p>Sentiment about a subject is the orientation (or polarity) of the opinion on the subject that deviates from the neutral state. Sentiment that expresses a desirable state (e.g., The picture is flawless.) has positive (or "+") polarity, while one representing an undesirable state (e.g., The product fails to meet our quality expectations.) has negative (or "-") polarity. The target of sentiment is the subject that the sentiment is directed to: the picture and the product for the examples above. SA uses sentiment terms defined in the sentiment lexicon and sentiment patterns in the sentiment pattern database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Sentiment Lexicon.</head><p>The sentiment lexicon contains the sentiment definition of individual words in the following form: &lt;lexical_entry&gt; &lt;POS&gt; &lt;sent_category&gt; -lexical_entry is a (possibly multi-word) term that has sentimental connotation. -POS is the required POS tag of lexical entry.</p><p>-sentiment_category : + | -The following is an example of the lexicon entry:</p><formula xml:id="formula_9">"excellent" JJ +</formula><p>We have collected sentiment words from several sources: General Inquirer (GI) 1 , Dictionary of Affect of Language (DAL) 2 <ref type="bibr" target="#b20">[21]</ref>, and WordNet <ref type="bibr" target="#b10">[11]</ref>. From GI, we extracted all words in Positive, Negative, and Hostile categories. From DAL, we extracted words whose affect scores are one standard deviation higher (positive) or lower (negative) than the mean. From WordNet, we extracted synonyms of known sentiment words. At present, we have about 3000 sentiment term entries including about 2500 adjectives and less than 500 nouns. Some verbs have positive or negative sentiment by themselves, but some verbs (we call them trans verb), such as "be" or "offer", do not. The sentiment of a subject in a sentence with a trans verb is determined by another component of the sentence. Some example sentiment patterns and sentences matching with them are: Initially, we collected sentiment verbs from GI, DAL, and WordNet. For GI and DAL, the sentiment verb extraction is the same as the sentiment term extraction as described in Section 3.1.1. From WordNet we extracted verbs from the emotion cluster. From the training datasets described in Section 2.3.1, we manually refined some of the patterns. The refinements typically involve the specification of sentiment source and target, as the typical error SA initially introduced was the association of the discovered sentiment to a wrong target. Currently, we have about 120 sentiment predicate patterns in the database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Sentiment Pattern</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Scope of Sentiment Analysis</head><p>As a preprocessing step to our sentiment analysis, we extract sentences from input documents containing mentions of subject terms of interest. Then, SA applies sentiment analysis to kernel sentences <ref type="bibr" target="#b6">[7]</ref> and some text fragments. Kernel sentences usually contain only one verb. For kernel sentences, SA extracts the following types of ternary expressions (T-expressions) <ref type="bibr" target="#b6">[7]</ref>:</p><p>• positive or negative sentiment verbs: &lt;target, verb, ""&gt; • trans verbs:</p><p>&lt;target, verb, source&gt;</p><p>The following illustrates T-expressions of given sentences: &lt;the camera, like, ""&gt; ex. I like the camera. &lt;the digital zoom, be, too grainy&gt; ex. The digital zoom is too grainy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>For text fragments, SA extracts binary expressions (Bexpressions),</head><p>&lt;adjective, target&gt; ex. good quality photo : &lt;good quality, photo&gt;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Sentiment Phrases and Sentiment Assignment</head><p>After parsing each input sentence by a syntactic parser, SA identifies sentiment phrases from subject, object, adjective, and prepositional phrases of the sentence.</p><p>Adjective phrases: Within the phrase, we identify all sentiment adjectives defined in the sentiment lexicon. For example, vibrant is positive sentiment phrase for the sentence "The colors are vibrant."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subject, object and prepositional phrases:</head><p>We extract all base noun phrases of the forms defined in Section 2.1.1 that consist of at least one sentiment word. The sentiment of the phrase is determined by the sentiment words in the phrase. For example, excellent pictures (JJ NN) is a positive sentiment phrase because excellent (JJ) is a positive sentiment word. For a sentiment phrase with a word with negative meaning, such as not,no, never, hardly, seldom, or little, the polarity of the sentiment is reversed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Semantic Relationship Analysis</head><p>SA extracts T-and B-expressions in order to make (subject, sentiment) association. From a T-expression, sentiment of the verb (for sentiment verbs) or source (for trans verb), and from a B-expression, sentiment of the adjective, is assigned to the target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1.">Sentiment Pattern based Analysis.</head><p>For each sentiment phrase detected (Section 3.3), SA determines its target and final polarity based on the sentiment pattern database (Section 3.1.2). SA first identifies the T-expression, and tries to find matching sentiment patterns. Once a matching sentiment pattern is found, the target and sentiment assignment are determined as defined in the sentiment pattern.</p><p>Some sentiment patterns define the target and its sentiment explicitly. Suppose the following sentence, sentiment pattern, and subject is given: I am impressed by the flash capabilities. pattern : "impress" + PP(by;with) subject : flash SA first identifies the T-expression of the sentence: &lt;flash capability, impress, ""&gt; and directly infers that the target (PP lead by "by" or "with"), the flash capabilities, has positive sentiment: (flash capability, +).</p><p>For sentences with a trans verb, SA first determines the sentiment of source, and assigns the sentiment to the target. For example, for the following sentence and the given subject term camera:</p><p>This camera takes excellent pictures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SA first parses the sentence and identifies:</head><p>-matching sentiment pattern: &lt;"take" OP SP&gt; -subject phrase (SP): this camera -object phrase (OP) : excellent pictures -sentiment of the OP: positive -T-expression : &lt;camera, take, excellent picture&gt; From this information, SA infers that the sentiment of source (OP) is positive, and associates positive sentiment to the target (SP): (camera, +).</p><p>During the semantic relationship analysis, SA takes negation into account at the sentence level: if an adverb with negative meaning (such as not, never, hardly, seldom, or little) appears in a verb phrase, SA reverses the sentiment of the sentence assigned by the corresponding sentiment pattern. For example, SA detects negative polarity from the following sentence:</p><p>This camera is not good for novice users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2.">Analysis without Sentiment Pattern.</head><p>There are many cases where sentiment pattern based analysis is not possible. Common cases include:</p><p>-No corresponding sentiment pattern is available.</p><p>-The sentence is not complete.</p><p>-Parser failure, possibly due to missing punctuation,  wrong spelling, etc. Examples of fragments containing sentiment are:</p><p>Poor performance in a dark room. (1) Many functionalities for the price. ( <ref type="formula">2</ref>)</p><p>SA creates B-expressions and makes the sentiment assignment on the basis of the phrase sentiment. The B-expressions and sentiment associations of sentences ( <ref type="formula">1</ref>) and ( <ref type="formula">2</ref>) are:</p><p>(1 B ) &lt;poor, performance&gt; : (performance, -) (2 B ) &lt;many, functionality&gt; : (functionaliry, +)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Evaluation</head><p>For experiments, we used the Talent<ref type="foot" target="#foot_0">3</ref> shallow parser for sentence parsing, and bBNP-L for feature extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1.">Product Review Dataset.</head><p>We ran SA on the review article datasets (Section 2.3.1). The review articles are a special class of web documents that typically have a high percentage of sentiment-bearing sentences. For each subject term, we manually assigned the sentiment. Then, we ran SA for each sentence with a subject term and compared the computed sentiment label with the manual label to compute the accuracy. The result is compared with the collocation algorithm and the best performing algorithm of ReviewSeer <ref type="bibr" target="#b2">[3]</ref>. To our knowledge, ReviewSeer is by far the latest and the best opinion classifier. The collocation algorithm assigns the polarity of a sentiment term to a subject term, if the sentiment term and the subject term exist in the same sentence. If positive and negative sentiment terms coexist, the polarity with more counts is selected.</p><p>The overall precision and recall of SA are 87% and 56%, respectively (Table <ref type="table" target="#tab_7">5</ref>). The accuracy of the best performing algorithm of ReviewSeer is 88.4% (vs. 85.6% of SA). The precision of the Collocation algorithm is significantly lower, only 18%, as expected, with high recall of 70%.</p><p>Although the results provide a rough comparison, they are not directly comparable. First, the test datasets are not the same. Although both SA and ReviewSeer use product review articles, the actual datasets are not identical. (We are not aware of any benchmark dataset for sentiment classification for evaluation purposes.) They have combined more categories (7 categories vs. 2 categories for SA). Secondly, ReviewSeer is a document level sentiment classifier, while SA is per subject-spot level. Third, ReviewSeer does not try to do subject association.</p><p>ReviewSeer might have produced better accuracy with fewer categories. On the other hand, since they do not try (subject, sentiment) association, their accuracy is not affected by the potential association error, while SA's is. That is, even though SA extracts sentiment polarity accurately, we consider it a failure if the (subject, sentiment) association is made wrong. It is not clear how much the subject association would impact ReviewSeer's accuracy. However, the experimental results on general web documents (Section 3.5.2) reveal how much subject association error degrades, at least partially, the accuracy of ReviewSeer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2.">General Web Documents.</head><p>Sentiment in general Web documents are typically very sparse in comparison to the review articles. This characteristic of general web documents may work against a document level classifier as there might not be enough sentiment-bearing expressions in a document to classify the entire document as sentiment-bearing.</p><p>In order to mitigate the problem, ReviewSeer applied the algorithm on the individual sentences with a subject word. This makes the comparison with SA on more equal ground. Table <ref type="table" target="#tab_8">6</ref> lists the results.</p><p>SA achieves high precision (86% ∼ 91%) and even higher accuracy (90% ∼ 93%) on general Web documents and news articles. The precision of SA was computed only on the test cases that SA extracted as either positive or negative, but did not include neutral cases. The accuracy of SA included the neutral cases as well, as did ReviewSeer's. The accuracy of SA is higher than the precision, because the majority of the test cases do not have any sentiment expression, and SA correctly classifies most of them as neutral.</p><p>On the contrary, ReviewSeer suffered with sentences from general web documents: the accuracy is only 38% (down from 88.4%). (The accuracy is computed based on the figures from Table <ref type="table" target="#tab_3">14</ref> of <ref type="bibr" target="#b2">[3]</ref>: we have averaged the accuracies of the three equal-size groups of a test set, 21%, 42% &amp; 50%, respectively.) The accuracy was improved to 68% after removing difficult cases and using only clearly posi-tive or negative sentences about the given subject. The set of difficult testing cases eliminated (called I class) include sentences that were ambiguous when taken out of context (case i), were not describing the product (case ii), or did not express any sentiment at all (case iii).</p><p>The challenge here is that these difficult cases are the majority of the sentences that any sentiment classifier has to deal with: 60% (356 out of 600) of the test cases for the Re-viewSeer experiment and even more (as high as over 90% on some domain) in our experiments. Case i is difficult for any sentiment classifier. We believe case ii is where the purely statistical methods do not perform well and sophisticated NLP can help. SA tries to solve the (subject, sentiment) association problem case ii by the relationship analysis. SA handles the neutral cases iii already very well as discussed earlier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Previous Work</head><p>[1] describes a procedure that aims at extracting part-of features, using possessive constructions and prepositional phrases, from news corpus. By contrast, we extract both part-of and attribute-of relations.</p><p>Some of the previous works on sentiment-based classification focused on classifying the semantic orientation of individual words or phrases, using linguistic heuristics, a preselected set of seed words, or by human labeling <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21]</ref>. <ref type="bibr" target="#b4">[5]</ref> developed an algorithm for automatically recognizing the semantic orientation of adjectives. <ref type="bibr" target="#b21">[22]</ref> identifies subjective adjectives (or sentiment adjectives) from corpora.</p><p>Past work on sentiment-based categorization of entire documents has often involved either the use of models inspired by cognitive linguistics <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16]</ref> or the manual or semimanual construction of discriminant-word lexicons <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19]</ref>. <ref type="bibr" target="#b5">[6]</ref> proposed a sentence interpretation model that attempts to answer directional queries based on the deep argumentative structure of the document, but with no implementation detail or any experimental results. <ref type="bibr" target="#b12">[13]</ref> compares three machine learning methods (Naive Bayes, maximum entropy classification, and SVM) for sentiment classification task.</p><p>[20] used the average "semantic orientation" of the phrases in the review. <ref type="bibr" target="#b14">[15]</ref> analysed emotional affect of various corpora computed as average of affect scores of individual affect terms in the articles. The sentiment classifiers often assumes 1) each document has only one subject, and 2) the subject of each document is known. However, these assumptions are often not true, especially for web documents. Moreover, even if the assumptions are met, sentiment classifiers are unable to reveal the sentiment about individual features, unlike SA.</p><p>Product Reputation Miner <ref type="bibr" target="#b11">[12]</ref> extracts positive or negative opinions based on a dictionary. Then it extracts characteristic words, co-occurrence words, and typical sentences for individual target categories. For each characteristic word or phrase they compute frequently co-occurring terms. However, their association of characteristic terms and co-occurring terms does not necessarily mean relevant opinion as was seen in collocation experiments. In contrast, our NLP based relationship analysis associates subjects to the corresponding sentiments.</p><p>ReviewSeer <ref type="bibr" target="#b2">[3]</ref> is a document level opinion classifier that uses mainly statistical techniques and some POS tagging information for some of their text term selection algorithms. It achieved high accuracy on review articles. However, the performance sharply degrades when applied to sentences with subject terms from the general web documents. In contrast, SA continued to perform with high accuracy. Unlike ReviewSeer, SA handles the neutral cases and subject association very well. In fact, the relationship analysis of SA was designed for these kinds of difficult cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion and Future Work</head><p>We applied NLP techniques to sentiment analysis. The feature extraction algorithm successfully identified topic related feature terms from online review articles, enabling sentiment analysis at finer granularity. SA consistently demonstrated high quality results of 87% for review articles, 86 ∼ 91% (precision) and 91 ∼ 93% (accuracy) for the general web pages and news articles. The results on review articles are comparable with the state of the art sentiment classifiers, and the results on general web pages are better than those of the state of the art algorithms by a wide margin (38% vs. 91 ∼ 93%).</p><p>However, from our initial experience with sentiment detection, we have identified a few areas of potentially substantial improvements. We expect full parsing will provide better sentence structure analysis, thus better relationship analysis. Second, more advanced sentiment patterns currently require a fair amount of manual validation. Although some amount of human expert involvement may be inevitable in the validation to handle the semantics accurately, we plan on more research on increasing the level of automation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 . The product review datasets</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">digital camera (38) music (31)</cell></row><row><cell>BNP-M</cell><cell>63%</cell><cell>61%</cell></row><row><cell>dBNP-M</cell><cell>68%</cell><cell>32%</cell></row><row><cell>bBNP-M</cell><cell>32%</cell><cell>29%</cell></row><row><cell>BNP-L</cell><cell>68%</cell><cell>92%</cell></row><row><cell>dBNP-L</cell><cell>81%</cell><cell>96%</cell></row><row><cell>bBNP-L</cell><cell>97%</cell><cell>100%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 . Precision of feature term extraction algo- rithms</head><label>3</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 . Top 20 feature terms extracted by bBNP-L in the order of their rank</head><label>4</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Database. Our sentiment pattern database contains sentiment extraction patterns for sentence predicates. The database entry is defined in the following form:</figDesc><table /><note><p>&lt;predicate&gt; &lt;sent_category&gt; &lt;target&gt; • predicate: typically a verb • sent_category: + | -| [˜]source source is a sentence component (SP|OP|CP|PP) whose sentiment is transferred to the target. SP, OP, CP, and PP represent subject, object, complement (or adjective), and prepositional phrases, respectively. The opposite sentiment polarity of source is assigned to the target, if ˜is specified in front of source. • target is a sentence component (SP| OP|PP) the sentiment is directed to.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 . Performance comparison of senti- ment extraction alorithms on the product review datasets.</head><label>5</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 . The performance of SA and ReviewSeer on general web documents and news articles.</head><label>6</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">Precision Accuracy Acc. w/o</cell></row><row><cell></cell><cell></cell><cell></cell><cell>I class</cell></row><row><cell>SA(Petroleum, Web)</cell><cell>86%</cell><cell>90%</cell><cell>N/A</cell></row><row><cell>SA(Pharmaceutical, Web)</cell><cell>91%</cell><cell>93%</cell><cell>N/A</cell></row><row><cell>SA(Petroleum, News)</cell><cell>88%</cell><cell>91%</cell><cell>N/A</cell></row><row><cell>ReviewSeer (Web)</cell><cell>N/A</cell><cell>38%</cell><cell>68%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>http://flahdo.watson.ibm.com/Talent/talent project.htm</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Finding parts in very large corpora</title>
		<author>
			<persName><forename type="first">M</forename><surname>Berland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 37th ACL Conf</title>
		<meeting>of the 37th ACL Conf</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="57" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Yahoo! for anazon: Extracting market sentiment from stock message boards</title>
		<author>
			<persName><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 8th APFA</title>
		<meeting>of the 8th APFA</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mining the peanut gallery: Opinion extraction and semantic classification of product reviews</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Pennock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 12th Int. WWW Conf</title>
		<meeting>of the 12th Int. WWW Conf</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Accurate methods for the statistics of surprise and coincidence</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Dunning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Predicting the semantic orientation of adjectives</title>
		<author>
			<persName><forename type="first">V</forename><surname>Hatzivassiloglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 35th ACL Conf</title>
		<meeting>of the 35th ACL Conf</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="174" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Direction-based text interpretation as an information access refinement</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hearst</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
	<note type="report_type">Text-Based Intelligent Systems</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">From sentence processing to information access on the world wide web</title>
		<author>
			<persName><forename type="first">B</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI Spring Symp. on NLP</title>
		<meeting>of AAAI Spring Symp. on NLP</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mining from open answers in questionnaire data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yamanishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 7th ACM SIGKDD Conf</title>
		<meeting>of the 7th ACM SIGKDD Conf</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schutze</surname></persName>
		</author>
		<title level="m">Foundations of Statistical Natural Language Processing</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: the penn treebank</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Nouns in WordNet : A lexical inheritance system</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<ptr target="ftp://ftp.cogsci.princeton.edu/pub/wordnet/5papers.ps" />
	</analytic>
	<monogr>
		<title level="j">Int. J. of Lexicography</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="245" to="264" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mining product reputations on the web</title>
		<author>
			<persName><forename type="first">S</forename><surname>Morinaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yamanishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Teteishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fukushima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 8th ACM SIGKDD Conf</title>
		<meeting>of the 8th ACM SIGKDD Conf</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Thumbs up? sentiment classification using machine learning techniques</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2002 ACL EMNLP Conf</title>
		<meeting>of the 2002 ACL EMNLP Conf</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A maximum entropy model for part-ofspeech tagging</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ratnaparkhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the EMNLP Conf</title>
		<meeting>of the EMNLP Conf</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="133" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Emotion and style in 30-second television advertisements targeted at men, women, boys, and girls</title>
		<author>
			<persName><forename type="first">L</forename><surname>Rovinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Whissell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perceptual and Motor Skills</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="1048" to="1050" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On the computation of point of view</title>
		<author>
			<persName><forename type="first">W</forename><surname>Sack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 12th AAAI Conf</title>
		<meeting>of the 12th AAAI Conf</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Affect analysis of text using fuzzy semantic typing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Subasic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huettner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Fuzzy Systems, Special Issue</title>
		<imprint>
			<date type="published" when="2001-08">Aug., 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">PHOAKS: A system for sharing recommendations</title>
		<author>
			<persName><forename type="first">L</forename><surname>Terveen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Amento</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Creter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CACM</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="59" to="62" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An operational system for detecting and tracking opinions in on-line discussion</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR Workshop on Operational Text Classification</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 40th ACL Conf</title>
		<meeting>of the 40th ACL Conf</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The dictionary of affect in language</title>
		<author>
			<persName><forename type="first">C</forename><surname>Whissell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Emotion: Theory, Research, and Experience</title>
		<imprint>
			<biblScope unit="page" from="113" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning subjective adjectives from corpora</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 17th AAAI Conf</title>
		<meeting>of the 17th AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Model-based feedback in the lanuage modeling approach to information retrieval</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 10th Information and Knowledge Management Conf</title>
		<meeting>of the 10th Information and Knowledge Management Conf</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Exact maximum likelihood estimation for word mixtures</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Text Learning</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
