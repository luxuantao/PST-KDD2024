<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-02-11">11 Feb 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
							<email>bengio@google.com</email>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-02-11">11 Feb 2017</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1611.01236v2[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet <ref type="bibr" target="#b13">(Russakovsky et al., 2014)</ref>. Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than singlestep attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a "label leaking" effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>It has been shown that machine learning models are often vulnerable to adversarial manipulation of their input intended to cause incorrect classification <ref type="bibr" target="#b2">(Dalvi et al., 2004)</ref>. In particular, neural networks and many other categories of machine learning models are highly vulnerable to attacks based on small modifications of the input to the model at test time <ref type="bibr" target="#b0">(Biggio et al., 2013;</ref><ref type="bibr" target="#b14">Szegedy et al., 2014;</ref><ref type="bibr" target="#b3">Goodfellow et al., 2014;</ref><ref type="bibr" target="#b9">Papernot et al., 2016b)</ref>.</p><p>The problem can be summarized as follows. Let's say there is a machine learning system M and input sample C which we call a clean example. Let's assume that sample C is correctly classified by the machine learning system, i.e. M (C) = y true . It's possible to construct an adversarial example A which is perceptually indistinguishable from C but is classified incorrectly, i.e. M (A) = y true . These adversarial examples are misclassified far more often than examples that have been perturbed by noise, even if the magnitude of the noise is much larger than the magnitude of the adversarial perturbation <ref type="bibr" target="#b14">(Szegedy et al., 2014)</ref>.</p><p>Adversarial examples pose potential security threats for practical machine learning applications. In particular, <ref type="bibr" target="#b14">Szegedy et al. (2014)</ref> showed that an adversarial example that was designed to be misclassified by a model M 1 is often also misclassified by a model M 2 . This adversarial example transferability property means that it is possible to generate adversarial examples and perform a misclassification attack on a machine learning system without access to the underlying model. <ref type="bibr" target="#b11">Papernot et al. (2016a)</ref> and <ref type="bibr" target="#b9">Papernot et al. (2016b)</ref> demonstrated such attacks in realistic scenarios.</p><p>It has been shown <ref type="bibr" target="#b3">(Goodfellow et al., 2014;</ref><ref type="bibr" target="#b4">Huang et al., 2015)</ref> that injecting adversarial examples into the training set (also called adversarial training) could increase robustness of neural networks to adversarial examples. Another existing approach is to use defensive distillation to train the network <ref type="bibr" target="#b10">(Papernot et al., 2015)</ref>. However all prior work studies defense measures only on relatively small datasets like MNIST and CIFAR10. Some concurrent work studies attack mechanisms on ImageNet <ref type="bibr" target="#b12">(Rozsa et al., 2016)</ref>, focusing on the question of how well adversarial examples transfer between different types of models, while we focus on defenses and studying how well different types of adversarial example generation procedures transfer between relatively similar models.</p><p>Published as a conference paper at <ref type="bibr">ICLR 2017</ref> In this paper we studied adversarial training of Inception models trained on ImageNet. The contributions of this paper are the following:</p><p>• We successfully used adversarial training to train an Inception v3 model <ref type="bibr" target="#b15">(Szegedy et al., 2015)</ref> on ImageNet dataset <ref type="bibr" target="#b13">(Russakovsky et al., 2014)</ref> and to significantly increase robustness against adversarial examples generated by the fast gradient sign method <ref type="bibr" target="#b3">(Goodfellow et al., 2014)</ref> as well as other one-step methods. • We demonstrated that different types of adversarial examples tend to have different transferability properties between models. In particular we observed that those adversarial examples which are harder to resist using adversarial training are less likely to be transferrable between models. • We showed that models which have higher capacity (i.e. number of parameters) tend to be more robust to adversarial examples compared to lower capacity model of the same architecture. This provides additional cue which could help building more robust models. • We also observed an interesting property we call "label leaking". Adversarial examples constructed with a single-step method making use of the true labels may be easier to classify than clean adversarial examples, because an adversarially trained model can learn to exploit regularities in the adversarial example construction process. This suggests using adversarial example construction processes that do not make use of the true label. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODS GENERATING ADVERSARIAL EXAMPLES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">TERMINOLOGY AND NOTATION</head><p>In this paper we use the following notation and terminology regarding adversarial examples:</p><p>1. X, the clean image -unmodified image from the dataset (either train or test set).</p><p>2. X adv , the adversarial image: the output of any procedure intended to produce an approximate worst-case modification of the clean image. We sometimes call this a candidate adversarial image to emphasize that an adversarial image is not necessarily misclassified by the neural network.</p><p>3. Misclassified adversarial image -candidate adversarial image which is misclassified by the neural network. In addition we are typically interested only in those misclassified adversarial images when the corresponding clean image is correctly classified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">:</head><p>The size of the adversarial perturbation. In most cases, we require the L ∞ norm of the perturbation to be less than , as done by <ref type="bibr" target="#b3">Goodfellow et al. (2014)</ref>. We always specify in terms of pixel values in the range <ref type="bibr">[0,</ref><ref type="bibr">255]</ref>. Note that some other work on adversarial examples minimizes the size of the perturbation rather than imposing a constraint on the size of the perturbation <ref type="bibr" target="#b14">(Szegedy et al., 2014)</ref>.</p><p>5. The cost function used to train the model is denoted J(X, y true ).</p><p>6. Clip X, (A) denotes element-wise clipping A, with A i,j clipped to the range [X i,j − , X i,j + ].</p><p>7. One-step methods of adversarial example generation generate a candidate adversarial image after computing only one gradient. They are often based on finding the optimal perturbation of a linear approximation of the cost or model. Iterative methods apply many gradient updates. They typically do not rely on any approximation of the model and typically produce more harmful adversarial examples when run for more iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">ATTACK METHODS</head><p>We study a variety of attack methods:</p><p>Fast gradient sign method <ref type="bibr" target="#b3">Goodfellow et al. (2014)</ref> proposed the fast gradient sign method (FGSM) as a simple way to generate adversarial examples:</p><formula xml:id="formula_0">X adv = X + sign ∇ X J(X, y true ) (1)</formula><p>This method is simple and computationally efficient compared to more complex methods like L-BFGS <ref type="bibr" target="#b14">(Szegedy et al., 2014)</ref>, however it usually has a lower success rate. On ImageNet, top-1 error rate on candidate adversarial images for the FGSM is about 63% − 69% for ∈ [2, 32].</p><p>One-step target class methods FGSM finds adversarial perturbations which increase the value of the loss function. An alternative approach is to maximize probability p(y target | X) of some specific target class y target which is unlikely to be the true class for a given image. For a neural network with cross-entropy loss this will lead to the following formula for the one-step target class method:</p><formula xml:id="formula_1">X adv = X − sign ∇ X J(X, y target )<label>(2)</label></formula><p>As a target class we can use the least likely class predicted by the network y LL = arg min y p(y | X) , as suggested by <ref type="bibr" target="#b6">Kurakin et al. (2016)</ref>. In such case we refer to this method as one-step least likely class or just "step l.l." Alternatively we can use a random class as target class. In such a case we refer to this method as "step rnd.".</p><p>Basic iterative method A straightforward extension of FGSM is to apply it multiple times with small step size:</p><formula xml:id="formula_2">X adv 0 = X, X adv N +1 = Clip X, X adv N + α sign ∇ X J(X adv N , y true )</formula><p>In our experiments we used α = 1, i.e. we changed the value of each pixel only by 1 on each step. We selected the number of iterations to be min <ref type="bibr">( + 4, 1.25 )</ref>. See more information on this method in <ref type="bibr" target="#b6">Kurakin et al. (2016)</ref>. Below we refer to this method as "iter. basic" method.</p><p>Iterative least-likely class method By running multiple iterations of the "step l.l." method we can get adversarial examples which are misclassified in more than 99% of the cases:</p><formula xml:id="formula_3">X adv 0 = X, X adv N +1 = Clip X, X adv N − α sign ∇ X J(X adv N , y LL )</formula><p>α and number of iterations were selected in the same way as for the basic iterative method. Below we refer to this method as the "iter. l.l.".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ADVERSARIAL TRAINING</head><p>The basic idea of adversarial training is to inject adversarial examples into the training set, continually generating new adversarial examples at every step of training <ref type="bibr" target="#b3">(Goodfellow et al., 2014)</ref>.</p><p>Adversarial training was originally developed for small models that did not use batch normalization. To scale adversarial training to ImageNet, we recommend using batch normalization <ref type="bibr">(Ioffe &amp; Szegedy, 2015)</ref>. To do so successfully, we found that it was important for examples to be grouped into batches containing both normal and adversarial examples before taking each training step, as described in algorithm 1.</p><p>We use a loss function that allows independent control of the number and relative weight of adversarial examples in each batch: Do one training step of network N using minibatch B 7: until training converged and k = 16. Note that we replace each clean example with its adversarial counterpart, for a total minibatch size of 32, which is a departure from previous approaches to adversarial training.</p><formula xml:id="formula_4">Loss = 1 (m − k) + λk i∈CLEAN L(X i |y i ) + λ i∈ADV L(</formula><p>Fraction and weight of adversarial examples which we used in each minibatch differs from <ref type="bibr" target="#b4">Huang et al. (2015)</ref> where authors replaced entire minibatch with adversarial examples. However their experiments was done on smaller datasets (MNIST and CIFAR-10) in which case adversarial training does not lead to decrease of accuracy on clean images. We found that our approach works better for ImageNet models (corresponding comparative experiments could be found in Appendix E).</p><p>We observed that if we fix during training then networks become robust only to that specific value of . We therefore recommend choosing randomly, independently for each training example. In our experiments we achieved best results when magnitudes were drawn from a truncated normal distribution defined in interval [0, 16] with underlying normal distribution N (µ = 0, σ = 8).<ref type="foot" target="#foot_0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We adversarially trained an Inception v3 model <ref type="bibr" target="#b15">(Szegedy et al., 2015)</ref> on ImageNet. All experiments were done using synchronous distributed training on 50 machines, with a minibatch of 32 examples on each machine. We observed that the network tends to reach maximum accuracy at around 130k − 150k iterations. If we continue training beyond 150k iterations then eventually accuracy might decrease by a fraction of a percent. Thus we ran experiments for around 150k iterations and then used the obtained accuracy as the final result of the experiment.</p><p>Similar to <ref type="bibr" target="#b15">Szegedy et al. (2015)</ref> we used RMSProp optimizer for training. We used a learning rate of 0.045 except where otherwise indicated.</p><p>We looked at interaction of adversarial training and other forms or regularization (dropout, label smoothing and weight decay). By default training of Inception v3 model uses all three of them. We noticed that disabling label smoothing and/or dropout leads to small decrease of accuracy on clean examples (by 0.1% -0.5% for top 1 accuracy) and small increase of accuracy on adversarial examples (by 1% -1.5% for top 1 accuracy). On the other hand reducing weight decay leads to decrease of accuracy on both clean and adversarial examples.</p><p>We experimented with delaying adversarial training by 0, 10k, 20k and 40k iterations. In such case we used only clean examples during the first N training iterations and after N iterations included both clean and adversarial examples in the minibatch. We noticed that delaying adversarial training has almost no effect on accuracy on clean examples (difference in accuracy within 0.2%) after sufficient number of training iterations (more than 70k in our case). At the same time we noticed that larger delays of adversarial training might cause up to 4% decline of accuracy on adversarial examples with high magnitude of adversarial perturbations. For small 10k delay changes of accuracy was not statistically significant to recommend against it. We used a delay of 10k because this allowed us to reuse the same partially trained model as a starting point for many different experiments.</p><p>For evaluation we used the ImageNet validation set which contains 50, 000 images and does not intersect with the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">RESULTS OF ADVERSARIAL TRAINING</head><p>We experimented with adversarial training using several types of one-step methods. We found that adversarial training using any type of one-step method increases robustness to all types of one-step adversarial examples that we tested. However there is still a gap between accuracy on clean and adversarial examples which could vary depending on the combination of methods used for training and evaluation.</p><p>Adversarial training caused a slight (less than 1%) decrease of accuracy on clean examples in our Im-ageNet experiments. This differs from results of adversarial training reported previously, where adversarial training increased accuracy on the test set <ref type="bibr" target="#b3">(Goodfellow et al., 2014;</ref><ref type="bibr" target="#b8">Miyato et al., 2016b;</ref><ref type="bibr">a)</ref>. One possible explanation is that adversarial training acts as a regularizer. For datasets with few labeled examples where overfitting is the primary concern, adversarial training reduces test error. For datasets like ImageNet where state-of-the-art models typically have high training set error, adding a regularizer like adversarial training can increase training set error more than it decreases the gap between training and test set error. Our results suggest that adversarial training should be employed in two scenarios:</p><p>1. When a model is overfitting, and a regularizer is required. 2. When security against adversarial examples is a concern. In this case, adversarial training is the method that provides the most security of any known defense, while losing only a small amount of accuracy.</p><p>By comparing different one-step methods for adversarial training we observed that the best results in terms or accuracy on test set are achieved using "step l.l." or "step rnd." method. Moreover using these two methods helped the model to become robust to adversarial examples generated by other one-step methods. Thus for final experiments we used "step l.l." adversarial method.</p><p>For brevity we omitted a detailed comparison of different one-step methods here, but the reader can find it in Appendix A.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">LABEL LEAKING</head><p>We discovered a label leaking effect: when a model is trained on FGSM adversarial examples and then evaluated using FGSM adversarial examples, the accuracy on adversarial images becomes much higher than the accuracy on clean images (see Table <ref type="table" target="#tab_5">3</ref>). This effect also occurs (but to a lesser degree) when using other one-step methods that require the true label as input.</p><p>We say that label for specific example has been leaked if and only if the model classifies an adversarial example correctly when that adversarial example is generated using the true label but misclassifies a corresponding adversarial example that was created without using the true label. If too many labels has been leaked then accuracy on adversarial examples might become bigger than accuracy on clean examples which we observed on ImageNet dataset.</p><p>We believe that the effect occurs because one-step methods that use the true label perform a very simple and predictable transformation that the model can learn to recognize. The adversarial example construction process thus inadvertently leaks information about the true label into the input. We found that the effect vanishes if we use adversarial example construction processes that do not use the true label. The effect also vanishes if an iterative method is used, presumably because the output of an iterative process is more diverse and less predictable than the output of a one-step process.</p><p>Overall due to the label leaking effect, we do not recommend to use FGSM or other methods defined with respect to the true class label to evaluate robustness to adversarial examples; we recommend to use other one-step methods that do not directly access the label instead.</p><p>We recommend to replace the true label with the most likely label predicted by the model. Alternately, one can maximize the cross-entropy between the full distribution over all predicted labels given the clean input and the distribution over all predicted labels given the perturbed input <ref type="bibr" target="#b8">(Miyato et al., 2016b)</ref>.</p><p>We revisited the adversarially trained MNIST classifier from <ref type="bibr" target="#b3">Goodfellow et al. (2014)</ref>     We studied how the size of the model (in terms of number of parameters) could affect robustness to adversarial examples. We picked Inception v3 as a base model and varied its size by changing the number of filters in each convolution.</p><p>For each experiment we picked a scale factor ρ and multiplied the number of filters in each convolution by ρ. In other words ρ = 1 means unchanged Inception v3, ρ = 0.5 means Inception with half of the usual number of filters in convolutions, etc . . . For each chosen ρ we trained two independent models: one with adversarial training and another without. Then we evaluated accuracy on clean and adversarial examples for both trained models. We have run these experiments for ρ ∈ [0.5, 2.0].</p><p>In earlier experiments (Table <ref type="table" target="#tab_2">1</ref>) we found that deeper models benefit more from adversarial training. The increased depth changed many aspects of the model architecture. These experiments varying ρ examine the effect in a more controlled setting, where the architecture remains constant except for the number of feature maps in each layer.</p><p>In all experiments we observed that accuracy on clean images kept increasing with increase of ρ, though its increase slowed down as ρ became bigger. Thus as a measure of robustness we used the ratio of accuracy on adversarial images to accuracy on clean images because an increase of this ratio means that the gap between accuracy on adversarial and clean images becomes smaller. If this ratio reaches 1 then the accuracy on adversarial images is the same as on clean ones. For a successful adversarial example construction technique, we would never expect this ratio to exceed 1, since this would imply that the adversary is actually helpful. Some defective adversarial example construction techniques, such as those suffering from label leaking, can inadvertently produce a ratio greater than 1.</p><p>Results with ratios of accuracy for various adversarial methods and are provided in Fig. <ref type="figure" target="#fig_1">1</ref>.</p><p>For models without adversarial training, we observed that there is an optimal value of ρ yielding best robustness. Models that are too large or too small perform worse. This may indicate that models become more robust to adversarial examples until they become large enough to overfit in some respect.</p><p>For adversarially trained models, we found that robustness consistently increases with increases in model size. We were not able to train large enough models to find when this process ends, but we did find that models with twice the normal size have an accuracy ratio approaching 1 for one-step adversarial examples. When evaluated on iterative adversarial examples, the trend toward increasing robustness with increasing size remains but has some exceptions. Also, none of our models was large enough to approach an accuracy ratio of 1 in this regime.</p><p>Overall we recommend exploring increase of accuracy (along with adversarial training) as a measure to improve robustness to adversarial examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">TRANSFERABILITY OF ADVERSARIAL EXAMPLES</head><p>From a security perspective, an important property of adversarial examples is that they tend to transfer from one model to another, enabling an attacker in the black-box scenario to create adversarial   Transfer rate was computed using two Inception v3 models with different random intializations. As could be seen from these plots, increase of leads to increase of transfer rate. It should be noted that transfer rate is a ratio of number of transferred adversarial examples to number of successful adversarial examples for source network. Both numerator and denominator of this ratio are increasing with increase of , however we observed that numerator (i.e. number of transferred examples) is increasing much faster compared to increase of denominator. For example when increases from 8 to 16 relative increase of denominator is less than 1% for each of the considered methods, at the same time relative increase of numerator is more than 20%.</p><p>examples for their own substitute model, then deploy those adversarial examples to fool a target model <ref type="bibr" target="#b14">(Szegedy et al., 2014;</ref><ref type="bibr" target="#b3">Goodfellow et al., 2014;</ref><ref type="bibr" target="#b9">Papernot et al., 2016b)</ref>.</p><p>We studied transferability of adversarial examples between the following models: two copies of normal Inception v3 (with different random initializations and order or training examples), Inception v4 <ref type="bibr" target="#b16">(Szegedy et al., 2016)</ref> and Inception v3 which uses ELU activation <ref type="bibr" target="#b1">(Clevert et al., 2015)</ref> instead of Relu<ref type="foot" target="#foot_1">2</ref> . All of these models were independently trained from scratch until they achieved maximum accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Published as a conference paper at ICLR 2017</head><p>In each experiment we fixed the source and target networks, constructed adversarial examples from 1000 randomly sampled clean images from the test set using the source network and performed classification of all of them using both source and target networks. These experiments were done independently for different adversarial methods.</p><p>We measured transferability using the following criteria. Among 1000 images we picked only misclassified adversarial example for the source model (i.e. clean classified correctly, adversarial misclassified) and measured what fraction of them were misclassified by the target model.</p><p>Transferability results for all combinations of models and = 16 are provided in Table <ref type="table" target="#tab_6">4</ref>. Results for various but fixed source and target model are provided in Fig. <ref type="figure" target="#fig_3">2</ref>.</p><p>As can be seen from the results, FGSM adversarial examples are the most transferable, while "iter l.l." are the least. On the other hand "iter l.l." method is able to fool the network in more than 99% cases (top 1 accuracy), while FGSM is the least likely to fool the network. This suggests that there might be an inverse relationship between transferability of specific method and ability of the method to fool the network. We haven't studied this phenomenon further, but one possible explanation could be the fact that iterative methods tend to overfit to specific network parameters.</p><p>In addition, we observed that for each of the considered methods transfer rate is increasing with increase of (see Fig. <ref type="figure" target="#fig_3">2</ref>). Thus potential adversary performing a black-box attack have an incentive to use higher to increase the chance of success of the attack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A COMPARISON OF ONE-STEP ADVERSARIAL METHODS</head><p>In addition to FGSM and "step l.l." methods we explored several other one-step adversarial methods both for training and evaluation. Generally all of these methods can be separated into two large categories. Methods which try to maximize the loss (similar to FGSM) are in the first category. The second category contains methods which try to maximize the probability of a specific target class (similar to "step l.l."). We also tried to use different types of random noise instead of adversarial images, but random noise didn't help with robustness against adversarial examples.</p><p>The full list of one-step methods we tried is as follows:</p><p>• Methods increasing loss function J -FGSM (described in details in Section 2.2):</p><formula xml:id="formula_5">X adv = X + sign ∇ X J(X, y true )</formula><p>-FGSM-pred or fast method with predicted class. It is similar to FGSM but uses the label of the class predicted by the network instead of true class y true . -"Fast entropy" or fast method designed to maximize the entropy of the predicted distribution, thereby causing the model to become less certain of the predicted class. -"Fast grad. L 2 " is similar to FGSM but uses the value of gradient instead of its sign.</p><p>The value of gradient is normalized to have unit L 2 norm:</p><formula xml:id="formula_6">X adv = X + ∇ X J(X, y true ) ∇ X J(X, y true ) 2</formula><p>Miyato et al. (2016b) advocate this method. -"Fast grad. L ∞ " is similar to "fast grad. L 2 " but uses L ∞ norm for normalization.</p><p>• Methods increasing the probability of the selected target class -"Step l.l." is one-step towards least likely class (also described in Section 2.2):</p><formula xml:id="formula_7">X adv = X − sign ∇ X J(X, y target )</formula><p>where y target = arg min y p(y | X) is least likely class prediction by the network. -"Step rnd." is similar to "step l.l." but uses random class instead of least likely class.</p><p>• Random perturbations -Sign of random perturbation. This is an attempt to construct random perturbation which has similar structure to perturbations generated by FGSM:</p><formula xml:id="formula_8">X adv = X + sign N</formula><p>where N is random normal variable with zero mean and identity covariance matrix. -Random truncated normal perturbation with zero mean and 0.5 standard deviation defined on [− , ] and uncorrelated pixels, which leads to the following formula for perturbed images:</p><formula xml:id="formula_9">X adv = X + T</formula><p>where T is a random variable with truncated normal distribution.</p><p>Overall, we observed that using only one of these single step methods during adversarial training is sufficient to gain robustness to all of them. Fig. <ref type="figure" target="#fig_4">3</ref> shows accuracy on various one-step adversarial examples when the network was trained using only "step l.l." method.</p><p>At the same time we observed that not all one-step methods are equally good for adversarial training, as shown in Step l.l.</p><p>Step rnd. This result might be interesting because it models the following attack. Instead of trying to pick "good" adversarial images an adversary tries to modify all available images in order to get as much misclassified images as possible.</p><p>To compute the error rate we randomly generated 1000 adversarial images using the source model and then classified them using the target model. Results for various models, adversarial methods and fixed = 16 are provided in Table <ref type="table" target="#tab_9">6</ref>. Results for fixed source and target models and various are provided in Fig. <ref type="figure" target="#fig_10">5</ref>.</p><p>Overall the error rate of transferred adversarial examples exhibits the same behavior as the transfer rate described in Section 4.4.    </p><formula xml:id="formula_10">relu6(x) = min(relu(x), 6) • ReluDecay β (x) = relu(x) 1+βrelu(x) 2 for β ∈ {0.1, 0.01, 0.001}</formula><p>Training converged using all of these activations, however test performance was not necessarily the same as with relu.</p><p>tanh and ReluDecay β=0.1 lose about 2%-3% of accuracy on clean examples and about 10%-20% on "step l.l." adversarial examples. relu6, ReluDecay β=0.01 and ReluDecay β=0.001 demonstrated similar accuracy (within ±1%) to relu on clean images and few percent loss of accuracy on "step l.l." images. At the same time all non-linear activation functions increased classification accuracy on some of the iterative adversarial images. Detailed results are provided in Table <ref type="table" target="#tab_10">7</ref>.   Overall non linear activation functions could be used as an additional measure of defense against iterative adversarial images. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>With adversarial training, "basic iter." adv. examples</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Influence of size of the model on top 1 classification accuracy of various adversarial examples. Left column -base model without adversarial training, right column -model with adversarial training using "step l.l." method. Top row -results on "step l.l." adversarial images, middle row -results on "iter. l.l." adversarial images, bottom row -results on "basic iter." adversarial images. See text of Section 4.3 for explanation of meaning of horizontal and vertical axes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Influence of the size of adversarial perturbation on transfer rate of adversarial examples.Transfer rate was computed using two Inception v3 models with different random intializations. As could be seen from these plots, increase of leads to increase of transfer rate. It should be noted that transfer rate is a ratio of number of transferred adversarial examples to number of successful adversarial examples for source network. Both numerator and denominator of this ratio are increasing with increase of , however we observed that numerator (i.e. number of transferred examples) is increasing much faster compared to increase of denominator. For example when increases from 8 to 16 relative increase of denominator is less than 1% for each of the considered methods, at the same time relative increase of numerator is more than 20%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Comparison of different one-step adversarial methods during eval. Adversarial training was done using "step l.l." method. Some evaluation methods show increasing accuracy with increasing over part of the curve, due to the label leaking effect.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>With adversarial training, "basic iter." adv. examples</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Influence of size of the model on top 5 classification accuracy of various adversarial examples. For a detailed explanation see Section 4.3 and Figure 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>•</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Influence of the size of adversarial perturbation on the error rate on adversarial examples generated for one model and classified using another model. Both source and target models were Inception v3 networks with different random intializations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>X|y) is a loss on a single example X with true class y; m is total number of training examples in the minibatch; k is number of adversarial examples in the minibatch and λ is a parameter which controls the relative weight of adversarial examples in the loss. We used λ = 0.3, m = 32, Algorithm 1 Adversarial training of network N . Size of the training minibatch is m. Number of adversarial images in the minibatch is k.</figDesc><table><row><cell>X adv i adv } from corresponding |y i ) Read minibatch B = {X 1 , . . . , X m } from training set 2: repeat 3: 4: Generate k adversarial examples {X 1 adv , . . . , X k clean examples {X 1 , . . . , X k } using current state of the network N 5: Make new minibatch B = {X 1 adv , . . . , X k adv , X k+1 , . . . , X m } where L(1: Randomly initialize network N 6:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell>Clean</cell><cell>= 2</cell><cell>= 4</cell><cell>= 8</cell><cell>= 16</cell></row><row><cell>Baseline</cell><cell cols="5">top 1 78.4% 30.8% 27.2% 27.2% 29.5%</cell></row><row><cell cols="6">(standard training) top 5 94.0% 60.0% 55.6% 55.1% 57.2%</cell></row><row><cell>Adv. training</cell><cell cols="5">top 1 77.6% 73.5% 74.0% 74.5% 73.9%</cell></row><row><cell></cell><cell cols="5">top 5 93.8% 91.7% 91.9% 92.0% 91.4%</cell></row><row><cell>Deeper model</cell><cell cols="5">top 1 78.7% 33.5% 30.0% 30.0% 31.6%</cell></row><row><cell cols="6">(standard training) top 5 94.4% 63.3% 58.9% 58.1% 59.5%</cell></row><row><cell>Deeper model</cell><cell cols="5">top 1 78.1% 75.4% 75.7% 75.6% 74.4%</cell></row><row><cell>(Adv. training)</cell><cell cols="5">top 5 94.1% 92.6% 92.7% 92.5% 91.6%</cell></row><row><cell cols="6">Results of adversarial training using "step l.l." method are provided in Table 1. As it can be seen from</cell></row><row><cell cols="6">the table we were able to significantly increase top-1 and top-5 accuracy on adversarial examples (up</cell></row><row><cell cols="6">to 74% and 92% correspondingly) to make it to be on par with accuracy on clean images. However</cell></row><row><cell cols="2">we lost about 0.8% accuracy on clean examples.</cell><cell></cell><cell></cell><cell></cell></row></table><note>Top 1 and top 5 accuracies of an adversarially trained network on clean images and adversarial images with various test-time . Both training and evaluation were done using "step l.l." method. Adversarially training caused the baseline model to become robust to adversarial examples but lost some accuracy on clean examples. We therefore also trained a deeper model with two additional Inception blocks. The deeper model benefits more from adversarial training in terms of robustness to adversarial perturbation, and loses less accuracy on clean examples than the smaller model does.We were able to slightly reduce the gap in the accuracy on clean images by slightly increasing the size of the model. This was done by adding two additional Inception blocks to the model. For specific details about Inception blocks refer to<ref type="bibr" target="#b15">Szegedy et al. (2015)</ref>.Unfortunately, training on one-step adversarial examples does not confer robustness to iterative adversarial examples, as shown in Table2.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Accuracy of adversarially trained network on iterative adversarial examples. Adversarial training was done using "step l.l." method. Results were computed after 140k iterations of training. Overall, we see that training on one-step adversarial examples does not confer resistance to iterative adversarial examples. We also tried to use iterative adversarial examples during training, however we were unable to gain any benefits out of it. It is computationally costly and we were not able to obtain robustness to adversarial examples or to prevent the procedure from reducing the accuracy on clean examples significantly. It is possible that much larger models are necessary to achieve robustness to such a large class of inputs.</figDesc><table><row><cell cols="2">Adv. method Training</cell><cell>Clean</cell><cell>= 2</cell><cell>= 4</cell><cell>= 8</cell><cell>= 16</cell></row><row><cell>Iter. l.l.</cell><cell cols="4">Adv. training top 1 77.4% 29.1% 7.5%</cell><cell>3.0%</cell><cell>1.5%</cell></row><row><cell></cell><cell></cell><cell cols="4">top 5 93.9% 56.9% 21.3% 9.4%</cell><cell>5.5%</cell></row><row><cell></cell><cell>Baseline</cell><cell cols="3">top 1 78.3% 23.3% 5.5%</cell><cell>1.8%</cell><cell>0.7%</cell></row><row><cell></cell><cell></cell><cell cols="4">top 5 94.1% 49.3% 18.8% 7.8%</cell><cell>4.4%</cell></row><row><cell>Iter. basic</cell><cell cols="6">Adv. training top 1 77.4% 30.0% 25.2% 23.5% 23.2%</cell></row><row><cell></cell><cell></cell><cell cols="5">top 5 93.9% 44.3% 33.6% 28.4% 26.8%</cell></row><row><cell></cell><cell>Baseline</cell><cell cols="5">top 1 78.3% 31.4% 28.1% 26.4% 25.9%</cell></row><row><cell></cell><cell></cell><cell cols="5">top 5 94.1% 43.1% 34.8% 30.2% 28.8%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Effect of label leaking on adversarial examples. When training and evaluation was done using FGSM accuracy on adversarial examples was higher than on clean examples. This effect was not happening when training and evaluation was done using "step l.l." method. In both experiments training was done for 150k iterations with initial learning rate 0.0225.</figDesc><table><row><cell>Clean</cell><cell>= 2</cell><cell>= 4</cell><cell>= 8</cell><cell>= 16</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Transfer rate of adversarial examples generated using different adversarial methods and perturbation size = 16. This is equivalent to the error rate in an attack scenario where the attacker prefilters their adversarial examples by ensuring that they are misclassified by the source model before deploying them against the target. Transfer rates are rounded to the nearest percent in order to fit the table on the page. The following models were used for comparison: A and B are Inception v3 models with different random initializations, C is Inception v3 model with ELU activations instead of Relu, D is Inception v4 model. See also Table6for the absolute error rate when the attack is not prefiltered, rather than the transfer rate of adversarial examples.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">FGSM</cell><cell></cell><cell></cell><cell cols="2">basic iter.</cell><cell></cell><cell>iter l.l.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>source</cell><cell></cell><cell></cell><cell cols="3">target model</cell><cell></cell><cell cols="2">target model</cell><cell></cell><cell>target model</cell></row><row><cell></cell><cell></cell><cell></cell><cell>model</cell><cell></cell><cell>A</cell><cell>B</cell><cell>C</cell><cell>D</cell><cell>A</cell><cell>B</cell><cell>C</cell><cell>D</cell><cell>A</cell><cell>B</cell><cell>C</cell><cell>D</cell></row><row><cell cols="4">top 1 A (v3)</cell><cell></cell><cell cols="9">100 56 58 47 100 46 45 33 100 13 13</cell><cell>9</cell></row><row><cell></cell><cell></cell><cell></cell><cell>B (v3)</cell><cell></cell><cell cols="4">58 100 59 51</cell><cell cols="4">41 100 40 30</cell><cell>15 100 13 10</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">C (v3 ELU) 56 58 100 52</cell><cell cols="4">44 44 100 32</cell><cell>12 11 100 9</cell></row><row><cell></cell><cell></cell><cell></cell><cell>D (v4)</cell><cell></cell><cell cols="9">50 54 52 100 35 39 37 100 12 13 13 100</cell></row><row><cell cols="4">top 5 A (v3)</cell><cell></cell><cell cols="9">100 50 50 36 100 15 17 11 100 8</cell><cell>7</cell><cell>5</cell></row><row><cell></cell><cell></cell><cell></cell><cell>B (v3)</cell><cell></cell><cell cols="4">51 100 50 37</cell><cell cols="4">16 100 14 10</cell><cell>7 100 5</cell><cell>4</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">C (v3 ELU) 44 45 100 37</cell><cell cols="4">16 18 100 13</cell><cell>6</cell><cell>6 100 4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>D (v4)</cell><cell></cell><cell cols="8">42 38 46 100 11 15 15 100</cell><cell>6</cell><cell>6</cell><cell>6 100</cell></row><row><cell>top 1 transfer rate</cell><cell>0.2 0.3 0.4 0.5 0.6</cell><cell></cell><cell cols="2">fast basic iter. iter l.l.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.0</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell><cell>12</cell><cell>14</cell><cell>16</cell><cell></cell><cell></cell><cell></cell></row></table><note>Top 1 transferability.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>The best results (achieving both good accuracy on clean data and good accuracy on adversarial inputs) were obtained when adversarial training was done using "step l.l." or "step rnd." methods.</figDesc><table><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>top1 accuracy</cell><cell>0.5 0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>top1 accuracy</cell><cell>0.5 0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>4</cell><cell>8</cell><cell>12</cell><cell>16 epsilon</cell><cell>20</cell><cell>24</cell><cell>28</cell><cell></cell><cell>0</cell><cell>4</cell><cell>8</cell><cell>12</cell><cell>16 epsilon</cell><cell>20</cell><cell>24</cell><cell>28</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">With adversarial training</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">No adversarial training</cell><cell></cell></row><row><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>top5 accuracy</cell><cell>0.7 0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>top5 accuracy</cell><cell>0.7 0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.5</cell><cell>0</cell><cell>4</cell><cell>8</cell><cell>12</cell><cell>16 epsilon</cell><cell>20</cell><cell>24</cell><cell>28</cell><cell>0.5</cell><cell>0</cell><cell>4</cell><cell>8</cell><cell>12</cell><cell>16 epsilon</cell><cell>20</cell><cell>24</cell><cell>28</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">With adversarial training</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">No adversarial training</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Clean FGSM</cell><cell cols="3">FGSM-pred Fast entropy</cell><cell cols="3">Fast grad. L 2 Fast grad. L ∞</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Comparison of different one-step adversarial methods for adversarial training. The evaluation was run after 90k training steps. *) In all cases except "fast grad L 2 " and "fast grad L ∞ " the evaluation was done using FGSM. For "fast grad L 2 " and "fast grad L ∞ " the evaluation was done using "step l.l." method. In the case where both training and testing were done with FGSM, the performance on adversarial examples is artificially high due to the label leaking effect. Based on this table, we recommend using "step rnd." or "step l.l." as the method of generating adversarial examples at training time, in order to obtain good accuracy on both clean and adversarial examples. We computed 95% confidence intervals based on the standard error of the mean around the test error, using the fact that the test error was evaluated with 50,000 samples. Within each column, we indicate which methods are statistically tied for the best using bold face.</figDesc><table><row><cell></cell><cell>Clean</cell><cell>= 2</cell><cell>= 4</cell><cell>= 8</cell><cell>= 16</cell></row><row><cell>No adversarial training</cell><cell>76.8%</cell><cell cols="4">40.7% 39.0% 37.9% 36.7%</cell></row><row><cell>FGSM</cell><cell>74.9%</cell><cell>79.3%</cell><cell>82.8%</cell><cell>85.3%</cell><cell>83.2%</cell></row><row><cell>Fast with predicted class</cell><cell>76.4%</cell><cell cols="4">43.2% 42.0% 40.9% 40.0%</cell></row><row><cell>Fast entropy</cell><cell>76.4%</cell><cell cols="4">62.8% 61.7% 59.5% 54.8%</cell></row><row><cell>Step rnd.</cell><cell cols="5">76.4% 73.0% 75.4% 76.5% 72.5%</cell></row><row><cell>Step l.l.</cell><cell cols="5">76.3% 72.9% 75.1% 76.2% 72.2%</cell></row><row><cell>Fast grad. L 2 *</cell><cell>76.8%</cell><cell cols="4">44.0% 33.2% 26.4% 22.5%</cell></row><row><cell>Fast grad. L ∞ *</cell><cell cols="5">75.6% 52.2% 39.7% 30.9% 25.0%</cell></row><row><cell cols="2">Sign of random perturbation 76.5%</cell><cell cols="4">38.8% 36.6% 35.0% 32.7%</cell></row><row><cell cols="2">Random normal perturbation 76.6%</cell><cell cols="4">38.3% 36.0% 34.4% 31.8%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Error rates on adversarial examples transferred between models, rounded to the nearest percent. Results are provided for adversarial images generated using different adversarial methods and fixed perturbation size = 16. The following models were used for comparison: A and B are Inception v3 models with different random initializations, C is Inception v3 model with ELU activations instead of Relu, D is Inception v4 model. See alsoTable 4 for the transfer rate of adversarial examples, rather than the absolute error rate.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">FGSM</cell><cell></cell><cell cols="2">basic iter.</cell><cell></cell><cell></cell><cell cols="2">iter l.l.</cell><cell></cell></row><row><cell>source</cell><cell></cell><cell cols="2">target model</cell><cell></cell><cell cols="2">target model</cell><cell></cell><cell></cell><cell cols="2">target model</cell><cell></cell></row><row><cell>model</cell><cell>A</cell><cell>B</cell><cell>C</cell><cell>D A</cell><cell>B</cell><cell>C</cell><cell>D</cell><cell>A</cell><cell>B</cell><cell>C</cell><cell>D</cell></row><row><cell>top 1 A (v3)</cell><cell cols="11">65 52 53 45 78 51 50 42 100 32 31 27</cell></row><row><cell>B (v3)</cell><cell cols="11">52 66 54 48 50 79 51 43 35 99 34 29</cell></row><row><cell cols="12">C (v3 ELU) 53 55 70 50 47 46 74 40 31 30 100 28</cell></row><row><cell>D (v4)</cell><cell cols="11">47 51 49 62 43 46 45 73 30 31 31 99</cell></row><row><cell>top 5 A (v3)</cell><cell cols="10">46 28 28 22 76 17 18 13 94 12 12</cell><cell>9</cell></row><row><cell>B (v3)</cell><cell cols="11">29 46 30 22 19 76 18 16 13 96 12 11</cell></row><row><cell cols="11">C (v3 ELU) 28 29 55 25 18 19 74 15 12 12 96</cell><cell>9</cell></row><row><cell>D (v4)</cell><cell cols="11">23 22 25 40 14 16 16 70 11 11 11 97</cell></row><row><cell cols="8">D RESULTS WITH DIFFERENT ACTIVATION FUNCTIONS</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>We evaluated robustness to adversarial examples when the network was trained using various nonlinear activation functions instead of the standard relu activation when used with adversarial training on "step l.l." adversarial images. We tried to use following activation functions instead of relu:• tanh(x)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Activation functions and robustness to adversarial examples. For each activation function we adversarially trained the network on "step l.l." adversarial images and then run classification of clean images and adversarial images generated using various adversarial methods and .We studied how number of adversarial examples k in the minibatch affect accuracy on clean and adversarial examples. Results are summarized in Table8.Overall we noticed that increase of k lead to increase of accuracy on adversarial examples and to decrease of accuracy on clean examples. At the same having more than half of adversarial examples in the minibatch (which correspond to k &gt; 16 in our case) does not provide significant improvement of accuracy on adversarial images, however lead to up to 1% of additional decrease of accuracy on clean images. Thus for most experiments in the paper we have chosen k = 16 as a reasonable trade-off between accuracy on clean and adversarial images.</figDesc><table><row><cell cols="2">Adv. method Activation</cell><cell>Clean</cell><cell>= 2</cell><cell>= 4</cell><cell>= 8</cell><cell>= 16</cell></row><row><cell>Step l.l.</cell><cell>relu</cell><cell cols="5">77.5% 74.6% 75.1% 75.5% 74.5%</cell></row><row><cell></cell><cell>relu6</cell><cell cols="5">77.7% 71.8% 73.5% 74.5% 74.0%</cell></row><row><cell></cell><cell cols="6">ReluDecay 0.001 78.0% 74.0% 74.9% 75.2% 73.9%</cell></row><row><cell></cell><cell>ReluDecay 0.01</cell><cell cols="5">77.4% 73.6% 74.6% 75.0% 73.6%</cell></row><row><cell></cell><cell>ReluDecay 0.1</cell><cell cols="5">75.3% 67.5% 67.5% 67.0% 64.8%</cell></row><row><cell></cell><cell>tanh</cell><cell cols="5">74.5% 63.7% 65.1% 65.8% 61.9%</cell></row><row><cell>Iter. l.l.</cell><cell>relu</cell><cell cols="2">77.5% 30.2%</cell><cell>8.0%</cell><cell>3.1%</cell><cell>1.6%</cell></row><row><cell></cell><cell>relu6</cell><cell cols="3">77.7% 39.8% 13.7%</cell><cell>4.1%</cell><cell>1.9%</cell></row><row><cell></cell><cell cols="4">ReluDecay 0.001 78.0% 39.9% 12.6%</cell><cell>3.8%</cell><cell>1.8%</cell></row><row><cell></cell><cell>ReluDecay 0.01</cell><cell cols="3">77.4% 36.2% 11.2%</cell><cell>3.2%</cell><cell>1.6%</cell></row><row><cell></cell><cell>ReluDecay 0.1</cell><cell cols="3">75.3% 47.0% 25.8%</cell><cell>6.5%</cell><cell>2.4%</cell></row><row><cell></cell><cell>tanh</cell><cell cols="2">74.5% 35.8%</cell><cell>6.6%</cell><cell>2.7%</cell><cell>0.9%</cell></row><row><cell>Basic iter.</cell><cell>relu</cell><cell cols="5">77.5% 28.4% 23.2% 21.5% 21.0%</cell></row><row><cell></cell><cell>relu6</cell><cell cols="5">77.7% 31.2% 26.1% 23.8% 23.2%</cell></row><row><cell></cell><cell cols="6">ReluDecay 0.001 78.0% 32.9% 27.2% 24.7% 24.1%</cell></row><row><cell></cell><cell>ReluDecay 0.01</cell><cell cols="5">77.4% 30.0% 24.2% 21.4% 20.5%</cell></row><row><cell></cell><cell>ReluDecay 0.1</cell><cell cols="5">75.3% 26.7% 20.6% 16.5% 15.2%</cell></row><row><cell></cell><cell>tanh</cell><cell cols="5">74.5% 24.5% 22.0% 20.9% 20.7%</cell></row><row><cell cols="7">E RESULTS WITH DIFFERENT NUMBER OF ADVERSARIAL EXAMPLES IN THE</cell></row><row><cell>MINIBATCH</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Results of adversarial training depending on k -number of adversarial examples in the minibatch. Adversarial examples for training and evaluation were generated using step l.l. method. Row 'No adv' is a baseline result without adversarial training (which is equivalent to k = 0). Rows 'Adv, k = X' are results of adversarial training with X adversarial examples in the minibatch. Total minibatch size is 32, thus k = 32 correspond to minibatch without clean examples. = 16 77.6% 73.8% 75.3% 76.1% 75.4% Adv, k = 24 77.1% 73.0% 75.3% 76.2% 76.0% Adv, k = 32 76.3% 73.4% 75.1% 75.9% 75.8%</figDesc><table><row><cell></cell><cell>Clean</cell><cell>= 2</cell><cell>= 4</cell><cell>= 8</cell><cell>= 16</cell></row><row><cell>No adv</cell><cell cols="5">78.2% 31.5% 27.7% 27.8% 29.7%</cell></row><row><cell>Adv, k = 4</cell><cell cols="5">78.3% 71.7% 71.3% 69.4% 65.8%</cell></row><row><cell>Adv, k = 8</cell><cell cols="5">78.1% 73.2% 73.2% 72.6% 70.5%</cell></row><row><cell>Adv, k</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">In TensorFlow this could be achieved by tf.abs(tf.truncated normal(shape, mean=0, stddev=8)).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">We achieved 78.0% top 1 and 94.1% top 5 accuracy on Inception v3 with ELU activations, which is comparable with accuracy of Inception v3 model with Relu activations.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2">CONCLUSIONIn this paper we studied how to increase robustness to adversarial examples of large models (Inception v3) trained on large dataset (ImageNet). We showed that adversarial training provides robustness to adversarial examples generated using one-step methods. While adversarial training didn't help much against iterative methods we observed that adversarial examples generated by iterative methods are less likely to be transferred between networks, which provides indirect robustness against black box adversarial attacks. In addition we observed that increase of model capacity could also help to increase robustness to adversarial examples especially when used in conjunction with adversarial training. Finally we discovered the effect of label leaking which resulted in higher accuracy on FGSM adversarial examples compared to clean examples when the network was adversarially trained.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Evasion attacks against machine learning at test time</title>
		<author>
			<persName><forename type="first">Battista</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igino</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Maiorca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blaine</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nedim</forename><surname>Šrndić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Laskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giorgio</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="387" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName><forename type="first">Djork-Arné</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno>CoRR, abs/1511.07289</idno>
		<ptr target="http://arxiv.org/abs/1511.07289" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adversarial classification</title>
		<author>
			<persName><forename type="first">Nilesh</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Sanghai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Verma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="99" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>CoRR, abs/1412.6572</idno>
		<ptr target="http://arxiv.org/abs/1412.6572" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning with a strong adversary</title>
		<author>
			<persName><forename type="first">Ruitong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Csaba</forename><surname>Szepesvári</surname></persName>
		</author>
		<idno>CoRR, abs/1511.03034</idno>
		<ptr target="http://arxiv.org/abs/1511.03034" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<idno>arXiv</idno>
		<ptr target="https://arxiv.org/abs/1607.02533" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Virtual adversarial training for semi-supervised text classification</title>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07725</idno>
		<imprint>
			<date type="published" when="2016">2016a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distributional smoothing with virtual adversarial training</title>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin-Ichi</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken</forename><surname>Nakae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin</forename><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR2016)</title>
				<imprint>
			<date type="published" when="2016-04">April 2016b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1605.07277" />
		<title level="m">Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples</title>
				<imprint>
			<date type="published" when="2016-05">May 2016b</date>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Distillation as a defense to adversarial perturbations against deep neural networks</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">Drew</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
		<idno>CoRR, abs/1511.04508</idno>
		<ptr target="http://arxiv.org/abs/1511.04508" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Practical black-box attacks against deep learning systems using adversarial examples</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">Drew</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Berkay</forename><surname>Celik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
		<idno>CoRR, abs/1602.02697</idno>
		<ptr target="http://arxiv.org/abs/1602.02697" />
		<imprint>
			<date type="published" when="2016">2016a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Andras</forename><surname>Rozsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Günther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terrance</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.04563</idno>
		<title level="m">Are accuracy and robustness correlated?</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0575</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno>ICLR, abs/1312.6199</idno>
		<ptr target="http://arxiv.org/abs/1312.6199" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
		<idno>CoRR, abs/1512.00567</idno>
		<ptr target="http://arxiv.org/abs/1512.00567" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<idno>CoRR, abs/1602.07261</idno>
		<ptr target="http://arxiv.org/abs/1602.07261" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
