<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spatio-Temporal Convolutional Features with Nested LSTM for Facial Expression Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-08-15">August 15, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Zhenbo</forename><surname>Yu</surname></persName>
							<email>zbyu@nuist.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information and Control</orgName>
								<orgName type="department" key="dep2">Science and Technology</orgName>
								<orgName type="laboratory">Jiangsu Key Laboratory of Big Data Analysis Technology</orgName>
								<orgName type="institution">Nanjing University of Information</orgName>
								<address>
									<postCode>210044</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guangcan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information and Control</orgName>
								<orgName type="department" key="dep2">Science and Technology</orgName>
								<orgName type="laboratory">Jiangsu Key Laboratory of Big Data Analysis Technology</orgName>
								<orgName type="institution">Nanjing University of Information</orgName>
								<address>
									<postCode>210044</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qingshan</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information and Control</orgName>
								<orgName type="department" key="dep2">Science and Technology</orgName>
								<orgName type="laboratory">Jiangsu Key Laboratory of Big Data Analysis Technology</orgName>
								<orgName type="institution">Nanjing University of Information</orgName>
								<address>
									<postCode>210044</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Computing</orgName>
								<orgName type="department" key="dep2">Office: 351 Huxley Building</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<addrLine>180 Queens Gate</addrLine>
									<postCode>SW7 2AZ</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Spatio-Temporal Convolutional Features with Nested LSTM for Facial Expression Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-08-15">August 15, 2018</date>
						</imprint>
					</monogr>
					<idno type="MD5">6BD33DBFF6E3C3D9FE0888CA9D4EF0DE</idno>
					<idno type="DOI">10.1016/j.neucom.2018.07.028</idno>
					<note type="submission">Received date: 23 January 2018 Revised date: 30 June 2018 Accepted date: 10 July 2018 Preprint submitted to Neurocomputing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Facial expression recognition</term>
					<term>LSTM</term>
					<term>3DCNN</term>
					<term>Multi-level features</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a novel end-to-end architecture termed Spatio-Temporal Convolutional features with Nested LSTM (STC-NLSTM), which learns the muti-level appearance features and temporal dynamics of facial expressions in a joint fashion. More precisely, 3DCNN is used to extract spatio-temporal convolutional features from the image sequences that represent facial expressions, and the dynamics of expressions are modeled by Nested LSTM, which is actually coupled by two sub-LSTMs, saying T-LSTM and C-LSTM. Namely, T-LSTM is used to model the temporal dynamics of the spatio-temporal features in each convolutional layer, and C-LSTM is adopted to integrate the outputs of all T-LSTMs together so as to encode the multi-level features encoded in the intermediate layers of the network.</p><p>We conduct experiments on four benchmark databases, CK+, Oulu-CASIA, MMI and BP4D, and the results show that the proposed method achieves a performance superior to the state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Facial Expression Recognition (FER) <ref type="bibr" target="#b0">[1]</ref>, in general, is to automatically group various kinds of facial muscle motions into similar emotion categories purely based on the visual information in images or videos. Due to its potentials in a broad range of applications such as face recognition <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>, face alignment <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref> and human-computer interface <ref type="bibr" target="#b14">[15]</ref>, FER has received extensive attentions in the literatures, e.g., <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>Essentially, facial expression is a dynamic process consisting of multiple stages, mainly including neutral, onset, apex and offset <ref type="bibr" target="#b18">[19]</ref>, so how to learn the dynamics of facial expressions is a key issue in FER <ref type="bibr" target="#b19">[20]</ref>.</p><p>Early FER methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> are often built upon some pre-defined features such as the Gabor filters, haar-like features and Local Binary Patterns (LBP). These methods may work well only on limited occasions, as the pre-defined features are incapable of fitting well with the data from a wide range of applications. To overcome this issue, it would be natural to consider the deep learning methods such as Convolutional Neural Network (CNN) <ref type="bibr" target="#b22">[23]</ref>, which can seamlessly integrate feature extraction and expression classification into a unified procedure. Extensive experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>demonstrate that CNN achieves substantial improvement in recognition accuracy over conventional methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b25">26]</ref>, but most of CNN-based methods consider a video as a collection of multiple static images, and thus they may not handle well the dynamic nature of facial expressions.</p><p>In order to make better use of the features that capture the motion of facial muscles, sequence-based methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>, which represent an expression by a sequence of images with known time stamp, have emerged as a preferable choice. To analyze sequential data, the deep learning community has also established several tools, e.g., Recurrent Neural Network (RNN) <ref type="bibr" target="#b29">[30]</ref>, Long Short-Term Memory (LSTM) <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref> and 3D Convolutional Neural Network (3DCNN) <ref type="bibr" target="#b34">[35]</ref>. Especially, the CNN-RNN (or CNN-LSTM) framework attracts much attention <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28]</ref>, in which RNN (or LSTM) takes the appearance features extracted by CNN over individual frames as inputs and encodes the temporal dynamics for later use, because it can combine the advantages of CNN and RNN to model both the appearance features and temporal dynamics simultaneously. Recently, researchers have investigated the framework of 3DCNN-RNN (or 3DCNN-LSTM) <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>. Unlike CNN, which only deals with 2D inputs, 3DCNN takes image sequences as inputs and can therefore extract directly the spatio-temporal features underlying the image sequences. Despite of the considerable improvement attained with the help of deep learning in recent few years, existing methods often use only the outputs of the last fully-connected layer as features for classification, discarding much useful information encoded in the intermediate layers of the network. As can be seen from Figure <ref type="figure" target="#fig_0">1</ref>, early convolutional layers extract fine-grained details (e.g., local boundaries or illuminations) of faces, while later layers capture more detailed information, e.g., the appearance patterns of mouths and eyes. It can be seen that the features from all layers of 3DCNN indeed provide FER with a hierarchical representation of multi-level features from fine to coarse. Such a hierarchical representation, intuitively, would be more effective than the features contained in the last layer only.</p><p>In this work, we propose an end-to-end FER method that can involve various visual clues, including the multi-level appearance features and the temporal dynamics of facial expressions. To this end, we propose a novel architecture termed Spatio-Temporal Convolutional features with Nested LSTM (STC-NLSTM), which is illustrated in Figure <ref type="figure" target="#fig_1">2</ref>.</p><p>In general, our STC-NLSTM contains three major components: 1) a 3DCNN module consisting of multiple convolutional layers, 2) multiple temporal-LSTM (T-LSTM) modules each of which corresponds to one layer of the 3DCNN, and 3) a convolutional-LSTM (C-LSTM) module that takes the outputs of T-LSTMs as inputs <ref type="foot" target="#foot_0">1</ref> . Given a sequence of images that represent an emotion class, first, the 3DCNN module extracts the spatio-temporal convolutional features of the expression for later use. Second, T-LSMT takes the spatio-temporal features as inputs and produces compact features that encode the appearance features as well as the temporal dynamics. Third, C-LSTM plays the role of integrating the outputs of all T-LSTMs together and encoding the multi-level features contained in each convolutional layer. Finally, the softmax classifier is used to categorize the given sequence into one of the six basic emotion  classes. In contrast to the existing sequence-based methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b27">28]</ref>, our STC-NLSTM can utilize not only the appearance features as well as the temporal dynamics of facial expressions, but also the multi-level semantics encoded in the individual layers of the network, so as to attain more reliable classification results. Experiments on CK+ <ref type="bibr" target="#b37">[38]</ref>,</p><formula xml:id="formula_0">A C C E P T E D M A N U S C R I P T</formula><p>Oulu-CASIA <ref type="bibr" target="#b38">[39]</ref>, MMI <ref type="bibr" target="#b18">[19]</ref> and BP4D <ref type="bibr" target="#b39">[40]</ref> show that the proposed STC-NLSTM is superior to the state-of-the-art</p><formula xml:id="formula_1">A C C E P T E D M A N U S C R I P T methods.</formula><p>The rest of this paper is organized as follows. Section 2 provides a brief survey for FER. Section 3 introduces the proposed STC-NLSTM method. Section 4 shows some empirical results and Section 5 concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Deep learning methods have exhibited superior performance for FER, showing dramatic improvement in accuracy and robustness over the conventional methods based on pre-defined features <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>According to how an expression is represented, existing methods can be roughly divided into two categories: imagebased and sequence-based methods.</p><p>In general, FER is a special pattern recognition problem, and thus the techniques for generic classification can be naturally applied to FER. Yu et al. <ref type="bibr" target="#b43">[44]</ref> proposed a FER method that combines together an ensemble of multiple CNNs by minimizing a mixture of the log likelihood loss and the hinge loss. Kim et al. <ref type="bibr" target="#b44">[45]</ref> devised a recognition framework by combining multiple CNNs to form a hierarchical network, and they won the first place of EmotiW 2015, an international competition of FER. Bargal et al. <ref type="bibr" target="#b40">[41]</ref> established a hybrid network that combines VGG16 <ref type="bibr" target="#b45">[46]</ref> with Residual Network <ref type="bibr" target="#b46">[47]</ref> to learn the appearance features of expressions, and they used SVM to produce the final classification results. Yao et al. <ref type="bibr" target="#b47">[48]</ref> proposed a deeper and wider network consisting of three inception modules, and Zhao et al. <ref type="bibr" target="#b25">[26]</ref> built a novel peak-piloted feature transformation network to capture the intrinsic correlations between the peak and weak expressions.</p><p>Different from the image-based methods, the sequence-based methods attempted to well capture the temporal variations of the appearance features, which are better for FER. Liu et al. <ref type="bibr" target="#b48">[49]</ref> proposed a FER method termed 3DCNN-DAP (DAP standards for deformable action part), in which 3DCNN is used to extract the spatio-temporal features and the strong spatial structural constraints among the dynamic action parts as well. Jung et al. <ref type="bibr" target="#b15">[16]</ref> studied an integrated network with joint fine-tuning to infer the appearance features and temporal dynamics of facial expressions. Jaiswal et al <ref type="bibr" target="#b24">[25]</ref> utilized CNN in combination with Bi-directional LSTM (BiLSTM) for FER, achieving a performance better than the winner of FERA 2015 <ref type="bibr" target="#b39">[40]</ref>. Fan et al. <ref type="bibr" target="#b27">[28]</ref> established a novel hybrid network that combines 3DCNN and RNN in a late-fusion fashion and won the first place in EmotoW2016 <ref type="bibr" target="#b41">[42]</ref>. The proposed STC-NLSTM method belongs to the sequence-based methods. Comparing to the previous works, our STC-NLSTM provides a fine grained approach for modeling multi-level features encoded in the intermediate layers of the network so as to achieve more accurate FER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Spatio-Temporal Convolutional Features with Nested LSTM</head><p>This section details the proposed STC-NLSTM. As shown in Figure <ref type="figure" target="#fig_1">2</ref>, our STC-NLSTM has three main components: 1) a 3DCNN module is used to extracted the spatio-temporal convolutional features (ST-Convs) of facial expressions, 2) multiple T-LSTM modules are adopted to capture the temporal dynamics of facial muscle motions, and 3) a C-LSTM module aims to seize the multi-level features encoded in the individual layers of the 3DCNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Spatio-Temporal Convolutional Features</head><p>Since facial expression is essentially a dynamic process, we attempt to extract directly the spatio-temporal features of facial expressions by a more straightforward approach, that is, the well recognized 3DCNN, which has been widely used in the fields of activity recognition, lip reading recognition, gesture recognition, and so on <ref type="bibr" target="#b34">[35]</ref>. Different from the traditional CNN that can only deal with 2D inputs, 3DCNN takes directly the image sequences as inputs and can therefore capture literally the spatio-temporal features of image sequences.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">MSPP-norm</head><p>Because the spatio-temporal features extracted by different layers of 3DCNN have different dimensions, it is impossible to input them directly to the LSTM units, which generally require the inputs to have the same dimension.</p><p>To fill this gap, we design the Multi-dimensional Spatial Pyramid Pooling normalization (MSPP-norm) operation, which is inspired by the Spatial Pyramid Pooling network (SPP-net) proposed by He et al. <ref type="bibr" target="#b49">[50]</ref>. The purpose of the MSSP-norm is to normalize the spatio-temporal features of different sizes to the same dimension. Given a 3D feature map of size N ×a×a, we partition it into N ×n×n (with n = 2, 4, 8) sub-regions and summarize the responses within each sub-region via max pooling, resulting in a feature vector with a fixed dimension determined by the parameter n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">T-LSTM and C-LSTM</head><p>After the process of MSPP-norm, the spatio-temporal features in each layer of 3DCNN are transferred to feature vectors of the same dimension. Thus, it is suitable to further analyze the spatio-temporal features by LSTM, which is an advanced RNN architecture for sequential data analysis including in FER <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b26">27]</ref>. The commonly used LSTM can model the temporal information by transforming a sequence of inputs to a sequence of outputs; this, in general, can partially capture the correlations among the spatio-temporal features extracted by 3DCNN. However, few conventional methods based on LSTM make full use of the information encoded in all the convolutional layers, because it is hard to involve all of the appearance features, temporal dynamics and multi-level features by simply combining 3DCNN with LSTM.</p><formula xml:id="formula_2">A C C E P T E D M A N U S C R I P T</formula><p>To deal with the above issues, we adopt two LSTM modules, saying T-LSTM and C-LSTM to cope with the spatio-temporal features extracted by 3DCNN. For each feature vector corresponding to a certain convolution layer, a T-LSTM is constructed by stacked LSTM units, which models the temporal dynamics of facial expressions. After that, a C-LSTM is constructed to take the outputs of T-LSTMs as inputs, so the desired multi-level features can be modeled in a seamless way.</p><p>Suppose that there are in total l convolutional layers in 3DCNN. Then the procedure of our STC-NLSTM method can be summarized as follows:</p><formula xml:id="formula_3">f j = 3DCNN(x), j = 1, • • • , l, f mspp j = MSPP-norm(f j ), j = 1, • • • , l, h j = T-LSTM j (f mspp j ), j = 1, • • • , l, h = {h 1 , • • • , h l }, o = C-LSTM(h),</formula><p>where x denotes an image sequence, f j is the 3D feature map produced by the jth convolutional layer of 3DCNN, h j is the feature vector from the jth T-LSTM module, and o denotes the final feature vector used for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Data</head><p>To verify the effectiveness of the proposed STC-NLSTM, we experiment with four benchmark datasets, CK+ <ref type="bibr" target="#b37">[38]</ref>,</p><p>Oulu-CASIA <ref type="bibr" target="#b38">[39]</ref>, MMI <ref type="bibr" target="#b18">[19]</ref> and BP4D <ref type="bibr" target="#b39">[40]</ref>. CK+: This dataset has six basic emotion classes, including anger(An), disgust(Di), fear(Fe), happiness(Ha), sadness(Sa) and surprise(Su). In addition, there is another special expression called "contempt". The dataset contains in total 593 image sequences from 123 subjects, but only 309 sequences are annotated with the six basic expression labels. We divide these 309 sequences into 10 groups, of which 9 groups are used for training and the rest for testing.</p><p>In this way, we can run various FER methods multiple times and obtain an averaged accuracy for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Oulu-CASIA:</head><p>We also consider for experiments the Oulu-CASIA dataset, which is a little bit more challenging than CK+. The dataset contains 480 image sequences of six basic emotion classes (including An, Di, Fe, Ha, Sa and Su) under normal illumination conditions. Each sequence begins with a neutral expression and ends with the peak expression. The same as in CK+, a 10-fold cross validation is performed to evaluate various FER methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MMI:</head><p>The third dataset used for experiments, MMI, is consist of 205 image sequences of the six basic emotion classes. Unlike CK+ and Oulu-CASIA, in which each sequence ends at the peak expression, the peak expressions in MMI are located in the middle of the sequences. The location of the peak frame is not provided as a prior information, which is usually the case for real-world videos. To obtain unbiased evaluation results, we perform a 10-fold cross validation in the same way as in CK+ and Oulu-CASIA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BP4D:</head><p>The last dataset used for experiments, BP4D, is divided into a fixed set of training, development and test data.</p><p>In total, the training partition contains 75,586 images, the development contains 71,261 images and the test contains 75,726 images. Each of these images in BP4D are annotated with 11 Action Units. Unlike above datasets, BP4D contains large number of annotated images which benefits deep learning algorithms and provides a good platform for a fair evaluation due to a fixed training and test set. Figure <ref type="figure" target="#fig_6">5</ref> shows some examples sampled from the above four datasets, and Table <ref type="table" target="#tab_0">1</ref> summarizes the number of sequences in each of the six emotion classes. To better perform FER, we need to use several pre-processing techniques, mainly including video normalization, face detection and data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>Video Normalization: Since the length of the image sequences is variable, but the dimension of the inputs for a neural network is usually fixed, the normalization along the time axis is required as input for neural networks. For the sequences in CK+ and Oulu-CASIA , which have respectively averaged lengths of 18 and 22, we make each sequence into the average length via either uniform sampling (for the sequences longer than the average) or replicating the last frame (for the sequences shorter than the average). Regarding the MMI dataset, which is based on video, we convert the videos into the image sequences by uniformly selecting 10 frames per second, and normalize the sequences into a fixed length of 22 in the same way as in CK+ and Oulu-CASIA. Face Detection: We utilize the Multi-Task Cascaded Convolutional Network (MTCCN) <ref type="bibr" target="#b51">[52]</ref> to obtain the coordinates of two eyes at first, then determine the final rectangular face by keeping the distances between two eyes invariable.</p><p>Finally, we turn a rectangle into a square through zero padding and resize the square to 64 × 64 (see Figure <ref type="figure" target="#fig_7">6</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Data</head><p>To evaluate the performance of the proposed STC-NLSTM, we compare it with 12 prevalent FER methods, including 3DCNN-DAP <ref type="bibr" target="#b48">[49]</ref>, 3DSIFT <ref type="bibr" target="#b52">[53]</ref>, ARDfee <ref type="bibr" target="#b53">[54]</ref>, CSPL <ref type="bibr" target="#b19">[20]</ref>, DTAGN <ref type="bibr" target="#b15">[16]</ref>, FN2EN <ref type="bibr" target="#b54">[55]</ref>, IDT+FV <ref type="bibr" target="#b55">[56]</ref>,</p><p>LOmo <ref type="bibr" target="#b42">[43]</ref>, PPDN <ref type="bibr" target="#b25">[26]</ref>, DCPN <ref type="bibr" target="#b56">[57]</ref>, STM-ExpLet <ref type="bibr" target="#b16">[17]</ref> and ST-RBM <ref type="bibr" target="#b57">[58]</ref>.</p><p>In addition, to further investigate the effectiveness of the proposed Nested LSTM, we design four baselines by amending the architecture of STC-NLSTM:</p><p>-STC (i.e., 3DCNN): This baseline is created by simply removing the T-LSTM and C-LSTM modules from the architecture of STC-NLSTM, and the outputs of the last convolutional layer of 3DCNN are taken as inputs to the softmax classifier so as to obtain the final classification results.</p><p>-STC-LSTM: This baseline is constructed by replacing the T-LSTM and C-LSTM modules with a traditional LSTM. Namely, the outputs of the last convolutional layer are taken as inputs to LSTM, which produces the final feature vectors for classification.</p><p>-STC-SLSTM: This baseline is similar to STC-LSTM. The only difference is that the outputs of all convolutional layer are taken as inputs to LSTM by sum fusion, which computes the sum of two feature maps at the same spatial locations and channels.</p><p>-DenseNet: This baseline <ref type="bibr" target="#b58">[59]</ref> is a simpler and more efficient network compared to Inception networks, which also utilizes the middle latent representation. We compared the DenseNet to the other FER methods under the standard setting <ref type="bibr" target="#b25">[26]</ref>, which uses the strong expressions in each sequence(e.g. the last one to three frames) for training and testing. Because the DenseNet method is only based on the static image, we train this baseline following to <ref type="bibr" target="#b56">[57]</ref>.</p><p>For the 12 previously proposed baselines, their results are directly quoted from the original reports. For STC, STC-LSTM and STC-SLSTM, we obtain their classification results using the same parametric configuration as STC-NLSTM.</p><p>As usual, we denote a deep network by a sequence of letters and numbers, e.  type patch size/stride input size</p><formula xml:id="formula_4">conv1 3 × 3 × 3/1 × 1 × 1 18 × 64 × 64 × 3 mspp1 [8,4,2] 18 × 5376 pool1 1 × 2 × 2/1 × 2 × 2 18 × 32 × 32 × 64 conv2 3 × 3 × 3/1 × 1 × 1 18 × 32 × 32 × 64 mspp2 [8,4,2] 18 × 5376 pool2 1 × 2 × 2/1 × 2 × 2 18 × 16 × 16 × 64 conv3 3 × 3 × 3/1 × 1 × 1 18 × 16 × 16 × 64 mspp3 [8,4,2] 18 × 5376 conv4 3 × 3 × 3/1 × 1 × 1 18 × 16 × 16 × 64 mspp4 [8,4,2] 18 × 5376 pool4 1 × 2 × 2/1 × 2 × 2 18 × 8 × 8 × 64 conv5 3 × 3 × 3/1 × 1 × 1 18 × 8 × 8 × 64 mspp5 [<label>8,4,2] 18 × 5376</label></formula><p>Our model is implemented based on the TensorFlow library <ref type="bibr" target="#b59">[60]</ref> and trained on four GeForce Titan X (pascal)</p><p>GPU with 12GB memory. The weights of the network are initialized randomly using the "xaiver" procedure <ref type="bibr" target="#b60">[61]</ref>.</p><p>We first set the learning rate as 0.0025 and train the network until 300 iterations, and then fine-tune the network by setting the learning rate to 0.000025 and running 200 iterations. In all the experiments, the weight decay parameter is consistently set as 0.0015.  For fair comparison, we follow <ref type="bibr" target="#b19">[20]</ref> to use 10-fold cross-validation and repeat the procedure 4 times, resulting in 40 trials in total. Table <ref type="table" target="#tab_3">3</ref> shows the comparison results of various FER methods. Since the expressions in this dataset   are easy to classify, several methods obtain superior classification results. In particular, as we can see from Table <ref type="table" target="#tab_4">4</ref>, our STC-NLSTM can achieve an accuracy near 100%. It performs well in anger and surprise, but for sadness , it is easy to be confused with disgust and fear. Figure <ref type="figure" target="#fig_10">7</ref> compares STC-NLSTM with STC, STC-LSTM and STC-SLSTM on each of the six basic emotion classes. It can be seen that STC-NLSTM performs consistently better than STC, STC-LSTM and STC-SLSTM, which confirms the effectiveness of our Nested LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Results on CK+</head><formula xml:id="formula_5">A C C E P T E D M A N U S C R I P T</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Results on Oulu-CASIA</head><p>Table <ref type="table" target="#tab_5">5</ref> shows the comparison results, and Table <ref type="table" target="#tab_6">6</ref> gives the confusion matrix produced by our STC-NLSTM method on the Oulu-CASIA dataset. This dataset is more difficult to classify than CK+. In the cases of disgust, fear, happiness and surprise, the performance is good, but the performance for anger and sadness is slightly poor. As   we can see, STC-NLSTM achieves an averaged accuracy of 93.45%, which is 4.2% higher than the 89.6% accuracy produced by the most close baseline, ARDfee. This illustrates the superiorities of STC-NLSTM over the state-of-theart FER methods. Figure <ref type="figure" target="#fig_11">8</ref> shows that STC-NLSTM is distinctly better than STC, STC-LSTM and STC-SLSTM on all expression classes except the class "happy". It is shown that that FER can benefit a lot from the modeling of the multi-level features encoded in each convolutional layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3.">Results on MMI</head><p>As shown in Table <ref type="table" target="#tab_7">7</ref>, the STC-NLSTM method can distinctly outperform previous state-of-the-art methods on the MMI dataset. Table <ref type="table" target="#tab_8">8</ref> shows the confusion matrix produced by STC-NLSTM. Actually, the averaged accuracy by STC-NLSTM reaches 84.53% (see Table <ref type="table" target="#tab_7">7</ref>), which is 2.9% better than ST-RBM, and which archives the best performance among the previous reports. Figure <ref type="figure" target="#fig_12">9</ref> shows that STC-NLSTM is distinctly better than STC-SLSTM on  all the emotion classes, which is same as the results on the other three datasets.</p><formula xml:id="formula_6">A C C E P T E D M A N U S C R I P T</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4.">Results on BP4D</head><p>Since the BP4D dataset has the training set and testing set, we do not need to use 10-fold cross-validation. Different from the above three datasets, the BP4D dataset is bigger than them. Figure <ref type="figure" target="#fig_12">9</ref> shows the experimental results. It can be seen that the STC-NLSTM obviously outperforms previous the state-of-the-art methods. This result supports the conclusion that our STC-NLSTM can also achieve the good performance on a large scale datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.5.">Influences of the Number of Layers</head><p>The above results illustrate that Nested LSTM plays a crucial role in our proposed method. To be more clear, we shall investigate the influences of the number of the convolutional layers contained in the architecture of STC-NLSTM.</p><p>Figure <ref type="figure" target="#fig_13">10</ref> shows the results. It can be seen that the classification accuracy gradually increases as the enlargement of the layer number, reaching the maximum at 5 convolutional layers. Since the datasets are not large, the performance drops while the number of layers exceeds 5. Regarding why the CK+ dataset is not so sensitive to the number of layers, the reason is that the dataset is easy to classify (see Table <ref type="table" target="#tab_3">3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed a novel method termed STC-NLSTM for FER. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Visualization of the convolutional features extracted from different layers of 3DCNN. The blue and red points correspond to the low and high response values, respectively. The emotion label for the input image sequence is surprise.</figDesc><graphic coords="4,145.64,363.72,279.80,229.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Architecture of the proposed STC-NLSTM, which consists of 3DCNN and Nested LSTM, and which is coupled by temporal-LSTM (T-LSTM) and convolutional-LSTM (C-LSTM). In the figure above, the term "ST-Convs" standards for the spatio-temporal convolutional features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Architecture of the adopted 3DCNN.</figDesc><graphic coords="6,145.64,218.28,279.80,84.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3</head><label>3</label><figDesc>Figure 3 illustrated the architecture of the 3DCNN. Given a sequence of images with known time stamp that represent a facial emotion class, 3DCNN processes the sequence by multiple convolutional and pooling layers, producing a collection of spatio-temporal features that characterize the expression.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>3. 2 .</head><label>2</label><figDesc>Nested LSTM To capture the multi-level features encoded in the intermediate layers of the network, we propose the so-called Nested LSTM shown in Figure 4, which is composed of MSPP-norm, T-LSTM and C-LSTM. MSSP-norm aims to normalize the spatio-temporal features of different sizes to the same dimension, while T-LSTM and C-LSTM can capture the temporal dynamics and seize the multi-level features encoded in the individual convolutional layers of the network respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Architecture of the proposed Nested LSTM.</figDesc><graphic coords="7,145.64,105.13,279.80,196.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Examples of the images used in our experiments. Top: CK+; Middle: Oulu-CASIA; Bottom: MMI.</figDesc><graphic coords="8,192.28,187.17,186.53,105.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Some examples of the face detection results in CK+ (top), Oulu-CASIA (middle) and MMI (bottom).</figDesc><graphic coords="9,215.59,418.35,139.91,105.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Data Augmentation:</head><label></label><figDesc>Facial expression datasets, e.g. CK+, Oulu-CASIA and MMI, often contain only hundreds of image sequences. However, a typical deep neural network has many parameters, and this will make a deep network prone to overfitting. To handle this issue, we first flip each image sequence horizontally so as to double the number of sequences. Then we rotate each image by an angle in {-7.5 • , -5 • , -2.5 • , 2.5 • , 5 • , 7.5 • }, resulting in a new dataset which is 14 times as big as the original one. Such a data augmentation process can not only make the learnt model robust against the slight rotational changes of the input images, but also broaden the number of training samples so as A C C E P T E D M A N U S C R I P T to avoid overfitting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>3 × 3 ,</head><label>33</label><figDesc>g., I(64,64,22)-C(3,64)-BN-P2-FC18-S6, where I(64,64,22) means the 64 × 64 × 22 input image sequences, C(3,64) is a convolutional layer with 64 filters of BN standards for the operation of batch normalization, P2 is a 2 × 2 max pooling layer, FC refers to a fully connected layer, and S6 denotes a softmax layer with six outputs. For simplicity, the architecture of the 3DCNN used in our STC-NLSTM is configured as I(64,64,18)-C(3,64)-BN-P2-C(3,64)-BN-P2-C(3,64)-C(3,64)-P2-C(3,64)-FC18, and a two-level LSTM architecture is used to construct the T-LSTM and C-LSTM modules. The stride of each layer is 1 with the exception of the pooling layer, which has a stride value of 2. Table 2 details the configurations of the network architecture.A C C E P T E D M A N U S C R I P T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Comparing STC-NLSTM with STC, STC-LSTM and STC-SLSTM on each of the six emotion classes in CK+.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Comparing STC-NLSTM with STC, STC-LSTM and STC-SLSTM on each of the six emotion classes in Oulu-CASIA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Comparing STC-NLSTM with STC, STC-LSTM and STC-SLSTM on each of the six emotion classes in MMI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Plotting the classification accuracy as a function of the number of convolutional layers in STC-NLSTM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>Unlike most of the existing deep learning based methods, which obtain the classification results based on the outputs of the last fully-connected layer, STC-NLSTM aims to taken into account the multi-level features encoded in the intermediate layers of the network. To achieve this, the architecture of STC-NLSTM is designed to involve three major components: 3DCNN, T-LSTMs and C-LSTM. Each component is devised carefully to own a specific ability. The 3DCNN module plays the role of extracting the spatio-temporal convolutional features of facial expressions. The T-LSTM modules take charge of capturing the temporal dynamics that depict the facial appearance variations in temporal domain, and the C-LSTM is responsible for seizing the multi-level features encoded in the individual convolutional layers of the network. All the three components are integrated into an end-to-end network so as to cooperate seamlessly with each other. Experiments on four public datasets demonstrated that STC-NLSTM is superior to the state-of-the-art methods. A C C E P T E D M A N U S C R I P T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The number of image sequences in each of the six basic emotion classes: anger(An),disgust(Di), fear(Fe), happiness(Ha), sadness(Sa) and surprise(Su).</figDesc><table><row><cell></cell><cell>An Di Fe Ha Sa Su All</cell></row><row><cell>CK+</cell><cell>45 59 25 69 28 83 309</cell></row><row><cell>Oulu</cell><cell>80 80 80 80 80 80 480</cell></row><row><cell>MMI</cell><cell>32 31 28 42 32 40 205</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Detailed configurations of the network for CK+.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Classification accuracies on CK+. The numbers for STC, STC-LSTM, STC-SLSTM and STC-NLSTM are averaged from 40 trails.</figDesc><table><row><cell>Method</cell><cell>Average Accuracy</cell></row><row><cell>3DCNN-DAP [49]</cell><cell>92.4</cell></row><row><cell>STM-ExpLet [17]</cell><cell>94.2</cell></row><row><cell>LOmo [43]</cell><cell>95.1</cell></row><row><cell>IDT+FV [56]</cell><cell>95.8</cell></row><row><cell>FN2EN [55]</cell><cell>96.9</cell></row><row><cell>DTAGN [16]</cell><cell>97.3</cell></row><row><cell>ARDfee [54]</cell><cell>98.7</cell></row><row><cell>PPDN [26]</cell><cell>99.3</cell></row><row><cell>DCPN [57]</cell><cell>99.6</cell></row><row><cell>STC</cell><cell>98.9</cell></row><row><cell>STC-LSTM</cell><cell>99.3</cell></row><row><cell>STC-SLSTM</cell><cell>99.4</cell></row><row><cell>DenseNet</cell><cell>97.6</cell></row><row><cell>STC-NLSTM</cell><cell>99.8(±0.2)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Confusion matrix of STC-NLSTM for CK+. The labels in the leftmost and topmost columns denote the ground truth and prediction results, respectively.</figDesc><table><row><cell></cell><cell>An</cell><cell>Di</cell><cell>Fe</cell><cell>Ha</cell><cell>Sa</cell><cell>Su</cell></row><row><cell>An</cell><cell>100</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>Di</cell><cell cols="2">0.15 99.68</cell><cell>0</cell><cell>0</cell><cell>0.17</cell><cell>0</cell></row><row><cell>Fe</cell><cell>0</cell><cell>0</cell><cell>99.71</cell><cell>0</cell><cell>0.29</cell><cell>0</cell></row><row><cell>Ha</cell><cell>0</cell><cell>0</cell><cell cols="2">0.11 99.89</cell><cell>0</cell><cell>0</cell></row><row><cell>Sa</cell><cell>0</cell><cell>0.29</cell><cell>0.57</cell><cell>0</cell><cell>99.14</cell><cell>0</cell></row><row><cell>Su</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>100</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Classification accuracies on Oulu-CASIA. The numbers for STC, STC-LSTM, STC-SLSTM and STC-NLSTM are obtained by averaging the accuracies from 40 trails.</figDesc><table><row><cell>Method</cell><cell>Average Accuracy</cell></row><row><cell>STM-ExpLet [17]</cell><cell>74.59</cell></row><row><cell>DTAGN [16]</cell><cell>81.64</cell></row><row><cell>LOmo [43]</cell><cell>82.10</cell></row><row><cell>PPDN [26]</cell><cell>84.59</cell></row><row><cell>DCPN [57]</cell><cell>86.23</cell></row><row><cell>FN2EN [55]</cell><cell>87.71</cell></row><row><cell>ARDfee [54]</cell><cell>89.60</cell></row><row><cell>STC</cell><cell>84.72</cell></row><row><cell>STC-LSTM</cell><cell>88.98</cell></row><row><cell>STC-SLSTM</cell><cell>90.12</cell></row><row><cell>DenseNet</cell><cell>87.28</cell></row><row><cell>STC-NLSTM</cell><cell>93.45(±0.43)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Confusion matrix of STC-NLSTM for Oulu-CASIA. The labels in the leftmost and topmost columns denote the ground truth and prediction results, respectively.</figDesc><table><row><cell></cell><cell>An</cell><cell>Di</cell><cell>Fe</cell><cell>Ha</cell><cell>Sa</cell><cell>Su</cell></row><row><cell cols="3">An 89.82 6.20</cell><cell>0.75</cell><cell>0</cell><cell>3.23</cell><cell>0</cell></row><row><cell>Di</cell><cell cols="3">1.38 95.20 0.30</cell><cell>0.95</cell><cell>2.17</cell><cell>0</cell></row><row><cell>Fe</cell><cell>0</cell><cell>0</cell><cell cols="2">96.14 0.50</cell><cell>0.65</cell><cell>2.71</cell></row><row><cell>Ha</cell><cell>0</cell><cell>0.90</cell><cell cols="3">3.83 94.78 0.49</cell><cell>0</cell></row><row><cell>Sa</cell><cell>4.4</cell><cell>2.38</cell><cell>0.56</cell><cell>0</cell><cell>92.66</cell><cell>0</cell></row><row><cell>Su</cell><cell>0</cell><cell>0</cell><cell>3.95</cell><cell>0</cell><cell>0</cell><cell>96.05</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Classification accuracies on MMI. The numbers for STC, STC-LSTM, STC-SLSTM and STC-NLSTM are averaged from 40 trails.</figDesc><table><row><cell>Method</cell><cell>Average Accuracy</cell></row><row><cell>3DCNN-DAP [49]</cell><cell>63.4</cell></row><row><cell>3DSIFT [53]</cell><cell>64.39</cell></row><row><cell>DTAGN [16]</cell><cell>70.24</cell></row><row><cell>CSPL [20]</cell><cell>73.53</cell></row><row><cell>STM-ExpLet [17]</cell><cell>75.12</cell></row><row><cell>ST-RBM [58]</cell><cell>81.63</cell></row><row><cell>STC</cell><cell>74.84</cell></row><row><cell>STC-LSTM</cell><cell>80.39</cell></row><row><cell>STC-SLSTM</cell><cell>81.92</cell></row><row><cell>DenseNet</cell><cell>77.68</cell></row><row><cell>STC-NLSTM</cell><cell>84.53(±0.67)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Confusion matrix of STC-NLSTM for MMI. The labels in the leftmost and topmost columns denote the ground truth and prediction results, respectively.</figDesc><table><row><cell></cell><cell></cell><cell>An</cell><cell>Di</cell><cell>Fe</cell><cell>Ha</cell><cell></cell><cell>Sa</cell><cell>Su</cell></row><row><cell></cell><cell cols="3">An 83.24 9.04</cell><cell>0</cell><cell cols="2">5.36</cell><cell>1.24</cell><cell>1.12</cell></row><row><cell></cell><cell>Di</cell><cell cols="2">6.72 88.21</cell><cell>0</cell><cell cols="2">2.74</cell><cell>2.33</cell><cell>0</cell></row><row><cell></cell><cell>Fe</cell><cell>4.34</cell><cell>0</cell><cell cols="3">81.24 1.23</cell><cell>1.56 11.63</cell></row><row><cell></cell><cell>Ha</cell><cell>3.62</cell><cell>0</cell><cell cols="3">3.16 93.22</cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell>Sa</cell><cell>1.55</cell><cell>1.12</cell><cell>9.18</cell><cell cols="3">1.18 85.77 1.20</cell></row><row><cell></cell><cell>Su</cell><cell>2.64</cell><cell>0</cell><cell>8.66</cell><cell cols="2">3.41</cell><cell>0</cell><cell>85.29</cell></row><row><cell></cell><cell>100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy</cell><cell>70 80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>STC</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>STC-LSTM</cell></row><row><cell></cell><cell>60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>STC-SLSTM</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>STC-NLSTM</cell></row><row><cell></cell><cell>50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>An</cell><cell>Di</cell><cell>Fe</cell><cell>Ha</cell><cell>Sa</cell><cell>Su</cell><cell>Total</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Emotion</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Performance (F1 scores) comparison on BP4D Test set.</figDesc><table><row><cell>Method</cell><cell>F1 Scores</cell></row><row><cell>LGBP [40]</cell><cell>0.44</cell></row><row><cell>GDNN [25]</cell><cell>0.48</cell></row><row><cell>DLE [62]</cell><cell>0.51</cell></row><row><cell>CNN+BLSTM [25]</cell><cell>0.52</cell></row><row><cell>STC-NLSTM</cell><cell>0.58</cell></row><row><cell cols="2">number of convolutional layers</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In addition to the three major components shown in Figure</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>2, STC-NLSTM actually contains another component called Multi-dimensional Spatial Pyramid Pooling normalization (MSPP-norm), which is in charge of normalizing the convolutional features of different dimensions into the same size. We shall clarify this detail in Section 3.2.1.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head><p>The work of Qingshan Liu is supported by National Natural Science Foundation of China (NSFC) under Grant61532009.</p><p>The work of Guangcan Liu is supported in part by NSFC under Grant 61622305 and Grant 61502238, and in part by the Natu-ral Science Foundation of Jiangsu Province of China (NSFJPC) under Grant BK20160040.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Constants across cultures in the face and emotion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="124" to="129" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Expression-invariant face recognition with expression classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Canadian Conference on Computer and Robot Vision</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="77" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Marginal loss for deep face recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPRW</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07698</idno>
		<title level="m">Arcface: Additive angular margin loss for deep face recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.04695</idno>
		<title level="m">Uv-gan: Adversarial facial uv map completion for pose-invariant face recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">M3 csr: Multi-view, multi-scale and multi-component cascade shape regression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="19" to="26" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Facial shape tracking via spatio-temporal cascade shape regression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="41" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robust facial landmark tracking via cascade regression</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="53" to="62" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Low rank driven robust facial landmark regression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="page" from="196" to="206" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dual sparse constrained cascade regression for robust face alignment</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="700" to="712" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adaptive cascade regression model for robust face alignment</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="797" to="807" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The menpo facial landmark localisation challenge: A step towards the solution</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chrysos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>CVPR Workshops</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cascade multi-view hourglass model for robust 3d face alignment</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zaferiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FG</title>
		<imprint>
			<biblScope unit="page" from="399" to="403" />
			<date type="published" when="2018">2018</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.06023</idno>
		<title level="m">Joint multi-view face alignment in the wild</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Facial expression recognition using neural network can overview</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Saudagare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Chaudhari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Soft Computing &amp; Engineering</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="238" to="241" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Joint fine-tuning in deep neural networks for facial expression recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2983" to="2991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning expressionlets on spatio-temporal manifold for dynamic facial expression recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1749" to="1756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Facial expression recognition via a boosted deep belief network</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1805" to="1812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Induced disgust, happiness and surprise: An addition to the mmi facial expression database</title>
		<author>
			<persName><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Workshop on Emotion Corpora for Research on Emotion &amp; Affect</title>
		<imprint>
			<biblScope unit="page" from="65" to="70" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning active facial patches for expression analysis</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2562" to="2569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Exploring bag of words architectures in the facial expression domain</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="250" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning multiscale active facial patches for expression analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2562" to="2569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Handwritten digit recognition with a backpropagation network</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="396" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Incremental boosting convolutional neural network for facial action unit recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deep learning the dynamic appearance and shape of facial action units</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jaiswal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Applications of Computer Vision</publisher>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Peak-piloted deep network for facial expression recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="425" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for emotion recognition in video</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Konda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimodal Interaction</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="467" to="474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Video-based emotion recognition using cnn-rnn and c3d hybrid networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimodal Interaction</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="445" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Exploring facial expressions with compositional features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2638" to="2644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A novel connectionist system for unconstrained handwriting recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fernàndez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bertolami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="855" to="868" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="C1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Visual classification by 1 -hypergraph modeling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2564" to="2574" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Beyond object proposals: Random crop pooling for multi-label image recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5678" to="5688" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hierarchical lstm for sign language translation</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L M W</forename><surname>Dan Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Learning spatiotemporal features using 3dcnn and convolutional lstm for gesture recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3120" to="3128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multimodal gesture recognition using 3-d convolution and convolutional lstm</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="4517" to="4524" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="94" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Facial expression recognition from near-infrared videos</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Taini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vision Comput</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="607" to="619" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fera 2015 -second facial expression recognition and analysis challenge</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Almaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference and Workshops on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Emotion recognition in the wild from videos using images</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Bargal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Barsoum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimodal Interaction</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="433" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Emotiw 2016: video and group-level emotion recognition challenges</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimodal Interaction</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="427" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Latent ordinal model for facial analysis in videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lomo</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5580" to="5589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Image based static facial expression recognition with multiple deep network learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimodal Interaction</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="435" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Hierarchical committee of deep cnns with exponentially-weighted decision fusion for static facial expression recognition</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Roh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimodal Interaction</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="427" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Holonet: towards robust emotion recognition in the wild</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimodal Interaction</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="472" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deeply learning deformable facial action parts model for dynamic expression analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="143" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Combining multiple kernel methods on riemannian manifold for emotion recognition in the wild</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimodal Interaction</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="494" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A 3-dimensional sift descriptor and its application to action recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Scovanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="357" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Automatic recognition of deceptive facial expressions of emotion</title>
		<author>
			<persName><forename type="first">I</forename><surname>Ofodile</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Corneanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Baro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hyniewska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Allik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Anbarjafari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Facenet2expnet: Regularizing a deep face recognition net for expression recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="118" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Facial expression recognition in the wild using improved dense trajectories and fisher vector encoding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Afshar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Salah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1517" to="1525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deeper cascaded peak-piloted network for weak expression recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Computer</title>
		<imprint>
			<biblScope unit="issue">6C8</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A spatio-temporal rbm-based model for facial expression recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Elaiwat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">C</biblScope>
			<biblScope unit="page" from="152" to="161" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<title level="m">Densely connected convolutional networks</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Discriminant multi-label manifold embedding for facial action unit detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Thiran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference and Workshops on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
