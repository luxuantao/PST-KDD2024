<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E5079696780B3CAD68213A2992B7EA74</idno>
					<idno type="DOI">10.1109/TII.2018.2791424</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>An Adaptive Dropout Deep Computation Model for</head><p>Industrial IoT Big Data Learning with Crowdsourcing to Cloud Computing Qingchen Zhang, Laurence T. Yang, Zhikui Chen, Peng Li, and Fanyu Bu</p><p>Abstract-Deep computation, as an advanced machine learning model, has achieved the sate-of-the-art performance for feature learning on big data in industrial Internet of Things. However, the current deep computation model usually suffers from overfitting due to the lack of public available labeled training samples, limiting its performance for big data feature learning. Motivated by the idea of active learning, an adaptive dropout deep computation model with crowdsourcing to cloud is proposed for industrial IoT big data feature learning in this paper. First, a distribution function is designed to set the dropout rate for each hidden layer to prevent over-fitting for the deep computation model. Furthermore, the outsourcing selection algorithm based on maximum entropy is employed to choose appropriate samples from the training set to crowdsource on the cloud platform. Finally, an improved SLME scheme is presented to aggregate answers given by human workers and to update the parameters of the adaptive dropout deep computation model simultaneously. Extensive experiments are conducted to evaluate the performance of the presented model by comparing with the dropout deep computation model and other state-of-the-art crowdsourcing algorithms. The results demonstrate that the proposed model can prevent over-fitting effectively and aggregate the labeled samples to train the parameters of the deep computation model with crowdsouring for industrial IoT big data feature learning.</p><p>Index Terms-Deep computation, Industrial Internet of Things, Big data, Cloud computing, Dropout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>R ECENTLY, Internet of Things have widely applied in industrial manufacturing, resulting in industrial Internet of Things <ref type="bibr" target="#b0">[1]</ref>. In a typical industrial IoT system, a large quantity of data about industrial manufacturing, usually called industrial IoT big data, is first collected by sensing terminals, and then it is transmitted to the cloud data center via wireless sensor networks or the Internet <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. Afterwards, the cloud data center can automatically control the process of industrial manufacture according to the collected industrial IoT big data. Moreover, if the cloud data center can analyze the collected data accurately before upcoming abnormal events, some actions can be taken in time to prevent the catastrophic destruction <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. Obviously, big data analytic and learning in the cloud center is playing an important role for industrial Q. Zhang and L. T. Yang are with the School of Electronic Engineering, University of Electronic Science and Technology of China, Chengdu, 611731, China, and the Department of Computer Science, St. Francis Xavier University, Antigonish, NS B2G 2W5, Canada. E-mail: ltyang@gmail.com Z. Chen, P. Li and F. Bu are with the School of Software Technology, Dalian University of Technology, Dalian, China.</p><p>Internet of Things to provide intelligent services such as smart city and intelligent transportation <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>.</p><p>As an important data learning technique, deep learning aims to automatically learn hierarchical features by stacking multiple traditional artificial neural networks <ref type="bibr" target="#b7">[8]</ref>. Representative deep learning models include stacked auto-encoders, deep belief networks and deep convolutional neural networks, which are built by auto-encoders, restricted Boltzmann machines and convolutional neural networks, respectively. In recent years, deep learning models have been widely used in image classification, video retrieval and cloud resource prediction. To learn features for big data, a deep computation model was developed by generalizing the stacking auto-encoders to the tensor space <ref type="bibr" target="#b8">[9]</ref>. Specially, the deep computation model has achieved better results than the conventional stacked auto-encoder model and multi-modal deep learning models for big data feature learning. Generally speaking, the deep computation model requires a large number of labeled samples to fine-tune the initial parameters for classification tasks. However, it is every expensive or tedious to acquire many labeled training samples in some cases. In fact, the lack of available labeled training samples is a major challenge for not only the deep computation model, but also other machine learning technologies such as deep learning and active learning. Furthermore, the deep computation model usually suffers from over-fitting due to the lack of public available labeled training samples, lowering the accuracy for big data classification tasks.</p><p>In this paper, an adaptive dropout deep computation model with crowdsourcing is presented for big data feature learning in industrial Internet of Things. Dropout was proposed to prevent over-fitting for deep learning models <ref type="bibr" target="#b9">[10]</ref>. Specially, dropout is especially effective to learn the large-scale deep neural networks with a small number of training samples. However, dropout ignores the relationship between the activating rate and the layer opposition since it always sets the same activating rate with 0.5 for each hidden layer, limiting its effectiveness in avoiding over-fitting for the deep computation model. To set an appropriate activating rate for each hidden layer, a distribution function with regard to the layer position is defined, and furthermore an adaptive dropout deep computation model based on the defined distribution function is presented to prevent over-fitting.</p><p>Motivated by the active learning framework, the crowdsourcing technique is used to aggregate the labeled samples for training the parameters of the adaptive dropout deep computation model in this paper. Crowdsourcing combines human intelligence and the computer power to solve the problem of the lack of available training samples, which is particularly potential for machine learning models without enough labeled training samples <ref type="bibr" target="#b10">[11]</ref>. Therefore, crowdsourcing, with cloud computing, is offering a potential opportunity to improve the performance of the deep computation model. To obtain the labeled training samples for the deep computation model, some unlabeled samples are outsourced to the cloud platform, and then the labels are acquired by aggregating answers given by human workers in the cloud platform. There are two challenging issues to be addressed for implementing the presented scheme. The first issue is how to select appropriate unlabeled samples to outsource, which is crucial to improve the performance of the answer aggregation. To tackle this challenge, the maximum entropy theory is used in this paper. Specially, the unlabeled samples with the least certainty are chosen as the outsourcing objects. The second issue is how to obtain the reliable labels from human workers by answer aggregation. In this paper, an answer aggregation method based on SLME is presented. SLME (supervised learning from multiple experts) was initially proposed by Raykar et al. <ref type="bibr" target="#b11">[12]</ref> for answer aggregation by introducing two well-known measures, i.e., sensitive and specificity, which is proved effective in many applications. However, SLME is initially designed for binary classification. Therefore, this strategy is extended for multi-labeling to train the parameters of the presented adaptive dropout deep computation model by introducing the confusion matrix in this paper. Finally, extensive experiments are conducted to compare the proposed model with the dropout deep computation model and other art-of-the-state crowdsourcing algorithms. Results demonstrate that the presented scheme can effectively obtain the labeled training samples and further prevent over-fitting, proving the potential of the presented model for big data feature learning in industrial Internet of Things.</p><p>Therefore, the contributions of this paper can be summarized as the following three aspects: To tackle this issue, this strategy is extended for multilabeling by introducing the confusion matrix. Furthermore, it is used to obtain the labels of outsourcing samples from human workers for fine-tuning the parameters of the adaptive dropout deep computation model. The paper is organized as follows. The related work is reviewed in Section II and the presented adaptive dropout deep computation model is introduced in Section III. The outsourcing sample selection algorithm and the aggregation answer method are described in Section IV and Section V, respectively. The experimental results are presented in Section VI and the paper is concluded in Section VII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>The related work, including the standard deep computation model and the existing answer aggregation algorithms, are reviewed in this section. The standard deep computation model is illustrated first, followed by the existing answer aggregation algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Deep Computation Model</head><p>The standard deep computation model is established by stacking tensor auto-encoders for big data feature learning <ref type="bibr" target="#b8">[9]</ref>. The tensor auto-encoder was proposed based on the tensor big data representation model where each sample is represented by a tensor. For example, an image in the RGB space can be represented by a 3-order tensor R I h ×Iw×Ic with I h × I w , I c denoting the resolution and the color channel, respectively. Specially, an image with the resolution of 1280 × 1024 in the RGB space can be represented by R 1280×1024×3 . Furthermore, a video chip in the MPEG-4 format can be represented by a 4-order tensor R I h ×Iw×Ic×In where I n denotes the number of the frames. Thus, a video chip with 150 frames, each of which is the same as the above image, in the MPEG-4 format can be represented by R 1280×1024×3×150 . Given the input sample X represented by a K-order tensor, the tensor auto-encoder projects it to the hidden layer H represented by a J-order tensor using a K + 1-order weight tensor W (1) ∈ R a×I1×I2×•••×I K with a sub-tensors and a K-order bias tensor b (1) :</p><formula xml:id="formula_0">H j1...jL = f ( I1•••IK ∑ i1•••iK W (1) αi1•••iK X i1•••iK + b (1) j1•••jL ),<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">α = j L + ∑ L-1 t=1 (j t -1) ∏ L s=t+1 J s , a =</formula><p>∏ L l=1 J L and f is the activation function. Specially, the Sigmoid function, f (x) = 1/(1+e -x ), is used as the activator since it is a differentiable activation function whose gradient can be obtained for training. Furthermore, Sigmoid can yield the output with an unnormalized probability, simplifying the classification tasks.</p><p>Afterwards, the tensor auto-encoder projects the hidden layer H to the output layer by a L + 1-order tensor W (2) ∈ R b×J1×J2×•••×JL with b sub-tensors and a L-order bias tensor b (2) :</p><formula xml:id="formula_2">Y i1...iK = f ( J1•••J L ∑ j1•••jL W (2) βj1•••jL H j1•••jL + b (1) i1...iK ),<label>(2)</label></formula><p>where</p><formula xml:id="formula_3">β = i K + ∑ K-1 t=1 (i t -1) ∏ K s=t+1 I s and b = ∏ K k=1 I k .</formula><p>For the training sample X, the tensor auto-encoder defines the reconstruction error function with regard to the parameter set θ = {W (1) , b (1) ; W (2) , b (2) } as:</p><formula xml:id="formula_4">J(θ; X) = (Y -X) T G(Y -X).</formula><p>(</p><formula xml:id="formula_5">)<label>3</label></formula><p>where G denotes the metric matrix <ref type="bibr" target="#b12">[13]</ref>.</p><p>To train the parameters of the tensor auto-encoder model, a high-order back-propagation algorithm was implemented in the tensor space. Furthermore, a deep computation model was established to learn hierarchical features for big data by stacking tensor auto-encoders.</p><p>To improve the training efficiency for the parameters, two improved deep computation models, i.e., a Tucker deep computation model and a CP deep computation model, were presented by using the tensor decomposition algorithms to compress the parameters significantly <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Answer Aggregation Techniques in Crowdsourcing</head><p>Answer aggregation is an important challenge in data-driven crowdsourcing. It aims to aggregate the answers given by human workers for the hidden ground truth <ref type="bibr" target="#b11">[12]</ref>. The human workers usually have different levels of background and expertise, resulting in the contradiction and uncertainty of the given answers. Moreover, some malicious workers will give incorrect answers sometimes. Therefore, answer aggregation is generally an extremely challenging issue. In the past few years, some algorithms have been introduced for the answer aggregation, which can be roughly categorized by two groups: non-iterative methods and iterative methods.</p><p>The most representative non-iterative method is majority voting which aggregates the answer of each sample independently <ref type="bibr" target="#b14">[15]</ref>. Given a sample that is annotated by many human workers, the majority voting algorithm (MV) counts the number of each label. The true label is recognized as one with the biggest number of votes. This algorithm is simple and straightforward, but it does not take into account the different background and expertise of workers. Honeypot (HP) <ref type="bibr" target="#b15">[16]</ref> and ELICE (expert label injected crowd estimation) <ref type="bibr" target="#b16">[17]</ref> are another two non-iterative algorithms for answer aggregation in data-driven crowdsourcing. They filter the untrustworthy workers by merging some trapping samples whose true labels are already known in a preprocessing step. They are more effective than majority voting, however, they still have several drawbacks in many applications. For example, the trapping samples are not always available and appropriate trapping samples are difficult to select in many cases.</p><p>The most typical algorithm of the iterative category is expectation maximization (EM) <ref type="bibr" target="#b17">[18]</ref>. EM iteratively calculates the probability of each sample towards every label by an expectation step and a maximization step. The expectation step estimates the sample probability depending on the current estimation of the workers expertise while the maximization step re-estimates the expertise level of each worker according to the current sample probability. SLME (supervised learning from multiple experts) is another algorithm of this type <ref type="bibr" target="#b11">[12]</ref>. SLME also utilizes the expectation maximization to estimate the sample probability and the expertise level of each worker. Other methods of this type include GLAD <ref type="bibr" target="#b18">[19]</ref>, ITER <ref type="bibr" target="#b19">[20]</ref> and M-X <ref type="bibr" target="#b20">[21]</ref>.</p><p>Table <ref type="table">I</ref> presents the summarization of the representative answer aggregation algorithms <ref type="bibr" target="#b21">[22]</ref>.</p><p>Although SLME achieves the best accuracy and works robustly against spammers, it is only suitable for binary classification. In this paper, we extend the SLME algorithm for multi-labeling applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. ADAPTIVE DROPOUT DEEP COMPUTATION MODEL</head><p>In this part, an adaptive distribution function is presented to set the activating rate with a probability ρ. Among the commonly used distribution functions, the normal distribution can reveal the law of nature effectively, and it is in accordance with the law of human cognition of things. However, the normal distribution is a monotonically increasing function, so it cannot be directly used to set the activating rate of each hidden layer with regard to the layer position. Therefore, a distribution model ρ with regard to the layer position l is defined by modifying the normal distribution function as follows.</p><formula xml:id="formula_6">ρ =    1 -1 σ √ 2π ∫ l -∞ exp(- (l-n 2 ) 2 2σ 2 )dl n = 2k 1 -1 σ √ 2π ∫ l -∞ exp(- (l-n+1 2 ) 2<label>2σ</label></formula><p>2</p><formula xml:id="formula_7">)dl n = 2k + 1 , (<label>4</label></formula><p>) where n represents the number of layers in the adaptive dropout deep computation model, l denotes the layer position and σ is used to control the decay velocity.</p><p>Obviously, the defined distribution function has the following three properties:</p><p>(1) Monotonically decreasing.</p><p>(2) The activating rate of the middle hidden layer equals 0.5.</p><p>(3) The activating rate is always in (0, 1) for l = 1, 2, . . . , n.</p><p>Since the latter part of the distribution function ρ is a normal distribution function with regard to l, the values of the activating rate in each hidden layer, which are set by the proposed distribution function ρ, will be subjected to the approximated normal distribution from top to bottom in the deep computation model. An adaptive dropout deep computation model can be constructed by applying the distribution function to the standard deep computation model. Furthermore, the parameters of the adaptive dropout deep computation model can be trained by combining the high-order back-propagation algorithm and the proposed distribution function, outlined in Algorithm 1.</p><p>Algorithm 1: Adaptive Dropout High-order Backpropagation Algorithm. 1) , b (1) ; W (2) , b (2) } 1 ρ = f (l); </p><formula xml:id="formula_8">Input: {(X (i) , Y (i) )} M i=1 , η, threshold Output: θ = { W (</formula><formula xml:id="formula_9">W = W -η × ( 1 M ∆W ); 26 b = b -η × ( 1 M ∆b);</formula><p>In Algorithm 1, the forward-pass is performed to compute the activating values of the hidden layer and the output layer combining with the presented adaptive distribution function on lines 3-9. Afterwards, the back-propagation is performed to compute the gradients of the reconstruction error function with regard to the parameters on lines 10-23. Finally, the gradient descent algorithm is performed to update the parameters on lines 25-26. Let I ∈ max{I 1 , I 2 , . . . , I N } and J ∈ max{J 1 , J 2 , . . . , J M }. From the steps of Algorithm 1, the computational complexity of the adaptive dropout high-order back-propagation algorithm is dominated by the computation of the partial derivatives of the reconstruction error function with regard to the parameters. Therefore, the computational complexity can be approximated as O(ρkI N J M ) where k denotes the number of iterations and ρ is the activating rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. OUTSOURCING SAMPLE SELECTION</head><p>The uncertainty sampling is an effective algorithm select the appropriate samples to outsource for binary classification. Uncertainty sampling always chooses the most ambivalent or least certain unlabeled samples to outsource. In detail, given a binary classification probabilistic model, such as p(z i = 1|x i ) = (1 + exp(-α T x i -β)) -1 , uncertainty sampling will select the samples with the probability p(z = 1|x) ≈ 1/2. Therefore, the uncertainty sampling algorithm can be viewed as the following optimization problem:</p><formula xml:id="formula_10">x := min x ( 1 2 -p(z|x)) 2 . (<label>5</label></formula><formula xml:id="formula_11">)</formula><p>Generally speaking, uncertainty sampling is very effective to select useful samples to outsource for binary classification probabilistic models. However, it cannot be applied to multiclassification models directly. To tackle this issue, the maximum entropy is used to select outsourcing samples.</p><p>An initial adaptive dropout deep computation model can be obtained after the parameters are pre-trained by Algorithm 1. The output of the initial model, Pr(z i |x i , θ), denotes the probability of the sample x i towards the class z i . Since the true class of x i is unknown, the predicted label z i is not useful to the purpose. However, Pr(z i |x i , θ) can be used to measure the confidence of the prediction. Furthermore, Pr(z i |x i , θ) can be used to compute the entropy of each sample. Specially, the information entropy with all class label probabilities of x i can be calculated by:</p><formula xml:id="formula_12">x Entropy i = - J ∑ j=1 Pr(z i |x i , θ) log Pr(z i |x i , θ). (<label>6</label></formula><formula xml:id="formula_13">)</formula><p>Since the entropy can measure the uncertainty of random variables effectively according to the maximum information entropy theory, the samples with the maximum entropy can be selected as the outsourcing objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. AGGREGATION ANSWER BASED ON EXPECTATION MAXIMIZATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Probabilistic Model</head><p>Given N samples X = {x 1 , x 2 , . . . , x N } selected from the training set according to the outsourcing sample selection algorithm proposed in the above section, each sample will be labeled by at most T annotators. Let y (t) i represent the label for the i-th sample given by the annotator t. Assuming that the number of classes is J, let z i be the unknown true label for the i-th sample. A confusion matrix {π Assuming that the annotators are independent for the training set X = {x 1 , x 2 , . . . , x N }, and the annotation provided by the annotator t only depends on the unknown truth label z i , the likelihood function of the parameters θ = {w, π</p><formula xml:id="formula_14">(t) ij } for the observation Y = {x i ; y (1) i , y (2) i , . . . , y (T ) i } N</formula><p>i=1 can be defined as:</p><formula xml:id="formula_15">Pr(Y |θ) = N ∏ i=1 Pr(y (1) i , . . . , y (T ) i |x i ; θ). (<label>7</label></formula><formula xml:id="formula_16">)</formula><p>By including the unknown true label Z = {z 1 , . . . , z N }, the likelihood function can be decomposed as:</p><formula xml:id="formula_17">Pr(Y |θ) = N ∏ i=1 ∑ zi Pr(y (1) i , . . . , y (T ) i |z i ; θ) • Pr(z i |x i ; θ). (8)</formula><p>Given the unknown ground-truth label z i with the assumption that the annotators make their decisions independently, the following equation can be obtained:</p><formula xml:id="formula_18">Pr(y (1) i , . . . , y (T ) i |z i ; θ) = T ∏ t=1 J ∏ j=1 (π (t) ij ) σ(y (t) i ,j) (9)</formula><p>Pr(z i |x i ; θ) denotes the output of the adaptive dropout deep computation model. For ease of exposition in the following part, Pr(z i |x i ; θ) is denoted as p i .</p><p>Therefore, the estimation of the parameters θ = {w, π (t) ij } can be obtained by maximizing the likelihood function:</p><formula xml:id="formula_19">θ := argmax θ {Pr(Y |θ)}. (<label>10</label></formula><formula xml:id="formula_20">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Maximum Likelihood Estimation</head><p>Maximizing the likelihood function will become the following problem after taking the logarithm and considering the unknown label Z = {z 1 , z , . . . , z N }: <ref type="bibr" target="#b10">(11)</ref> In this paper, this problem is solved by using the expectation maximization (EM) algorithm as follows:</p><formula xml:id="formula_21">θ := N ∑ i=1 ln ∑ zi Pr(y (1) i , . . . , y (T ) i |z i ; θ) • Pr(z i |x i ; θ).</formula><p>1) Initialization: In this paper, the majority voting is employed to initialize the aggregated labels µ i depending on the crowdsourcing matrix Y = {x i ; y</p><formula xml:id="formula_22">(1) i , y (2) i , . . . , y (T ) i } N i=1 . Afterwards, µ i is used to initialize the confusion matrix {π (t) ij }.</formula><p>Finally, the parameters w are initialized by the unsupervised pre-training in the adaptive dropout deep computation model.</p><p>2) E-Step: According to the crowdsourcing matrix Y and the estimation of the parameters θ = {w, π (t) ij }, the conditional expectation is computed via:</p><formula xml:id="formula_23">E{ln Pr(Y, Z|θ)} = N ∑ i=1 ∑ zi u i ln a i p i . (<label>12</label></formula><formula xml:id="formula_24">)</formula><p>where a i = Pr(y</p><formula xml:id="formula_25">(1) i , y<label>(2)</label></formula><p>i , . . . , y (T ) i |z i ; θ), p i = Pr(z i |x i ; θ), and the expectation is with regard to Pr(Y, Z|θ).</p><p>The E-step aims to calculate µ i by Bayes theorem as follows:</p><formula xml:id="formula_26">µ i = p i • T ∏ t=1 J ∏ j=1 (π (t) ij ) σ(y (t) i ,j) ∑ zi p i • T ∏ t=1 J ∏ j=1 (π (t) ij ) σ(y (t) i ,j) (13)</formula><p>3) M-Step: Depending on the current estimated values of the aggregated labels µ i , the parameters π (t) ij are calculated by equating the derivative of E{ln Pr(Y, Z|θ)} with regard to π (t) ij to 0 as follows:</p><formula xml:id="formula_27">π (t) ij = ∑ N i=1 µ i • σ(y (t) i , j) ∑ N i=1 µ i . (<label>14</label></formula><formula xml:id="formula_28">)</formula><p>Then the parameters w can be updated by the adaptive dropout deep computation model.</p><p>From the procedure of the answer aggregation based on expectation maximization, the E-Step and the M-Step are calculated by the forward-pass and the back-propagation, respectively, in each iteration. Therefore, given the labels as the supervised samples from the crowdsourcing, the parameters of the adaptive dropout deep computation model can be refined by iterating E-step and M-step till convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENT</head><p>In the experiments, the adaptive dropout deep computation model (ADDCM) and the adaptive dropout deep computation model with crowdsourcing to cloud (ADDCM-DDC) are compared with the standard deep computation model (DCM) and the dropout deep computation model (DDCM) on two representative data sets, i.e., CUAVE and SNAE2 <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>. Specially, CUAVE is a bi-modal dataset consisting of 1800 utterances of digits 0 to 9 said by 36 individuals for 5 times, which is widely used to evaluate the performance of deep learning models. SNAE2 consists of four groups of video clips, i.e., sport, new, advertisement and entertainment, each with 2 labels. Top-1 and top-2 error rates are used to evaluate the classification performance of the presented model since top-N is the most widely used statistics metric for validating deep learning models <ref type="bibr" target="#b24">[25]</ref>. Top-1 is used to check if the top class (the one having the highest probability) is the same as the target label while top-N is used to check if the target label is one of the top N predictions (the N ones with the highest probabilities). Since there are only a few labels in the two experimental datasets, top-1 and top-2 are selected to evaluate the performance of the presented model. Furthermore, each model is run for 5 times, each running with distinct initial parameters, to evaluate the robustness of the presented model. The average top-1 and top-2 error rates are reported in the following parts. The experiments are conducted using MATLAB R2014b on the laptop with 3.2 GHz Core i7 CPU, 8GB memory and 1T driver.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Performance Evaluation of Adaptive Dropout Deep Computation Model (ADDCM)</head><p>In this section, ADDCM is compared with DDCM with a constant activating rate of 0.5 and DCM on CUAVE and   <ref type="table">II</ref>-V, the classification accuracy produced by DCM becomes gradually lower in terms of top-1 and top-2 when the number of hidden layers is more than 3 due to the over-fitting. However, the classification accuracy produced by ADDCM and DDCM still increases slightly when they have 4 hidden layers. Such observations demonstrate that both ADDCM and DDCM can prevent the over-fitting more effectively than the standard deep computation model. Furthermore, ADDCM yields smaller top-1 and top-2 values than DDCM. For example, when they have 4 hidden layers, ADDCM yields the top-1 and top-2 values of 15.2% and 12.9% while DDCM yields 18.7% and 15.5%. Therefore, the presented model achieves higher classification accuracy than DDCM in terms of top-1 and top-2 on CUAVE and SNAE2.</p><p>Furthermore, the influence of the parameter σ that is used to control the decay velocity of the activating rate ρ with regard to the layer l is verified.     in terms of top-1 and top-2 on CUAVE and SNAE2.</p><p>From Table <ref type="table" target="#tab_4">VI</ref>-IX, the classification accuracy is very low when σ &lt; 10 since most of connections in the higher layers are disconnected. When σ ≥ 10, the classification accuracy changes significantly with σ varying. Specially, top-2 keeps the same value of 7.5% when σ ≥ 10 on CUAVE. Such results demonstrate that some of the connections in the fullyconnected layers are redundant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance Evaluation of Adaptive Dropout Deep Computation Model Based on Data-Drive Crowdsourcing (ADDCM-DDC)</head><p>Now, the performance of the adaptive dropout deep computation model with crowdsourcing to cloud (ADDCM-DDC) is evaluated on CUAVE and SNAE2. The unsupervised pretraining on ADDCM is performed using the training set to get the initial parameters, denoted by ADDCM-UN. Afterwards, the proposed outsourcing sample selection algorithm is used to choose 50% samples from the training set to be labeled by the annotators and then the ADDCM-DDC model is used to finetune the parameters, denoted by ADDCM-DDC. Moreover, the supervised fine-tuning on ADDCM is performed using the ground-truth labels in the training set to get final parameters,    achieves the smallest top-1 and top-2 values since the ground-truth labeled data are used to fine-tune the parameters. Such two observations indicate that the labeled samples are very important to the classification accuracy of deep computation models. Finally, ADDCM-DDC obtains the lower classification accuracy than ADDCM-SU while it yields the significantly higher top-1 and top-2 values than ADDCM-UN. For example, ADDCM-DDC yields top-1 value of 15.4 while ADDCM-SU and ADDCM-UN produce 15.2 and 19.5, respectively, when they have 4 hidden layers on SNAE2. Such results demonstrate that the proposed scheme is effective to train the parameters by the data-driven crowdsourcing.</p><p>In order to further validate the performance of the presented model, the random selection strategy is substituted for the proposed outsourcing samples section algorithm, denoted by ADDCM-RS. Furthermore, the proposed answer aggregation scheme is replaced with three representative techniques, i.e., majority voting, Honeypot and expectation maximization, denoted by ADDCM-MV, ADDCM-HP and ADDCM-EM, respectively. In this experiment, the samples with the least     <ref type="table" target="#tab_11">XIV</ref>-XVII show that ADDCM-DDC produces the higher classification accuracy in terms of top-1 and top-2 than ADDCM-RS on CUAVE and SNAE2, validating the effectiveness of the proposed outsourcing samples selection algorithm. Futhermore, ADDCM-DDC outperforms ADDCM-EM, ADDCM-MV and ADDCM-HP since ADDCM-DDC produces smaller top-1 and top-2 values than the other three models. For example, when they have 4 hidden layers, ADDCM-DDC produces the top-2 value 9% while ADDCM-EM, ADDCM-EM and ADDCM-HP produce 9.8%, 1.2% and 12.8%, respectively, on CUAVE. Such results demonstrate that the presented answer aggregation scheme performs better than other answer aggregation techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>In this paper, an adaptive dropout deep computation model with crowdsourcing to the cloud computing is presented for big data feature learning in industrial Internet of Things. One  Generally, the classification results of deep learning models are affected significantly by the initialization in most cases. However, the optimal techniques for initialization were not considered in the presented scheme. To evaluate the robustness of the presented model, each model was trained for 5 times in experiments and the average classification accuracy was used to validated the effectiveness of the presented model. In future work, some optimal techniques for initialization will be investigated to apply for the presented model, which is expected to further improve the performance of the presented model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>7 H ( 2 ) 15 σ ( 2 ) 17 ∆b ( 2 ) 19 ∆w ( 2 ) 21 b ( 1 ) 23 ∆w ( 1 )</head><label>72152172192211231</label><figDesc>2 for example = 1, 2, ..., M do 3 for j l = 1, . . . , J l (l = 1, . . . , L) do 4 Use Eq.(1) to compute H j1...jL ; 5 Use Eq.(4) to compute ρ; 6 mark{j 1 . . . , j L } = rand(size(H (2) j1...jL ) &gt; ρ); j1...jL = H (2) j1...jL . * mark{j 1 . . . , j L };8 for i k = 1, . . . , I k (k = 1, . . . , K) do 9 Use Eq.(2) to compute Y i1...iK ; 10 if J T AE (θ) &gt; threshold then 11 for i = 1, ..., I 1 × I 2 × • • • × I K do 12 Compute σ (3) i [13];13 for j l = 1, . . . , J l (l = 1, . . . , L) do 14 Compute σ (2) j1j2...jL [13]; j1...jL = σ (2) j1...jL . * [ones(size(σ (2) j1...jL , 1), 1)mark{j 1 . . . , j L }]; 16 for i k = 1, . . . , I k (k = 1, . . . , K) do i1...iK = ∆b (2) i1...iK + σ (3) i1...iK ; 18 for j l = 1, . . . , J l (l = 1, . . . , L) do i1...iK j1...jL = ∆w (2) i1...iK j1...jL + H (2) j1...jL • σ (3) i1...iK ; 20 for j l = 1, . . . , J l (l = 1, . . . , L) do j1...j L = ∆b (1) j1...j L + σ (2) j1...j L ; 22 for i n = 1, . . . , I n (n = 1, . . . , N ) do j1...jLi1...iK = ∆w (1) j1...jLi1...iK + x i1...i K • σ (2) j1...jL ; 24 //Update the parameters 25</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>ij } is defined in this paper, where π (t) ij denotes the probability that the annotator t assigns class j to the sample with the true class i. The aggregation answer aims to estimate the parameters {w, π (t) ij }, where w denotes the weights of the adaptive dropout deep computation model, and to produce an estimate for the groundtruth Z = {z 1 , z 2 , . . . , z N }.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>To test the effect of the number of hidden layers, several deep architectures are also established, each architecture with different number of hidden layers. For each architecture, the unsupervised pre-training is first performed to obtain the initial parameters, and then the supervised fine-tuning using the training set is performed to obtain the final parameters. Finally, different parameters are used to classify the testing set. The results are presented in TableII-TableV.From Table</figDesc><table><row><cell></cell><cell cols="2">TABLE III</cell><cell></cell><cell></cell></row><row><cell cols="4">TOP-2 ERROR RATES ON CUAVE</cell><cell></cell></row><row><cell>Model/Layers</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell>DCM</cell><cell>13.3%</cell><cell cols="3">10.4% 14.6% 20.3%</cell></row><row><cell>DDCM</cell><cell>14.4%</cell><cell>9.4%</cell><cell>9.2%</cell><cell>13.6%</cell></row><row><cell>ADDCM</cell><cell>10.5%</cell><cell>7.6%</cell><cell>7.5%</cell><cell>9.7%</cell></row><row><cell>SNAE2.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table VI -</head><label>VI</label><figDesc>IX present the average classification results of the presented model with different σ</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TII.2018.2791424, IEEE Transactions on Industrial Informatics</figDesc><table><row><cell>TABLE IX TOP-2 WITH DIFFERENT σ ON SNAE2 3 5 10 15 46.4% 36% 18.4% 17% 39.1% 27.2% 15.2% 13.4% 13.4% 20 16.8% 36.6% 22.1% 13.7% 12.9% 12.8% 44.IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS Layers/σ 2 3 4 5</cell><cell>7</cell></row></table><note><p>5% 26.1% 17.1% 16.6% 16.5% 1551-3203 (c) 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE X</head><label>X</label><figDesc></figDesc><table><row><cell cols="4">TOP-1 ERROR RATES ON CUAVE</cell><cell></cell></row><row><cell>Model/Layers</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell>ADDCM-UN</cell><cell cols="4">20.8% 17.4% 16.2% 20.2%</cell></row><row><cell>ADDCM-SU</cell><cell cols="4">16.4% 12.5% 10.7% 16.5%</cell></row><row><cell cols="5">ADDCM-DDC 17.2% 13.1% 11.5% 17.4%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>The results are reported in Table X-XIII. From Table X-XIII, the worst results are generally produced by ADDCM-UN since the parameters are not fine-tuned.</figDesc><table><row><cell></cell><cell cols="2">TABLE XI</cell><cell></cell><cell></cell></row><row><cell cols="4">TOP-2 ERROR RATES ON CUAVE</cell><cell></cell></row><row><cell>Model/Layers</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell>ADDCM-UN</cell><cell cols="3">14.4% 12.2% 11.5%</cell><cell>13%</cell></row><row><cell>ADDCM-SU</cell><cell>10.5%</cell><cell>7.6%</cell><cell>7.5%</cell><cell>9.7%</cell></row><row><cell>ADDCM-DDC</cell><cell>12%</cell><cell>9.6%</cell><cell>9%</cell><cell>11.6%</cell></row><row><cell cols="5">denoted by ADDCM-SU. Finally, ADDCM-UN, ADDCM-</cell></row><row><cell cols="5">DDC and ADDCM-SU are used to classify CUAVE and</cell></row><row><cell>SNAE2, respectively.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE XIII</head><label>XIII</label><figDesc></figDesc><table><row><cell cols="4">TOP-2 ERROR RATES ON SNAE2</cell><cell></cell></row><row><cell>Model/Layers</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell>ADDCM-UN</cell><cell cols="3">21.2% 17.4% 17.1%</cell><cell>20%</cell></row><row><cell>ADDCM-SU</cell><cell>17%</cell><cell cols="3">13.4% 12.9% 16.6%</cell></row><row><cell cols="2">ADDCM-DDC 17.8%</cell><cell>15%</cell><cell cols="2">14.4% 20.2%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>are selected as the trapping questions of the Honeypot technique. The results are presented in Table XIV-XVII.Table</figDesc><table><row><cell></cell><cell cols="2">TABLE XV</cell><cell></cell><cell></cell></row><row><cell cols="4">TOP-2 ERROR RATES ON CUAVE</cell><cell></cell></row><row><cell>Model</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell>ADDCM-DDC</cell><cell>12%</cell><cell>9.6%</cell><cell>9%</cell><cell>11.6%</cell></row><row><cell>ADDCM-RS</cell><cell>14.8%</cell><cell>12%</cell><cell cols="2">0.13.8% 12.6%</cell></row><row><cell>ADDCM-EM</cell><cell cols="2">13.1% 9.8%</cell><cell>9.8%</cell><cell>12.1%</cell></row><row><cell>ADDCM-MV</cell><cell cols="2">13.4% 9.8%</cell><cell>10.2%</cell><cell>12.4%</cell></row><row><cell>ADDCM-HP</cell><cell>14.3%</cell><cell>12%</cell><cell>12.8%</cell><cell>12.9%</cell></row></table><note><p>entropy</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head></head><label></label><figDesc>of the presented model is to design an adaptive distribution function to set the dropout rate of each hidden layer for preventing over-fitting. Another advantage is the combination of the crowdsourcing technique and the parameter training, which can address the problem of the lack of available training samples for training parameters in the deep computation model. Experimental results demonstrated that the proposed model could avoid over-fitting effectively and provide the labeled samples for training the parameters of the deep computation model, which is potential for big data feature learning in industrial Internet of Things.</figDesc><table><row><cell></cell><cell cols="2">TABLE XVII</cell><cell></cell><cell></cell></row><row><cell cols="4">TOP-2 ERROR RATES ON SNAE2</cell><cell></cell></row><row><cell>Model</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell>ADDCM-DDC</cell><cell>17.8%</cell><cell>15%</cell><cell cols="2">14.4% 20.2%</cell></row><row><cell>ADDCM-RS</cell><cell cols="2">20.2% 17.4%</cell><cell>17%</cell><cell>22%</cell></row><row><cell>ADDCM-EM</cell><cell cols="4">18.5% 15.9% 15.7% 21.6%</cell></row><row><cell>ADDCM-MV</cell><cell cols="2">18.6% 16.8%</cell><cell>15%</cell><cell>21.8%</cell></row><row><cell>ADDCM-HP</cell><cell cols="3">20.4% 17.3% 16.1%</cell><cell>22%</cell></row></table><note><p>advantage</p></note></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>, IEEE Transactions on Industrial Informatics IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS 1 , IEEE Transactions on Industrial Informatics IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Internet of Things in Industries: A Survey</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="2233" to="2243" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Localized Confident Information Coverage Hole Detection in Internet of Things for Radioactive Pollution Monitoring</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="18665" to="18674" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dependable Structural Health Monitoring Using Wireless Sensor Networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Z A</forename><surname>Bhuiyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Dependable and Secure Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="363" to="376" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">PPHOPCM: Privacypreserving High-order Possibilistic c-Means Algorithm for Heterogeneous Data Fuzzy Clustering</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/TBDATA.2017.2701816</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Distributed Feature Selection for Efficient Economic Big Data Analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TBDATA.2016.2601934</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Privacy-preserving Double-projection Deep Computation Model with Crowdsourcing on Cloud for Big Data Feature Learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Deen</surname></persName>
		</author>
		<idno type="DOI">10.1109/JIOT.2017.2732735</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet of Things Journal</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multimedia Processing Pricing Strategy in GPU-accelerated Cloud Computing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vasilakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nagano</surname></persName>
		</author>
		<idno type="DOI">10.1109/TC-C.2017.2672554</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cloud Computing</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Survey on Deep Learning for Big Data</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="146" to="157" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep Computation Model for Unsupervised Feature Learning on Big Data</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Services Computing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="161" to="171" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Improving Neural Networks by Preventing Co-adaptation of Feature Detectors</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Challenges in Data Crowdsourcing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Garcia-Molina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joglekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Parameswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Verroios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="901" to="911" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning from Crowds</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">C</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Valadez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Florin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bogoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Moy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1297" to="1322" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An Improved Deep Computation Model Based on Canonical Polyadic Decomposition</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/TSMC.2017.2701797</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics: Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Energy-Efficient Scheduling for Real-Time Systems Based on Deep Q-Learning Model</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/T-SUSC.2017.2743704</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Sustainable Computing</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Limits on the Majority Vote Accuracy in Classifier Fusion</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">I</forename><surname>Kuncheva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="31" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The Social Honeypot Project: Protecting Online Communities from Spammers</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Caverlee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on World Wide Web</title>
		<meeting>International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1139" to="1140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Quality Control of Crowd Labeling Through Expert Evaluation</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">K</forename><surname>Khattak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Salleb-Aouissi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS Workshop on Computational Social Science and the Wisdom of Crowds</title>
		<meeting>NIPS Workshop on Computational Social Science and the Wisdom of Crowds</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Active Learning from Crowds</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rosales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Dy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1161" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Whose Vote Should Count More: Optimal Integration of Labels from Labelers of Unknown Expertise</title>
		<author>
			<persName><forename type="first">J</forename><surname>Whitehill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bergsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Movellan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Ruvolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2035" to="2043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Iterative Learning for Reliable Crowdsourcing Systems</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Karger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1953" to="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A Crowdsourcing Worker Quality Evalution Algorithm on MapReduce for Gig Data Applications</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1879" to="1888" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An evaluation of Aggregation Techniques in Crowdsourcing</title>
		<author>
			<persName><forename type="first">N</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">T</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Aberer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Web Information Systems Engineering</title>
		<meeting>International Conference on Web Information Systems Engineering</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Doddington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Goudie-Marshall</surname></persName>
		</author>
		<title level="m">The Darpa Speech Recognition Research Database: Specifications and Status</title>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="93" to="99" />
		</imprint>
	</monogr>
	<note>Proceedings of DARPA Workshop on Speech Recognition</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cuave: A New Audio-visual Database for Multimodal Human-computer Interface Research</title>
		<author>
			<persName><forename type="first">E</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gurbuz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tufekci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gowdy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2002 IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting>2002 IEEE International Conference on Acoustics, Speech, and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="2017" to="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Qingchen Zhang received the B.Eng. degree in Southwest University, China, and the Ph.D. degree in Dalian University of Technology</title>
		<meeting><address><addrLine>China; St. Francis Xavier University</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
	<note>His research interests include Big Data and Deep Learning</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">D degree at University of Victoria, Canada. He is currently a professor at University of Electronic Science and Technology of China and St. Francis Xavier University, Canada. His research has been supported by the National Sciences and Engineering Research Council, and the Canada Foundation for Innovation</title>
		<author>
			<persName><forename type="first">Laurence</forename><forename type="middle">T</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">His research interests include parallel and distribut</title>
		<imprint/>
	</monogr>
	<note>Tsinghua University, China, and the Ph. big data and cyber-physical-social systems</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
