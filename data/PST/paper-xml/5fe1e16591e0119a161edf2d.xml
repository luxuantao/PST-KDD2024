<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">COAD: Contrastive Pre-training with Adversarial Fine-tuning for Zero-shot Expert Linking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
							<email>zhang-jing@ruc.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Xiaokang</forename><surname>Zhang</surname></persName>
							<email>zhangp31@tsinghua.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Zhi</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Xiaobin Tang</orgName>
								<address>
									<addrLine>Lingfan Cai, Jie Tang</addrLine>
									<settlement>Hong Chen, Cuiping Li, Peng Zhang, Fellow</settlement>
									<region>IEEE</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Hong Chen and Cuiping Li are with Information School</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<addrLine>Jing Zhang, Xiaokang Zhang, Lingfan Cai</addrLine>
									<settlement>Bo Chen, Xiaobin Tang, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Peng Zhang are with Department of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">Tsinghua National Laboratory for Information Science and Technology (TNList)</orgName>
								<orgName type="institution" key="instit1">Jie Tang</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">AI research institute of Tsinghua University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">COAD: Contrastive Pre-training with Adversarial Fine-tuning for Zero-shot Expert Linking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Expert Linking</term>
					<term>Pre-training</term>
					<term>Contrastive Learning</term>
					<term>Adversarial Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Expert finding, a popular service provided by many online websites such as Expertise Finder, LinkedIn, and AMiner, benefits seeking consultants, collaborators, and candidate qualifications. However, its quality is suffered from a single source of support information for experts. This paper employs AMiner, a free online academic search and mining system, having collected more than over 100 million researcher profiles together with 200 million papers from multiple publication databases [36], as the basis for investigating the problem of expert linking, which aims at linking any external information of persons to experts in AMiner. A critical challenge is how to perform zero-shot expert linking without any labeled linkages from the external information to AMiner experts, as it is infeasible to acquire sufficient labels for arbitrary external sources. Inspired by the success of self-supervised learning in computer vision and natural language processing, we propose to train a self-supervised expert linking model, which is first pre-trained by contrastive learning on AMiner data to capture the common representation and matching patterns of experts across AMiner and external sources, and is then fine-tuned by adversarial learning on AMiner and the unlabeled external sources to improve the model transferability. Experimental results demonstrate that COAD significantly outperforms various baselines without contrastive learning of experts on two widely studied downstream tasks -author identification (improving up to 32.1% in HitRatio@1) and paper clustering (improving up to 14.8% in Pairwise-F1). Expert linking on two genres of external sources also indicates the superiority of the proposed adversarial fine-tuning method compared with other domain adaptation ways (improving up to 2.3% in HitRatio@1).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>O LINE websites such as Toptal 1 , Expertise Finder 2 , LinkedIn 3 and AMiner 4 provide valuable services of expert finding for governments or research groups to find consultants, collaborators and reviewers, and also for companies to find candidate qualifications, team dynamics, etc. However, the information about an expert is dispersed across different sources. For example, Google Scholar and AMiner mainly maintain the published papers, LinkedIn keeps the skills and background, while a large number of real-time news articles report the dynamic activities of experts. A single source of support information is far from comprehensive and convincing to support the high-quality expert finding, which demands to integrate heterogeneous support information about experts together.</p><p>In this paper, employing AMiner, a researcher-centric search system that allows users to find domain experts and collaborators, as the basis for our experimental data, we explain how to deal with the problem of expert linking across heterogeneous sources. AMiner is a free online academic search and mining system <ref type="bibr" target="#b35">[36]</ref>, which automatically collects researchers' profiles from the Web and integrates with published papers after name disambiguation <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b49">[50]</ref>. To date, it has collected more than 133,199,983 researchers' profiles together with 271,863,708 papers. We target at linking arbitrary information that describes an expert from external sources to the experts in AMiner. Figure <ref type="figure" target="#fig_0">1</ref> presents an example of the candidate AMiner experts for two ambiguous names mentioned in a news article. The problem is challenging as many candidates with the same affiliation or even similar research fields are to be distinguished.</p><p>Expert linking is closely related to the standard entity arXiv:2012.11336v1 [cs.IR] 14 Dec 2020</p><p>linking task, which usually links the mentions-the named entities extracted from the unstructured text-to the entities in a knowledge graph (often Wikipedia or Freebase) <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>. The latest models usually convert the entities and the mentions into low-dimensional embeddings such that the aligned mentions and the entities can be close to each other. Most of the extensive study for the entity linking are in a supervised manner depending on a large labeled dataset such as AIDA CoNLL-YAGO and KBP Track at TAC 2010 5 . Unfortunately, the linkages between external information and the AMiner experts are often arduous to obtain. For example, in AMiner, it usually spends up to several hours to correct the collected papers for a top expert by a skilled annotator. Moreover, the external information about experts come from arbitrary sources, making it unforeseeable for us to annotate the corresponding labels beforehand. In view of this limitation of lacking the labels, we pay attention to the problem of zero-shot expert linking.</p><p>A natural question arises: can we learn a universal expert linking model from abundant AMiner experts such that it can be transferable to unseen external expert linking?</p><p>The similar question has also be studied in natural language processing (NLP), computer vision (CV) and graph representation (GR). Facing abundant unlabeled data and a small number of labeled data, the most powerful paradigm in these fields is self-supervised learning targeting at pretraining a function on the large unlabeled data and finetuning it on the small number of labeled data. The function can represent a fundamental unit such as the natural language texts, images or graphs/nodes to capture the inherent correlations between them during pre-training, and is able to be transferred to represent the unseen units after finetuning. The typical self-supervised models include BERT <ref type="bibr" target="#b7">[8]</ref> and XLNet <ref type="bibr" target="#b43">[44]</ref> in NLP, MoCo <ref type="bibr" target="#b14">[15]</ref> and SimCLR <ref type="bibr" target="#b3">[4]</ref> in CV, and GCC <ref type="bibr" target="#b29">[30]</ref> and GPT-GNN <ref type="bibr" target="#b17">[18]</ref> in GR.</p><p>Present Work. Inspired by the success of pre-training in NLP, CV and GR, we propose to pre-train an expert linking model on the large unlabeled AMiner data. Because of the ultimate goal of expert linking, the model should be able to capture both the common representation patterns of experts and the matching patterns between experts across AMiner and external sources. To achieve this goal, two main challenges should be addressed. First, since an expert, usually described by different types of support information such as demographic attributes, papers and news articles, is neither a continuous, high-dimensional signal as an image nor a single discrete signal as a word, it is challenging to determine how to represent an expert and further how to measure the similarity of two experts. Second, sometimes, the gap of morphology, syntax, topics between the external sources and AMiner is obvious, which encourages us to finetune the pre-trained model on external sources to improve its transferability. But unlike the assumption of the small labeled data in downstream tasks, none of the labels can be obtained on most external sources, which increases the difficulty of fine-tuning.</p><p>We propose the COntrastive Pre-training with ADversarial Fine-tuning model (COAD) to enable expert linking from the external sources to AMiner experts. We 5. http://nlpprogress.com/english/entity linking.html leverage contrastive learning <ref type="bibr" target="#b38">[39]</ref> to pre-train the model. The basic idea is to define expert discrimination as the pre-training task which samples, for each AMiner expert, the random instances, makes them from the same experts close and discriminates them from the different experts. Specifically, in COAD, we define an expert instance as a set of papers published by the expert, use BERT to encode each expert instance, and propose an interaction-based metric function to measure the similarity of two expert instances. Within the model, BERT can universally encode various types of support information such as demographic attributes, papers and news articles. And the interactionbased metric function can capture the fine-grained semantic matches between support information. After pre-training the BERT encoder and the metric function by contrastive learning, to make the functions transferable to unlabeled external sources, we leverage adversarial learning to align the representations of the external persons to those of the AMiner experts.</p><p>Although some pre-training models have been studied to tackle the zero-shot entity linking problem in some domains, the pre-training models are trained in a supervised manner on the large linkages of other domains <ref type="bibr" target="#b24">[25]</ref>. However, the proposed pre-training model is in a self-supervised manner on the large unlabeled data.</p><p>We conduct extensive experiments on AMiner, News and LinkedIn to evaluate the performance of COAD. We evaluate the contrastive pre-training module and the adversarial fine-tuning module in COAD in two steps. Specifically, we first pre-train COAD on the pseudo labels of expert instances sampled from AMiner and evaluate its representation and the matching capacity by two intrinsic tasks on AMiner-author identification and paper clustering. After pre-training, we fine-tune COAD on both the AMiner data and the unlinked news articles or LinkedIn homepages. Then we evaluate the transferability of the model by the extrinsic task-linking from the name mentions in news articles or LinkedIn users to AMiner experts.</p><p>We summarize our contributions as follows: • We propose COAD, consisting of a contrastive pretraining module and an adversarial fine-tuning module, to address zero-shot expert linking from external sources to AMiner. • We design the pre-training task as expert discrimination to characterize the universal representation patterns of experts and matching patterns between experts. An interaction-based metric function is adopted to capture the fine-grained matches. Adversarial learning is leveraged to improve the transferability of pre-trained module. • The extensive experiments on author identification and paper clustering of AMiner, and expert linking from news and LinkedIn to AMiner demonstrate the superior representation and matching capacity both within and across domains. All codes and data are publicly available 6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PROBLEM FORMULATION</head><p>This section defines the input of experts and formulate the problem of zero-shot expert linking in an self-supervised manner.</p><p>6. https://github.com/BoChen-Daniel/Expert-Linking               The support information of experts differs from different sources. For example, if an expert e is derived from a news article, c e can be the surrounding text of its name mentioned in the news article. The text can be divided into multiple sentences, indicating multiple pieces of support information. If e is derived from the LinkedIn website, c e can be e's homepage, which contains multiple attributes such as summary, affiliations, skills, etc. Since we aim at linking arbitrary external information to AMiner, we particularly denote the support information of an AMiner expert by c e = {p 1 , • • • , p ne }, where each p i indicates a paper which contains title, keywords, venue, author names, author affiliations, etc.</p><formula xml:id="formula_0">F Y h G r b k A 1 C i 6 x Z b g R 2 E 0 U 0 i g Q 2 A k m t 3 O / 8 4 R K 8 1 g + m m m C f k R H k o e c U W O l T v V + k O G s O i h X 3 J q 7 A F k n X k 4 q k K M 5 K H / 1 h z F L I 5 S G C a p 1 z 3 M T 4 2 d U G c 4 E z k r 9 V G N C 2 Y S O s G e p p B F q P 1 u c O y M X V h m S M F a 2 p C E L 9 f d E R i O t p 1 F g O y N q x n r V m 4 v / e b 3 U h D d + x m W S G p R s u S h M B T E x m f 9 O h l w h M 2 J q C W W K 2 1 s J G 1 N F m b E J l W w I 3 u r L 6 6 R d r 3 l u z X u o V x p X e R x F O I N z u A Q P r q E B d 9 C E F j C Y w D O 8 w p u T O C / O u / O x b C 0 4 + c w p / I H z + Q O M Y o 7 9 &lt; / l a t e x i t &gt; I + e &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U V 9 O j d A G C E 9 o V 1 8 p + K R 5 R l y 9 c A E = " &gt; A A A B 8 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y C I I Q t g N g h 4 D X v Q W w T w k i W F 2 0 p s M m d l d Z m a F s O Q r v H h Q x K u f 4 8 2 / c Z L s Q R M L G o q q b r q 7 / F h w b V z 3 2 1 l Z X V v f 2 M x t 5 b d 3 d v f 2 C w e H D R 0 l i m G d R S J S L Z 9 q F D z E u u F G Y C t W S K U v s O m P r q d + 8 w m V 5 l F 4 b 8 Y x d i U d h D z g j B o r P Z R u H 8 9 7 K U 5 K v U L R L b s z k G X i Z a Q I G W q 9 w l e n H 7 F E Y m i Y o F q 3 P T c 2 3 Z Q q w 5 n A S b 6 T a I w p G 9 E B t i 0 N q U T d T W c H T 8 i p V f o k i J S t 0 J C Z + n s i p V L r s f R t p 6 R m q B e 9 q f i f 1 0 5 M c N V N e R g n B k M 2 X x Q k g p i I T L 8 n f a 6 Q G T G 2 h D L F 7 a 2 E D a m i z N i M 8 j Y E b / H l Z d K o l D 2 3 7 N 1 V i t W L L I 4 c H M M J n I E H l 1 C F G 6 h B H R h I e I Z X e H O U 8 + K 8 O x / z 1 h U n m z m C P 3 A + f w C p 3 I + a &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U V 9 O j d A G C E 9 o V 1 8 p + K R 5 R l y 9 c A E = " &gt; A A A B 8 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y C I I Q t g N g h 4 D X v Q W w T w k i W F 2 0 p s M m d l d Z m a F s O Q r v H h Q x K u f 4 8 2 / c Z L s Q R M L G o q q b r q 7 / F h w b V z 3 2 1 l Z X V v f 2 M x t 5 b d 3 d v f 2 C w e H D R 0 l i m G d R S J S L Z 9 q F D z E u u F G Y C t W S K U v s O m P r q d + 8 w m V 5 l F 4 b 8 Y x d i U d h D z g j B o r P Z R u H 8 9 7 K U 5 K v U L R L b s z k G X i Z a Q I G W q 9 w l e n H 7 F E Y m i Y o F q 3 P T c 2 3 Z Q q w 5 n A S b 6 T a I w p G 9 E B t i 0 N q U T d T W c H T 8 i p V f o k i J S t 0 J C Z + n s i p V L r s f R t p 6 R m q B e 9 q f i f 1 0 5 M c N V N e R g n B k M 2 X x Q k g p i I T L 8 n f a 6 Q G T G 2 h D L F 7 a 2 E D a m i z N i M 8 j Y E b / H l Z d K o l D 2 3 7 N 1 V i t W L L I 4 c H M M J n I E H l 1 C F G 6 h B H R h I e I Z X e H O U 8 + K 8 O x / z 1 h U n m z m C P 3 A + f w C p 3 I + a &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U V 9 O j d A G C E 9 o V 1 8 p + K R 5 R l y 9 c A E = " &gt; A A A B 8 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y C I I Q t g N g h 4 D X v Q W w T w k i W F 2 0 p s M m d l d Z m a F s O Q r v H h Q x K u f 4 8 2 / c Z L s Q R M L G o q q b r q 7 / F h w b V z 3 2 1 l Z X V v f 2 M x t 5 b d 3 d v f 2 C w e H D R 0 l i m G d R S J S L Z 9 q F D z E u u F G Y C t W S K U v s O m P r q d + 8 w m V 5 l F 4 b 8 Y x d i U d h D z g j B o r P Z R u H 8 9 7 K U 5 K v U L R L b s z k G X i Z a Q I G W q 9 w l e n H 7 F E Y m i Y o F q 3 P T c 2 3 Z Q q w 5 n A S b 6 T a I w p G 9 E B t i 0 N q U T d T W c H T 8 i p V f o k i J S t 0 J C Z + n s i p V L r s f R t p 6 R m q B e 9 q f i f 1 0 5 M c N V N e R g n B k M 2 X x Q k g p i I T L 8 n f a 6 Q G T G 2 h D L F 7 a 2 E D a m i z N i M 8 j Y E b / H l Z d K o l D 2 3 7 N 1 V i t W L L I 4 c H M M J n I E H l 1 C F G 6 h B H R h I e I Z X e H O U 8 + K 8 O x / z 1 h U n m z m C P 3 A + f w C p 3 I + a &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U V 9 O j d A G C E 9 o V 1 8 p + K R 5 R l y 9 c A E = " &gt; A A A B 8 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y C I I Q t g N g h 4 D X v Q W w T w k i W F 2 0 p s M m d l d Z m a F s O Q r v H h Q x K u f 4 8 2 / c Z L s Q R M L G o q q b r q 7 / F h w b V z 3 2 1 l Z X V v f 2 M x t 5 b d 3 d v f 2 C w e H D R 0 l i m G d R S J S L Z 9 q F D z E u u F G Y C t W S K U v s O m P r q d + 8 w m V 5 l F 4 b 8 Y x d i U d h D z g j B o r P Z R u H 8 9 7 K U 5 K v U L R L b s z k G X i Z a Q I G W q 9 w l e n H 7 F E Y m i Y o F q 3 P T c 2 3 Z Q q w 5 n A S b 6 T a I w p G 9 E B t i 0 N q U T d T W c H T 8 i p V f o k i J S t 0 J C Z + n s i p V L r s f R t p 6 R m q B e 9 q f i f 1 0 5 M c N V N e R g n B k M 2 X x Q k g p i I T L 8 n f a 6 Q G T G 2 h D L F 7 a 2 E D a m i z N i M 8 j Y E b / H l Z d K o l D 2 3 7 N 1 V i t W L L I 4 c H M M J n I E H l 1 C F G 6 h B H R h I e I Z X</formula><formula xml:id="formula_1">O I h G p l k 8 1 C h 5 i 3 X A j s B U r p N I X 2 P R H 1 1 O / + Y R K 8 y i 8 N + M Y u 5 I O Q h 5 w R o 2 V H k q 3 j + e 9 F C e l X q H o l t 0 Z y D L x M l K E D L V e 4 a v T j 1 g i M T R M U K 3 b n h u b b k q V 4 U z g J N 9 J N M a U j e g A 2 5 a G V K L u p r O D J + T U K n 0 S R M p W a M h M / T 2 R U q n 1 W P q 2 U 1 I z 1 I v e V</formula><formula xml:id="formula_2">O I h G p l k 8 1 C h 5 i 3 X A j s B U r p N I X 2 P R H 1 1 O / + Y R K 8 y i 8 N + M Y u 5 I O Q h 5 w R o 2 V H k q 3 j + e 9 F C e l X q H o l t 0 Z y D L x M l K E D L V e 4 a v T j 1 g i M T R M U K 3 b n h u b b k q V 4 U z g J N 9 J N M a U j e g A 2 5 a G V K L u p r O D J + T U K n 0 S R M p W a M h M / T 2 R U q n 1 W P q 2 U 1 I z 1 I v e V</formula><formula xml:id="formula_3">O I h G p l k 8 1 C h 5 i 3 X A j s B U r p N I X 2 P R H 1 1 O / + Y R K 8 y i 8 N + M Y u 5 I O Q h 5 w R o 2 V H k q 3 j + e 9 F C e l X q H o l t 0 Z y D L x M l K E D L V e 4 a v T j 1 g i M T R M U K 3 b n h u b b k q V 4 U z g J N 9 J N M a U j e g A 2 5 a G V K L u p r O D J + T U K n 0 S R M p W a M h M / T 2 R U q n 1 W P q 2 U 1 I z 1 I v e V</formula><formula xml:id="formula_4">O I h G p l k 8 1 C h 5 i 3 X A j s B U r p N I X 2 P R H 1 1 O / + Y R K 8 y i 8 N + M Y u 5 I O Q h 5 w R o 2 V H k q 3 j + e 9 F C e l X q H o l t 0 Z y D L x M l K E D L V e 4 a v T j 1 g i M T R M U K 3 b n h u b b k q V 4 U z g J N 9 J N M a U j e g A 2 5 a G V K L u p r O D J + T U K n 0 S R M p W a M h M / T 2 R U q n 1 W P q 2 U 1 I z 1 I v e V</formula><formula xml:id="formula_5">M Y v Z A M B Q s Y J d p I 9 + W g g u f 4 c H Z a 7 h d L T t W Z w V 4 m b k Z K k K H R L 3 7 1 B h F N Q h S a c q J U 1 3 V i 7 a V E a k Y 5 T g q 9 R G F M 6 C M Z Y t d Q Q U J U X j q 7 e G K f G G V g B 5 E 0 J b Q 9 U 3 9 P p C R U a h z 6 p j M k e q Q W v a n 4 n 9 d N d H D l p U z E i U Z B 5 4 u C h N s 6 s q f v 2 w M m k W o + N o R Q y c y t N h 0 R S a g 2 I R V M C O 7 i y 8 u k V a u 6 T t W 9 r Z X q F 1 k c e T i C Y 6 i A C 5 d Q h x t o Q B M o C H i G V 3 i z l P V i v V s f</formula><formula xml:id="formula_6">M Y v Z A M B Q s Y J d p I 9 + W g g u f 4 c H Z a 7 h d L T t W Z w V 4 m b k Z K k K H R L 3 7 1 B h F N Q h S a c q J U 1 3 V i 7 a V E a k Y 5 T g q 9 R G F M 6 C M Z Y t d Q Q U J U X j q 7 e G K f G G V g B 5 E 0 J b Q 9 U 3 9 P p C R U a h z 6 p j M k e q Q W v a n 4 n 9 d N d H D l p U z E i U Z B 5 4 u C h N s 6 s q f v 2 w M m k W o + N o R Q y c y t N h 0 R S a g 2 I R V M C O 7 i y 8 u k V a u 6 T t W 9 r Z X q F 1 k c e T i C Y 6 i A C 5 d Q h x t o Q B M o C H i G V 3 i z l P V i v V s f</formula><formula xml:id="formula_7">M Y v Z A M B Q s Y J d p I 9 + W g g u f 4 c H Z a 7 h d L T t W Z w V 4 m b k Z K k K H R L 3 7 1 B h F N Q h S a c q J U 1 3 V i 7 a V E a k Y 5 T g q 9 R G F M 6 C M Z Y t d Q Q U J U X j q 7 e G K f G G V g B 5 E 0 J b Q 9 U 3 9 P p C R U a h z 6 p j M k e q Q W v a n 4 n 9 d N d H D l p U z E i U Z B 5 4 u C h N s 6 s q f v 2 w M m k W o + N o R Q y c y t N h 0 R S a g 2 I R V M C O 7 i y 8 u k V a u 6 T t W 9 r Z X q F 1 k c e T i C Y 6 i A C 5 d Q h x t o Q B M o C H i G V 3 i z l P V i v V s f</formula><formula xml:id="formula_8">M Y v Z A M B Q s Y J d p I 9 + W g g u f 4 c H Z a 7 h d L T t W Z w V 4 m b k Z K k K H R L 3 7 1 B h F N Q h S a c q J U 1 3 V i 7 a V E a k Y 5 T g q 9 R G F M 6 C M Z Y t d Q Q U J U X j q 7 e G K f G G V g B 5 E 0 J b Q 9 U 3 9 P p C R U a h z 6 p j M k e q Q W v a n 4 n 9 d N d H D l p U z E i U Z B 5 4 u C h N s 6 s q f v 2 w M m k W o + N o R Q y c y t N h 0 R S a g 2 I R V M C O 7 i y 8 u k V a u 6 T t W 9 r Z X q F 1 k c e T i C Y 6 i A C 5 d Q h x t o Q B M o C H i G V 3 i z l P V i v V s f</formula><formula xml:id="formula_9">M Y v Z A M B Q s Y J d p I 9 + W g g m f 4 c H 5 a 7 h d L T t W Z w V 4 m b k Z K k K H R L 3 7 1 B h F N Q h S a c q J U 1 3 V i 7 a V E a k Y 5 T g q 9 R G F M 6 C M Z Y t d Q Q U J U X j q 7 e G K f G G V g B 5 E 0 J b Q 9 U 3 9 P p C R U a h z 6 p j M k e q Q W v a n 4 n 9 d N d H D l p U z E i U Z B 5 4 u C h N s 6 s q f v 2 w M m k W o + N o R Q y c y t N h 0 R S a g 2 I R V M C O 7 i y 8 u k V a u 6 T t W 9 r Z X q F 1 k c e T i C Y 6 i A C 5 d Q h x t o Q B M o C H i G V 3 i z l P V i v V s f</formula><formula xml:id="formula_10">M Y v Z A M B Q s Y J d p I 9 + W g g m f 4 c H 5 a 7 h d L T t W Z w V 4 m b k Z K k K H R L 3 7 1 B h F N Q h S a c q J U 1 3 V i 7 a V E a k Y 5 T g q 9 R G F M 6 C M Z Y t d Q Q U J U X j q 7 e G K f G G V g B 5 E 0 J b Q 9 U 3 9 P p C R U a h z 6 p j M k e q Q W v a n 4 n 9 d N d H D l p U z E i U Z B 5 4 u C h N s 6 s q f v 2 w M m k W o + N o R Q y c y t N h 0 R S a g 2 I R V M C O 7 i y 8 u k V a u 6 T t W 9 r Z X q F 1 k c e T i C Y 6 i A C 5 d Q h x t o Q B M o C H i G V 3 i z l P V i v V s f</formula><formula xml:id="formula_11">M Y v Z A M B Q s Y J d p I 9 + W g g m f 4 c H 5 a 7 h d L T t W Z w V 4 m b k Z K k K H R L 3 7 1 B h F N Q h S a c q J U 1 3 V i 7 a V E a k Y 5 T g q 9 R G F M 6 C M Z Y t d Q Q U J U X j q 7 e G K f G G V g B 5 E 0 J b Q 9 U 3 9 P p C R U a h z 6 p j M k e q Q W v a n 4 n 9 d N d H D l p U z E i U Z B 5 4 u C h N s 6 s q f v 2 w M m k W o + N o R Q y c y t N h 0 R S a g 2 I R V M C O 7 i y 8 u k V a u 6 T t W 9 r Z X q F 1 k c e T i C Y 6 i A C 5 d Q h x t o Q B M o C H i G V 3 i z l P V i v V s f</formula><formula xml:id="formula_12">M Y v Z A M B Q s Y J d p I 9 + W g g m f 4 c H 5 a 7 h d L T t W Z w V 4 m b k Z K k K H R L 3 7 1 B h F N Q h S a c q J U 1 3 V i 7 a V E a k Y 5 T g q 9 R G F M 6 C M Z Y t d Q Q U J U X j q 7 e G K f G G V g B 5 E 0 J b Q 9 U 3 9 P p C R U a h z 6 p j M k e q Q W v a n 4 n 9 d N d H D l p U z E i U Z B 5 4 u C h N s 6 s q f v 2 w M m k W o + N o R Q y c y t N h 0 R S a g 2 I R V M C O 7 i y 8 u k V a u 6 T t W 9 r Z X q F 1 k c e T i C Y 6 i A C 5 d Q h x t o Q B M o C H i G V 3 i z l P V i v V s f</formula><p>As it is infeasible to prepare sufficient labeled data beforehand for training a supervised expert linking model on arbitrary external data, we address the zero-shot expert linking task in a self-supervised manner. Specifically, we study the problem of self-supervised expert linking by pretraining plus fine-tuning, one typical learning paradigm with self-supervision. The problem is to learn functions that can not only encode and align experts with similar support information on AMiner, but also be transferable to arbitrary external sources to encode and link unseen persons to AMiner experts. It is formulated as follows: Problem 1. Self-supervised Expert Linking. Given a set of experts {e} in AMiner, we aim at learning an expert encoder g that can represent each expert e into a low-dimensional feature vector. Based on the feature vector g(e) for each epxert, we learn a metric function f : {g(e), g(e )} → {y} that can infer the alignment label y between any two experts e and e , where y is a binary value with y = 1 indicating e and e are equivalent and y = 0 otherwise. After pre-training, we fine-tune g and f on {e} and a set of external experts {ẽ} such that g and f are transferable to capture the representation patterns of ẽ and the matching patterns between ẽ and the corresponding expert e in AMiner.</p><p>As the problem is quite challenging, we assume any names from the external sources can be certainly aligned to an expert in AMiner. We leave the problem of non-existing alignment to the future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE COAD FRAMEWORK</head><p>This section introduces the proposed self-supervised expert linking model consisting of two sub modules, the contrastive pre-training module and the adversarial fine-tuning module. The former module pre-trains the expert encoder g and the metric function f purely on AMiner by contrastive learning, and the later module fine-tunes g and f on both AMiner and the external data by adversarial learning. Figure <ref type="figure" target="#fig_15">2</ref> illustrates the whole framework of COAD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Contrastive Pre-training Module</head><p>The pre-training module is to train an encoder to capture the universal representation patterns of experts. Based on the output of the encoder, it also learns a metric function to capture the universal matching patterns between experts. For this purpose, we need to design proper pre-training tasks. Instance discrimination in contrastive learning <ref type="bibr" target="#b38">[39]</ref> is a widely used pre-training task in various domains. For example, in CV, random image instances are often obtained by croping, resizing or rotating of an image, and two image instances are contrasted according to the images from which they are created <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b14">[15]</ref>. Similarly, in NLP, word instances are contrasted according to whether they belong to the same context or not <ref type="bibr" target="#b7">[8]</ref>. In graph representation, subgraphs are extracted from the neighbors of a node, and two sub graphs are contrasted according to their center nodes <ref type="bibr" target="#b29">[30]</ref> .</p><p>Inspired by the above success of contrastive learning in CV, NLP and GR, we define expert discrimination as our pre-training task to perform contrastive learning. Via expert discrimination, we expect the output functions to be able to not only capture the representation patterns of experts but also capture the matching patterns between two experts. To achieve this, four questions should be answered carefully:</p><p>• Q1: What is a random instance of an expert? • Q2: How to encode an expert instance?</p><p>• Q3: What is an effective way to measure the similarity between two expert instances? • Q4: Which kind of loss function should be selected? Q1: A Random Instance of an Expert. According to the definition of expert in AMiner, we define a random instance of an expert as a set of randomly sampled papers of the expert. Specifically, given the support information c e = {p 1 , • • • , p ne } of an expert e, a random instance I e of e can be formulated as follows:</p><formula xml:id="formula_13">I e = {p 1 , • • • , p L },<label>(1)</label></formula><p>where each p n ∈ c e and L is the maximal number of papers sampled for each expert instance. Two random instances sampled from the same expert are viewed as a positive pair, while the two sampled from different experts are treated as a negative pair.</p><p>Q2: BERT-based Expert Encoder. BERT has advanced the state-of-the-art in various NLP tasks by pre-training on large Wikipedia dumps. Since the support information of the external experts may be in different languages from the papers of the experts in AMiner, we leverage a pre-trained multi-lingual BERT model <ref type="bibr" target="#b37">[38]</ref>, which projects words or sentences from different languages into the same semantic space, to encode the experts from different sources. Straightforwardly, we can encode an expert by inputting all the tokens of its papers to BERT and taking the BERT output as the expert's representation. However, as many experts have a large number of papers, it is not feasible for BERT to accept so many tokens in all these papers simultaneously. In practice, we encode each paper as a basic representation unit of an expert instead of directly encoding each expert. Specifically, we concatenate the attributes of a paper, including title, keywords, venues, author names, author affiliations as the input of BERT and apply a Multilayer Perceptron (MLP) on the BERT output as the paper embedding:</p><formula xml:id="formula_14">p = g(p) = MLP(CLS(p)),<label>(2)</label></formula><p>where CLS(p) indicates the output CLS embedding of BERT.</p><p>Then we take the embedding p of each paper as the input of the following metric function. Essentially, we connect the BERT model with the following metric function as an end-to-end framework to fine-tune BERT and learn the parameters of the metric function together.</p><p>Q3: Metric Function. We propose an interaction-based metric function to measure the similarity between two expert instances. Before that, we also explain the representationbased metric function and the reasons for the proposal.</p><p>Representation-based Metric Function. Representationbased metric function aims to encode each expert into an embedding, and then directly estimate the distance between two experts in the vector space. Specifically, we average  </p><formula xml:id="formula_15">Expert encoder … p 1 p 2 p L … s 00 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l i u N h A E r O 5 i C f o h b J 8 K P / d p m J / Y = " &gt; A A A B 7 3 i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C b a C p 5 I t g h 4 L X j x W s B / Q L i W b Z t v Q J L s m W a E s / R N e P C j i 1 b / j z X 9 j 2 u 5 B W x 8 M P N 6 b Y W Z e m A h u L M b f X m F j c 2 t 7 p 7 h b 2 t s / O D w q H 5 + 0 T Z x q y l o 0 F r H u h s Q w w R V r W W 4 F 6 y a a E R k K 1 g k n t 3 O / 8 8 S 0 4 b F 6 s N O E B Z K M F I 8 4 J d Z J 3 a o Z Z B j P q o N y B d f w A m i d + D m p Q I 7 m o P z V H 8 Y 0 l U x Z K o g x P R 8 n N s i I t p w K N i v 1 U 8 M S Q i d k x H q O K i K Z C b L F v T N 0 4 Z Q h i m L t S l m 0 U H 9 P Z E Q a M 5 W h 6 5 T E j s 2 q N x f / 8 3 q p j W 6 C j K s k t U z R 5 a I o F c j G a P 4 8 G n L N q B V T R w j V 3 N 2 K 6 J h o Q q 2 L q O R C 8 F d f X i f t e s 3 H N f + + X m l c 5 X E U 4 Q z O 4 R J 8 u I Y G 3 E E T W k B B w D O 8 w p v 3 6 L 1 4 7 9 7 H s r X g 5 T O n 8 A f e 5 w / r z 4 8 s &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l i u N h A E r O 5 i C f o h b J 8 K P / d p m J / Y = " &gt; A A A B 7 3 i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C b a C p 5 I t g h 4 L X j x W s B / Q L i W b Z t v Q J L s m W a E s / R N e P C j i 1 b / j z X 9 j 2 u 5 B W x 8 M P N 6 b Y W Z e m A h u L M b f X m F j c 2 t 7 p 7 h b 2 t s / O D w q H 5 + 0 T Z x q y l o 0 F r H u h s Q w w R V r W W 4 F 6 y a a E R k K 1 g k n t 3 O / 8 8 S 0 4 b F 6 s N O E B Z K M F I 8 4 J d Z J 3 a o Z Z B j P q o N y B d f w A m i d + D m p Q I 7 m o P z V H 8 Y 0 l U x Z K o g x P R 8 n N s i I t p w K N i v 1 U 8 M S Q i d k x H q O K i K Z C b L F v T N 0 4 Z Q h i m L t S l m 0 U H 9 P Z E Q a M 5 W h 6 5 T E j s 2 q N x f / 8 3 q p j W 6 C j K s k t U z R 5 a I o F c j G a P 4 8 G n L N q B V T R w j V 3 N 2 K 6 J h o Q q 2 L q O R C 8 F d f X i f t e s 3 H N f + + X m l c 5 X E U 4 Q z O 4 R J 8 u I Y G 3 E E T W k B B w D O 8 w p v 3 6 L 1 4 7 9 7 H s r X g 5 T O n 8 A f e 5 w / r z 4 8 s &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l i u N h A E r O 5 i C f o h b J 8 K P / d p m J / Y = " &gt; A A A B 7 3 i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C b a C p 5 I t g h 4 L X j x W s B / Q L i W b Z t v Q J L s m W a E s / R N e P C j i 1 b / j z X 9 j 2 u 5 B W x 8 M P N 6 b Y W Z e m A h u L M b f X m F j c 2 t 7 p 7 h b 2 t s / O D w q H 5 + 0 T Z x q y l o 0 F r H u h s Q w w R V r W W 4 F 6 y a a E R k K 1 g k n t 3 O / 8 8 S 0 4 b F 6 s N O E B Z K M F I 8 4 J d Z J 3 a o Z Z B j P q o N y B d f w A m i d + D m p Q I 7 m o P z V H 8 Y 0 l U x Z K o g x P R 8 n N s i I t p w K N i v 1 U 8 M S Q i d k x H q O K i K Z C b L F v T N 0 4 Z Q h i m L t S l m 0 U H 9 P Z E Q a M 5 W h 6 5 T E j s 2 q N x f / 8 3 q p j W 6 C j K s k t U z R 5 a I o F c j G a P 4 8 G n L N q B V T R w j V 3 N 2 K 6 J h o Q q 2 L q O R C 8 F d f X i f t e s 3 H N f + + X m l c 5 X E U 4 Q z O 4 R J 8 u I Y G 3 E E T W k B B w D O 8 w p v 3 6 L 1 4 7 9 7 H s r X g 5 T O n 8 A f e 5 w / r z 4 8 s &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l i u N h A E r O 5 i C f o h b J 8 K P / d p m J / Y = " &gt; A A A B 7 3 i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C b a C p 5 I t g h 4 L X j x W s B / Q L i W b Z t v Q J L s m W a E s / R N e P C j i 1 b / j z X 9 j 2 u 5 B W x 8 M P N 6 b Y W Z e m A h u L M b f X m F j c 2 t 7 p 7 h b 2 t s / O D w q H 5 + 0 T Z x q y l o 0 F r H u h s Q w w R V r W W 4 F 6 y a a E R k K 1 g k n t 3 O / 8 8 S 0 4 b F 6 s N O E B Z K M F I 8 4 J d Z J 3 a o Z Z B j P q o N y B d f w A m i d + D m p Q I 7 m o P z V H 8 Y 0 l U x Z K o g x P R 8 n N s i I t p w K N i v 1 U 8 M S Q i d k x H q O K i K Z C b L F v T N 0 4 Z Q h i m L t S l m 0 U H 9 P Z E Q a M 5 W h 6 5 T E j s 2 q N x f / 8 3 q p j W 6 C j K s k t U z R 5 a I o F c j G a P 4 8 G n L N q B V T R w j V 3 N 2 K 6 J h o Q q 2 L q O R C 8 F d f X i f t e s 3 H N f + + X m l c 5 X E U 4 Q z O 4 R J 8 u I Y G 3 E E T W k B B w D O 8 w p v 3 6 L 1 4 7 9 7 H s r X g 5 T O n 8 A f e 5 w / r z 4 8 s &lt; / l a t e x i t &gt; s 01 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t 8 X V w I e W y Q N 1 j 8 w i d 1 5 W W V k f m 0 w = " &gt; A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L L a C p 5 I U Q Y 8 F L x 4 r 2 A 9 o Q 9 l s J + 3 S z S b u b o Q S + i e 8 e F D E q 3 / H m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 7 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p W 9 W D z P V m 1 U G 5 4 t b c B c g 6 8 X J S g R z N Q f m r P 4 x Z G q E 0 T F C t e 5 6 b G D + j y n A m c F b q p x o T y i Z 0 h D 1 L J Y 1 Q + 9 n i 3 h m 5 s M q Q h L G y J Q 1 Z q L 8 n M h p p P Y 0 C 2 x l R M 9 a r 3 l z 8 z + u l J r z x M y 6 T 1 K B k y 0 V h K o i J y f x 5 M u Q K m R F T S y h T 3 N 5 K 2 J g q y o y N q G R D 8 F Z f X i f t e s 1 z a 9 5 9 v d K 4 y u M o w h m c w y V 4 c A 0 N u I M m t I C B g G d 4 h T f n 0 X l x 3 p 2 P Z W v B y W d O 4 Q + c z x / t V Y 8 t &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t 8 X V w I e W y Q N 1 j 8 w i d 1 5 W W V k f m 0 w = " &gt; A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L L a C p 5 I U Q Y 8 F L x 4 r 2 A 9 o Q 9 l s J + 3 S z S b u b o Q S + i e 8 e F D E q 3 / H m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 7 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p W 9 W D z P V m 1 U G 5 4 t b c B c g 6 8 X J S g R z N Q f m r P 4 x Z G q E 0 T F C t e 5 6 b G D + j y n A m c F b q p x o T y i Z 0 h D 1 L J Y 1 Q + 9 n i 3 h m 5 s M q Q h L G y J Q 1 Z q L 8 n M h p p P Y 0 C 2 x l R M 9 a r 3 l z 8 z + u l J r z x M y 6 T 1 K B k y 0 V h K o i J y f x 5 M u Q K m R F T S y h T 3 N 5 K 2 J g q y o y N q G R D 8 F Z f X i f t e s 1 z a 9 5 9 v d K 4 y u M o w h m c w y V 4 c A 0 N u I M m t I C B g G d 4 h T f n 0 X l x 3 p 2 P Z W v B y W d O 4 Q + c z x / t V Y 8 t &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t 8 X V w I e W y Q N 1 j 8 w i d 1 5 W W V k f m 0 w = " &gt; A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L L a C p 5 I U Q Y 8 F L x 4 r 2 A 9 o Q 9 l s J + 3 S z S b u b o Q S + i e 8 e F D E q 3 / H m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 7 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p W 9 W D z P V m 1 U G 5 4 t b c B c g 6 8 X J S g R z N Q f m r P 4 x Z G q E 0 T F C t e 5 6 b G D + j y n A m c F b q p x o T y i Z 0 h D 1 L J Y 1 Q + 9 n i 3 h m 5 s M q Q h L G y J Q 1 Z q L 8 n M h p p P Y 0 C 2 x l R M 9 a r 3 l z 8 z + u l J r z x M y 6 T 1 K B k y 0 V h K o i J y f x 5 M u Q K m R F T S y h T 3 N 5 K 2 J g q y o y N q G R D 8 F Z f X i f t e s 1 z a 9 5 9 v d K 4 y u M o w h m c w y V 4 c A 0 N u I M m t I C B g G d 4 h T f n 0 X l x 3 p 2 P Z W v B y W d O 4 Q + c z x / t V Y 8 t &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t 8 X V w I e W y Q N 1 j 8 w i d 1 5 W W V k f m 0 w = " &gt; A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L L a C p 5 I U Q Y 8 F L x 4 r 2 A 9 o Q 9 l s J + 3 S z S b u b o Q S + i e 8 e F D E q 3 / H m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 7 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p W 9 W D z P V m 1 U G 5 4 t b c B c g 6 8 X J S g R z N Q f m r P 4 x Z G q E 0 T F C t e 5 6 b G D + j y n A m c F b q p x o T y i Z 0 h D 1 L J Y 1 Q + 9 n i 3 h m 5 s M q Q h L G y J Q 1 Z q L 8 n M h p p P Y 0 C 2 x l R M 9 a r 3 l z 8 z + u l J r z x M y 6 T 1 K B k y 0 V h K o i J y f x 5 M u Q K m R F T S y h T 3 N 5 K 2 J g q y o y N q G R D 8 F Z f X i f t e s 1 z a 9 5 9 v d K 4 y u M o w h m c w y V 4 c A 0 N u I M m t I C B g G d 4 h T f n 0 X l x 3 p 2 P Z W v B y W d O 4 Q + c z x / t V Y 8 t &lt; / l a t e x i t &gt; s 0L &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " h F E W L 8 h + g K J g N d a j a x O N U s x B F v 4 = " &gt; A A A B 7 3 i c b V A 9 S w N B E J 3 z M 8 a v q K X N Y i J Y h b s g a B m w s b C I Y D 4 g O c L e Z i 9 Z s r d 3 7 s 4 J 4 c i f s L F Q x N a / Y + e / c Z N c o Y k P B h 7 v z T A z L 0 i k M O i 6 3 8 7 a + s b m 1 n Z h p 7 i 7 t 3 9 w W D o 6 b p k 4 1 Y w 3 W S x j 3 Q m o 4 V I o 3 k S B k n c S z W k U S N 4 O x j c z v / 3 E t R G x e s B J w v 2 I D p U I B a N o p U 7 F 9 D P 3 b l r p l 8 p u 1 Z 2 D r B I v J 2 X I 0 e i X v n q D m K U R V 8 g k N a b r u Q n 6 G d U o m O T T Y i 8 1 P K F s T I e 8 a 6 m i E T d + N r 9 3 S s 6 t M i B h r G 0 p J H P 1 9 0 R G I 2 M m U W A 7 I 4 o j s + z N x P + 8 b o r h t Z 8 J l a T I F V s s C l N J M C a z 5 8 l A a M 5 Q T i y h T A t 7 K 2 E j q i l D G 1 H R h u A t v 7 x K W r W q 5 1 a 9 + 1 q 5 f p n H U Y B T O I M L 8 O A K 6 n A L D W g C A w n P 8 A p v z q P z 4 r w 7 H 4 v W N S e f O Y E / c D 5 / A B a G j 0 g = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " h F E W L 8 h + g K J g N d a j a x O N U s x B F v 4 = " &gt; A A A B 7 3 i c b V A 9 S w N B E J 3 z M 8 a v q K X N Y i J Y h b s g a B m w s b C I Y D 4 g O c L e Z i 9 Z s r d 3 7 s 4 J 4 c i f s L F Q x N a / Y + e / c Z N c o Y k P B h 7 v z T A z L 0 i k M O i 6 3 8 7 a + s b m 1 n Z h p 7 i 7 t 3 9 w W D o 6 b p k 4 1 Y w 3 W S x j 3 Q m o 4 V I o 3 k S B k n c S z W k U S N 4 O x j c z v / 3 E t R G x e s B J w v 2 I D p U I B a N o p U 7 F 9 D P 3 b l r p l 8 p u 1 Z 2 D r B I v J 2 X I 0 e i X v n q D m K U R V 8 g k N a b r u Q n 6 G d U o m O T T Y i 8 1 P K F s T I e 8 a 6 m i E T d + N r 9 3 S s 6 t M i B h r G 0 p J H P 1 9 0 R G I 2 M m U W A 7 I 4 o j s + z N x P + 8 b o r h t Z 8 J l a T I F V s s C l N J M C a z 5 8 l A a M 5 Q T i y h T A t 7 K 2 E j q i l D G 1 H R h u A t v 7 x K W r W q 5 1 a 9 + 1 q 5 f p n H U Y B T O I M L 8 O A K 6 n A L D W g C A w n P 8 A p v z q P z 4 r w 7 H 4 v W N S e f O Y E / c D 5 / A B a G j 0 g = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " h F E W L 8 h + g K J g N d a j a x O N U s x B F v 4 = " &gt; A A A B 7 3 i c b V A 9 S w N B E J 3 z M 8 a v q K X N Y i J Y h b s g a B m w s b C I Y D 4 g O c L e Z i 9 Z s r d 3 7 s 4 J 4 c i f s L F Q x N a / Y + e / c Z N c o Y k P B h 7 v z T A z L 0 i k M O i 6 3 8 7 a + s b m 1 n Z h p 7 i 7 t 3 9 w W D o 6 b p k 4 1 Y w 3 W S x j 3 Q m o 4 V I o 3 k S B k n c S z W k U S N 4 O x j c z v / 3 E t R G x e s B J w v 2 I D p U I B a N o p U 7 F 9 D P 3 b l r p l 8 p u 1 Z 2 D r B I v J 2 X I 0 e i X v n q D m K U R V 8 g k N a b r u Q n 6 G d U o m O T T Y i 8 1 P K F s T I e 8 a 6 m i E T d + N r 9 3 S s 6 t M i B h r G 0 p J H P 1 9 0 R G I 2 M m U W A 7 I 4 o j s + z N x P + 8 b o r h t Z 8 J l a T I F V s s C l N J M C a z 5 8 l A a M 5 Q T i y h T A t 7 K 2 E j q i l D G 1 H R h u A t v 7 x K W r W q 5 1 a 9 + 1 q 5 f p n H U Y B T O I M L 8 O A K 6 n A L D W g C A w n P 8 A p v z q P z 4 r w 7 H 4 v W N S e f O Y E / c D 5 / A B a G j 0 g = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " h F E W L 8 h + g K J g N d a j a x O N U s x B F v 4 = " &gt; A A A B 7 3 i c b V A 9 S w N B E J 3 z M 8 a v q K X N Y i J Y h b s g a B m w s b C I Y D 4 g O c L e Z i 9 Z s r d 3 7 s 4 J 4 c i f s L F Q x N a / Y + e / c Z N c o Y k P B h 7 v z T A z L 0 i k M O i 6 3 8 7 a + s b m 1 n Z h p 7 i 7 t 3 9 w W D o 6 b p k 4 1 Y w 3 W S x j 3 Q m o 4 V I o 3 k S B k n c S z W k U S N 4 O x j c z v / 3 E t R G x e s B J w v 2 I D p U I B a N o p U 7 F 9 D P 3 b l r p l 8 p u 1 Z 2 D r B I v J 2 X I 0 e i X v n q D m K U R V 8 g k N a b r u Q n 6 G d U o m O T T Y i 8 1 P K F s T I e 8 a 6 m i E T d + N r 9 3 S s 6 t M i B h r G 0 p J H P 1 9 0 R G I 2 M m U W A 7 I 4 o j s + z N x P + 8 b o r h t Z 8 J l a T I F V s s C l N J M C a z 5 8 l A a M 5 Q T i y h T A t 7 K 2 E j q i l D G 1 H R h u A t v 7 x K W r W q 5 1 a 9 + 1 q 5 f p n H U Y B T O I M L 8 O A K 6 n A L D W g C A w n P 8 A p v z q P z 4 r w 7 H 4 v W N S e f O Y E / c D 5 / A B a G j 0 g = &lt; / l a t e x i t &gt; s 10 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B 4 R V + m x W d h h 1 b 3 B E 0 G / i T 8 B A A N M = " &gt; A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L L a C p 5 I U Q Y 8 F L x 4 r 2 A 9 o Q 9 l s J + 3 S z S b u b o Q S + i e 8 e F D E q 3 / H m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 7 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p W 9 W D z H N n 1 U G 5 4 t b c B c g 6 8 X J S g R z N Q f m r P 4 x Z G q E 0 T F C t e 5 6 b G D + j y n A m c F b q p x o T y i Z 0 h D 1 L J Y 1 Q + 9 n i 3 h m 5 s M q Q h L G y J Q 1 Z q L 8 n M h p p P Y 0 C 2 x l R M 9 a r 3 l z 8 z + u l J r z x M y 6 T 1 K B k y 0 V h K o i J y f x 5 M u Q K m R F T S y h T 3 N 5 K 2 J g q y o y N q G R D 8 F Z f X i f t e s 1 z a 9 5 9 v d K 4 y u M o w h m c w y V 4 c A 0 N u I M m t I C B g G d 4 h T f n 0 X l x 3 p 2 P Z W v B y W d O 4 Q + c z x / t V o 8 t &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B 4 R V + m x W d h h 1 b 3 B E 0 G / i T 8 B A A N M = " &gt; A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L L a C p 5 I U Q Y 8 F L x 4 r 2 A 9 o Q 9 l s J + 3 S z S b u b o Q S + i e 8 e F D E q 3 / H m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 7 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p W 9 W D z H N n 1 U G 5 4 t b c B c g 6 8 X J S g R z N Q f m r P 4 x Z G q E 0 T F C t e 5 6 b G D + j y n A m c F b q p x o T y i Z 0 h D 1 L J Y 1 Q + 9 n i 3 h m 5 s M q Q h L G y J Q 1 Z q L 8 n M h p p P Y 0 C 2 x l R M 9 a r 3 l z 8 z + u l J r z x M y 6 T 1 K B k y 0 V h K o i J y f x 5 M u Q K m R F T S y h T 3 N 5 K 2 J g q y o y N q G R D 8 F Z f X i f t e s 1 z a 9 5 9 v d K 4 y u M o w h m c w y V 4 c A 0 N u I M m t I C B g G d 4 h T f n 0 X l x 3 p 2 P Z W v B y W d O 4 Q + c z x / t V o 8 t &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B 4 R V + m x W d h h 1 b 3 B E 0 G / i T 8 B A A N M = " &gt; A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L L a C p 5 I U Q Y 8 F L x 4 r 2 A 9 o Q 9 l s J + 3 S z S b u b o Q S + i e 8 e F D E q 3 / H m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 7 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p W 9 W D z H N n 1 U G 5 4 t b c B c g 6 8 X J S g R z N Q f m r P 4 x Z G q E 0 T F C t e 5 6 b G D + j y n A m c F b q p x o T y i Z 0 h D 1 L J Y 1 Q + 9 n i 3 h m 5 s M q Q h L G y J Q 1 Z q L 8 n M h p p P Y 0 C 2 x l R M 9 a r 3 l z 8 z + u l J r z x M y 6 T 1 K B k y 0 V h K o i J y f x 5 M u Q K m R F T S y h T 3 N 5 K 2 J g q y o y N q G R D 8 F Z f X i f t e s 1 z a 9 5 9 v d K 4 y u M o w h m c w y V 4 c A 0 N u I M m t I C B g G d 4 h T f n 0 X l x 3 p 2 P Z W v B y W d O 4 Q + c z x / t V o 8 t &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B 4 R V + m x W d h h 1 b 3 B E 0 G / i T 8 B A A N M = " &gt; A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L L a C p 5 I U Q Y 8 F L x 4 r 2 A 9 o Q 9 l s J + 3 S z S b u b o Q S + i e 8 e F D E q 3 / H m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 7 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p W 9 W D z H N n 1 U G 5 4 t b c B c g 6 8 X J S g R z N Q f m r P 4 x Z G q E 0 T F C t e 5 6 b G D + j y n A m c F b q p x o T y i Z 0 h D 1 L J Y 1 Q + 9 n i 3 h m 5 s M q Q h L G y J Q 1 Z q L 8 n M h p p P Y 0 C 2 x l R M 9 a r 3 l z 8 z + u l J r z x M y 6 T 1 K B k y 0 V h K o i J y f x 5 M u Q K m R F T S y h T 3 N 5 K 2 J g q y o y N q G R D 8 F Z f X i f t e s 1 z a 9 5 9 v d K 4 y u M o w h m c w y V 4 c A 0 N u I M m t I C B g G d 4 h T f n 0 X l x 3 p 2 P Z W v B y W d O 4 Q + c z x / t V o 8 t &lt; / l a t e x i t &gt; s 11 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E V X S L j B w Y M 6 o v Q i Z 1 u d E Y h 4 i j O U = " &gt; A A A B 7 3 i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C b a C p 7 I p g h 4 L X j x W s B / Q L i W b Z t v Q b L I m W a E s / R N e P C j i 1 b / j z X 9 j 2 u 5 B W x 8 M P N 6 b Y W Z e m A h u r O 9 / e 4 W N z a 3 t n e J u a W / / 4 P C o f H z S N i r V l L W o E k p 3 Q 2 K Y 4 J K 1 L L e C d R P N S B w K 1 g k n t 3 O / 8 8 S 0 4 U o + 2 G n C g p i M J I 8 4 J d Z J 3 a o Z Z B j P q o N y x a / 5 C 6 B 1 g n N S g R z N Q f m r P 1 Q 0 j Z m 0 V B B j e t h P b J A R b T k V b F b q p 4 Y l h E 7 I i P U c l S R m J s g W 9 8 7 Q h V O G K F L a l b R o o f 6 e y E h s z D Q O X W d M 7 N i s e n P x P 6 + X 2 u g m y L h M U s s k X S 6 K U o G s Q v P n 0 Z B r R q 2 Y O k K o 5 u 5 W R M d E E 2 p d R C U X A l 5 9 e Z 2 0 6 z X s 1 / B 9 v d K 4 y u M o w h m c w y V g u I Y G 3 E E T W k B B w D O 8 w p v 3 6 L 1 4 7 9 7 H s r X g 5 T O n 8 A f e 5 w / u 3 I 8 u &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E V X S L j B w Y M 6 o v Q i Z 1 u d E Y h 4 i j O U = " &gt; A A A B 7 3 i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C b a C p 7 I p g h 4 L X j x W s B / Q L i W b Z t v Q b L I m W a E s / R N e P C j i 1 b / j z X 9 j 2 u 5 B W x 8 M P N 6 b Y W Z e m A h u r O 9 / e 4 W N z a 3 t n e J u a W / / 4 P C o f H z S N i r V l L W o E k p 3 Q 2 K Y 4 J K 1 L L e C d R P N S B w K 1 g k n t 3 O / 8 8 S 0 4 U o + 2 G n C g p i M J I 8 4 J d Z J 3 a o Z Z B j P q o N y x a / 5 C 6 B 1 g n N S g R z N Q f m r P 1 Q 0 j Z m 0 V B B j e t h P b J A R b T k V b F b q p 4 Y l h E 7 I i P U c l S R m J s g W 9 8 7 Q h V O G K F L a l b R o o f 6 e y E h s z D Q O X W d M 7 N i s e n P x P 6 + X 2 u g m y L h M U s s k X S 6 K U o G s Q v P n 0 Z B r R q 2 Y O k K o 5 u 5 W R M d E E 2 p d R C U X A l 5 9 e Z 2 0 6 z X s 1 / B 9 v d K 4 y u M o w h m c w y V g u I Y G 3 E E T W k B B w D O 8 w p v 3 6 L 1 4 7 9 7 H s r X g 5 T O n 8 A f e 5 w / u 3 I 8 u &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E V X S L j B w Y M 6 o v Q i Z 1 u d E Y h 4 i j O U = " &gt; A A A B 7 3 i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C b a C p 7 I p g h 4 L X j x W s B / Q L i W b Z t v Q b L I m W a E s / R N e P C j i 1 b / j z X 9 j 2 u 5 B W x 8 M P N 6 b Y W Z e m A h u r O 9 / e 4 W N z a 3 t n e J u a W / / 4 P C o f H z S N i r V l L W o E k p 3 Q 2 K Y 4 J K 1 L L e C d R P N S B w K 1 g k n t 3 O / 8 8 S 0 4 U o + 2 G n C g p i M J I 8 4 J d Z J 3 a o Z Z B j P q o N y x a / 5 C 6 B 1 g n N S g R z N Q f m r P 1 Q 0 j Z m 0 V B B j e t h P b J A R b T k V b F b q p 4 Y l h E 7 I i P U c l S R m J s g W 9 8 7 Q h V O G K F L a l b R o o f 6 e y E h s z D Q O X W d M 7 N i s e n P x P 6 + X 2 u g m y L h M U s s k X S 6 K U o G s Q v P n 0 Z B r R q 2 Y O k K o 5 u 5 W R M d E E 2 p d R C U X A l 5 9 e Z 2 0 6 z X s 1 / B 9 v d K 4 y u M o w h m c w y V g u I Y G 3 E E T W k B B w D O 8 w p v 3 6 L 1 4 7 9 7 H s r X g 5 T O n 8 A f e 5 w / u 3 I 8 u &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E V X S L j B w Y M 6 o v Q i Z 1 u d E Y h 4 i j O U = " &gt; A A A B 7 3 i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C b a C p 7 I p g h 4 L X j x W s B / Q L i W b Z t v Q b L I m W a E s / R N e P C j i 1 b / j z X 9 j 2 u 5 B W x 8 M P N 6 b Y W Z e m A h u r O 9 / e 4 W N z a 3 t n e J u a W / / 4 P C o f H z S N i r V l L W o E k p 3 Q 2 K Y 4 J K 1 L L e C d R P N S B w K 1 g k n t 3 O / 8 8 S 0 4 U o + 2 G n C g p i M J I 8 4 J d Z J 3 a o Z Z B j P q o N y x a / 5 C 6 B 1 g n N S g R z N Q f m r P 1 Q 0 j Z m 0 V B B j e t h P b J A R b T k V b F b q p 4 Y l h E 7 I i P U c l S R m J s g W 9 8 7 Q h V O G K F L a l b R o o f 6 e y E h s z D Q O X W d M 7 N i s e n P x P 6 + X 2 u g m y L h M U s s k X S 6 K U o G s Q v P n 0 Z B r R q 2 Y O k K o 5 u 5 W R M d E E 2 p d R C U X A l 5 9 e Z 2 0 6 z X s 1 / B 9 v d K 4 y u M o w h m c w y V g u I Y G 3 E E T W k B B</formula><formula xml:id="formula_16">F d F c l E M K 6 Y F L g P K 8 Q 6 1 u S b M c t a U = " &gt; A A A B 7 3 i c b V A 9 S w N B E J 3 z M 8 a v q K X N Y i J Y h b s g a B m w s b C I Y D 4 g O c L e Z i 5 Z s r d 3 7 u 4 J 4 c i f s L F Q x N a / Y + e / c Z N c o Y k P B h 7 v z T A z L 0 g E 1 8 Z 1 v 5 2 1 9 Y 3 N r e 3 C T n F 3 b / / g s H R 0 3 N J x q h g 2 W S x i 1 Q m o R s E l N g 0 3 A j u J Q h o F A t v B + G b m t 5 9 Q a R 7 L B z N J 0 I / o U P K Q M 2 q s 1 K n o f u b d T S v 9 U t m t u n O Q V e L l p A w 5 G v 3 S V 2 8 Q s z R C a Z i g W n c 9 N z F + R p X h T O C 0 2 E s 1 J p S N 6 R C 7 l k o a o f a z + b 1 T c m 6 V A Q l j Z U s a M l d / T 2 Q 0 0 n o S B b Y z o m a k l 7 2 Z + J / X T U 1 4 7 W d c J q l B y R a L w l Q Q E 5 P Z 8 2 T A F T I j J p Z Q p r i 9 l b A R V Z Q Z G 1 H R h u A t v 7 x K W r W q 5 1 a 9 + 1 q 5 f p n H U Y B T O I M L 8 O A K 6 n A L D W g C A w H P 8 A p v z q P z 4 r w 7 H 4 v W N S e f O Y E / c D 5 / A B g N j 0 k = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F d F c l E M K 6 Y F L g P K 8 Q 6 1 u S b M c t a U = " &gt; A A A B 7 3 i c b V A 9 S w N B E J 3 z M 8 a v q K X N Y i J Y h b s g a B m w s b C I Y D 4 g O c L e Z i 5 Z s r d 3 7 u 4 J 4 c i f s L F Q x N a / Y + e / c Z N c o Y k P B h 7 v z T A z L 0 g E 1 8 Z 1 v 5 2 1 9 Y 3 N r e 3 C T n F 3 b / / g s H R 0 3 N J x q h g 2 W S x i 1 Q m o R s E l N g 0 3 A j u J Q h o F A t v B + G b m t 5 9 Q a R 7 L B z N J 0 I / o U P K Q M 2 q s 1 K n o f u b d T S v 9 U t m t u n O Q V e L l p A w 5 G v 3 S V 2 8 Q s z R C a Z i g W n c 9 N z F + R p X h T O C 0 2 E s 1 J p S N 6 R C 7 l k o a o f a z + b 1 T c m 6 V A Q l j Z U s a M l d / T 2 Q 0 0 n o S B b Y z o m a k l 7 2 Z + J / X T U 1 4 7 W d c J q l B y R a L w l Q Q E 5 P Z 8 2 T A F T I j J p Z Q p r i 9 l b A R V Z Q Z G 1 H R h u A t v 7 x K W r W q 5 1 a 9 + 1 q 5 f p n H U Y B T O I M L 8 O A K 6 n A L D W g C A w H P 8 A p v z q P z 4 r w 7 H 4 v W N S e f O Y E / c D 5 / A B g N j 0 k = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F d F c l E M K 6 Y F L g P K 8 Q 6 1 u S b M c t a U = " &gt; A A A B 7 3 i c b V A 9 S w N B E J 3 z M 8 a v q K X N Y i J Y h b s g a B m w s b C I Y D 4 g O c L e Z i 5 Z s r d 3 7 u 4 J 4 c i f s L F Q x N a / Y + e / c Z N c o Y k P B h 7 v z T A z L 0 g E 1 8 Z 1 v 5 2 1 9 Y 3 N r e 3 C T n F 3 b / / g s H R 0 3 N J x q h g 2 W S x i 1 Q m o R s E l N g 0 3 A j u J Q h o F A t v B + G b m t 5 9 Q a R 7 L B z N J 0 I / o U P K Q M 2 q s 1 K n o f u b d T S v 9 U t m t u n O Q V e L l p A w 5 G v 3 S V 2 8 Q s z R C a Z i g W n c 9 N z F + R p X h T O C 0 2 E s 1 J p S N 6 R C 7 l k o a o f a z + b 1 T c m 6 V A Q l j Z U s a M l d / T 2 Q 0 0 n o S B b Y z o m a k l 7 2 Z + J / X T U 1 4 7 W d c J q l B y R a L w l Q Q E 5 P Z 8 2 T A F T I j J p Z Q p r i 9 l b A R V Z Q Z G 1 H R h u A t v 7 x K W r W q 5 1 a 9 + 1 q 5 f p n H U Y B T O I M L 8 O A K 6 n A L D W g C A w H P 8 A p v z q P z 4 r w 7 H 4 v W N S e f O Y E / c D 5 / A B g N j 0 k = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F d F c l E M K 6 Y F L g P K 8 Q 6 1 u S b M c t a U = " &gt; A A A B 7 3 i c b V A 9 S w N B E J 3 z M 8 a v q K X N Y i J Y h b s g a B m w s b C I Y D 4 g O c L e Z i 5 Z s r d 3 7 u 4 J 4 c i f s L F Q x N a / Y + e / c Z N c o Y k P B h 7 v z T A z L 0 g E 1 8 Z 1 v 5 2 1 9 Y 3 N r e 3 C T n F 3 b / / g s H R 0 3 N J x q h g 2 W S x i 1 Q m o R s E l N g 0 3 A j u J Q h o F A t v B + G b m t 5 9 Q a R 7 L B z N J 0 I / o U P K Q M 2 q s 1 K n o f u b d T S v 9 U t m t u n O Q V e L l p A w 5 G v 3 S V 2 8 Q s z R C a Z i g W n c 9 N z F + R p X h T O C 0 2 E s 1 J p S N 6 R C 7 l k o a o f a z + b 1 T c m 6 V A Q l j Z U s a M l d / T 2 Q 0 0 n o S B b Y z o m a k l 7 2 Z + J / X T U 1 4 7 W d c J q l B y R a L w l Q Q E 5 P Z 8 2 T A F T I j J p Z Q p r i 9 l b A R V Z Q Z G 1 H R h u A t v 7 x K W r W q 5 1 a 9 + 1 q 5 f p n H U Y B T O I M L 8 O A K 6 n A L D W g C A</formula><formula xml:id="formula_17">T i y h T A t 7 K 2 E j q i l D G 1 H R h u A t v 7 x K W r W q 5 1 a 9 + 1 q 5 f p n H U Y B T O I M L 8 O A K 6 n A L D W g C A w n P 8 A p v z q P z 4 r w 7 H 4 v W N S e f O Y E / c D 5 / A B a i j 0 g = &lt; / l a t e x i t &gt; s L1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C u o G M K U q M t M Q J z 6 p B W g m q o p N v V g = " &gt; A A A B 7 3 i c b V A 9 S w N B E J 3 z M 8 a v q K X N Y i J Y h b s g a B m w s b C I Y D 4 g O c L e Z i 5 Z s r d 3 7 u 4 J 4 c i f s L F Q x N a / Y + e / c Z N c o Y k P B h 7 v z T A z L 0 g E 1 8 Z 1 v 5 2 1 9 Y 3 N r e 3 C T n F 3 b / / g s H R 0 3 N J x q h g 2 W S x i 1 Q m o R s E l N g 0 3 A j u J Q h o F A t v B + G b m t 5 9 Q a R 7 L B z N J 0 I / o U P K Q M 2 q s 1 K n o f n b n T S v 9 U t m t u n O Q V e L l p A w 5 G v 3 S V 2 8 Q s z R C a Z i g W n c 9 N z F + R p X h T O C 0 2 E s 1 J p S N 6 R C 7 l k o a o f a z + b 1 T c m 6 V A Q l j Z U s a M l d / T 2 Q 0 0 n o S B b Y z o m a k l 7 2 Z + J / X T U 1 4 7 W d c J q l B y R a L w l Q Q E 5 P Z 8 2 T A F T I j J p Z Q p r i 9 l b A R V Z Q Z G 1 H R h u A t v 7 x K W r W q 5 1 a 9 + 1 q 5 f p n H U Y B T O I M L 8 O A K 6 n A L D W g C A w H P 8 A p v z q P z 4 r w 7 H 4 v W N S e f O Y E / c D 5 / A B g o j 0 k = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C u o G M K U q M t M Q J z 6 p B W g m q o p N v V g = " &gt; A A A B 7 3 i c b V A 9 S w N B E J 3 z M 8 a v q K X N Y i J Y h b s g a B m w s b C I Y D 4 g O c L e Z i 5 Z s r d 3 7 u 4 J 4 c i f s L F Q x N a / Y + e / c Z N c o Y k P B h 7 v z T A z L 0 g E 1 8 Z 1 v 5 2 1 9 Y 3 N r e 3 C T n F 3 b / / g s H R 0 3 N J x q h g 2 W S x i 1 Q m o R s E l N g 0 3 A j u J Q h o F A t v B + G b m t 5 9 Q a R 7 L B z N J 0 I / o U P K Q M 2 q s 1 K n o f n b n T S v 9 U t m t u n O Q V e L l p A w 5 G v 3 S V 2 8 Q s z R C a Z i g W n c 9 N z F + R p X h T O C 0 2 E s 1 J p S N 6 R C 7 l k o a o f a z + b 1 T c m 6 V A Q l j Z U s a M l d / T 2 Q 0 0 n o S B b Y z o m a k l 7 2 Z + J / X T U 1 4 7 W d c J q l B y R a L w l Q Q E 5 P Z 8 2 T A F T I j J p Z Q p r i 9 l b A R V Z Q Z G 1 H R h u A t v 7 x K W r W q 5 1 a 9 + 1 q 5 f p n H U Y B T O I M L 8 O A K 6 n A L D W g C A w H P 8 A p v z q P z 4 r w 7 H 4 v W N S e f O Y E / c D 5 / A B g o j 0 k = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C u o G M K U q M t M Q J z 6 p B W g m q o p N v V g = " &gt; A A A B 7 3 i c b V A 9 S w N B E J 3 z M 8 a v q K X N Y i J Y h b s g a B m w s b C I Y D 4 g O c L e Z i 5 Z s r d 3 7 u 4 J 4 c i f s L F Q x N a / Y + e / c Z N c o Y k P B h 7 v z T A z L 0 g E 1 8 Z 1 v 5 2 1 9 Y 3 N r e 3 C T n F 3 b / / g s H R 0 3 N J x q h g 2 W S x i 1 Q m o R s E l N g 0 3 A j u J Q h o F A t v B + G b m t 5 9 Q a R 7 L B z N J 0 I / o U P K Q M 2 q s 1 K n o f n b n T S v 9 U t m t u n O Q V e L l p A w 5 G v 3 S V 2 8 Q s z R C a Z i g W n c 9 N z F + R p X h T O C 0 2 E s 1 J p S N 6 R C 7 l k o a o f a z + b 1 T c m 6 V A Q l j Z U s a M l d / T 2 Q 0 0 n o S B b Y z o m a k l 7 2 Z + J / X T U 1 4 7 W d c J q l B y R a L w l Q Q E 5 P Z 8 2 T A F T I j J p Z Q p r i 9 l b A R V Z Q Z G 1 H R h u A t v 7 x K W r W q 5 1 a 9 + 1 q 5 f p n H U Y B T O I M L 8 O A K 6 n A L D W g C A w H P 8 A p v z q P z 4 r w 7 H 4 v W N S e f O Y E / c D 5 / A B g o j 0 k = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C u o G M K U q M t M Q J z 6 p B W g m q o p N v V g = " &gt; A A A B 7 3 i c b V A 9 S w N B E J 3 z M 8 a v q K X N Y i J Y h b s g a B m w s b C I Y D 4 g O c L e Z i 5 Z s r d 3 7 u 4 J 4 c i f s L F Q x N a / Y + e / c Z N c o Y k P B h 7 v z T A z L 0 g E 1 8 Z 1 v 5 2 1 9 Y 3 N r e 3 C T n F 3 b / / g s H R 0 3 N J x q h g 2 W S x i 1 Q m o R s E l N g 0 3 A j u J Q h o F A t v B + G b m t 5 9 Q a R 7 L B z N J 0 I / o U P K Q M 2 q s 1 K n o f n b n T S v 9 U t m t u n O Q V e L l p A w 5 G v 3 S V 2 8 Q s z R C a Z i g W n c 9 N z F + R p X h T O C 0 2 E s 1 J p S N 6 R C 7 l k o a o f a z + b 1 T c m 6 V A Q l j Z U s a M l d / T 2 Q 0 0 n o S B b Y z o m a k l 7 2 Z + J / X T U 1 4 7 W d c J q l B y R a L w l Q Q E 5 P Z 8 2 T A F T I j J p Z Q p r i 9 l b A R V Z Q Z G 1 H R h u A t v 7 x K W r W q 5 1 a 9 + 1 q 5 f p n H U Y B T O I M L 8 O A K 6 n A L D W g C A w H P 8 A p v z q P z 4 r w 7 H 4 v W N S e f O Y E / c D 5 / A B g o j 0 k = &lt; / l a t e x i t &gt; s LL &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N t + W u v 1 Q o N 5 o t w h t A 4 j w p D d G 5 k 0 = " &gt; A A A B 7 3 i c b V A 9 S w N B E J 2 L X z F + R S 1 t F h P B K t w F Q c u A j U W K C O Y D k i P s b T b J k r 2 9 c 3 d O C E f + h I 2 F I r b + H T v / j Z v k C k 1 8 M P B 4 b 4 a Z e U E s h U H X / X Z y G 5 t b 2 z v 5 3 c L e / s H h U f H 4 p G W i R D P e Z J G M d C e g h k u h e B M F S t 6 J N a d h I H k 7 m N z O / f Y T 1 0 Z E 6 g G n M f d D O l J i K B h F K 3 X K p p / W 6 7 N y v 1 h y K + 4 C Z J 1 4 G S l B h k a / + N U b R C w J u U I m q T F d z 4 3 R T 6 l G w S S f F X q J 4 T F l E z r i X U s V D b n x 0 8 W 9 M 3 J h l Q E Z R t q W Q r J Q f 0 + k N D R m G g a 2 M 6 Q 4 N q v e X P z P 6 y Y 4 v P F T o e I E u W L L R c N E E o z I / H k y E J o z l F N L K N P C 3 k r Y m G r K 0 E Z U s C F 4 q y + v k 1 a 1 4 r k V 7 7 5 a q l 1 l c e T h D M 7 h E j y 4 h h r c Q Q O a w E D C M 7 z C m / P o v D j v z s e y N e d k M 6 f w B 8 7 n D 0 F K j 2 Q = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N t + W u v 1 Q o N 5 o t w h t A 4 j w p D d G 5 k 0 = " &gt; A A A B 7 3 i c b V A 9 S w N B E J 2 L X z F + R S 1 t F h P B K t w F Q c u A j U W K C O Y D k i P s b T b J k r 2 9 c 3 d O C E f + h I 2 F I r b + H T v / j Z v k C k 1 8 M P B 4 b 4 a Z e U E s h U H X / X Z y G 5 t b 2 z v 5 3 c L e / s H h U f H 4 p G W i R D P e Z J G M d C e g h k u h e B M F S t 6 J N a d h I H k 7 m N z O / f Y T 1 0 Z E 6 g G n M f d D O l J i K B h F K 3 X K p p / W 6 7 N y v 1 h y K + 4 C Z J 1 4 G S l B h k a / + N U b R C w J u U I m q T F d z 4 3 R T 6 l G w S S f F X q J 4 T F l E z r i X U s V D b n x 0 8 W 9 M 3 J h l Q E Z R t q W Q r J Q f 0 + k N D R m G g a 2 M 6 Q 4 N q v e X P z P 6 y Y 4 v P F T o e I E u W L L R c N E E o z I / H k y E J o z l F N L K N P C 3 k r Y m G r K 0 E Z U s C F 4 q y + v k 1 a 1 4 r k V 7 7 5 a q l 1 l c e T h D M 7 h E j y 4 h h r c Q Q O a w E D C M 7 z C m / P o v D j v z s e y N e d k M 6 f w B 8 7 n D 0 F K j 2 Q = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N t + W u v 1 Q o N 5 o t w h t A 4 j w p D d G 5 k 0 = " &gt; A A A B 7 3 i c b V A 9 S w N B E J 2 L X z F + R S 1 t F h P B K t w F Q c u A j U W K C O Y D k i P s b T b J k r 2 9 c 3 d O C E f + h I 2 F I r b + H T v / j Z v k C k 1 8 M P B 4 b 4 a Z e U E s h U H X / X Z y G 5 t b 2 z v 5 3 c L e / s H h U f H 4 p G W i R D P e Z J G M d C e g h k u h e B M F S t 6 J N a d h I H k 7 m N z O / f Y T 1 0 Z E 6 g G n M f d D O l J i K B h F K 3 X K p p / W 6 7 N y v 1 h y K + 4 C Z J 1 4 G S l B h k a / + N U b R C w J u U I m q T F d z 4 3 R T 6 l G w S S f F X q J 4 T F l E z r i X U s V D b n x 0 8 W 9 M 3 J h l Q E Z R t q W Q r J Q f 0 + k N D R m G g a 2 M 6 Q 4 N q v e X P z P 6 y Y 4 v P F T o e I E u W L L R c N E E o z I / H k y E J o z l F N L K N P C 3 k r Y m G r K 0 E Z U s C F 4 q y + v k 1 a 1 4 r k V 7 7 5 a q l 1 l c e T h D M 7 h E j y 4 h h r c Q Q O a w E D C M 7 z C m / P o v D j v z s e y N e d k M 6 f w B 8 7 n D 0 F K j 2 Q = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N t + W u v 1 Q o N 5 o t w h t A 4 j w p D d G 5 k 0 = " &gt; A A A B 7 3 i c b V A 9 S w N B E J 2 L X z F + R S 1 t F h P B K t w F Q c u A j U W K C O Y D k i P s b T b J k r 2 9 c 3 d O C E f + h I 2 F I r b + H T v / j Z v k C k 1 8 M P B 4 b 4 a Z e U E s h U H X / X Z y G 5 t b 2 z v 5 3 c L e / s H h U f H 4 p G W i R D P e Z J G M d C e g h k u h e B M F S t 6 J N a d h I H k 7 m N z O / f Y T 1 0 Z E 6 g G n M f d D O l J i K B h F K 3 X K p p / W 6 7 N y v 1 h y K + 4 C Z J 1 4 G S l B h k a / + N U b R C w J u U I m q T F d z 4 3 R T 6 l G w S S f F X q J 4 T F l E z r i X U s V D b n x 0 8 W 9 M 3 J h l Q E Z R t q W Q r J Q f 0 + k N D R m G g a 2 M 6 Q 4 N q v e X P z P 6 y Y 4 v P F T o e I E u W L L R c N E E o z I / H k y E J o z l F N L K N P C 3 k r Y m G r K 0 E Z U s C F 4 q y + v k 1 a 1 4 r k V 7 7 5 a q l 1 l c e T h D M 7 h E j y 4 h h r c Q Q O a w E D C M 7 z C m / P o v D j v z s e y N e d k M 6 f w B 8 7 n D 0 F K j 2 Q = &lt; / l a t e x i t &gt;</formula><p>Average &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 0 O U c / J Q H k X v 7 p n G l 3 f J c s p b x S s 8 = " &gt; A A A B + X i c b V C 7 T s M w F L 0 p r 1 J e A U a W i B a J q U q 6 w F g E A 2 O R 6 E N q o 8 p x n d a q 4 0 T 2 T U U V 9 U 9 Y G E C I l T 9 h 4 2 9 w 2 w z Q c i R L R + e c a 1 + f I B F c o + t + W 4 W N z a 3 t n e J u a W / / 4 P D I P j 5 p 6 T h</p><formula xml:id="formula_18">V l D V p L G L V C Y h m g k v W R I 6 C d R L F S B Q I 1 g 7 G t 3 O / P W F K 8 1 g + 4 j R h f k S G k o e c E j R S 3 7 Y r P W R P m N 2 Y E B m y W a V v l 9 2 q u 4 C z T r y c l C F H o 2 9 / 9 Q Y x T S M m k Q q i d d d z E / Q z o p B T w W a l X q p Z Q u j Y 3 N 4 1 V J K I a T 9 b b D 5 z L o w y c M J Y m S P R W a i / J z I S a T 2 N A p O M C I 7 0 q j c X / / O 6 K Y b X f s Z l k i K T d P l Q m A o H Y 2 d e g z P g i l E U U 0 M I V d z s 6 t A R U Y S i K a t k S v B W v 7 x O W r W q 5 1 a 9 h 1 q 5 f p f X U Y Q z O I d L 8 O A K 6 n A P D W g C h Q k 8 w y u 8 W Z n 1 Y r 1 b H 8 t o w c p n T u E P r M 8 f Q x q T Z w = = &lt; / l a t e x i t &gt;</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 0 O U c / J Q H k X v 7 p n G l 3 f J c s p b x S s 8 = " &gt; A A A B + X i c b V C 7 T s M w F L 0 p r 1 J e A U a W i B a J q U q 6 w F g E A 2 O R 6 E N q o 8 p x n d a q 4 0 T 2 T U U V 9 U 9 Y G E C I l T 9 h 4 2 9 w 2 w z Q c i R L R + e c a 1 + f I B F c o + t + W 4 W N z a 3 t n e J u a W / / 4 P D I P j 5 p 6 T h</p><formula xml:id="formula_19">V l D V p L G L V C Y h m g k v W R I 6 C d R L F S B Q I 1 g 7 G t 3 O / P W F K 8 1 g + 4 j R h f k S G k o e c E j R S 3 7 Y r P W R P m N 2 Y E B m y W a V v l 9 2 q u 4 C z T r y c l C F H o 2 9 / 9 Q Y x T S M m k Q q i d d d z E / Q z o p B T w W a l X q p Z Q u j Y 3 N 4 1 V J K I a T 9 b b D 5 z L o w y c M J Y m S P R W a i / J z I S a T 2 N A p O M C I 7 0 q j c X / / O 6 K Y b X f s Z l k i K T d P l Q m A o H Y 2 d e g z P g i l E U U 0 M I V d z s 6 t A R U Y S i K a t k S v B W v 7 x O W r W q 5 1 a 9 h 1 q 5 f p f X U Y Q z O I d L 8 O A K 6 n A P D W g C h Q k 8 w y u 8 W Z n 1 Y r 1 b H 8 t o w c p n T u E P r M 8 f Q x q T Z</formula><p>w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 0 O U c / J Q H k X v 7 p n G l 3 f J c s p b x S s 8 = " &gt; A A A B + X i c b V C 7 T s M w F L 0 p r 1 J e A U a W i B a J q U q 6 w F g E A 2 O R 6 E N q o 8 p x n d a q 4 0 T 2 T U U V 9 U 9 Y G E C I l T 9 h 4 2 9 w 2 w z Q c i R L R + e c a 1 + f I B F c o + t + W 4 W N z a 3 t n e J u a W / / 4 P D I P j 5 p 6 T h</p><formula xml:id="formula_20">V l D V p L G L V C Y h m g k v W R I 6 C d R L F S B Q I 1 g 7 G t 3 O / P W F K 8 1 g + 4 j R h f k S G k o e c E j R S 3 7 Y r P W R P m N 2 Y E B m y W a V v l 9 2 q u 4 C z T r y c l C F H o 2 9 / 9 Q Y x T S M m k Q q i d d d z E / Q z o p B T w W a l X q p Z Q u j Y 3 N 4 1 V J K I a T 9 b b D 5 z L o w y c M J Y m S P R W a i / J z I S a T 2 N A p O M C I 7 0 q j c X / / O 6 K Y b X f s Z l k i K T d P l Q m A o H Y 2 d e g z P g i l E U U 0 M I V d z s 6 t A R U Y S i K a t k S v B W v 7 x O W r W q 5 1 a 9 h 1 q 5 f p f X U Y Q z O I d L 8 O A K 6 n A P D W g C h Q k 8 w y u 8 W Z n 1 Y r 1 b H 8 t o w c p n T u E P r M 8 f Q x q T Z</formula><p>w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 0 O U c / J Q H k X v 7 p n G l 3 f J c s p b x S s 8 = " &gt; A A A B + X i c b V C 7 T s M w F L 0 p r 1 J e A U a W i B a J q U q 6 w F g E A 2 O R 6 E N q o 8 p x n d a q 4 0 T 2 T U U V 9 U 9 Y G E C I l T 9 h 4 2 9 w 2 w z Q c i R L R + e c a 1 + f I B F c o + t + W 4 W N z a 3 t n e J u a W / / 4 P D I P j 5 p 6 T h</p><formula xml:id="formula_21">V l D V p L G L V C Y h m g k v W R I 6 C d R L F S B Q I 1 g 7 G t 3 O / P W F K 8 1 g + 4 j R h f k S G k o e c E j R S 3 7 Y r P W R P m N 2 Y E B m y W a V v l 9 2 q u 4 C z T r y c l C F H o 2 9 / 9 Q Y x T S M m k Q q i d d d z E / Q z o p B T w W a l X q p Z Q u j Y 3 N 4 1 V J K I a T 9 b b D 5 z L o w y c M J Y m S P R W a i / J z I S a T 2 N A p O M C I 7 0 q j c X / / O 6 K Y b X f s Z l k i K T d P l Q m A o H Y 2 d e g z P g i l E U U 0 M I V d z s 6 t A R U Y S i K a t k S v B W v 7 x O W r W q 5 1 a 9 h 1 q 5 f p f X U Y Q z O I d L 8 O A K 6 n A P D W g C h Q k 8 w y u 8 W Z n 1 Y r 1 b H 8 t o w c p n T u E P r M 8 f Q x q T Z w = = &lt; / l a t e x i t &gt;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kernel aggregation</head><p>Kernel aggregation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K(S L )</head><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " q X C / P 8 u 9 m f u h</p><formula xml:id="formula_22">Y 4 B 3 o u m g P / K H V D Y = " &gt; A A A C B X i c b Z D L S s N A F I Y n 9 V b r L e p S F 4 O t U D c l K Y I u C 2 4 E X V S 0 F 2 h D m E w n 7 d D J J M x M h B K y c e O r u H G h i F v f w Z 1 v 4 6 S N o K 0 / D H z 8 5 x z m n N + L G J X K s r 6 M w t L y y u p a c b 2 0 s b m 1 v W P u 7 r V l G A t M W j h k o e h 6 S B J G O W k p q h j p R o K g w G O k 4 4 0 v s n r n n g h J Q 3 6 n J h F x A j T k 1 K c Y K W 2 5 5 m G l H y A 1 8 v z k K q 3 + 4 G 3 q J t f p S c U 1 y 1 b N m g o u g p 1 D G e R q u u Z n f x D i O C B c Y Y a k 7 N l W p J w E C U U x I 2 m p H 0 s S I T x G Q 9 L T y F F A p J N M r 0 j h s X Y G 0 A + F f l z B q f t 7 I k G B l J P A 0 5 3 Z n n K + l p n / 1 X q x 8 s + d h P I o V o T j 2 U d + z K A K Y R Y J H F B B s G I T D Q g L q n e F e I Q E w k o H V 9 I h 2 P M n L 0 K 7 X r O t m n 1 T L z d O 8 z i K 4 A A c g S q w w R l o g E v Q B C 2 A w Q N 4 A i / g 1 X g 0 n o 0 3 4 3 3 W W j D y m X 3 w R 8 b H N 9 0 + m B k = &lt; /</formula><p>l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " q X C / P 8 u 9 m f u h</p><formula xml:id="formula_23">Y 4 B 3 o u m g P / K H V D Y = " &gt; A A A C B X i c b Z D L S s N A F I Y n 9 V b r L e p S F 4 O t U D c l K Y I u C 2 4 E X V S 0 F 2 h D m E w n 7 d D J J M x M h B K y c e O r u H G h i F v f w Z 1 v 4 6 S N o K 0 / D H z 8 5 x z m n N + L G J X K s r 6 M w t L y y u p a c b 2 0 s b m 1 v W P u 7 r V l G A t M W j h k o e h 6 S B J G O W k p q h j p R o K g w G O k 4 4 0 v s n r n n g h J Q 3 6 n J h F x A j T k 1 K c Y K W 2 5 5 m G l H y A 1 8 v z k K q 3 + 4 G 3 q J t f p S c U 1 y 1 b N m g o u g p 1 D G e R q u u Z n f x D i O C B c Y Y a k 7 N l W p J w E C U U x I 2 m p H 0 s S I T x G Q 9 L T y F F A p J N M r 0 j h s X Y G 0 A + F f l z B q f t 7 I k G B l J P A 0 5 3 Z n n K + l p n / 1 X q x 8 s + d h P I o V o T j 2 U d + z K A K Y R Y J H F B B s G I T D Q g L q n e F e I Q E w k o H V 9 I h 2 P M n L 0 K 7 X r O t m n 1 T L z d O 8 z i K 4 A A c g S q w w R l o g E v Q B C 2 A w Q N 4 A i / g 1 X g 0 n o 0 3 4 3 3 W W j D y m X 3 w R 8 b H N 9 0 + m B k = &lt; /</formula><p>l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " q X C / P 8 u 9 m f u h</p><formula xml:id="formula_24">Y 4 B 3 o u m g P / K H V D Y = " &gt; A A A C B X i c b Z D L S s N A F I Y n 9 V b r L e p S F 4 O t U D c l K Y I u C 2 4 E X V S 0 F 2 h D m E w n 7 d D J J M x M h B K y c e O r u H G h i F v f w Z 1 v 4 6 S N o K 0 / D H z 8 5 x z m n N + L G J X K s r 6 M w t L y y u p a c b 2 0 s b m 1 v W P u 7 r V l G A t M W j h k o e h 6 S B J G O W k p q h j p R o K g w G O k 4 4 0 v s n r n n g h J Q 3 6 n J h F x A j T k 1 K c Y K W 2 5 5 m G l H y A 1 8 v z k K q 3 + 4 G 3 q J t f p S c U 1 y 1 b N m g o u g p 1 D G e R q u u Z n f x D i O C B c Y Y a k 7 N l W p J w E C U U x I 2 m p H 0 s S I T x G Q 9 L T y F F A p J N M r 0 j h s X Y G 0 A + F f l z B q f t 7 I k G B l J P A 0 5 3 Z n n K + l p n / 1 X q x 8 s + d h P I o V o T j 2 U d + z K A K Y R Y J H F B B s G I T D Q g L q n e F e I Q E w k o H V 9 I h 2 P M n L 0 K 7 X r O t m n 1 T L z d O 8 z i K 4 A A c g S q w w R l o g E v Q B C 2 A w Q N 4 A i / g 1 X g 0 n o 0 3 4 3 3 W W j D y m X 3 w R 8 b H N 9 0 + m B k = &lt; /</formula><p>l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " q X C / P 8 u 9 m f u h</p><formula xml:id="formula_25">Y 4 B 3 o u m g P / K H V D Y = " &gt; A A A C B X i c b Z D L S s N A F I Y n 9 V b r L e p S F 4 O t U D c l K Y I u C 2 4 E X V S 0 F 2 h D m E w n 7 d D J J M x M h B K y c e O r u H G h i F v f w Z 1 v 4 6 S N o K 0 / D H z 8 5 x z m n N + L G J X K s r 6 M w t L y y u p a c b 2 0 s b m 1 v W P u 7 r V l G A t M W j h k o e h 6 S B J G O W k p q h j p R o K g w G O k 4 4 0 v s n r n n g h J Q 3 6 n J h F x A j T k 1 K c Y K W 2 5 5 m G l H y A 1 8 v z k K q 3 + 4 G 3 q J t f p S c U 1 y 1 b N m g o u g p 1 D G e R q u u Z n f x D i O C B c Y Y a k 7 N l W p J w E C U U x I 2 m p H 0 s S I T x G Q 9 L T y F F A p J N M r 0 j h s X Y G 0 A + F f l z B q f t 7 I k G B l J P A 0 5 3 Z n n K + l p n / 1 X q x 8 s + d h P I o V o T j 2 U d + z K A K Y R Y J H F B B s G I T D Q g L q n e F e I Q E w k o H V 9 I h 2 P M n L 0 K 7 X r O t m n 1 T L z d O 8 z i K 4 A A c g S q w w R l o g E v Q B C 2 A w Q N 4 A i / g 1 X g 0 n o 0 3 4 3 3 W W j D y m X 3 w R 8 b H N 9 0 + m B k = &lt; / l a t e x i t &gt; K(S 0 )</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " k W N w y</p><formula xml:id="formula_26">+ v i F f V W + F z / K v e F x 2 G / t v s = " &gt; A A A C A 3 i c b Z D L S s N A F I Z P v N Z 6 i 7 r T z W A r 1 E 1 J i q D L g h v B T U V 7 g T a E y X T S D p 1 c m J k I J R T c + C p u X C j i 1 p d w 5 9 s 4 a S N o 6 w 8 D H / 8 5 h z n n 9 2 L O p L K s L 2 N p e W V 1 b b 2 w U d z c 2 t 7 Z N f f 2 W z J K B K F N E v F I d D w s K W c h b S q m O O 3 E g u L A 4 7 T t j S 6 z e v u e C s m i 8 E 6 N Y + o E e B A y n x G s t O W a h + V e g N X Q 8 9 P r S e U H b y e u d V p 2 z Z J V t a Z C i 2 D n U I J c D d f 8 7 P U j k g Q 0 V I R j K b u 2 F S s n x U I x w u m k 2 E s k j T E Z 4 Q H t a g x x Q K W T T m + Y o B P t 9 J E f C f 1 C h a b u 7 4 k U B 1 K O A 0 9 3 Z l v K + V p m / l f r J s q / c F I W x o m i I Z l 9 5 C c c q Q h l g a A + E 5 Q o P t a A i W B 6 V 0 S G W G C i d G x F H Y I 9 f / I i t G p V 2 6 r a N 7 V S / S y P o w B H c A w V s O E c 6 n A F D W g C g Q d 4 g h d 4 N R 6 N Z + P N e J + 1 L h n 5 z A H 8 k f H x D d t Q l v E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " k W N w y + v i F f V W + F z / K v e F x 2 G / t v s = " &gt; A A A C A 3 i c b Z D L S s N A F I Z P v N Z 6 i 7 r T z W A r 1 E 1 J i q D L g h v B T U V 7 g T a E y X T S D p 1 c m J k I J R T c + C p u X C j i 1 p d w 5 9 s 4 a S N o 6 w 8 D H / 8 5 h z n n 9 2 L O p L K s L 2 N p e W V 1 b b 2 w U d z c 2 t 7 Z N f f 2 W z J K B K F N E v F I d D w s K W c h b S q m O O 3 E g u L A 4 7 T t j S 6 z e v u e C s m i 8 E 6 N Y + o E e B A y n x G s t O W a h + V e g N X Q 8 9 P r S e U H b y e u d V p 2 z Z J V t a Z C i 2 D n U I J c D d f 8 7 P U j k g Q 0 V I R j K b u 2 F S s n x U I x w u m k 2 E s k j T E Z 4 Q H t a g x x Q K W T T m + Y o B P t 9 J E f C f 1 C h a b u 7 4 k U B 1 K O A 0 9 3 Z l v K + V p m / l f r J s q / c F I W x o m i I Z l 9 5 C c c q Q h l g a A + E 5 Q o P t a A i W B 6 V 0 S G W G C i d G x F H Y I 9 f / I i t G p V 2 6 r a N 7 V S / S y P o w B H c A w V s O E c 6 n A F D W g C g Q d 4 g h d 4 N R 6 N Z + P N e J + 1 L h n 5 z A H 8 k f H x D d t Q l v E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " k W N w y + v i F f V W + F z / K v e F x 2 G / t v s = " &gt; A A A C A 3 i c b Z D L S s N A F I Z P v N Z 6 i 7 r T z W A r 1 E 1 J i q D L g h v B T U V 7 g T a E y X T S D p 1 c m J k I J R T c + C p u X C j i 1 p d w 5 9 s 4 a S N o 6 w 8 D H / 8 5 h z n n 9 2 L O p L K s L 2 N p e W V 1 b b 2 w U d z c 2 t 7 Z N f f 2 W z J K B K F N E v F I d D w s K W c h b S q m O O 3 E g u L A 4 7 T t j S 6 z e v u e C s m i 8 E 6 N Y + o E e B A y n x G s t O W a h + V e g N X Q 8 9 P r S e U H b y e u d V p 2 z Z J V t a Z C i 2 D n U I J c D d f 8 7 P U j k g Q 0 V I R j K b u 2 F S s n x U I x w u m k 2 E s k j T E Z 4 Q H t a g x x Q K W T T m + Y o B P t 9 J E f C f 1 C h a b u 7 4 k U B 1 K O A 0 9 3 Z l v K + V p m / l f r J s q / c F I W x o m i I Z l 9 5 C c c q Q h l g a A + E 5 Q o P t a A i W B 6 V 0 S G W G C i d G x F H Y I 9 f / I i t G p V 2 6 r a N 7 V S / S y P o w B H c A w V s O E c 6 n A F D W g C g Q d 4 g h d 4 N R 6 N Z + P N e J + 1 L h n 5 z A H 8 k f H x D d t Q l v E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " k W N w y + v i F f V W + F z / K v e F x 2 G / t v s = " &gt; A A A C A 3 i c b Z D L S s N A F I Z P v N Z 6 i 7 r T z W A r 1 E 1 J i q D L g h v B T U V 7 g T a E y X T S D p 1 c m J k I J R T c + C p u X C j i 1 p d w 5 9 s 4 a S N o 6 w 8 D H / 8 5 h z n n 9 2 L O p L K s L 2 N p e W V 1 b b 2 w U d z c 2 t 7 Z N f f 2 W z J K B K F N E v F I d D w s K W c h b S q m O O 3 E g u L A 4 7 T t j S 6 z e v u e C s m i 8 E 6 N Y + o E e B A y n x G s t O W a h + V e g N X Q 8 9 P r S e U H b y e u d V p 2 z Z J V t a Z C i 2 D n U I J c D d f 8 7 P U j k g Q 0 V I R j K b u 2 F S s n x U I x w u m k 2 E s k j T E Z 4 Q H t a g x x Q K W T T m + Y o B P t 9 J E f C f 1 C h a b u 7 4 k U B 1 K O A 0 9 3 Z l v K + V p m / l f r J s q / c F I W x o m i I Z l 9 5 C c c q Q h l g a A + E 5 Q o P t a A i W B 6 V 0 S G W G C i d G x F H Y I 9 f / I i t G p V 2 6 r a N 7 V S / S y P o w B H c A w V s O E c 6 n A F D W g C g Q d 4 g h d 4 N R 6 N Z + P N e J + 1 L h n 5 z A H 8 k f H x D d t Q l v E = &lt; / l a t e x i t &gt; … MLP MLP (e, e 0 ) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e v 2 a M k J r W G T l m T x w X w k 2 l v s C i f o = " &gt; A A A B 8 3 i c b V B N S w M x E M 3 W r 1 q / q h 6 9 B F u x g p T d I u i x 4 M V j B f s B 3 a V k 0 9 k 2 N J s N S V Y o S / + G F w + K e P X P e P P f m L Z 7 0 O q D g c d 7 M 8 z M C y V n 2 r j u l 1 N Y W 9 / Y 3 C p u l 3 Z 2 9 / Y P y o d H H Z 2 k i k K b J j x R v Z B o 4 E x A 2 z D D o S c V k D j k 0 A 0 n t 3 O / + w h K s 0 Q 8 m K m E I C Y j w S J G i b G S X / X l m N X g E s 4 v q o N y x a 2 7 C + C / x M t J B e V o D c q f / j C h a Q z C U E 6 0 7 n u u N E F G l G G U w 6 z k p x o k o R M y g r 6 l g s S g g 2 x x 8 w y f W W W I o 0 T Z E g Y v 1 J 8 T G Y m 1 n s a h 7 Y y J G e t V b y 7 + 5 / V T E 9 0 E G R M y N S D o c l G U c m w S P A 8 A D 5 k C a v j U E k I V s 7 d i O i a K U G N j K t k Q v N W X / 5 J O o + 6 5 d e + + U W l e 5 X E U 0 Q k 6 R T X k o W v U R H e o h d q I I o m e 0 A t 6 d V L n 2 X l z 3 p e t B S e f O U a / 4 H x 8 A + B V k D U = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e v 2 a M k J r W G T l m T x w X w k 2 l v s C i f o = " &gt; A A A B 8 3 i c b V B N S w M x E M 3 W r 1 q / q h 6 9 B F u x g p T d I u i x 4 M V j B f s B 3 a V k 0 9 k 2 N J s N S V Y o S / + G F w + K e P X P e P P f m L Z 7 0 O q D g c d 7 M 8 z M C y V n 2 r j u l 1 N Y W 9 / Y 3 C p u l 3 Z 2 9 / Y P y o d H H Z 2 k i k K b J j x R v Z B o 4 E x A 2 z D D o S c V k D j k 0 A 0 n t 3 O / + w h K s 0 Q 8 m K m E I C Y j w S J G i b G S X / X l m N X g E s 4 v q o N y x a 2 7 C + C / x M t J B e V o D c q f / j C h a Q z C U E 6 0 7 n u u N E F G l G G U w 6 z k p x o k o R M y g r 6 l g s S g g 2 x x 8 w y f W W W I o 0 T Z E g Y v 1 J 8 T G Y m 1 n s a h 7 Y y J G e t V b y 7 + 5 / V T E 9 0 E G R M y N S D o c l G U c m w S P A 8 A D 5 k C a v j U E k I V s 7 d i O i a K U G N j K t k Q v N W X / 5 J O o + 6 5 d e + + U W l e 5 X E U 0 Q k 6 R T X k o W v U R H e o h d q I I o m e 0 A t 6 d V L n 2 X l z 3 p e t B S e f O U a / 4 H x 8 A + B V k D U = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e v 2 a M k J r W G T l m T x w X w k 2 l v s C i f o = " &gt; A A A B 8 3 i c b V B N S w M x E M 3 W r 1 q / q h 6 9 B F u x g p T d I u i x 4 M V j B f s B 3 a V k 0 9 k 2 N J s N S V Y o S / + G F w + K e P X P e P P f m L Z 7 0 O q D g c d 7 M 8 z M C y V n 2 r j u l 1 N Y W 9 / Y 3 C p u l 3 Z 2 9 / Y P y o d H H Z 2 k i k K b J j x R v Z B o 4 E x A 2 z D D o S c V k D j k 0 A 0 n t 3 O / + w h K s 0 Q 8 m K m E I C Y j w S J G i b G S X / X l m N X g E s 4 v q o N y x a 2 7 C + C / x M t J B e V o D c q f / j C h a Q z C U E 6 0 7 n u u N E F G l G G U w 6 z k p x o k o R M y g r 6 l g s S g g 2 x x 8 w y f W W W I o 0 T Z E g Y v 1 J 8 T G Y m 1 n s a h 7 Y y J G e t V b y 7 + 5 / V T E 9 0 E G R M y N S D o c l G U c m w S P A 8 A D 5 k C a v j U E k I V s 7 d i O i a K U G N j K t k Q v N W X / 5 J O o + 6 5 d e + + U W l e 5 X E U 0 Q k 6 R T X k o W v U R H e o h d q I I o m e 0 A t 6 d V L n 2 X l z 3 p e t B S e f O U a / 4 H x 8 A + B V k D U = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e v 2 a M k J r W G T l m T x w X w k 2 l v s C i f o = " &gt; A A A B 8 3 i c b V B N S w M x E M 3 W r 1 q / q h 6 9 B F u x g p T d I u i x 4 M V j B f s B 3 a V k 0 9 k 2 N J s N S V Y o S / + G F w + K e P X P e P P f m L Z 7 0 O q D g c d 7 M 8 z M C y V n 2 r j u l 1 N Y W 9 / Y 3 C p u l 3 Z 2 9 / Y P y o d H H Z 2 k i k K b J j x R v Z B o 4 E x A 2 z D D o S c V k D j k 0 A 0 n t 3 O / + w h K s 0 Q 8 m K m E I C Y j w S J G i b G S X / X l m N X g E s 4 v q o N y x a 2 7 C + C / x M t J B e V o D c q f / j C h a Q z C U E 6 0 7 n u u N E F G l G G U w 6 z k p x o k o R M y g r 6 l g s S g g 2 x x 8 w y f W W W I o 0 T Z E g Y v 1 J 8 T G Y m 1 n s a h 7 Y y J G e t V b y 7 + 5 / V T E 9 0 E G R M y N S D o c l G U c m w S P A 8 A D 5 k C a v j U E k I V s 7 d i O i a K U G N j K t k Q v N W X / 5 J O o + 6 5 d e + + U W l e 5 X E U 0 Q k 6 R T X k o W v U R H e o h d q I I o m e 0 A t 6 d V L n 2 X l z 3 p e t B S e f O U a / 4 H x 8 A + B V k D U = &lt; / l a t e x i t &gt; Metric function BERT Tok N Tok 2 Tok 1 CLS E [CLS] E 1 E 2 E N CLS T 1 T 2 T N … … … Gaussian processing learning p 0 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C r h Y T g H p m B i K k Y z / 7 T e r W w n 0 M 1 8 = " &gt; A A A B + H i c b V D L S g N B E O z 1 G e M j q x 6 9 D C a i p 7 A b B D 0 G v H i M Y B 6 Q L G F 2 M p s M m X 0 w 0 y v G J V / i x Y M i X v 0 U b / 6 N k 2 Q P m l j Q U F R 1 0 9 3 l J 1 J o d J x v a 2 1 9 Y 3 N r u 7 B T 3 N 3 b P y j Z h 0 c t H a e K 8 S a L Z a w 6 P t V c i o g 3 U a D k n U R x G v q S t / 3 x z c x v P 3 C l R R z d 4 y T h X k i H k Q g E o 2 i k v l 2 q 9 J A / o h 9 k y f S 8 7 1 b 6 d t m p O n O Q V e L m p A w 5 G n 3 7 q z e I W R r y C J m k W n d d J 0 E v o w o F k 3 x a 7 K W a J 5 S N 6 Z B 3 D Y 1 o y L W X z Q + f k j O j D E g Q K 1 M R k r n 6 e y K j o d a T 0 D e d I c W R X v Z m 4 n 9 e N 8 X g 2 s t E l K T I I 7 Z Y F K S S Y E x m K Z C B U J y h n B h C m R L m V s J G V F G G J q u i C c F d f n m V t G p V 1 6 m 6 d 7 V y / T K P o w A n c A o X 4 M I V 1 O E W G t A E B i k 8 w y u 8 W U / W i / V u f S x a 1 6 x 8 5 h j + w P r 8 A d 2 v k o E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C r h Y T g H p m B i K k Y z / 7 T e r W w n 0 M 1 8 = " &gt; A A A B + H i c b V D L S g N B E O z 1 G e M j q x 6 9 D C a i p 7 A b B D 0 G v H i M Y B 6 Q L G F 2 M p s M m X 0 w 0 y v G J V / i x Y M i X v 0 U b / 6 N k 2 Q P m l j Q U F R 1 0 9 3 l J 1 J o d J x v a 2 1 9 Y 3 N r u 7 B T 3 N 3 b P y j Z h 0 c t H a e K 8 S a L Z a w 6 P t V c i o g 3 U a D k n U R x G v q S t / 3 x z c x v P 3 C l R R z d 4 y T h X k i H k Q g E o 2 i k v l 2 q 9 J A / o h 9 k y f S 8 7 1 b 6 d t m p O n O Q V e L m p A w 5 G n 3 7 q z e I W R r y C J m k W n d d J 0 E v o w o F k 3 x a 7 K W a J 5 S N 6 Z B 3 D Y 1 o y L W X z Q + f k j O j D E g Q K 1 M R k r n 6 e y K j o d a T 0 D e d I c W R X v Z m 4 n 9 e N 8 X g 2 s t E l K T I I 7 Z Y F K S S Y E x m K Z C B U J y h n B h C m R L m V s J G V F G G J q u i C c F d f n m V t G p V 1 6 m 6 d 7 V y / T K P o w A n c A o X 4 M I V 1 O E W G t A E B i k 8 w y u 8 W U / W i / V u f S x a 1 6 x 8 5 h j + w P r 8 A d 2 v k o E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C r h Y T g H p m B i K k Y z / 7 T e r W w n 0 M 1 8 = " &gt; A A A B + H i c b V D L S g N B E O z 1 G e M j q x 6 9 D C a i p 7 A b B D 0 G v H i M Y B 6 Q L G F 2 M p s M m X 0 w 0 y v G J V / i x Y M i X v 0 U b / 6 N k 2 Q P m l j Q U F R 1 0 9 3 l J 1 J o d J x v a 2 1 9 Y 3 N r u 7 B T 3 N 3 b P y j Z h 0 c t H a e K 8 S a L Z a w 6 P t V c i o g 3 U a D k n U R x G v q S t / 3 x z c x v P 3 C l R R z d 4 y T h X k i H k Q g E o 2 i k v l 2 q 9 J A / o h 9 k y f S 8 7 1 b 6 d t m p O n O Q V e L m p A w 5 G n 3 7 q z e I W R r y C J m k W n d d J 0 E v o w o F k 3 x a 7 K W a J 5 S N 6 Z B 3 D Y 1 o y L W X z Q + f k j O j D E g Q K 1 M R k r n 6 e y K j o d a T 0 D e d I c W R X v Z m 4 n 9 e N 8 X g 2 s t E l K T I I 7 Z Y F K S S Y E x m K Z C B U J y h n B h C m R L m V s J G V F G G J q u i C c F d f n m V t G p V 1 6 m 6 d 7 V y / T K P o w A n c A o X 4 M I V 1 O E W G t A E B i k 8 w y u 8 W U / W i / V u f S x a 1 6 x 8 5 h j + w P r 8 A d 2 v k o E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C r h Y T g H p m B i K k Y z / 7 T e r W w n 0 M 1 8 = " &gt; A A A B + H i c b V D L S g N B E O z 1 G e M j q x 6 9 D C a i p 7 A b B D 0 G v H i M Y B 6 Q L G F 2 M p s M m X 0 w 0 y v G J V / i x Y M i X v 0 U b / 6 N k 2 Q P m l j Q U F R 1 0 9 3 l J 1 J o d J x v a 2 1 9 Y 3 N r u 7 B T 3 N 3 b P y j Z h 0 c t H a e K 8 S a L Z a w 6 P t V c i o g 3 U a D k n U R x G v q S t / 3 x z c x v P 3 C l R R z d 4 y T h X k i H k Q g E o 2 i k v l 2 q 9 J A / o h 9 k y f S 8 7 1 b 6 d t m p O n O Q V e L m p A w 5 G n 3 7 q z e I W R r y C J m k W n d d J 0 E v o w o F k 3 x a 7 K W a J 5 S N 6 Z B 3 D Y 1 o y L W X z Q + f k j O j D E g Q K 1 M R k r n 6 e y K j o d a T 0 D e d I c W R X v Z m 4 n 9 e N 8 X g 2 s t E l K T I I 7 Z Y F K S S Y E x m K Z C B U J y h n B h C m R L m V s J G V F G G J q u i C c F d f n m V t G p V 1 6 m 6 d 7 V y / T K P o w A n c A o X 4 M I V 1 O E W G t A E B i k 8 w y u 8 W U / W i / V u f S x a 1 6 x 8 5 h j + w P r 8 A d 2 v k o E = &lt; / l a t e x i t &gt; p 0 2</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " P e Q X f w 5</p><formula xml:id="formula_27">v j i R B T z q g 1 D Z E c o X c J N w = " &gt; A A A B + H i c b V B N S 8 N A E J 3 4 W e t H o x 6 9 B F v R U 0 m K o M e C F 4 8 V 7 A e 0 o W y 2 m 3 b p Z h N 2 J 2 I N / S V e P C j i 1 Z / i z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u 0 X W / r b X 1 j c 2 t 7 c J O c X d v / 6 B k H x 6 1 d J w q y p o 0 F r H q B E Q z w S V r I k f B O o l i J A o E a w f j m 5 n f f m B K 8 1 j e 4 y R h f k S G k o e c E j R S 3 y 5 V e s g e M Q i z Z H r e r 1 X 6 d t m t u n M 4 q 8 T L S R l y N P r 2 V 2 8 Q 0 z R i E q k g W n c 9 N 0 E / I w o 5 F W x a 7 K W a J Y S O y Z B 1 D Z U k Y t r P 5 o d P n T O j D J w w V q Y k O n P 1 9 0 R G I q 0 n U W A 6 I 4 I j v e z N x P + 8 b o r h t Z 9 x m a T I J F 0 s C l P h Y O z M U n A G X D G K Y m I I o Y q b W x 0 6 I o p Q N F k V T Q j e 8 s u r p F W r e m 7 V u 6 u V 6 5 d 5 H A U 4 g V O 4 A A + u o A 6 3 0 I A m U E j h G V 7 h z X</formula><p>q y X q x 3 6 2 P R u m b l M 8 f w B 9 b n D 9 8 0 k o I = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " P e Q X f w 5</p><formula xml:id="formula_28">v j i R B T z q g 1 D Z E c o X c J N w = " &gt; A A A B + H i c b V B N S 8 N A E J 3 4 W e t H o x 6 9 B F v R U 0 m K o M e C F 4 8 V 7 A e 0 o W y 2 m 3 b p Z h N 2 J 2 I N / S V e P C j i 1 Z / i z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u 0 X W / r b X 1 j c 2 t 7 c J O c X d v / 6 B k H x 6 1 d J w q y p o 0 F r H q B E Q z w S V r I k f B O o l i J A o E a w f j m 5 n f f m B K 8 1 j e 4 y R h f k S G k o e c E j R S 3 y 5 V e s g e M Q i z Z H r e r 1 X 6 d t m t u n M 4 q 8 T L S R l y N P r 2 V 2 8 Q 0 z R i E q k g W n c 9 N 0 E / I w o 5 F W x a 7 K W a J Y S O y Z B 1 D Z U k Y t r P 5 o d P n T O j D J w w V q Y k O n P 1 9 0 R G I q 0 n U W A 6 I 4 I j v e z N x P + 8 b o r h t Z 9 x m a T I J F 0 s C l P h Y O z M U n A G X D G K Y m I I o Y q b W x 0 6 I o p Q N F k V T Q j e 8 s u r p F W r e m 7 V u 6 u V 6 5 d 5 H A U 4 g V O 4 A A + u o A 6 3 0 I A m U E j h G V 7 h z X</formula><p>q y X q x 3 6 2 P R u m b l M 8 f w B 9 b n D 9 8 0 k o I = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " P e Q X f w 5 </p><formula xml:id="formula_29">v j i R B T z q g 1 D Z E c o X c J N w = " &gt; A A A B + H i c b V B N S 8 N A E J 3 4 W e t H o x 6 9 B F v R U 0 m K o M e C F 4 8 V 7 A e 0 o W y 2 m 3 b p Z h N 2 J 2 I N / S V e P C j i 1 Z / i z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u 0 X W / r b X 1 j c 2 t 7 c J O c X d v / 6 B k H x 6 1 d J w q y p o 0 F r H q B E Q z w S V r I k f B O o l i J A o E a w f j m 5 n f f m B K 8 1 j e 4 y R h f k S G k o e c E j R S 3 y 5 V e s g e M Q i z Z H r e r 1 X 6 d t m t u n M 4 q 8 T L S R l y N P r 2 V 2 8 Q 0 z R i E q k g W n c 9 N 0 E / I w o 5 F W x a 7 K W a J Y S O y Z B 1 D Z U k Y t r P 5 o d P n T O j D J w w V q Y k O n P 1 9 0 R G I q 0 n U W A 6 I 4 I j v e z N x P + 8 b o r h t Z 9 x m a T I J F 0 s C l P h Y O z M U n A G X D G K Y m I I o Y q b W x 0 6 I o p Q N F k V T Q j e 8 s u r p F W r e m 7 V u 6 u V 6 5 d 5 H A U 4 g V O 4 A A + u o A 6 3 0 I A m U E j h G V 7 h z X q y X q x 3 6 2 P R u m b l M 8 f w B 9 b n D 9 8 0 k o I = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " P e Q X f w 5 v j i R B T z q g 1 D Z E c o X c J N w = " &gt; A A A B + H i c b V B N S 8 N A E J 3 4 W e t H o x 6 9 B F v R U 0 m K o M e C F 4 8 V 7 A e 0 o W y 2 m 3 b p Z h N 2 J 2 I N / S V e P C j i 1 Z / i z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u 0 X W / r b X 1 j c 2 t 7 c J O c X d v / 6 B k H x 6 1 d J w q y p o 0 F r H q B E Q z w S V r I k f B O o l i J A o E a w f j m 5 n f f m B K 8 1 j e 4 y R h f k S G k o e c E j R S 3 y 5 V e s g e M Q i z Z H r e r 1 X 6 d t m t u n M 4 q 8 T L S R l y N P r 2 V 2 8 Q 0 z R i E q k g W n c 9 N 0 E / I w o 5 F W x a 7 K W a J Y S O y Z B 1 D Z U k Y t r P 5 o d P n T O j D J w w V q Y k O n P 1 9 0 R G I q 0 n U W A 6 I 4 I j v e z N x P + 8 b o r h t Z 9 x m a T I J F 0 s C l P h Y O z M U n A G X D G K Y m I I o Y q b W x 0 6 I o p Q N F k V T Q j e 8 s u r p F W r e m 7 V u 6 u V 6 5 d 5 H A U 4 g V O 4 A A + u o A 6 3 0 I A m U E j h G V 7 h z X q y X q x 3 6 2 P R u m b l M 8 f w B 9 b n D 9 8 0 k o I = &lt; / l a t e x i t &gt; p 0 L &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F C 1 d / i G S b 0 s 7 v 1 j 5 Z s w d Y 6 J H 4 z w = " &gt; A A A B + H i c b V B N S 8 N A E N 3 U r 1 o / G v X o Z b E V P Z W k C H o s e P H g o Y L 9 g D a E z X b T L t 1 s w u 5 E r K G / x I s H R b z 6 U 7 z 5 b 9 y 2 O W j r g 4 H H e z P M z A s S w T U 4 z r d V W F v f 2 N w q b p d 2 d v f 2 y / b B Y V v H q a K s R W M R q 2 5 A N B N c s h Z w E K y b K E a i Q L B O M L 6 e + Z 0 H p j S P 5 T 1 M E u Z F Z C h 5 y C k B I / l 2 u d o H 9 g h B m C X T M / + 2 6 t s V p + b M g V e J m 5 M K y t H 0 7 a / + I K Z p x C R Q Q b T u u U 4 C X k Y U c C r Y t N R P N U s I H Z M h 6 x k q S c S 0 l 8 0 P n + J T o w x w G C t T E v B c / T 2 R k U j r S R S Y z o j A S C 9 7 M / E / r 5 d C e O V l X C Y p M E k X i 8 J U Y I j x L A U 8 4 I p R E B N D C F X c 3 I r p i C h C w W R V M i G 4 y y + v k n a 9 5 j o 1 9 6 5 e a V z k c R T R M T p B 5 8 h F l 6 i B b l A T t R B F K X p G r + j N e r J e r H f r Y 9 F a s P K Z I / Q H 1 u c P B s W S n A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F C 1 d / i G S b 0 s 7 v 1 j 5 Z s w d Y 6 J H 4 z w = " &gt; A A A B + H i c b V B N S 8 N A E N 3 U r 1 o / G v X o Z b E V P Z W k C H o s e P H g o Y L 9 g D a E z X b T L t 1 s w u 5 E r K G / x I s H R b z 6 U 7 z 5 b 9 y 2 O W j r g 4 H H e z P M z A s S w T U 4 z r d V W F v f 2 N w q b p d 2 d v f 2 y / b B Y V v H q a K s R W M R q 2 5 A N B N c s h Z w E K y b K E a i Q L B O M L 6 e + Z 0 H p j S P 5 T 1 M E u Z F Z C h 5 y C k B I / l 2 u d o H 9 g h B m C X T M / + 2 6 t s V p + b M g V e J m 5 M K y t H 0 7 a / + I K Z p x C R Q Q b T u u U 4 C X k Y U c C r Y t N R P N U s I H Z M h 6 x k q S c S 0 l 8 0 P n + J T o w x w G C t T E v B c / T 2 R k U j r S R S Y z o j A S C 9 7 M / E / r 5 d C e O V l X C Y p M E k X i 8 J U Y I j x L A U 8 4 I p R E B N D C F X c 3 I r p i C h C w W R V M i G 4 y y + v k n a 9 5 j o 1 9 6 5 e a V z k c R T R M T p B 5 8 h F l 6 i B b l A T t R B F K X p G r + j N e r J e r H f r Y 9 F a s P K Z I / Q H 1 u c P B s W S n A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F C 1 d / i G S b 0 s 7 v 1 j 5 Z s w d Y 6 J H 4 z w = " &gt; A A A B + H i c b V B N S 8 N A E N 3 U r 1 o / G v X o Z b E V P Z W k C H o s e P H g o Y L 9 g D a E z X b T L t 1 s w u 5 E r K G / x I s H R b z 6 U 7 z 5 b 9 y 2 O W j r g 4 H H e z P M z A s S w T U 4 z r d V W F v f 2 N w q b p d 2 d v f 2 y / b B Y V v H q a K s R W M R q 2 5 A N B N c s h Z w E K y b K E a i Q L B O M L 6 e + Z 0 H p j S P 5 T 1 M E u Z F Z C h 5 y C k B I / l 2 u d o H 9 g h B m C X T M / + 2 6 t s V p + b M g V e J m 5 M K y t H 0 7 a / + I K Z p x C R Q Q b T u u U 4 C X k Y U c C r Y t N R P N U s I H Z M h 6 x k q S c S 0 l 8 0 P n + J T o w x w G C t T E v B c / T 2 R k U j r S R S Y z o j A S C 9 7 M / E / r 5 d C e O V l X C Y p M E k X i 8 J U Y I j x L A U 8 4 I p R E B N D C F X c 3 I r p i C h C w W R V M i G 4 y y + v k n a 9 5 j o 1 9 6 5 e a V z k c R T R M T p B 5 8 h F l 6 i B b l A T t R B F K X p G r + j N e r J e r H f r Y 9 F a s P K Z I / Q H 1 u c P B s W S n A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F C 1 d / i G S b 0 s 7 v 1 j 5 Z s w d Y 6 J H 4 z w = " &gt; A A A B + H i c b V B N S 8 N A E N 3 U r 1 o / G v X o Z b E V P Z W k C H o s e P H g o Y L 9 g D a E z X b T L t 1 s w u 5 E r K G / x I s H R b z 6 U 7 z 5 b 9 y 2 O W j r g 4 H H e z P M z A s S w T U 4 z r d V W F v f 2 N w q b p d 2 d v f 2 y / b B Y V v H q a K s R W M R q 2 5 A N B N c s h Z w E K y b K E a i Q L B O M L 6 e + Z 0 H p j S P 5 T 1 M E u Z F Z C h 5 y C k B I / l 2 u d o H 9 g h B m C X T M / + 2 6 t s V p + b M g V e J m 5 M K y t H 0 7 a / + I K Z p x C R Q Q b T u u U 4 C X k Y U c C r Y t N R P N U s I H Z M h 6 x k q S c S 0 l 8 0 P n + J T o w x w G C t T E v B c / T 2 R k U j r S R S Y z o j A S C 9 7 M / E / r 5 d C e O V l X C Y p M E k X i 8 J U Y I j x L A U 8 4 I p R E B N D C F X c 3 I r p i C h C w W R V M i G 4 y y + v k n a 9 5 j o 1 9 6 5 e a V z k c R T R M T p B 5 8 h F l 6 i B b l A T t R B F K X p G r + j N e r J e</formula><p>Interaction-based Metric Function. The representationbased metric function mixes an expert instance's papers together, which may suffer from semantic drift. Because if an expert publishes a large number of papers in multiple topics, two expert instances sampled from the same expert may include papers of different topics, but only a few similar papers. For this case, mixing all the paper embeddings of each expert instance will dilute the effect of the truly similar papers between two expert instances. To deal with the problem, we propose an interaction-based metric function to compare each pair of paper embeddings of two expert instances instead of comparing the embeddings of two expert instances. This similar idea is widely used in information retrieval to capture the exact and soft matches between a query and a candidate document <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b39">[40]</ref>. Formally, we utilize Eq.( <ref type="formula" target="#formula_14">2</ref>) to obtain a set of paper embeddings {p m } L m=1 and {p n } L n=1 for I e and I e respectively, and then compute a similarity matrix S between the two embedding sets. Each element s mn in S is computed by s mn = p m − p n 2 , standing for the Euclidean distance between the m-th paper and n-th paper in two expert instances. Then we apply an aggregation function to extract similarity patterns from the similarity matrix S. As the papers of expert instances are order independent, we adopt a RBF kernel aggregation function <ref type="bibr" target="#b39">[40]</ref>, paying attention to how many similar pairs of papers within two expert instances, to extract the similarity patterns. Specifically, we first transfer each s mn into [0, 1) by the tanh function (Eq.( <ref type="formula" target="#formula_31">5</ref>)), and then transform it into a K-dimensional distribution (Eq.( <ref type="formula" target="#formula_33">7</ref>)), each k-th element of which is converted by the k-th RBF kernel with mean µ k and variance σ k (Eq.( <ref type="formula" target="#formula_32">6</ref>)), indicating how likely s mn corresponds to the k-th similarity pattern. Then we sum up all the distributions over each row to represent the accumulated likelihood of the similarities between the m-th paper in l e and all the papers in I e , and further sum up all the rows into φ(e, e ) to represent the similarity patterns between I e and I e (Eq.( <ref type="formula" target="#formula_34">8</ref>)). The log operation before the outermost summation in Eq.( <ref type="formula" target="#formula_34">8</ref>) is to reduce the negative influence of the different number of papers in different I e for a given I e . s mn = p m − p n 2 , (4) smn = tanh(s mn ), </p><formula xml:id="formula_31">K k (s mn ) = exp − (s mn − µ k ) 2 2σ 2 k ,<label>(5)</label></formula><formula xml:id="formula_32">K(s mn ) = [K 1 (s mn ), • • • , K K (s mn )] ,<label>(6)</label></formula><p>The kernel with µ = 0 and σ → 0 only considers the exact matches between two papers, but others such as µ = 0.5, measure the semantic similarity between two papers. Finally, we apply a MLP layer f (e, e ) = MLP(φ(e, e ))</p><p>to get the similarity between the expert instances I e and I e .</p><p>Q4: Loss Function. We leverage the triplet loss instead of the widely used contrastive loss <ref type="bibr" target="#b14">[15]</ref> in our problem. As mentioned by <ref type="bibr" target="#b49">[50]</ref>, the contrastive loss encourages different expert instances sampled from the same expert to be projected into a single point in the embedding space. Since many experts in AMiner publish papers on different topics, it is weird to force different papers into a single point. On the contrary, triplet loss maintains a distance between positive and negative pairs, without optimizing the absolute positions of the positive and negative pairs as the contrastive loss does. Thus the triplet loss is a better choice for contrastive learning over experts. Specifically, for each anchor instance I e sampled from the expert e, the positive counterpart I + e is sampled from the same expert e, while a negative counterpart I − e is sampled from a different expert from e. I e and I + e comprise a positive pair. I e and I − e comprise a negative pair. Given a set of triplets {(I e , I + e , I − e )}, the triplet loss function is defined as:</p><formula xml:id="formula_36">L pre-train (θ g , θ f ) = (Ie,I + e ,I − e ) max{0, m + f (e, e − ) − f (e, e + )},<label>(10)</label></formula><p>where m is a margin enforced between positive pairs and negative pairs, and θ g , θ f indicate the parameters of g and f respectively. Note to avoid trivial solutions, two instances within a positive pair are sampled without replacement to remove the overlap between the two instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Adversarial Fine-tuning Module</head><p>The pre-training module can be directly applied on the external sources for expert linking. Specifically, given an external expert ẽ, we use the pre-trained expert encoder g to encode ẽ and apply the pre-trained metric function f to estimate the similarity between ẽ and each candidate expert e in AMiner. viewed as the linking result of ẽ. However, the morphology, syntax, topics of the external support information may be significantly different from the papers in AMiner, which encourages fine-tuning the pre-trained module to improve its transferability.</p><p>Recently, adversarial learning is widely adopted to achieve domain adaptation <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b32">[33]</ref>. The basic idea is, given a data sample, to design a generator to extract features for it, a classifier to predict the task-related label of it, and a domain discriminator to identify the source of it <ref type="bibr" target="#b12">[13]</ref>. By trading-off the minimization of the task prediction loss and the maximization of the domain classification loss, the framework attempts to learn a generator that maps a sample from either the source or the target domain into a representation allowing the task predictor to classify source samples accurately, but crippling the ability of the domain classifier to distinguish the source of the sample.</p><p>We design an adversarial learning algorithm to fine-tune g and f to make them adapt to the external data. The specific generator, domain discriminator, task predictor and the corresponding loss functions are explained as follows.</p><p>Generator. We use the pre-trained expert encoder g as the shared generator to extract the similar features as the AMiner experts from the external experts. Meanwhile, we create a private generator by the same Eq.( <ref type="formula" target="#formula_14">2</ref>) to only extract the domain-specific features from the external experts. No private generator is executed on AMiner experts, as they are the targets to be aligned from the other external experts. To encourage the shared generator and private generator to encode different aspects of the external experts, following <ref type="bibr" target="#b23">[24]</ref>, we introduce orthogonality constraints as a difference loss:</p><formula xml:id="formula_37">L diff (θ shared g , θ private g ) = Next i=1 ||g shared (s i ) T g private (s i )|| 2 F ,<label>(11)</label></formula><p>where ||.|| 2 F is the squared Frobenius norm, s i is the ith support information of the external source, N ext is the number of the pieces of the support information from the external source, g shared is the shared generator with θ shared g as the parameters, and g private and θ private g correspond to the shared generator. Domain Discriminator. To detach the external private features from the shared feature space, we design a domain discriminator to distinguish the sources of the shared features. If the discriminator cannot distinguish the sources of the shared features, it indicates the features shared with the AMiner source are extracted from the external sources. Specifically, given a piece of support information s i from either the AMiner source or the external source, we use the shared generator g shared to extract its shared features, and apply a classifier h to predict whether it is from the external source or the AMiner source. We minimize the following loss function to confuse the classifier h via Gradient Reversed Layer <ref type="bibr" target="#b12">[13]</ref>, such that it cannot distinguish the source of any support information:</p><formula xml:id="formula_38">L adv (θ shared g , θ h ) = NAMiner i=0 log(p i ) + Next i=0 log(1 − pi ),<label>(12)</label></formula><formula xml:id="formula_39">pi = h g shared (s i ) = MLP g shared (s i ) ,</formula><p>where pi indicates the probability of the support informa- tion s i being from the AMiner source. N AMiner refers to the number of the papers in AMiner. The classifier h is instantiated into a MLP layer.</p><p>Task Predictor. We define two task predictors. The first one is the same as the pre-training task (Cf. L pre-traing in Eq.( <ref type="formula" target="#formula_36">10</ref>)) defined only on the AMiner source to guide the learning of the shared generator. The second one is defined only on the external source to predict whether the private features extracted by the private generator are from the external source or not. If the classifier can predict the source correctly, it indicates the private features are detached from the shared features. We minimize the following loss function to enhance the predictive ability of the classifier h, so that it can distinguish the source of any external private information:</p><formula xml:id="formula_40">L ext (θ private g , θ h ) = − Next i=0 log(1 − pi ), pi = h(g private (ẽ i )),<label>(13)</label></formula><p>where pi indicates the probability of the support informa- tion s i encoded by g private being from the AMiner source. The classifier h is the same as Eq. <ref type="bibr" target="#b11">(12)</ref>.</p><p>Overall Loss Function. We combine the loss functions by:</p><formula xml:id="formula_41">L(θ shared g , θ private g , θ f , θ h ) = L pre-train +αL adv +βL diff +γL ext , (<label>14</label></formula><formula xml:id="formula_42">)</formula><p>where α, β and γ are trade-off hyper-parameters. Specifically, we first optimize Eq.( <ref type="formula" target="#formula_36">10</ref>) on AMiner experts to obtain an expert encoder g shared and a metric function f by pretraining. Then by incorporating additional external experts, we optimize Eq.( <ref type="formula" target="#formula_41">14</ref>) to fine-tune g shared and f by adversarial learning. Algorithm 1 presents the whole training process of the proposed COAD. The fine-tuned θ shared g and θ f are used to represent and measure the similarity between AMiner and external experts.</p><p>Discussions. Most of the existing endeavors assume each domain is comprised of domain-agnostic features and domain-specific features, so they learn a shared generator and a private generator for each domain <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b32">[33]</ref>. However, in our problem, as the purpose is to link different external experts to a single source, i.e., AMiner experts, we need to extract the similar features as AMiner experts from external experts as much as possible, to allow the pre-trained metric function on AMiner to better capture the similarity patterns between external experts and AMiner experts. Thus we only create a private generator for the external experts to get rid of the dissimilar features with AMiner experts. The shared generator is to keep all the features of AMiner experts, but to extract partial features from the external experts.</p><p>In addition, unlike the multi-task learning tasks <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b32">[33]</ref>, the external sources are unlabeled, thus we introduce a novel domain prediction task purely on the external private features to better separate them from the shared features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we evaluate the proposed expert linking model on three datasets. We first evaluate the representation capacity of the pre-training module by the intrinsic tasks of author identification and paper clustering on AMiner. Then we evaluate the transferability of the fine-tuning module by the extrinsic task of external expert linking, i.e., linking external names from the news or the users from LinkedIn.com to AMiner experts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets.</head><p>Three datasets are introduced as below. Table <ref type="table" target="#tab_2">1</ref> summarizes the data statistics.</p><p>• AMiner <ref type="bibr" target="#b2">[3]</ref>. We employ OAG-WhoIsWho 7 , the largest name disambiguation dataset collected from AMiner, as the anchor dataset to be aligned. The dataset contains 421 names, 45,187 experts and 399,255 papers in English.</p><p>The relationships between the papers and the experts are annotated by humans. • News. We collect 20,658 news articles from several Chinese technique websites such as news.sciencenet.cn, jiqizhixin.com, etc. We extract 1,824 person names from these articles by a Chinese NER tool 8 and then link them to the experts in AMiner by a majority voting of three annotators' results. We extract six sentences before and after a name in a news article as the support information.</p><p>• LinkedIn <ref type="bibr" target="#b48">[49]</ref>. This dataset consists of 50,000 LinkedIn English homepages, 1,665 of which are linked to AMiner experts by human annotators via majority voting. We use affiliation, skills and summary in a homepage as the support information. The affiliation and the concatenated keywords in skills are separate pieces of support information. The long-text summary is divided into multiple pieces of support information. Name Variants. Given a person to be linked, the candidates have variant names obtained by moving the last name to the first or keeping the initials of the names except for the last name. For example, the variants of "Jing Zhang" include "Zhang Jing", "J Zhang" and "Z Jing". The queried names in the News articles are translated into English before linking. Advanced blocking techniques <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b33">[34]</ref> can be considered in the future to improve the quality of the candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation of the Pre-training Module.</head><p>We adopt two widely-studied tasks, author identification <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b46">[47]</ref> and paper clustering <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b49">[50]</ref>, as the intrinsic tasks to directly evaluate the representation capacity of the pre-training module without fine-tuning. Both of the evaluations are performed only on AMiner. This section explains the experimental settings and results on these two tasks. More details about the pre-training and test settings are left in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Author Identification</head><p>Task Description. Author identification follows the second task of a name disambiguation competition 9 to assign a new paper to the right author in AMiner. Thus, we adopt the same test set as the competition. After pre-training, we estimate the similarity score between a new paper and each candidate expert in the test set by the pre-trained encoder g and the metric function f . Then we rank all the candidate experts by the similarity scores and return the top-ranked expert as the right answer.</p><p>Evaluation Metrics. We use HitRatio@K (HR@K, K=1,3) to measure the proportion of the correctly assigned experts ranked in top K candidates, and use MRR to measure the average reciprocal ranks of the correctly assigned experts.</p><p>Baselines. The standard entity linking task links the mentions extracted from the unstructured text to the entities in Wikipedia or Freebase <ref type="bibr" target="#b30">[31]</ref>. Compared with it, the mentions and entities of author identification refer to papers and experts, which are more complex objects, making it infeasible to directly apply the standard entity linking methods to identify experts for papers. The baselines addressing author identification specially are as follows:</p><p>• GBDT <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b22">[23]</ref>: is a frequently adopted model to address the same problem in KDD Cup 2013 <ref type="bibr" target="#b31">[32]</ref>. We train a GBDT  <ref type="table">6</ref> to measure the similarity between a paper and an expert.</p><p>• Camel <ref type="bibr" target="#b46">[47]</ref>: represents a paper by GRU-embeded title and keywords and represents an expert by an one-hot embedding. The metric function is the Euclidean distance between the embeddings of a paper and an expert. Camel is optimized by the same triplet loss as Eq. <ref type="bibr" target="#b9">(10)</ref>. To enable Camel to align unseen experts in the test set, we represent each unseen expert by averaging its paper embeddings.</p><p>• HetNetE <ref type="bibr" target="#b4">[5]</ref>: is similar to Camel except that each paper is represented by the author names, affiliations, venues in addition to the title and keywords.</p><p>• CONNA <ref type="bibr" target="#b2">[3]</ref>: is also an interaction-based model, but the basic interaction matrix is built between the token embeddings of two attributes such as titles or authors. Then the matrices of different attributes are aggregated as the interactions between the new paper and a paper in an expert, finally different papers interactions are aggregated as the interactions between the new paper and an expert.</p><p>Experimental Results. Table <ref type="table" target="#tab_3">2</ref> shows the performance of the pre-training module evaluated by author identification on AMiner. It is averaged over 30 runs with random initailizations of model parameters.</p><p>From the results, we can see that COAD (pre-train) outperforms Camel and HetNetE by 31.6∼32.1% in HR@1. Instead of the interaction-based metric function in COAD (pre-train), Camel and HetNetE adopt a representationbased metric function, which embed a paper and an expert respectively, and compute a single similarity score between them. They fail to capture the fine-grained similarities between papers and experts. Besides, one-hot embeddings for experts aren't able to address unseen experts. Although we avoid the issue by averaging the paper embeddings of an expert, the testing deviates from the training process.</p><p>COAD (pre-train) is comparable to GBDT and CONNA. GBDT, extracting the interaction features between a paper and an expert, and CONNA, computing the interactions of the tokens between a paper and an expert, perform much better than the representation-based Camel and HetNetE (+29.1∼33.4% in HR@1), which also demonstrates the effectiveness of the interaction-based strategy. CONNA even outperforms COAD (pre-train) by 1.3% in HR@1, because CONNA is more carefully designed, which first builds the basic interactions of the tokens between two attributes and then aggregates the similarities of attributes and the similarities of the papers together by attention mechanisms. How- ever, GBDT and CONNA, the models specially designed for author identification on AMiner, cannot be adapted to other domains. Instead of directly computing the interactions between tokens, COAD (pre-train) first encodes each paper into a basic embedding and then computes the interactions between papers and experts. This expert encoder g can be generalized to other domains such as news articles to encode sentences or LinkedIn homepages to encode users' attributes. Moreover, instead of optimizing the relationship between a paper and an expert by GBDT and CONNA, COAD (pre-train) performs a contrastive learning between two expert instances, which can result in a more general encoder g to benefit multiple downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Paper Clustering</head><p>Task Description. Paper clustering, aiming at clustering papers belonging to a same person together, is a classical yet important task on online academic platforms. Since it is exactly the first task of the name disambiguation competition 10 , we adopt the same test set as the competition.</p><p>We cluster all the papers with a same author name in the test set by hierarchical agglomerative clustering algorithm (HAC) based on the paper embeddings generated by the expert encoder g.</p><p>Evaluation Metrics. We use pairwise Precision, Recall, and F1-score (P-Pre., P-Rec, and P-F1) to evaluate the clustering results of each name, and then calculate the macro averaged score of each metric over all the names.</p><p>Baselines. We compare with three state-of-the-art methods for paper clustering. They all use HAC to cluster papers. For a fair comparison, we assume the true number of clusters within each name is known. The differences lie in the embedding mechanisms.</p><p>• Louppe et al. <ref type="bibr" target="#b25">[26]</ref>: trains a similarity function based on hand-crafted pairwise features to directly measure the similarity between papers without embedding them. • Zhang et al. <ref type="bibr" target="#b45">[46]</ref>: constructs an expert-expert network (two coauthors are linked), an expert-paper network (a paper is linked to its author) and a paper-paper network (two papers coauthored by the same author are linked) for each name. Then they perform graph embedding by preserving node connectivity on each network to learn paper embeddings. • G/L-Emb <ref type="bibr" target="#b49">[50]</ref>: first learns paper embeddings on a global paper-paper network (two papers coauthored by the same 10. https://biendata.xyz/competition/aminer2019/    still performs the best because of the contrastive learning between expert instances, which results in a general encoder g to be adapted to various downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis of the Pre-training Module.</head><p>Ablation Study. To verify the efficacy of different components in COAD (pre-train), we make four variants:</p><p>• Unsupervised: directly uses the pre-trained multi-lingual BERT to obtain a CLS embedding for each paper, and averages all the paper embeddings for an expert. Euclidean distance is to measure the similarity between two papers or between a paper and an expert. • Paper-paper pseudo labels: samples paper-paper pseudo labels instead of the pairs of two expert instances, where two papers are viewed as a positive pair if they are published by a same expert and a negative pair otherwise. • Paper-expert pseudo labels: samples paper-expert pseudo labels instead of two expert instances, where a paperexpert pair is a positive pair if the paper is published by the expert and a negative pair otherwise.</p><p>• Representation metric function: uses the representationbased metric function formulated by Eq.( <ref type="formula" target="#formula_30">3</ref>) instead of the interaction metric function in our module.</p><p>We present the performance of the above variant models in Table <ref type="table" target="#tab_3">2</ref> and Table <ref type="table" target="#tab_4">3</ref>. First, we can see that the unsupervised model performs the worst (-18.5% in HR@1 and -33.0% in Pairwise-F1 compared with COAD (pre-train)), which indicates that BERT without any fine-tuning fails to measure the inherent correlations between papers or experts. Second, both the paper-paper pseudo labels and the paper-expert pseudo labels underperform our module by 0.6∼3.4% in HR@1 and by 0.6∼4.1% in Pairwise-F1, which indicates that contrasting two expert instances by COAD (pre-train) can result in better representations of papers and experts. Finally, the representation-based metric function underperforms our module by 2.8% in HR@1 and by 9.0% in Pairwise-F1, which emphasizes the advantage of the interaction-based metric function.</p><p>Effect of Paper Sampling. We investigate how the maximal number L of the papers sampled for an expert instance affects the performance of COAD (pre-train). We vary L (#Papers) from 1 to 15 with interval 3 and present the performance of both author identification and paper clustering in Figure <ref type="figure" target="#fig_21">4</ref>(a). We can see that when #Papers equals to 6, COAD (pre-train) performs the best in both the tasks. Either too few or two many papers sampled for an expert will harm the performance. Too few papers may result in particularly dissimilar expert instances even if they are sampled from a same expert, making it difficult to distinguish the positive pairs of expert instances from the negative pairs. On the contrary, two many papers will result in particularly close expert instances if they are sampled from a same expert. The loss of these positive pairs will become zero, leading to homogeneously pushing all the expert instances away from each other, which is non-intuitive to justify <ref type="bibr" target="#b44">[45]</ref>.</p><p>Effect of Negative Sampling. The number of negative instances (#Negative instances) also influences the model performance. Many endeavors <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b14">[15]</ref> have shown that more negative instances can result in better performance. To verify the conclusion, we vary the number of the negative instances from 1 to 9 with interval 2 in both the tasks and present the results in Figure <ref type="figure" target="#fig_21">4</ref>(b). We can see that when the number of the negative instances increases from 1 to 9, COAD (pre-train) consistently improves HR@1 by 7.2% on author identification and improves Pairwise-F1 by 5.3% on paper clustering. Since all the experiments are conducted on a single NVIDIA Tesla P40 with 24 GB memory, more than 9 negative instances for each positive instance will exceed the capacity of GPUs even if the batch size is set to 1. Thus we set the number of the negative instances as 9. But it is possible to improve the performance when GPUs with larger memory size are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation of the Adversarial Fine-tuning Module.</head><p>We also evaluate the transferable ability of our model by external expert linking. We fine-tune COAD (pre-train) on both AMiner and the unlabeled external dataset, and evaluate it on the test set. Specifically, we represent each external person and AMiner expert by the fine-tuned encoder g and measure the similarity of them by the fine-tuned metric function f . HR@K and MRR are also used as the metrics to evaluate the ranked AMiner candidates.</p><p>Baselines. We compare with three domain adaptation methods which transfer the encoder g and the metric function f from AMiner to the external data.</p><p>• Chain Pre-training <ref type="bibr" target="#b24">[25]</ref>: chains together a series of pretraining stages for domain adaptation. Following it, we first fine-tune the original BERT on both AMiner and the external data, then fine-tune it only on the external data, and finally train COAD (pre-train) based on the finedtuned BERT on AMiner data. • DANN <ref type="bibr" target="#b12">[13]</ref>: extracts domain-agnostic but not domainprivate features. Following it, we only use a shared generator to encode both AMiner and external experts. • ASP-MTL <ref type="bibr" target="#b23">[24]</ref>: extracts both the domain-shared and domain-private features. Following it, in addition to a shared generator, we create both AMiner private generator and the external private generator.</p><p>Experimental Results. Table <ref type="table" target="#tab_5">4</ref> shows the performance of linking from the external persons mentioned in news articles or from the LinkedIn users to AMiner experts. All the results are averaged over 30 runs with random initializations of the model parameters. We can see that the proposed COAD outperforms all the baselines by 0.1∼1.4% in terms of HR@1.</p><p>The Chain Pre-training baseline performs the worst, as it fine-tunes only the BERT encoder g but not the metric function f . Although DANN fine-tunes both g and f , it does not detach the private features from the shared features. On the contrary, ASP-MTL, creating both the private and shared encoders, outperforms the above two baselines. Compared with ASP-MTL, the proposed COAD creates the private encoder only for the external data but not for AMiner data.</p><p>Because the aim of external expert linking is to extract the similar features as AMiner experts from the external persons as much as possible, there is no need to extract the private features of AMiner experts. Moreover, we create a special loss function to predict whether the private features are from the external data or not, which makes the external private features special enough to be identified.   We present the performance of the above variant models in Table <ref type="table" target="#tab_5">4</ref>. Among them, the unsupervised model performs the worst (-9.9∼42.4% in HR@1 compared with COAD), as the original BERT isn't fine-tuned at all on the expert linking data. Although the BERT encoder g and the metric function f in AMiner-Only are fine-tuned on AMiner, they aren't fine-tuned on the external data at all, thus AMiner-Only still underperforms COAD by 0.7∼1.6%. Among the three model variants which remove different loss functions from COAD , COAD w/o L adv performs the worst (-3.1∼3.2% in HR@1 compared with COAD), which indicates the domain discriminator takes the most important role in the fine-tuning process. Both COAD w/o L diff and COAD w/o L ext underperform COAD by 0.3∼1.0% in HR@1, which demonstrates that both the loss functions in our adversarial fine-tuning module can enhance the model performance. We also observe that COAD w/o L diff performs slightly better than COAD w/o L ext by 0.2∼0.3% in HR@1. Because we expect more about distinguishing the external private features from the AMiner features than forcing the private and the shared features of the external data orthogonal with each other.</p><p>Effect of Hyper-parameters. We investigate whether the values of α, β and γ, the weights to balance the four loss functions in Eq.( <ref type="formula" target="#formula_41">14</ref>), will affect the fine-tuning performance. We compare three settings, α, β, γ = 0 (i.e., AMiner-Only), α, β, γ = 1 and α, β, γ = 0.1, and present the results in Figure <ref type="figure" target="#fig_24">5</ref>(a). It is shown that the larger weights for adversarial learning harms the fine-tuning performance, which is even worse than the pure pre-training model AMiner-Only. Because with larger adversarial weights, the shared generator in the fine-tuning module will be trained more thoroughly. The representations of the AMiner experts will  be closer to those of the external persons. However, the metric function, being trained only on the pseudo labels of AMiner, will be harmed if it accepts the more externalperson-like experts as input.</p><p>Effect of Labels in the External Sources. We investigate whether the model performance will be improved when adding additional labeled data from the external sources. Specifically, we add 202 linkages from the persons mentioned in news articles to AMiner experts, and add 336 linkages from the LinkedIn users to AMiner linkages when fine-tuning our model. The prediction loss on these additional labels is the same as Eq.( <ref type="formula" target="#formula_36">10</ref>). The results in Figure <ref type="figure" target="#fig_24">5</ref>(b) show that a few labels from the external sources can indeed improve the external linking performance. Moreover, we observe that the performance growth is more obvious on news than on LinkedIn. Because the news articles are in different language from AMiner data. It is more difficult for the expert encoder and the metric function to be transferred to the external data without any labeled data. Thus the performance growth is more significant when adding labels on news data.</p><p>Visualization. In Figure <ref type="figure" target="#fig_27">6</ref>, we utilize t-SNE <ref type="bibr" target="#b26">[27]</ref> to visualize the papers from AMiner and the sentences from the news articles embedded by the expert encoder g trained before and after the adversarial fine-tuning. Figure <ref type="figure" target="#fig_27">6</ref>(a) indicates that when applying COAD (pre-train), the embedding distribution of the news articles is not aligned with that of the AMiner papers, whille Figure <ref type="figure" target="#fig_27">6</ref>(b) shows that COAD , after adversarial fine-tuning, can mitigate the domain shift.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Online Deployment</head><p>COAD has been deployed online for linking news articles to AMiner experts 11 . In the demo, each name extracted from a news article is linked to the corresponding AMiner expert with a brief introduction on the right of the news article. Due to the efficiency of the BERT model, the linkages from the news articles to AMiner experts are predicted by COAD offline. The efficiency issue will be addressed in future works. Since a name mention may not be linked to any AMiner experts, we train an additional classifier using the output similarity embeddings in Eq.( <ref type="formula" target="#formula_34">8</ref>) as features to predict whether the mention should be linked to the top-ranked expert by COAD or not <ref type="bibr" target="#b2">[3]</ref>.</p><p>11. https://newsminer.net/ExpertLinking/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>Entity Linking. Most of the works on entity linking focus on a standard setting which links the mentions from the unstructured text to the entities in a knowledge graph (often Wikipedia of Freebase) <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>, where the mentions and entities are usually represented by CNN <ref type="bibr" target="#b27">[28]</ref>, LSTM <ref type="bibr" target="#b13">[14]</ref>, bag-of-words embeddings <ref type="bibr" target="#b11">[12]</ref>, etc. Recent work has also investigated a cross-domain setting, which links mentions from different types of text such as blogposts and news articles to Wikipedia. They train a model only on the labeled mentions in Wiki-pages and directly apply it on different text <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b21">[22]</ref>. However, the gap between the support information of AMiner experts and the external persons is much more significant than that between Wiki-pages and other text, which prevents us from directly using the pre-trained model on AMiner to any external sources. Logeswaran et al. <ref type="bibr" target="#b24">[25]</ref> trains a BERT-based mentionentity linking model on the source domain and then adapt it to the target domain. Our work differs from it in two aspects: first, they have explicit labels on the source domain, while the linkage labels are unknown on AMiner; second, an expert comprised by multiple papers is more complex than an entity represented by a text description.</p><p>The problem is also related to Entity Alignment, which targets at aligning the entities coming from different knowledge graphs but indicating the equivalent ones. Factor graph model <ref type="bibr" target="#b48">[49]</ref>, TransE <ref type="bibr" target="#b34">[35]</ref>, graph convolution networks <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b47">[48]</ref> and BERT-based interaction matching model <ref type="bibr" target="#b36">[37]</ref> are studied to solve the problem, but most of them are in a supervised way.</p><p>Contrastive Learning. Contrastive learning is widely used to pre-train the representations of images or words, where an instance of an image is usually formed by transforming images <ref type="bibr" target="#b9">[10]</ref>, ordering patches <ref type="bibr" target="#b8">[9]</ref> or segmenting objects <ref type="bibr" target="#b28">[29]</ref>, and the pseudo-labels for words are usually formed by the contextual words or sentences <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b43">[44]</ref>. Different from them, in addition to learn expert representations, we also need to train a good metric function for matching experts by contrastive learning.</p><p>The proposed pre-training task of expert discrimination is related to name disambiguation, which aims to partition the given papers into a set of disjoint clusters to make each cluster correspond to a real world person <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b49">[50]</ref>. Expert discrimination is also related to the problem of author identification, which is to assign the authors to an anonymous paper <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b46">[47]</ref>. Both the tasks learn paper or author embeddings based on the ownership of papers to authors. Different from them, we follow the idea of contrastive learning to contrasting random expert instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adversarial Domain Adaptation.</head><p>Adversarial learning has been extensively studied for cross-lingual and crossdomain transfer <ref type="bibr" target="#b12">[13]</ref> such as word translation <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b20">[21]</ref>, text classification <ref type="bibr" target="#b23">[24]</ref> and relation extraction <ref type="bibr" target="#b32">[33]</ref> in two different domains. Different from the existing end-to-end adversarial learning, we adopt adversarial learning for finetuning, which is an independent process following a pretraining stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>We solve that problem of zero-shot expert linking from external sources to AMiner. We propose COAD consisting of a pre-training module and a fine-tuning module. The former one performs the pre-training task of expert discrimination by contrastive learning, which learns an expert encoder to capture the universal representation patters of experts and learns a metric function to capture the universal matching patterns between experts. The later one fine-tunes the functions on both AMiner and the external source to improve their transferability. Experimental results on AMiner and two external sources, news articles and LinkedIn homepages, demonstrate the superiority of the proposed model compared with the state-of-the-art baselines and variants. In the future, NIL recognition will be considered. It is also promising to design a better encoder than BERT to encode the structured and semi-structured support information of an expert. 1, 806/100 ≈ 18, we randomly sample 17 negative experts together the ground truth expert to comprise the candidate list for a queried paper. The maximal number of papers sampled for each candidate expert is set to 100. For testing, we first leverage the pre-trained expert encoder g to generate the paper embedding for each candidate expert, then use the pre-trained metric function f to measure the similarity between a paper and each candidate expert. Finally, we rank all the candidate experts for a paper according to their similarity scores and return the top expert as the expert to be linked.</p><p>• Paper Clustering: follows the first task of the name disambiguation competition 11 . We directly use its test set. For testing, we apply the pre-trained expert encoder g to get the paper embeddings and then use the HAC algorithm to cluster the papers belonging to a same expert together. For a fair comparison, the true number of clusters in each name are provided to the HAC algorithm.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Linking news articles to AMiner experts. The candidate AMiner experts of two ambiguous names are presented.</figDesc><graphic url="image-2.png" coords="1,295.06,388.37,78.83,107.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " N X D c Y r U 1 4 K h l a O F 1 k a S K b 0 k j b y g = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L L a C p 5 I U Q Y 8 F L 3 q r Y D + g D W W z n b R L N 5 u w u x F K 6 I / w 4 k E R r / 4 e b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y l s b G 5 t 7 x R 3 S 3 v 7 B 4 d H 5 e O T t o 5 T x b DF Y h G r b k A 1 C i 6 x Z b g R 2 E 0 U 0 i g Q 2 A k m t 3 O / 8 4 R K 8 1 g + m m m C f k R H k o e c U W O l T v V + k O G s O i h X 3 J q 7 A F k n X k 4 q k K M 5 K H / 1 h z F L I 5 S G C a p 1 z 3 M T 4 2 d U G c 4 E z k r 9 V G N C 2 Y S O s G e p p B F q P 1 u c O y M X V h m S M F a 2 p C E L 9 f d E R i O t p 1 F g O y N q x n r V m 4 v / e b 3 U h D d + x m W S G p R s u S h M B T E x m f 9 O h l w h M 2 J q C W W K 2 1 s J G 1 N F m b E J l W w I 3 u r L 6 6 R d r 3 l u z X u o V x p X e R x F O I N z u A Q P r q E B d 9 C E F j C Y w D O 8 w p u T O C / O u / O x b C 0 4 + c w p / I H z + Q O M Y o 7 9 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N X D c Y r U 1 4 K h l a O F 1 k a S K b 0 k j b y g = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L L a C p 5 I U Q Y 8 F L 3 q r Y D + g D W W z n b R L N 5 u w u x F K 6 I / w 4 k E Rr / 4 e b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y l s b G 5 t 7 x R 3 S 3 v 7 B 4 d H 5 e O T t o 5 T x b DF Y h G r b k A 1 C i 6 x Z b g R 2 E 0 U 0 i g Q 2 A k m t 3 O / 8 4 R K 8 1 g + m m m C f k R H k o e c U W O l T v V + k O G s O i h X 3 J q 7 A F k n X k 4 q k K M 5 K H / 1 h z F L I 5 S G C a p 1 z 3 M T 4 2 d U G c 4 E z k r 9 V G N C 2 Y S O s G e p p B F q P 1 u c O y M X V h m S M F a 2 p C E L 9 f d E R i O t p 1 F g O y N q x n r V m 4 v / e b 3 U h D d + x m W S G p R s u S h M B T E x m f 9 O h l w h M 2 J q C W W K 2 1 s J G 1 N F m b E J l W w I 3 u r L 6 6 R d r 3 l u z X u o V x p X eR x F O I N z u A Q P r q E B d 9 C E F j C Y w D O 8 w p u T O C / O u / O x b C 0 4 + c w p / I H z + Q O M Y o 7 9 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N X D c Y r U 1 4 K h l a O F 1 k a S K b 0 k j b y g = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L L a C p 5 I U Q Y 8 F L 3 q r Y D + g D W W z n b R L N 5 u w u x F K 6 I / w 4 k E R r / 4 e b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y l s b G 5 t 7 x R 3 S 3 v 7 B 4 d H 5 e O T t o 5 T x b DF Y h G r b k A 1 C i 6 x Z b g R 2 E 0 U 0 i g Q 2 A k m t 3 O / 8 4 R K 8 1 g + m m m C f k R H k o e c U W O l T v V + k O G s O i h X 3 J q 7 A F k n X k 4 q k K M 5 K H /1 h z F L I 5 S G C a p 1 z 3 M T 4 2 d U G c 4 E z k r 9 V G N C 2 Y S O s G e p p B F q P 1 u c O y M X V h m S M F a 2 p C E L 9 f d E R i O t p 1 F g O y N q x n r V m 4 v / e b 3 U h D d + x m W S G p R s u S h M B T E x m f 9 O h l w h M 2 J q C W W K 2 1 s J G 1 N F m b E J l W w I 3 u r L 6 6 R d r 3 l u z X u o V x p X e R x F O I N z u A Q P r q E B d 9 C E F j C Y w D O 8 w p u T O C / O u / O x b C 0 4 + c w p / I H z + Q O M Y o 7 9 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N X D c Y r U 1 4 K h l a O F 1 k a S K b 0 k j b y g = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L L a C p 5 I U Q Y 8 F L 3 q r Y D + g D W W z n b R L N 5 u w u x F K 6 I / w 4 k E R r / 4 e b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y l s b G 5 t 7 x R 3 S 3 v 7 B 4 d H 5 e O T t o 5 T x b D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>e H O U 8 + K 8 O x / z 1 h U n m z m C P 3 A + f w C p 3 I + a &lt; / l a t e x i t &gt; I e &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U v D e m u i b s 0 0 Q P / L 0 Q L c b c e A c A b I = " &gt; A A A B 8 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y C J 4 M e w G Q Y 8 B L 3 q L Y B 6 S x D A 7 6 U 2 G z O w u M 7 N C W P I V X j w o 4 t X P 8 e b f O E n 2 o I k F D U V V N 9 1 d f i y 4 N q 7 7 7 a y s r q 1 v b O a 2 8 t s 7 u 3 v 7 h Y P D h o 4 S x b D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>P z P a y c m u O q m P I w T g y G b L w o S Q U x E p t + T P l f I j B h b Q p n i 9 l b C h l R R Z m x G e R u C t / j y M m l U y p 5 b 9 u 4 q x e p F F k c O j u E E z s C D S 6 j C D d S g D g w k P M M r v D n K e X H e n Y 9 5 6 4 q T z R z B H z i f P 6 z u j 5 w = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U v D e m u i b s 0 0 Q P / L 0 Q L c b c e A c A b I = " &gt; A A A B 8 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y C J 4 M e w G Q Y 8 B L 3 q L Y B 6 S x D A 7 6 U 2 G z O w u M 7 N C W P I V X j w o 4 t X P 8 e b f O E n 2 o I k F D U V V N 9 1 d f i y 4 N q 7 7 7 a y s r q 1 v b O a 2 8 t s 7 u 3 v 7 h Y P D h o 4 S x b D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>P z P a y c m u O q m P I w T g y G b L w o S Q U x E p t + T P l f I j B h b Q p n i 9 l b C h l R R Z m x G e R u C t / j y M m l U y p 5 b 9 u 4 q x e p F F k c O j u E E z s C D S 6 j C D d S g D g w k P M M r v D n K e X H e n Y 9 5 6 4 q T z R z B H z i f P 6 z u j 5 w = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U v D e m u i b s 0 0 Q P / L 0 Q L c b c e A c A b I = " &gt; A A A B 8 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y C J 4 M e w G Q Y 8 B L 3 q L Y B 6 S x D A 7 6 U 2 G z O w u M 7 N C W P I V X j w o 4 t X P 8 e b f O E n 2 o I k F D U V V N 9 1 d f i y 4 N q 7 7 7 a y s r q 1 v b O a 2 8 t s 7 u 3 v 7 h Y P D h o 4 S x b D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>P z P a y c m u O q m P I w T g y G b L w o S Q U x E p t + T P l f I j B h b Q p n i 9 l b C h l R R Z m x G e R u C t / j y M m l U y p 5 b 9 u 4 q x e p F F k c O j u E E z s C D S 6 j C D d S g D g w k P M M r v D n K e X H e n Y 9 5 6 4 q T z R z B H z i f P 6 z u j 5 w = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U v D e m u i b s 0 0 Q P / L 0 Q L c b c e A c A b I = " &gt; A A A B 8 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y C J 4 M e w G Q Y 8 B L 3 q L Y B 6 S x D A 7 6 U 2 G z O w u M 7 N C W P I V X j w o 4 t X P 8 e b f O E n 2 o I k F D U V V N 9 1 d f i y 4 N q 7 7 7 a y s r q 1 v b O a 2 8 t s 7 u 3 v 7 h Y P D h o 4 S x b D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>P z P a y c m u O q m P I w T g y G b L w o S Q U x E p t + T P l f I j B h b Q p n i 9 l b C h l R R Z m x G e R u C t / j y M m l U y p 5 b 9 u 4 q x e p F F k c O j u E E z s C D S 6 j C D d S g D g w k P M M r v D n K e X H e n Y 9 5 6 4 q T z R z B H z i f P 6 z u j 5 w = &lt; / l a t e x i t &gt; , e + ) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t K O H U 2 s x v G U R H M e u U T N d Y I P y 3 n c = " &gt; A A A B 8 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 B F u h o p S k C H o s e P F Y w X 5 g G 8 t m O 2 k X N 5 u w u x F K 6 L / w 4 k E R r / 4 b b / 4 b t 2 0 O 2 v p g 4 P H e D D P z / J g z p R 3 n 2 8 q t r K 6 t b + Q 3 C 1 v b O 7 t 7 x f 2 D l o o S S b F J I x 7 J j k 8 U c i a w q Z n m 2 I k l k t D n 2 P Y f r 6 d + + w m l Y p G 4 0 +</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>8 9 a c l c 0 c w h 9 Y n z 9 L B Y 9 M &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t K O H U 2 s x v G U R H M e u U T N d Y I P y 3 n c = " &gt; A A A B 8 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 B F u h o p S k C H o s e P F Y w X 5 g G 8 t m O 2 k X N 5 u w u x F K 6 L / w 4 k E R r / 4 b b / 4 b t 2 0 O 2 v p g 4 P H e D D P z / J g z p R 3 n 2 8 q t r K 6 t b + Q 3 C 1 v b O 7 t 7 x f 2 D l o o S S b F J I x 7 J j k 8 U c i a w q Z n m 2 I k l k t D n 2 P Y f r 6 d + + w m l Y p G 4 0 +</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>8 9 a c l c 0 c w h 9 Y n z 9 L B Y 9 M &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t K O H U 2 s x v G U R H M e u U T N d Y I P y 3 n c = " &gt; A A A B 8 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 B F u h o p S k C H o s e P F Y w X 5 g G 8 t m O 2 k X N 5 u w u x F K 6 L / w 4 k E R r / 4 b b / 4 b t 2 0 O 2 v p g 4 P H e D D P z / J g z p R 3 n 2 8 q t r K 6 t b + Q 3 C 1 v b O 7 t 7 x f 2 D l o o S S b F J I x 7 J j k 8 U c i a w q Z n m 2 I k l k t D n 2 P Y f r 6 d + + w m l Y p G 4 0 +</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>8 9 a c l c 0 c w h 9 Y n z 9 L B Y 9 M &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t K O H U 2 s x v G U R H M e u U T N d Y I P y 3 n c = " &gt; A A A B 8 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 B F u h o p S k C H o s e P F Y w X 5 g G 8 t m O 2 k X N 5 u w u x F K 6 L / w 4 k E R r / 4 b b / 4 b t 2 0 O 2 v p g 4 P H e D D P z / J g z p R 3 n 2 8 q t r K 6 t b + Q 3 C 1 v b O 7 t 7 x f 2 D l o o S S b F J I x 7 J j k 8 U c i a w q Z n m 2 I k l k t D n 2 P Y f r 6 d + + w m l Y p G 4 0 +</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>8 9 a c l c 0 c w h 9 Y n z 9 L B Y 9 M &lt; / l a t e x i t &gt; f(e, e ) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Q A s G H 2 c l X A u u 3 N r y g j R 0 X G 7 Z a S w = " &gt; A A A B 8 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 B F u h g p a k C H o s e P F Y w X 5 g G 8 t m O 2 k X N 5 u w u x F K 6 L / w 4 k E R r / 4 b b / 4 b t 2 0 O 2 v p g 4 P H e D D P z / J g z p R 3 n 2 8 q t r K 6 t b + Q 3 C 1 v b O 7 t 7 x f 2 D l o o S S b F J I x 7 J j k 8 U c i a w q Z n m 2 I k l k t D n 2 P Y f r 6 d + + w m l Y p G 4 0 +</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>8 9 a c l c 0 c w h 9 Y n z 9 O E Y 9 O &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Q A s G H 2 c l X A u u 3 N r y g j R 0 X G 7 Z a S w = " &gt; A A A B 8 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 B F u h g p a k C H o s e P F Y w X 5 g G 8 t m O 2 k X N 5 u w u x F K 6 L / w 4 k E R r / 4 b b / 4 b t 2 0 O 2 v p g 4 P H e D D P z / J g z p R 3 n 2 8 q t r K 6 t b + Q 3 C 1 v b O 7 t 7 x f 2 D l o o S S b F J I x 7 J j k 8 U c i a w q Z n m 2 I k l k t D n 2 P Y f r 6 d + + w m l Y p G 4 0 +</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>8 9 a c l c 0 c w h 9 Y n z 9 O E Y 9 O &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Q A s G H 2 c l X A u u 3 N r y g j R 0 X G 7 Z a S w = " &gt; A A A B 8 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 B F u h g p a k C H o s e P F Y w X 5 g G 8 t m O 2 k X N 5 u w u x F K 6 L / w 4 k E R r / 4 b b / 4 b t 2 0 O 2 v p g 4 P H e D D P z / J g z p R 3 n 2 8 q t r K 6 t b + Q 3 C 1 v b O 7 t 7 x f 2 D l o o S S b F J I x 7 J j k 8 U c i a w q Z n m 2 I k l k t D n 2 P Y f r 6 d + + w m l Y p G 4 0 +</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>8 9 a c l c 0 c w h 9 Y n z 9 O E Y 9 O &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Q A s G H 2 c l X A u u 3 N r y g j R 0 X G 7 Z a S w = " &gt; A A A B 8 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 B F u h g p a k C H o s e P F Y w X 5 g G 8 t m O 2 k X N 5 u w u x F K 6 L / w 4 k E R r / 4 b b / 4 b t 2 0 O 2 v p g 4 P H e D D P z / J g z p R 3 n 2 8 q t r K 6 t b + Q 3 C 1 v b O 7 t 7 x f 2 D l o o S S b F J I x 7 J j k 8 U c i a w q Z n m 2 I k l k t D n 2 P Y f r 6 d + + w m l Y p G 4 0 +</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>8 9 a c l c 0 c w h 9 Y n z 9 O E Y 9 O &lt; / l a t e x i t &gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Model overview. By discriminating the positive expert instance pair (Ie, I + e ) from the negative instance pair (Ie, I − e ), we train an expert encoder g and a metric function f , which are then fine-tuned on the unlabeled external experts by adversarial learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>w H P 8 A p v z q P z 4 r w 7 H 4 v W N S e f O Y E / c D 5 / A B g N j 0 k = &lt; / l a t e x i t &gt; s L0 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m L B 0 B F Y M y b 5 W / U u X 4 e w 2 L p P Q y Q g = " &gt; A A A B 7 3 i c b V A 9 S w N B E J 3 z M 8 a v q K X N Y i J Y h b s g a B m w s b C I Y D 4 g O c L e Z i 9 Z s r d3 7  s 4 J 4 c i f s L F Q x N a / Y + e / c Z N c o Y k P B h 7 v z T A z L 0 i k M O i 6 3 8 7 a + s b m 1 n Z h p 7 i 7 t 3 9 w W D o 6 b p k 4 1 Y w 3 W S x j 3 Q m o 4 V I o 3 k S B k n c S z W k U S N 4 O x j c z v / 3 E t R G x e s B J w v 2 I D p U I B a N o p U 7 F 9 L M 7 d 1 r p l 8 p u 1 Z 2 D r B I v J 2 X I 0 e i X v n q D m K U R V 8 g k N a b r u Q n 6 G d U o m O T T Y i 8 1 P K F s T I e 8 a 6 m i E T d + N r 9 3 S s 6 t M i B h r G 0 p J H P 1 9 0 R G I 2 M m U W A 7 I 4 o j s + z N x P + 8 b o r h t Z 8 J l a T I F V s s C l N J M C a z 5 8 l A a M 5 Q T i y h T A t 7 K 2 E j q i l D G 1 H R h u A t v 7 x K W r W q 5 1 a 9 + 1 q 5 f p n H U Y B T O I M L 8 O A K 6 n A L D W g C A w n P 8 A p v z q P z 4 r w 7 H 4 v W N S e f O Y E / c D 5 / A B a i j 0 g = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m L B 0 B F Y M y b 5 W / U u X 4 e w 2 L p P Q y Q g = " &gt; A A A B 7 3 i c b V A 9 S w N B E J 3 z M 8 a v q K X N Y i J Y h b s g a B m w s b C I Y D 4 g O c L e Z i 9 Z s r d 3 7 s 4 J 4 c i f s L F Q x N a / Y + e / c Z N c o Y k P B h 7 v z T A z L 0 i k M O i 6 3 8 7 a + s b m 1 n Z h p 7 i 7 t 3 9 w W D o 6 b p k 4 1 Y w 3 W S x j 3 Q m o 4 V I o 3 k S B k n c S z W k U S N 4 O x j c z v / 3 E t R G x e s B J w v 2 I D p U I B a N o p U 7 F 9 L M 7 d 1 r p l 8 p u 1 Z 2 D r B I v J 2 X I 0 e i X v n q D m K U R V 8 g k N a b r u Q n 6 G d U o m O T T Y i 8 1 P K F s T I e 8 a 6 m i E T d + N r 9 3 S s 6 t M i B h r G 0 p J H P 1 9 0 R G I 2 M m U W A 7 I 4 o j s + z N x P + 8 b o r h t Z 8 J l a T I F V s s C l N J M C a z 5 8 l A a M 5 Q T i y h T A t 7 K 2 E j q i l D G 1 H R h u A t v 7x K W r W q 5 1 a 9 + 1 q 5 f p n H U Y B T O I M L 8 O A K 6 n A L D W g C A w n P 8 A p v z q P z 4 r w 7 H 4 v W N S e f O Y E / c D 5 / A B a i j 0 g = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m L B 0 B F Y M y b 5 W / U u X 4 e w 2 L p P Q yQ g = " &gt; A A A B 7 3 i c b V A 9 S w N B E J 3 z M 8 a v q K X N Y i J Y h b s g a B m w s b C I Y D 4 g O c L e Z i 9 Z s r d 3 7 s 4 J 4 c i f s L F Q x N a / Y + e / c Z N c o Y k P B h 7 v z T A z L 0 i k M O i 6 3 8 7 a + s b m 1 n Z h p 7 i 7 t 3 9 w W D o 6 b p k 4 1 Y w 3 W S x j 3 Q m o 4 V I o 3 k S B k n c S z W k U S N 4 O x j c z v / 3 E t R G x e s B J w v 2 I D p U I B a N o p U 7 F 9 L M 7 d 1 r p l 8 p u 1 Z 2 D r B I v J 2 X I 0 e i X v n q D m K U R V 8 g k N a b r u Q n 6 G d U o m O T T Y i 8 1 P K F s T I e 8 a 6 m i E T d + N r 9 3 S s 6 t M i B h r G 0 p J H P 1 9 0 R G I 2 M m U W A 7 I 4 o j s + z N x P + 8 b o r h t Z 8 J l a T I F V s s C l N J M C a z 5 8 l A a M 5 Q T i y h T A t 7 K 2 E j q i l D G 1 H R h u A t v 7 x K W r Wq 5 1 a 9 + 1 q 5 f p n H U Y B T O I M L 8 O A K 6 n A L D W g C A w n P 8 A p v z q P z 4 r w 7 H 4 v W N S e f O Y E / c D 5 / A B a i j 0 g = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m L B 0 B F Y M y b 5 W / U u X 4 e w 2 L p P Q y Q g = " &gt; A A A B 7 3 i c b V A 9 S w N B E J 3 z M 8 a v q K X N Y i J Y h b s g a B m w s b C I Y D 4 g O c L e Z i 9 Z s r d 3 7 s 4 J 4 c i f s L F Q x N a / Y + e / c Z N c o Y k P B h 7 v z T A z L 0 i k M O i 6 3 8 7 a + s b m 1 n Z h p 7 i 7 t 3 9 w W D o 6 b p k 4 1 Y w 3 W S x j 3 Q m o 4 V I o 3 k S B k n c S z W k U S N 4 O x j c z v / 3 E t R G x e s B J w v 2 I D p U I B a N o p U 7 F 9 L M 7 d 1 r p l 8 p u 1 Z 2 D r B I v J 2 X I 0 e i X v n q D m K U R V 8 g k N a b r u Q n 6 G d U o m O T T Y i 8 1 P K F s T I e 8 a 6 m i E T d + N r 9 3 S s 6 t M i B h r G 0 p J H P 1 9 0 R G I 2 M m U W A 7 I 4 o j s + z N x P + 8 b o r h t Z 8 J l a T I F V s s C l N J M C a z 5 8 l A a M 5 Q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The expert encoder and the metric function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The Effect of (a) paper sampling and (b) negative sampling. AI and PC stand for author identification and paper clustering respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head></head><label></label><figDesc>author are linked) by graph embedding and then finetunes the paper embeddings on a local paper-paper network built for each name (two similar papers are linked) by graph auto-encoding.Experimental Results.Table 3 presents the performance of the pre-training module evaluated by paper clustering on AMiner. Overall, COAD (pre-train) advances other baselines by 3.1∼14.8% in terms of Pairwise-F1. Among all the baselines, the model proposed by Louppe et al. performs the worst. Because the global distribution of the papers, largely determined by the embeddings of the papers, plays the critical role on the clustering results. However, instead of learning paper embeddings, this baseline learns the local similarity of two papers, which is more suitable to the classification task such as author identification. the model proposed by Zhang et al. and G/L-Emb, building the local paper-paper network for each name and leveraging the graph structures to learn paper embeddings, outperform the model proposed by Louppe et al. by 3.5∼11.7% in Pairwise-F1. In addition to the local embedding, G/L-Emb learns global embeddings via preserving the connectivity in global networks, making it outperform the model proposed by Zhang et al by +8.2% in Pairwise-F1. Our model, COAD (pre-train), discards the local graph embedding for its limitation in inductive learning. But COAD (pre-train)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The effect of the hyper-parameters and the labeled data in the external sources.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>4. 5</head><label>5</label><figDesc>Analysis of the Fine-tuning Module.Ablation Study. To verify different components of the finetuning module, five model variants are also compared.• Unsupervised: is the same as the unsupervised variant model for evaluating the pre-training module. • AMiner-Only: directly applies COAD (pre-train) to encode and link the external persons to AMiner experts. • COAD w/o L adv : removes the domain discriminator in Eq.(12) from COAD. • COAD w/o L diff : removes the orthogonality constraints in Eq.(11) from COAD. • COAD w/o L ext : removes the external private predictor in Eq.(13) from COAD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head></head><label></label><figDesc>(a) Before fine-tuning.(b) After fine-tuning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. T-SNE visualization before and after the adversarial fine-tuning for news articles (Purple) and AMiner papers (Red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Hong</head><label></label><figDesc>Chen received PhD degree from the Institute of Computing Technology, CAS. She is a professor of Renmin University of China. Her research interests include data privacy, big data management, and data analysis based on new hardwares. She is a distinguished member of the CCF. Cuiping Li received the PhD degree from the Institute of Computing Technology, CAS. She is currently a professor of Renmin University of China. Her current research interests include database systems, social network analysis, and data mining. Peng Zhang is a senior engineer in the Department of Computer Science and Technology, Tsinghua University, and also a Ph.D. of Tsinghua University Innovation Leadership Project. He focused in text mining, knowledge graph construction and application. Jie Tang is a professor in the Department of Computer Science and Technology, Tsinghua University. His research interests include data mining, social network, and machine learning. He was honored with the SIGKDD Test-of-Time Award in Applied Data Science.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>AMiner and external experts, learning rates µg, µ f , µ h .</figDesc><table><row><cell></cell><cell cols="3">Output: Learned parameters θ shared g</cell><cell cols="2">, θ</cell><cell>private g</cell><cell>, θ f , θ h .</cell></row><row><cell></cell><cell cols="7">/ * The contrastive pre-training module.</cell><cell>* /</cell></row><row><cell cols="2">1 repeat</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2</cell><cell cols="7">foreach minibatch of triplets {(Ie, I + e , I − e )} do</cell></row><row><cell>3</cell><cell>θ shared g</cell><cell>← θ shared g</cell><cell cols="2">− µg ∂L pre-train ∂θ shared g</cell><cell cols="3">; θ f ← θ f − µ f</cell><cell>∂L pre-train ∂θ f</cell><cell>;</cell></row><row><cell cols="3">4 until Converges;</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="7">/ * The adversarial fine-tuning module.</cell><cell>* /</cell></row><row><cell cols="2">5 repeat</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>6</cell><cell cols="7">foreach minibatch of triplets {(Ie, I + e , I − e )} do</cell></row></table><note>Then the top-matched expert can be Algorithm 1: Training Process of COAD Input:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1</head><label>1</label><figDesc>Data statistics. Persons in AMiner are experts. Persons in News indicate the names mentioned in news articles. Persons in LinkedIn indicate the users in LinkedIn. The support information in them are papers, news articles and LinkedIn homepages respectively. #Average candidates is the average number of candidate AMiner experts for an author in a paper, for a name in a news article or for a LinkedIn user respectively.</figDesc><table><row><cell></cell><cell cols="2">AMiner News</cell><cell>LinkedIn</cell></row><row><cell>#Persons</cell><cell>45,187</cell><cell>1,824</cell><cell>1,665</cell></row><row><cell cols="3">#Support Information 399,255 20,658</cell><cell>50,000</cell></row><row><cell>#Average candidates</cell><cell>18</cell><cell>8.79</cell><cell>4.85</cell></row></table><note>7. https://www.aminer.cn/whoiswho 8. http://thulac.thunlp.org/</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2</head><label>2</label><figDesc>Performance of Author Identification on AMiner.</figDesc><table><row><cell>Model</cell><cell cols="3">HR@1 HR@3 MRR</cell></row><row><cell>GBDT</cell><cell>0.873</cell><cell>0.981</cell><cell>0.927</cell></row><row><cell>Camel</cell><cell>0.577</cell><cell>0.737</cell><cell>0.644</cell></row><row><cell>HetNetE</cell><cell>0.582</cell><cell>0.759</cell><cell>0.697</cell></row><row><cell>CONNA</cell><cell>0.911</cell><cell>0.985</cell><cell>0.949</cell></row><row><cell>Unsupervised</cell><cell>0.713</cell><cell>0.875</cell><cell>0.808</cell></row><row><cell>Paper-paper pseudo labels</cell><cell>0.864</cell><cell>0.960</cell><cell>0.915</cell></row><row><cell>Paper-expert pseudo lables</cell><cell>0.892</cell><cell>0.970</cell><cell>0.933</cell></row><row><cell>Representation metric function</cell><cell>0.870</cell><cell>0.956</cell><cell>0.918</cell></row><row><cell>COAD (pre-train)</cell><cell>0.898</cell><cell>0.964</cell><cell>0.934</cell></row><row><cell cols="4">model with hand-crafted features shown in Table</cell></row></table><note>9. https://biendata.xyz/competition/aminer2019 2/</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3</head><label>3</label><figDesc>Performance of Paper Clustering on AMiner.</figDesc><table><row><cell>Model</cell><cell cols="2">P-Pre. P-Rec.</cell><cell>P-F1</cell></row><row><cell>Louppe et al.</cell><cell>0.609</cell><cell>0.605</cell><cell>0.607</cell></row><row><cell>Zhang et al.</cell><cell>0.768</cell><cell>0.551</cell><cell>0.642</cell></row><row><cell>G/L-Emb</cell><cell>0.835</cell><cell>0.640</cell><cell>0.724</cell></row><row><cell>Unsupervised</cell><cell>0.332</cell><cell>0.591</cell><cell>0.425</cell></row><row><cell>Paper-paper pseudo labels</cell><cell>0.659</cell><cell>0.779</cell><cell>0.714</cell></row><row><cell>Paper-expert pseudo lables</cell><cell>0.715</cell><cell>0.786</cell><cell>0.749</cell></row><row><cell>Representation metric function</cell><cell>0.595</cell><cell>0.754</cell><cell>0.665</cell></row><row><cell>COAD (pre-train)</cell><cell>0.724</cell><cell>0.789</cell><cell>0.755</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4</head><label>4</label><figDesc>Performance of External Expert Linking to AMiner.</figDesc><table><row><cell></cell><cell>News</cell><cell>LinkedIn</cell></row><row><cell>External Sources</cell><cell cols="2">HR@1 HR@3 MRR HR@1 HR@3 MRR</cell></row><row><cell cols="3">Chain Pre-training 0.739 0.927 0.839 0.895 0.978 0.939</cell></row><row><cell>DANN</cell><cell cols="2">0.743 0.928 0.842 0.901 0.983 0.943</cell></row><row><cell>ASP-MTL</cell><cell cols="2">0.746 0.930 0.843 0.903 0.983 0.944</cell></row><row><cell>Unsupervised</cell><cell cols="2">0.329 0.731 0.559 0.805 0.963 0.886</cell></row><row><cell>AMiner-Only</cell><cell cols="2">0.737 0.927 0.837 0.897 0.982 0.940</cell></row><row><cell>COAD</cell><cell cols="2">0.753 0.936 0.848 0.904 0.987 0.945</cell></row><row><cell>COAD w/o L adv</cell><cell cols="2">0.721 0.923 0.825 0.873 0.981 0.927</cell></row><row><cell>COAD w/o L diff</cell><cell cols="2">0.745 0.927 0.841 0.901 0.985 0.942</cell></row><row><cell>COAD w/o L ext</cell><cell>0.743 0.925 0.84</cell><cell>0.898 0.983 0.941</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Bo Chen is a graduate student in Information School, Renmin University of China. His research interests include data integration and knowledge graph mining, and he recently focuses on entity disambiguation in academic networks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Jing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model architecture</head><p>Negative slope is 0.2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning rate</head><p>µg for encoder 2e-5 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Implementation Details</head><p>The detailed hyper-parameters are listed in Table <ref type="table">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.1 Running Environment</head><p>We implement the model by Python 3.6.8, PyTorch 1.1.0, and conduct the experiments on an Enterprise Linux Server with 40 Intel(R) Xeon(R) CPU cores (E5-2640 v4 @ 2.40 GHz and 252G memory), and a single NVIDIA Tesla with 24 GB memory size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.2 Pre-training Module</head><p>The dimension of the BERT output embedding is 768, which is then fed into a one-layer MLP in Eq.( <ref type="formula">2</ref>) to get the corresponding outputs:</p><p>where W ∈ R 768×512 .</p><p>The two-layers MLP on top of the metric function in Eq.( <ref type="formula">9</ref>) is defined as follows:</p><p>where</p><p>In Eq.( <ref type="formula">6</ref>), we use 21 kernels, where µ is from 0 to 1 with interval 0.05. The kernel with µ = 0.0 and σ = 10 −3 corresponds to the exact matching kernel, while σ is set as 0.1 for other kernels capture the semantic matches at different scales.</p><p>The margin m of the triplet loss in Eq.( <ref type="formula">10</ref>) is set to 1.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.3 Adversarial Fine-tuning Module</head><p>The two-layers MLP for the domain discriminator in Eq.( <ref type="formula">12</ref>) is defined as:</p><p>where Test Settings. Author identification and paper clustering are two intrinsic tasks for testing the pre-training module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Training and</head><p>• Author Identification: follows the second task of the name disambiguation competition 10 . Thus, we adopt the same test set as the competition. Since the average number of the experts per name in the dataset is about Implementation of baselines. The heuristic features of GBDT <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b22">[23]</ref> are specified in Table <ref type="table">6</ref>. Apart from GBDT, We implement other baselines following their released code and settings. The training and test settings of all the baselines are the same as COAD (pre-train).</p><p>• Camel <ref type="bibr" target="#b46">[47]</ref>: https://github.com/chuxuzhang/code Camel WWW2018 • HetNetE <ref type="bibr" target="#b4">[5]</ref>: https://github.com/chentingpc/GuidedHeteEmbedding • CONNA <ref type="bibr" target="#b2">[3]</ref>: https://github.com/BoChen-Daniel/TKDE-2019-CONNA • louppe at el <ref type="bibr" target="#b25">[26]</ref>: https://github.com/glouppe/paper-author-disambiguation • Zhang et al <ref type="bibr" target="#b45">[46]</ref>: https://github.com/baichuan/disambiguation embedding • G/L-Emb <ref type="bibr" target="#b49">[50]</ref>: https://github.com/neozhangthe1/disambiguation/ Both Camel and HetNetE define an additional loss function on the indirect relationships between a paper and an expert generated by the pre-defined meta-paths. We ignore this part for the following reasons. Since author identification in their work aim to predict the authors of an anonymous paper, name ambiguity is not the key problem to be tackled. Thus they can collect all the papers published in some related venues as the training data. The connectivity of the resultant heterogeneous graph is good enough to find the indirect relationships between any two nodes. However, to disambiguate experts with same names, we only collect the papers with the same author names. The resultant graph is too sparse to find the indirect relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.2 Evaluation of the Fine-tuning Module</head><p>We evaluate of the fine-tuning module on two external datasets, News and LinkedIn, by the extrinsic task of external expert linking. We first fine-tune COAD (pre-train) on both AMiner and the unlabeled external dataset, and evaluate it on the manually-labeled test set.</p><p>News. The fine-tuning settings for news are as follows:</p><p>• Training. We use 20,658 news articles for fine-tuning. We divide the contextual text of a name in a news article into sentences and treat each sentence as a piece of support information. The maximal length of the input tokens for both the shared encoder and the private encoder is set as 64.</p><p>• Test. We annotate 1,622 names in news articles with linkages to AMiner experts for testing. We choose the candidates by name variants and sample up to 100 papers for each candidate.</p><p>LinkedIn. The settings for LinkedIn are as follows:</p><p>• Training. We use 50,000 LinkedIn homepages for finetuning. We select three common semi-structured attributes including affiliation, skills and summary in a homepage as the support information. The affiliation and the concatenated keywords in skills are separate pieces of support information. The long-text summary is divided into multiple pieces of support information. The maximal length of the input tokens for both the shared encoder and the private encoder is set as 64. • Test. We annotate 1,329 linkages between LinkedIn users and AMiner experts for testing. Other settings are the same as News.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The impact of name-matching and blocking on author disambiguation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Backes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM&apos;18</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="803" to="812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adversarial transfer learning for chinese named entity recognition with self-attention mechanism</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP&apos;18</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="182" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Addressing name disambiguation on the fly</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">20</biblScope>
			<pubPlace>Conna</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<title level="m">A simple framework for contrastive learning of visual representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Task-guided and path-augmented heterogeneous network embedding for author identification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM&apos;17</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="295" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised multilingual word embeddings</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP&apos;18</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="261" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for mention-ranking coreference models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL&apos;16</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pretraining of deep bidirectional transformers for language understanding</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV&apos;15</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;14</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="766" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Kdd cup 2013-author-paper identification challenge: Second place team</title>
		<author>
			<persName><forename type="first">D</forename><surname>Efimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Solecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD Cup 2013 Workshop</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep joint entity disambiguation with local neural attention</title>
		<author>
			<persName><forename type="first">O.-E</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP&apos;17</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Entity linking via joint encoding of types, descriptions, and context</title>
		<author>
			<persName><forename type="first">N</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP&apos;17</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2681" to="2690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05722</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving entity linking through semantic reinforced entity embeddings</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL&apos;20</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6843" to="6848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;14</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2042" to="2050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gpt-gnn: Generative pre-training of graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;20</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1857" to="1867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">From zero to hero: Human-in-the-loop entity linking in low resource domains</title>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Klie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>De Castilho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL&apos;20</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6982" to="6993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">End-to-end neural entity linking</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kolitsas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O.-E</forename><surname>Ganea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL&apos;18</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Word translation without parallel data</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR&apos;18</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improving entity linking by modeling latent relations between mentions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL&apos;18</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Feature engineering and tree modeling for author-paper identification challenge</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD Cup 2013 Workshop</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adversarial multi-task learning for text classification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL&apos;17</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Zero-shot entity linking by reading entity descriptions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL&apos;19</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ethnicity sensitive author disambiguation using semi-supervised learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Louppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Al-Natsheh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Susik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Maguire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international conference on knowledge engineering and the semantic web</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="272" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hierarchical losses and new resources for fine-grained entity typing and linking</title>
		<author>
			<persName><forename type="first">S</forename><surname>Murty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Radovanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL&apos;18</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="97" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning features by watching objects move</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;17</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2701" to="2710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Gcc: Graph contrastive coding for graph neural network pre-training</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;20</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1150" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Wikification and beyond: The challenges of entity and concept grounding</title>
		<author>
			<persName><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cassidy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (Tutorial Abstracts)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The microsoft academic search dataset and kdd cup</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Cock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mandava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dalessandro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Perlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cukierski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hamner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD cup 2013 workshop</title>
				<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Genre separation network with adversarial training for crossgenre relation extraction</title>
		<author>
			<persName><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP&apos;18</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1018" to="1023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A comparison of blocking methods for record linkage</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Steorts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Ventura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sadinle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Fienberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PSD&apos;14</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="253" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bootstrapping entity alignment with knowledge graph embedding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI&apos;18</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4396" to="4402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Arnetminer: extraction and mining of academic social networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD&apos;08</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="990" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bert-int:a bert-based interaction model for knowledge graph alignment</title>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI-20</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3174" to="3180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Transformers: Stateof-the-art natural language processing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP&apos;20: System Demonstrations</title>
				<imprint>
			<date type="published" when="2020-10">Oct. 2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;18</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">End-to-end neural ad-hoc ranking with kernel pooling</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Power</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR&apos;17</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Crosslingual knowledge graph alignment via graph matching neural network</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL&apos;19</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1452" to="1461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep contextualized entity representations with entityaware self-attention</title>
		<author>
			<persName><forename type="first">I</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName><surname>Luke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP&apos;20</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6442" to="6454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Joint learning of the embedding of words and entities for named entity disambiguation</title>
		<author>
			<persName><forename type="first">I</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Takefuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGNLL&apos;16</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="250" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;19</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5754" to="5764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Name disambiguation in anonymized graphs using network embedding</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Al</forename><surname>Hasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM&apos;17</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1239" to="1248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Camel: Content-aware and meta-path augmented metric learning for author identification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW&apos;18</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="709" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Mego2vec: Embedding matched ego networks for user alignment across social networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM&apos;18t</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="327" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Cosnet: Connecting heterogeneous social networks with local and global consistency</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD&apos;15</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1485" to="1494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Name disambiguation in aminer: Clustering, maintenance, and human in the loop</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD&apos;18</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1002" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
