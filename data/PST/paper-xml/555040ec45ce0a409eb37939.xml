<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">National Engineering Research Center for Multimedia Software</orgName>
								<orgName type="department" key="dep2">School of Computer</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E2E91D61D0DCC91E52AFAF86C691628A</idno>
					<idno type="DOI">10.1109/TMM.2014.2311320</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Noise Robust Face Hallucination via Locality-Constrained Representation</head><p>Junjun Jiang, Ruimin Hu, Senior Member, IEEE, Zhongyuan Wang, Member, IEEE, and Zhen Han Abstract-Recently, position-patch based approaches have been proposed to replace the probabilistic graph-based or manifold learning-based models for face hallucination. In order to obtain the optimal weights of face hallucination, these approaches represent one image patch through other patches at the same position of training faces by employing least square estimation or sparse coding. However, they cannot provide unbiased approximations or satisfy rational priors, thus the obtained representation is not satisfactory. In this paper, we propose a simpler yet more effective scheme called Locality-constrained Representation (LcR). Compared with Least Square Representation (LSR) and Sparse Representation (SR), our scheme incorporates a locality constraint into the least square inversion problem to maintain locality and sparsity simultaneously. Our scheme is capable of capturing the non-linear manifold structure of image patch samples while exploiting the sparse property of the redundant data representation. Moreover, when the locality constraint is satisfied, face hallucination is robust to noise, a property that is desirable for video surveillance applications. A statistical analysis of the properties of LcR is given together with experimental results on some public face databases and surveillance images to show the superiority of our proposed scheme over state-of-the-art face hallucination approaches.</p><p>Index Terms-Face hallucination, locality-constrained representation, neighbor embedding, position-patch, sparse representation, super-resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>W ITH the rapid development of intelligent surveillance systems, surveillance cameras have been deployed in various areas including security and protection systems. Surveillance images, especially face images, can provide very important clues to criminal investigation. However, the resolution of a video camera is usually not High-Definition (HD) (see Fig. <ref type="figure" target="#fig_0">1(a)</ref>), and the low resolution of the interested face in the picture resulted from the long distance between the object and the camera (see Fig. <ref type="figure" target="#fig_0">1(c</ref>)) makes it almost impossible to provide useful information (see Fig. <ref type="figure" target="#fig_0">1(b)</ref>). Moreover, in real surveillance scenarios, the qualities of the surveillance images are deteriorated by many environmental factors, such as underexposure, optical blurring, and defocusing. Consequently, the face images of interest are too blurred to be identifiable by humans. In order to obtain enough facial feature details for recognition, a new technique called face super-resolution or face hallucination is adopted to generate High-Resolution (HR) face image from Low-Resolution (LR) images. Existing image hallucination methods mainly fall into two categories: reconstruction-based techniques and learning-based techniques. Based on registration and alignment of multiple LR images of the same scene in sub-pixel accuracy, the former are more susceptible to illconditioned registration and inappropriate blurring operators <ref type="bibr" target="#b0">[1]</ref>, while the latter can generate better performance and higher magnification factor-with the help of a set of training examples. We focus on learning-based method in the sequel.</p><p>Learning-based image hallucination has attracted much attention in recent years <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b32">[33]</ref>. With the help of a training set of LR and HR images, one can obtain an HR image from an LR one by building a co-occurrence model. The first learning-based super-resolution method was proposed by Freeman et al. <ref type="bibr" target="#b19">[20]</ref> who utilized a patch-wise Markov network to model the relationship between local regions of images and the underlying scenes. But this approach is computationally intensive and sensitive to training examples. Baker and Kanade <ref type="bibr" target="#b20">[21]</ref> developed a learning-based super-resolution method specifically for human face. Given an LR input image, their algorithm infers the missing high-frequency components from a parent structure with LR and HR training samples. In <ref type="bibr" target="#b1">[2]</ref>, Liu et al. proposed to integrate a global parametric Principal Component Analysis (PCA) model with a local nonparametric Markov Random Field (MRF) model for face hallucination. The work of <ref type="bibr" target="#b20">[21]</ref> and <ref type="bibr" target="#b1">[2]</ref> spurred much follow-up research that fall into two categories: global face based parameter estimation methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b21">[22]</ref> and local patch image restoration methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>.</p><p>Global face based parameter estimation methods: Wang and Tang <ref type="bibr" target="#b2">[3]</ref> proposed a face hallucination approach using an eigentransformation. In this approach, an LR face image is first decomposed into a linear combination of LR face images in the training set using PCA. Then, the target HR face image is reconstructed by replacing the LR training images with the corresponding HR ones, while using the same coefficients. This approach is easy to be implemented and its performance is reasonably good. Because of its selection of eigenfaces and the maximization of facial information from the LR face image, it is robust against noise to some extent. In <ref type="bibr" target="#b21">[22]</ref>, Chakrabarti et al. utilized a Kernel Principal Component Analysis (KPCA) model to hallucinate face images. Park et al. <ref type="bibr" target="#b10">[11]</ref> decomposed an LR input face into many prototype faces via PCA and applied a recursive error back-propagation method to reconstruct the HR face. All these PCA based global approaches can well capture the global face appearance variations. However, they fail to render effectively the fine individual details of an input face, especially when it is different from the training samples or when the size of the training samples is small. To alleviate the above problem, techniques of decomposing a complete image into smaller patches have been introduced recently.</p><p>Local patch image restoration methods: Inspired by Locally Linear Embedding (LLE) <ref type="bibr" target="#b18">[19]</ref>, Chang et al. <ref type="bibr" target="#b3">[4]</ref> developed a Neighbor Embedding (NE) algorithm for super-resolution of general images. Assuming the LR image patch space and the HR one share the same local geometry, the local geometry of the LR patch space is mapped to the HR patch space to generate the HR image patches through linear combination. In <ref type="bibr" target="#b4">[5]</ref>, from the perspective of manifold alignment, Li et al. extended the approach of <ref type="bibr" target="#b3">[4]</ref> to perform face hallucination on a synthesized common manifold by two explicit mappings. Different from those patch based approaches using a fixed number of neighbors for reconstruction, a single image super-resolution method based on sparse coding that adaptively selects the most relevant neighbors to minimize the reconstruction error was recently introduced by Yang et al. <ref type="bibr" target="#b7">[8]</ref>. However, this method has limited subjective visual effects because it fails to make full use of the prior knowledge of human faces.</p><p>Recognizing the fact that human face is a class of highly structured object and consequently position information plays an important role in its reconstruction, several position-patch based face hallucination methods have been proposed <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>. For example, Ma et al. <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> obtained the representation vector by solving a constrained least square problem, which is referred as Least Square Representation (LSR) in this paper. Compared with some manifold learning-based methods which do not incorporate the position priors, LSR is more efficient because it reasonably utilizes position information. However, when the number of the training samples is much larger than the dimension of a patch, the solution to least square estimation in LSR is not unique <ref type="bibr" target="#b7">[8]</ref>. In order to find an unbiased solution, Jung et al. <ref type="bibr" target="#b8">[9]</ref> employed a sparsity-constrained optimization to replace least square estimation, which is referred as Sparse Representation (SR) in this work. By combing the sparse coding method <ref type="bibr" target="#b7">[8]</ref> and the LSR method <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, SR outperforms both <ref type="bibr" target="#b7">[8]</ref> and LSR. It is well known that locality is more important than sparsity in revealing the non-linear manifold structure of face image patches <ref type="bibr" target="#b12">[13]</ref>, yet SR overemphasizes sparsity and neglects locality. As a result, to reconstruct the input image patch, very distinct patches may be chosen. In addition, both LSR and SR fail to consider the effect of noise. In fact, LSR magnifies the noise rather than suppresses it, and the SR solution is usually not stable, especially when the noise is strong. Thus, the obtained weights of LSR and SR are not robust for actual surveillance images.</p><p>In <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, Zhang and Cham proposed a -nearest neighbors embedding based position-patch method for inferring local features of face image in the Discrete Cosine Transform (DCT) space. In this method, the DC coefficient is estimated by an interpolation-based method, while the AC coefficients (the highfrequency face features) are estimated by linearly combining the nearest sample patches through a LLE based simplified MRF model. This method is very efficient and robust to low illumination. Most recently, Yang et al. <ref type="bibr" target="#b31">[32]</ref> proposed to represent a face by three categories including face components, edges, and smooth regions, and exploit these local image structures for face hallucination under various poses and expressions.</p><p>Since local image patches are similar, it will be more accurate to represent one image patch using a few neighbor patches, leading to a local representation of image patches. Moreover, locality based representation is robust against noise because it replaces the noisy image patch with similar "clean" ones rather than synthesizes noisy image patch as in LSR and SR. Inspired by this, in this paper, we introduce a novel patch representation method for face image hallucination, called Locality-constrained Representation (LcR) in which a locality constraint is incorporated into the least square inversion problem. Unlike existing methods that represent one input image patch collaboratively <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> or sparsely <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, our proposed method projects each image patch into its neighborhoods in the training set adaptively so that both sparsity and locality are preserved. When compared with <ref type="bibr" target="#b29">[30]</ref>, which preserves the locality in a hard way (using a fixed number neighbors for reconstruction) and may lead to over-or under-fitting problem <ref type="bibr" target="#b7">[8]</ref>, our method can adaptively select the neighbors by a locality adaptor. The proposed method has the following distinct features.</p><p>• The locality constraint helps reveal the non-linear manifold structure of face image patch space because it makes the solution of the least square problem fixed on the one hand, and captures the fundamental similarities between neighbor patches on the other; • Compared with traditional neighbor embedding methods that use a fixed number of neighbors for reconstruction, it adaptively chooses the most relevant patches to avoid overor under-fitting while giving sharper contours and richer details; • By adaptively choosing the neighbor patches, it is very robust against noise in real surveillance scenarios. In addition to preliminary results published in <ref type="bibr" target="#b26">[27]</ref>, here we give a detailed description of our LcR method with: i) an LcR ensemble to improve the performance of our original LcR model; ii) analysis on the properties of sparsity and locality; iii) extensive experimental evaluations on its performance, especially on its robustness against noise.</p><p>The rest of this paper is organized as follows. Section II reviews LSR and SR. Section III presents the proposed LcR method. Section IV illustrates the sparsity and locality of LcR.</p><p>Experimental results are presented in Section V. Section VI concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. EXISTING POSITION-PATCH BASED REPRESENTATION APPROACHES</head><p>Let denote the training face images, , where is the size of the training samples. Each face image is divided into small overlapping patch sets , , represents the patch number in every column, represents the patch number in every row, and the term indicates the position information (please refer to the previous version <ref type="bibr" target="#b26">[27]</ref> for a detailed description). For the patch located at position , it can be represented by training samples located at the same position with a weight vector, . For the input face image denoted in patches as , different representation schemes convert each patch into a -dimensional optimal weight vector to generate the final patch representation. In this section, we review two existing patch representation schemes, Ma et al.'s LSR <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> and Jung et al.'s SR <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Least Square Representation</head><p>By incorporating the prior of position information, LSR <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> uses patches from all training samples at the same position to represent each patch collaboratively with <ref type="bibr" target="#b0">(1)</ref> where is the reconstruction error vector.</p><p>The reconstruction weights of the input image patch can be computed by the following constrained least square fitting problem <ref type="bibr" target="#b1">(2)</ref> It is a constrained least squares problem whose closed-form solution <ref type="bibr" target="#b3">[4]</ref> can be solved by computing a Gram matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Sparse Representation</head><p>In practice, the solution of equation ( <ref type="formula">2</ref>) may be unstable or does not exist. The way around is to impose regularization terms onto the objective function. Jung et al. <ref type="bibr" target="#b8">[9]</ref> introduced the sparse representation theory and used a small subset of patches to represent in the place of collaborative performance over the whole training samples. It converts equation ( <ref type="formula">2</ref>) to a standard sparse representation problem <ref type="bibr" target="#b2">(3)</ref> where the -norm counts the number of nonzero entries in a vector. Since there are both the numerical unstable and NP-hard problems in combinatorial -norm minimization <ref type="bibr" target="#b23">[24]</ref>, recent theories developed from sparse representation <ref type="bibr" target="#b24">[25]</ref> suggest that if the solution is sufficiently sparse, then the sparsest solution can be recovered via -norm minimization <ref type="bibr" target="#b3">(4)</ref> where -norm sums up the absolute weights of all entries in a vector, and is the allowed error tolerance. Note that the sparsity constraint not only leads to the exact solution of the under-determined problem, but also allows the learned representation for each patch to capture salient properties, yielding minimized reconstruction error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Locality-Constrained Representation</head><p>The work of <ref type="bibr" target="#b8">[9]</ref> emphasizes that strong sparsity of the weight vector is important in representing the input patch, however, it neglects a locality constraint, which is more important than sparsity in revealing the true geometry of a nonlinear manifold <ref type="bibr" target="#b12">[13]</ref>. In other words, SR might represent one input patch by distinct training patches.</p><p>Following the intuition that Local Coordinate Coding (LCC) <ref type="bibr" target="#b12">[13]</ref> explicitly encourages a local representation, we incorporate a locality constraint into the objective function. Additionally, we adopt shrinkage measures as in ridge regression on the weight vector. Thus, our objective is to <ref type="bibr" target="#b4">(5)</ref> where denotes point-wise vector product and is a -dimensional vector that penalizes the distance between and each training patch at the same position. It is simply determined by the Euclidean distance <ref type="bibr" target="#b5">(6)</ref> The Lagrangian for equation ( <ref type="formula">5</ref>) becomes <ref type="bibr" target="#b6">(7)</ref> Equation ( <ref type="formula">7</ref>) consists of two parts: the first term measures the reconstruction error while the second one preserves locality, with representing the regularization parameter that balances the contribution of the reconstruction error and locality of the solution. When , LcR reduces to LSR. More discusses about are given in Section V-C. In our proposed LcR method, the roles of the locality constraint are twofold: On one hand, it makes the solution fixed; on the other hand, as discussed in Section IV-A, it introduces a locality-constrained sparse representation to each patch, yet this "sparsity" is much weaker than that in the sense of -norm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Optimization</head><p>The objective function <ref type="bibr" target="#b6">(7)</ref> can be represented in the following matrix form <ref type="bibr" target="#b7">(8)</ref> where is a matrix with its columns being training patches and is the diagonal matrix with ( <ref type="formula">9</ref>)</p><p>Following <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref>, the solution of a regularized least square in equation ( <ref type="formula">8</ref>) can be derived analytically as <ref type="bibr" target="#b9">(10)</ref> where is a column vector of ones, the operator "\" denotes the left matrix division operation, and is the covariance matrix for as <ref type="bibr" target="#b10">(11)</ref> with <ref type="bibr" target="#b11">(12)</ref> The final optimal weight is obtained by rescaling to satisfy .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Face Hallucination via LcR</head><p>For face hallucination, the training set is composed of LR and HR face image pairs. HR face images are denoted as while their LR counterparts are denoted as . The primary task is to reconstruct the HR face image from the observed LR face image . At the beginning, we divide the training face images and the LR input face image into patches using the same dividing scheme as in <ref type="bibr" target="#b26">[27]</ref>. For each LR input image patch, it is approximated by a linear combination of the LR patch at the same position using LcR, and we obtain a set of weights on the LR training image patches. Since the LR patch image manifold and the HR one share the same topology <ref type="bibr" target="#b3">[4]</ref>, a new HR patch of the same position can be synthesized by keeping the weights and replacing the LR training image patches with the corresponding HR ones. By concatenating all the HR patches to their corresponding positions and averaging pixel values in the overlapping regions, we can get an estimation of the HR target face. The entire face hallucination process is given in Algorithm 1.</p><p>Pre-processing Based on Image Priors: In order to increase the overall efficiency of our proposed method, we employ one pre-processing step and exploit an assumption about the nature of images.</p><p>Following <ref type="bibr" target="#b19">[20]</ref>, we assume the high-frequency band to be conditionally independent of the low-frequency band , given the middle-frequency band . Mathematically, we have <ref type="bibr" target="#b12">(13)</ref> Based on this assumption, to predict the high-frequency band, only the mid-frequency band rather than all lower frequency bands of the LR input image will be utilized. As in <ref type="bibr" target="#b27">[28]</ref>, in order to obtain the middle frequency components, we linearly interpolate (e.g., by using the Bicubic interpolation algorithm) each blurred LR face image back up to the original HR grids to form an LR input face image.</p><p>Algorithm 1 (Face hallucination via LcR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Input:</head><p>Training set and , an LR image , patch_size, overlap and regularization parameter .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Divide each of the training images and the LR input image into small patches according to the same location of face respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. SPARSITY AND LOCALITY OF LCR</head><p>Recently the importance of sparsity and locality for data representation and classification has attracted a lot attention. Inspired by biological visual systems, many researchers have been arguing that sparse features of signals are useful for settling computer vision problems <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b25">[26]</ref>. On the other hand, in general pattern recognition problems such as data representation and dimension reduction, data locality has been proved to be critical <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b18">[19]</ref>. In this section, we show that the sparsity and locality of LSR, SR and LcR through qualitative and quantitative analysis, respectively. All the experiments in this section are conducted on FEI Face Database <ref type="bibr" target="#b13">[14]</ref>-more details about the database are given in Section V-A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Sparsity of LcR</head><p>For a better understanding of our approach, we plot the concatenated optimal weight vector for different representation methods, LSR, SR and LcR. Note that the optimal weight vector is formed by concatenating the optimal weight vector of each patch of all 40 test face images, thus the length ( ) of can be calculated by the number of test face images the length of the optimal weight of one patch the patch number in every column the patch number in every row . Note that the length of the optimal weight of one patch is equal to the sample number of the training set, and the patch number in every column and the patch number in every row can be obtain from and by substituting , , and to them respectively. ceil( ) is the function that rounds the elements of to the nearest integers towards infinity.</p><p>As in the first row on the right of Fig. <ref type="figure" target="#fig_2">2</ref>, the sorted plot of shows that the reconstruction weight vector of LSR is not sparse since it treats all the samples as equals. Our proposed LcR approach (third row) obtains a sparse result to some extent compared with SR (second row). It shows that LcR can truly reveal the image space, which is embedded in a nonlinear manifold <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b18">[19]</ref>. At the same time, we also observe an encouraging phenomenon: the weights obtained by LSR and SR are equably positive and negative, whereas the reconstruction weights generated by LcR are nearly all non-negative (most values are larger than zero). This indicates that when several neighbor patches are used to represent the target image patch, all the image patches used contributes positively.</p><p>We also quantitatively evaluate and compare the sparsity of these three methods. The Gini Index<ref type="foot" target="#foot_0">1</ref> (GI) <ref type="bibr" target="#b17">[18]</ref> (14) is another metric of measurement introduced to measure the sparsity of the optimal weight vector , where is the -th element of re-ordered optimal weight vector , and , where are the new indices after the sorting operation. Table <ref type="table" target="#tab_0">I</ref> gives the GI results of three different representation methods. It indicates that although the sparsest representation method is SR, the optimal weight vector obtained by LcR is indeed sparse (24% less than that obtained by the SR method and 85% more than that obtained by the LSR method).   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Locality of LcR</head><p>As argued in <ref type="bibr" target="#b12">[13]</ref>, locality is more important than sparsity. Thus, <ref type="bibr" target="#b18">[19]</ref> states that each point on the well-sampled manifold can be linearly represented by a few neighbors of the given data. We test the locality of LSR, SR and LcR, respectively. As proposed in Section IV-A (the formation of the optimal weight vector ), we combine all the distances of each patch of all 40 test images to form a dimensional distance vector . Then, we sort the distance vector in (15) According to the sorted distance vector , the new optimal weight vector is <ref type="bibr" target="#b15">(16)</ref> We plot in Fig. <ref type="figure" target="#fig_3">3</ref> the sorted distance as abscissa and the new optimal weight vector as coordinate for LSR, SR and LcR, respectively. It is seen that the weights decrease with the increase of distance. This trend is more pronounced for LcR than for LSR and SR. As for LcR, large weights mostly concentrate on small distances. We also notice that the weights of LSR and SR have many large spikes as the sorted distance increases, while our proposed LcR approach is capable of removing these spike coefficients, thereby leading to improved locality.</p><p>In addition, we also quantitatively check the locality of LSR, SR and LcR. At the beginning, we define an evaluation metric of locality called ( -MD). Let and denote the inference patch and the most significant patches in training set with the same position respectively, where is the patch index set. The larger the entries in the weight vector , the more important the corresponding patch. The -MD is defined as <ref type="bibr" target="#b16">(17)</ref> According to the -MD definition, the locality of a representation approach is measured by computing the distance between the LR input patch and largest weight patches in the representation of the inference patch. Therefore, the smaller -MD, the better locality.</p><p>The locality comparison is given in Fig. <ref type="figure" target="#fig_4">4</ref>, from which we see that the average values of -MD of LcR are much smaller than those of LSR and SR (regardless of the values of , such as , , or ). Thus LcR can better capture the locality property than LSR and SR. Meanwhile, we also notice that the -MDs with different either remain constant or change very little in the LSR and SR cases. But for LcR, the value of -MD increases with . This implies that those image patches given by LcR are closer to the input one with larger weights, while in LSR and SR cases, all the training patches are treated in the same way. This indicates that our proposed LcR method pays more attention to those patches with small distance to the input (to preserve locality).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENT RESULTS</head><p>To evaluate the proposed LcR algorithm, we compare it with some other state-of-the-art methods for face hallucination: Wang et al.'s global face method <ref type="bibr" target="#b2">[3]</ref>, NE method <ref type="bibr" target="#b3">[4]</ref>, LSR <ref type="bibr" target="#b6">[7]</ref>, SR <ref type="bibr" target="#b8">[9]</ref> and DCT <ref type="bibr" target="#b29">[30]</ref>. Experiments are performed on FEI Face Database<ref type="foot" target="#foot_1">2</ref>  <ref type="bibr" target="#b13">[14]</ref> and brief description of the dataset is provided along with the details of the experiments in Section V-A. The objective results and the objective metrics, i.e., PSNR and SSIM index <ref type="bibr" target="#b14">[15]</ref>, will be reported in Section V-B. We also test the effect of parameter settings in Section V-C and the robustness against noise in Section V-D. In order to further verify the superiority of LcR over other methods, we repeat the experiments on some real-world images in Section V-E. Matlab code available upon e-mail request (junjun0595@163.com).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Database and Parameter Settings</head><p>Experiments described in this paper are conducted on the frontal and pre-aligned images of FEI Face Database. It contains 400 images from 200 subjects (100 men and 100 women) and each subject has two frontal images, one with a neutral expression and the other with a smiling facial expression. Human faces in the database are mainly from 19 to 40 years old with distinct appearances, hairstyles and adornments (some samples are shown in Fig. <ref type="figure" target="#fig_5">5</ref>). All the images are cropped to pixels and we randomly choose 360 images (180 subjects) as the training set, leaving the rest 40 images (20 subjects) for ) and down-sampling (Unless otherwise specified, the down-sampling factor is 4, thus the size of LR face images are pixels) corresponding HR images. To pursue the best performance, we tune the parameters for all comparative methods their best possible results. In particular, for Wang et al.'s global face method, we let the variance accumulation contribution rate of PCA be 99% (around 100 bases). The number of neighbors in NE is set to around 50. For the SR method, we set error tolerance to 1.0. For Wang et al.'s DCT method, the number of neighbors is set to 10 (we change the value and find 10 is the best choice). For these patch based methods, such as NE, LSR, SR and our proposed LcR, we recommend to use the size of pixels for HR image patch and the overlap between neighbor patches is 4 pixels, while corresponding LR image patch size is set to pixels with an overlap of 1 pixel. For more details about the performance under different path size and overlap, please refer to Section V-C. Our algorithm has only one free parameter . We carefully tune it to gain the best performance and we will demonstrate the performance versus the value of in Section V-C. The following reported hallucination results are obtained under the best parameter, for LcR model and for the enhanced LcR approach with pre-processing presented in Section III-C. In the following, we use PreLcR to denote the enhanced LcR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results on FEI Face Database</head><p>Fig. <ref type="figure" target="#fig_6">6</ref> shows some samples of the hallucinated results generated by different methods. The first column are the LR input faces, the last column are the ground true HR faces, while column 2 to column 8 are the hallucinated HR faces based on seven different methods. From the visual results of hallucinated faces, the following points can be observed:</p><p>• Patch based methods outperform global faces reconstruction methods. Wang et al.'s global face method <ref type="bibr" target="#b2">[3]</ref> did not maintain the global smoothness of images, but leads to "ghost" artifacts around contours on the contrary. In comparison, patch based methods show their superiority in further enhancing the edges and textures. This is mainly because that <ref type="bibr" target="#b2">[3]</ref> is based on a statistical mode, which could We found that DCT based method is no better than some other patch based methods, and this is due to the fail of manifold assumption in the DCT space. In particular, the consistence between LR and HR patches in pixel space is better than that in the DCT space. LcR achieves the highest PSNR and SSIM values. The average PSNR and SSIM improvements of LcR method over the second best The improvement of PreLcR approach is much more effective, which gains 1.02 dB and 0.0168 more than SR method. This serves to show the importance of exploring prior information on the image representation model. To compare the ability of hallucinating very LR face images, the performances of different methods with the down-sampling factor of 8 and 16 are evaluated. The third and fourth columns of Table II tabulate the results. We note that the gain of our proposed method over the comparison methods is still evident although with a high down-sampling factor. In the meantime we see that when using very LR input, the superiority of employing the holistic structures of facial images will be more apparent (see the forth column, Wang et al.'s global face method is better than some local patch based methods). In Fig. <ref type="figure" target="#fig_7">7</ref>, we have given results for different methods with different down-sampling factors. The overall quality of results for high down-sampling factor is lower because of increased inconsistency between LR image HR images <ref type="bibr" target="#b16">[17]</ref>. In spite of this, the proposed method yields reasonable results, while the comparison methods produce several artifacts, particularly around the eyes, the mouth and the contours. From the table and the figure, it seems that the best alternative is a position-patch based method that incorporates suitable prior information, e.g., the locality constraint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Parameter Analysis</head><p>In this subsection, we investigate the effect of the different parameter settings.</p><p>1) The Performance of Different : To demonstrate the effectiveness of locality, we evaluate the influence of the proposed LcR method by choosing different regularization parameters ( ), which controls the weights of locality constraint in the objective function. As shown in Fig. <ref type="figure" target="#fig_8">8,</ref><ref type="figure">when</ref> , which can be regarded as the case of LSR, the performance of LcR is restricted. With the increase of , more benefits on performance can be gained. This implies that the locality constraint is essential for patch reconstruction. However, we should also see that the value of could not be set too high. Therefore, with a proper regularization parameter , LcR will gain good results.</p><p>2) The Performance of Different Patch Size: For the local patch based method, the size of a patch is important for getting  <ref type="bibr" target="#b2">[3]</ref>, NE <ref type="bibr" target="#b3">[4]</ref>, LSR <ref type="bibr" target="#b6">[7]</ref>, SR <ref type="bibr" target="#b8">[9]</ref>, DCT <ref type="bibr" target="#b29">[30]</ref>, and PreLcR, and the last column is the original HR faces. Note that the first two rows and last two rows are the hallucinated results with down-sampling factor of 8 and 16 respectively. reliable results. If a patch is too small, it would give little information and cannot capture the geometric structure of a human face and the hallucinated face becomes noisy. On the other hand, as a patch becomes larger, the hallucinated face may be smooth and lost some visual details, in addition, the dimension of a patch becomes larger, it needs much more training images to extract reliable generalized basis, especially when the test image is far different from the training face images. Table <ref type="table" target="#tab_2">III</ref> tabulates the PSNR and SSIM indexes of different methods under different patch size and overlap pixels. Given the same patch size, the larger the overlap degree is, the better the performance is. However, a larger overlap degree means higher computational complexity. From Table <ref type="table" target="#tab_2">III</ref>, we can learn that the superiority of the proposed method over the comparison methods, such as NE, LSR and SR, is unaffected by the patch size and overlap, e.g., the patch size takes the value between 4 pixels (the smallest patch) and 24 pixels while the overlap takes the value between 0 pixels and 20 pixels.</p><p>3) The Influence of Registration Error: Accurate registration of the input face image is vital for the position-patch based methods. The registration error will relieve the effects of face position prior. In all above experiments, we test the proposed face hallucination method based on the assumption that LR face  image is previously extracted by manual or algorithmic operations and aligned to the training set. In order to simulate a real-world environment, we evaluate the impact of registration error to the performance of the proposed method. We firstly generated new test LR face images by translating HR face images on the horizontal and vertical directions respectively. Translation size (offset) varied from -3 to 3 pixels for the test LR face images. In this experiment, we also used the all the 40 test face images from FEI face database. As shown in Fig. <ref type="figure" target="#fig_9">9</ref>, PSNR and SSIM indexes were inferior, as the size of the offset is large. Fig. <ref type="figure" target="#fig_10">10</ref> gives one of the 40 hallucinated faces when the translation size is set to different values, and we can see that the misalignment can dramatically degrade the hallucination results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Robustness Against Noise</head><p>Most methods in previous work do not consider the influence of noise, and they simply assume that the LR input face image is noiseless. However, in the practical environment, the observed LR image is inevitably affected by noise. Denoising and superresolution are usually adopted by previous approaches to offset the noise impact. However, artifacts introduced by denoising will be kept or even magnified in the latter super-resolution process. In this subsection, we show that by formulating the problem into our LcR model, our proposed method can well handle both noise impact and super-resolution simultaneously. In our experiments, we add zero mean Gaussian noises with seven different standard deviations ( ) on the LR input face images and adjust the parameters for NE (neighbor number ), SR (error tolerance ) and LcR method (regularization parameter ) to achieve the best performance. Note that larger patch size and more overlap pixels will lead to better noise robustness performance. For fair comparison, we choose the same patch size and overlap pixels as presented in Section V-A. With the increase of noise level, the performances of NE, LSR and DCT reduce rapidly as shown in Fig. <ref type="figure" target="#fig_11">11</ref>. When , they cannot remove the noise anymore. However, SR and the proposed method can remove most of the noise; when the noise is very strong, i.e., , the SR method will cause severe distortion, while the (e) LSR <ref type="bibr" target="#b6">[7]</ref>; (f) SR <ref type="bibr" target="#b8">[9]</ref>; (g) DCT <ref type="bibr" target="#b29">[30]</ref>; (h) our method. Note that the values of are the best parameters under different noise levels of our proposed method.</p><p>LcR can maintain the primitive facial feature information as shown in Fig. <ref type="figure" target="#fig_11">11</ref>(h), though the hallucinated face images tend to become smooth with minor blocking effects. This is consistent with the sparse representation theory which states that sparse recovery is robust against small magnitude noise in the observation. In other words, it is not feasible to achieve the noise robustness upon relying only on sparsity constraint of the weights. From Fig. <ref type="figure" target="#fig_11">11</ref>, we also find that Wang et al.'s global face reconstruction method can well preserve the characteristics of human face and remove the noise well, but the hallucinated face images are dirty and different from the ground truth especially when the noise increases. From Fig. <ref type="figure" target="#fig_11">11</ref>(h), we learn that the regularization parameter plays an important role in removing noise. The value of depends on the noise level of the LR input faces. To be specific, we can give a larger value to to gain a good performance as the LR input face images become noisier (For more details about the relationship between and , please refer to the Appendix A).</p><p>We can also explain this potential of denoising of LcR from another perspective: an extremely noisy LR input image patch. In that situation, it is quite sensible to replace the noisy patch with similar "clean" ones rather than synthesize and "explain" the noisy patch as in LSR and SR. What must be emphasized here is that the number of selected patch must be small (sparsity) and the selected patch similar to the input one (locality), which are the typical characteristics of LcR discussed in Section IV.</p><p>In this paper, we attribute the robustness against noise of LcR to the capability of adaptively selecting the neighbor patches. Specifically, by adaptively choosing the neighbor patches, the proposed LcR approach can choose the most relevant patches to avoid over-or under-fitting while giving sharper contours and richer details. When compared with the approach that uses a fixed number of neighbors for reconstruction (we denote this approach by -NN, and it differs from Chang's NE method, which doesn't consider the position prior of human faces), our method is much more robust against noise. In the following, we will give some comparative experiments (adaptive versus fixed ) and analysis to validate the merit of choosing adaptive neighbor patches. As in Section IV-C, we conduct the experiments on the 360 training images and 40 test images. Fig. <ref type="figure" target="#fig_2">12</ref> shows the performance in term of PSNR under different noise levels. Note that we try different and to achieve the best results for LcR and -NN respectively. We can learn that LcR is better than -NN in all case. In particular, the gain of LcR over -NN is getting larger with the increase of the noise level, e.g., the gains are 0.30 dB, 0.73 dB, 0.94 dB, 1.09 dB, 1.36 dB and 1.57 dB when varied from 0 to 25. Fig. <ref type="figure" target="#fig_2">12</ref> shows that, when , the optimal neighbor number for -NN is very small, e.g., 1 or 3. It once again proves that locality constraint is very important for noise robust face hallucination, since too many training samples can only lead to the reconstruction of added noise, not noise removal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Experiments with Real World Image</head><p>The LR input face images of all the above experiments are formed by smoothing and down-sampling HR images, which cannot represent the true spatial feature relationship between the HR image and the degraded LR one <ref type="bibr" target="#b22">[23]</ref>. In an actual condition, it is too difficult for us to simulate the image degradation process or know how different types of image degradation processes affect an image's structure and statistics. Therefore, in order to further testify the efficacy of our method, we perform two more experiments: i) experiment on CMU+MIT face database <ref type="bibr" target="#b28">[29]</ref>; ii) experiment on real surveillance imaging condition image.</p><p>For CMU+MIT face database, firstly, we manually extract and align the input faces to the training samples according to the two center points of eyes, which have 25 to 45 pixels in each dimension. Secondly, we use Bicubic interpolation to enlarge those raw images to the size of pixels (To avoid information lost because of down-sampling, we use the pre-processing method as in Section III-C to interpolate the input raw image back up to pixels to form an LR input face image for all comparison methods). Finally, we construct the target HR face image through the respective approaches. Note that we set the values of all the parameters equal to those mentioned in Section V-C except for the regularization parameter , which is set with respect to the noise levels of the observation images. The results of the test images collection are shown in Fig. <ref type="figure" target="#fig_3">13</ref>. Due to space limitations, we only give five groups of comparison results in Fig. <ref type="figure" target="#fig_12">14</ref>. Obviously, the LcR method can produce reasonable results even though the test images are drastically different from the training examples and degraded by different noise levels. In addition, compared with five other methods, our proposed method is much more robust against noise with different levels (see the fifth row of Fig. <ref type="figure" target="#fig_12">14</ref>, our result is very well).</p><p>Fig. <ref type="figure" target="#fig_13">15</ref> are pictures with a CIF-size ( pixels) taken by a surveillance camera. The images in the first row are obtained in the condition of underexposure. The images in the second row are obtained when the light condition is normal and Fig. <ref type="figure" target="#fig_2">12</ref>. PSNR comparisons between adaptive approach and fixed approach under different noise level . The red horizontal line denotes the result of the LcR method. The fall around 10 of -NN method can be explained by that the least squares solution of neighbor embedding is "too fitted" on the LR data <ref type="bibr" target="#b32">[33]</ref>. Fig. <ref type="figure" target="#fig_3">13</ref>. Some hallucinated results of our method on CMU+MIT face database. For each example, the input image is at left, the extracted, aligned LR face in the middle, and the HR hallucinated face at the right.  <ref type="bibr" target="#b2">[3]</ref>, NE <ref type="bibr" target="#b3">[4]</ref>, LSR <ref type="bibr" target="#b6">[7]</ref>, SR <ref type="bibr" target="#b8">[9]</ref>, DCT <ref type="bibr" target="#b29">[30]</ref>, and the proposed method. the involved person is near to the camera, thus these images are used only for visual comparisons and seen as the "ground truth" of the first row. Firstly, we manually extract and align the faces of interest to the training samples according to the two center points of eyes as above, which have 50 to 63 pixels in each dimension. And then, the LR input faces are generated by converting the cropped faces to grayscale, adjusting their levels, and upsampling to the size of pixels. The first column of Fig. <ref type="figure" target="#fig_14">16</ref> shows the four LR input faces from the surveillance camera. As for our proposed method, we set the values of all the parameters equal to those mentioned in Section V-C except for the regularization parameter , which is set to 0.6 in our experiments.</p><p>Fig. <ref type="figure" target="#fig_14">16</ref> compares visual results of different methods on four real surveillance LR images. When the surveillance face images are of low-quality (noisy and blurring), the proposed LcR method performs quite well for the hallucinating task. In addition, we can also see that, for non-Gaussian noise, Wang et al.'s global face method doesn't work. LSR adds noise to the hallucinated results rather than removes it and SR's results are very dirty. NE, DCT and our method can suppress the noise, and [3], DCT <ref type="bibr" target="#b29">[30]</ref>, NE <ref type="bibr" target="#b3">[4]</ref>, LSR <ref type="bibr" target="#b6">[7]</ref>, SR <ref type="bibr" target="#b8">[9]</ref>, the proposed method and the "ground truth".</p><p>we attribute this to the locality of these two representation approaches. However, due to the over-or under-fitting solution of NE and DCT, their hallucinated faces have serious blocking-artifact. In conclusion, our proposed LcR method achieves the best performance. It removes most of the noise and is to a great extent similar to the "ground truth".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION AND FUTURE WORK</head><p>In this paper, we propose a novel patch representation algorithm, called Locality-constrained Representation (LcR), for face hallucination. It imposes a locality constraint together with sparsity constraint onto least square inversion problem, aiming at obtaining the optimal representation of one image patch. In this method, each patch is represented by a small number of bases, which are adaptively selected from its neighborhoods, thus achieving sparsity and locality simultaneously. Experimental results on some public databases and surveillance images have demonstrated the superiority of the proposed method over some state-of-the-art methods.</p><p>However, there are several problems that need to be investigated in the future:</p><p>• Experiments show that the locality prior is vital for patch representation. However, in this paper, we measure the similarity (locality) between the LR input patch and the LR training patches by pair wise Euclidean distances. This fails to discover the intrinsic geometrical structure of the data set, which is essential to the real applications <ref type="bibr" target="#b33">[34]</ref>. Therefore, designing a universal distance measurement algorithm, which can exploit the intrinsic manifold structure (accurate locality prior) of training sample patches, should be our future work. • Note that the overlap patch representation and reconstruction is very time consuming, and this leads to the difficulty of our method in practical applications, e.g., face recognition and real time 3D face synthesis. Thanks to the independence of the reconstruction of each target HR patch, we can straightforward to accelerate the algorithm via parallel computation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Typical frames from surveillance videos. (a) and (c) are the surveillance images from a camera with CIF size ( pixels) and a camera with 720P size ( pixels) respectively; (b) shows two interested faces extracted from (a) and (c).</figDesc><graphic coords="1,304.02,162.12,249.96,82.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>3 .. End for 5 .</head><label>35</label><figDesc>For each patch of do • Calculate the Euclidean distance between the LR input image patch and all the training image patches at position : • Compute the optimal weight vector for the LR input image patch with the LR training image patches : • Construct the HR patch by 4Integrate all the reconstructed HR patches above according to the original position. The final HR image can be generated by averaging pixel values in the overlapping regions. 6. Output: HR hallucinated face image .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The sparsity of the concatenated optimal weight vector for different representation methods: LSR (top row), SR (middle row), LcR (bottom row).The left figures are the plots of the concatenated optimal weight vector and the right figures are the corresponding plots of sorted ones. The more smooth (at the same time close to zero) the sorted ones, the sparser the concatenated optimal weight vector.</figDesc><graphic coords="5,303.00,63.12,250.02,259.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The weight vector of different representation methods according to the sorted distance.</figDesc><graphic coords="5,303.00,466.14,250.02,111.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Histograms of the -mean distance for different methods: LSR (top row), SR (middle row), LcR (bottom row). The red arrows indicate the average values.</figDesc><graphic coords="6,75.00,64.14,439.98,234.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Some training faces in FEI Face Database.</figDesc><graphic coords="7,40.02,65.10,249.96,147.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Comparison of results based on different methods. From left to right: LR input faces, hallucinated faces by Wang et al.<ref type="bibr" target="#b2">[3]</ref>, NE<ref type="bibr" target="#b3">[4]</ref>, LSR<ref type="bibr" target="#b6">[7]</ref>, SR<ref type="bibr" target="#b8">[9]</ref>, DCT<ref type="bibr" target="#b29">[30]</ref>, LcR, and PreLcR, and the last column is the original HR faces. (Note that the effect is more pronounced if the figure of the electronic version is zoomed.)</figDesc><graphic coords="7,306.00,63.12,250.02,135.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Hallucinated face images with different down-sampling factors. From left to right: LR input faces, hallucinated faces by Wang et al.<ref type="bibr" target="#b2">[3]</ref>, NE<ref type="bibr" target="#b3">[4]</ref>, LSR<ref type="bibr" target="#b6">[7]</ref>, SR<ref type="bibr" target="#b8">[9]</ref>, DCT<ref type="bibr" target="#b29">[30]</ref>, and PreLcR, and the last column is the original HR faces. Note that the first two rows and last two rows are the hallucinated results with down-sampling factor of 8 and 16 respectively.</figDesc><graphic coords="8,303.00,65.10,250.02,156.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. The average PSNR and SSIM values of the proposed LcR method with different .</figDesc><graphic coords="8,302.04,303.12,249.96,127.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. The changes of PSNR (left) and SSIM (right) values with the offset of a misaligned face from original face regions at (0, 0).</figDesc><graphic coords="9,40.02,308.16,249.96,93.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Examples of face hallucination using various translation sizes. (0,0) indicates that the LR input face is well aligned.</figDesc><graphic coords="9,328.98,310.14,201.12,247.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Hallucinated faces with noise using different methods: (a) Original HR face image; (b) LR face images with noise; (c) Wang et al. [3]; (d) NE [4];(e) LSR<ref type="bibr" target="#b6">[7]</ref>; (f) SR<ref type="bibr" target="#b8">[9]</ref>; (g) DCT<ref type="bibr" target="#b29">[30]</ref>; (h) our method. Note that the values of are the best parameters under different noise levels of our proposed method.</figDesc><graphic coords="10,39.00,64.14,249.12,247.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. The comparison of visual results on five face images from CMU+MIT face database. From left to right: LR input faces, hallucinated faces by Wang et al.<ref type="bibr" target="#b2">[3]</ref>, NE<ref type="bibr" target="#b3">[4]</ref>, LSR<ref type="bibr" target="#b6">[7]</ref>, SR<ref type="bibr" target="#b8">[9]</ref>, DCT<ref type="bibr" target="#b29">[30]</ref>, and the proposed method.</figDesc><graphic coords="12,37.98,64.14,250.02,216.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 15 .</head><label>15</label><figDesc>Fig.<ref type="bibr" target="#b14">15</ref>. Pictures captured by surveillance camera. We extract the interest face images from the first row, which are captured in a low light and at a distance location; meanwhile, we display the "ground faces" (second row) captured in a normal light and near the camera.</figDesc><graphic coords="12,39.00,334.14,250.02,105.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 16 .</head><label>16</label><figDesc>Fig. 16. The comparison of visual results on surveillance images of different methods. From left to right: LR input faces, hallucinated faces by Wang et al.[3], DCT<ref type="bibr" target="#b29">[30]</ref>, NE<ref type="bibr" target="#b3">[4]</ref>, LSR<ref type="bibr" target="#b6">[7]</ref>, SR<ref type="bibr" target="#b8">[9]</ref>, the proposed method and the "ground truth".</figDesc><graphic coords="12,302.04,64.14,249.96,151.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="11,57.00,66.12,478.02,286.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="11,109.98,419.16,373.98,312.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I THE</head><label>I</label><figDesc>GINI INDEX OF THREE DIFFERENT REPRESENTATION METHODS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II PSNR</head><label>II</label><figDesc>(dB) AND SSIM COMPARISON OF DIFFERENT METHODS WITH DIFFERENT DOWN-SAMPLING FACTORS method, i.e., SR, are 0.65 dB and 0.0097, respectively.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III PSNR</head><label>III</label><figDesc>(dB) AND SSIM COMPARISON OF DIFFERENT METHODS UNDER DIFFERENT PATCH SIZE AND OVERLAP PIXELS</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>GI is normalized, and assumes values between 0 and 1 for any vector. Further, it is 0 for the least sparse signal with all the coefficients having an equal amount of energy; and 1 for the sparsest one which has all the energy concentrated in just one coefficient.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>It is publicly available on http://fei.edu.br/~cet/facedatabase.html.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>The higher the SSIM value, the better is the face hallucination quality. The maximum value of SSIM is 1, which means a perfect reconstruction. Compared with the measure of PSNR, SSIM can better reflect the structure similarity between the target image and the reference image.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors are very grateful to Wei Zhang who is the author of <ref type="bibr" target="#b29">[30]</ref> and <ref type="bibr" target="#b30">[31]</ref> for providing the source code of his method. The authors would like to express the sincere gratitude for the invaluable comments and constructive suggestions by anonymous reviewers.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by the major national science and technology special projects (2010ZX03004-003-03), the National Key Technologies R&amp;D Program (2013AA014602), the National Natural Science Foundation of China (61231015, 61070080, 61003184, 61172173, 61303114, and 61170023), and the China Postdoctoral Science Foundation funded project (2013M530350). The associate editor coordinating the review of this manuscript and approving it for publication was Dr. Shin'ichi Satoh.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A</head><p>In the following, we explain the relationship between the parameter and the noise level of the input data. Let , the objective function <ref type="bibr" target="#b7">(8)</ref> can be rewritten as <ref type="bibr" target="#b17">(18)</ref> where and the optimal weight can be obtained by . When it does not lead to a misunderstanding, we drop the term for convenient. Eq. ( <ref type="formula">18</ref>) can be rewritten as <ref type="bibr" target="#b18">(19)</ref> We reformulate the objective function <ref type="bibr" target="#b18">(19)</ref> from the Bayesian framework, and the optimal weight is estimated by <ref type="bibr" target="#b19">(20)</ref> Here, is the conditional probability and is prior distribution of . Note that is an observation, is a constant and it can be ignored in Eq. <ref type="bibr" target="#b19">(20)</ref>. In order to compute the MAP estimation, we assume the observation is contaminated with additive Gaussian noise of standard deviation, we have <ref type="bibr" target="#b20">(21)</ref> The prior distribution of is characterized by an i.i.d. zero-mean Gaussian probability model <ref type="bibr" target="#b21">(22)</ref> where is the standard deviation of . By plugging and into Eq. ( <ref type="formula">20</ref>), we could readily derive <ref type="bibr" target="#b22">(23)</ref> Suppose the standard deviation of is fixed, the more noisy of the LR input face image ( is bigger), the larger of the value should be. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Limits on super-resolution and how to break them</title>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1167" to="1183" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Face hallucination: Theory and practice</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="115" to="134" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hallucinating face by eigen-transformation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. C</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="425" to="434" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Super-resolution through neighbor embedding</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Int. Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="275" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Aligning coupled manifolds for face hallucination</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="957" to="960" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Position-based face hallucination method</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Multimedia and Expo</title>
		<meeting>IEEE Int. Conf. Multimedia and Expo</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="290" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hallucinating face by position-patch</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3178" to="3194" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Face hallucination via sparse coding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Processing</title>
		<meeting>IEEE Int. Conf. Image essing</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1264" to="1267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Position-patch based face hallucination using convex optimization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="367" to="370" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A simple approach to multiview face hallucination</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="579" to="582" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An example-based face hallucination method for single-frame, low-resolution facial images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1806" to="1816" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generalized face super-resolution</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="873" to="886" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Nonlinear learning using local coordinate coding</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>Advances in Neural Information essing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2223" to="2231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A new ranking method for principal components analysis and its application to face image analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Thomaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Giraldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vision Comput</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="902" to="913" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Magic: Recovery of sparse signals via convex programming</title>
		<author>
			<persName><forename type="first">E</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rombergt</surname></persName>
		</author>
		<ptr target="http://www.acm.caltech.edu/l1magic/" />
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neighborhood in single-frame image super-resolution</title>
		<author>
			<persName><forename type="first">K</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Multimedia and Expo (ICME)</title>
		<meeting>IEEE Int. Conf. Multimedia and Expo (ICME)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1122" to="1125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gini index as sparsity measure for signal reconstruction from compressive samples</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zonoobi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Kassim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">V</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Select. Topics Signal Process</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="927" to="932" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning low-level vision</title>
		<author>
			<persName><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pasztor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Carmichael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="47" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hallucinating faces</title>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Automatic Face and Gesture Recognition (FG)</title>
		<meeting>IEEE Int. Conf. Automatic Face and Gesture Recognition (FG)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="83" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Superresolution of face images using kernel PCA-based prior</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Ayan Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajagopalan</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="888" to="892" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">New learning based super-resolution: Use of DWT and IGMRF prior</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Gajjar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1201" to="1213" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">From sparse solutions of systems of equations to sparse modeling of signals and images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bruckstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Rev</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="34" to="81" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">For most large underdetermined systems of linear equations the minimal -norm solution is also the sparsest solution</title>
		<author>
			<persName><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. Pure Appl. Math</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="797" to="829" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sparse representation for computer vision and pattern recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="1031" to="1044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Position-patch based face hallucination via locality-constrained representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Multimedia and Expo</title>
		<meeting>IEEE Int. Conf. Multimedia and Expo</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="212" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Joint learning for single image super-resolution via coupled constraint</title>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="469" to="480" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neural network-based face detection</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Rowley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="38" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hallucinating face in the DCT domain</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-K</forename><surname>Cham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2769" to="2779" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning-based face hallucination in DCT domain</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-K</forename><surname>Cham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Int. Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Structured face hallucination</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hsuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1099" to="1106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Low-complexity single-image super-resolution based on non-negative neighbor embedding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Alberi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Machine Vision Conf. (BMVC)</title>
		<meeting>British Machine Vision Conf. (BMVC)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ranking on data manifolds</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schlkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="169" to="176" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
