<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Negative sampling in semi-supervised learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-11-12">12 Nov 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">John</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rice University University of Texas at Austin Rice University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vatsal</forename><surname>Shah</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rice University University of Texas at Austin Rice University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anastasios</forename><surname>Kyrillidis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rice University University of Texas at Austin Rice University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Negative sampling in semi-supervised learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-11-12">12 Nov 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1911.05166v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce Negative Sampling in Semi-Supervised Learning (NS 3 L), a simple, fast, easy to tune algorithm for semi-supervised learning (SSL). NS 3 L is motivated by the success of negative sampling/contrastive estimation. We demonstrate that adding the NS 3 L loss to state-of-the-art SSL algorithms, such as the Virtual Adversarial Training (VAT), significantly improves upon vanilla VAT and its variant, VAT with Entropy Minimization. By adding the NS 3 L loss to MixMatch, the current state-of-the-art approach on semisupervised tasks, we observe significant improvements over vanilla MixMatch. We conduct extensive experiments on the CIFAR10, CIFAR100, SVHN and STL10 benchmark datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning has been hugely successful in areas such as image classification <ref type="bibr" target="#b21">(Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b12">He et al., 2016;</ref><ref type="bibr" target="#b44">Zagoruyko and Komodakis, 2016;</ref><ref type="bibr" target="#b13">Huang et al., 2017)</ref> and speech recognition <ref type="bibr" target="#b34">(Sak et al., 2014;</ref><ref type="bibr" target="#b37">Sercu et al., 2016)</ref>, where a large amount of labeled data is available. However, in practice it is often prohibitively expensive to create a large, high quality labeled dataset, due to lack of time, resources, or other factors. For example, the ImageNet datasetwhich consists of 3.2 million labeled images in 5247 categories-took nearly two and half years to complete with the aid of Amazon's Mechanical Turk <ref type="bibr" target="#b7">(Deng et al., 2009)</ref>. Some medical tasks may require months of preparation, expensive hardware, the collaboration of many experts, and often are limited by the number of participants <ref type="bibr" target="#b26">(Miotto et al., 2016)</ref>. As a result, it is desirable to exploit unlabeled data to aid the training of deep learning models. This form of learning is semi-supervised learning <ref type="bibr" target="#b5">(Chapelle and Scholkopf, 2006)</ref>  <ref type="bibr">(SSL)</ref>. Unlike supervised learning, the aim of SSL is to leverage unlabeled data, in conjunction with labeled data, to improve performance. SSL is typically evaluated on labeled datasets where a certain proportion of labels have been discarded. There have been a number of instances in which SSL is reported to achieve performance close to purely supervised learning <ref type="bibr" target="#b22">(Laine and Aila, 2017;</ref><ref type="bibr" target="#b27">Miyato et al., 2017;</ref><ref type="bibr" target="#b41">Tarvainen and Valpola, 2017;</ref><ref type="bibr" target="#b4">Berthelot et al., 2019)</ref>, where the purely supervised learning model is trained on the much larger whole dataset. However, despite significant progress in this field, it is still difficult to quantify when unlabeled data may aid the performance except in a handful of cases <ref type="bibr" target="#b0">(Balcan and Blum, 2005;</ref><ref type="bibr" target="#b2">Ben-David et al., 2008;</ref><ref type="bibr" target="#b17">Kääriäinen, 2005;</ref><ref type="bibr" target="#b29">Niyogi, 2013;</ref><ref type="bibr" target="#b32">Rigollet, 2007;</ref><ref type="bibr" target="#b38">Singh et al., 2009;</ref><ref type="bibr" target="#b43">Wasserman and Lafferty, 2008)</ref>.</p><p>In this work, we restrict our attention to SSL algorithms which add a loss term to the neural network loss. These algorithms are the most flexible and practical given the difficulties in hyperparameter tuning in the entire model training process, in addition to achieving the state-of-the-art performance.</p><p>We introduce Negative Sampling in Semi-Supervised Learning (NS 3 L): a simple, fast, easy to tune SSL algorithm, motivated by negative sampling/contrastive estimation <ref type="bibr" target="#b25">(Mikolov et al., 2013;</ref><ref type="bibr" target="#b39">Smith and Eisner, 2005)</ref>. In negative sampling/contrastive estimation, in order to train a model on unlabeled data, we exploit implicit negative evidence, originating from the unlabeled samples: Using negative sampling, we seek for good models that discriminate a supervised example from its neighborhood, comprised of unsupervised examples, assigned with a random (and potentially wrong) class. Stated differently, the learner learns that not only the supervised example is good, but that the same example is locally optimal in the space of examples, and that alternative examples are inferior. With negative sampling/contrastive estimation, instead of explaining and exploiting all of the data (that is not available during training), the model implicitly must only explain why the observed, supervised example is better than its unsupervised neighbors.</p><p>Overall, NS 3 L adds a loss term to the learning objective, and is shown to improve performance simply by doing so to other state-of-the-art SSL objectives. Since modern datasets often have a large number of classes <ref type="bibr" target="#b33">(Russakovsky et al., 2014)</ref>, we are motivated by the observation that it is often much easier to label a sample with a class or classes it is not, as opposed to the one class it is, exploiting ideas from negative sampling/contrastive estimation <ref type="bibr" target="#b25">(Mikolov et al., 2013;</ref><ref type="bibr" target="#b39">Smith and Eisner, 2005)</ref>.</p><p>Key Contributions. Our findings can be summarized as follows:</p><p>i) We propose a new SSL algorithm, which is easy to tune, and improves SSL performance of other state of the art algorithms, simply by adding the NS 3 L loss in their objective.</p><p>ii) Adding the NS 3 L loss to the state-of-the-art-non-Mixup <ref type="bibr" target="#b45">(Zhang et al., 2017)</ref>-loss for unlabeled data, i.e., Virtual Adversarial Training (VAT) <ref type="bibr" target="#b27">(Miyato et al., 2017)</ref>, we observe superior performance compared to state-of-the-art alternatives, such as Pseudo-Label <ref type="bibr" target="#b23">(Lee, 2013)</ref>, plain VAT <ref type="bibr" target="#b27">(Miyato et al., 2017)</ref>, and VAT with Entropy Minimization <ref type="bibr" target="#b27">(Miyato et al., 2017;</ref><ref type="bibr" target="#b30">Oliver et al., 2018)</ref>, for the standard SSL benchmarks of SVHN and CIFAR10.</p><p>iii) Adding the NS 3 L loss to the state-of-theart Mixup SSL, i.e., the MixMatch procedure <ref type="bibr" target="#b4">(Berthelot et al., 2019)</ref>, NS 3 L combined with Mix-Match produces superior performance for the standard SSL benchmarks of SVHN, CIFAR10 and STL-10.</p><p>Namely, adding the NS 3 L loss to existing SSL algorithms is an easy way to improve performance, and requires limited extra computational resources for hyperparameter tuning, since it is interpretable, fast, and sufficiently easy to tune.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Negative Sampling in Semi-Supervised Learning</head><p>Let the set of labeled samples be denoted as {x i , y i } n i=1 , x i being the input and y i being the associated label, and the set of unlabeled samples be denoted as {x u i } nu i=1 , each with unknown correct label y u i . For the rest of the text, we will consider the cross-entropy loss, which is one of the most widely used loss functions for classification. The objective function for cross entropy loss over the labeled examples is:</p><formula xml:id="formula_0">L ({x i , y i } n i=1 ) = − 1 n n i=1 K k=1 y ik log µ ik ,</formula><p>where there are n labeled samples, K classes, y ik = 1 k=yi is the identity operator that equals 1 when k = y i , and µ ik is the output of the classifier for sample i for class k.</p><p>For the sake of simplicity, we will perform the following relabeling: for all i ∈ [n u ], x i+n = x u i and y i+n = y u i . In the hypothetical scenario where the labels for the unlabeled data are known and for w the parameters of the model, the likelihood would be:</p><formula xml:id="formula_1">P {y i } n+nu i=1 | {x i } n+nu i=1 , w = n+nu i=1 P [y i | x i , w] = n+nu i=1 K k=1 µ y ik ik , = n i1=1 K k=1 µ y i 1 k i1k • nu i2=1 K k=1 µ y u i 2 k i2k</formula><p>Observe that,</p><formula xml:id="formula_2">K k=1 µ y u i 2 k i2k = 1 − j:yi 2 j =1 µ i2j ,</formula><p>which follows from the definition of the quantities µ : that represent a probability distribution and, consequently, sum up to one.</p><p>Taking negative logarithms allows us to split the loss function into two components: i) the supervised part and ii) the unsupervised part. The log-likelihood loss function can now be written as follows:</p><formula xml:id="formula_3">L {x i , y i } n+nu i=1 = − 1 n n i1=1 K k=1 y ik log µ ik :=supervised part − 1 nu nu i2=1 log   1 − j =True label µ i2j   :=unsupervised part</formula><p>While the true labels need to be known for the unsupervised part to be accurate, we draw ideas from negative sampling/contrastive estimation <ref type="bibr" target="#b25">(Mikolov et al., 2013;</ref><ref type="bibr" target="#b39">Smith and Eisner, 2005)</ref> in our approach. I.e., for each unlabeled example in the unsupervised part, we randomly assign P labels from the set of labels. These P labels indicate classes that the sample does not belong to: as the number of labels in the task increase, the probability of including the correct label in the set of P labels is small. The way labels are selected could be uniformly at random or by using Nearest Neighbor search, or even based on the output probabilities of the network, with the hope that the correct label is not picked. Our idea is analogous to the word2vec setting <ref type="bibr" target="#b25">(Mikolov et al., 2013)</ref>, which is described in Appendix A.</p><p>The approach above assumes the use of the full dataset, both for the supervised and unsupervised parts. In practice, more often than not we train models based on stochastic gradient descent, and we implement a minibatch variant of this approach with different batch sizes B 1 and B 2 for labeled and unlabeled data, respectively. Particularly, for the supervised mini-batch of size B 1 for labeled data, the objective term is approximated as:</p><formula xml:id="formula_4">1 n n i1=1 K k=1 y ik log µ ik ≈ 1 |B1| i1∈B1 K k=1 y ik log µ ik .</formula><p>The unsupervised part with mini-batch size of B 2 and NS 3 L loss, where each unlabeled sample is connected with P hopefully incorrect labels, is approximated as:</p><formula xml:id="formula_5">1 nu nu i2=1 log   1 − j =True label µ i2j   ≈ 1 |B2| i2∈B2 log   1 − P j=1 µ i2j  </formula><p>Based on the above, our NS 3 L loss looks as follows:</p><formula xml:id="formula_6">LB1,B2 {x i , y i } n+nu i=1 = − 1 |B1| i1∈B1 K k=1 y ik log µ ik − 1 |B2| i2∈B2 log   1 − P j=1 µ i2j   :=NS 3 L loss .</formula><p>Thus, the NS 3 L loss is just an additive loss term that can be easily included in many existing SSL algorithms, as we show next. For clarity, a pseudocode implementation of the algorithm where negative labels are identified by the label probability being below a threshold, as the output of the classifier or otherwise, is given in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>In this paper, we restrict our attention to a subset of SSL algorithms which add a loss to the supervised </p><formula xml:id="formula_7">1 ŷ′ b = isTrue(ŷ b &lt; T ).</formula><p>5:</p><formula xml:id="formula_8">L NS 3 L = L NS 3 L − log 1 − K k=1 1 ŷ′ bk µ bk . 6: end for 7: return 1 B L NS 3 L</formula><p>loss function. These algorithms tend to be more practical in terms of hyperparameter tuning <ref type="bibr" target="#b4">(Berthelot et al., 2019)</ref>. There are a number of SSL algorithms not discussed in this paper, including "transductive" models <ref type="bibr" target="#b15">(Joachims, 1999</ref><ref type="bibr" target="#b16">(Joachims, , 2003;;</ref><ref type="bibr" target="#b8">Gammerman et al., 1998)</ref>, graph-based methods <ref type="bibr" target="#b46">(Zhu et al., 2003;</ref><ref type="bibr" target="#b3">Bengio et al., 2006)</ref>, and generative modeling <ref type="bibr" target="#b16">(Joachims, 2003;</ref><ref type="bibr" target="#b1">Belkin and Niyogi, 2002;</ref><ref type="bibr" target="#b35">Salakhutdinov and Hinton, 2007;</ref><ref type="bibr" target="#b6">Coates and Ng, 2011;</ref><ref type="bibr" target="#b9">Goodfellow et al., 2011;</ref><ref type="bibr">Kingma et al., 2014;</ref><ref type="bibr" target="#b30">Odena, 2016;</ref><ref type="bibr" target="#b31">Pu et al., 2016;</ref><ref type="bibr" target="#b36">Salimans et al., 2016)</ref>. For a comprehensive overview of SSL methods, refer to <ref type="bibr" target="#b5">Chapelle and Scholkopf (2006)</ref>, or <ref type="bibr" target="#b46">Zhu et al. (2003)</ref>. We describe below the relevant categories of SSL in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Consistency Regularization</head><p>Consistency regularization applies data augmentation to semi-supervised learning with the following intuition: Small perturbations for each sample should not significantly change the output of the network. This is usually achieved by minimizing some distance measure between the output of the network, with and without perturbations in the input. The most straightforward distance measure is the mean squared error used by the Π model <ref type="bibr" target="#b22">(Laine and Aila, 2017;</ref><ref type="bibr">Sajjadi et al., 2016)</ref>.</p><formula xml:id="formula_9">The Π model adds the distance term d(f θ (x), f θ (x)),</formula><p>where x is the result of a stochastic perturbation to x, to the supervised classification loss as a regularizer, with some weight.</p><p>Mean teacher <ref type="bibr" target="#b41">(Tarvainen and Valpola, 2017)</ref> observes the potentially unstable target prediction over the course of training with the Π model approach, and proposes a prediction function parameterized by an exponential moving average of model parameter values.</p><p>Mean teacher adds the distance function d(f θ (x), f θ ′ (x)), where θ ′ is an exponential moving average of θ, to the supervised classification loss with some weight. However, these methods are domain specific.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Virtual Adversarial Training</head><p>Virtual Adversarial Training <ref type="bibr" target="#b27">(Miyato et al., 2017)</ref> (VAT) approximates perturbations to be applied over the input to most significantly affect the output class distribution, inspired by adversarial examples <ref type="bibr" target="#b10">(Goodfellow et al., 2015;</ref><ref type="bibr" target="#b40">Szegedy et al., 2014)</ref>. VAT computes an approximation of the perturbation as:</p><formula xml:id="formula_10">r ∼ N 0, ξ dim(x) I g = ∇ r d (f θ (x), f θ (x + r)) r adv = ǫ g g 2</formula><p>where x is an input data sample, dim(•) is its dimension, d is a non-negative function that measures the divergence between two distributions, ξ and ǫ are scalar hyperparameters. Consistency regularization is then used to minimize the distance between the output of the network, with and without the perturbations in the input. Since we follow the work in Oliver et al. ( <ref type="formula">2018</ref>) almost exactly, we select the best performing consistency regularization SSL method in that work, VAT, for comparison and combination with NS 3 L for non-Mixup SSL; Mixup procedure will be described later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Entropy minimization</head><p>The goal of entropy minimization <ref type="bibr" target="#b11">(Grandvalet and Bengio, 2005)</ref> is to discourage the decision boundary from passing near samples where the network produces low-confidence predictions. One way to achieve this is by adding a simple loss term to minimize the entropy for unlabeled data x with total K classes:</p><formula xml:id="formula_11">− K k=1 µ xk log µ xk</formula><p>Entropy minimization on its own has not demonstrated competitive performance in SSL, however it can be combined with VAT for stronger results <ref type="bibr" target="#b27">(Miyato et al., 2017;</ref><ref type="bibr" target="#b30">Oliver et al., 2018)</ref>. We include entropy minimization with VAT in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Pseudo-Labeling</head><p>Pseudo-Labeling <ref type="bibr" target="#b23">(Lee, 2013)</ref> is a simple and easy to tune method which is widely used in practice. For a particular sample, it requires only the probability value of each class, the output of the network, and labels the sample with a class if the probability value crosses a certain threshold. The sample is then treated as a labeled sample with the standard supervised loss function. Pseudo-Labeling is closely related to entropy minimization, but only enforces low-entropy predictions for predictions which are already low-entropy. We emphasize here that the popularity of Pseudo-Labeling is likely due to its simplicity and limited extra cost for hyperparameter search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Mixup based SSL</head><p>Mixup <ref type="bibr" target="#b45">(Zhang et al., 2017)</ref> combines pairs of samples and their one-hot labels (x 1 , y 1 ), (x 2 , y 2 ) with the following operations λ ∼ Beta(α, α),</p><formula xml:id="formula_12">x ′ = λx 1 + (1 − λ)x 2 , y ′ = λy 1 + (1 − λ)y 2 ,</formula><p>to produce a new sample (x ′ , y ′ ) where α is a hyperparameter. Mixup is a form of regularization which encourages the neural network to behave linearly between training examples, justified by Occam's Razor. In SSL, the labels y 1 , y 2 are typically the predicted labels by a neural network with some processing steps.</p><p>Applying Mixup to SSL led to Interpolation Consistency Training (ICT) <ref type="bibr" target="#b42">(Verma et al., 2019)</ref> and Mix-Match <ref type="bibr" target="#b4">(Berthelot et al., 2019)</ref>, which significantly improved upon previous results with SSL on the standard benchmarks of CIFAR10 and SVHN.</p><p>ICT trains the model f θ to output predictions similar to a mean-teacher f θ ′ , where θ ′ is an exponential moving average of θ. Namely, on unlabeled data, ICT en-</p><formula xml:id="formula_13">courages f θ (Mixup(x i , x j )) ≈ Mixup(f θ ′ (x i ), f θ ′ (x j )).</formula><p>MixMatch applies a number of processing steps for labeled and unlabeled data on each iteration and mixes both labeled and unlabeled data together. The final loss is given by</p><formula xml:id="formula_14">X ′ , U ′ = MixMatch(X , U, E, A, α), L supervised = 1 |X ′ | i1∈X ′ K k=1 y i1k log µ i1k , L unsupervised = 1 K|U ′ | i2∈U ′ K k=1 (y i2k − µ i2k ) 2 , L = L supervised + λ 3 L ubsupervised ,</formula><p>where X is the labeled data {x i1 , y i1 } n i1=1 , U is the unlabeled data {x u i2 } nu i2=1 , X ′ and U ′ are the output samples labeled by MixMatch, and E, A, α, λ 3 are hyperparameters. Given a batch of labeled and unlabeled samples, MixMatch applies A data augmentations on each unlabeled sample x i2 , averages the predictions across the A augmentations,</p><formula xml:id="formula_15">p = 1 A A a=1 f θ (Augment(x u i2 ))</formula><p>and applies temperature sharpening,</p><formula xml:id="formula_16">Sharpen(p, E) k := p 1/E k K k=1 p 1/E k ,</formula><p>to the average prediction. A is typically 2 in practice, and E is 0.5. The unlabeled data is labeled with this sharpened average prediction.</p><p>Let the collection of labeled unlabeled data be U. Standard data augmentation is applied to the originally labeled data and let this be denoted X . Let W denote the shuffled collection of U and X . MixMatch alters Mixup by adding a max operation</p><formula xml:id="formula_17">λ ∼ Beta(α, α), λ ′ = max(λ, 1 − λ), x ′ = λ ′ x 1 + (1 − λ ′ )x 2 , y ′ = λ ′ y 1 + (1 − λ ′ )y 2 , and produces X ′ = Mixup( X i1 , W i1 ) and U ′ = Mixup( U i2 , W i2+| X | ).</formula><p>Since MixMatch performs the strongest empirically, we select MixMatch as the best performing Mixupbased SSL method for comparison and combination with NS 3 L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We separate experiments into non-Mixup-based SSL and Mixup-based SSL. For reproducibility, we follow the methodology of Oliver et al. ( <ref type="formula">2018</ref>) almost exactly for our non-Mixup-based SSL experiments, and reproduce many of the key findings. In that work, they compare non-Mixup-based SSL methods and, thus, we use their consistent testbed for the same purpose. MixMatch uses an almost identical setup, but with a slightly different evaluation method, and we use the official implementation for MixMatch for our Mixupbased experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Non-Mixup-based SSL</head><p>Following Oliver et al. ( <ref type="formula">2018</ref>), the model employed is the standard Wide ResNet (WRN) <ref type="bibr" target="#b44">(Zagoruyko and Komodakis, 2016)</ref> with depth 28 and width 2, batch normalization <ref type="bibr" target="#b14">(Ioffe and Szegedy, 2015)</ref>, and leaky ReLU activations <ref type="bibr" target="#b24">(Maas and Ng, 2013)</ref>.</p><p>The optimizer is the Adam optimizer (Kingma and Ba, 2014). The batch size is 100, half of which are labeled and half are unlabeled. Standard procedures for regularization, data augmentation, and preprocessing are followed.</p><p>We use the standard training data/validation data split for SVHN, with 65,932 training images and 7,325 validation images. All but 1,000 examples are turned "unlabeled". Similarly, we use the standard training/data validation data split for CIFAR10, with 45,000 training images and 5,000 validation images. All but 4,000 labels are turned "unlabeled". We also use the standard training data/validation data split for CIFAR100, with 45,000 training images and 5,000 validation images. All but 10,000 labels are turned "unlabeled".</p><p>Hyperparameters are optimized to minimize validation error; test error is reported at the point of lowest validation error. We select hyperparameters which perform well for both SVHN and CIFAR10. After selecting hyperparameters on CIFAR10 and SVHN, we run almost the exact same hyperparameters with practically no further tuning on CIFAR100 to determine the ability of each method to generalize to new datasets. Since VAT and VAT + EntMin use different hyperparameters for CIFAR10 and SVHN, we use those tuned for CIFAR10 for the CIFAR100 dataset. For NS 3 L and NS 3 L + VAT, we divide the threshold T by 10 since there are 10x classes in CIFAR100. We run 5 seeds for all cases.</p><p>Since models are typically trained on CIFAR10 <ref type="bibr" target="#b20">(Krizhevsky, 2009)</ref> and SVHN <ref type="bibr" target="#b28">(Netzer et al., 2011)</ref> for fewer than the 500,000 iterations (1,000 epochs) <ref type="bibr" target="#b30">(Oliver et al., 2018)</ref>, we make the only changes of reducing the total iterations to 200,000, warmup period <ref type="bibr" target="#b41">(Tarvainen and Valpola, 2017)</ref> to 50,000, and iteration of learning rate decay to 130,000. All other methodology follows that work <ref type="bibr" target="#b30">(Oliver et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Baseline Methods</head><p>For baseline methods, we consider Pseudo-Labeling, due to its simplicity on the level of NS 3 L, and VAT for its performance, in addition to VAT + Entropy Minimization. We omit the Π model and Mean Teacher, since we follow the experiments of Oliver et al. ( <ref type="formula">2018</ref>) and both produce worse performance than VAT. The supervised baseline is trained on the remaining labeled data after some labels have been removed. We generally follow the tuned hyperparameters in the literature and do not observe noticeable gains from further hyperparameter tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Implementation of NS 3 L</head><p>We implement NS 3 L using the output probabilities of the network with the unlabeled samples, namely</p><formula xml:id="formula_18">L NS 3 L = NS 3 L({x i2 , µ i2 } B i2=1 , T ).</formula><p>The performance of NS 3 L with random negative sampling assignment or Nearest Neighbor-based assignment is given in Section 5. We label a sample with negative labels for the classes whose probability value falls below a certain threshold. We then simply add the NS 3 L loss to the existing SSL loss function. Using NS 3 L on its own gives</p><formula xml:id="formula_19">L = L supervised + λ 1 L NS 3 L</formula><p>for some weighting λ 1 . For adding NS 3 L to VAT, this gives</p><formula xml:id="formula_20">L = L supervised + λ 2 L VAT + λ 1 L NS 3 L</formula><p>for some weighting λ i , i ∈ {1, 2}. The weighting is a common practice in SSL, also used in MixMatch. This is the simplest form of NS 3 L and we believe there are large gains to be made with more complex methods of choosing the negative labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Results</head><p>We follow the practice in Oliver et al. ( <ref type="formula">2018</ref>) and use the same hyperparameters for plain NS 3 L and NS 3 L in NS 3 L + VAT for both CIFAR10 and SVHN. After selecting hyperparameters on CIFAR10 and SVHN, we run almost the exact same hyperparameters with little further tuning on CIFAR100, where the threshold T is divided by 10 since there are 10x classes in CIFAR100.</p><p>CIFAR10. We evaluate the accuracy of each method with 4,000 labeled samples and 41,000 unlabeled samples, as is standard practice. The results are given in Table <ref type="table">1</ref>. For NS 3 L, we use a threshold T = 0.04, learning rate of 6e-4, and λ 1 = 1. For VAT + NS 3 L, we use a shared learning rate of 6e-4 and reduce λ 1 from 1 to 0.3, which is identical to λ 2 . All other settings remain as is optimized individually.</p><p>We created 5 splits of 4,000 labeled samples, each with a different seed. Each model is trained on a different split and test error is reported with mean and standard deviation. We find that NS 3 L performs reasonably well and significantly better than Pseudo-Labeling, over a 1.5% improvement. A significant gain over all algorithms is attained by adding the NS 3 L loss to the VAT loss. VAT + NS 3 L achieves almost a 1% improvement over VAT, and is about 0.5% better than VAT + EntMin. This underscores the flexibility of NS 3 L to improve existing methods.</p><p>SVHN. We evaluate the accuracy of each method with 1,000 labeled samples and 64,932 unlabeled samples, as is standard practice. The results are shown in Table <ref type="table">1</ref>. We use the same hyperparameters for NS 3 L and VAT + NS 3 L as in CIFAR10.</p><p>Again, 5 splits are created, each with a different seed.</p><p>Each model is trained on a different split and test error is reported with mean and standard deviation. Here, NS 3 L achieves competitive learning rate with VAT, 6.52% versus 6.20%, and is significantly better than Pseudo-Labeling, at 7.70%. By combining NS 3 L with VAT, test error is further reduced by a notable margin, almost 1% better than VAT alone and more than 0.5% better than VAT + EntMin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR100.</head><p>We evaluate the accuracy of each method with 10,000 labeled samples and 35,000 unlabeled samples, as is standard practice. The results are given in Table <ref type="table">1</ref>. For NS 3 L, we use a threshold T = 0.04/10 = 0.004, learning rate of 6e-4, and λ 1 = 1, following the settings in CIFAR10 and SVHN. For VAT + NS 3 L in CIFAR100, we use a shared learning rate of 3e-3 and λ 1 = 0.3, λ 2 = 0.6.</p><p>As before, we created 5 splits of 10,000 labeled samples, each with a different seed, and each model is trained on a different split. Test error is reported with mean and standard deviation. NS 3 L is observed to improve 0.6% test error over Pseudo-Labeling and adding NS 3 L to VAT reduces test error slightly and achieves the best performance. This suggests that EntMin and NS 3 L boosts VAT even with little hyperparameter tuning, and perhaps should be used as default. We note that the performance of SSL methods can be sensitive to hyperparameter tuning, and minor hyperparameter tuning may improve performance greatly. In our experiments, NS 3 L alone runs more than 2x faster than VAT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Mixup-based SSL</head><p>We follow the methodology of <ref type="bibr" target="#b4">Berthelot et al. (2019)</ref> and continue to use the same model described in section 4.1. In the previous section, we use the standard training data/validation data split for SVHN and CI-FAR10, with all but 1,000 labels and all but 4,000 labels discarded respectively. Since the performance of MixMatch is particularly strong using only a small number of labeled samples, we include experiments for SVHN with all but 250 labels discarded, and CIFAR10 with all but 250 labels discarded. We also include experiments on STL10, a dataset designed for SSL, which has 5,000 labeled images and 100,000 unlabeled images drawn from a slightly different distribution than the labeled data. All but 1,000 labels are discarded for STL10.  <ref type="bibr">(2018)</ref> in that it evaluates an exponential moving average of the model parameters, as opposed to using a learning rate decay schedule, and uses weight decay.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Baseline Methods</head><p>We run MixMatch with the official implementation, and use the parameters recommended in the original work for each dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Implementation of NS 3 L</head><p>Recall that MixMatch outputs X ′ , U ′ = MixMatch(X , U, T, A, α) collections of samples with their generated labels. We label each sample x i ∈ X ′ U ′ with negative labels for the classes whose generated probability value falls below a certain threshold. We then simply add the NS 3 L loss to the existing SSL loss function, computing the NS 3 L loss using the probability outputs of the network as usual. Namely,</p><formula xml:id="formula_21">X ′ , U ′ = MixMatch(X , U, E, A, α) L supervised = 1 |X ′ | i1∈X ′ K k=1 y i1k log µ i1k L unsupervised = 1 K|U ′ | i2∈U ′ K k=1 (y i2k − µ i2k ) 2 L NS 3 L = NS 3 L(X ′ U ′ , T ) L = L supervised + λ 3 L unsupervised + λ 1 L NS 3 L</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Results</head><p>We follow the practice of <ref type="bibr" target="#b4">Berthelot et al. (2019)</ref> and tune NS 3 L separately for each dataset. MixMatch + NS 3 L only takes marginally longer runtime than Mix-Match on its own. The learning rate is fixed.</p><p>CIFAR10. We evaluate the accuracy of each method with 4,000 labeled samples and 41,000 unlabeled samples, as is standard practice, and 250 labeled samples and 44,750 unlabeled samples, where MixMatch performs much stronger than other SSL methods. The results are given in Table <ref type="table">2</ref>.</p><p>As in <ref type="bibr" target="#b4">Berthelot et al. (2019)</ref>, we use α = 0.75 and λ 3 = 75. For NS 3 L, we use a threshold of T = 0.05 and a coefficient of λ 1 = 5 for 250 labeled samples and λ 1 = 10 for 4,000 labeled samples.</p><p>We created 5 splits of the number of labeled samples, each with a different seed. Each model is trained on a different split and test error is reported with mean and standard deviation.</p><p>Similar to the previous section, we find that adding NS 3 L immediately improves the performance of Mix-Match, with a 2% improvement with 250 labeled samples and a small improvement for 4,000 samples. The 250 labeled samples case may be the more interesting case since it highlights the sample efficiency of the method.</p><p>CIFAR10 250 4,000 MixMatch 14.49 ± 1.60 7.05 ± 0.10 Mixmatch + NS 3 L 12.48 ± 1.21 6.92 ± 0.12 Table 2: Test errors achieved by MixMatch and Mix-Match + NS 3 L on the standard benchmark of CI-FAR10, with all but 250 labels removed and all but 4,000 labels removed.</p><p>SVHN. We evaluate the accuracy of each method with 1,000 labeled samples and 64,932 unlabeled samples, as is standard practice, and 250 labeled samples and 65,682 unlabeled samples. The results are shown in Table <ref type="table">3</ref>.</p><p>Following the literature, we use α = 0.75 and λ 3 = 250. For NS 3 L, we again use a threshold of T = 0.05 and a coefficient of λ 1 = 2 for both 250 labeled samples and 1,000 labeled samples.</p><p>We created 5 splits with 5 different seeds, where each model is trained on a different split and test error is reported with mean and standard deviation.</p><p>By adding NS 3 L to MixMatch, the model achieves almost the same test error with 250 labeled samples than it does using only MixMatch on 1,000 labeled samples. In other words, in this case applying NS 3 L improves performance almost equivalent to having 4x the amount of labeled data. In the cases of 250 labeled samples and 1,000 labeled samples, adding NS 3 L to MixMatch improves performance by 0.4% and 0.15% respectively, achieving state-of-the-art results.</p><p>SVHN 250 1,000 MixMatch 3.75 ± 0.09 3.28 ± 0.11 Mixmatch + NS 3 L 3.38 ± 0.08 3.14 ± 0.11 Table <ref type="table">3</ref>: Test errors achieved by MixMatch and Mix-Match + NS 3 L on the standard benchmark of SVHN, with all but 250 labels removed and all but 1,000 labels removed.</p><p>STL10. We evaluate the accuracy of each method with 1,000 labeled samples and 100,000 unlabeled samples. The results are given in Table <ref type="table" target="#tab_1">4</ref>.</p><p>Following the literature, we use α = 0.75 and λ 3 = 50. For NS 3 L, we again use a threshold of T = 0.05 and λ 1 = 2. We trained the model for a significantly fewer epochs than in <ref type="bibr" target="#b4">Berthelot et al. (2019)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Alternative NS 3 L methods</head><p>With computational efficiency in mind, we compare several methods of implementing NS 3 L in Table <ref type="table" target="#tab_2">5</ref> on the F-MNIST dataset with a small Convolutional Neural Network. We split the F-MNIST dataset into a 2,000/58,000 labeled/unlabeled split and report validation error at the end of training. Specifically, we compare:</p><p>• Supervised: trained only on the 2,000 labeled samples. We use a small CNN trained for 50 epochs. Where applicable, the number after the dash indicates the number of negative labels per sample selected.</p><p>Selecting negative labels uniformly over all classes appears to hurt performance, suggesting that negative labels must be selected more carefully in the classification setting. NN methods appear to improve over purely supervised training, however the effectiveness is limited by long preprocessing times and the high dimensionality of the data.</p><p>The method described in section 4.1.2, listed here as Threshold, achieves superior test error in comparison to NN and Uniform methods. In particular, it is competitive with Oracle -1, an oracle which labels each unlabeled sample with one negative label which the sample is not a class of.</p><p>It is no surprise that Oracle -3 improves substantially over Oracle -1, and it is not inconceivable to develop methods which can accurately select a small number of negative labels, and these may lead to even better results when combined with other SSL methods.</p><p>We stress that this is not a definitive list of methods to implement negative sampling in SSL, and our fast proposed method, when combined with other SSL, already improves over the state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>With simplicity, speed, and ease of tuning in mind, we proposed Negative Sampling in Semi-Supervised Learning (NS 3 L), a semi-supervised learning method inspired by negative sampling, which simply adds a loss function. We demonstrate the effectiveness of NS 3 L when combined with existing SSL algorithms, producing the overall best result for non-Mixup-based SSL, by combining NS 3 L with VAT, and Mixup-based SSL, by combining NS 3 L with MixMatch. We show improvements across a variety of tasks with only a minor increase in training time.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1</head><label>1</label><figDesc>NS 3 L 1: Input: Mini batch size B, batch of examples x b and their predicted vector of label probabilities ŷb using the output of the classifier {x b , ŷb } B b=1 , threshold T . 2: L NS 3 L = 0. 3: for b = 1, . . . , B do 4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4 :</head><label>4</label><figDesc>Test errors achieved by MixMatch and Mix-Match + NS 3 L on the standard benchmark of STL10, with all but 1,000 labels removed.</figDesc><table><row><cell cols="2">, however even in this case NS 3 L can improve upon MixMatch, reducing</cell></row><row><cell>test error slightly.</cell><cell></cell></row><row><cell>STL10</cell><cell>1,000</cell></row><row><cell>MixMatch</cell><cell>22.20 ± 0.89</cell></row><row><cell cols="2">Mixmatch + NS 3 L 21.74 ± 0.33</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 :</head><label>5</label><figDesc>• Uniform: negative labels are selected uniformly over all classes.• NN: We use the Nearest Neighbor (NN) method to the exclude the class of the NN, exclude four classes with the NNs, or to label with the class with the furthest NN. • Threshold: refers to the method of section 4.1.2 • Oracle: negative labels are selected uniformly over all wrong classes. Test error achieved by various NS 3 L techniques on F-MNIST with all but 2,000 labels removed.</figDesc><table><row><cell>F-MNIST</cell><cell>2,000</cell></row><row><cell>Supervised</cell><cell>17.25 ± .22</cell></row><row><cell>Uniform -1</cell><cell>18.64 ± .38</cell></row><row><cell>Uniform -3</cell><cell>19.35 ± .33</cell></row><row><cell>Exclude class of NN -1</cell><cell>17.12 ± .15</cell></row><row><cell cols="2">Exclude 4 nearest classes with NN -1 17.13 ± .21</cell></row><row><cell>Furthest class with NN -1</cell><cell>16.76 ± .15</cell></row><row><cell>Threshold T = 0.03</cell><cell>16.47 ± .18</cell></row><row><cell>Threshold T = 0.05</cell><cell>16.59 ± .19</cell></row><row><cell>Oracle -1</cell><cell>16.37 ± .12</cell></row><row><cell>Oracle -3</cell><cell>15.20 ± .66</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Negative Sampling</head><p>We present the case of word2vec for negative sampling where the number of words and contexts is such that picking a random pair of (word, context) is with high probability not related. To make the resemblance, let us describe the intuition behind word2vec. Here, the task is to relate words -represented as w-with contexts -represented as c. We can theoretically conceptualize words w being related with x, and contexts being related to labels y. The negative sampling by <ref type="bibr">Mikolov et al.</ref>, considers the following objective function: consider a pair (w, c) of a word and a context. If this pair comes from valid data that correctly connects these two, then we can say that the data pair (w, c) came from the true data distribution; if this pair does otherwise, then we claim that (w, c) does not come from the true distribution.</p><p>In math, we will denote by P [D = 1 | w, c] as the probability that (w, c) satisfies the first case, and P [D = 0 | w, c] otherwise. The paper models these probabilities as:</p><p>where v c , v w correspond to the vector representation of the context and word, respectively. Now, in order to find good vector representations θ := {v c , v w } (we naively group all variables into θ), given the data, we perform maximum log-likelihood as follows:</p><p>Of course, we never take the whole dataset (whole corpus D) and do gradient descent; rather we perform SGD by considering only a subset of the data for the first term: , where the tildes represent the "non-valid" data.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A pac-style model for learning from labeled and unlabeled data</title>
		<author>
			<persName><forename type="first">Maria-Florina</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Learning Theory</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="111" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps and spectral techniques for embedding and clustering</title>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Does unlabeled data provably help? worst-case analysis of the sample complexity of semi-supervised learning</title>
		<author>
			<persName><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dávid</forename><surname>Pál</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="33" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Label propagation and quadratic criterion</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Delalleau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avital</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02249</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Semisupervised learning</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Scholkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The importance of encoding versus training with sparse coding and vector quantization</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
				<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning by transduction</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Gammerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volodya</forename><surname>Vovk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence</title>
				<meeting>the Fourteenth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spike-and-slab sparse coding for unsupervised feature discovery</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Challenges in Learning Hierarchical Models</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Transductive inference for text classification using support vector machines</title>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Transductive learning via spectral graph partitioning</title>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generalization error bounds using unlabeled data</title>
		<author>
			<persName><forename type="first">Matti</forename><surname>Kääriäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Learning Theory</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="127" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semisupervised learning with deep generative models</title>
		<author>
			<persName><forename type="first">Shakir</forename><surname>Diederik P Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Jimenes Rezende</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Challenges in Representation Learning</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName><forename type="first">Hannun</forename><surname>Awni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffery</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep patient: an unsupervised representation to predict the future of patients from the electronic health records</title>
		<author>
			<persName><forename type="first">Riccardo</forename><surname>Miotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">A</forename><surname>Kidd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">T</forename><surname>Dudley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">26094</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin-Ichi</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin</forename><surname>Ishii</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03976</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Manifold regularization and semisupervised learning: Some theoretical analyses</title>
		<author>
			<persName><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1229" to="1250" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Realistic evaluation of deep semi-supervised learning algorithms</title>
		<author>
			<persName><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">; Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekin</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01583</idno>
		<idno>arXiv:1804.09170</idno>
		<imprint>
			<date type="published" when="2016">2016. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Semi-supervised learning with generative adversarial networks</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Variational autoencoder for deep learning of images, labels and captions</title>
		<author>
			<persName><forename type="first">Yunchen</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gan</forename><surname>Zhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Generalization error bounds in semisupervised classification under the cluster assumption</title>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Rigollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1369" to="1392" />
			<date type="published" when="2007-07">Jul. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic transformations and perturbations for deep semi-supervised learning</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0575</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Imagenet large scale visual recognition challenge</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Long short-term memory recurrent neural network architectures for large scale acoustic modeling</title>
		<author>
			<persName><forename type="first">Haşim</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Françoise</forename><surname>Beaufays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifteenth annual conference of the international speech communication association</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Using deep belief nets to learn covariance kernels for gaussian processes</title>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton E Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Very deep multilingual convolutional neural networks for LVCSR</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4955" to="4959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unlabeled data: Now it helps, now it doesn&apos;t</title>
		<author>
			<persName><forename type="first">Aarti</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1513" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Contrastive estimation: Training log-linear models on unlabeled data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
				<meeting>the 43rd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="354" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Interpolation consistency training for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Pas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.03825</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Statistical analysis of semi-supervised regression</title>
		<author>
			<persName><forename type="first">Larry</forename><surname>Wasserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="801" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<title level="m">Wide residual networks</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Pas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
