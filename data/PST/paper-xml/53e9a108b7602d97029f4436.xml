<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">vGreen: A System for Energy Efficient Computing in Virtualized Environments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Giacomo</forename><surname>Marchetti</surname></persName>
							<email>gmarchet@ucsd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Gaurav Dhiman</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>San Diego La Jolla</addrLine>
									<postCode>92093-0404</postCode>
									<region>CA</region>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Center for Networked Systems (CNS) at UCSD</orgName>
								<orgName type="institution">UC MICRO</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">MARCO/DARPA Gigascale Systems Research Center</orgName>
								<orgName type="institution" key="instit2">NSF Greenlight</orgName>
							</affiliation>
							<affiliation key="aff4">
								<address>
									<settlement>San Francisco</settlement>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">vGreen: A System for Energy Efficient Computing in Virtualized Environments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8F8A1A14BFC21A53FAC1E3C0F0058135</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>D.4.7 [Operating Systems]: Distributed Systems Design</term>
					<term>Experimentation</term>
					<term>Performance Virtualization</term>
					<term>Migration</term>
					<term>Energy</term>
					<term>Workload Characterization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present vGreen, a multi-tiered software system for energy efficient computing in virtualized environments. It comprises of novel hierarchical metrics that capture power and performance characteristics of virtual and physical machines, and policies, which use it for energy efficient virtual machine scheduling across the whole deployment. We show through real life implementation on a state of the art testbed of server machines that vGreen can improve both performance and system level energy savings by 20% and 15% across benchmarks with varying characteristics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Power consumption is a critical design parameter in modern data center and enterprise environments, since it directly impacts both the deployment (peak power delivery capacity) and operational costs (power supply, cooling). The energy consumption of the compute equipment is a major component of these costs. For instance, for a 10MW data center, this can range in the order of millions of dollars per year <ref type="bibr" target="#b15">[15]</ref>.</p><p>Modern data centers use virtualization (eg. Xen <ref type="bibr" target="#b2">[2]</ref> and VMware), to get better fault and performance isolation, improved system manageability and reduced infrastructure cost through resource consolidation and live migration <ref type="bibr" target="#b5">[5]</ref>. Consolidating multiple servers running in different virtual machines (VMs) on a single physical machine (PM) increases the overall utilization and efficiency of the equipment across the whole deployment. However, as we show later on, based on the utilization levels and characteristics of these different co-located VMs, the overall power consumption and performance of the VMs can vary a lot. This can create hotspots of activity, and degrade overall performance and energy efficiency.</p><p>In this paper, we introduce vGreen, a multi-tiered software system to manage VM scheduling across different PMs with the objective of managing the overall energy efficiency and performance. The basic premise behind vGreen is to understand and exploit the relationship between the architectural characteristics of a VM (eg. instructions per cycle, memory accesses etc.) and its performance and power consumption. vGreen is based on a client server model, where a central server (referred to as 'vgserv') performs the scheduling of VMs across the PMs (referred to as 'vgnodes'). The vgnodes perform online characterization of the VMs running on them and regularly update the vgserv with this information. These updates allow vgserv to understand the performance and power profile of the different VMs and aids it to intelligently place them across the vgnodes to improve overall performance and energy efficiency.</p><p>We implemented vGreen on a testbed of state of the art servers running Xen as the virtualization software (known as hypervisor). For evaluation, we created and allocated VMs across the PM cluster, which ran benchmarks with varying workload characteristics. We show that vGreen is able to dynamically characterize and accordingly schedule the VMs across the PM cluster, improving the overall performance and energy efficiency by 20% and 15% respectively compared to state of the art VM scheduling policies. Furthermore, vGreen is extremely lightweight with negligible runtime overhead.</p><p>The rest of the paper is organized as follows. In section 2, we discuss the related work followed by the overall design and architecture of vGreen in section 3. We then provide the implementation details, methodology and experimental results in section 4, before concluding in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Systems for management of VMs across a cluster of PMs have been proposed in the past. Eucalyptus <ref type="bibr" target="#b13">[13]</ref> and Usher <ref type="bibr" target="#b10">[10]</ref> are open source systems, which include support for managing VM creation and allocation across a PM cluster. However, they do not have VM scheduling policies to dynamically consolidate or redistribute VMs. VM scheduling policies for this purpose have also been investigated in the past. In <ref type="bibr" target="#b18">[18]</ref>, the authors propose a VM scheduling system, which dynamically schedules the VMs across the PMs based on their CPU, memory and network utilization. The primary objective of the system is to avoid hotspots of activity on PMs for better overall performance. The Distributed resource scheduler  (DRS) from VMware <ref type="bibr" target="#b1">[1]</ref>, which is a proprietary solution, also uses VM scheduling to perform automated load balancing in response to CPU and memory pressure. Similarly, in <ref type="bibr" target="#b4">[4]</ref>, the authors propose VM scheduling algorithms for dynamic consolidation and redistribution of VMs for managing performance and SLA (server level agreements) violations. The authors in <ref type="bibr" target="#b7">[7]</ref> propose Entropy, which uses constraint programming to determine a globally optimal solution for VM scheduling in contrast to the first fit decreasing heuristic used by <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b4">4]</ref>, which can result in globally sub-optimal placement of VMs. However, none of these VM scheduling algorithms take into account the impact of the policy decisions on the energy consumption in the system.</p><p>The problem of power management in virtualized environments has also been investigated. In <ref type="bibr" target="#b12">[12]</ref>, the authors propose Virtu-alPower, which uses the power management decisions of the guest OS on virtual power states as hints to run local and global policies across the PMs. It relies on efficient power management policies in the guest OS, and does no VM characterization at the hypervisor level. This can be a potential problem, since it is difficult to port some of the state of the art power management policies like <ref type="bibr">[6,</ref><ref type="bibr" target="#b8">8]</ref> in guest OS because of lack of exclusive access to privileged resources such as CPU performance counters. In <ref type="bibr" target="#b14">[14]</ref>, a co-ordinated multi-level solution for power management in data centers is proposed. The model uses power estimation (using power sensors) and workload utilization levels to drive VM placement and power management. However, the model and results are based on offline trace driven analysis and simulations. Furthermore, both <ref type="bibr" target="#b12">[12]</ref> and <ref type="bibr" target="#b14">[14]</ref> do not take the architectural characteristics of the VM into account, which, as we show in section 3, directly determine the VM performance and power profile. In <ref type="bibr" target="#b17">[17]</ref>, the authors use VM characteristics like cache footprint and working set to drive power aware placement of VMs. But their study assumes an HPC application environment, where the VM characteristics are known in advance. Besides, their evaluation is based on simulations. In contrast, vGreen assumes a general purpose workload setup with no apriori knowledge on their characteristics.</p><p>The concept of dynamic architectural characterization of workloads using CPU performance counters for power management <ref type="bibr">[6,</ref><ref type="bibr" target="#b8">8]</ref>, performance <ref type="bibr" target="#b9">[9]</ref> and thermal management <ref type="bibr" target="#b11">[11]</ref> on standalone systems has been explored before. However, in virtualized environments, it is a more complex activity due to multiple abstractions involved (virtual and physical cpus, VM, PM etc) and has been largely unexplored.</p><p>Based on this discussion, the primary contributions of our work are as follows: 1) We propose a system for characterization of VMs in a virtualized environment. It is based on novel hierarchical metrics that capture power and performance profiles of the VMs, 2) We use the online VM characterization to drive dynamic VM scheduling across a PM cluster for overall performance and energy efficiency. In our knowledge, this is the first work that exploits the architectural characteristics of VMs to perform dynamic VM scheduling for general purpose workloads. 3) We implement the proposed system on a real life testbed and through extensive experiments and measurements highlight the benefits of the approach over existing state of the art VM scheduling systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">VGREEN DESIGN 3.1 Motivation</head><p>The nature of workload executed in each VM determines the power profile and performance of the VM, and hence its energy consumption. In virtualized environments, VMs with different or same characteristics could be co-located on the same PM. In this section we show, that co-location of VMs with heterogeneous characteristics on PMs is beneficial for overall performance, energy efficiency and also power consumption balancing across the PM cluster.</p><p>To understand the co-relation between VM characteristics and these metrics we performed some offline experiments and analysis using benchmarks from SPEC-CPU 2000 suite, namely mcf and perl. These two benchmarks have contrasting characteristics in terms of their CPU and memory utilization. While mcf has high memory accesses per cycle (MPC) and low instructions per cycle (IPC), perl has low MPC and high IPC. We used a testbed of two dual Intel quad core Xeon based machines (eight physical CPUs each) running Xen. On each of these PMs, we created two VMs with four virtual CPUs (VCPUs) each (total of four VMs). Inside each VM we executed either perl or mcf as the workload. Since SPEC benchmarks are single threaded programs, to get results for higher processor utilization rates, we ran multiple instances of the benchmark. For our PM (eight physical CPUs), this implies two instances for 25% utilization, four instances for 50% and eight instances for 100% utilization.</p><p>In our first set of experiments, we ran homogeneous VMs on each PM, i.e. the two VMs with mcf on one PM, and two with perl on the other. During the execution, we measured the system power consumption using an AC power analyzer, and also recorded the execution time. Figure <ref type="figure" target="#fig_1">1a</ref> shows the normalized execution time results for different number of instances of the benchmarks, where the execution times are normalized against the execution time with two instances (one instance per VM). The results for this case are marked as 'same' indicating homogeneity. We can observe that for mcf (shown as 'mcf (same)'), as the number of instances increase, the execution time almost increases linearly. For eight instances of mcf, the execution time almost quadruples. In contrast, for perl ('perl (same)'), the execution time is fairly independent of the number of instances. The reason for this difference is due to the contrasting MPC of the two benchmarks. The high MPC of mcf results in higher cache conflict rate when multiple instances execute, which decreases its IPC and hence increases its execution time. On the other hand for perl, the MPC of the VM is much lower. Hence, this analysis indicates that the performance of a VM has a strong negative co-relation to MPC of the workload running inside it.</p><p>Similarly Figure <ref type="figure" target="#fig_1">1b</ref> shows the system level power consumption of the benchmarks normalized against the power consumption with two instances. We can observe that for perl, as the utilization increases, the power consumption increases almost linearly, while for mcf, the power consumption increases initially but then it saturates. For eight instances, the difference in power consumption is almost 25%. The reason for this is the difference in their IPC. The high IPC of perl results in higher utilization of CPU core resources, which translates into higher CPU and system level power consumption. In contrast, mcf has a low IPC, which results in much lower power consumption. This analysis indicates that the power consumption of a VM has direct co-relation to IPC of the workload running inside it.</p><p>These results indicate that co-scheduling VMs with similar characteristics is not beneficial from energy efficiency and power consumption balance point of view at high utilization rates. The PM running mcf contributes to higher system energy consumption (since it runs longer), while the PM running perl contributes to power imbalance (since it consumes higher power). To understand the benefits of co-scheduling heterogeneous workloads, we swapped two VMs on the PMs, hence running VMs with mcf and perl on both the PMs. Figure <ref type="figure" target="#fig_1">1</ref> shows the results (indicated as 'mixed') achieved for this configuration in terms of normalized execution time and power consumption. We can observe that perl execution time almost stays the same, while mcf execution time goes down significantly for higher number of instances (around 150% reduction for eight instances). The average power consumption of the two PMs become identical, and is almost an average of the two PMs in the homogeneous case. Due to the high performance improvement of mcf, this results in system level energy savings of up to 20%.</p><p>In summary, this exercise indicates that co-scheduling VMs with heterogeneous characteristics on the same PM is beneficial from both energy efficiency and performance point of view. This is achievable in virtualized environments, since VMs can be dynamically migrated across PMs at low overhead using 'live migration' <ref type="bibr" target="#b5">[5]</ref>. This provides strong motivation for online characterization of VMs in the system, which has been largely unexplored in previous work as described in section 2. We next describe the overall architecture of vGreen, and present details on how it constructs VM characteristics dynamically at run time using a novel hierarchical approach.  The vgxen is a module compiled into Xen (see Figure <ref type="figure" target="#fig_2">2</ref>) and is responsible for characterizing the running VMs. In Xen, a VM The estimation of MPC, IPC and utilization metrics for the VMs in vGreen is a hierarchical process. Figure <ref type="figure" target="#fig_4">3</ref> illustrates the hierarchy, which starts from the VCPU level, which is the actual unit of execution in Xen. When a VCPU is scheduled on a PCPU by the Xen scheduler, vgxen starts the CPU performance counters of that PCPU to count the following events: 1) Instructions Retired (INST), 2) Clock cycles (CLK), and 3) Memory accesses (MEM). When that VCPU consumes its time slice (or blocks) and is removed from the PCPU, vgxen reads the performance counter values and estimates its MPC (MEM/CLK) and IPC (INST/CLK) for the period it executed. This process is performed for every VCPU executing in the system across all the PCPUs. To effectively estimate the impact of these metrics on the VCPU power consumption and performance, vgxen also keeps track of the CPU utilization (util) of each VCPU, i.e. how much time it actually spends executing on a PCPU over a period of time. This is important, since even a high IPC benchmark will cause high power consumption only if it is executing continuously on the PCPU. Hence, the metric derived for each VCPU is weighted by its util, and is referred to as the current weighted MPC and IPC (wM P C cur and wIP Ccur) as shown below:</p><formula xml:id="formula_0">wM P Ccur = MP C • util wIP Ccur = IP C • util (1)</formula><p>They are referred to as 'current', since they are estimated based on the IPC/MPC values from the latest run of a VCPU. To also take into account the previous value of these metrics, we maintain them as running exponential averages, and refer to them as weighted metrics. The equation below shows how weighted MPC is estimated:</p><formula xml:id="formula_1">wM P C = α • wM P Ccur + (1 -α) • wM P Cprev (2)</formula><p>where, the new value of weighted MPC (wM P C) is calculated as an exponential average of wM P Cprev, the previous value of wM P C, and wM P Ccur (equation 1). The factor α determines the weight of current value (wM P Ccur) and history (wM P Cprev).</p><p>In our implementation we use α=0.5, thus giving equal weight to both. We store these averaged metrics in the Xen VCPU structure to preserve them faithfully across VCPU context switches. This constitutes the metric estimation at the lowest level in the system 15: end if as shown in Figure <ref type="figure" target="#fig_4">3</ref>. At the next level, vgxen estimates the aggregate metrics (vMPC, vIPC, vutil) for each VM by adding up the corresponding metrics of its constituent VCPUs, as shown in the middle level of Figure <ref type="figure" target="#fig_4">3</ref>. This information is stored in VM structure of Xen to personalize metrics at per VM level and is exported to Dom0 through a shared page.</p><p>The second vGreen module of vgnode is the vgdom (see Figure <ref type="figure" target="#fig_2">2</ref>). Its main role is to periodically (T up_period ) read the shared page exported by vgxen, and update the vgserv with latest data on the characteristics of different VMs running on the vgnode. Apart from this, vgdom also acts as an interface for the vgnode to the vgserv. It is responsible for registering the vgnode with the vgserv and also for receiving and executing the commands sent by the vgserv as shown in Figure <ref type="figure" target="#fig_2">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>vgserv:</head><p>The vgserv acts as the cluster controller and is responsible for managing VM scheduling across the vgnode cluster. The vgpolicy is the core of vgserv, which makes the scheduling decisions based on periodic updates on the VM metrics from the vgnodes. The metrics of each VM are aggregated by the vgpolicy to construct the top level or node level metrics (nMPC, nIPC, nutil) as shown in Figure <ref type="figure" target="#fig_4">3</ref>. Thus, the knowledge of both the node level and VM level metrics allow the vgpolicy to understand not only the overall power and performance profile of the whole vgnode, but also fine grained knowhow of the breakdown at VM level.</p><p>Based on these metrics, the vgpolicy runs its balancing algorithm periodically (T p_period ). The basic algorithm is motivated by the fact that VMs with heterogeneous characteristics should be co-scheduled on the same vgnode (section 3.1). The algorithm runs in four steps. 1) MPC balance: This step ensures that nMPC is balanced across all the vgnodes in the system for better overall performance and energy efficiency across the cluster. Table <ref type="table" target="#tab_0">1</ref> gives an overview of how the MPC balance algorithm works for a vgnode n1. The algorithm first of all checks, if the nMPC of n1 is greater than a threshold nM P C th (step 1 in Table <ref type="table" target="#tab_0">1</ref>). This threshold is representative of whether high MPC is affecting the performance of the VMs in that vgnode (based on the observation in section 3.1). If nMPC is smaller, the function returns, since there is no MPC balancing required for n1 (step 2 in Table <ref type="table" target="#tab_0">1</ref>). If it is higher, step 4 finds the VM with the minimum vMPC in n1 (referred to as vm_min), which can be migrated to a vgnode with a lower nMPC than n1 to get a better MPC balance in the system. The target vgnode, to which vm_min can be migrated, is stored in pm_min, which in step 5 is initialized to NULL. In steps 6-12, the algorithm ) is lower than the nMPC of the vgnode currently stored as pm_min. This way, once the loop in step 6 completes, it is able to locate the vgnode in the system with the least nMPC (pm_min). Once the pm_min is found, the algorithm performs a final migration check in step 13 to confirm if the migration of vm_min from n1 to pm_min creates more balance in the system, and does not reverse the imbalance by making nMPC of pm_min more than that of n1. If this condition holds, then in step 14, the algorithm invokes the do_migrate function to live migrate vm_min from n1 to pm_min <ref type="bibr" target="#b5">[5]</ref>. The decisions taken by the vgpolicy (updates, migration) are communicated to the vgnodes in form of commands as shown in Figure <ref type="figure" target="#fig_2">2</ref>. 2) IPC balance: This step ensures nIPC is balanced across the vgnodes for better balance of power consumption across the PMs. The algorithm is similar to MPC balance, but uses nIPC instead of nMPC. 3) Util balance: This step balances the utilization of vgnodes to ensure there are no overcommitted nodes in the system, if there are other underutilized vgnodes. 4) VM consolidation: If all the metrics in the prior three steps are balanced, then this step tries to consolidate low utilization VMs to generate idle vgnodes in the system, which could be then be moved into low power states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation &amp; Methodology</head><p>Our testbed for vGreen include two state of the art 45nm Dual Intel Quad Core Xeon E5440 (8 PCPUs each) based server machines, which act as the vgnodes, and a Core2Duo based desktop machine that acts as the vgserv. The vgnodes run Xen3.3.1, and use Xeno-Linux 2.6.18 for Dom0. The vgxen module is implemented as part of the Xen credit scheduler (the default scheduler) to record VM metrics and exposes a new hypercall to map the shared page for sharing VM metrics data with vgdom (as explained in section 3). The vgdom module is implemented in two parts on Dom0: 1) A Linux driver that interfaces with vgxen to get the VM characteristics and exposes it to the application layer, 2) An application client module that gets the VM metrics data from the driver and passes it on to vgserv. It also accepts vgserv commands and processes it. vGreen requires no modifications to the VMs running on a node.</p><p>The vgserv runs on Linux 2.6.23, and is implemented as an application server module. On initialization, it opens a well known port and waits for new vgnodes to register with it. When vgnodes connect, vgserv instructs them to regularly update it with VM characteristics (T up_period ), and accordingly updates the node and VM level metrics. It runs the vgpolicy every T p_period and performs balancing/consolidation decisions as described in section 3.</p><p>For our experiments, we use benchmarks with varying characteristics from the SPECCPU 2000 and PARSEC <ref type="bibr" target="#b3">[3]</ref> suites. PARSEC benchmarks are parallel and multi-threaded, while SPEC benchmarks are single threaded. The used benchmarks and their characteristics are illustrated in Table <ref type="table" target="#tab_1">2</ref>. We can observe that freqmine, perl and bzip2 are high IPC/low MPC CPU intensive benchmarks, while streamcluster, mcf and equake are low IPC/high MPC memory intensive benchmarks. We run each of these benchmarks inside a VM, which is initialized with four VCPUs and 0.5GB of memory. We generate experimental workloads by running multiple VMs together, each running one of the benchmarks. For each combination run we sample the system power consumption of both the vgnodes every 2s using AC power analyzer.</p><p>We compare vGreen to a VM scheduler that mimics the Eucalyptus VM scheduler <ref type="bibr" target="#b13">[13]</ref> for our evaluation. Eucalyptus is an open source cloud computing system, that can manage VM creation and allocation across a cluster of PMs. The default Eucalyptus VM scheduler assigns VMs using a greedy policy, i.e. it allocates VMs to a PM until its resources (number of CPUs and memory) are full. However, this assignment is static, and it does not perform any dynamic VM migration based on actual PM utilization at runtime. For fair comparison, we augment the eucalyptus scheduler with the utilization metrics and utilization/consolidation algorithms proposed in the previous section, which allow it to redistribute/consolidate VMs dynamically at run-time. This enhancement is representative of the metrics employed by the existing state of the art policies (see section 2). We refer to this enhanced scheduler as E+. Furthermore, we use the same initial assignment of VMs to the PMs as done by the E+ scheduler for vGreen as well.</p><p>We report the comparative results of vGreen and E+ for three parameters: 1) Energy savings: We estimate the energy reduction in executing each combination of VMs using vGreen over E+. This is calculated by measuring the total energy consumption for a VM combination with E+ and vGreen, and then taking their difference. Note, that the combinations may execute for different times with E+ and vGreen, and since we do not know the state of the system after the execution (could be active if there are more jobs, or be in sleep state if nothing to do), we only compare the energy consumed during active execution of each combination. 2) Weighted Speedup: We also estimate the average speedup of each VM combination with vGreen. For this, we use the weighted speedup (WS) based on a similar metric defined in <ref type="bibr" target="#b16">[16]</ref>. It is defined as:</p><formula xml:id="formula_2">W S = P V M i T e+ i T alone i P V M i Tvgreen i T alone i -1<label>(3)</label></formula><p>where, T alone i is the execution time of VMi when it runs alone on a PM, and Te+ i and Tvgreen i are its execution time as part of a VM combination with E+ and vGreen respectively. To calculate WS, we normalize Te+ i and Tvgreen i against T alone i for each VM, and then take ratio of the sum of these normalized times across all the VMs in the combination as shown in equation 3. WS &gt; 0 implies that the VM combination runs faster with vGreen and vice versa. 3) Reduction in Power Imbalance: We also estimate the % reduction in power imbalance in the system with vGreen due to IPC balancing compared to E+. We estimate this using %reduction in variance in power consumption of the two vgnodes. A high value for this metric indicates better power balance across the two vgnodes with vGreen. For all our experiments, we use P p_period and P up_period as 5s. In our experience, this was frequent enough to detect and resolve imbalances in the system for different workload combinations with minimal runtime overhead. Based on our experiments across dif-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Heterogeneous Workloads: In the first set of experiments, we use combinations of VMs running benchmarks with heterogeneous characteristics. Each VM consists of four instances (for SPEC) or four threads (for PARSEC) of the benchmark. In total we run four VMs, which make both the vgnodes close to 100% utilized. We run high IPC benchmarks in first two VMs, and high MPC benchmarks in the other two.</p><p>Figure <ref type="figure" target="#fig_6">4</ref> shows the results for weighted speedup and energy savings. The x-axis on the graphs shows the initial distribution of VMs on the physical machines. For instance, 2freq/2mcf means that two VMs running freqmine are on the first PM, while the two VMs running mcf are on the second. We can observe in Figure <ref type="figure" target="#fig_6">4a</ref>, that vGreen achieves an average of around 20% weighted speedup over E+ across all the combinations. The reason for this is that E+ colocates the high IPC VMs on one vgnode, and the high MPC ones on the second one. Thereafter, since the CPU utilization of both the vgnodes is close to 100%, no dynamic relocation of VMs is done. With vGreen, although the initial assignment of the VMs is same as with E+, the dynamic characterization of VMs allow the vgserv to detect a heavy MPC imbalance. This initiates migration of high MPC VM to the second vgnode running the high IPC VMs. This results in an IPC and utilization imbalance between the two vgnodes, since the second vgnode now runs a total of three high utilization VMs. This is detected by vgserv and it responds by migrating a high IPC VM to the first vgnode. This creates a perfect balance in terms of MPC, IPC and utilization across both the vgnodes. This balance avoids unnecessary cache conflicts, and results  <ref type="figure" target="#fig_6">4a</ref>. We can see in Figure <ref type="figure" target="#fig_6">4a</ref>, that some combinations achieve higher weighted speedup compared to others. For instance, for 2freq/2mcf combination is it over 45%, while for 2perl/2mcf it is around 30%. This difference is due to the fact that co-location of perl and mcf VMs results in a small slowdown of perl, while co-location of freqmine and mcf has no impact on the execution time of freqmine. This results in higher weighted speedup for 2freq/2mcf compared to 2perl/2mcf (refer to equation 3). The speedup also results in lower system energy consumption, since now the benchmarks run and consume active power for a smaller duration. We can see in Figure <ref type="figure" target="#fig_6">4b</ref> that vGreen results in average system energy savings of around 15% across all the benchmark combinations. We can observe some combinations to have more savings than the others, which is primarily due to their different weighted speedups.</p><p>Figure <ref type="figure" target="#fig_7">5</ref> illustrates the balance in power consumption across the two vgnodes achieved using vGreen over E+. The figure shows the %reduction in power consumption variance across the vgnodes to capture the degree of balance achieved. We can see that vGreen reduces the average power variance across the two vgnodes by close to 80%. This happens due to the better balance of IPC and utilization across the machines with vGreen, which results in well balanced and similar power consumption for the vgnodes compared to E+. This results in a better overall thermal and power profile and reduces power hotspots in the cluster.</p><p>Homogeneous Workloads: We also experimented with combination of VMs running homogeneous benchmarks. We did experiments for all the six benchmarks in Table <ref type="table" target="#tab_1">2</ref>, where all the fours VMs ran the same benchmark. We observed that in all the experiments, there was no possibility of rebalancing based on characteristics, since the MPC and IPC of the VMs were already balanced. Consequently, the results for all the three parameters were the same as that for E+ across all the experiments.</p><p>In summary, based on these experiments, we conclude that vGreen performs significantly better than E+, when the VM characteristics vary, and does as well as E+, when they are identical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Overhead</head><p>In our experiments we observed negligible runtime overhead due to vGreen. On the vgnodes, vgxen is implemented as a small module, which does simple performance counter operations and VCPU and VM metric updates. The performance counters are hardware entities with no overhead on software execution and accessing them is just a simple register read/write operation. The vgdom executes every T up_period (5s in our experiments), and as explained in section 3 just reads and transmits the VM metrics information to the vgserv. In our experiments, we observed negligible difference in execution time of all the benchmarks with and without vgxen and vgdom. We also observed negligible overhead of VM migration on execution times of benchmarks, which is consistent with the findings in <ref type="bibr" target="#b5">[5]</ref>. Across all our experiments, we had an average of around two VM migrations for each VM combination run, which is a very small overhead (&lt; 1%) compared to the overall timeframe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>In this paper we presented vGreen, a system for energy efficient computing in virtualized environments. The key idea behind vGreen is linking workload characterization to VM scheduling decisions to achieve better performance, energy efficiency and power balance in the system. We designed novel hierarchical metrics to capture VM characteristics, and develop simple balancing policies to achieve the above mentioned benefits. We implemented vGreen on a real life testbed of state of the art server machines, and show with SPEC and PARSEC benchmarks, that it can achieve improvement in performance and system level energy savings by up to 20% and 15% respectively over state of the art policies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Normalized Execution Time (b) Normalized Power Consumption</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparison of perl and mcf</figDesc><graphic coords="2,181.07,60.53,117.06,103.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overall vGreen design 3.2 Architecture</figDesc><graphic coords="3,85.43,386.89,178.79,128.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2</head><label>2</label><figDesc>Figure2illustrates the overall architecture of vGreen, which is based on a client-server model. Each PM in the cluster is referred to as a vGreen client/node (vgnode). There is one central vGreen server (vgserv), which manages VM scheduling across the vgnodes based on a policy (vgpolicy) running on the vgserv. The vgpolicy decisions are based on the value of different metrics, which capture MPC, IPC, and utilization of different VMs, that it receives as updates from the vgnodes running those VMs. The metrics are evaluated and updated dynamically by the vGreen modules in Xen (vgxen) and Dom0 (vgdom) on each vgnode. We now present the details of these vGreen modules.vgnode: A vgnode refers to an individual PM in the cluster. Each vgnode has vGreen modules (vgxen and vgdom) installed on them. The vgxen is a module compiled into Xen (see Figure2) and is responsible for characterizing the running VMs. In Xen, a VM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An example of Hierarchical Metrics in vGreen</figDesc><graphic coords="3,331.91,50.48,211.65,145.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a) Weighted Speedup of vGreen over E+ (b) Energy Savings of vGreen over E+</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Energy Savings and Weighted Speedup comparison ferent benchmarks, we choose nM P C th as 0.02 and nIP C th as 8. These threshold values allowed us to comfortably separate high MPC and high IPC VMs from each other.</figDesc><graphic coords="5,322.43,210.53,230.32,147.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: % Reduction in Power Imbalancein significant speedup of the high MPC VMs as observed in Figure4a. We can see in Figure4a, that some combinations achieve higher weighted speedup compared to others. For instance, for 2freq/2mcf combination is it over 45%, while for 2perl/2mcf it is around 30%. This difference is due to the fact that co-location of perl and mcf VMs results in a small slowdown of perl, while co-location of freqmine and mcf has no impact on the execution time of freqmine. This results in higher weighted speedup for 2freq/2mcf compared to 2perl/2mcf (refer to equation 3). The speedup also results in lower system energy consumption, since now the benchmarks run and consume active power for a smaller duration. We can see in Figure4bthat vGreen results in average system energy savings of around 15% across all the benchmark combinations. We can observe some combinations to have more savings than the others, which is primarily due to their different weighted speedups.Figure5illustrates the balance in power consumption across the two vgnodes achieved using vGreen over E+. The figure shows the %reduction in power consumption variance across the vgnodes to capture the degree of balance achieved. We can see that vGreen reduces the average power variance across the two vgnodes by close to 80%. This happens due to the better balance of IPC and utilization across the machines with vGreen, which results in well balanced and similar power consumption for the vgnodes compared to E+. This results in a better overall thermal and power profile and reduces power hotspots in the cluster.Homogeneous Workloads: We also experimented with combination of VMs running homogeneous benchmarks. We did experiments for all the six benchmarks in Table2, where all the fours VMs ran the same benchmark. We observed that in all the experiments, there was no possibility of rebalancing based on characteristics, since the MPC and IPC of the VMs were already balanced. Consequently, the results for all the three parameters were the same as that for E+ across all the experiments.In summary, based on these experiments, we conclude that vGreen performs significantly better than E+, when the VM characteristics vary, and does as well as E+, when they are identical.</figDesc><graphic coords="6,65.87,50.50,217.43,153.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>MPC Balance Algorithm</figDesc><table><row><cell cols="2">Input: vgnode n1</cell></row><row><cell cols="2">1: if nM P C n1 &lt; nMP C th then</cell></row><row><cell>2:</cell><cell>return</cell></row><row><cell cols="2">3: end if</cell></row><row><cell cols="2">4: vm_min ← min_mpc_vm(n1)</cell></row><row><cell cols="2">5: pm_min ← NULL</cell></row><row><cell cols="2">6: for all vgnodes n i except n1 do</cell></row><row><cell>7:</cell><cell></cell></row><row><cell>8: 9:</cell><cell>if !pm_min or nM P C pm_min &gt; nMP Cn i then pm_min ← n i</cell></row><row><cell>10:</cell><cell>end if</cell></row><row><cell>11:</cell><cell>end if</cell></row><row><cell cols="2">12: end for</cell></row><row><cell cols="2">13: if pm_min and (nM P C n1 -nM P C pm_min ) &gt; vMP C vm_min</cell></row><row><cell></cell><cell>then</cell></row><row><cell>14:</cell><cell>do_migrate(vm_min, n1, pm_min)</cell></row></table><note><p>if (nM P Cn i &lt; nMP C th ) and (nM P C n1 -nM P C th ) &gt; (nM P C th -nM P Cn i ) then</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>the target vgnode with the minimum nMPC subject to the condition in step 7. The condition states that the target vgnode (ni) nMPC (nM P Cn i ) must be below nM P C th by atleast (nM P Cn1 -nM P C th ). This is required, since otherwise migration of a VM from n1 to ni cannot bring n1 below the MPC threshold or might make ni go above the MPC threshold. In step 8 and 9, it stores the node ni as target minimum nMPC vgnode (pm_min), if its nMPC (nM P Cn i</figDesc><table><row><cell></cell><cell cols="2">Benchmarks Used</cell></row><row><cell>Suite</cell><cell>Benchmark</cell><cell>Characteristics</cell></row><row><cell>PARSEC</cell><cell cols="2">freqmine streamcluster Low IPC/High MPC High IPC/Low MPC</cell></row><row><cell></cell><cell>perl</cell><cell>High IPC/Low MPC</cell></row><row><cell>SPEC2K</cell><cell>bzip2 equake</cell><cell>High IPC/Low MPC Low IPC/High MPC</cell></row><row><cell></cell><cell>mcf</cell><cell>Low IPC/High MPC</cell></row><row><cell>tries to find</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="http://www.vmware.com/files/pdf/drs_datasheet.pdf" />
		<title level="m">VMware Dynamic Resource Scheduler</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Xen and the art of virtualization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dragovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Neugebauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Warfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SOSP &apos;03</title>
		<meeting>SOSP &apos;03</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The PARSEC benchmark suite: characterization and architectural implications</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bienia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. PACT &apos;08</title>
		<meeting>PACT &apos;08</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dynamic placement of virtual machines for managing SLA violations</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bobroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kochut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Beaty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Symposium on Integrated Network Management &apos;07</title>
		<meeting>International Symposium on Integrated Network Management &apos;07</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Live migration of virtual machines</title>
		<author>
			<persName><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Limpach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Warfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NSDI&apos;05</title>
		<meeting>NSDI&apos;05</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">System-level power management using online learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dhiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rosing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on CAD</title>
		<imprint>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Entropy: a consolidation manager for clusters</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hermenier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lorca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Menaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lawall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VEE&apos;09</title>
		<meeting>VEE&apos;09</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Live, runtime phase monitoring and prediction on real systems with application to dynamic power management</title>
		<author>
			<persName><forename type="first">C</forename><surname>Isci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Contreras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martonosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MICRO&apos;06</title>
		<meeting>MICRO&apos;06</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Using OS observations to improve performance in multicore systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Knauerhase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hohlt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Usher: an extensible framework for managing custers of virtual machines</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mcnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Voelker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. LISA&apos;07</title>
		<meeting>LISA&apos;07</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Balancing power consumption in multiprocessor systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Merkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bellosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGOPS Oper. Syst. Rev</title>
		<imprint>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">VirtualPower: coordinated power management in virtualized enterprise systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nathuji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schwan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SOSP &apos;07</title>
		<meeting>SOSP &apos;07</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The Eucalyptus open-source cloud-computing system</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nurmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Grzegorczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Obertelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Youseff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zagorodnov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Cloud Computing and Its Applications&apos;08</title>
		<meeting>Cloud Computing and Its Applications&apos;08</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">No &quot;power&quot; struggles: coordinated multi-level power management for the data center</title>
		<author>
			<persName><forename type="first">R</forename><surname>Raghavendra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc ASPLOS&apos;08</title>
		<meeting>ASPLOS&apos;08</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ensemble-level power management for dense blade servers</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Leech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Irwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chase</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISCA &apos;06</title>
		<meeting>ISCA &apos;06</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Symbiotic jobscheduling for a simultaneous mutlithreading processor</title>
		<author>
			<persName><forename type="first">A</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASPLOS&apos;00</title>
		<meeting>ASPLOS&apos;00</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Power-aware dynamic placement of HPC applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neogi</surname></persName>
		</author>
		<idno>ICS &apos;08</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Black-box and gray-box strategies for virtual machine migration</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Shenoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Yousif</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NSDI&apos;07</title>
		<meeting>NSDI&apos;07</meeting>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
