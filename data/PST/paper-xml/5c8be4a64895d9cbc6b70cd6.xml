<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Breast Cancer Diagnosis in DCE-MRI using Mixture Ensemble of Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Reza</forename><surname>Rasti</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering of K. N</orgName>
								<orgName type="laboratory">Artificial Intelligent Lab</orgName>
								<orgName type="institution">Toosi University of Technology</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Biomedical Engineering</orgName>
								<orgName type="institution">Isfahan University of Medical Sciences</orgName>
								<address>
									<settlement>Isfahan</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mohammad</forename><surname>Teshnehlab</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Electrical Engineering of K. N</orgName>
								<orgName type="institution">Toosi University of Technology</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Son</forename><surname>Lam</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">School of Electrical, Computer and Telecommunications Engineering</orgName>
								<orgName type="institution">University of Wollongong</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Breast Cancer Diagnosis in DCE-MRI using Mixture Ensemble of Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4402D771C4802E8A986003FD32F547C0</idno>
					<idno type="DOI">10.1016/j.patcog.2017.08.004</idno>
					<note type="submission">Received date: 19 February 2017 Revised date: 24 June 2017 Accepted date: 3 August 2017</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Pattern Recognition Breast cancer</term>
					<term>DCE-MRI</term>
					<term>convolutional neural networks</term>
					<term>mixture ensemble of experts</term>
					<term>CAD systems</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work addresses a novel computer-aided diagnosis (CAD) system in breast dynamic contrastenhanced magnetic resonance imaging (DCE-MRI). The CAD system is designed based on a mixture ensemble of convolutional neural networks (ME-CNN) to discriminate between benign and malignant breast tumors. The ME-CNN is a modular and image-based ensemble, which can stochastically partition the high-dimensional image space through simultaneous and competitive learning of its modules. The proposed system was assessed on our database of 112 DCE-MRI studies including solid breast masses, using a wide range of classification measures. The ME-CNN model composed of three CNN experts and one convolutional gating network achieves an accuracy of 96.39%, a sensitivity of 97.73% and a specificity of 94.87%. The experimental results also show that it has competitive classification performances compared to three existing single-classifier methods and two convolutional ensemble methods. The proposed ME-CNN model could provide an effective tool for radiologists to analyse breast DCE-MRI images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Highlights</head><p>• A new method for breast cancer diagnosis in DCE-MRI is presented.</p><p>• We propose a mixture ensemble of convolutional neural networks for image classification.</p><p>• A convolutional gating network coordinates simultaneous, competitive learning of CNN experts.</p><p>• ME-CNN ensemble model is efficient for biomedical problems with a limited number of samples.</p><p>• The proposed model performs comparatively well on a DCE-MRI dataset of 112 patients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Breast cancer is the second leading cause worldwide of cancer-related deaths in women, exceeded only by lung cancer <ref type="bibr" target="#b0">[1]</ref>. According to the American Cancer Society, in the US alone an estimate of 231,840 new cases of invasive breast cancer and 40,290 breast cancer deaths occurred in 2015. On a more positive note, death rates from breast cancer have been declining since 1989, with larger decreases in women under the age of 50. One reason for this decline is attributed to earlier detection of breast masses via screening, which is an important step in the treatment of this disease <ref type="bibr" target="#b1">[2]</ref>.</p><p>In breast image analysis for cancer diagnosis, the multiplicity and complexity of the lesions may occur with dense tissue interactions, so it will be difficult for radiologists to detect and analyze masses precisely. Therefore, many imaging techniques and computer aided systems Email addresses: mr.r.rasti@ieee.org (Reza Rasti), teshnehlab@eetd.kntu.ac.ir (Mohammad Teshnehlab), phung@uow.edu.au (Son Lam Phung)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprint submitted to Pattern Recognition August 4, 2017</head><p>A C C E P T E D M A N U S C R I P T have been developed to assist radiologists. These developments have a major contribution to the identification and treatment of breast cancer.</p><p>It has been shown that DCE-MRI between different clinical imaging is a valuable diagnostic tool for breast cancer due to the high sensitivity and high resolution in dense breast tissues <ref type="bibr" target="#b2">[3]</ref>. Compared to mammography and ultrasound, MR imaging is the most sensitive imaging modality for detecting breast cancer among high-risk populations <ref type="bibr" target="#b3">[4]</ref>. Correct interpretation of breast DCE-MRI images depends significantly on the visualization quality, operator experience, and time available for data analysis. Because manual analysis of MR slices is time-consuming and error-prone, many dedicated systems have been developed to assist radiologists in the localization or diagnosis of breast lesions. Although some systems are currently used in clinical situations, fully automatic detection or diagnosis of breast lesions is still an open problem in DCE-MRI <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>This study proposes a novel CAD system for breast DCE-MRI. The system has two major stages: i) tumor candidate segmentation based on the intensity and morphological information of the masses in the image; and ii) tumor classification based on a new deep learning ensemble of convolutional neural networks (CNNs). The performances of the proposed segmentation algorithm and the diagnostic ensemble model are assessed in a real dataset constituted of 112 patients (women with high or intermediate risk).</p><p>The paper is structured as follows: Section 2 presents the related works in the breast cancer detection and diagnostic systems, and recent trends in convolutional ensemble methods. Section 3 first describes the image acquisition, pre-processing, ROI selection processes, and CNN model and a batch training algorithm based on resilient propagation inspired by <ref type="bibr" target="#b6">[7]</ref>. It then presents the proposed mixture of CNN experts (ME-CNN), and a learning method for an ensemble of six-layer CNNs. Section 4 presents the experimental results of the proposed ensemble method and discusses its comparative performances with several existing methods. Section 5 gives the research conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Several CAD systems utilizing different breast imaging techniques have been developed for the detection and diagnosis of breast masses. However, there are only a few publications on CAD systems using DCE-MRI, which are summarized in Table <ref type="table" target="#tab_0">1</ref>. In general, the existing approaches use either hand-crafted features <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8]</ref> or automatically-learned features <ref type="bibr" target="#b8">[9]</ref>. For hand-crafted features, traditional steps are applied to extract features from the input image, and the focus is on feature engineering. For automatically learned features, the focus is on learning directly the mapping from the input image to the classification label.</p><p>Convolutional neural network (CNN) is one of the most powerful techniques in the automatic feature-learning approach. It has been used successfully for numerous applications, including object recognition and image classification <ref type="bibr" target="#b13">[14]</ref>, handwritten digit recognition <ref type="bibr" target="#b14">[15]</ref>, character recognition <ref type="bibr" target="#b15">[16]</ref>, face detection <ref type="bibr" target="#b16">[17]</ref>, speech recognition <ref type="bibr" target="#b17">[18]</ref>. In medical image analysis area, CNNs have been applied for many tasks and imaging modalities, such as pulmonary nodules detection in chest CT scans <ref type="bibr" target="#b18">[19]</ref>, colonic polyps detection in CT colonography <ref type="bibr" target="#b19">[20]</ref>, nuclei detection and classification in histopathological images <ref type="bibr" target="#b20">[21]</ref>, brain lesion segmentation in MRI <ref type="bibr" target="#b21">[22]</ref>, and hemorrhage detection in color fundus images <ref type="bibr" target="#b22">[23]</ref>.</p><p>To improve the classification accuracy, ensemble methods have been developed, where several CNN outputs are fused using an additional network <ref type="bibr" target="#b23">[24]</ref>, or a softmax layer <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>. Table 2 summarizes the recent works on medical applications based on an ensemble of CNNs. More theoretical basis for mixture ensemble is given in <ref type="bibr" target="#b26">[27]</ref>. In this article, we explore the capability of CNN and convolutional ensemble methods for breast cancer diagnosis in DCE-MRI. It is an extended version of our work in <ref type="bibr" target="#b8">[9]</ref> to include a significantly larger database, an improved segmentation method, and a more accurate diagnostic classifier.</p><formula xml:id="formula_0">A C C E P T E D M A N U S C R I P T</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Material and Methods</head><p>In this section, we first describe the image data acquisition for our DCE-MRI dataset, and present several pre-processing techniques to segment and select the region of interest from an input breast MR image. We then describe the architecture and mathematical model of convolutional neural network. Finally, we present the proposed mixture ensemble of CNNs for the classification of benign versus malignant breast tumors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Image Data Acquisition</head><p>The data used in this work were collected at the Imaging Center of Milad Hospital in Tehran. The screening protocol was a 1.5 Tesla Siemens Healthcare MRI system, which has a dedicated 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>bipolar phased-array breast coil. This provided T1-weighted axial image data over the whole breast, obtained with a radio frequency spoiled gradient-recalled sequence with the image size of 512 × 512 pixels. Imaging was performed prior to and subsequent to a bolus injection of 0.2 mmol/kg of Gd-DTPA in both manual and intravenous ways. The injection was followed by the 15cc normal saline. Then, 26 breast MR image acquisitions in each case, including one pre-contrast and other post-contrast series, were acquired. After agent injection, the first acquisition was at the 90th second, and subsequent acquisitions were obtained every 100 seconds approximately. Each axial acquisition series included 128 axial slices on average. Overall, 112 DCE-MRI breast examinations from high-or intermediate-risk patients were obtained. These examinations were histopathologically proven to be malignant (53 cases) or benign (59 cases). The patients' age ranged from 37 to 71 years (the mean age: 48 years). The effective lesion radius had a range of 3.8-31.4 mm and an average of 12.3 mm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pre-processing, Segmentation, and ROI Selection</head><p>A set of breast axial slices was selected and annotated by an expert radiologist. The set presented the best appearance of the lesions among other image slices. The slices were labeled according to patients' IDs. For the first selected post-contrast slices, the subtraction image set was obtained by subtracting the corresponding pre-contrast ones.</p><p>The general steps of the automatic ROI selection algorithm are shown in Fig. <ref type="figure" target="#fig_0">1</ref>. The first step was background reduction via first post-contrast subtraction, followed by contrast enhancement and breast regions cropping. The aim of breast regions cropping step was to locate approximately the breast tissues and reduce the disturbing effects of other anatomical structures such as the chest. The size and position of the rectangular cropping was determined empirically based on the image acquisition settings. Next, the global Otsu thresholding <ref type="bibr" target="#b34">[35]</ref> and the morphological top-hat filtering <ref type="bibr" target="#b35">[36]</ref> were performed to remove non-lesion structures. After connected components labeling, pseudo lesions were isolated for further processing. Radius-based filtering was then applied to remove regions with radius outside the range [r min , r max ]. In our case, r min = 3.8 mm, and r max = 31.4 mm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background Reduction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contrast Enhancement</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Breast Regions Cropping</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global Otsu Thresholding</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Active Contour Segmentation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Radius-based Filtering</head><note type="other">Connected</note><p>Then, localized active contour (LAC) segmentation was applied to each remaining lesion of interest. Here, we used the Chan-Vese active contour model <ref type="bibr" target="#b36">[37]</ref>, with parameters with Max-iter = 60, Mu = 0.1, and Sigma = 4. The LAC segmentation step was useful because it recovered tumor pixels that were removed by morphological operations. 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>To reduce false positives, companess-based filtering was applied next. The compactness C of a region is defined as C = s x s y max(s x ,s y ) 2 , where s x and s y are width and height of the bounding box, respectively <ref type="bibr" target="#b37">[38]</ref>. A region was removed if its compactness C was less than 0.2. Fig. <ref type="figure" target="#fig_1">2</ref> shows example outputs of the preprocessing, segmentation and ROI selection, described in this section. Overall, 562 ROIs (244 malignant and 318 benign) were extracted with the ROI selection algorithm over the whole set slices.</p><p>To conclude Section 3.2, the techniques presented here are effective for the real DCE-MRI dataset we have to deal with. However, they can be made more general and less ad-hoc by devising methods to determine the thresholds adaptively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Convolutional Neural Network</head><p>A brief description of convolutional neural network is warranted because it is a key tool in this paper. CNN, originally proposed by LeCun <ref type="bibr" target="#b38">[39]</ref>, is a neural network model with three main architectural ideas: local receptive fields, weight sharing, and sub-sampling in the spatial domain. The network was initially designed for the recognition of two-dimensional image patterns, and has become one of the major architectures in deep learning <ref type="bibr" target="#b39">[40]</ref>. CNN has many strengths. First, feature extraction and classification are integrated into one structure and are fully adaptive. Second, the network extracts 2-D image features at increasing dyadic scales. Third, it is relatively invariant to image noise and local geometric distortions.</p><p>A CNN consists of three main types of layers: (i) convolution layers, (ii) sub-sampling (or max-pooling) layers, and (iii) an output layer. Network layers are arranged in a feed-forward structure: each convolution layer is followed by a sub-sampling layer, and the last convolution layer is followed by the output layer. The convolution and sub-sampling layers are considered as 2-D layers, whereas the output layer is considered as a 1-D layer. In CNN, each 2-D layer has several planes. A plane consists of neurons that are arranged in a 2-D array. The output of a plane is called a feature map.</p><p>Mathematical model: Consider a CNN shown in Fig. <ref type="figure" target="#fig_2">3</ref> with 6 layers: convolution C1, subsampling S2, convolution C3, subsampling S4, convolution C5, and output O6. For convolutional layer l (where l = 1, 3, 5, ...), let f l be the activation function of layer l. Let W l m,n be the 2-D convolution kernel for the connection from feature map m in layer (l -1) to feature map n in layer l. Let b l n be the bias term associated with feature map n. Let p l n denote the set of all planes in the layer (l -1) that are connected to feature map n. The output feature map n of convolution layer l is defined as  where ⊗ denotes the 2-D convolution, and o l-1 m is output feature map of layer l -1. For the sub-sampling layer l (where l = 2, 4, ...), output feature map n is defined as</p><formula xml:id="formula_1">o l n = f l          m∈p l n o l-1 m ⊗ W l m,n + b l n          ,<label>(1)</label></formula><formula xml:id="formula_2">A C C E P T E D M A N U S C</formula><formula xml:id="formula_3">o l n = f l w l n P(o l-1 n ) + b l n ,<label>(2)</label></formula><p>where w l n and b l n are the scalar weights, and the bias term. Here, P(.) denote the pooling operation (e.g. max-pooling or average pooling).</p><p>For output layer l, each neuron is fully connected to all feature maps of the previous layer. The response of an output neuron is given as</p><formula xml:id="formula_4">o l = f l          m∈p l o l-1 m w l m + b l          .<label>(3)</label></formula><p>Here, p l denotes the set of feature maps in layer C l-1 that are connected to the output neuron.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN training algorithm:</head><p>In this paper, we use a batch training algorithm based on the resilient back-propagation or RPROP, proposed in <ref type="bibr" target="#b40">[41]</ref>. In the RPROP algorithm, the learning step is increased or decreased, depending only on the sign of the error gradient. This algorithm has been shown to converge faster compared to the standard gradient descent algorithm <ref type="bibr" target="#b6">[7]</ref>. RPROP works well even when the gradient has very small magnitudes. Furthermore, it is computationally efficient because only the first-order derivative of the error function is required. Detail implementation of RPROP training algorithm for CNN is given in our technical report <ref type="bibr" target="#b41">[42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Proposed Mixture Ensemble of Convolutional Experts</head><p>We propose a mixture ensemble of convolutional neural networks (or ME-CNN) for classification of an ROI image as benign or malignant. An overview of the mixture ensemble model is shown in Fig. <ref type="figure" target="#fig_3">4</ref>. There are L experts and one gating network, which share the same input. Each expert could be specialized in one region of the high-dimensional input space. The gating network is trained to produce input-adaptive weights that are used to fuse the outputs of the experts. In the </p><formula xml:id="formula_5">g i = exp(G i ) L j=1 exp(G j ) .<label>(4)</label></formula><p>Clearly, the fusion weights satisfy the condition:</p><formula xml:id="formula_6">L i=1 g i = 1.<label>(5)</label></formula><p>The final output of the mixture ensemble of convolutional experts is given as</p><formula xml:id="formula_7">O = L i=1 g i O i .<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ME-CNN training:</head><p>The use of CNNs in the mixture ensemble requires significant modification to the training algorithm. As mentioned previously, CNN experts and convolutional gating network are trained simultaneously in our approach. Consider a training set of K samples, where x k is the k-th input sample and d k the desired output vector.</p><p>For the CNN experts, we define, similarly to <ref type="bibr" target="#b26">[27]</ref>, their error function for the k-th input sample as</p><formula xml:id="formula_8">E k expert = -ln        L i=1 g k i exp - 1 2 d k -O k i 2        ,<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>where O k i is the output vector of CNN expert i. For CNN expert i, the effective error signal for all training samples (a component in the derivative of E k expert ) can be written as</p><formula xml:id="formula_9">e i = K k=1 h k i (d k -O k i ),<label>(8)</label></formula><p>where</p><formula xml:id="formula_10">h k i = g k i exp -1 2 d k -O k i 2 L j=1 g k j exp -1 2 d k -O k j 2 .<label>(9)</label></formula><p>Here, h k i can be considered as a posterior-probability estimate produced by expert i for input sample x k . Conceptually, it means that h k i is high if: 1) output O k i of expert i for input sample x k is close to the desired output d k ; and 2) weight g k i given by the CGN to expert i is high. For the convolutional gating network, we define its total error function as</p><formula xml:id="formula_11">E gate = 1 2 K k=1 h k -g k 2 ,<label>(10)</label></formula><p>where</p><formula xml:id="formula_12">h k [h k 1 , h k 2 , . . . , h k L ]</formula><p>T is a vector of the experts' posterior probability estimates, and</p><formula xml:id="formula_13">g k [g k 1 , g k 2 , . . . , g k L ]</formula><p>T is the CGN output vector. Once the error signal for CNN experts is defined in <ref type="bibr" target="#b7">(8)</ref> and the error function for the CGN is defined in <ref type="bibr" target="#b9">(10)</ref>, we can compute the gradient w.r.t the free parameters of the networks via the error back-propagation algorithm, in a similar way as <ref type="bibr" target="#b41">[42]</ref>. Subsequently, the RPROP algorithm can be used to determine simultaneously the free parameters of the CNNs and CGN. In this way, each CNN expert is trained to specialize in a region of the high-dimensional input space, whereas the convolutional gating network is trained to produce optimized fusion weights g i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results and Discussion</head><p>In this section, we first describe the performance measurements and the experimental method (Subsection 4.1). We then analyze the classification performance of the CNN in comparison with three other single-classifier methods (Subsection 4.2). Finally, we discuss the performance of the proposed ME-CNN method in relation to two other convolutional ensemble methods in Subsection 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Performance Measures and Experimental Method</head><p>Classification performance in this binary problem (benign versus malignant breast tumor) was assessed based on the following basic measures:</p><p>• True positives (TP) is the number of test samples in the malignant class that are correctly classified.</p><p>• True negatives (TN) is the number of test samples in the benign class that are correctly classified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>• False positives (FP) is the number of test samples in the benign class that are incorrectly classified.</p><p>• False negatives (FN) is the number of test samples in the malignant class that are incorrectly classified.</p><p>Subsequently, other measures including accuracy (Acc), sensitivity (Se), specificity (Sp), positive predictive value (PPV), negative predictive value (NPV) are computed as follows:</p><formula xml:id="formula_14">Accuracy = T P + T N T P + T N + FP + FN , (<label>11</label></formula><formula xml:id="formula_15">)</formula><formula xml:id="formula_16">Sensitivity = T P T P + FN , (<label>12</label></formula><formula xml:id="formula_17">)</formula><formula xml:id="formula_18">Specificity = T N T N + FP , (<label>13</label></formula><formula xml:id="formula_19">)</formula><formula xml:id="formula_20">PPV = T P T P + FP , (<label>14</label></formula><formula xml:id="formula_21">)</formula><formula xml:id="formula_22">NPV = T N T N + FN . (<label>15</label></formula><formula xml:id="formula_23">)</formula><p>Other performance indicators include the Receiver Operating Characteristic (ROC) curve and Area Under the ROC Curve (AUC).</p><p>In addition to the above measures, statistical significance of performance difference between two methods was determined by the bootstrap method. For this purpose, 10,000 bootstraps were applied to compute the 95% confidence interval for the AUC value <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>. The p-values of pairwise comparison of ROC curves were calculated. If the p-value is less than 0.05, the conclusion is that the two methods have statistically different performances.</p><p>To compute classification measures, 5-fold cross-validation was applied at the patient level. The data set was divided into five partitions of approximately equal sizes. Four partitions were used for training and the remaining partition was used for testing. This step was repeated five times until all different choices for the test set were evaluated. The classification measures were averaged over the five folds. Furthermore, to increase the amount of training data, each sample in the training partitions was rotated by 90 o , 180 o , and 270 o , and flipped horizontally to give 4 additional training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparative Evaluation of the CNN</head><p>In this experiment, we analyzed the performance of CNN in classifying benign versus malignant breast MR tumors, and compared it with other feature extraction and classification methods. Following this purpose, for all extracted ROIs, 10 scalar features were extracted according to <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b44">[45]</ref> and <ref type="bibr" target="#b45">[46]</ref>. They included five shape features ( circulatory, spiculation, roughness, NRLmean, and NRL-entropy) and five texture features (angular second moment (ASM), correlation, sum average, sum variance and sum entropy).</p><p>In the benchmark analysis, we investigated four different classifiers: multilayer perceptron, support vector machine, random forests classifier, convolutional neural network.</p><p>• Multi-layer perceptron (MLP): Several fully-connected MLPs with one hidden layer and sigmoid neurons were considered. The number of neurons in a hidden layer were varied</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T from 10 to 30 in a step of 2. The number of neurons in the output layer was 2. Four different training algorithms were applied: gradient descent with momentum (GDM), resilient propagation (RPROP), scaled conjugate gradient (SCG), and Levenberg-Marquardt (LM).</p><p>The training parameters are: max epoch =100, and MSE goal = 10 -5 .</p><p>• Support vector machine (SVM): Radial basis function was used as the kernel; the kernel coefficient was set to γ = 0.1. Penalty parameter C was set to 2 i , with i varied between -4 and 4 with a step of 2.</p><p>• Random forests (RF): The maximum number of trees was varied among 50, 100, 200, and 500. The max-depth of the tree was equal to the number of features (n = 10) <ref type="bibr" target="#b46">[47]</ref>. RF classifier is a state-of-the-art method for breast cancer detection in DCR-MRI <ref type="bibr" target="#b5">[6]</ref>.</p><p>• Convolutional neural network (CNN): The CNN was applied to process directly each ROI of size 32 × 32 pixels. We evaluated five different CNNs, each having six active layers. The activation function for C-layers was the sigmoid, which was proposed by LeCun <ref type="bibr" target="#b47">[48]</ref>: f (x) = 1.7159 tanh(2x/3). The activation function for S-layers was the linear function. The activation function for the output layer was the tanh. Three sub-sampling methods were considered for 2 × 2 pixel blocks: ave-pooling, sum-pooling, and maxpooling. Tables <ref type="table">3</ref> and<ref type="table" target="#tab_3">4</ref> summarize the structural information of these networks: feature map sizes, kernel sizes, number of feature maps per layer, connection types, and the total number of free parameters.</p><p>Training was done in batch mode, and the batch size varied from 32, 64, 128, 256 samples to the full-size (100% of the training samples). The RPROP algorithm <ref type="bibr" target="#b40">[41]</ref> was used for training the CNNs, where the step size for weight update was dynamically changed by an increment factor η + or a decrement factor η -, depending on the sign of the gradient, see <ref type="bibr" target="#b6">[7]</ref>. Different values for η + and η -were evaluated on the training set in order to find suitable ones: η + from 1.01 to 1.10, and η -from 0.90 to 0.99 with a step of 0.01. A good property of the RPROP algorithm is that it works for many values of η + and η -, as long as they are close to 1. The maximum number of training epochs is max epoch =300, and the training target is MSE goal = 10 -2 .</p><p>We implemented the CNNs in MATLAB; this library is available online <ref type="foot" target="#foot_0">1</ref> . The experiment was run on a computer with Core-i7 CPU at 4GHz (AMD FX-8350: 4.7M), 16 GB of RAM, and Windows-7 64-bit operating system. Training CNN5 took 3.8s per ROI on average. Testing CNN5 took 0.003s per ROI on average (classification time).</p><p>Table <ref type="table">3</ref>: Size of feature maps and kernels for the implemented CNNs with 6 active layers.</p><formula xml:id="formula_24">Layer C1 S2 C3 S4 C5 O6 Feature map size 28×28 14×14 12×12 6×6 1×1 1 Kernel size 5×5 2×2 3×3 2×2 6×6 1</formula><p>Table <ref type="table" target="#tab_4">5</ref> shows comparative performances of the individual MLP, SVM, RF and CNN classifiers. A number of observations can be drawn. First, if only hand-crafted features were used, the random forests classifier performed better than the SVM and the MLP. Second, the CNN classifier (CNN5 with 1001 free parameters) achieved a classification rate of 92.77%, which is statistically higher than the RF, SVM, and MLP classifiers. Using the bootstrapping method, the p-value between the ROC curves of the RF and CNN classifier is 0.041 (p &lt; 0.05). This result show that the features automatically learnt by the CNN are more discriminative than the existing hand-crafted features.</p><formula xml:id="formula_25">A C C E P T E D M A N U S C R I P T</formula><p>The classification results also indicated that the pooling method and the numbers of 2-D feature maps in C1 and C3 layers played an important role in the diagnostic performance. Max pooling performed better than average pooling or sum pooling in this classification problem (benign-versus-malignant breast tumor). For fixed training data, an increase in the number of feature maps leads to an increase in the number of free parameters in the network and causes overfitting. At present, there is no theoretical framework to decide the optimal number of feature maps. In our diagnostic problem, this decision was made based on empirical experimentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparative Evaluation of the Proposed ME-CNN</head><p>In this experiment, we evaluated the classification performance of the proposed ME-CNN. The simple CNN1 structure, described in Tables <ref type="table">3</ref> and<ref type="table" target="#tab_3">4</ref> , was used as CNN expert. Max-pooling was selected for sub-sampling the convolutional feature maps. The number of CNN experts were varied between 2, 3, 4, and 5. A convolutional gating network (CGN) as shown in Table <ref type="table" target="#tab_5">6</ref> was used. Note that the number of output neurons in the CGN is equal to the number of CNN experts. The CGN used the sigmoid activation function for all layers.</p><p>For training the ME-CNN, we used the RPROP algorithm in the full batch mode. The MSE goal was 0.01, and the maximum number of epochs was 300. Using a grid search, the following learning parameters were selected: i) η + exp = 1.02, and η - exp = 0.98 for CNN experts; ii)  To gain insights on the comparative performances of the proposed ME-CNN, we also evaluated two other convolutional ensemble methods: ave-ensemble and soft-ensemble.</p><formula xml:id="formula_26">η + g = A C C E P T E D M A N U S C R I P T</formula><p>• Ave-ensemble <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>: This method trains several CNNs from the available groundtruth labels. For a given test image, the outputs of the CNNs are averaged to generate the final output of the ensemble. We experimented with ensembles of 2, 3, 4, and 5 CNNs with structures as shown in Table <ref type="table" target="#tab_3">4</ref>.</p><p>• Soft-ensemble <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>: This method uses several parallel CNN branches. At the end of each convolutional branch, feature maps are concatenated. All features are then processed by a fully-connected layer, followed by a softmax layer for final output. In other words, this method does not use a convolutional gating network for fusion. In our experiment, the number of CNN experts were varied between 2, 3, 4, and 5.</p><p>Note that for both ave-ensemble and soft-ensemble methods, the standard MSE cost function was used for training. The size of 2-D feature maps and kernels was the same as in Tables <ref type="table">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computation Efficiency:</head><p>Training the ME-CNN took 4.6s per ROI on average. In comparison, training the Ave-ensemble and the Soft-ensemble took on average 7.9s and 4.2s per ROI, respectively. Testing the ME-CNN took 0.004s per ROI on average. Testing the Ave-ensemble and the Soft-ensemble took on average 0.007s and 0.004s per ROI, respectively.</p><p>Classification Accuracy: Table <ref type="table" target="#tab_6">7</ref> shows the classification performances, estimated via the 5fold cross validation, of three methods: ME-CNN, ave-ensemble, and soft-ensemble. To provide a baseline for comparison, the classification measures for the single-expert methods (CNN5 and RF) are also included in the table. The diagnostic accuracy for different ensemble methods were: ME-CNN = 96.39%, Soft-ensemble = 95.18%, Ave-ensemble = 93.98%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T   <ref type="table" target="#tab_7">8</ref> shows the p-values, which were obtained via bootstrapping, for pair-wise comparison between the RF, CNN5, Ave-ensemble, Soft-ensemble, and ME-CNN. By combining Tables 7 and 8, we can conclude that the ME-CNN had significantly higher accuracy than the Ave-ensemble and the single-classifier methods (CNN5 and RF); the corresponding pair-wise p-values were all less than 0.05 (for 95% confidence level). Furthermore, the ME-CNN has a statistically similar accuracy as the Soft-ensemble; the p-value between the two methods was 0.064 (greater than 0.05).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Roles of CNN experts and CGN:</head><p>The above improvement in classification accuracy of the proposed ME-CNN can be attributed to the fact that CNN experts are trained simultaneously in a competitive manner. The convolutional gating network is trained to play the adaptive weighting role. Over time, the CGN can partition the high-dimensional input space into subspaces depending on individual expert's performance.</p><p>To analyze the CNN experts' ability to partition the input space, we computed disagreement factors and the correlation coefficients. For a pair of CNN experts, the disagreement factor is the ratio between the number of samples on which they disagree versus the total number of samples <ref type="bibr" target="#b48">[49]</ref>. The correlation coefficient is computed over the two sets of real outputs that the two CNNs produced for the test set. For the trained ME-CNN in our experiment, the average disagreement factor between the three experts was relatively high (0.61), and the average correlation coefficient was very low (0.03).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ME-CNN versus Soft-ensemble:</head><p>Although the classification accuracy of the ME-CNN and Soft-ensemble was similar, the ME-CNN had several advantages. Firstly, the ME-CNN used significantly fewer free parameters than the Soft-ensemble (703 versus 1046 parameters), and therefore was less prone to overfitting. This is useful for medical diagnosis applications where the number of training samples is typically small, due to the limited number of patients. Secondly, the ME-CNN took fewer training iterations (epochs) converge than the Soft-ensemble, as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>shown in Fig. <ref type="figure" target="#fig_5">6</ref>(a). Thirdly, in our experiment as the number of experts or branches increased, the classification accuracy of the ME-CNN did not deteriorate as quickly as the Soft-ensemble, see Fig. <ref type="figure" target="#fig_5">6</ref>(b). At present, there is no analytical method to determine the optimum number of convolutional experts. For practical applications, a possible method for estimating a suitable number of experts is through empirical evaluation (e.g. cross-validation) on the training data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This article presents a CAD system for the diagnosis of breast cancer in DCE-MRI. For the localization stage, we present an automatic segmentation method based on local active contours for finding the region of interest from a breast MR image. For the diagnosis stage, we propose a new model for mixture ensemble of convolutional neural networks, called ME-CNN, for classification of benign versus malignant breast tumors. In the proposed model, the experts and gating network, all based on the CNNs, are trained simultaneously in an end-to-end optimization approach. Experimental results on a dataset of 112 DCE-MRI cases have shown that the proposed ME-CNN achieves competitive classification performances, compared with existing classifiers and ensemble methods. It also has the advantages of fast execution time in both training and testing, and a compact structure with small number of free parameters. By expanding the dataset to include non-mass lesions and more cases (patients), the proposed diagnostic approach has the potential to support radiologists in breast MR data analysis. Among several promising directions, one could extend the ME-CNN approach to the pre-processing stage, by combining with recent advances in fully convolutional networks for semantic segmentation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The main steps for ROI selection in breast DCE-MRI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example of automatic MR tumor segmentation and ROI generation. Top row: benign tumors; Bottom row: malignant tumors.</figDesc><graphic coords="7,207.18,256.23,154.05,58.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A convolutional neural network with 6 layers for processing input image of size 32 × 32 pixels. From left to right, the number of feature maps is 2, 2, 5, 5, 5, 1, and the size of weight kernels is 5 × 5, 1 × 1, 3 × 3, 1 × 1, 6 × 6, 1 × 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Schematic diagram of the proposed mixture ensemble of convolutional experts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 : 2 -</head><label>52</label><figDesc>Figure 5: 2-D feature maps by layer C1 of different modules in the ME-CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Comparison of the ME-CNN and Soft-ensemble: (a) Average training MSE versus training epoch; (b) Classification rate on the test sets versus the number of experts/branches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of recent computer-aided systems for breast DCE-MRI.</figDesc><table><row><cell cols="2">Author Year</cell><cell>Database</cell><cell>Problem</cell><cell>Method</cell><cell>Results</cell></row><row><cell>Chang et al. [10]</cell><cell>2012</cell><cell>132 solid masses (63 benign, 69 malignant)</cell><cell>Diagnosis</cell><cell>Analysis of the characteristic kinetic curve of MR masses derived from fuzzy c-means clustering (FCM) + binary logistic regression classification.</cell><cell>Acc=86.36%, Se=85.51%, Sp=87.30%, AUC=0.9154</cell></row><row><cell>Huang et al. [8]</cell><cell>2013</cell><cell>95 solid masses (44 benign, 51 malignant), 82 patients</cell><cell>Diagnosis</cell><cell>3-D MR morphological features extracted using texture-based gray-level co-occurrence matrix (GLCM), tumor shape + ellipsoid fitting classification.</cell><cell>Acc=88.42%, Se=88.24%, Sp=88.64%, AUC=0.8926</cell></row><row><cell>Soares et al. [11]</cell><cell>2013</cell><cell>35 lesions (20 benign, 15 malignant)</cell><cell>Diagnosis</cell><cell>3D lacunarity multi-fractal analysis to characterize spatial complexity of DCE-MR breast tumors at multiple scales + SVM classifier.</cell><cell>AUC=0.96</cell></row><row><cell>Chang et,al. [12]</cell><cell>2014</cell><cell>95 biopsy confirmed lesions (28 benign, 67 malignant), 54 patients</cell><cell>Detection</cell><cell>Motion registration, detection of focal tumor breast lesions using kinetic features extracted from pixel-based time-signal intensity curve (TIC), morphological features of detected lesions.</cell><cell>Detection rate of 92.63% of all tumor lesions, with 6.15 FP/case.</cell></row><row><cell>Hassanien et al. [13]</cell><cell>2014</cell><cell>25 MR images, 46 test samples 90 train samples,</cell><cell>Diagnosis</cell><cell>Hybrid approach combining type-II fuzzy sets, adaptive ant-based statistical-based feature extraction clustering, multilayer perceptron (MLP) classifier,</cell><cell>Acc=95.1%</cell></row><row><cell>Rasti et al. [9]</cell><cell>2015</cell><cell>30 MR images, 120 samples each for benign and malignant</cell><cell>Diagnosis</cell><cell>A semi-automatic algorithm for lesion segmentation and ROI selection ROI classification by CNNs with ave-pooling</cell><cell>Acc=98.7%</cell></row><row><cell>Gubern et al. [6]</cell><cell>2015</cell><cell>209 lesions: 114 benign, 105 malignant (55 mass, 50 non-mass)</cell><cell>Detection</cell><cell>Lesion detection using features of blobs and enhanced voxels, Estimation of malignancy score using RF classifiier and region-based morphological and kinetic features.</cell><cell>Acc=89% (91% for mass-like, 86% for non-mass-like malignant lesions) at 4 FP/normal case.</cell></row><row><cell cols="5">Acc=accuracy; Se=sensisivity; Sp=specificity; AUC=area under the ROC curve (See Section 4.1).</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>A summary of convolutional ensemble methods in medical applications.</figDesc><table><row><cell cols="2">Author Year</cell><cell>Application</cell><cell>Ensemble Method</cell><cell>Notes</cell></row><row><cell>Ciresan et al. [28]</cell><cell>2013</cell><cell>Mitosis detection in breast cancer histology images</cell><cell>Output map averaging of 2 CNNs</cell><cell>Fusion of 2 independently trained CNNs</cell></row><row><cell>Wang et al. [29]</cell><cell>2014</cell><cell>Mitosis detection in breast cancer histology images</cell><cell>Cascaded ensemble of a learned CNN and a set of + a weighted average of the outputs hand-crafted features + Random forests classifiers</cell><cell>Features are extracted independently.</cell></row><row><cell>Brebisson et al. [30]</cell><cell>2015</cell><cell>Anatomical brain segmentation in MRI</cell><cell>8 simple CNN branches fused at fully connected layers according to 8 different inputs + 2 FC layers + a softmax layer output</cell><cell>The model is a deep neural network architecture with end-to-end learning for segmentation.</cell></row><row><cell>Song et al. [31]</cell><cell>2015</cell><cell>Cervical cytoplasm and nuclei segmentation</cell><cell>3 branches of CNNs fused at fully connected layers and a softmax layer output</cell><cell>Each branch has a different scaled input -a multi-scale CNN with end-to-end learning.</cell></row><row><cell>Maji et al. [32]</cell><cell>2016</cell><cell>Retinal vessels detection in fundus images</cell><cell>Output map averaging of 12 CNNs</cell><cell>Each CNN is trained independently on patches randomly selected from the training images.</cell></row><row><cell>Chen et al. [33]</cell><cell>2016</cell><cell>Mitosis detection in breast cancer histology images</cell><cell>Output map averaging of 3 CNNs</cell><cell>Each CNNs is a different CaffeNet [34] trained independently with transfer learning method.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Other structural information of the implemented CNNs.</figDesc><table><row><cell>Structure</cell><cell>C1 FM</cell><cell>C3 FM</cell><cell>C5 FM</cell><cell>O6 FM</cell><cell>C3 connections</cell><cell>C1, S2, S4, C5 connections</cell><cell>Free parameters</cell></row><row><cell>CNN1</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>1</cell><cell>full</cell><cell>1-to-1</cell><cell>175</cell></row><row><cell>CNN2</cell><cell>2</cell><cell>4</cell><cell>4</cell><cell>1</cell><cell>full</cell><cell>1-to-1</cell><cell>293</cell></row><row><cell>CNN3</cell><cell>3</cell><cell>6</cell><cell>6</cell><cell>1</cell><cell>full</cell><cell>1-to-1</cell><cell>493</cell></row><row><cell>CNN4</cell><cell>4</cell><cell>8</cell><cell>8</cell><cell>1</cell><cell>full</cell><cell>1-to-1</cell><cell>729</cell></row><row><cell>CNN5</cell><cell>5</cell><cell>10</cell><cell>10</cell><cell>1</cell><cell>full</cell><cell>1-to-1</cell><cell>1001</cell></row><row><cell cols="4">FM: number of feature maps</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Classification performance of individual MLP, SVM, RF and CNN classifier. PPV=positive predictive value; NPV=negative predictive value.</figDesc><table><row><cell>Classification Measures</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Structural information of the convolutional gating network (CGN).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Classification performances of different ensemble methods.</figDesc><table><row><cell>Classification Measures</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Pair-wise p-values obtained via bootstrapping for comparison of 5 methods.</figDesc><table><row><cell>Method</cell><cell>RF</cell><cell cols="4">CNN5 Ave-ensemble Soft-ensemble ME-CNN</cell></row><row><cell>RF</cell><cell>-</cell><cell>0.041</cell><cell>0.011</cell><cell>0.008</cell><cell>0.003</cell></row><row><cell>CNN5</cell><cell>0.041</cell><cell>-</cell><cell>0.092</cell><cell>0.043</cell><cell>0.039</cell></row><row><cell cols="3">Ave-ensemble 0.011 0.092</cell><cell>-</cell><cell>0.188</cell><cell>0.041</cell></row><row><cell cols="3">Soft-ensemble 0.008 0.043</cell><cell>0.188</cell><cell>-</cell><cell>0.064</cell></row><row><cell>ME-CNN</cell><cell cols="2">0.003 0.039</cell><cell>0.041</cell><cell>0.064</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table</head><label></label><figDesc></figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>URL: www.uow.edu.au/ phung/docs/cnn-matlab.zip</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>The authors would like to thank the anonymous reviewers and Dr. Hossein Rabbani for their constructive comments, which helped improving the paper.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Desantis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Fedewa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Convergence of incidence rates between black and white women</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Breast cancer statistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Breast imaging</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Kopans</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Lippincott Williams and Wilkins: Harvard Medical School</publisher>
			<pubPlace>Boston, Massachusetts</pubPlace>
		</imprint>
	</monogr>
	<note>3rd Edition</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quantitative analysis of lesion morphology and texture features for diagnostic prediction in breast MRI</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Hon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Nalcioglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Academic Radiology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1513" to="1525" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Comparison of breast magnetic resonance imaging, mammography, and ultrasound for surveillance of women at high risk for hereditary breast cancer</title>
		<author>
			<persName><forename type="first">E</forename><surname>Warner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Plewes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shumak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Catzavelos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Di Prospero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yaffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ramsay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Clinical Oncology</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="3524" to="3531" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Breast cancer detected on an incident (second or subsequent) round of screening MRI: MRI features of false-negative cases</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schacht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Newstead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Verp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">I</forename><surname>Olopade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Abe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Roentgenology</title>
		<imprint>
			<biblScope unit="volume">201</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1155" to="1163" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automated localization of breast cancer in DCE-MRI</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gubern-Mérida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Martí</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Melendez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Hauth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Karssemeijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Platel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="265" to="274" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A pyramidal neural network for visual pattern recognition</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Phung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bouzerdoum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="329" to="343" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Computer-aided diagnosis of mass-like lesion in breast MRI: Differential analysis of the 3-D morphology between benign and malignant tumors</title>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Methods and Programs in Biomedicine</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="508" to="517" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A CAD system for identification and classification of breast cancer tumors in DCE-MR images based on hierarchical convolutional neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rasti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Teshnehlab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jafari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence in Electrical Engineering</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Classification of breast mass lesions using model-based analysis of the characteristic kinetic curve derived from fuzzy c-means clustering</title>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magnetic Resonance Imaging</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="312" to="322" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">3D lacunarity in multifractal analysis of breast tumor lesions in dynamic contrast-enhanced magnetic resonance imaging</title>
		<author>
			<persName><forename type="first">F</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Janela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Seabra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Freire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4422" to="4435" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Computerized breast lesions detection using kinetic and morphologic analysis for dynamic contrast-enhanced MRI</title>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magnetic Resonance Imaging</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="514" to="522" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">MRI breast cancer diagnosis hybrid approach using adaptive ant-based segmentation and multilayer perceptron neural networks classifier</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Hassanien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Moftah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shoman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="62" to="71" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A novel hybrid CNN-SVM classifier for recognizing handwritten digits</title>
		<author>
			<persName><forename type="first">X.-X</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1318" to="1325" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dropsample: A new training method to enhance deep convolutional neural networks for large-scale unconstrained handwritten Chinese character recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="190" to="203" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Convolutional face finder: A neural architecture for fast and robust face detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Delakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1408" to="1423" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for speech recognition</title>
		<author>
			<persName><forename type="first">O</forename><surname>Abdel-Hamid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-R</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Penn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1533" to="1545" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pulmonary nodule detection in CT images: false positive reduction using multi-view convolutional networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A A</forename><surname>Setio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ciompi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gerke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Van Riel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M W</forename><surname>Wille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Naqibullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1160" to="1169" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving computer-aided detection using convolutional neural networks and random view aggregation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1170" to="1181" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Locality sensitive deep learning for detection and classification of nuclei in routine colon cancer histology images</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sirinukunwattana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E A</forename><surname>Raza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Snead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Cree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Rajpoot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1196" to="1206" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Brain tumor segmentation using convolutional neural networks in MRI images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Alves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1240" to="1251" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fast convolutional neural network training using selective data sampling: Application to hemorrhage detection in color fundus images</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Van Grinsven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Hoyng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Theelen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Sánchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1273" to="1284" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Training a feedback loop for hand pose estimation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3316" to="3324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.07224</idno>
		<title level="m">Deep CNN ensemble with data augmentation for object detection</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Minimalistic CNN-based ensemble model for gender prediction from face images</title>
		<author>
			<persName><forename type="first">G</forename><surname>Antipov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-A</forename><surname>Berrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Dugelay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="59" to="65" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adaptive mixtures of local experts</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Nowlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="87" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mitosis detection in breast cancer histology images with deep neural networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Cires ¸an</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Giusti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="411" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cascaded ensemble of convolutional neural networks and handcrafted features for mitosis detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cruz-Roa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Basavanhally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gilmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tomaszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madabhushi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SPIE Medical Imaging</title>
		<imprint>
			<biblScope unit="page" from="90410B" to="90410" />
			<date type="published" when="2014">2014</date>
			<publisher>International Society for Optics and Photonics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep neural networks for anatomical brain segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>De Brebisson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Montana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="20" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Accurate segmentation of cervical cytoplasm and nuclei based on multiscale convolutional network and graph partitioning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2421" to="2433" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Ensemble of deep convolutional neural networks for learning to detect retinal vessels in fundus images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Santara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sheet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04833</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mitosis detection in breast cancer histology images via deep cascaded networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1160" to="1166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<author>
			<persName><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A threshold selection method from gray-level histograms</title>
		<imprint>
			<date type="published" when="1975">1975</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="23" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Hands-on morphological image processing</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Dougherty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Lotufo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>SPIE press</publisher>
			<biblScope unit="volume">59</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Active contours without edges</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Vese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="266" to="277" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fast and adaptive detection of pulmonary nodules in thoracic CT images using a hierarchical vector quantization scheme</title>
		<author>
			<persName><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Biomedical and Health Informatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="648" to="659" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A direct adaptive method for faster backpropagation learning: The RPROP algorithm</title>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Braun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Networks</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="586" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Matlab library for convolutional neural networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Phung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bouzerdoum</surname></persName>
		</author>
		<ptr target="http://www.uow.edu.au/~phung/download.html" />
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>University of Wollongong</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Comparing image detection algorithms using resampling</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">W</forename><surname>Samuelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Petrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Biomedical Imaging</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1312" to="1315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Advantages and examples of resampling for CAD evaluation</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">W</forename><surname>Samuelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Petrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Paquerault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Biomedical Imaging</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="492" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Breast MRI lesion classification: Improved performance of human readers with a backpropagation neural network computer-aided diagnosis (CAD) system</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Meinel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Stolpen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Berbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Fajardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Reinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Magnetic Resonance Imaging</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="89" to="95" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Textural features for image classification</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Haralick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Dinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man and Cybernetics</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="610" to="621" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Efficient backprop</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">A</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="9" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Classification of ECG arrhythmia by a modular neural network based on mixture of experts and negatively correlated learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Javadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A A A</forename><surname>Arani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sajedin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ebrahimpour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomedical Signal Processing and Control</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
