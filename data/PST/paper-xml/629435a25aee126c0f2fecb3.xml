<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</title>
				<funder ref="#_ksVhqZB">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
				</funder>
				<funder ref="#_DXYYyAZ">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_We9EVW4">
					<orgName type="full">ONR</orgName>
				</funder>
				<funder>
					<orgName type="full">Department of Defense (DoD)</orgName>
				</funder>
				<funder ref="#_ufPPWWS">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder>
					<orgName type="full">Ericsson</orgName>
				</funder>
				<funder>
					<orgName type="full">Analog Devices</orgName>
				</funder>
				<funder>
					<orgName type="full">Stanford Data Science Initiative</orgName>
					<orgName type="abbreviated">SDSI</orgName>
				</funder>
				<funder>
					<orgName type="full">Total</orgName>
				</funder>
				<funder>
					<orgName type="full">National Defense Science and Engineering Graduate Fellowship</orgName>
					<orgName type="abbreviated">NDSEG</orgName>
				</funder>
				<funder>
					<orgName type="full">Salesforce</orgName>
				</funder>
				<funder ref="#_tCdPFkp">
					<orgName type="full">Interactive Human-AI Teaming)</orgName>
				</funder>
				<funder>
					<orgName type="full">Qualcomm</orgName>
				</funder>
				<funder ref="#_uQtRHp9">
					<orgName type="full">Xilinx</orgName>
				</funder>
				<funder>
					<orgName type="full">Accenture</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-05-27">27 May 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
							<email>trid@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
							<email>ermon@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University at Buffalo</orgName>
								<address>
									<region>SUNY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christopher</forename><surname>R?</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-05-27">27 May 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2205.14135v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IOaware-accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3? speedup on GPT-2 (seq. length 1K), and 2.4? speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transformer models <ref type="bibr" target="#b78">[79]</ref> have emerged as the most widely used architecture in applications such as natural language processing and image classification. Transformers have grown larger <ref type="bibr" target="#b4">[5]</ref> and deeper <ref type="bibr" target="#b79">[80]</ref>, but equipping them with longer context remains difficult <ref type="bibr" target="#b76">[77]</ref>, since the self-attention module at their heart has time and memory complexity quadratic in sequence length. An important question is whether making attention faster and more memory-efficient can help Transformer models address their runtime and memory challenges for long sequences.</p><p>Many approximate attention methods have aimed to reduce the compute and memory requirements of attention. These methods range from sparse-approximation <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b70">71]</ref> to low-rank approximation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b80">81]</ref>, and their combinations <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b88">89]</ref>. Although these methods reduce the compute requirements to linear or near-linear in sequence length, many of them do not display wall-clock speedup against standard attention and have not gained wide adoption. One main reason is that they focus on FLOP reduction (which may not correlate with wall-clock speed) and tend to ignore overheads from memory access (IO).</p><p>In this paper, we argue that a missing principle is making attention algorithms IO-aware <ref type="bibr" target="#b0">[1]</ref>-that is, carefully accounting for reads and writes to different levels of fast and slow memory (e.g., between fast GPU on-chip SRAM and relatively slow GPU high bandwidth memory, or HBM <ref type="bibr" target="#b42">[43]</ref>, Figure <ref type="figure">1 left</ref>). On modern Figure <ref type="figure">1</ref>: Left: FlashAttention uses tiling to prevent materialization of the large ? ? ? attention matrix (dotted box) on (relatively) slow GPU HBM. In the outer loop (red arrows), FlashAttention loops through blocks of the K and V matrices and loads them to fast on-chip SRAM. In each block, FlashAttention loops over blocks of Q matrix (blue arrows), loading them to SRAM, and writing the output of the attention computation back to HBM. Right: Speedup over the PyTorch implementation of attention on GPT-2. FlashAttention does not read and write the large ? ? ? attention matrix to HBM, resulting in an 7.6? speedup on the attention computation.</p><p>GPUs, compute speed has out-paced memory speed <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b59">60]</ref>, and most operations in Transformers are bottlenecked by memory accesses <ref type="bibr" target="#b40">[41]</ref>. IO-aware algorithms have been critical for similar memory-bound operations, when reading and writing data can account for a large portion of the runtime-such as database joins <ref type="bibr" target="#b67">[68]</ref>, image processing <ref type="bibr" target="#b66">[67]</ref>, numerical linear algebra <ref type="bibr" target="#b3">[4]</ref>, and more <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b81">82]</ref>. However, common Python interfaces to deep learning such as PyTorch and Tensorflow do not allow fine-grained control of memory access.</p><p>We propose FlashAttention, a new attention algorithm that computes exact attention with far fewer memory accesses. Our main goal is to avoid reading and writing the attention matrix to and from HBM. This requires (i) computing the softmax reduction without access to the whole input (ii) not storing the large intermediate attention matrix for the backward pass. We apply two well-established techniques to address these challenges. (i) We restructure the attention computation to split the input into blocks and make several passes over input blocks, thus incrementally performing the softmax reduction (also known as tiling). (ii) We store the softmax normalization factor from the forward pass to quickly recompute attention on-chip in the backward pass, which is faster than the standard approach of reading the intermediate attention matrix from HBM. We implement FlashAttention in CUDA to achieve fine-grained control over memory access and fuse all the attention operations into one GPU kernel. Even with the increased FLOPs due to recomputation, our algorithm both runs faster (up to 7.6x on GPT-2 <ref type="bibr" target="#b63">[64]</ref>, Figure <ref type="figure">1 right</ref>) and uses less memory-linear in sequence length-than standard attention, thanks to the massively reduced amount of HBM access.</p><p>We analyze the IO complexity <ref type="bibr" target="#b0">[1]</ref> of FlashAttention, proving that it requires ? (? 2 ? 2 ? -1 ) HBM accesses where ? is the head dimension and ? is the size of SRAM, as compared to ?(? ? + ? 2 ) of standard attention. For typical values of ? and ?, FlashAttention requires many times fewer HBM accesses compared to standard attention (up to 9? fewer, as shown in Fig. <ref type="figure">2</ref>). Moreover, we provide a lower bound, showing that no exact attention algorithm can asymptotically improve on the number of HBM accesses over all SRAM sizes.</p><p>We also show that FlashAttention can serve as a useful primitive for realizing the potential of approximate attention algorithms by overcoming their issues with memory access overhead. As a proof of concept, we implement block-sparse FlashAttention, a sparse attention algorithm that is 2-4? faster than even FlashAttention, scaling up to sequence length of 64k. We prove that block-sparse FlashAttention has better IO complexity than FlashAttention by a factor proportional to the sparsity ratio. We discuss further extensions to other operations (attention on multi-GPU, kernel regression, block-sparse matrix multiply) in Section 5. We open-source FlashAttention to make it easier to build on this primitive. <ref type="foot" target="#foot_0">1</ref>We empirically validate that FlashAttention speeds up model training and improves model quality by modeling longer context. We also benchmark the runtime and memory footprint of FlashAttention and block-sparse FlashAttention compared to prior attention implementations.</p><p>? Faster Model Training. FlashAttention trains Transformer models faster in wall-clock time. We train BERT-large (seq. length 512) 15% faster than the training speed record in MLPerf 1.1 <ref type="bibr" target="#b55">[56]</ref>, GPT2 (seq. length 1K) 3? faster than baseline implementations from HuggingFace <ref type="bibr" target="#b83">[84]</ref> and Megatron-LM <ref type="bibr" target="#b73">[74]</ref>, and long-range arena (seq. length 1K-4K) 2.4? faster than baselines.</p><p>? Higher Quality Models. FlashAttention scales Transformers to longer sequences, which improves their quality and enables new capabilities. We observe a 0.7 improvement in perplexity on GPT-2 and 6.4 points of lift from modeling longer sequences on long-document classification <ref type="bibr" target="#b11">[12]</ref>. FlashAttention enables the first Transformer that can achieve better-than-chance performance on the Path-X <ref type="bibr" target="#b76">[77]</ref> challenge, solely from using a longer sequence length (16K). Block-sparse FlashAttention enables a Transformer to scale to even longer sequences (64K), resulting in the first model that can achieve better-than-chance performance on Path-256.</p><p>? Benchmarking Attention. In benchmarks, FlashAttention is up to 3? faster than the standard attention implementation across common sequence lengths from 128 to 2K and scales up to 64K. Up to sequence length of 512, FlashAttention is both faster and more memory-efficient than any existing attention method, whereas for sequence length beyond 1K, some approximate attention methods (e.g., Linformer) start to become faster. On the other hand, block-sparse FlashAttention is faster than all existing approximate attention methods that we know of.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>We provide some background on the performance characteristics of common deep learning operations on modern hardware (GPUs). We also describe the standard implementation of attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Hardware Performance</head><p>We focus here on GPUs. Performance on other hardware accelerators are similar <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b45">46]</ref>. GPU Memory Hierarchy. The GPU memory hierarchy (Fig. <ref type="figure">1</ref> left) comprises multiple forms of memory of different sizes and speeds, with smaller memory being faster. As an example, the A100 GPU has 40-80GB of high bandwidth memory (HBM) with bandwidth 1.5-2.0TB/s and 192KB of on-chip SRAM per each of 108 streaming multiprocessors with bandwidth estimated around 19TB/s <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref>. The on-chip SRAM is an order of magnitude faster than HBM but many orders of magnitude smaller in size. As compute has gotten faster relative to memory speed <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b59">60]</ref>, operations are increasingly bottlenecked by memory (HBM) accesses. Thus exploiting fast SRAM becomes more important.</p><p>Execution Model. GPUs have a massive number of threads to execute an operation (called a kernel). Each kernel loads inputs from HBM to registers and SRAM, computes, then writes outputs to HBM.</p><p>Performance characteristics. Depending on the balance of computation and memory accesses, operations can be classified as either compute-bound or memory-bound. This is commonly measured by the arithmetic intensity <ref type="bibr" target="#b81">[82]</ref>, which is the number of arithmetic operations per byte of memory access. 1. Compute-bound: the time taken by the operation is determined by how many arithmetic operations there are, while time accessing HBM is much smaller. Typical examples are matrix multiply with large inner dimension, and convolution with large number of channels.</p><p>2. Memory-bound: the time taken by the operation is determined by the number of memory accesses, while time spent in computation is much smaller. Examples include most other operations: elementwise (e.g., activation, dropout), and reduction (e.g., sum, softmax, batch norm, layer norm). Kernel fusion. The most common approach to accelerate memory-bound operations is kernel fusion: if there are multiple operations applied to the same input, the input can be loaded once from HBM, instead of multiple times for each operation. Compilers can automatically fuse many elementwise operations <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b71">72]</ref>.</p><p>However, in the context of model training, the intermediate values still need to be written to HBM to save for the backward pass, reducing the effectiveness of naive kernel fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Standard Attention Implementation</head><p>Given input sequences Q, K, V ? R ? ?? where ? is the sequence length and ? is the head dimension, we want to compute the attention output O ? R ? ?? :</p><formula xml:id="formula_0">S = QK ? R ? ?? , P = softmax(S) ? R ? ?? , O = PV ? R ? ?? ,</formula><p>where softmax is applied row-wise.</p><p>Standard attention implementations materialize the matrices S and P to HBM, which takes ? (? 2 ) memory. Often ? ? (e.g., for GPT2, ? = 1024 and ? = 64). We describe the standard attention implementation in Algorithm 0. As some or most of the operations are memory-bound (e.g., softmax), the large number of memory accesses translates to slow wall-clock time.</p><p>This problem is exacerbated by other elementwise operations applied to the attention matrix, such as masking applied to S or dropout applied to P. As a result, there have been many attempts to fuse several elementwise operations, such as fusing masking with softmax <ref type="bibr" target="#b73">[74]</ref>.</p><p>In Section 3.2, we will show that the standard attention implementation performs HBM accesses quadratic in the sequence length ?. We also compare the number of FLOPs and number of HBM accesses of standard attention and of our method (FlashAttention).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 0 Standard Attention Implementation</head><p>Require: Matrices Q, K, V ? R ? ?? in HBM.</p><p>1: Load Q, K by blocks from HBM, compute S = QK , write S to HBM.</p><p>2: Read S from HBM, compute P = softmax(S), write P to HBM. 3 FlashAttention: Algorithm, Analysis, and Extensions</p><p>We show how to compute exact attention with fewer HBM reads/writes and without storing large intermediate matrices for the backward pass. This yields an attention algorithm that is both memory efficient and faster in wall-clock time. We analyze its IO complexity, showing that our method requires much fewer HBM accesses compared to standard attention. We further show that FlashAttention can serve as a useful primitive by extending it to handle block-sparse attention.</p><p>We focus here on the forward pass for ease of exposition; Appendix B contains details for the backward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">An Efficient Attention Algorithm With Tiling and Recomputation</head><p>Given the inputs Q, K, V ? R ? ?? in HBM, we aim to compute the attention output O ? R ? ?? and write it to HBM. Our goal is to reduce the amount of HBM accesses (to sub-quadratic in ?).</p><p>We apply two established techniques (tiling, recomputation) to overcome the technical challenge of computing exact attention in sub-quadratic HBM accesses. We describe this in Algorithm 1. The main idea is that we split the inputs Q, K, V into blocks, load them from slow HBM to fast SRAM, then compute the attention output with respect to those blocks. By scaling the output of each block by the right normalization factor before adding them up, we get the correct result at the end.</p><p>Tiling. We compute attention by blocks. Softmax couples columns of K, so we decompose the large softmax with scaling <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b62">63]</ref>. For numerical stability, the softmax of vector ? ? R ? is computed as:</p><formula xml:id="formula_1">?(?) := max ? ? ? , ? (?) := ? ? 1 -?( ?) . . . ? ? ? -?( ?) , ?(?) := ?? ? ? (?) ? , softmax(?) := ? (?) ?(?)</formula><p>.</p><p>For vectors ? (1) , ? (2) ? R ? , we can decompose the softmax of the concatenated ? = ? (1) ? (2) ? R 2? as: ?(?) = ?( ? (1) ? (2) ) = max(?(? (1) ), ?(? (2) )), ? (?) = ? ?( ? (1) )-?( ?) ? (? (1) ) ? ?( ? (2) )-?( ?) ? (? (2) ) , ?(?) = ?( ? (1) ? (2) ) = ? ?( ? (1) )-?( ?) ?(? (1) ) + ? ?( ? (2) )-?( ?) ?(? (2) ), softmax(?) =</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? (?) ?(?)</head><p>.</p><p>Therefore if we keep track of some extra statistics (?(?), ?(?)), we can compute softmax one block at a time. <ref type="foot" target="#foot_1">2</ref>We thus split the inputs Q, K, V into blocks (Algorithm 1 line 3), compute the softmax values along with extra statistics (Algorithm 1 line 10), and combine the results (Algorithm 1 line 12).</p><p>Recomputation. One of our goals is to not store ? (? 2 ) intermediate values for the backward pass. The backward pass typically requires the matrices S, P ? R ? ?? to compute the gradients with respect to Q, K, V. However, by storing the output O and the softmax normalization factor ?, we can recompute the attention matrix S and P easily in the backward pass from blocks of Q, K, V in SRAM. This can be seen as a form of selective gradient checkpointing <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b31">32]</ref>. While gradient checkpointing has been suggested to reduce the maximum amount of memory required <ref type="bibr" target="#b62">[63]</ref>, all implementations (that we know off) have to trade speed for memory. In contrast, even with more FLOPs, our recomputation speeds up the backward pass due to reduced HBM accesses (Fig. <ref type="figure">2</ref>). The full backward pass description is in Appendix B.</p><p>Implementation details: Kernel fusion. Tiling enables us to implement our algorithm in one CUDA kernel, loading input from HBM, performing all the computation steps (matrix multiply, softmax, optionally masking and dropout, matrix multiply), then write the result back to HBM (masking and dropout in Appendix B). This avoids repeatedly reading and writing of inputs and outputs from and to HBM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 FlashAttention</head><p>Require: Load K ? , V ? from HBM to on-chip SRAM. Load Q ? , O ? , ? ? , ? ? from HBM to on-chip SRAM.</p><formula xml:id="formula_2">Matrices Q, K, V ? R ? ?? in HBM, on-chip SRAM of size ?. 1: Set block sizes ? ? = ? 4? , ? ? = min ? 4? , ? . 2: Initialize O = (0) ? ?? ? R ? ?? , ? = (0) ? ? R ? , ? = (-?) ? ? R ? in HBM. 3: Divide Q into ? ? = ? ? ? blocks Q 1 , . . . ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>On chip, compute</p><formula xml:id="formula_3">S ? ? = Q ? K ? ? ? R ? ? ?? ? . 10: On chip, compute m? ? = rowmax(S ? ? ) ? R ? ? , P? ? = exp(S ? ? -m? ? ) ? R ? ? ?? ? (pointwise), l? ? = rowsum( P? ? ) ? R ? ? . 11: On chip, compute ? new ? = max(? ? , m? ? ) ? R ? ? , ? new ? = ? ? ? -? new ? ? ? + ? m? ? -? new ? l? ? ? R ? ? .</formula><p>12:</p><formula xml:id="formula_4">Write O ? ? diag(? new ? ) -1 (diag(? ? )? ? ? -? new ? O ? + ? m? ? -? new ? P? ? V ? ) to HBM. 13: Write ? ? ? ? new ? , ? ? ? ? new</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>to HBM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>14:</head><p>end for 15: end for 16: Return O.</p><p>We show FlashAttention's correctness, runtime, and memory requirement (proof in Appendix C).</p><p>Theorem 1. Algorithm 1 returns O = softmax(QK )V with ? (? 2 ?) FLOPs and requires ? (?) additional memory beyond inputs and output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Analysis: IO Complexity of FlashAttention</head><p>We analyze the IO complexity of FlashAttention, showing significant reduction in HBM accesses compared to standard attention. We also provide a lower bound, proving that no exact attention algorithm can For typical values of ? (64-128) and ? (around 100KB), ? 2 is many times smaller than ?, and thus FlashAttention requires many times fewer HBM accesses than standard implementation. This leads to both faster execution and lower memory footprint, which we validate in Section 4.3.</p><p>The main idea of the proof is that given the SRAM size of ?, we can load blocks of K, V of size ?(?) each (Algorithm 1 line 6). For each block of K and V, we iterate over all blocks of Q (Algorithm 1 line 8) to compute the intermediate values, resulting in ?(? ?? -1 ) passes over Q. Each pass loads ?(? ?) elements, which amounts to ?(? 2 ? 2 ? -1 ) HBM accesses. We similarly prove that the backward pass of standard attention requires ?(? ? + ? 2 ) HBM accesses while the backward pass of FlashAttention requires ?(? 2 ? 2 ? -1 ) HBM accesses (Appendix B).</p><p>We prove a lower-bound: one cannot asymptotically improve on the number of HBM accesses for all values of ? (the SRAM size) when computing exact attention. Proposition 3. Let ? be the sequence length, ? be the head dimension, and ? be size of fast on-chip memory. There does not exist an algorithm to compute exact attention with ?(? 2 ? 2 ? -1 ) HBM accesses for all ? in the range [?, ? ?].</p><p>The proof relies on the fact that for ? = ?(? ?) any algorithm must perform ?(? 2 ? 2 ? -1 ) = ?(? ?) HBM accesses. This type of lower bound over a subrange of ? is common in the streaming algorithms literature <ref type="bibr" target="#b84">[85]</ref>. We leave proving parameterized complexity <ref type="bibr" target="#b24">[25]</ref> lower bounds in terms of ? as exciting future work.</p><p>We validate that the number of HBM accesses is the main determining factor of attention run-time. In Fig. <ref type="figure">2</ref> (left), we see that even though FlashAttention has higher FLOP count compared to standard attention (due to recomputation in the backward pass), it has much fewer HBM accesses, resulting in much faster runtime. In Fig. <ref type="figure">2</ref> (middle), we vary the block size ? ? of FlashAttention, which results in different amounts of HBM accesses, and measure the runtime of the forward pass. As block size increases, the number of HBM accesses decreases (as we make fewer passes over the input), and runtime decreases. For large enough block size (beyond 256), the runtime is then bottlenecked by other factors (e.g., arithmetic operations). Moreover, larger block size will not fit into the small SRAM size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Extension: Block-Sparse FlashAttention</head><p>We extend FlashAttention to approximate attention: we propose block-sparse FlashAttention, whose IO complexity is smaller than FlashAttention by a factor proportional to the sparsity.</p><p>Given inputs Q, K, V ? R ? ?? and a mask matrix M ? {0, 1} ? ?? , we want to compute:</p><formula xml:id="formula_5">S = QK ? R ? ?? , P = softmax(S ? M) ? R ? ?? , O = PV ? R ? ?? ,</formula><p>where</p><formula xml:id="formula_6">(S ? M) ?? = S ?? if M?? = 1 and -? if M ?? = 0.</formula><p>We require M to have block form: for some block sizes ? ? , ? ? , for all ?, ?, M?,? = M ? ? with ? = ?/? ? , ? = ?/? ? for some M ? {0, 1} ? /? ? ?? /? ? .</p><p>Given a predefined block sparsity mask M ? {0, 1} ? /? ? ?? /? ? we can easily adapt Algorithm 1 to only compute the nonzero blocks of the attention matrix. The algorithm is identical to Algorithm 1, except we skip zero blocks. We reproduce the algorithm description in Algorithm 5 in Appendix B.</p><p>We also analyze the IO complexity of block-sparse FlashAttention.</p><p>Proposition 4. Let ? be the sequence length, ? be the head dimension, and ? be size of SRAM with ? ? ? ? ? ?. Block-sparse FlashAttention (Algorithm 5) requires ?(? ? + ? 2 ? 2 ? -1 ?) HBM accesses where ? is the fraction of nonzero blocks in the block-sparsity mask.</p><p>We see that applying block-sparsity yields a direct improvement by the sparsity to the larger term in the IO complexity. For large sequence lengths ?, ? is often set to ? -1/2 <ref type="bibr" target="#b9">[10]</ref> or ? -1 log ? <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b88">89]</ref>, resulting in ?(? ? ?) or ?(? log ?) IO complexity. For downstream experiments, we use the fixed butterfly sparsity pattern <ref type="bibr" target="#b15">[16]</ref>, which has been shown to be able to approximate arbitrary sparsity <ref type="bibr" target="#b14">[15]</ref>.</p><p>In Fig. <ref type="figure">2</ref> (right), we validate that as the sparsity increases, the runtime of block-sparse FlashAttention improves proportionally. On the LRA benchmark, block-sparse FlashAttention achieves 2.8? speedup, while performing on par with standard attention (Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate the impact of using FlashAttention to train Transformer models. We validate two claims about training time and model accuracy, and report attention runtime and memory benchmarks.</p><p>? Training Speed. FlashAttention outperforms the MLPerf 1.1 <ref type="bibr" target="#b55">[56]</ref> speed record for BERT by 15%, and speeds up GPT-2 up to 3? over HuggingFace <ref type="bibr" target="#b83">[84]</ref> and 1.8? over Megatron <ref type="bibr" target="#b73">[74]</ref> over standard Transformers.</p><p>FlashAttention speeds up the long-range arena (LRA) benchmark 2.4?. ? Quality. FlashAttention scales Transformers to longer sequences, yielding higher quality. FlashAttention trains GPT-2 with context length 4K faster than Megatron trains GPT-2 with context length 1K, while achieving 0.7 better perplexity. Modeling longer sequences yields 6.4 points of lift on two longdocument classification tasks. Finally, FlashAttention yields the first Transformer that can achieve better-than-random performance on the challenging Path-X task (sequence length 16K), and block-sparse FlashAttention yields the first sequence model that we know of that can achieve better-than-random performance on Path-256 (sequence length 64K).</p><p>? Benchmarking Attention. We measure the runtime and memory performance of FlashAttention and block-sparse FlashAttention based on sequence length. We confirm that the memory footprint of FlashAttention scales linearly with seq. length and is up to 3? faster than standard attention for common seq. lengths (up to 2K). We confirm that runtime of block-sparse FlashAttention scales linearly in seq. length and is faster than all existing approximate attention baselines. Additional experiment details are in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Faster Models with FlashAttention</head><p>BERT. FlashAttention yields the fastest single-node BERT training speed that we know of. We train a BERT-large <ref type="bibr" target="#b20">[21]</ref> model with FlashAttention on Wikipedia. Table <ref type="table">1</ref> compares our training time to the implementation from Nvidia that set the training speed record for MLPerf 1.1 <ref type="bibr" target="#b55">[56]</ref>. Our implementation is 15% faster.</p><p>Table <ref type="table">1</ref>: Training time of BERT-large, starting from the same initialization provided by the MLPerf benchmark, to reach the target accuracy of 72.0% on masked language modeling. Averaged over 10 runs on 8?A100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BERT Implementation</head><p>Training time (minutes) Nvidia MLPerf 1.1 <ref type="bibr" target="#b55">[56]</ref> 20.0 ? 1.5 FlashAttention (ours)</p><p>17.4 ? 1.4</p><p>GPT-2. FlashAttention yields faster training times for GPT-2 <ref type="bibr" target="#b63">[64]</ref> on the large OpenWebtext dataset <ref type="bibr" target="#b29">[30]</ref> than the widely used HuggingFace <ref type="bibr" target="#b83">[84]</ref> and Megatron-LM <ref type="bibr" target="#b73">[74]</ref> implementations. Table <ref type="table" target="#tab_2">2</ref> shows up to 3? endto-end speedup compared to Huggingface and 1.7? speedup compared to Megatron-LM. FlashAttention achieves the same perplexity as the other two implementations, as we do not change the model definition.</p><p>Appendix E includes plots of the validation perplexity throughout training, confirming that FlashAttention is as numerically stable as the baselines and produces the same training / validation curves. Long-range Arena. We compare vanilla Transformer (with either standard implementation or FlashAttention) on the long-range arena (LRA <ref type="bibr" target="#b76">[77]</ref>) benchmark. We measure accuracy, throughput, and training time of all models. Each task has a different sequence length varying between 1024 and 4096. We follow the implementation and experimental setting in Tay et al. <ref type="bibr" target="#b76">[77]</ref>and Xiong et al. <ref type="bibr" target="#b86">[87]</ref>. <ref type="foot" target="#foot_2">3</ref> Table <ref type="table" target="#tab_3">3</ref> shows that FlashAttention achieves up 2.4? speed-up compared to standard attention. Block-sparse FlashAttention is faster than all of the approximate attention methods that we have tested. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Better Models with Longer Sequences</head><p>Language Modeling with Long Context. The runtime and memory-efficiency of FlashAttention allow us to increase the context length of GPT-2 by 4? while still running faster than the optimized implementation from Megatron-LM. Table <ref type="table" target="#tab_4">4</ref> shows that that GPT-2 with FlashAttention and context length 4K is still 30% faster than GPT-2 from Megatron with context length 1K, while achieving 0.7 better perplexity.  <ref type="bibr" target="#b53">[54]</ref> (we repeat the positional embeddings, as in Beltagy et al. <ref type="bibr" target="#b2">[3]</ref>).</p><p>Table <ref type="table" target="#tab_5">5</ref> shows that sequence length 16K outperforms length 512 by 4.3 points on MIMIC, and that length 8K outperforms length 512 by 8.5 points on ECtHR. The discrepancies may be due to subtle distribution shifts: MIMIC-III contains specialized medical text and thus may be more susceptible to a distribution shift in the document length, whereas ECtHR contains general language. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Path-X Path-256 Transformer Linformer <ref type="bibr" target="#b80">[81]</ref> Linear Attention <ref type="bibr" target="#b47">[48]</ref> Performer <ref type="bibr" target="#b10">[11]</ref> Local Attention <ref type="bibr" target="#b76">[77]</ref> Reformer <ref type="bibr" target="#b48">[49]</ref> SMYRF <ref type="bibr">[</ref> Path-X and Path-256. The Path-X and Path-256 benchmarks are challenging tasks from the long-range arena benchmark designed to test long context. The task is to classify whether two points in a black and white 128?128 (or 256?256) image have a path connecting them, and the images are fed to the transformer one pixel at a time. In prior work, all transformer models have either run out of memory, or only achieved random performance <ref type="bibr" target="#b76">[77]</ref>. There has been a search for alternative architectures that can model such long context <ref type="bibr" target="#b34">[35]</ref>. We present here the first result of Transformer models being able to solve Path-X and Path-256 (Table <ref type="table" target="#tab_6">6</ref>). We pretrain a transformer on Path-64, and then transfer to Path-X by spatially interpolating the positional embeddings. FlashAttention achieves 61.4 accuracy on Path-X. Additionally, block-sparse FlashAttention enables the Transformers to scale to sequence length 64K, achieving 63.1 accuracy<ref type="foot" target="#foot_3">4</ref> on Path-256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Benchmarking Attention</head><p>We vary sequence length and measure runtime and memory usage of FlashAttention and block-sparse FlashAttention against various attention baselines on one A100 GPU with 40 GB HBM, with dropout and a padding mask. We compare against reference implementations for exact attention, approximate attention, and sparse attention. We report a subset of baselines in the main body; Appendix E contains more baselines and full details.</p><p>Runtime. Figure <ref type="figure">3</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Limitations and Future Directions</head><p>We discuss limitations of our approach and future directions. Related work is given in Appendix A.</p><p>Compiling to CUDA. Our current approach to building IO-aware implementations of attention requires writing a new CUDA kernel for each new attention implementation. This requires writing the attention algorithm in a considerably lower-level language than PyTorch, and requires significant engineering effort. Implementations may also not be transferrable across GPU architectures. These limitations suggest the need for a method that supports writing attention algorithms in a high-level language (e.g., PyTorch), and compiling to IO-aware implementations in CUDA-similar to efforts such as Halide in image processing <ref type="bibr" target="#b66">[67]</ref>.</p><p>IO-Aware Deep Learning. We believe that the IO-aware approach can extend beyond attention. Attention is the most memory-intensive computation in Transformers, but every layer in a deep network touches GPU HBM. We hope our work inspires IO-aware implementations of additional modules. We discuss these potential extensions in Appendix D.</p><p>Multi-GPU IO-Aware Methods. Our IO-aware implementation of attention is optimal within constants for computing attention on a single GPU. However, the attention computation may be parallelizable across multiple GPUs <ref type="bibr" target="#b68">[69]</ref>. Using multiple GPUs adds an additional layer to IO analysis-accounting for data transfer between GPUs. We hope our work inspires future work in this direction.</p><p>expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of NIH, ONR, or the U.S. Government. Atri Rudra's and Jessica Grogan's research is supported by NSF grant CCF-1763481.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Related Work</head><p>IO-Aware Runtime Optimization. The broad concept of optimizing for reading and writing to fast/slow memory has a long history in computer science and has been known by many names. We draw the most direct connection to the literature of analyzing I/O complexity in this work <ref type="bibr" target="#b0">[1]</ref>, but concepts of memory hierarchies are fundamental and has appeared in many forms, from the working set model <ref type="bibr" target="#b19">[20]</ref>, to data locality <ref type="bibr" target="#b82">[83]</ref>, to the Roofline model of arithmetic intensity <ref type="bibr" target="#b81">[82]</ref>, to analyses of scalability <ref type="bibr" target="#b56">[57]</ref>, to standard textbook treatments of computer architecture <ref type="bibr" target="#b37">[38]</ref>. We hope that this work encourages the community to adopt these ideas in more parts of the deep learning stack.</p><p>Efficient ML Models with Structured Matrices. Matrix multiply is the core computational bottleneck of most machine learning models. To reduce the computational complexity, there have been numerous approaches to learn over a more efficient set of matrices. These matrices are called structured matrices, which have subquadratic (?(? 2 ) for dimension ? ? ?) number of parameters and runtime. Most common examples of structured matrices are sparse and low-rank matrices, along with fast transforms commonly encountered in signal processing (Fourier, Chebyshev, sine/cosine, orthogonal polynomials). There have been several more general classes of structured matrices proposed in machine learning: Toeplitz-like <ref type="bibr" target="#b74">[75]</ref>, low-displacement rank <ref type="bibr" target="#b46">[47]</ref>, quasi-separable <ref type="bibr" target="#b23">[24]</ref>). The butterfly pattern we use for our block-sparse attention is motivated by the fact that butterfly matrices <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b60">61]</ref> and their products have been shown to be able to express any structured matrices with almost optimal runtime and number of parameters <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19]</ref>. However, even though structured matrices are efficient in theory, they have not seen wide adoption since it is hard to translate their efficiency to wall-clock speedup since dense unconstrained matrix multiply has very optimize implementation, a phenomenon known as the hardware lottery <ref type="bibr" target="#b38">[39]</ref>. Extensions of butterfly matrices <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> aimed to make butterfly matrices more hardware-friendly.</p><p>Sparse Training. Our block-sparse FlashAttention can be seen as a step towards making sparse model training more efficient. Sparse models have seen success in compressing models for inference (pruning) by sparsifying the weight matrices <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b72">73]</ref>. For model training, the lottery tickets <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref> suggests that there are a set of small sub-networks derived from a larger dense network that performs as well as the original dense network. Out block-sparse FlashAttention can also be seen as a fixed lottery ticket in the context of attention: we fix the sparsity pattern to be the butterfly pattern through training, and observe that it performs almost as well as the (dense) FlashAttention on the Long-range Arena tasks.</p><p>Efficient Transformer. Transformer-based models are becoming the most widely-used architecture in natural language processing <ref type="bibr" target="#b20">[21]</ref> and computation vision <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b87">88]</ref>. However, one of their computational bottlenecks is that their time and memory scales quadratic in the sequence length. There are numerous approaches to overcome this bottleneck, including approximation such as with hashing (i.e., sparse) such as Reformer <ref type="bibr" target="#b48">[49]</ref> and Smyrf <ref type="bibr" target="#b17">[18]</ref> and with low-rank approximation such as Performer <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b51">52]</ref>. One can even combine sparse and low-rank approximation for better accuracy (e.g., Longformer <ref type="bibr" target="#b2">[3]</ref>, BigBird <ref type="bibr" target="#b88">[89]</ref>, Scatterbrain <ref type="bibr" target="#b7">[8]</ref>, Long-short transformer <ref type="bibr" target="#b90">[91]</ref>, Combiner <ref type="bibr" target="#b69">[70]</ref>). Other approaches include compressing along the sequence dimension to attend to multiple tokens at once <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b85">86]</ref>. One can also attend over the states from previous sequences to help lengthen the context (e.g., Transformer-XL <ref type="bibr" target="#b12">[13]</ref> and Compressive Transformer <ref type="bibr" target="#b65">[66]</ref>). We recommend the survey <ref type="bibr" target="#b77">[78]</ref> for more details.</p><p>There are several lines of work on developing other modules instead of attention to model longer context. HiPPO <ref type="bibr" target="#b32">[33]</ref> and its extensions, most notably S4 <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref> projects the history on a polynomial basis, allowing accurate reconstruction of the history through state-space models. They combine the strengths of CNNs (efficient training), RNNs (efficient inference), and continuous models (robust to change in sampling rates). LambdaNetworks <ref type="bibr" target="#b1">[2]</ref>, AFT <ref type="bibr" target="#b89">[90]</ref> and FLASH <ref type="bibr" target="#b39">[40]</ref> are other attempts at replacing attention in the context of image classification and language modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Algorithm Details</head><p>We first derive the forward and backward passes of attention and show that they can be computed in a memory-efficient manner (requiring extra memory linear instead of quadratic in the sequence length). Though they reduce the amount of extra memory required, naively they still incur quadratic HBM accesses, resulting in slower execution speed. We describe the FlashAttention algorithm to implement both the forward and the backward passes on GPUs that reduces HBM accesses, leading to both faster runtime and smaller memory footprint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Memory-efficient forward pass</head><p>The main challenge in making attention memory-efficient is the softmax that couples the columns of K (and columns of V). Our approach is to compute the softmax normalization constant separately to decouple the columns. This technique has been used in the literature <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b62">63]</ref> to show that attention computation does not need quadratic extra memory (though the number of HBM accesses is still quadratic, resulting in slow run-time).</p><p>For simplicity, we omit here the max-shifting step during softmax. The full algorithm in Appendix B.3 contains all the steps.</p><p>Recall that given input sequences Q, K, V ? R ? ?? , we want to compute the attention output O ? R ? ?? :</p><formula xml:id="formula_7">S = QK ? R ? ?? , P = softmax(S) ? R ? ?? , O = PV ? R ? ?? .</formula><p>We have that ? ? ? = ? ? ? ? ? where ? ? and ? ? are the ?-th and ?-th columns of Q and K respectively. Define the normalization constants of softmax:</p><formula xml:id="formula_8">? ? = ?? ? ? ? ? ? ? ? .<label>(1)</label></formula><p>Let ? ? be the ?-th column of V, then the ?-th columns of the output is</p><formula xml:id="formula_9">? ? = ? ?: V = ?? ? ? ? ? ? ? = ?? ? ? ? ? ? ? ? ? ? ? ? .<label>(2)</label></formula><p>We see that once ? ? is computed, we can compute ? ? without extra memory by repeatedly summing</p><formula xml:id="formula_10">? ? ? ? ? ?</formula><p>? ? ? ? . Therefore the forward pass can be computed with ? (?) extra memory: 1. Compute ? ? for all ? according to Eq. ( <ref type="formula" target="#formula_8">1</ref>), which takes ? (?) extra memory.</p><p>2. Compute ? ? for all ? according to Eq. ( <ref type="formula" target="#formula_9">2</ref>), which takes ? (?) extra memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Memory-efficient backward pass</head><p>We derive the backward pass of attention and show that it can also be computed with linear memory. Rabe and Staats <ref type="bibr" target="#b62">[63]</ref> suggests that the backward pass can be done without quadratic extra memory by applying gradient checkpointing to the memory-efficient forward pass. We instead derive the backward pass explicitly and show how it can be computed in a memory-efficient manner. Suppose that there is a scalar loss function ?, and let the output gradient be dO ? R ??? (where dO denotes ?? ?O ). We want to compute the input gradients dQ, dK, dV ? R ??? (where dQ, dK, dV denote ?? ?Q , ?? ?K , ?? ?V respectively). The gradient dV is easy to see. Applying reverse-mode autodiff by hand (aka the chain rule), we obtain (in matrix notation) dV = P ? dO. Thus:</p><formula xml:id="formula_11">?? ? = ?? ? ? ? ? ?? ? = ?? ? ? ? ? ? ? ? ? ? ?? ? .<label>(3)</label></formula><p>Since we already computed ? ? , ?? ? can be computed without extra memory by repeated summing. The gradients dQ and dK are a little more complicated. We go through the gradients dP and dS first. From Eq. (2), we have that dP = dOV ? , and so:</p><formula xml:id="formula_12">?? ? ? = ?? ? ? ? ? .</formula><p>Recall that ? ?: = softmax(? ?: ). Using the fact that the Jacobian of ? = softmax(?) is diag(?) -?? ? , we have that </p><formula xml:id="formula_13">?? ?: = (diag(</formula><p>then ?? ?: = ? ?: ? ?? ?: -? ? ? ?: . Hence ?? ? ? = ? ? ? ?? ? ? -? ? ? ? ? = ? ? ? (?? ? ? -? ? ). Now we can get the gradients dQ and dK. Recall that ? ? ? = ? ? ? ? ? , so</p><formula xml:id="formula_15">?? ? = ?? ? ?? ? ? ? ? = ?? ? ? ? ? (?? ? ? -? ? )? ? = ?? ? ? ? ? ? ? ? ? ? (?? ? ? ? ? -? ? )? ? .<label>(5)</label></formula><p>Similarly,</p><formula xml:id="formula_16">?? ? = ?? ? ?? ? ? ? ? = ?? ? ? ? ? (?? ? ? -? ? )? ? = ?? ? ? ? ? ? ? ? ? ? (?? ? ? ? ? -? ? )? ? . (<label>6</label></formula><formula xml:id="formula_17">)</formula><p>Therefore the backward pass can also be computed with ? (?) extra memory:</p><p>1. Compute ?? ? for all ? according to Eq. ( <ref type="formula" target="#formula_11">3</ref>), which takes ? (?) extra memory.</p><p>2. Compute ? ? for all ? according to Eq. ( <ref type="formula" target="#formula_14">4</ref>), which takes ? (?) extra memory.</p><p>3. Compute ?? ? for all ? according to Eq. ( <ref type="formula" target="#formula_15">5</ref>), which takes ? (?) extra memory.</p><p>4. Compute ?? ? for all ? according to Eq. ( <ref type="formula" target="#formula_16">6</ref>), which takes ? (?) extra memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 FlashAttention: Forward Pass</head><p>We describe the full details of FlashAttention forward pass. Given input sequences Q, K, V ? R ? ?? , we want to compute the attention output O ? R ? ?? :</p><formula xml:id="formula_18">S = ?QK ? R ? ?? , S masked = mask(?) ? R ? ?? , P = softmax(S masked ) ? R ? ?? , P dropped = dropout(P, ? drop ), O = P dropped V ? R ? ?? ,</formula><p>where ? ? R is some softmax scaling (typically</p><formula xml:id="formula_19">1 ? ?</formula><p>), mask is some masking function that sets some entries of the input to -? and keep other entries the same (e.g., key padding mask when sequences in the batch don't have the same lengths and are padded), and drop(?, ?) applies dropout to ? (i.e., output ? 1-? with probability 1 -? and output 0 with probability ?).</p><p>The full algorithm is in Algorithm 2. We save the output O, the softmax statistics ? and ?, and the pseudo-random number generator state R for the backward pass.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 FlashAttention Forward Pass</head><p>Require: Matrices Q, K, V ? R ? ?? in HBM, on-chip SRAM of size ?, softmax scaling constant ? ? R, masking function mask, dropout probability ? drop . 1: Initialize the pseudo-random number generator state R and save to HBM. 2: Set block sizes ? ? = ? 4? , ? ? = min ? 4? , ? . Load K ? , V ? from HBM to on-chip SRAM.</p><formula xml:id="formula_20">3: Initialize O = (0) ? ?? ? R ? ?? , ? = (0) ? ? R ? , ? = (-?) ? ? R ? in HBM. 4: Divide Q into ? ? = ? ? ? blocks Q 1 , . . . ,</formula><p>8:</p><formula xml:id="formula_21">for 1 ? ? ? ? ? do 9:</formula><p>Load Q ? , O ? , ? ? , ? ? from HBM to on-chip SRAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10:</head><p>On chip, compute S ? ? = ?Q ? K ? ? ? R ? ? ?? ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><p>On chip, compute S masked</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? ?</head><p>= mask(S ? ? ).</p><p>12:</p><p>On chip, compute m? ? = rowmax(S masked ? ?</p><formula xml:id="formula_22">) ? R ? ? , P? ? = exp(S masked ? ? -m? ? ) ? R ? ? ?? ? (pointwise), l? ? = rowsum( P? ? ) ? R ? ? . 13: On chip, compute ? new ? = max(? ? , m? ? ) ? R ? ? , ? new ? = ? ? ? -? new ? ? ? + ? m? ? -? new ? l? ? ? R ? ? . 14:</formula><p>On chip, compute Pdropped ? ? = dropout( P? ? , ? drop ).</p><p>15:  </p><formula xml:id="formula_23">Write O ? ? diag(? new ? ) -1 (diag(? ? )? ? ? -? new ? O ? + ? m? ? -? new ? Pdropped ? ? V ? ) to HBM.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 FlashAttention: Backward Pass</head><p>We describe the full details of FlashAttention backward pass. Given input sequences Q, K, V ? R ? ?? , the output O ? R ? ?? , and the output gradient dO, we want to compute the input gradients dQ, dK, dV ? R ? ?? .</p><p>We first describe the standard attention backward pass in Algorithm 3 for completeness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3 Standard Attention Backward Pass</head><p>Require: Matrices Q, K, V, dO ? R ? ?? , P ? R ? ?? in HBM. We now make two observations about FlashAttention backward pass:</p><p>1. We do not need to store the dropout mask of size ? (? 2 ) from the forward pass. Instead, we can save the pseudo-random number generator states from the forward pass and re-generate the dropout mask in the backward pass. This allows us to only use ? (?) extra memory.</p><p>2. When computing the softmax gradient, we use Eq. ( <ref type="formula" target="#formula_14">4</ref>) to compute ? ? = ? ?: ?? ?: without reducing over ? ?: and ?? ?: of size ? (they might not fit into SRAM). Instead we can rewrite ? ? = ?? ? ? ? and compute the dot product between vectors of size ?. In the first step of computing the matrix multiply S = QK , the inputs Q, K are read from HBM and the output S ? R ? ?? is written to HBM (Algorithm 0 line 1). This incurs ?(? ? + ? 2 ) HBM accesses.</p><p>In the second step of computing P = softmax(S), the input S is read from HBM and the output P is written to HBM (Algorithm 0 line 2). This incurs ?(? 2 ) HBM accesses.</p><p>In the last step of computing O = PV, the inputs P, V are read from global memory and the output O is written to HBM (Algorithm 0 line 3). This incurs ?(? ? + ? 2 ) HBM accesses.</p><p>Overall, standard attention implementation requires ?(? ? + ? 2 ) global memory accesses.</p><p>We now analyze the IO complexity of streaming attention.</p><p>Following Algorithm 1, we see that each element of K and V is loaded from HBM once (Algorithm 1 line 6). We make ? ? passes over Q and O, each pass loading all of Q and all of O to HBM (Algorithm 1 line 8). Therefore the number of HBM accesses is ? (? ? + ? ?? ? ) = ?(? ?? ? ).</p><p>We derive the conditions on the block sizes ? ? and ? ? . We need the blocks K ? and V ? of size ? ? ? ? to fit into on-chip memory, which translates to:</p><formula xml:id="formula_24">? ? ? = ? (?) ? ? ? = ? ? ? .</formula><p>Similarly, we need the blocks Q ? , O ? of size ? ? ? ? to fit into on-chip memory, which translates to:</p><formula xml:id="formula_25">? ? ? = ? (?) ? ? ? = ? ? ? .</formula><p>Finally, we need the block S ? ? of size ? ? ? ? ? to fit into on-chip memory, which translates to:</p><formula xml:id="formula_26">? ? ? ? = ? (?).</formula><p>We therefore set:</p><formula xml:id="formula_27">? ? = ? ? ? , ? ? = ? min ? ? , ? ? ? = ? min ? ? , ? .</formula><p>We then have:</p><formula xml:id="formula_28">? ? = ? ? ? = ? ? ? ? .</formula><p>As a result, the number of HBM accesses is:</p><formula xml:id="formula_29">? (? ?? ? ) = ? ? 2 ? 2 ? .</formula><p>Proof of Proposition 3. For contradiction, suppose that there exists an algorithm that computes exact attention where the number for HBM access for all ? ? [?, ? ?] is</p><formula xml:id="formula_30">? ? 2 ? 2 ? .</formula><p>In the regime of ? = ?(? ?), this results in the number of HBM accesses:</p><formula xml:id="formula_31">? ? 2 ? 2 ? ? = ?(? ?).</formula><p>However, the input to attention (matrices Q, K, V) and the output O have size ? ? and they start out being in HBM, so if the algorithm computes exact attention it must incur at least ?(? ?) HBM accesses. This is a contradiction.</p><p>Proof of Theorem 5. The IO complexity of the attention backward is very similar to the IO complexity of the attention forward (Theorem 2). Here we provide a sketch of the proof.</p><p>We first analyze the IO complexity of standard attention backward pass. The inputs Q, K, V, dO ? R ? ?? reside in HBM, and the at the end of the algorithm the outputs dQ, dK, dV ? R ? ?? are written to HBM.</p><p>At each step of the standard attention backward pass, one needs to load inputs of size ? ? or ? 2 from HBM, and needs to write the outputs of size ? 2 or ? ? to HBM. This incurs ?(? ? + ? 2 ) HBM accesses.</p><p>We now analyze the IO complexity of FlashAttention backward pass. Similar to Theorem 2, we see that each element of K and V is loaded from HBM once. Each element of dK and dV is only written to HBM once. We make ? ? passes over Q, O, dO, each pass loading all of Q, O, dO to HBM. We also make ? ? passes over dQ, each pass reading/writing all of dQ from/to HBM. Therefore the number of HBM accesses is ? (? ? + ? ?? ? ) = ?(? ?? ? ).</p><p>As in the proof of Theorem 2, the constraints on the block sizes are that:</p><formula xml:id="formula_32">? ? = ? ? ? , ? ? = ? min ? ? , ? .</formula><p>We then have:</p><formula xml:id="formula_33">? ? = ? ? ? = ? ? ? ? .</formula><p>As a result, the number of HBM accesses is:</p><formula xml:id="formula_34">? (? ?? ? ) = ? ? 2 ? 2 ? .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Extension Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Block-sparse FlashAttention</head><p>We describe the full block-sparse FlashAttention algorithm in Algorithm 5. The algorithm is identical to Algorithm 2, except that we skip zero blocks. We prove the IO-complexity of block-sparse FlashAttention.</p><p>Proof of Proposition 4. The proof is very similar to the proof of Theorem 2. For the block-sparse case, notice that we only need to load blocks corresponding to nonzero blocks. As a result, the number of HBM accesses are scaled by ?, the fraction of nonzero blocks in the block-sparsity mask. However, for small values of ?, we would still need to write the result O ? R ? ?? . Therefore the number of HBM accesses is</p><formula xml:id="formula_35">? ? ? + ? 2 ? 2 ? ? .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Potential Extensions</head><p>We discuss here a few potential extensions of the IO-aware approach to speed up deep learning training.</p><p>Multi-GPU Attention. Large language models are trained on hundreds or thousands of GPUs, and one typically splits the attention computation between 4-8 GPUs on the same node <ref type="bibr" target="#b73">[74]</ref>. This introduces another level of memory hierarchy: beside GPU SRAM and GPU HBM, we also have the HBM of other GPUs. For very long sequences, the different GPUs on the same node can cooperate to compute attention by taking into account the asymmetry of different levels of memory hierarchy.</p><p>Sparse MLP layers. Typical dense MLP layers are compute-bound and not memory-bound. To improve their efficiency, MLP layers with sparse weight matrices can be used <ref type="bibr" target="#b15">[16]</ref>. However, many sparse MLP layers are instead memory-bound, and their speedup is often not proportional to the sparsity. We believe that an IO-aware implementation can alleviate this issue and realize the benefits of sparsity. We are excited about future work in this direction, to reduce the computational requirement of large models and improve their wall-block runtime.</p><p>Kernel regression. Our approach in FlashAttention relies on the fact that the ? ? ? attention matrix is a function of a low-rank matrix QK (of rank ? ?). As a result, we can repeatedly load the Algorithm 5 Block-Sparse FlashAttention Forward Pass</p><p>Require: Matrices Q, K, V ? R ? ?? in HBM, on-chip SRAM of size ?, softmax scaling constant ? ? R, masking function mask, dropout probability ? drop , block sizes ? ? = ? 4? , ? ? = min ? 4? , ? , block sparsity mask ? ? {0, 1} ? /? ? ?? /? ? .. 1: Initialize the pseudo-random number generator state R and save to HBM. Load K ? , V ? from HBM to on-chip SRAM. if ? ? ? ? 0 then 9:</p><formula xml:id="formula_36">2: Initialize O = (0) ? ?? ? R ? ?? , ? = (0) ? ? R ? , ? = (-?) ? ? R ? in HBM. 3: Divide Q into ? ? = ? ? ? blocks Q 1 , . . . ,</formula><p>Load Q ? , O ? , ? ? , ? ? from HBM to on-chip SRAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10:</head><p>On chip, compute S ? ? = ?Q ? K ? ? ? R ? ? ?? ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><p>On chip, compute S masked</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? ?</head><p>= mask(S ? ? ).</p><p>12:</p><p>On chip, compute m? ? = rowmax(S masked ? ?</p><formula xml:id="formula_37">) ? R ? ? , P? ? = exp(S masked ? ? -m? ? ) ? R ? ? ?? ? (pointwise), l? ? = rowsum( P? ? ) ? R ? ? . 13: On chip, compute ? new ? = max(? ? , m? ? ) ? R ? ? , ? new ? = ? ? ? -? new ? ? ? + ? m? ? -? new ? l? ? ? R ? ? . 14:</formula><p>On chip, compute Pdropped    inputs Q, K and recompute the block of the attention matrix that we need, significantly reducing HBM access. As similar scenario happens in kernel regression: each element ? ? ? of the ? ? ? kernel matrix K is a function of two vectors of size ? ?, as it measures the similarity between two datapoints ? ? and ? ? . We hope that our IO-aware approach motivate methods to speed up kernel regression.</p><formula xml:id="formula_38">Write O ? ? diag(? new ? ) -1 (diag(? ? )? ? ? -? new ? O ? + ? m? ? -? new ? Pdropped ? ? V ? ) to HBM.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Full Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 BERT</head><p>We train BERT-large following the training procedure and hyperparameters of the reference MLPerf 1.1 implementation. In particular, we use the LAMB optimizer with learning rate 3.75e-3, with batch size 448, trained for at most 7100 steps. The training is stopped once the validation accuracy (for masked language modeling) reaches the target 72.0%, and the wall-clock run-time is measured. We train with FP16 precision using Apex AMP (with O2 optimization level).</p><p>We compare our results with the reported training speed from Nvidia that was submitted to MLPerf 1.1 (Table <ref type="table">1</ref>).</p><p>We use the same train / validation data split provided by MLPerf 1.1 reference implementation. In particular, we evaluate on the same 10000 validation examples as the baseline from Nvidia.</p><p>We train the model on 8?A100-80GB GPUs. Each training run takes between 16 and 19 minutes, and we average the results of 10 runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 GPT-2</head><p>We use the standard implementations of GPT-2 <ref type="bibr" target="#b63">[64]</ref> from Huggingface transformers library and from Nvidia's Megatron-LM repo. We follow the training recipe of the Megatron-LM repo.</p><p>We use an effective batch size of 512, and use gradient accumulation to fit into available GPU memory. We use the AdamW optimizer, with learning rate 6e-4 for GPT-2 small and 1.5e-4 for GPT-2 medium, and weight decay of 0.1. All models are trained with the same hyperparameters for 400K steps. We run all implementations with mixed-precision training (PyTorch AMP).</p><p>We use the Openwebtext dataset, with the GPT-2 BPE tokenizer. We randomly select 0.5% of the dataset as the validation set, with the rest being used as training set. This random selection of validation set is done once, and all models are evaluated on the same validation set.</p><p>We train the model on 8?A100-40GB GPUs, and we measure the wall-clock training time. Training GPT-2 small takes between 2.7-9.5 days, and training GPT-2 medium takes between 6.9-21.0 days (Table <ref type="table" target="#tab_2">2</ref>).</p><p>In Fig. <ref type="figure" target="#fig_10">4</ref>, we plot of the validation perplexity throughout training of GPT-2 small/medium, using either HuggingFace implementation or our FlashAttention implementation. We see that FlashAttention behaves the same as the baseline implementation and the validation perplexity curves of the two implementations almost lie on top of each other.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Long Document Classification</head><p>For MIMIC-III and ECtHR, we follow the hyperparameters of Dai et al. <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4 LRA</head><p>We follow the hyperparameters from the Long-range arena paper <ref type="bibr" target="#b76">[77]</ref>, the Long-range arena repo (https: //github.com/google-research/long-range-arena), and the Nystr?mformer reproduction <ref type="bibr" target="#b86">[87]</ref>. To be generous to the baseline methods, if we are unable to reproduce the performance of any baseline for any of the five tasks, we report the better performance from Tay et al. <ref type="bibr" target="#b76">[77]</ref> or Xiong et al. <ref type="bibr" target="#b86">[87]</ref> for that baseline on that task.            </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>3 : 4 :</head><label>34</label><figDesc>Load P and V by blocks from HBM, compute O = PV, write O to HBM. Return O.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>7 : 1 ?</head><label>71</label><figDesc>for ? ? ? ? do 8:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>16 :</head><label>16</label><figDesc>Write ? ? ? ? new ? , ? ? ? ? new ? to HBM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>19 :</head><label>19</label><figDesc>Return O, ?, ?, R.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>7 : 1 ?</head><label>71</label><figDesc>for ? ? ? ? do 8:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>=</head><label></label><figDesc>dropout( P? ? , ? drop ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>16 :</head><label>16</label><figDesc>Write ? ? ? ? new ? , ? ? ? ? new ? to HBM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>20 :</head><label>20</label><figDesc>Return O, ?, ?, R.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: Validation perplexity of GPT-2 small/medium using two implementations. We confirm that FlashAttention yields the same validation curves as the baseline implementation from HuggingFace.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Q ? ? of size ? ? ? ? each, and divide K, V in to ? ? = ? ? ? blocks K 1 , . . . , K ? ? and V 1 , . . . , V ? ? , of size ? ? ? ? each. 4: Divide O into ? ? blocks O ? , . . . , O ? ? of size ? ? ? ? each, divide ? into ? ? blocks ? ? , . . . , ? ? ? of size ? ? each, divide ? into ? ? blocks ? 1 , . . . , ? ? ? of size ? ? each. 5: for 1 ? ? ? ? ? do</figDesc><table><row><cell>6:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Dense FlashAttention B lo c k -S p a r s e F la s h A t t e n t io n 2 4 6 Runtime H B M A c c e ss e s</head><label></label><figDesc>FlashAttention is faster than FlashAttention by a factor proportional to the sparsity.asymptotically improve on HBM accesses over all SRAM sizes. Proofs are in Appendix C. Theorem 2. Let ? be the sequence length, ? be the head dimension, and ? be size of SRAM with ? ? ? ? ? ?. Standard attention (Algorithm 0) requires ?(? ? + ? 2 ) HBM accesses, while FlashAttention (Algorithm 1) requires ?(? 2 ? 2 ? -1 ) HBM accesses.</figDesc><table><row><cell>Attention GFLOPs HBM R/W (GB) Runtime (ms)</cell><cell>Standard FlashAttention 66.6 75.2 40.3 4.4 41.7 7.3</cell><cell>HBM Accesses (GB)</cell><cell>E ect of Block Size 64 128 256</cell><cell>512</cell><cell>6 2</cell><cell>Fwd Runtime (ms)</cell><cell>Fwd + Bwd (ms)</cell><cell>50 100 150</cell><cell>Sparsity Speedup 20 60</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Block Size</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>% Non-Zero Blocks</cell></row><row><cell cols="10">Figure 2: Left: Forward + backward runtime of standard attention and FlashAttention for GPT-2 medium</cell></row><row><cell cols="10">(seq. length 1024, head dim. 64, 16 heads, batch size 64) on A100 GPU. HBM access is the primary factor affecting</cell></row><row><cell cols="10">runtime. Middle: Forward runtime of FlashAttention (seq. length 1024, head dim. 64, 16 heads, batch size 64) on</cell></row><row><cell cols="10">A100 GPU. Fewer HBM accesses result in faster runtime, up to a point. Right: The runtime (for seq. length 4K) of</cell></row><row><cell>block-sparse</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>GPT-2 small and medium using FlashAttention achieve up to 3? speed up compared to Huggingface implementation and up to 1.7? compared to Megatron-LM. Training time reported on 8?A100s GPUs.</figDesc><table><row><cell>Model implementations</cell><cell cols="2">OpenWebText (ppl) Training time (speedup)</cell></row><row><cell>GPT-2 small -Huggingface [84]</cell><cell>18.2</cell><cell>9.5 days (1.0?)</cell></row><row><cell>GPT-2 small -Megatron-LM [74]</cell><cell>18.2</cell><cell>4.7 days (2.0?)</cell></row><row><cell>GPT-2 small -FlashAttention</cell><cell>18.2</cell><cell>2.7 days (3.5?)</cell></row><row><cell>GPT-2 medium -Huggingface [84]</cell><cell>14.2</cell><cell>21.0 days (1.0?)</cell></row><row><cell>GPT-2 medium -Megatron-LM [74]</cell><cell>14.3</cell><cell>11.5 days (1.8?)</cell></row><row><cell>GPT-2 medium -FlashAttention</cell><cell>14.3</cell><cell>6.9 days (3.0?)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>The performance of standard attention, FlashAttention, block-sparse FlashAttention, and approximate attention baselines on the Long-Range-Arena benchmarks.</figDesc><table><row><cell>Models</cell><cell cols="7">ListOps Text Retrieval Image Pathfinder Avg Speedup</cell></row><row><cell>Transformer</cell><cell>36.0</cell><cell>63.6</cell><cell>81.6</cell><cell>42.3</cell><cell>72.7</cell><cell>59.3</cell><cell>-</cell></row><row><cell>FlashAttention</cell><cell>37.6</cell><cell>63.9</cell><cell>81.4</cell><cell>43.5</cell><cell>72.7</cell><cell>59.8</cell><cell>2.4?</cell></row><row><cell>Block-sparse FlashAttention</cell><cell>37.0</cell><cell>63.0</cell><cell>81.3</cell><cell>43.6</cell><cell>73.3</cell><cell>59.6</cell><cell>2.8?</cell></row><row><cell>Linformer [81]</cell><cell>35.6</cell><cell>55.9</cell><cell>77.7</cell><cell>37.8</cell><cell>67.6</cell><cell>54.9</cell><cell>2.5?</cell></row><row><cell>Linear Attention [48]</cell><cell>38.8</cell><cell>63.2</cell><cell>80.7</cell><cell>42.6</cell><cell>72.5</cell><cell>59.6</cell><cell>2.3?</cell></row><row><cell>Performer [11]</cell><cell>36.8</cell><cell>63.6</cell><cell>82.2</cell><cell>42.1</cell><cell>69.9</cell><cell>58.9</cell><cell>1.8?</cell></row><row><cell>Local Attention [77]</cell><cell>36.1</cell><cell>60.2</cell><cell>76.7</cell><cell>40.6</cell><cell>66.6</cell><cell>56.0</cell><cell>1.7?</cell></row><row><cell>Reformer [49]</cell><cell>36.5</cell><cell>63.8</cell><cell>78.5</cell><cell>39.6</cell><cell>69.4</cell><cell>57.6</cell><cell>1.3?</cell></row><row><cell>Smyrf [18]</cell><cell>36.1</cell><cell>64.1</cell><cell>79.0</cell><cell>39.6</cell><cell>70.5</cell><cell>57.9</cell><cell>1.7?</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>GPT-2 small with FlashAttention, with 4? larger context length compared to Megatron-LM, is still 30% faster while achieving 0.7 better perplexity. Training time on 8?A100 GPUs is reported. Left: runtime of forward pass + backward pass. Right: attention memory usage.European Court of Human Rights, each of which is mapped to articles of the Convention of Human Rights that were allegedly violaged. Both of these datasets contain very long text documents; the average number of tokens in MIMIC is 2,395 tokens, and the longest document contains 14,562 tokens, while the average and longest numbers in ECtHR are 2,197 and 49,392, respectively. We evaluate lift from increasing the sequence length of a pretrained RoBERTa model</figDesc><table><row><cell>Model implementations</cell><cell cols="3">Context length OpenWebText (ppl) Training time (speedup)</cell></row><row><cell>GPT-2 small -Megatron-LM</cell><cell>1k</cell><cell>18.2</cell><cell>4.7 days (1.0?)</cell></row><row><cell>GPT-2 small -FlashAttention</cell><cell>1k</cell><cell>18.2</cell><cell>2.7 days (1.7?)</cell></row><row><cell>GPT-2 small -FlashAttention</cell><cell>2k</cell><cell>17.6</cell><cell>3.0 days (1.6?)</cell></row><row><cell>GPT-2 small -FlashAttention</cell><cell>4k</cell><cell>17.5</cell><cell>3.6 days (1.3?)</cell></row></table><note><p><p><p><p><p>Long Document Classification. Training Transformers with longer sequences with FlashAttention improves performance on the MIMIC-III</p><ref type="bibr" target="#b44">[45]</ref> </p>and ECtHR</p><ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> </p>datasets. MIMIC-III contains intensive care unit patient discharge summaries, each annotated with multiple labels. ECtHR contains legal cases from the</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Long Document performance (micro ? 1 ) at different sequence lengths using FlashAttention.</figDesc><table><row><cell cols="5">512 1024 2048 4096 8192 16384</cell></row><row><cell>MIMIC-III 52.8 50.7</cell><cell>51.7</cell><cell>54.6</cell><cell>56.4</cell><cell>57.1</cell></row><row><cell>ECtHR 72.2 74.3</cell><cell>77.1</cell><cell cols="2">78.6 80.7</cell><cell>79.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table /><note><p>We report the first Transformer model that can achieve non-random performance on Path-X and Path-256.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>(left) reports the runtime in milliseconds of the forward + backward pass of FlashAttention and block-sparse FlashAttention compared to the baselines in exact, approximate, and sparse attention (exact numbers in Appendix E). Runtime grows quadratically with sequence length, but FlashAttention runs significantly faster than exact attention baselines, up to 3? faster than the PyTorch implementation. The runtimes of many approximate/sparse attention mechanisms grow linearly with sequence length, but FlashAttention still runs faster than approximate and sparse attention for short sequences due to fewer memory accesses. The approximate attention runtimes begin to cross over with FlashAttention at sequences between 512 and 1024. On the other hand, block-sparse FlashAttention is faster than all implementations of exact, sparse, and approximate attention that we know of, across all sequence lengths. Memory Footprint. Figure3(right) shows the memory footprint of FlashAttention and block-sparse FlashAttention compared to various exact, approximate, and sparse attention baselines. FlashAttention and block-sparse FlashAttention have the same memory footprint, which grows linearly with sequence length. FlashAttention is up to 20? more memory efficient than exact attention baselines, and is more memory-efficient than the approximate attention baselines. All other algorithms except for Linformer run out of memory on an A100 GPU before 64K, and FlashAttention is still 2? more efficient than Linformer.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Q ? ? of size ? ? ? ? each, and divide K, V in to ? ? = ? ? ? blocks K 1 , . . . , K ? ? and V 1 , . . . , V ? ? , of size ? ? ? ? each. 5: Divide O into ? ? blocks O ? , . . . , O ? ? of size ? ? ? ? each, divide ? into ? ? blocks ? ? , . . . , ? ? ? of size ? ? each, divide ? into ? ? blocks ? 1 , . . . , ? ? ? of size ? ? each. 6: for 1 ? ? ? ? ? do</figDesc><table><row><cell>7:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>? ?? where ?? ? ? = ? ? ? (?? ? ? -? ? ?? ?? ?? ), write dS to HBM.</figDesc><table /><note><p>1: Load P, dO by blocks from HBM, compute dV = P dO ? R ? ?? , write dV to HBM. 2: Load dO, V by blocks from HBM, compute dP = dOV ? R ? ?? , write dP to HBM. 3: Read P, dP from HBM, compute dS ? R 4: Load dS and K by blocks from HBM, compute dQ = dSK, write dQ to HBM. 5: Load dS and Q by blocks from HBM, compute dK = dS Q, write dK to HBM. 6: Return dQ, dK, dV.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>The full FlashAttention backward pass algorithm is in Algorithm 4. Conceptually it is just a block version of the derivation in Appendix B.2. FlashAttention Backward Pass Require: Matrices Q, K, V, O, dO ? R ? ?? in HBM, vectors ?, ? ? R ? in HBM, on-chip SRAM of size ?, softmax scaling constant ? ? R, masking function mask, dropout probability ? drop , pseudo-random number generator state R from the forward pass. 1: Set the pseudo-random number generator state to R. 2: Set block sizes ? ? = ? 4? , ? ? = min ? 4? , ? . 3: Divide Q into ? ? = ? ? ? blocks Q 1 , . . . , Q ? ? of size ? ? ? ? each, and divide K, V in to ? ? = ? ? ? blocks K 1 , . . . , K ? ? and V 1 , . . . , V ? ? , of size ? ? ? ? each. 4: Divide O into ? ? blocks O ? , . . . , O ? ? of size ? ? ? ? each, divide dO into ? ? blocks dO ? , . . . , dO ? ? of size ? ? ? ? each, divide ? into ? ? blocks ? ? , . . . , ? ? ? of size ? ? each, divide ? into ? ? blocks ? 1 , . . . , ? ? ? of size ? ? each. 5: Initialize dQ = (0) ? ?? in HBM and divide it into ? ? blocks dQ 1 , . . . , dQ ? ? of size ? ? ? ? each. Initialize dK = (0) ? ?? , dV = (0) ? ?? in HBM and divide dK, dV in to ? ? blocks dK 1 , . . . , dK ? ? and dV 1 , . . . , dV ? ? , of size ? ? ? ? each.</figDesc><table><row><cell>Algorithm 4</cell></row></table><note><p>6: for 1 ? ? ? ? ? do</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>Q ? ? of size ? ? ? ? each, and divide K, V in to ? ? = ? ? ? blocks K 1 , . . . , K ? ? and V 1 , . . . , V ? ? , of size ? ? ? ? each. 4: Divide O into ? ? blocks O ? , . . . , O ? ? of size ? ? ? ? each, divide ? into ? ? blocks ? ? , . . . , ? ? ? of size ? ? each, divide ? into ? ? blocks ? 1 , . . . , ? ? ? of size ? ? each. 5: for 1 ? ? ? ? ? do</figDesc><table><row><cell>6:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 7 :</head><label>7</label><figDesc>Pointers to results tables.</figDesc><table><row><cell cols="2">Dropout Masking</cell><cell>Pass</cell><cell>Table</cell></row><row><cell>Yes</cell><cell>Yes</cell><cell>Forward</cell><cell>Table 8</cell></row><row><cell>Yes</cell><cell>Yes</cell><cell>Backward</cell><cell>Table 9</cell></row><row><cell>Yes</cell><cell>Yes</cell><cell>Combined</cell><cell>Table 10</cell></row><row><cell>No</cell><cell>Yes</cell><cell>Forward</cell><cell>Table 11</cell></row><row><cell>No</cell><cell>Yes</cell><cell>Backward</cell><cell>Table 12</cell></row><row><cell>No</cell><cell>Yes</cell><cell>Combined</cell><cell>Table 13</cell></row><row><cell>Yes</cell><cell>No</cell><cell>Forward</cell><cell>Table 14</cell></row><row><cell>Yes</cell><cell>No</cell><cell>Backward</cell><cell>Table 15</cell></row><row><cell>Yes</cell><cell>No</cell><cell>Combined</cell><cell>Table 16</cell></row><row><cell>No</cell><cell>No</cell><cell>Forward</cell><cell>Table 17</cell></row><row><cell>No</cell><cell>No</cell><cell>Backward</cell><cell>Table 18</cell></row><row><cell>No</cell><cell>No</cell><cell>Combined</cell><cell>Table 19</cell></row><row><cell>No</cell><cell>No</cell><cell cols="2">Memory Usage (Combined) Table 20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 8 :</head><label>8</label><figDesc>Forward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length, with dropout and masking. Best in bold, second best underlined.</figDesc><table><row><cell>Attention Method</cell><cell>128</cell><cell>256</cell><cell>512</cell><cell>1024</cell><cell>2048</cell><cell>4096</cell><cell>8192</cell><cell>16384</cell><cell>32768</cell><cell>65536</cell></row><row><cell>PyTorch Attention</cell><cell>0.36</cell><cell>0.34</cell><cell>0.78</cell><cell>2.54</cell><cell>9.33</cell><cell>36.33</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Megatron</cell><cell>0.40</cell><cell>0.40</cell><cell>1.10</cell><cell>3.65</cell><cell>16.19</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Reformer</cell><cell>2.03</cell><cell>3.15</cell><cell>5.67</cell><cell>11.02</cell><cell>22.59</cell><cell>46.14</cell><cell>97.38</cell><cell>212.13</cell><cell>-</cell><cell>-</cell></row><row><cell>Local Attention</cell><cell>0.83</cell><cell>0.86</cell><cell>1.01</cell><cell>2.20</cell><cell>7.13</cell><cell>14.32</cell><cell>28.60</cell><cell>57.79</cell><cell>117.67</cell><cell>-</cell></row><row><cell>Linformer</cell><cell>0.67</cell><cell>0.52</cell><cell>0.69</cell><cell>0.71</cell><cell>1.65</cell><cell>3.18</cell><cell>6.15</cell><cell>12.16</cell><cell>24.17</cell><cell>52.39</cell></row><row><cell>Smyrf</cell><cell>2.27</cell><cell>2.34</cell><cell>3.91</cell><cell>7.44</cell><cell>14.71</cell><cell>29.22</cell><cell>58.27</cell><cell>116.41</cell><cell>-</cell><cell>-</cell></row><row><cell>LSformer</cell><cell>1.18</cell><cell>1.27</cell><cell>1.34</cell><cell>3.38</cell><cell>11.40</cell><cell>22.55</cell><cell>44.95</cell><cell>89.76</cell><cell>179.66</cell><cell>-</cell></row><row><cell>Block Sparse</cell><cell>1.12</cell><cell>1.11</cell><cell>2.13</cell><cell>2.77</cell><cell>6.95</cell><cell>20.91</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Longformer</cell><cell>1.22</cell><cell>1.14</cell><cell>1.08</cell><cell>1.95</cell><cell>5.72</cell><cell>12.98</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>BigBird</cell><cell>1.13</cell><cell>1.12</cell><cell>1.12</cell><cell>1.77</cell><cell>6.03</cell><cell>13.68</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>FlashAttention</cell><cell>0.04</cell><cell>0.06</cell><cell>0.21</cell><cell>0.82</cell><cell>2.85</cell><cell>10.41</cell><cell>41.74</cell><cell>167.19</cell><cell>670.76</cell><cell>2682.35</cell></row><row><cell>Block-Sparse FlashAttention</cell><cell>0.06</cell><cell>0.06</cell><cell>0.06</cell><cell>0.12</cell><cell>0.44</cell><cell>0.86</cell><cell>1.70</cell><cell>3.29</cell><cell>6.55</cell><cell>13.34</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 9 :</head><label>9</label><figDesc>Backward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length, with dropout and masking. Best in bold, second best underlined.</figDesc><table><row><cell>Attention Method</cell><cell>128</cell><cell>256</cell><cell>512</cell><cell>1024</cell><cell>2048</cell><cell>4096</cell><cell>8192</cell><cell>16384</cell><cell>32768</cell><cell>65536</cell></row><row><cell>PyTorch Attention</cell><cell>0.37</cell><cell>0.49</cell><cell>1.66</cell><cell>5.81</cell><cell>22.32</cell><cell>87.67</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Megatron</cell><cell>0.35</cell><cell>0.32</cell><cell>0.77</cell><cell>2.42</cell><cell>8.43</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Reformer</cell><cell>2.37</cell><cell>4.59</cell><cell>8.91</cell><cell>17.68</cell><cell>35.13</cell><cell>70.05</cell><cell>140.01</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Local Attention</cell><cell>0.55</cell><cell>0.62</cell><cell>1.49</cell><cell>4.03</cell><cell>13.78</cell><cell>27.61</cell><cell>55.20</cell><cell>110.27</cell><cell>221.40</cell><cell>-</cell></row><row><cell>Linformer</cell><cell>0.89</cell><cell>0.80</cell><cell>0.81</cell><cell>0.93</cell><cell>2.48</cell><cell>4.75</cell><cell>9.29</cell><cell>18.27</cell><cell>36.53</cell><cell>-</cell></row><row><cell>Smyrf</cell><cell>1.41</cell><cell>2.83</cell><cell>5.43</cell><cell>10.72</cell><cell>21.25</cell><cell>42.31</cell><cell>84.48</cell><cell>168.95</cell><cell>-</cell><cell>-</cell></row><row><cell>LSformer</cell><cell>1.75</cell><cell>1.76</cell><cell>3.01</cell><cell>7.50</cell><cell>20.07</cell><cell>39.08</cell><cell>76.39</cell><cell>150.82</cell><cell>-</cell><cell>-</cell></row><row><cell>Block Sparse</cell><cell>1.29</cell><cell>1.28</cell><cell>2.18</cell><cell>3.04</cell><cell>7.27</cell><cell>21.16</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Longformer</cell><cell>1.27</cell><cell>1.31</cell><cell>1.29</cell><cell>2.04</cell><cell>5.24</cell><cell>10.74</cell><cell>25.95</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>BigBird</cell><cell>1.33</cell><cell>1.28</cell><cell>1.32</cell><cell>1.81</cell><cell>5.55</cell><cell>11.44</cell><cell>27.45</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>FlashAttention</cell><cell>0.30</cell><cell>0.26</cell><cell>0.68</cell><cell>2.02</cell><cell>6.84</cell><cell>26.89</cell><cell>105.70</cell><cell>418.96</cell><cell>1666.89</cell><cell>6660.44</cell></row><row><cell>Block-Sparse FlashAttention</cell><cell>0.30</cell><cell>0.27</cell><cell>0.29</cell><cell>0.59</cell><cell>1.50</cell><cell>2.94</cell><cell>5.82</cell><cell>11.85</cell><cell>23.98</cell><cell>47.61</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 10 :</head><label>10</label><figDesc>Forward pass + backward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length, with dropout and masking. Best in bold, second best underlined.</figDesc><table><row><cell>Attention Method</cell><cell>128</cell><cell>256</cell><cell>512</cell><cell>1024</cell><cell>2048</cell><cell>4096</cell><cell>8192</cell><cell>16384</cell><cell>32768</cell><cell>65536</cell></row><row><cell>PyTorch Attention</cell><cell>0.84</cell><cell>0.86</cell><cell>2.35</cell><cell>8.29</cell><cell>31.75</cell><cell>124.19</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Megatron</cell><cell>0.87</cell><cell>0.89</cell><cell>1.33</cell><cell>4.21</cell><cell>16.50</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Reformer</cell><cell>4.30</cell><cell>7.76</cell><cell>14.60</cell><cell>28.74</cell><cell>57.79</cell><cell>116.34</cell><cell>237.57</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Local Attention</cell><cell>1.40</cell><cell>1.60</cell><cell>2.06</cell><cell>6.06</cell><cell>20.94</cell><cell>42.01</cell><cell>84.08</cell><cell>168.48</cell><cell>339.45</cell><cell>-</cell></row><row><cell>Linformer</cell><cell>1.57</cell><cell>1.49</cell><cell>1.55</cell><cell>1.60</cell><cell>4.19</cell><cell>8.04</cell><cell>15.71</cell><cell>30.92</cell><cell>61.47</cell><cell>-</cell></row><row><cell>Smyrf</cell><cell>3.41</cell><cell>5.08</cell><cell>9.35</cell><cell>18.18</cell><cell>36.03</cell><cell>71.68</cell><cell>143.04</cell><cell>285.87</cell><cell>-</cell><cell>-</cell></row><row><cell>LSformer</cell><cell>3.08</cell><cell>3.10</cell><cell>4.26</cell><cell>10.90</cell><cell>31.59</cell><cell>61.72</cell><cell>121.51</cell><cell>241.18</cell><cell>-</cell><cell>-</cell></row><row><cell>Block Sparse</cell><cell>2.54</cell><cell>2.52</cell><cell>3.71</cell><cell>5.44</cell><cell>13.29</cell><cell>39.19</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Longformer</cell><cell>2.47</cell><cell>2.49</cell><cell>2.51</cell><cell>3.10</cell><cell>10.39</cell><cell>22.49</cell><cell>60.44</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>BigBird</cell><cell>2.51</cell><cell>2.49</cell><cell>2.52</cell><cell>3.40</cell><cell>10.97</cell><cell>23.89</cell><cell>63.28</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>FlashAttention</cell><cell>0.43</cell><cell>0.41</cell><cell>0.95</cell><cell>2.55</cell><cell>9.56</cell><cell>37.49</cell><cell>147.75</cell><cell>586.61</cell><cell>2339.11</cell><cell>9341.30</cell></row><row><cell>Block-Sparse FlashAttention</cell><cell>0.44</cell><cell>0.44</cell><cell>0.45</cell><cell>0.89</cell><cell>1.95</cell><cell>4.12</cell><cell>7.64</cell><cell>16.60</cell><cell>32.73</cell><cell>64.11</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 11 :</head><label>11</label><figDesc>Forward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length, with masking. Best in bold, second best underlined.</figDesc><table><row><cell>Attention Method</cell><cell>128</cell><cell>256</cell><cell>512</cell><cell>1024</cell><cell>2048</cell><cell>4096</cell><cell>8192</cell><cell>16384</cell><cell>32768</cell><cell>65536</cell></row><row><cell>PyTorch Attention</cell><cell>0.30</cell><cell>0.30</cell><cell>0.63</cell><cell>1.93</cell><cell>7.08</cell><cell>27.45</cell><cell>112.90</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Megatron</cell><cell>0.45</cell><cell>0.41</cell><cell>0.43</cell><cell>1.52</cell><cell>5.80</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Reformer</cell><cell>1.87</cell><cell>3.00</cell><cell>5.37</cell><cell>10.43</cell><cell>21.40</cell><cell>43.83</cell><cell>92.80</cell><cell>203.24</cell><cell>-</cell><cell>-</cell></row><row><cell>Local Attention</cell><cell>0.70</cell><cell>0.81</cell><cell>1.02</cell><cell>2.09</cell><cell>6.64</cell><cell>13.34</cell><cell>26.77</cell><cell>54.02</cell><cell>110.11</cell><cell>-</cell></row><row><cell>Linformer</cell><cell>0.63</cell><cell>0.50</cell><cell>0.67</cell><cell>0.65</cell><cell>1.36</cell><cell>2.60</cell><cell>5.04</cell><cell>9.92</cell><cell>19.69</cell><cell>43.47</cell></row><row><cell>Smyrf</cell><cell>2.38</cell><cell>2.32</cell><cell>3.76</cell><cell>7.16</cell><cell>14.14</cell><cell>28.09</cell><cell>55.98</cell><cell>111.73</cell><cell>-</cell><cell>-</cell></row><row><cell>LSformer</cell><cell>1.22</cell><cell>1.29</cell><cell>1.44</cell><cell>3.28</cell><cell>10.99</cell><cell>21.72</cell><cell>43.29</cell><cell>86.32</cell><cell>172.76</cell><cell>-</cell></row><row><cell>Block Sparse</cell><cell>0.96</cell><cell>1.04</cell><cell>1.66</cell><cell>2.16</cell><cell>5.41</cell><cell>16.15</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Longformer</cell><cell>0.99</cell><cell>0.98</cell><cell>0.99</cell><cell>1.56</cell><cell>4.79</cell><cell>11.07</cell><cell>32.98</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>BigBird</cell><cell>0.96</cell><cell>1.02</cell><cell>1.02</cell><cell>1.48</cell><cell>5.05</cell><cell>11.59</cell><cell>34.16</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>FlashAttention</cell><cell>0.03</cell><cell>0.04</cell><cell>0.17</cell><cell>0.68</cell><cell>2.28</cell><cell>8.40</cell><cell>33.55</cell><cell>134.14</cell><cell>537.50</cell><cell>2150.88</cell></row><row><cell>Block-Sparse FlashAttention</cell><cell>0.05</cell><cell>0.04</cell><cell>0.05</cell><cell>0.11</cell><cell>0.35</cell><cell>0.68</cell><cell>1.33</cell><cell>2.54</cell><cell>5.34</cell><cell>10.73</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 12 :</head><label>12</label><figDesc>Backward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length, with masking. Best in bold, second best underlined.</figDesc><table><row><cell>Attention Method</cell><cell>128</cell><cell>256</cell><cell>512</cell><cell>1024</cell><cell>2048</cell><cell>4096</cell><cell>8192</cell><cell>16384</cell><cell>32768</cell><cell>65536</cell></row><row><cell>PyTorch Attention</cell><cell>0.44</cell><cell>0.46</cell><cell>1.53</cell><cell>5.33</cell><cell>20.34</cell><cell>79.87</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Megatron</cell><cell>0.29</cell><cell>0.31</cell><cell>0.65</cell><cell>1.95</cell><cell>6.49</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Reformer</cell><cell>2.31</cell><cell>4.47</cell><cell>8.68</cell><cell>17.20</cell><cell>34.14</cell><cell>68.09</cell><cell>136.02</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Local Attention</cell><cell>0.51</cell><cell>0.62</cell><cell>1.30</cell><cell>3.81</cell><cell>13.33</cell><cell>26.72</cell><cell>53.41</cell><cell>106.82</cell><cell>214.15</cell><cell>-</cell></row><row><cell>Linformer</cell><cell>0.76</cell><cell>0.81</cell><cell>0.94</cell><cell>0.87</cell><cell>2.24</cell><cell>4.25</cell><cell>8.35</cell><cell>16.38</cell><cell>32.67</cell><cell>72.11</cell></row><row><cell>Smyrf</cell><cell>1.34</cell><cell>2.77</cell><cell>5.30</cell><cell>10.46</cell><cell>20.73</cell><cell>41.27</cell><cell>82.41</cell><cell>164.86</cell><cell>-</cell><cell>-</cell></row><row><cell>LSformer</cell><cell>1.66</cell><cell>1.61</cell><cell>3.09</cell><cell>7.42</cell><cell>19.68</cell><cell>38.35</cell><cell>74.92</cell><cell>147.86</cell><cell>-</cell><cell>-</cell></row><row><cell>Block Sparse</cell><cell>1.24</cell><cell>1.25</cell><cell>2.04</cell><cell>2.91</cell><cell>6.78</cell><cell>19.67</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Longformer</cell><cell>1.27</cell><cell>1.23</cell><cell>1.24</cell><cell>1.85</cell><cell>4.99</cell><cell>10.21</cell><cell>24.89</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>BigBird</cell><cell>1.43</cell><cell>1.50</cell><cell>1.44</cell><cell>1.69</cell><cell>5.25</cell><cell>10.86</cell><cell>26.26</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>FlashAttention</cell><cell>0.21</cell><cell>0.22</cell><cell>0.62</cell><cell>1.84</cell><cell>5.77</cell><cell>22.25</cell><cell>86.21</cell><cell>338.91</cell><cell>1343.91</cell><cell>5361.09</cell></row><row><cell>Block-Sparse FlashAttention</cell><cell>0.22</cell><cell>0.22</cell><cell>0.26</cell><cell>0.57</cell><cell>1.55</cell><cell>3.13</cell><cell>5.98</cell><cell>12.21</cell><cell>23.49</cell><cell>47.85</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 13 :</head><label>13</label><figDesc>Forward pass + backward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length, with masking. Best in bold, second best underlined.</figDesc><table><row><cell>Attention Method</cell><cell>128</cell><cell>256</cell><cell>512</cell><cell>1024</cell><cell>2048</cell><cell>4096</cell><cell>8192</cell><cell>16384</cell><cell>32768</cell><cell>65536</cell></row><row><cell>PyTorch Attention</cell><cell>0.80</cell><cell>0.81</cell><cell>2.08</cell><cell>7.23</cell><cell>27.51</cell><cell>107.58</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Megatron</cell><cell>0.81</cell><cell>0.83</cell><cell>1.09</cell><cell>3.36</cell><cell>12.39</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Reformer</cell><cell>4.16</cell><cell>7.46</cell><cell>14.06</cell><cell>27.68</cell><cell>55.66</cell><cell>112.15</cell><cell>229.37</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Local Attention</cell><cell>1.39</cell><cell>1.68</cell><cell>2.08</cell><cell>5.83</cell><cell>20.04</cell><cell>40.16</cell><cell>80.44</cell><cell>161.35</cell><cell>325.11</cell><cell>-</cell></row><row><cell>Linformer</cell><cell>1.51</cell><cell>1.42</cell><cell>1.56</cell><cell>1.67</cell><cell>3.67</cell><cell>6.99</cell><cell>13.63</cell><cell>26.77</cell><cell>53.36</cell><cell>117.56</cell></row><row><cell>Smyrf</cell><cell>3.38</cell><cell>4.93</cell><cell>9.07</cell><cell>17.66</cell><cell>34.94</cell><cell>69.55</cell><cell>138.72</cell><cell>277.41</cell><cell>-</cell><cell>-</cell></row><row><cell>LSformer</cell><cell>3.08</cell><cell>3.10</cell><cell>4.26</cell><cell>10.90</cell><cell>31.59</cell><cell>61.72</cell><cell>121.51</cell><cell>241.18</cell><cell>-</cell><cell>-</cell></row><row><cell>Block Sparse</cell><cell>2.39</cell><cell>2.40</cell><cell>3.31</cell><cell>5.02</cell><cell>12.25</cell><cell>35.94</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Longformer</cell><cell>2.36</cell><cell>2.34</cell><cell>2.38</cell><cell>2.94</cell><cell>9.83</cell><cell>21.35</cell><cell>58.12</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>BigBird</cell><cell>2.35</cell><cell>2.35</cell><cell>2.37</cell><cell>3.25</cell><cell>10.36</cell><cell>22.57</cell><cell>60.63</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>FlashAttention</cell><cell>0.32</cell><cell>0.30</cell><cell>0.83</cell><cell>2.37</cell><cell>7.95</cell><cell>30.77</cell><cell>119.98</cell><cell>473.65</cell><cell>1883.43</cell><cell>7513.01</cell></row><row><cell>Block-Sparse FlashAttention</cell><cell>0.34</cell><cell>0.34</cell><cell>0.36</cell><cell>0.69</cell><cell>1.85</cell><cell>3.89</cell><cell>7.16</cell><cell>14.85</cell><cell>30.46</cell><cell>60.03</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 14 :</head><label>14</label><figDesc>Forward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length, with dropout. Best in bold, second best underlined.</figDesc><table><row><cell>Attention Method</cell><cell>128</cell><cell>256</cell><cell>512</cell><cell>1024</cell><cell>2048</cell><cell>4096</cell><cell>8192</cell><cell>16384</cell><cell>32768</cell><cell>65536</cell></row><row><cell>PyTorch Attention</cell><cell>0.26</cell><cell>0.24</cell><cell>0.57</cell><cell>1.80</cell><cell>6.56</cell><cell>25.34</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Megatron</cell><cell>0.27</cell><cell>0.27</cell><cell>0.56</cell><cell>1.88</cell><cell>6.56</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Reformer</cell><cell>1.83</cell><cell>2.96</cell><cell>5.31</cell><cell>10.33</cell><cell>21.19</cell><cell>43.42</cell><cell>91.96</cell><cell>201.34</cell><cell>-</cell><cell>-</cell></row><row><cell>Local Attention</cell><cell>0.51</cell><cell>0.60</cell><cell>0.78</cell><cell>2.01</cell><cell>6.23</cell><cell>12.52</cell><cell>25.07</cell><cell>50.50</cell><cell>102.18</cell><cell>-</cell></row><row><cell>Linformer</cell><cell>0.47</cell><cell>0.37</cell><cell>0.49</cell><cell>0.52</cell><cell>1.37</cell><cell>2.65</cell><cell>5.12</cell><cell>10.13</cell><cell>20.25</cell><cell>44.16</cell></row><row><cell>Smyrf</cell><cell>2.12</cell><cell>2.01</cell><cell>3.15</cell><cell>5.97</cell><cell>11.83</cell><cell>23.36</cell><cell>46.48</cell><cell>92.72</cell><cell>-</cell><cell>-</cell></row><row><cell>LSformer</cell><cell>1.28</cell><cell>1.33</cell><cell>1.51</cell><cell>3.39</cell><cell>11.40</cell><cell>22.54</cell><cell>44.96</cell><cell>89.85</cell><cell>179.73</cell><cell>-</cell></row><row><cell>Block Sparse</cell><cell>1.03</cell><cell>1.00</cell><cell>1.72</cell><cell>2.39</cell><cell>5.96</cell><cell>17.88</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Longformer</cell><cell>1.02</cell><cell>1.03</cell><cell>1.03</cell><cell>1.73</cell><cell>5.10</cell><cell>11.63</cell><cell>34.22</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>BigBird</cell><cell>0.99</cell><cell>1.03</cell><cell>1.01</cell><cell>1.58</cell><cell>5.36</cell><cell>12.27</cell><cell>35.56</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>FlashAttention</cell><cell>0.10</cell><cell>0.10</cell><cell>0.22</cell><cell>0.83</cell><cell>2.81</cell><cell>10.38</cell><cell>41.63</cell><cell>167.01</cell><cell>668.74</cell><cell>2678.11</cell></row><row><cell>Block-Sparse FlashAttention</cell><cell>0.54</cell><cell>0.51</cell><cell>0.68</cell><cell>0.61</cell><cell>0.67</cell><cell>1.10</cell><cell>1.89</cell><cell>3.71</cell><cell>7.18</cell><cell>14.41</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 15 :</head><label>15</label><figDesc>Backward pass runtime (ms) of various attention mechanisms by sequence length, with dropout. Best in bold, second best underlined.</figDesc><table><row><cell>Attention Method</cell><cell>128</cell><cell>256</cell><cell>512</cell><cell>1024</cell><cell>2048</cell><cell>4096</cell><cell>8192</cell><cell>16384</cell><cell>32768</cell><cell>65536</cell></row><row><cell>PyTorch Attention</cell><cell>0.44</cell><cell>0.35</cell><cell>0.90</cell><cell>2.94</cell><cell>10.77</cell><cell>41.67</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Megatron</cell><cell>0.28</cell><cell>0.33</cell><cell>0.92</cell><cell>2.94</cell><cell>10.80</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Reformer</cell><cell>2.24</cell><cell>4.34</cell><cell>8.39</cell><cell>16.62</cell><cell>33.02</cell><cell>65.77</cell><cell>131.52</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Local Attention</cell><cell>0.51</cell><cell>0.58</cell><cell>1.41</cell><cell>3.71</cell><cell>12.96</cell><cell>25.98</cell><cell>51.94</cell><cell>103.72</cell><cell>207.78</cell><cell>-</cell></row><row><cell>Linformer</cell><cell>0.84</cell><cell>0.74</cell><cell>0.79</cell><cell>0.85</cell><cell>2.28</cell><cell>4.37</cell><cell>8.66</cell><cell>17.02</cell><cell>33.78</cell><cell>-</cell></row><row><cell>Smyrf</cell><cell>1.27</cell><cell>2.56</cell><cell>4.90</cell><cell>9.66</cell><cell>19.16</cell><cell>38.13</cell><cell>76.17</cell><cell>152.39</cell><cell>-</cell><cell>-</cell></row><row><cell>LSformer</cell><cell>1.67</cell><cell>1.77</cell><cell>3.03</cell><cell>7.52</cell><cell>20.10</cell><cell>39.13</cell><cell>76.35</cell><cell>150.83</cell><cell>-</cell><cell>-</cell></row><row><cell>Block Sparse</cell><cell>1.27</cell><cell>1.36</cell><cell>2.15</cell><cell>3.04</cell><cell>7.27</cell><cell>21.18</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Longformer</cell><cell>1.28</cell><cell>1.34</cell><cell>1.38</cell><cell>1.98</cell><cell>5.24</cell><cell>10.74</cell><cell>25.95</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>BigBird</cell><cell>1.48</cell><cell>1.47</cell><cell>1.50</cell><cell>1.81</cell><cell>5.57</cell><cell>11.38</cell><cell>27.43</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>FlashAttention</cell><cell>0.15</cell><cell>0.18</cell><cell>0.58</cell><cell>1.86</cell><cell>6.50</cell><cell>26.21</cell><cell>104.27</cell><cell>416.10</cell><cell>1661.92</cell><cell>6643.01</cell></row><row><cell>Block-Sparse FlashAttention</cell><cell>0.17</cell><cell>0.17</cell><cell>0.17</cell><cell>0.40</cell><cell>1.10</cell><cell>2.04</cell><cell>4.43</cell><cell>9.33</cell><cell>18.28</cell><cell>37.31</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 16 :</head><label>16</label><figDesc>Forward pass + backward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length, with dropout. Best in bold, second best underlined.</figDesc><table><row><cell>Attention Method</cell><cell>128</cell><cell>256</cell><cell>512</cell><cell>1024</cell><cell>2048</cell><cell>4096</cell><cell>8192</cell><cell>16384</cell><cell>32768</cell><cell>65536</cell></row><row><cell>PyTorch Attention</cell><cell>0.66</cell><cell>0.67</cell><cell>1.43</cell><cell>4.82</cell><cell>17.47</cell><cell>67.29</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Megatron</cell><cell>0.88</cell><cell>0.90</cell><cell>1.49</cell><cell>4.73</cell><cell>17.41</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Reformer</cell><cell>4.06</cell><cell>7.28</cell><cell>13.68</cell><cell>26.98</cell><cell>54.27</cell><cell>109.39</cell><cell>223.80</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Local Attention</cell><cell>1.09</cell><cell>1.40</cell><cell>1.99</cell><cell>5.61</cell><cell>19.23</cell><cell>38.62</cell><cell>77.30</cell><cell>154.63</cell><cell>311.12</cell><cell>-</cell></row><row><cell>Linformer</cell><cell>1.31</cell><cell>1.21</cell><cell>1.30</cell><cell>1.39</cell><cell>3.73</cell><cell>7.15</cell><cell>14.05</cell><cell>27.69</cell><cell>55.00</cell><cell>-</cell></row><row><cell>Smyrf</cell><cell>3.00</cell><cell>4.37</cell><cell>8.05</cell><cell>15.66</cell><cell>31.04</cell><cell>61.64</cell><cell>123.04</cell><cell>245.65</cell><cell>-</cell><cell>-</cell></row><row><cell>LSformer</cell><cell>3.07</cell><cell>3.17</cell><cell>4.31</cell><cell>10.89</cell><cell>31.54</cell><cell>61.78</cell><cell>121.56</cell><cell>240.94</cell><cell>-</cell><cell>-</cell></row><row><cell>Block Sparse</cell><cell>2.54</cell><cell>2.52</cell><cell>3.71</cell><cell>5.44</cell><cell>13.29</cell><cell>39.19</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Longformer</cell><cell>2.47</cell><cell>2.49</cell><cell>2.51</cell><cell>3.10</cell><cell>10.39</cell><cell>22.49</cell><cell>60.44</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>BigBird</cell><cell>2.51</cell><cell>2.49</cell><cell>2.52</cell><cell>3.40</cell><cell>10.97</cell><cell>23.89</cell><cell>63.28</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>FlashAttention</cell><cell>0.35</cell><cell>0.36</cell><cell>0.80</cell><cell>2.52</cell><cell>9.16</cell><cell>36.70</cell><cell>146.13</cell><cell>583.45</cell><cell>2332.01</cell><cell>9323.63</cell></row><row><cell>Block-Sparse FlashAttention</cell><cell>0.91</cell><cell>0.83</cell><cell>0.94</cell><cell>0.92</cell><cell>1.83</cell><cell>3.50</cell><cell>7.02</cell><cell>13.56</cell><cell>26.71</cell><cell>53.92</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 17 :</head><label>17</label><figDesc>Forward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length. Best in bold, second best underlined.</figDesc><table><row><cell>Attention Method</cell><cell>128</cell><cell>256</cell><cell>512</cell><cell>1024</cell><cell>2048</cell><cell>4096</cell><cell>8192</cell><cell>16384</cell><cell>32768</cell><cell>65536</cell></row><row><cell>PyTorch Attention</cell><cell>0.21</cell><cell>0.22</cell><cell>0.43</cell><cell>1.27</cell><cell>4.32</cell><cell>16.47</cell><cell>67.77</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Megatron</cell><cell>0.24</cell><cell>0.26</cell><cell>0.42</cell><cell>1.33</cell><cell>4.28</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Reformer</cell><cell>1.77</cell><cell>2.82</cell><cell>5.01</cell><cell>9.74</cell><cell>20.03</cell><cell>41.11</cell><cell>87.39</cell><cell>192.40</cell><cell>-</cell><cell>-</cell></row><row><cell>Local Attention</cell><cell>0.48</cell><cell>0.57</cell><cell>0.80</cell><cell>1.90</cell><cell>5.76</cell><cell>11.56</cell><cell>23.13</cell><cell>46.65</cell><cell>94.74</cell><cell>-</cell></row><row><cell>Linformer</cell><cell>0.46</cell><cell>0.36</cell><cell>0.45</cell><cell>0.50</cell><cell>1.09</cell><cell>2.09</cell><cell>4.01</cell><cell>7.90</cell><cell>15.70</cell><cell>35.40</cell></row><row><cell>Smyrf</cell><cell>1.94</cell><cell>1.96</cell><cell>3.01</cell><cell>5.69</cell><cell>11.26</cell><cell>22.23</cell><cell>44.21</cell><cell>88.22</cell><cell>-</cell><cell>-</cell></row><row><cell>LSformer</cell><cell>1.21</cell><cell>1.34</cell><cell>1.34</cell><cell>3.31</cell><cell>11.01</cell><cell>21.71</cell><cell>43.27</cell><cell>86.32</cell><cell>172.85</cell><cell>-</cell></row><row><cell>Block Sparse</cell><cell>0.96</cell><cell>1.04</cell><cell>1.66</cell><cell>2.16</cell><cell>5.41</cell><cell>16.15</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Longformer</cell><cell>0.99</cell><cell>0.98</cell><cell>0.99</cell><cell>1.56</cell><cell>4.79</cell><cell>11.07</cell><cell>32.98</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>BigBird</cell><cell>0.96</cell><cell>1.02</cell><cell>1.02</cell><cell>1.48</cell><cell>5.05</cell><cell>11.59</cell><cell>34.16</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>FlashAttention</cell><cell>0.08</cell><cell>0.09</cell><cell>0.18</cell><cell>0.68</cell><cell>2.40</cell><cell>8.42</cell><cell>33.54</cell><cell>134.03</cell><cell>535.95</cell><cell>2147.05</cell></row><row><cell>Block-Sparse FlashAttention</cell><cell>0.56</cell><cell>0.52</cell><cell>0.63</cell><cell>0.65</cell><cell>0.61</cell><cell>0.96</cell><cell>1.69</cell><cell>3.02</cell><cell>5.69</cell><cell>11.77</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 18 :</head><label>18</label><figDesc>Backward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length. Best in bold, second best underlined.</figDesc><table><row><cell>Attention Method</cell><cell>128</cell><cell>256</cell><cell>512</cell><cell>1024</cell><cell>2048</cell><cell>4096</cell><cell>8192</cell><cell>16384</cell><cell>32768</cell><cell>65536</cell></row><row><cell>PyTorch Attention</cell><cell>0.26</cell><cell>0.29</cell><cell>0.78</cell><cell>2.44</cell><cell>8.82</cell><cell>33.87</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Megatron</cell><cell>0.29</cell><cell>0.30</cell><cell>0.80</cell><cell>2.59</cell><cell>8.86</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Reformer</cell><cell>2.18</cell><cell>4.21</cell><cell>8.14</cell><cell>16.12</cell><cell>32.02</cell><cell>63.84</cell><cell>127.60</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Local Attention</cell><cell>0.51</cell><cell>0.64</cell><cell>1.28</cell><cell>3.60</cell><cell>12.52</cell><cell>25.08</cell><cell>50.22</cell><cell>100.23</cell><cell>200.66</cell><cell>-</cell></row><row><cell>Linformer</cell><cell>0.69</cell><cell>0.76</cell><cell>0.69</cell><cell>0.80</cell><cell>2.04</cell><cell>3.88</cell><cell>7.67</cell><cell>15.04</cell><cell>30.11</cell><cell>63.15</cell></row><row><cell>Smyrf</cell><cell>1.24</cell><cell>2.49</cell><cell>4.77</cell><cell>9.42</cell><cell>18.65</cell><cell>37.12</cell><cell>74.15</cell><cell>148.35</cell><cell>-</cell><cell>-</cell></row><row><cell>LSformer</cell><cell>1.68</cell><cell>1.61</cell><cell>3.02</cell><cell>7.40</cell><cell>19.72</cell><cell>38.27</cell><cell>74.89</cell><cell>147.99</cell><cell>-</cell><cell>-</cell></row><row><cell>Block Sparse</cell><cell>1.24</cell><cell>1.25</cell><cell>2.04</cell><cell>2.91</cell><cell>6.78</cell><cell>19.67</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Longformer</cell><cell>1.27</cell><cell>1.23</cell><cell>1.24</cell><cell>1.85</cell><cell>4.99</cell><cell>10.21</cell><cell>24.89</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>BigBird</cell><cell>1.43</cell><cell>1.50</cell><cell>1.44</cell><cell>1.69</cell><cell>5.25</cell><cell>10.86</cell><cell>26.26</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>FlashAttention</cell><cell>0.11</cell><cell>0.16</cell><cell>0.52</cell><cell>1.62</cell><cell>5.45</cell><cell>21.57</cell><cell>84.75</cell><cell>336.00</cell><cell>1338.56</cell><cell>5343.19</cell></row><row><cell>Block-Sparse FlashAttention</cell><cell>0.11</cell><cell>0.12</cell><cell>0.16</cell><cell>0.38</cell><cell>1.20</cell><cell>2.34</cell><cell>4.69</cell><cell>9.10</cell><cell>18.74</cell><cell>37.04</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>Table 19 :</head><label>19</label><figDesc>Forward pass + backward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length. Best in bold, second best underlined.</figDesc><table><row><cell>Attention Method</cell><cell>128</cell><cell>256</cell><cell>512</cell><cell>1024</cell><cell>2048</cell><cell>4096</cell><cell>8192</cell><cell>16384</cell><cell>32768</cell><cell>65536</cell></row><row><cell>PyTorch Attention</cell><cell>0.67</cell><cell>0.70</cell><cell>1.18</cell><cell>3.67</cell><cell>13.22</cell><cell>50.44</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Megatron</cell><cell>0.74</cell><cell>0.65</cell><cell>1.23</cell><cell>3.80</cell><cell>13.21</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Reformer</cell><cell>3.93</cell><cell>7.01</cell><cell>13.15</cell><cell>25.89</cell><cell>52.09</cell><cell>105.00</cell><cell>215.13</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Local Attention</cell><cell>1.09</cell><cell>1.27</cell><cell>1.99</cell><cell>5.38</cell><cell>18.32</cell><cell>36.77</cell><cell>73.67</cell><cell>147.29</cell><cell>296.35</cell><cell>-</cell></row><row><cell>Linformer</cell><cell>1.31</cell><cell>1.25</cell><cell>1.30</cell><cell>1.29</cell><cell>3.20</cell><cell>6.10</cell><cell>11.93</cell><cell>23.39</cell><cell>46.72</cell><cell>100.52</cell></row><row><cell>Smyrf</cell><cell>2.98</cell><cell>4.23</cell><cell>7.78</cell><cell>15.12</cell><cell>29.96</cell><cell>59.45</cell><cell>118.60</cell><cell>237.02</cell><cell>-</cell><cell>-</cell></row><row><cell>LSformer</cell><cell>3.03</cell><cell>3.05</cell><cell>4.26</cell><cell>10.70</cell><cell>30.77</cell><cell>60.15</cell><cell>118.33</cell><cell>234.94</cell><cell>-</cell><cell>-</cell></row><row><cell>Block Sparse</cell><cell>2.39</cell><cell>2.40</cell><cell>3.31</cell><cell>5.02</cell><cell>12.25</cell><cell>35.94</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Longformer</cell><cell>2.36</cell><cell>2.34</cell><cell>2.38</cell><cell>2.94</cell><cell>9.83</cell><cell>21.35</cell><cell>58.12</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>BigBird</cell><cell>2.35</cell><cell>2.35</cell><cell>2.37</cell><cell>3.25</cell><cell>10.36</cell><cell>22.57</cell><cell>60.63</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>FlashAttention</cell><cell>0.31</cell><cell>0.31</cell><cell>0.73</cell><cell>2.29</cell><cell>7.64</cell><cell>30.09</cell><cell>118.50</cell><cell>470.51</cell><cell>1876.08</cell><cell>7492.85</cell></row><row><cell>Block-Sparse FlashAttention</cell><cell>0.74</cell><cell>0.77</cell><cell>0.82</cell><cell>0.88</cell><cell>1.71</cell><cell>3.21</cell><cell>6.56</cell><cell>12.60</cell><cell>24.93</cell><cell>50.39</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Table 20 :</head><label>20</label><figDesc>Memory usage (MB) of various exact/approximate/sparse attention mechanisms by sequence length. Best in bold, second best underlined.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>FlashAttention code is available at https://github.com/HazyResearch/flash-attention</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>This style of aggregation is called algebraic aggregation<ref type="bibr" target="#b30">[31]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>LRA accuracy results are known to be highly dependent on the tuning procedure<ref type="bibr" target="#b86">[87]</ref>. Our reproduced baselines perform better than as reported in the original comparison<ref type="bibr" target="#b76">[77]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Path-256 requires longer sequences but has relatively shorter paths than Path-X, so it is easier to obtain a higher accuracy.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>Our implementation uses Apex's FMHA code (https://github.com/NVIDIA/apex/tree/master/apex/ contrib/csrc/fmha) as a starting point. We thank <rs type="person">Young-Jun Ko</rs> for the in-depth explanation of his FMHA implementation and for his thoughtful answers to our questions about CUDA. We thank <rs type="person">Sabri Eyuboglu</rs>, <rs type="person">Megan Leszczynski</rs>, <rs type="person">Laurel Orr</rs>, <rs type="person">Yuhuai Wu</rs>, <rs type="person">Beidi Chen</rs>, and <rs type="person">Xun Huang</rs> for their helpful discussions and feedback on early drafts of the paper.</p><p>We gratefully acknowledge the support of <rs type="funder">NIH</rs> under No. <rs type="grantNumber">U54EB020405</rs> (Mobilize), <rs type="funder">NSF</rs> under Nos. <rs type="grantNumber">CCF1763315</rs> (<rs type="affiliation">Beyond Sparsity</rs>), CCF1563078 (Volume to Velocity), and 1937301 (RTML); ARL under No. <rs type="grantNumber">W911NF-21-2-0251</rs> (<rs type="funder">Interactive Human-AI Teaming)</rs>; <rs type="funder">ONR</rs> under No. <rs type="grantNumber">N000141712266</rs> (<rs type="affiliation">Unifying Weak Supervision</rs>); <rs type="grantNumber">ONR N00014-20-1-2480</rs>: Understanding and Applying Non-Euclidean Geometry in <rs type="person">Machine Learning</rs>; <rs type="grantNumber">N000142012275</rs> (NEPTUNE); NXP, <rs type="funder">Xilinx</rs>, <rs type="affiliation">LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF</rs>, <rs type="funder">Accenture</rs>, <rs type="funder">Ericsson</rs>, <rs type="funder">Qualcomm</rs>, <rs type="funder">Analog Devices</rs>, <rs type="person">Google Cloud</rs>, <rs type="funder">Salesforce</rs>, <rs type="funder">Total</rs>, the <rs type="institution">HAI-GCP &amp; HAI-Azure Cloud Credits for Research program</rs>, the <rs type="funder">Stanford Data Science Initiative (SDSI)</rs>, <rs type="funder">Department of Defense (DoD)</rs> through the <rs type="funder">National Defense Science and Engineering Graduate Fellowship (NDSEG) Program</rs>, and members of the <rs type="institution">Stanford DAWN</rs> project: <rs type="institution">Facebook, Google</rs>, and VMWare. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_ksVhqZB">
					<idno type="grant-number">U54EB020405</idno>
				</org>
				<org type="funding" xml:id="_DXYYyAZ">
					<idno type="grant-number">CCF1763315</idno>
				</org>
				<org type="funding" xml:id="_tCdPFkp">
					<idno type="grant-number">W911NF-21-2-0251</idno>
				</org>
				<org type="funding" xml:id="_We9EVW4">
					<idno type="grant-number">N000141712266</idno>
				</org>
				<org type="funding" xml:id="_ufPPWWS">
					<idno type="grant-number">ONR N00014-20-1-2480</idno>
				</org>
				<org type="funding" xml:id="_uQtRHp9">
					<idno type="grant-number">N000142012275</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>Load K ? , V ? from HBM to on-chip SRAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>Initialize dK ? = (0) ? ? ?? , dV ? = (0) ? ? ?? on SRAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>for 1 ? ? ? ? ? do 10:</p><p>Load Q ? , O ? , dO ? , dQ ? , ? ? , ? ? from HBM to on-chip SRAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><p>On chip, compute S ? ? = ?Q ? K ? ? ? R ? ? ?? ? .</p><p>12:</p><p>On chip, compute S masked</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? ?</head><p>= mask(S ? ? ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>13:</head><p>On chip, compute P ? ? = diag(? ? ) -1 exp(S masked ? ? -? ? ) ? R ? ? ?? ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>14:</head><p>On chip, compute dropout mask Z ? ? ? R ? ? ?? ? where each entry has value 1 1-? drop with probability 1 -? drop and value 0 with probability ? drop .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>15:</head><p>On chip, compute P dropped ? ? = P ? ? ? Z ? ? (pointwise multiply).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>16:</head><p>On chip, compute d V ? ? d V ? + (P dropped</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? ?</head><p>) dO ? ? R ? ? ?? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>17:</head><p>On chip, compute dP dropped</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? ?</head><p>= dO ? V ? ? R ? ? ?? ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>18:</head><p>On chip, compute dP ? ? = dP dropped ? ?</p><p>? Z ? ? (pointwise multiply).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>19:</head><p>On chip, compute</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>20:</head><p>On chip, compute dS ? ? = P ? ? ? (dP ? ? -? ? ) ? R ? ? ?? ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>21:</head><p>Write dQ ? ? dQ ? + ?dS ? ? K ? ? R ? ? ?? to HBM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>22:</head><p>On chip, compute dK ? ? dK ? + ?dS ? ? Q ? ? R ? ? ?? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>23:</head><p>end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>24:</head><p>Write dK ? ? d K ? , dV ? ? d V ? to HBM. 25: end for 26: Return dQ, dK, dV.</p><p>We see that similar to the forward pass, the backward pass performs ? (? 2 ) FLOPs and only requires ? (?) extra memory beyond inputs, output, output gradient, and input gradients.</p><p>We analyze the IO-complexity of the backward pass, similar to the forward pass (Theorem 2).</p><p>Theorem 5. Let ? be the sequence length, ? be the head dimension, and ? be size of SRAM with ? ? ? ? ? ?.</p><p>Standard attention (Algorithm 0) backward pass requires ?(? ? + ? 2 ) HBM accesses, while FlashAttention backward pass (Algorithm 4) requires ?(? 2 ? 2 ? -1 ) HBM accesses.</p><p>The proof is in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Proofs</head><p>Proof of Theorem 1. We first count the number of FLOPs and extra memory required. The dominating FLOPs are from matrix multiplication. In the inner loop, (Algorithm 1 line 9), we compute Q ? K ? ? R ? ? ?? ? for Q ? ? R ? ? ?? and K ? ? R ? ? ?? , which takes ? (? ? ? ? ?) FLOPs. We also compute (Algorithm 1 line 12) P? ? V ? ? R ? ? ?? for P? ? ? R ? ? ?? ? and V ? ? R ? ? ?? , which takes ? (? ? ? ? ?) FLOPs. We execute the inner loops ? ? ? ? = ? ? ? ? ? ? times. Therefore the total number of FLOPs is</p><p>In terms of extra memory required, we see that we need ? (?) memory to store the statistics (?, ?). We now prove the algorithm's correctness by induction on ? for 0 ? ? ? ? ? . Let K : ? ? R ? ? ? ?? be the first ? ? ? rows of K, and similarly V : ? ? R ? ? ? ?? the the first ? ? ? rows of V. Let S :,: ? = QK : ? ? R ? ? ? ? ? , and P :,: ? = softmax(S :,: ? ) ? R ? ? ? ? ? (softmax applied row-wise). Let ? ? , ? ( ?) , O ( ?) be the values of ?, ?, O in HBM after the ?-th iteration of the outer loop (Algorithm 1 line 5). (Note that these values of ?, ?, O updated after each iteration of the outer loop.) We want to show that after the ?-th iteration of the outer loop, we have computed in HBM:</p><p>Based on our initialization (Algorithm 1 line 2), this claim is true for ? = 0 (i.e., before the any iteration of the outer loop is executed). Suppose that the claim holds for some ? = 0, . . . , ? ? -1. We want to show that the claim also holds for ? + 1. Indeed, when we update the statistics in the inner loop (Algorithm 1 line 10) on the ( ? + 1)-th iteration of the outer loop, we update ? ( ?+1) = max(? ( ?) , m) where m ? R ? is the row-max of S :, ?: ?+1 , the slice of S from column ? ? ? to column ( ? + 1)? ? -1. This implies that</p><p>Similarly, we update</p><p>? ( ?) + ? m-? ( ?+1) l, where l = rowsum(exp(S :, ?: ?+1m)) ? R ? . By the same algebraic manipulation in Section 3.1, we obtain:</p><p>Let V ?: ?+1 be the slice of V from column ? ? ? to column ( ? + 1)? ? -1, we also update:</p><p>V : ? V ?: ?+1</p><p>= softmax(S : ?+1 )V : ?+1 .</p><p>We then see that the claim is also true for ? + 1. By induction, the claim is true for all ? = 0, . . . , ? ? . When ? = ? ? , we conclude that the final value of O in HBM is softmax(S)V = softmax(QK )V.</p><p>Proof of Theorem 2. We first analyze the IO complexity of standard attention implementation. The inputs Q, K, V ? R ? ?? reside in HBM, and the at the end of the algorithm the output O ? R ? ?? is written to HBM.</p><p>After hyperparameter tuning, almost all of the attention methods achieve similar accuracy on all of the five LRA tasks.</p><p>We run all methods with mixed-precision training, except for Performer (not stable with mixed precision) and Local Attention (implementation does not support FP16).</p><p>To calculate the overall wallclock-time speedup, we take the geometric mean of the wallclock-time speedup of each of the five tasks.</p><p>Path-X For Path-X and Path-256, we follow the hyperparameters from the PathFinder-32 experiments from the long-range arena paper <ref type="bibr" target="#b76">[77]</ref>. For both, we first pretrain a model on Path-64. We take the checkpoint after 200 epochs, upsample its positional embedding (we duplicate the positional embeddings gridwise in space), and fine-tune it on the downstream task for 200 epochs with one epoch of linear warmup, and cosine decay of the learning rate. For Path-X, we take the best performing checkpoint (according to val accuracy), and additionally fine-tune it for 200 epochs with the same warmup and learning rate (this adds roughly 4 points of accuracy to FlashAttention for Path-X, but the model starts overfitting afterwards).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.5 Full Benchmarking Results</head><p>We report the full benchmarking results and experimental details.</p><p>Baselines We compare against reference implementations for exact attention from PyTorch/HuggingFace and Megatron, approximate attention, and sparse attention. For approximate attention, we compare against reference implementations of Reformer <ref type="bibr" target="#b48">[49]</ref>, Local Attention <ref type="bibr" target="#b64">[65]</ref>, Linformer Attention <ref type="bibr" target="#b80">[81]</ref>, Smyrf <ref type="bibr" target="#b17">[18]</ref>, and LongShortFormer (LSFormer) <ref type="bibr" target="#b90">[91]</ref>. For sparse attention, we compare against reference implementations of Block-Sparse Attention form OpenAI <ref type="bibr" target="#b9">[10]</ref>, Longformer <ref type="bibr" target="#b2">[3]</ref>, and BigBird Attention <ref type="bibr" target="#b88">[89]</ref>. For the approximate and sparse attention, we use a compression ratio of 1/8, or a compressed sequence length of 256, whichever is smaller.</p><p>Setup We measure runtime and memory usage of the attention computation with 8 heads of dimension 64, and batch size 128 on a machine with one A100 GPU with 40 GB of GPU HBM. We vary sequence length in our experiments. We compute attention on random vectors for Q, K, and V (we do not measure the projection from the hidden layer). For dropout, we use dropout 0.1; for masking, we use a padding mask with uniformly-random mask lengths between the total sequence length and the total sequence length minus 20. To measure runtime, we take the average of 100 measurements of the attention call. We only measure memory footprint once, since it does not vary between runs.</p><p>We report timing results on the forward pass, backward pass, and combined forward + backward pass. We measure each method with and without dropout, masking, or both-except for Block Sparse, Longformer, and BigBird. These methods did not successfully run the backward pass with masking due to a bug in external libraries, so we measured them without masking to be generous. We use FP16 for all measurements, except for Local Attention, whose implementation only supports FP32.</p><p>For each baseline, we increase sequence length until it runs out of memory on the GPU, except for the following exceptions: The Megatron implementation does not support sequence lengths longer than 2048. Block-Sparse (OpenAI) does not support sequence lengths longer than 4096. Longformer and BigBird do not support sequence lengths longer than 8092.</p><p>We measure memory usage on the combined forward + backward pass, without dropout or masking.</p><p>Results Table <ref type="table">7</ref> summarizes all the experimental configurations and contains pointers to the results tables. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The input/output complexity of sorting and related problems</title>
		<author>
			<persName><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Vitter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1116" to="1127" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName><surname>Lambdanetworks</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.08602</idno>
		<title level="m">Modeling long-range interactions without attention</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><surname>Longformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<title level="m">The long-document transformer</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An updated set of basic linear algebra subprograms (blas)</title>
		<author>
			<persName><forename type="first">Susan</forename><surname>Blackford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Petitet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roldan</forename><surname>Pozo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karin</forename><surname>Remington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clint</forename><surname>Whaley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Dongarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Duff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Hammarling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Henry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Mathematical Software</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="135" to="151" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural legal judgment prediction in English</title>
		<author>
			<persName><forename type="first">Ilias</forename><surname>Chalkidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Aletras</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1424</idno>
		<ptr target="https://www.aclweb.org/anthology/P19-1424" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4317" to="4323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Nikolaos Aletras, Ion Androutsopoulos, and Prodromos Malakasiotis. Paragraph-level rationale extraction through regularization: A case study on european court of human rights cases</title>
		<author>
			<persName><forename type="first">Ilias</forename><surname>Chalkidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manos</forename><surname>Fergadiotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Tsarapatsanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the Annual Conference of the North American Chapter of the Association for Computational Linguistics<address><addrLine>Mexico City, Mexico</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scatterbrain: Unifying sparse and low-rank attention</title>
		<author>
			<persName><forename type="first">Beidi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Winsor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Training deep nets with sublinear memory cost</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.06174</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rethinking attention with performers</title>
		<author>
			<persName><forename type="first">Valerii</forename><surname>Krzysztof Marcin Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyou</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreea</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamas</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">Quincy</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afroz</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Revisiting transformer-based models for long document classification</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilias</forename><surname>Chalkidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sune</forename><surname>Darkner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06683</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Transformer-XL: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2978" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning fast algorithms for linear transforms using butterfly factorizations</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Eichhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Kaleidoscope: An efficient, learnable representation for all structured linear maps</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nimit</forename><surname>Sohoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Eichhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Blonder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Megan</forename><surname>Leszczynski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pixelated butterfly: Simple and efficient sparse training for neural network models</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beidi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaizhao</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Monarch: Expressive structured matrices for efficient and accurate training</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beidi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nimit</forename><surname>Sohoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Poli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Grogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniruddh</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Smyrf-efficient attention using asymmetric clustering</title>
		<author>
			<persName><forename type="first">Giannis</forename><surname>Daras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6476" to="6489" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A two-pronged progress in structured dense matrix vector multiplication</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>De Sa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Puttagunta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms</title>
		<meeting>the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1060" to="1079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The working set model for program behavior</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><surname>Denning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="323" to="333" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning to prune deep neural networks via layer-wise optimal brain surgeon</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinno</forename><surname>Jialin Pan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07565</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On a new class of structured matrices</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Eidelman</surname></persName>
		</author>
		<author>
			<persName><surname>Gohberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Integral Equations and Operator Theory</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="293" to="324" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">J?rg</forename><surname>Flum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Grohe</surname></persName>
		</author>
		<title level="m">Parameterized Complexity Theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The lottery ticket hypothesis: Finding sparse, trainable neural networks</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Carbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gintare</forename><surname>Karolina Dziugaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Carbin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.01611</idno>
		<title level="m">Stabilizing the lottery ticket hypothesis</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Linear mode connectivity and the lottery ticket hypothesis</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karolina</forename><surname>Gintare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Dziugaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><surname>Carbin</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3259" to="3269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">It&apos;s raw! audio generation with state-space models</title>
		<author>
			<persName><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Gokaslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vanya</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavlick</forename><surname>Ellie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Tellex</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Openwebtext corpus</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Data cube: A relational aggregation operator generalizing group-by, cross-tab, and sub-totals</title>
		<author>
			<persName><forename type="first">Jim</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surajit</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Bosworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Layman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Don</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murali</forename><surname>Venkatrao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Pellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamid</forename><surname>Pirahesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data mining and knowledge discovery</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="53" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Evaluating derivatives: principles and techniques of algorithmic differentiation</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Griewank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Walther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hippo: Recurrent memory with optimal polynomial projections</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Combining recurrent, convolutional, and continuous-time models with linear state space layers</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isys</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khaled</forename><surname>Saab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Efficiently modeling long sequences with structured state spaces</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02626</idno>
		<title level="m">Learning both weights and connections for efficient neural networks</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Memory hierarchy design. Computer Architecture: A Quantitative Approach</title>
		<author>
			<persName><forename type="first">John</forename><surname>Hennessy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Patterson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="390" to="525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Sara</forename><surname>Hooker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06489</idno>
		<title level="m">The hardware lottery</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.10447</idno>
		<title level="m">Transformer quality in linear time</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Data movement is all you need: A case study on optimizing transformers</title>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikoli</forename><surname>Dryden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shigang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Hoefler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="711" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Dissecting the Ampere GPU architecture via microbenchmarking</title>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Van Sandt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
		<respStmt>
			<orgName>GPU Technology Conference</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Maggioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Staiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniele</forename><forename type="middle">P</forename><surname>Scarpazza</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06826</idno>
		<title level="m">Dissecting the nvidia Volta GPU architecture via microbenchmarking</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><surname>Tillman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Maggioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniele</forename><forename type="middle">Paolo</forename><surname>Scarpazza</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.03413</idno>
		<title level="m">Dissecting the graphcore IPU architecture via microbenchmarking</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Alistair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Wei H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengling</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><forename type="middle">G</forename><surname>Anthony Celi</surname></persName>
		</author>
		<author>
			<persName><surname>Mark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mimic-iii, a freely accessible critical care database. Scientific data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">In-datacenter performance analysis of a tensor processing unit</title>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Norman P Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raminder</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName><surname>Borchers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th annual international symposium on computer architecture</title>
		<meeting>the 44th annual international symposium on computer architecture</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Displacement ranks of matrices and linear equations</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Kailath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sun-Yuan</forename><surname>Kung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Morf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="395" to="407" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Transformers are RNNs: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName><forename type="first">Angelos</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fran?ois</forename><surname>Fleuret</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5156" to="5165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Albert: A lite BEDRT for self-supervised learning of language representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The deep learning compiler: A comprehensive survey</title>
		<author>
			<persName><forename type="first">Mingzhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingxiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongzhi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangwen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Depei</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="708" to="727" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Sub-linear memory: How to make performers slim</title>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Valerii Likhosherstov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyou</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><surname>Weller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.11346</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Runtime neural pruning</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou ; In</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Linear unified nested attention</title>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><surname>Luna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Mlperf training benchmark</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Mattson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cody</forename><surname>Coleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paulius</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanlin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><surname>Gu-Yeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Bailis</surname></persName>
		</author>
		<author>
			<persName><surname>Bittorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="336" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Scalability! but at what {COST}?</title>
		<author>
			<persName><forename type="first">Frank</forename><surname>Mcsherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th Workshop on Hot Topics in Operating Systems (HotOS XV)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<title level="m">Tesla V100 GPU architecture</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Nvidia A100 tensor core GPU architecture</title>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Nvidia H100 tensor core GPU architecture</title>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Random butterfly transformations with applications in computational linear algebra</title>
		<author>
			<persName><forename type="first">Parker</forename><surname>Stott</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Self-attention does not need ? (? 2 ) memory</title>
		<author>
			<persName><forename type="first">N</forename><surname>Markus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Rabe</surname></persName>
		</author>
		<author>
			<persName><surname>Staats</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.05682</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Do transformers need deep long-range memory?</title>
		<author>
			<persName><forename type="first">Jack</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.672" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07">July 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Compressive transformers for long-range sequence modelling</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Jack W Rae</surname></persName>
		</author>
		<author>
			<persName><surname>Potapenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fr?do</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saman</forename><surname>Amarasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Sigplan Notices</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="519" to="530" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Database management systems</title>
		<author>
			<persName><forename type="first">Raghu</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Gehrke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>McGraw-Hill</publisher>
			<biblScope unit="volume">3</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Parallel stochastic gradient algorithms for large-scale matrix completion</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming Computation</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="226" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Combiner: Full attention transformer with sparse computation cost</title>
		<author>
			<persName><forename type="first">Hanjun</forename><surname>Hongyu Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengjiao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Efficient content-based sparse attention with routing transformers</title>
		<author>
			<persName><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="53" to="68" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">XLA: Compiling machine learning for peak performance</title>
		<author>
			<persName><forename type="first">Amit</forename><surname>Sabne</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.07683</idno>
		<title level="m">Movement pruning: Adaptive sparsity by fine-tuning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08053</idno>
		<title level="m">Megatron-LM: Training multi-billion parameter language models using model parallelism</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Structured transforms for small-footprint deep learning</title>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3088" to="3096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Adaptive attention span in transformers</title>
		<author>
			<persName><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Long range arena: A benchmark for efficient transformers</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06732</idno>
		<title level="m">Efficient transformers: A survey</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Deepnet: Scaling transformers to 1,000 layers</title>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.00555</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Belinda</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Roofline: an insightful visual performance model for multicore architectures</title>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Waterman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Patterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="65" to="76" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">A data locality optimizing algorithm</title>
		<author>
			<persName><forename type="first">E</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monica</forename><forename type="middle">S</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGPLAN 1991 conference on Programming language design and implementation</title>
		<meeting>the ACM SIGPLAN 1991 conference on Programming language design and implementation</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="30" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.emnlp-demos.6" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-10">October 2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Optimal space lower bounds for all frequency moments</title>
		<author>
			<persName><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><surname>Woodruff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SODA</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="167" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Pay less attention with lightweight and dynamic convolutions</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Nystr?mformer: A nyst?m-based algorithm for approximating self-attention</title>
		<author>
			<persName><forename type="first">Yunyang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanpeng</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rudrasis</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Glenn</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence. AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence. AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">14138</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi-Hang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><forename type="middle">Eh</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="558" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avinava</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifan</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">An attention free transformer</title>
		<author>
			<persName><forename type="first">Shuangfei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><surname>Talbott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanlin</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruixiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Susskind</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.14103</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Long-short transformer: Efficient transformers for language and vision</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaowei</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
