<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GCD 2 : A Globally Optimizing Compiler for Mapping DNNs to Mobile DSPs</title>
				<funder>
					<orgName type="full">Army Research Office, or Department of Defense</orgName>
				</funder>
				<funder ref="#_jJHQDpN #_xeNGgeg #_n9R9NhW #_n9pYWCY #_jjKEWhH #_Ua7ADHU">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_DHvJ7NA">
					<orgName type="full">Army Research Office/Army Research Laboratory</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wei</forename><surname>Niu</surname></persName>
							<email>wniu@wm.edu</email>
						</author>
						<author>
							<persName><forename type="first">Yanzhi</forename><surname>Wang</surname></persName>
							<email>yanz.wang@northeastern.edu</email>
						</author>
						<author>
							<persName><forename type="first">Jiexiong</forename><surname>Guan</surname></persName>
							<email>jguan@wm.edu</email>
						</author>
						<author>
							<persName><forename type="first">Gagan</forename><surname>Agrawal</surname></persName>
							<email>gagrawal@augusta.edu</email>
						</author>
						<author>
							<persName><forename type="first">Xipeng</forename><surname>Shen</surname></persName>
							<email>xshen5@ncsu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Bin</forename><surname>Ren</surname></persName>
							<email>bren@wm.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">William &amp; Mary</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">William &amp; Mary</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Augusta University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">North Carolina State University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">William &amp; Mary</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<address>
									<addrLine>EfficientNet-b0 [33] 0.4G 53 11.3 9.1 10.7 1.6 1.0 ResNet [34] 4.1G 62 34.4 13.9 6.2 2.3 1.0 PixOr [35] 8.8G 280 64.6 43 6.7 1.8 1.0 CycleGAN [36] 186G 4320 477 450 5.5 1.2 1.0</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GCD 2 : A Globally Optimizing Compiler for Mapping DNNs to Mobile DSPs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/MICRO56248.2022.00044</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>VLIW instruction packing</term>
					<term>compiler optimization</term>
					<term>deep neural network</term>
					<term>mobile devices</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>More specialized chips are exploiting available high transistor density to expose parallelism at a large scale with more intricate instruction sets. This paper reports on a compilation system GCD 2 , developed to support complex Deep Neural Network (DNN) workloads on mobile DSP chips. We observe several challenges in fully exploiting this architecture, related to SIMD width, more complex SIMD/vector instructions, and VLIW pipeline with the notion of soft dependencies. GCD 2 comprises the following contributions: 1) development of matrix layout formats that support the use of different novel SIMD instructions, 2) formulation and solution of a global optimization problem related to choosing the best instruction (and associated layout) for implementation of each operator in a complete DNN, and 3) SDA, an algorithm for packing instructions with consideration for soft dependencies. These solutions are incorporated in a complete compilation system that is extensively evaluated against other systems using 10 large DNN models. Evaluation results show that GCD 2 outperforms two productlevel state-of-the-art end-to-end DNN execution frameworks (TFLite and Qualcomm SNPE) that support mobile DSPs by up to 6.0? speedup, and outperforms three established compilers (Halide, TVM, and RAKE) by up to 4.5 ?, 3.4 ?, and 4.0 ? speedup, respectively. GCD 2 is also unique in supporting real-time execution of certain DNNs, while its implementation enables two major DNNs to execute on a mobile DSP for the first time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Despite the upcoming end of Moore's law, the last several years have seen a quick increase in transistors density. For example, in going from 22 nm technology to 10 nm, Intel chips saw a nearly 7? increase in transistor density, and the most chip manufacturers are building chips with more than 100 million transistor per square millimeter at the time of writing this paper 1 . All processors, but more particularly the specialized ones, have exploited this density by supporting an increasing amount of parallelism, often combined with intricate ways in which this parallelism can be exploited. Even in mainstream processors, the SIMD width has increased 1 https://www.techcenturion.com/7nm-10nm-14nm-fabrication and the flexibility of programming API has improved with AVX-512 instruction set that has features like scatter, gather, and masks.</p><p>An example of a class of specialized chips that offer a programming interface suited for general purpose processing is the Digital Signal Processing (DSP) chips. Particularly, smartphones have invested in sophisticated DSP chips that are also capable of accelerating other highly parallel workloads. To date, however, there is only a limited exploration on the use of DSP chips for other workloads <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>.</p><p>In recent years, machine learning (ML) or deep learning (DL) workloads, particularly the Deep Neural Networks (DNNs), have emerged as important workloads that have been targeted on a range of hardware -from mainstream processors and accelerators <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> to mobile devices <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref> (including mobile DSP <ref type="bibr" target="#b23">[24]</ref>) to chips specifically designed for them <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>. A particular requirement, and the driver of our work, is performing inference using complex Deep Neural Network (DNN) models on mobile phones in a time, memory, and power-efficient manner. We observe that DSP chips are a natural candidate for accelerating DNN inference in a mobile setting, not only because mobile phone already have a DSP chip, but also because these chips are optimized for matrix and vector computations on fixed-point values.</p><p>This paper reports a compilation system that optimizes Deep Neural Networks (DNNs) for execution on a mobile DSP chip. As a quick motivation for this effort, results from Table <ref type="table">I</ref> show that with an existing framework, TFLite <ref type="bibr" target="#b13">[14]</ref>, execution on a DSP chip outperforms both mobile CPU and GPU in terms of execution time and power. Conceptually, however, it also turns out that compiling for the DSP chip involves dealing with many advanced features, especially with respect to low-level parallelism exposed through its instruction set, requiring techniques well beyond the ones implemented in current systems or otherwise developed. More specifically, modern (mobile) DSP chips have much more complex SIMD instruction sets with both a larger Table <ref type="table">I</ref>: Latency and Power Comparisons among Mobile CPU, GPU, and DSP. Experiments are conducted on a Samsung Galaxy S20 with TFLite <ref type="bibr" target="#b13">[14]</ref>. CPU, GPU, and DSP uses int8, float16, and int8, respectively. Power is collected by the Android system interface. Results are for each inference.</p><p>width and a greater variety of instructions as compared to the mainstream processors, and thus require techniques beyond those explored in current literature <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>. Besides 1024-bit width, there are instructions combining vector operations and reductions in different ways, going even beyond Intel's additions under VNNIW and FMAPS extentions <ref type="bibr" target="#b31">[32]</ref>. In addition, VLIW instructions exist that can combine multiple SIMD instructions for simultaneous execution, and there are other performance characteristics that require new methods for effective mapping of the workload.</p><p>This paper develops techniques for exploiting these architectural features. Our contributions include:</p><p>? Methods for Exploiting Disparate SIMD Instructions.</p><p>We develop data layouts and execution schemes that use different new instructions for key Deep Learning (DL) kernels. We also investigate the trade-offs between different approaches depending upon the size of the operands. ? Formulating and Solving a Global Optimization Problem. We show how the choice of instruction (and their corresponding data layouts) for one operator impacts the choice for their successor and formulate a global optimization problem. We show an optimal linear-time solution for this problem when the operators form a linear chain, and develop useful heuristics for the general case of a computational graph. ? VLIW Packing (i.e., Scheduling) Problem. Considering many unique aspects of our target architecture (including the notion of soft dependencies, and latency sensitivity), we present a novel Soft Dependencies Aware (SDA) algorithm for instructions packing. ? Design of an End-to-End Compilation System. We engineer a system that includes a nuanced code generation design and several additional optimizations. GCD<ref type="foot" target="#foot_0">2</ref> is extensively evaluated on 10 real-world large DNNs, with a range of model sizes and operator counts and designed for various ML tasks, targeting popular mobile DSPs. Compared with two state-of-the-art DNN frameworks (TFLite <ref type="bibr" target="#b13">[14]</ref> and Qualcomm SNPE <ref type="bibr" target="#b36">[37]</ref>) that support endto-end mobile DSPs execution, GCD 2 achieves 2.8? and 2.1? speedup (in geometric mean), respectively, reaching real-time execution for some of them for the first time. In fact, for two of the models, GCD 2 implementation supports mobile DSP execution for the first time. While comparing with three established compilers (Halide <ref type="bibr" target="#b38">[38]</ref>, TVM <ref type="bibr" target="#b4">[5]</ref>, and RAKE <ref type="bibr" target="#b3">[4]</ref>) that support efficient kernels execution on mobile DSPs, GCD 2 achieves 4.5?, 3.4?, and 4.0? speedup, respectively. GCD 2 outperforms others primarily because of improved SIMD execution and optimized VLIW instruction scheduling and the evaluation justifies the choices made in GCD 2 's algorithms for these optimizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. EXECUTING DNNS ON MOBILE DSPS</head><p>Modern mobile DSPs have become increasingly powerful with key features as follows: 1) larger SIMD widths, 2) richer vector instructions with growing computation capabilities, and 3) more flexible instruction pipelines that can tolerate certain data dependencies. Take Qualcomm Hexagon 698 DSP <ref type="bibr" target="#b39">[39]</ref> 2 as an example. Its SIMD width is 1024-bit, twice that of Hexagon 680 <ref type="bibr" target="#b40">[40]</ref> and its instruction set includes multiple SIMD/vector instructions (e.g., vmpy, vmpa, and vrmpy elaborated in Section III), and can support complicated MAC (multiply-accumulate) operations. Multiple vector (and scalar) instructions can be packed into a VLIW pipeline, further improving the computational throughput. Finally, the pipeline offers hardware mechanisms to guarantee execution correctness even in the presence of certain dependencies, thus offering more flexibility.</p><p>Mobile DSPs support fix-point operations (8/16/32-bit) with extremely high performance (e.g., the theoretical peak for Hexagon 698 DSP is 15 TOPS <ref type="bibr" target="#b41">[41]</ref>). While considering DSP chips for DNN execution, the important context here is that Quantization, a well-known technique to convert floating-point values to integer ones, has been very effective in accelerating DNN executions, particularly on resourceconstraint devices <ref type="bibr" target="#b42">[42]</ref>, <ref type="bibr" target="#b43">[43]</ref>. The cutting-edge DNN acceleration frameworks, (e.g., TFLite <ref type="bibr" target="#b13">[14]</ref> and SNPE <ref type="bibr" target="#b36">[37]</ref>, and Qualcomm's built-in library Hexagon NN <ref type="bibr" target="#b44">[44]</ref>) aim to combine the benefits of both quantization and mobile DSPs to accelerate DNN execution, achieving both (near) state-ofthe-art model accuracy and lower latency as compared to the other parts of the mobile SoC (i.e., CPUs and GPUs). Similarly, MobiSR schedules the super-resolution model over Heterogeneous Mobile Processors (including CPU, GPU, and DSP) <ref type="bibr" target="#b45">[45]</ref>.</p><p>Despite these rapid developments, compilers and libraries built for DSP chips cannot fully exploit the device's computation power -this applies to, but is not limited to, the compilers and libraries for DNN execution listed above. Specifically, the performance of the mobile DSP is sensitive to 1) the input/output data layout, and 2) the VLIW instruction packing (or scheduling) in view of all hardware resource constraints. This is because first, various SIMD/vector instructions are designed to perform MAC operations in different ways and they are friendly to different input/output data sizes and data layouts. Second, the VLIW pipeline imposes many constraints on the instructions that can be packed together.</p><p>In the context of DNN acceleration, complex DNN designs challenge the DSP-oriented implementations in multiple ways. First, modern DNNs usually consist of many operators (e.g., the latest BERT consists of over 1000 operators <ref type="bibr" target="#b46">[46]</ref>), and even with the same operator, operands can be of different shapes and sizes. Mapping growing SIMD/vector width and instruction set (variety) to these operators and operands is challenging. Second, as discussed above, the complex opportunities and constraints in VLIW packing need to be considered for implementations of specific operators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. INSTRUCTIONS AND LAYOUTS</head><p>Our target instruction set includes novel and complex SIMD instructions capable of optimizing computations found in ML (and scientific) workloads. We show three representative instructions in Figure <ref type="figure">1</ref>. While these instructions are used for multiple operators in a DNN (e.g., the convolutions), our presentation here uses matrix multiplication for illustration. Similarly, other instructions like vtmpy and vmpye can also be used to implement these operators. Our discussion here considers only three instructions. However, as a motivation, we first show the trade-offs between their use.</p><p>Table <ref type="table" target="#tab_0">II</ref> shows how the cost of matrix multiplication varies with the three choices when input tensors have different shapes. We can see that the instruction vmpy (and the corresponding 1-column layout, both are elaborated later) provides better execution efficiency if the operands have a certain length. However, for other cases, this instruction causes padding overheads, thus making the other instructions more time-and space -efficient.</p><p>As additional background, many recent works show that the floating-point representations (and operations) for weights and activations are not necessary to achieve good accuracy for DNNs, but instead fixed point (8-bit or even less) suffices <ref type="bibr" target="#b42">[42]</ref>, <ref type="bibr" target="#b47">[47]</ref>, <ref type="bibr" target="#b48">[48]</ref>, <ref type="bibr" target="#b49">[49]</ref>, <ref type="bibr" target="#b50">[50]</ref>, <ref type="bibr" target="#b51">[51]</ref>. However, one caution is that the product between two 8-bit values should be stored in 16-bits to avoid data overflow, and similarly, accumulating several such products requires 32-bits. In either case, a requantization phase is required to generate the 8-bit final output.</p><p>With this motivation and background, we explain the existing instructions and associated data layouts we have developed. In Figure <ref type="figure">1</ref> (a), we show the instruction vmpy, whose inputs are a vector with 128 8-bit values and four scalar values. In vmpy, four consecutive values in the vector are multiplied by four distinct scalars, with the output being two vectors with 64 16-bit values, each storing alternate results of multiplications.</p><p>In Figure <ref type="figure">1</ref> (b), the input for the instruction vmpa are two vectors with 128 8-bit values each. A pair of corresponding values from the two vectors are multiplied by two scalar values and then added together. Specifically, alternate pairs are multiplied with the first two and the last two scalars, respectively, and accumulated to two different output vectors.</p><p>Finally, in Figure <ref type="figure">1</ref> (c), the instruction vrmpy is illustrated -here, four consecutive values from the vector are successively multiplied by four distinct scalar values, and accumulated together. The result is a vector with 32 32-bit values.</p><p>In this work, we have developed novel dense matrix data layouts that optimize the use of these instructions for multiple key operators in DNN computations (e.g., MatMul, CONV, Depthwise CONV, etc.), and this part takes matrix multiplication (MatMul), a critical kernel for our target workload as an example. Developing layouts for implementing arbitrary loop nests using these or similar instructions is an open problem beyond the scope of this paper.</p><p>In Figure <ref type="figure" target="#fig_1">2</ref> (a), we show the layout that enables the use of vmpy instruction shown earlier in Figure <ref type="figure">1 (a)</ref>. For efficiency, it is very important that the set of values that are to be loaded to or stored from a vector register are stored in a contiguous fashion. The layout we use is referred to as the 1-column layout. The numbers shown in the boxes represent the offset of the location of that element. In 1-column layout, a set of 128 rows is stored in a column-major way, and this pattern is repeated for the next set of rows. In carrying out the matrix multiplication, the first column is loaded to a vector and all values are multiplied with the first weight (0) stored in the scalar register. The outputs are two vectors storing 64 16-bit elements each, which will eventually be shuffled to obtain an output layout matching the input layout. The process continues by loading the next 128 elements physically stored in our layout, multiplying them with the second weight (1), and reducing the output to the same two vectors.</p><p>In Figure <ref type="figure" target="#fig_1">2</ref> (b), we show the layout and the key steps of matrix multiplication with the instruction vmpa, which was shown earlier in Figure <ref type="figure">1 (b)</ref>. The layout we have designed is referred to as 2-column layout -within the </p><formula xml:id="formula_0">v v v v ? v v v v s s s s v s + v s + v s + v s ? v s + v s + v s + v s (c) vrmpy v v v v ? v v v v s s s s v s v s ? v s v s v s v s ? v s v s (a) vmpy v v v v ? v v v v v v v v ? v v v v s s s s v s + v s v s + v s ? v s + v s v s + v s v s + v s v s + v s ? v s + v s v s + v</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Weight Output  64-row panels, values for 2 columns are stored adjacent to each other, before following the column-major storage. In applying matrix multiplication, elements 0, 1, 128, and 129, which are from the same rows of the matrix, are multiplied by the output weights 0, 1, 2, and 3, respectively, stored in scalar registers. Note that the two corresponding output elements in the output vectors need to be further added to obtain the results.</p><p>In Figure <ref type="figure" target="#fig_1">2</ref> (c), we show the matrix multiplication operations and layouts with the use of the instruction vrmpy shown in Figure <ref type="figure">1 (c</ref>). The input and weight matrix are of different shapes as compared to the previous examples, in order to illustrate the layout and the computation. Here, panels of 32 rows are used and four elements from each row are stored together. Four elements in a row are multiplied with four weights stored in scalar registers. We also note that while there is an instruction somewhat similar to vrmpy in Intel instruction set (vpdpbusd), there are no counterparts to vmpy or vmpa at the current time. Overall, our work considers a relatively small number of candidate instructions for implementing a single operation, using a "pre-designed" approach for each pair of operator and instruction. Efforts do exist on trying to automate the selection of instruction and code generating using the instruction <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b52">[52]</ref>. We have conducted a brief comparison of our approach against the code generated by the most recent of these efforts (which also targets the same instruction set), i.e. RAKE <ref type="bibr" target="#b3">[4]</ref>. As shown in Table <ref type="table" target="#tab_2">III</ref>, our approach is able to deliver significantly higher performance. Thus, while automation of instruction selection and code generation is valuable, current approaches are not matching the "predesigned" approach we are taking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. SYSTEM DESIGN OF GCD 2</head><p>This section highlights the major optimizations developed in GCD 2 , followed by a brief summary of implementations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. SIMD Global Opt. Problem Formulation</head><p>From the discussion earlier in Section III, the important takeaway is that different instructions can be used for the same operation, but with different requirements on input formats, resulting in different output formats, and with different trade-offs (which were summarized earlier in Table <ref type="table" target="#tab_0">II</ref>).</p><p>With a relatively small number of instructions available to implement a single operation, the instruction and the layout selection can be performed (in isolation) by explicitly  <ref type="bibr" target="#b33">[34]</ref> and TinyBERT <ref type="bibr" target="#b53">[53]</ref>. considering all choices and choosing the one that requires the fewest cycles for execution. However, it turns out that with distinct input and output layouts for different instructions, choices for each operation cannot be made in isolation. Suppose an operator A can be implemented in the most efficient fashion using the instruction vmpa. Let the output of the operation A be the input to the operation B. Without considering the need for the formatting of input tensors, let the most efficient implementation of B be using the instruction vrmpy. However, the output tensor from the operation A will be in the two-column format (Figure <ref type="figure" target="#fig_1">2</ref> (b)), whereas if B is implemented using vrmpy, it is expected that the input tensors are in the four-column format (Figure <ref type="figure" target="#fig_1">2</ref> (c)). Converting the layout of a tensor itself is a timeconsuming step. Thus, if the sequence of two operators A and B are considered, it is possible that the most efficient implementation involves using the same instructor (and thus layouts) for the two operators. In practice, DNN models use many operators (e.g., the model EfficientDet-D0 used in our evaluation has 822 operators), and thus, we have a complex optimization problem.</p><p>To formulate this global optimization problem, we use an existing intermediate representation called the Computational Graph (CG) <ref type="bibr" target="#b4">[5]</ref>, which captures the data-flow and basic operator information like the operator type and parameters. Figure <ref type="figure" target="#fig_2">3</ref> shows examples of such graphs. Let V be the set of vertices in a CG and let E be the set of edges. Each vertex is an operation that produces exactly one output tensor. A directed edge (v i , v j ) denotes that the output of the vertex (operation) v i is (one of) input(s) to the operation v j . The source of the edge e is also denoted as vin(e) and similarly, the destination of e is denoted as vout(e). Now, given an operator (vertex) O in the CG, let it have a set of immediate predecessors we denote as Pre(O). By each predecessor, we denote interchangeably both the operators and their output. After performing the local analysis of possible implementations and associated layouts for the operator O we obtain a set of possible execution plans EP(O), comprising execution plans ep 1 (O), ep 2 (O), and so on. Associated with every execution plan, there is a cost of execution, denoted by Cost(ep i (O), which is based on the number of instructions (cycles) required. This cost calculation assumes that all input tensors are already stored in the required layout for the SIMD instruction used.</p><p>We consider an execution plan ep i (O) and a predecessor tensor of O, which we denote as I (I ? Pre(O)). If the operator I is executed with the plan ep j (I ), then there could be a data transformation with the associated cost TC(ep j (I ), ep i (O)) (this cost will be 0 when data transformation is not required).</p><p>Given this background, the global optimization problem is as follows. For each operator (vertex) v in the CG, we want to select an execution plan ep v , such that the total cost of execution for the graph G, which is denoted as</p><formula xml:id="formula_1">Agg Cost(G) = ? v?V Cost(ep v (v)) + ? e?E TC(ep vin(e) (vin(e)), ep vout(e) (vout(e)))<label>(1)</label></formula><p>is minimized. In the expression above for Agg Cost(G), the first term is the cost of execution associated with each operation under the choice of plan made, whereas the second term is the cost of data transformation between the layouts for the source and the sink of the edge, under the choices of implementation plans chosen for the source and sink operators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Layout &amp; Instruction Select Solution</head><p>It is easy to see that a trivial approach for solving this problem will involve comparing k |V | options, where |V | is the number of vertices in the graph and k is number of (assumed fixed for all operators) options available for each operator. Even when k is 2 or 3, this cost can be easily prohibitive for realistic DNN models. Furthermore, the above problem is really a Partitioned Boolean Quadratic Programming (PBQP) problem, which is known to be NP-hard <ref type="bibr" target="#b54">[54]</ref>.</p><p>If we simply have a linear chain of operations O 1 , O 2 ,...,O n , then the following approach can be used to solve the problem. Let Sol(i, j) denote the lowest possible cost of execution operations O 1 , O 2 ,...,O i such that the output from the operator O i is the j th available choice ( j ? k, where k is the number of choices available for each operator). Then, we have</p><formula xml:id="formula_2">Sol(i, j) = min l=1,...,k (Sol(i -1, l) + TC(ep l (O i-1 ), ep j (O i )) (2)</formula><p>Here, Sol(i, j) is computed by comparing k choices, which are the lowest cost ways of reaching each of the k different output formats for the previous operation in the chain. It is easy to see that this recurrence can be solved in O(|V | ? k 2 ) time. Moreover, this solution can be easily extended to the cases when either every path from a "source" vertex of a DAG to a given vertex is of the same length, or when every vertex has at most one output. However, this approach does not work for an arbitrary DAG and we focus on an effective heuristic solution. While considering a PBQP solver <ref type="bibr" target="#b54">[54]</ref>, <ref type="bibr" target="#b55">[55]</ref>, which is not guaranteed to provide an optimal solution but is in practice close, is an option, we instead focus on exploiting the properties of our target domain. For this, we consider the following definition: Definition IV.1 (Cost Optimal Partitioning). Given a computational graph G, a cost optimal partitioning of G is a disjoint graph partitioning P = {G 1 , G 2 ,...,G n }, such that for Agg Cost(G) (as defined in Equation <ref type="formula" target="#formula_1">1</ref>), we have</p><formula xml:id="formula_3">Agg Cost(G) = Agg Cost(G 1 ) + Agg Cost(G 2 ) + ... + Agg Cost(G n )<label>(3)</label></formula><p>Note that we use the popular definition of graph partitioning, where the edges between vertices that are in different partitions are not considered part of either partition. If such partitioning can be found, the optimal plans for all operators within each partition can be determined in isolation, translating to a significant reduction in the complexity of search.</p><p>In practice, What we can hope to achieve is to find a set of partitions that can be optimized independently, i.e. where the lowest cost for the entire graph is achieved by choosing plans within each partition independently. To achieve this, we note that an edge e = (v i , v j ) is a desirable partitioning edge if 1) the node v j has only one predecessor (v i ), and 2) The operator v j is a layout transformation operator or the transformation along the edge e is a profitable transformation. Typical examples of layout transformation operators include Reshape and Transpose -they do not perform any computations but change the shape of the operand. A transformation along an edge is considered profitable if the reduction in execution time of the successor operator with the transformed layout is higher than the cost of the data transformation itself. The intuition for this definition of desirable partitioning edge is that decisions on nodes leading up to this edge and vertices following this edge can be made in isolation.</p><p>However, as next challenge for us, partitioning a graph typically involves many cut edges. Now, if we can find a cut edge that is dominant, i.e, if every path from the (assumed to be unique) source vertex in the DAG to the (again, assumed unique) sink vertex passes through this edge, then the problem is simplified. When this is not feasible, we can add complementary edges to the identified cut edges to create complete partitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. VLIW Optimization</head><p>Instruction packing or scheduling is a long-standing issue in VLIW research that has been proved to be NPhard <ref type="bibr" target="#b56">[56]</ref>, <ref type="bibr" target="#b57">[57]</ref>. Because of the specific opportunities as well as challenges associated with our target architecture, a new algorithm is developed in this work. Optimization Foundation: Hard/Soft Dependencies. For our target architecture, dependencies between two instructions  can be characterized into two types with respect to their implication on placing them in the same VLIW packet<ref type="foot" target="#foot_2">3</ref> </p><p>? Hard dependency denotes a strict dependence relationship where placing such instructions in the same packet likely produces incorrect results. ? Soft dependency denotes a relaxed dependence relationship, and placing instructions together produces correct results; however, the resulted execution performance is likely degraded to a certain degree. An example of the soft dependency in our target architecture is the one between a scalar addition operation and a consumer of the result of such an addition.</p><p>To further illustrate the nuances associated with soft dependencies, we show two examples of packing instructions with soft dependencies in Figure <ref type="figure" target="#fig_3">4</ref>. Figure <ref type="figure" target="#fig_3">4</ref> (a) shows a dependency between a load operation and an arithmetic operation that consumes the loaded value. Each of these two instructions takes 3 clock cycles <ref type="foot" target="#foot_3">4</ref> individually, however, if they are packed in the same packet, they can execute correctly by taking 4 cycles in total <ref type="foot" target="#foot_4">5</ref> . This example shows that packing instructions with soft dependencies together takes less clock cycles than not packing them together at all (i.e., treating the soft dependency as a hard dependency). However, if sufficient number of instructions are available without any dependencies between them, we will prefer to not pack instructions with soft dependencies together. Figure <ref type="figure" target="#fig_3">4</ref> (b) shows a similar example with a soft dependency between an arithmetic operator and a store operation. Which dependencies are soft and which ones are hard depends upon the microarchitecture. This information needs to be obtained from the details of processor implementation (e.g., <ref type="bibr" target="#b58">[58]</ref>) and made available to the instruction packing algorithm. Soft Dependencies Aware (SDA) VLIW Packing Algorithm. Because of the notion of soft dependencies, we have developed a new VLIW instruction packing algorithm. Besides handling the distinction between soft/hard dependencies, the algorithm is cognizant of other constraints. While a packet can have up to 4 instructions, there can be a limited number of slots for each type of instruction. As an example, packing two shift operations together is not allowed. This instruction packing is implemented as an additional optimization step of LLVM's assembly code generation. Like much of the previous work, the packing algorithm uses the notion of a critical path <ref type="bibr" target="#b59">[59]</ref>. and its overall goal of minimizing execution time as two sub-goals: 1) reducing the total number of instruction packets, and 2) packing instructions with identical or similar latency together to minimize VLIW pipeline stalls. The work also has many similarities with algorithms for code generation targeting superscalars, in the sense the goal is to minimize intra-bundle RAW stalls <ref type="bibr" target="#b60">[60]</ref>.</p><p>Our presentation of the algorithm (as shown in Algorithm 1) is supported by the running example from Figure <ref type="figure" target="#fig_4">5</ref>. Its left hand side shows the pseudo assembly code for a part of an innermost nested loop of a frequently occurring Add operator in deep neural networks (R = A + B + C), where A, B, and C are two-dimensional uint8 arrays and R is a two-dimensional int16 array. Take the instruction of v2 : 1 = vadd(v1, v2) in this pseudo assembly code as an example. v1 and v2 are two 8-bit registers. v2:1 denotes a Returning to our algorithm, it first builds a Control-Flow Graph (CFG) on assembly for each operator, and finds the basic block corresponding to the computation kernel of each operator (usually the largest basic block). Next, it builds an instruction dependency DAG (called IDG) based on the hard/soft dependency information, and finds the critical path with the longest execution latency. The middle part of Figure <ref type="figure" target="#fig_4">5</ref> shows the IDG -here, a vertex represents an instruction, and an edge represents the dependence between two instructions. A solid edge represents a hard dependency and a dotted edge represents a soft dependency. Take the instructions (or vertices) 4, 5, 6, and 7 in this figure as an example. The dependencies between the instructions 4 and 5, 4 and 6, and 4 and 7 are all soft dependencies. IDG also contains an artificial entry vertex. The number shown with the vertex corresponds to the assembly instruction in the left. The critical path is colored in red. The vertices with identical colors have the same rank (distance to the entry).</p><p>Based on the IDG and the critical path, the algorithm now packs instructions. When creating a new packet, the algorithm always uses the last (unpacked) instruction in the critical path as a seed (line 9). Next, such an instruction is packed with other instructions that either do not have any outgoing edges or have only soft-dependence edges to an instruction to be packed (all of these instructions are called free instructions). This step consists of three major sub-steps: i) iterating through all free instructions (line 7), 2) finding a candidate instruction from the set of free instructions (line 11), and 3) grouping the candidate instruction into the current packet. Particularly, the key second sub-step (i.e., finding a candidate instruction) comprises of two steps: first, for the current working packet, the algorithm finds all instructions that can be packed while meeting the hardware constraints (line 20), and also determines the highest latency (hi lat) among the instructions that are already in the current packet; second, it iterates these available instructions to pick up the best instruction and returns it (lines 25 to 30). Note that, the best instruction selection is based on this instruction's score (i.score) that is calculated as follows:</p><formula xml:id="formula_4">i.score = (i.order + i.pred)?w-abs(hi lat -i.lat) ? (1-w)<label>(4)</label></formula><p>According to Equation <ref type="formula" target="#formula_4">4</ref>, the score of an instruction is decided by its three attributes, its distance from the entry node (i.order), its predecessor instruction count (i.pred), and its latency (i.lat). The first two have positive impacts on the score, while the absolute difference between this instruction's latency (i.lat) and the latency of the longest instruction already in the current packet (hi lat) has a negative impact on the score. The former is because it is desirable to include instructions that have a longer chain of dependencies and/or a total large number of instructions that it is dependent on. The latter, on the other hand, wants to create more efficiency by packing instructions of the same (or very similar) latency values together. This algorithm introduces two new parameters (w is short of weight, and p is short for penalty) that are empirically decided. w aims to control the weight of the three factors' impact (line 26), while p aims to control the impact of soft dependency on this packing (line 28). Specifically, the value p depends both on the instruction i under consideration and the instructions already placed in the packet, and captures the stall that the soft dependence will cause. For comparison in our experiments, we also create a version of our algorithm that reduces all soft dependencies to 'none' or no dependence -this version of the algorithm will ignore the calculation of this penalty. Next, to complete the description of our algorithm, after one packet is created, the algorithm repeats by finding the critical path of the remaining sub-graph. Returning to Figure <ref type="figure" target="#fig_4">5</ref>, the right part shows the packets after scheduling (N denotes an empty slot). This example compares our Soft Dependencies Aware (SDA) packing algorithm  (bottom) with a sub-optimal algorithm (called soft to hard) that treats all soft dependencies as hard ones (top). Taking the first seed (vertex 8, i.e., the last instruction in the critical path) as an example. 8 and 6 cannot be packed together (because of hardware constraints) and 8 can be packed with 7. Our packing algorithm can continue to explore the packing opportunity between 4 and 5/6 because 5 and 6 only have soft dependencies to 4, and the soft dependencies allow the packing for 1, 2, and 3; however, these opportunities do not exist in soft to hard version of the algorithm. In summary, our algorithm delivers a schedule with only three packets, while the sub-optimal soft to hard version generates a schedule with two additional packets. Evaluation results in Section V further validate our algorithm's efficacy. Impact of Unrolling. Loop unrolling plays an important role in the schedule quality by affecting the scheduling scope and the register pressure. Different from previous work like <ref type="bibr" target="#b61">[61]</ref>, GCD 2 employs a low-cost heuristic solution specifically designed for DNN operators. The basic idea is to perform a fast adaptive unrolling setting selection according to the shape of output tensors, for example, for GEMM, different unrolling settings are designed for varied output shapes (skinny, nearsquare, and fat). Our empirical study in Section V proves that this approach outperforms some simple selections while also yielding comparable performance gains to a much more expensive exhaustive search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Putting Everything Together</head><p>GCD 2 is implemented on top of an existing end-to-end DNN execution framework, PatDNN <ref type="bibr" target="#b62">[62]</ref>, <ref type="bibr" target="#b63">[63]</ref>, <ref type="bibr" target="#b64">[64]</ref> to support efficient mobile DSP execution. Figure <ref type="figure" target="#fig_6">6</ref> shows the system workflow of GCD 2 . First, it converts the post-training quantized model to a computational graph (and optimizes it with various techniques, e.g., constant folding) by leveraging the existing framework. Second, it feeds the (optimized) computational graph to the SIMD global optimization module to conduct the local layout (instruction) enumeration and the global layout (instruction) selection. The result here is an optimized SIMD code generation plan including the data layout for each operator and corresponding SIMD instructions to use. This is followed by a pass where other optimizations are applied, e.g., replacing an expensive division operation with a database lookup operation. As the next step, the existing framework and the optimized SIMD code generation plan lead to a "low-level" C code with input/output tensor storage details and optimized SIMD intrinsics. Finally, it employs LLVM <ref type="bibr" target="#b65">[65]</ref> with our VLIW packing optimization to generate the optimized executable code on the mobile DSP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EVALUATION</head><p>This section evaluates the performance of GCD 2 by comparing it with five state-of-the-art frameworks, TFLite <ref type="bibr" target="#b13">[14]</ref> (V2.6.0), SNPE <ref type="bibr" target="#b36">[37]</ref> (V1.55), Halide <ref type="bibr" target="#b38">[38]</ref> (V12.0.1), TVM <ref type="bibr" target="#b4">[5]</ref> (V0.8.0), and RAKE <ref type="bibr" target="#b3">[4]</ref> (V1f99df1). More specifically, TFLite, SNPE, and TVM are the state-of-the-art productionlevel DNN execution frameworks that can support (or partially support) our target mobile DSP. Both TFLite and SNPE call Hexagon NN, an expert-written hand-tuned library designed by Qualcomm. However, as end-to-end DNN execution frameworks, their computational graph optimizations (graph rewriting, operator fusion, etc.) are different, thus resulting in very different execution performance (as shown in Table <ref type="table" target="#tab_5">IV</ref>). Halide, TVM, and RAKE use LLVM as their back-end to generate DSP instructions. They perform packet generation without distinguishing between soft and hard dependencies (i.e., they treat each soft dependency as a hard dependency). It should be noted that Halide, TVM, and RAKE are tensor compilers, while GCD 2 comprises both tensor compiler optimizations (e.g., global data layout optimization) and language compiler optimization (instruction packing). We introduce a version of GCD 2 to facilitate a comparison of tensor compiler aspect of our work with these systems, as we will describe later. Our evaluation has four main objectives: 1) to demonstrate that GCD 2 outperforms all of these state-of-the-art frameworks on mobile DSP (Section V-B); 2) to identify the benefits of specific optimizations and the choices made in our algorithms (Section V-C); 3) to study the power consumption and energy efficiency of GCD 2 against alternative implementations on the same chip (Section V-D); 4) to compare the inference speed and energy efficiency of our mobile DSP-based solution with other embedded DNN accelerators (Section V-E).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Evaluation Setup</head><p>Models and Datasets. GCD 2 is evaluated on 10 state-of-theart neural networks (see Table <ref type="table" target="#tab_5">IV</ref>) that are categorized into seven groups according to the tasks they perform. Particularly, they include 1) three image classification twodimensional CNNs (MobileNet-V3 <ref type="bibr" target="#b66">[66]</ref>, EfficientNet-b0 <ref type="bibr" target="#b32">[33]</ref>, and ResNet-50 <ref type="bibr" target="#b33">[34]</ref>); 2) one image style transfer two-dimensional CNN (FST <ref type="bibr" target="#b67">[67]</ref>); 3) one image-to-image translation GAN (CycleGAN <ref type="bibr" target="#b35">[36]</ref>); 4) one super resolution two-dimensional CNN (WDSR-b <ref type="bibr" target="#b68">[68]</ref>); 5) two object detection two-dimensional CNNs (EfficientDet-d0 <ref type="bibr" target="#b69">[69]</ref>, and PixOr <ref type="bibr" target="#b34">[35]</ref>); 6) one trans-former-based NLP model (Tiny-BERT <ref type="bibr" target="#b53">[53]</ref>); and finally, 7) one transformer-based speech recognition model (Conformer <ref type="bibr" target="#b70">[70]</ref>). All the evaluated models in this section are quantized by a standard approach used by well-known TFlite <ref type="bibr" target="#b71">[71]</ref>(with identical post-training quantization across all frameworks) with 8-bit integers being used for weights and feature maps (activations).</p><p>It should be noted that the choice of datasets has a negligible impact on the final inference latency or relative execution speeds, which are the primary metrics in our evaluation. Therefore, and also because of space limitations, we report results from one dataset for each model. MobileNet-V3, EfficientNet-B0, ResNet-50, and CycleGAN are trained on the ImageNet dataset <ref type="bibr" target="#b72">[72]</ref>, WDSR-b is trained on DIV2K <ref type="bibr" target="#b73">[73]</ref>, EfficientDet-d0 and FST are trained on COCO <ref type="bibr" target="#b74">[74]</ref>, PixOr is trained on KITTI <ref type="bibr" target="#b75">[75]</ref>, TinyBERT is trained on BooksCorpus <ref type="bibr" target="#b46">[46]</ref> and English Wikipedia <ref type="bibr" target="#b46">[46]</ref>, and Conformer is trained on <ref type="bibr" target="#b76">[76]</ref>. Because all frameworks employ the identical model quantization approach, they achieve the same accuracy on all models and datasets, and thus accuracy is not reported. Test Bed. Most of the experiments described in this section are conducted on a Samsung Galaxy S20 (with Snapdragon 865 SoC <ref type="bibr" target="#b39">[39]</ref>) that consists of an octa-core Kryo 585 CPU, Adreno 650 GPU, and Hexagon 698 DSP (with Vector eXtensions support). We also tested our framework on older series Snapdragon platforms, which show the similar performance gains against other baseline frameworks. We omit the results due to the space constraints. We note that our optimization designs are general, potentially applicable to other mobile DSP architectures (e.g., Cadence DSPs with increasingly complex SIMD and VLIW supports). All models are executed with their best configurations while the same parameters are used for all execution platforms. Each data involves inferences on 50 different inputs. After excluding the highest/lowest time, an average is taken and reported. As the variation is negligible, ranges are not reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison with Other Frameworks</head><p>This part evaluates the overall performance of GCD 2 by comparing it against five state-of-the-art frameworks, TFLite, SNPE, Halide, TVM, and RAKE. We compare the performance of GCD 2 with TFLite and SNPE over 10 models. While Halide, TVM, and RAKE have the capability to generate code for the DSP chip, they currently cannot execute full DNN models on this platform. Thus, a Conv2d kernel is used for comparison against Halide, TVM, and RAKE. Execution Latency. Table <ref type="table" target="#tab_5">IV</ref> shows the overall performance comparison for all 10 models. TFLite and SNPE do not support Transformer-based models. For the other 8 models, GCD 2 achieves 1.5? to 6.0?, and 1.5? to 4.1? speedup over TFLite and SNPE, respectively. Table <ref type="table" target="#tab_5">IV</ref> shows that GCD 2 outperforms TFLite and SNPE mainly because of 1) optimized SIMD instruction selection and layout transformation, and 2) optimized SDA VLIW packing by taking soft dependencies into account. TFLite and SNPE employ a uniform SIMD implementation for each operator type to support mobile DSP execution, and their VLIW   packing does not consider soft dependencies as GCD 2 . It turns out that GCD 2 achieves the most speedup (6.0? over TFLite) on WDSR-b. The reason is that feature map shapes in WDSR vary significantly among different operators, and our instruction selection and layout transformation optimizations deliver much better performance over others.</p><p>We also note that GCD 2 for the first time enables mobile DSP execution of two DNNs (TinyBERT and Conformer) because it supports more operators than TFLite and SNPE, e.g., more variants of MatMul, and Pow. It also the first time supports real-time mobile DSP execution of another (EfficientDet-d0).</p><p>Next, we compare several individual convolutional computation kernels with Halide, TVM, and RAKE. Because our native compiler optimizations (SDA VLIW instruction packing) built on LLVM can be applied to all other frameworks as well to further improve their performance, we separate tensor compiler optimizations (e.g., our data layout and instruction selection) and native/language compiler optimizations (e.g., SDA VLIW instruction packing) in this comparison by introducing a new version of GCD 2 called GCD b . GCD b only contains tensor compiler optimizations, and can be viewed as a more fair comparison against these three tensor compilers. In this comparison, the first 8 unique Conv2D operators in ResNet-50 are used. Figure <ref type="figure" target="#fig_8">7</ref>   the speedup and the packet count for these 8 Conv2D kernels, respectively, and all results are normalized by Halide. It turns out that GCD 2 outperforms Halide, TVM, and RAKE with significant speedups due to both its layout optimizations and VLIW instruction packing. In comparing GCD b with other tensor compilers, GCD b achieves up to 3.8?, 2.7?, and 3.3? over Halide, TVM, and RAKE due to tensor compiler optimizations like layout and instruction selection. In addition, our instruction packing algorithm results in fewer numbers of packets (25% &lt; Halide, 19% &lt; TVM, and 21% &lt; RAKE on average, respectively). Please also refer to Section V-C for a more detailed performance breakdown study.</p><p>Overall Performance Analysis. To further understand the performance difference among above frameworks, Figure <ref type="figure" target="#fig_10">8</ref> compares DSP utilization and memory bandwidth. This experiment uses 5 representative models out of 8 supported by both TFLite and SNPE, including EfficientNet-B0 (ENT-B0), ResNet-50 (RNT-50), FST, WDSR, and PixOr. Experiments on other models show similar trends and are excluded because of space limits. The data is collected from Snapdragon Profiler <ref type="bibr" target="#b77">[77]</ref>. For DSP utilization, TFLite and SNPE can only achieve 88% to 93%, and 89% to 95% of GCD 2 's utilization, respectively. For memory bandwidth, TFLite and SNPE can only utilize 86% to 93% and 90% to 94% of GCD 2 's, respectively. These results show GCD 2 better utilizes mobile DSP's computing and memory resources with better VLIW instruction pipeline execution and higher SIMD parallelism.</p><p>It should be noted that the theoretical peak performance of Hexagon 698 reported by Qualcomm is 15 TOPS <ref type="bibr" target="#b41">[41]</ref>. Figure <ref type="figure">9</ref>: Performance Breakdown Analysis. Speedup over the baseline (normalized with the no-opt version). DSP utilization and memory bandwidth analysis (both normalized with the GCD 2 optimal version as 100%). The results are collected from Snapdragon Profiler <ref type="bibr" target="#b77">[77]</ref>.</p><p>However, this number includes its Neural Processing Unit that is not publicly programmable yet. To get the peak performance of the publicly available vector processing unit (HVX), we test the highly optimized matrix multiplication kernel in the Qualcomm Hexagon SDK with small inputs that can fit into the L-1 cache, and achieve the performance of 3.7 TOPS. Our evaluation shows GCD   global optimal always conducts an (expensive) exhaustive search on the entire computational graph to find the optimal solution.</p><p>For the purpose of these experiments, partial computational graphs are extracted from ResNet-50 using contiguous operators. Figure <ref type="figure" target="#fig_13">10</ref> (a) compares the model execution performance among local optimal, global optimal, and our two versions -GCD 2 (13) and GCD 2 (17) mean the maximum number of operators within each sub-graph is 13, and 17, respectively. Compared with local optimal, GCD 2 brings 1.55? to 1.7? speedup, while global optimal brings 1.56? to 1.72? speedup. This validates the design choice we have made -specifically, the performance of GCD 2 (13) is almost identical to global optimal. At the same time, it is clear that local-only decisions impose large data transformation overheads and do not achieve good performance.</p><p>Figure <ref type="figure" target="#fig_13">10</ref> (b) compares the search time for the four solutions. Obviously, the search time in global optimal solution increases exponentially, making it impracticable even when there are 25 operators (complete models have more operators, see Table <ref type="table" target="#tab_5">IV</ref>). The search time is over 80 hours with only 25 operators in the graph, while GCD 2 (13) and GCD 2 (17) need less than 2 seconds and 1 minute, respectively. VLIW Packing Analysis. One of the unique aspects of our SDA VLIW instruction packing is the treatment of soft dependencies. We evaluate this by comparing our method against two versions: 1) all soft dependencies are treated as hard dependencies, i.e., separating all instructions with soft dependencies into different packets (soft to hard; 2) all soft dependencies are treated as no dependencies soft to none (i.e., removing lines 27, 28 in Algorithm 1 and thus not associating with penalty with packing an instruction with a soft dependency). Figure <ref type="figure">11</ref> reports the effectiveness of our optimization using 5 models and establishes our current algorithm does better than either of these choices. GCD 2 achieves up to 2.1?, and 1.4? speedup compared with soft to hard and soft to none, respectively because of better packing efficiency as compared to soft to hard and fewer runtime stalls as compared to soft to none. Unrolling Analysis.   comparison of different unrolling strategies for a matrix multiplication kernel (three loop-levels): Out (only unroll the outer-most-level loop), Mid (only unroll the mid-level loop), and Exhaustive (unroll the loops by an exhaustive search). We omit the inner-most-level loop as a possibility as vectorization is performed at that level. The x-axis denotes the unrolling factor, while the speedup is normalized by no unrolling, i.e., when the unrolling factor is 1. The unrolling settings of GCD 2 for both loop levels are also labeled in this figure. The best configuration by exhaustive search is 4 -4. GCD 2 achieves higher performance compared with the other two options. For all options, we see the expected result that the performance drops if unrolling factor is too large due to increasing register spilling. Figure <ref type="figure" target="#fig_16">12</ref> (b) compares the performance of Out, Mid, Exhaustive search, and GCD 2 under different matrix multiplication kernels -here again the y-axis is normalized by No unrolling in each kernel. Unrolling factor in No unrolling is 1, while Out and Mid both use the best unrolling factor obtained from Figure <ref type="figure" target="#fig_16">12 (a)</ref>. Compared with exhaustive search (Exhaustive that searches the best unroll plan for a loop structure in some common unrolling configurations), GCD 2 achieves very comparable performance while saving significant search time (exhaustive search generally takes over 3 minutes for each kernel). GCD 2 unrolling achieves much higher performance compared with the other two strategies across all kernels.   SoC (TFlite-GPU) is also included. Figure <ref type="figure" target="#fig_17">13</ref> (a) shows the total power consumption of each solution, where we see that TFLite-GPU consumes the most power (ranging from 2.1 Watt to 3.8 Watt), and three DSP-based solutions consume less power. GCD 2 -DSP consumes less power than TFLite-GPU (by around 3.6% on average) while consuming slightly higher power than TFLite-DSP and SNPE-DSP (7.2% and 6.7% on average, respectively). GCD 2 -DSP consumes more power than other DSP solutions mainly because of its better DSP and memory utilization. As this results in reduced execution times, GCD 2 -DSP achieves much better energy efficiency as measured in inference frames per Watt -specifically improving on TFLite-DSP and SNPE-DSP by around 1.7? and 1.5? on average, respectively (Figure <ref type="figure" target="#fig_17">13</ref>). Figure <ref type="figure" target="#fig_17">13</ref> also shows that all mobile DSP-based solutions result in better energy efficiency than the state-of-the-art mobile GPU-based solution, TFLite-GPU. Specifically, GCD 2 outperforms it by 2.9? in energy efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Power Consumption and Energy Efficiency</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Comparison with Other DNN Accelerators</head><p>To better understand the inference speed and energy efficiency of mobile DSP, we also compare GCD 2 with two popular embedded DNN accelerator-based solutions, EdgeTPU <ref type="bibr" target="#b78">[78]</ref> and Jetson Xavier <ref type="bibr" target="#b79">[79]</ref> using a representative DNN (ResNet-50). EdgeTPU is a low-power embedded platform with an edge TPU aiming to accelerate integer computations. Jetson Xavier utilizes both a GPU and DLA (deep learning accelerator), with operators not supported by DLA executed by the GPU. In this evaluation, EdgeTPU and Jetson Xavier use TFLite, and TensorRT, respectively, as their inference engine. The evaluation results are presented in Table <ref type="table" target="#tab_7">V</ref>. Jetson Xavier with int8 results in the highest FPS (frames per second) though with more power consumption. Our mobile DSP solution, GCD 2 achieves 6.1? and 1.48? better energy efficiency (FPW) with the same data type (int8) over EdgeTPU and Jetson Xavier, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RELATED WORK</head><p>This section discusses efforts related to DNN acceleration and compilation, SIMD optimizations, VLIW instruction packing, and other compilation work targeting DSP chips. DNN Acceleration and ML/DL Compilers. There are many recent efforts on accelerating DNN inference on edge and mobile devices including DeepX <ref type="bibr" target="#b12">[13]</ref>, TFLite <ref type="bibr" target="#b13">[14]</ref>, TVM <ref type="bibr" target="#b4">[5]</ref>, MNN <ref type="bibr" target="#b14">[15]</ref>, DeepCache <ref type="bibr" target="#b15">[16]</ref>, DeepMon <ref type="bibr" target="#b16">[17]</ref>, DeepSense <ref type="bibr" target="#b17">[18]</ref>, MCDNN <ref type="bibr" target="#b18">[19]</ref>, and MobiSR <ref type="bibr" target="#b45">[45]</ref>. Some of them (e.g., TVM, and TFLite) rely extensively on compiler techniques, and hence are called ML or DL compilers. Most of these efforts do not target DSP, except TVM, TFLite, and MobiSR that offer options to call certain versions of Hexagon NN <ref type="bibr" target="#b44">[44]</ref>. They do not focus on SIMD/VLIW optimizations as GCD 2 . TASO <ref type="bibr" target="#b8">[9]</ref> and AccPar <ref type="bibr" target="#b80">[80]</ref> are two recent DNN acceleration efforts with some similarities to GCD 2 . TASO's computationgraph-level optimization is restricted to a sub-graph with a limited number of operators, aiming to assist in their proposed effective operator substitution; while GCD 2 focuses on a global optimization aiming to find a data layout solution that can result in the optimized execution of the entire DNN. The partitioning problems considered by AccPar have similarities with the data layout (and instruction) selection problem GCD 2 considered. However, AccPar's formulation is different and can always be solved by dynamic programming, while GCD 2 's problem maps to an NP-complete problem, PBQP <ref type="bibr" target="#b54">[54]</ref>, and thus requires a different solution.</p><p>Compiling for DSP Chips. Digital Signal Processing chips have been around for several decades and there have been multiple systems developed for compiling for them <ref type="bibr" target="#b81">[81]</ref>, <ref type="bibr" target="#b82">[82]</ref>, <ref type="bibr" target="#b83">[83]</ref>, <ref type="bibr" target="#b84">[84]</ref>, including considering SIMD features <ref type="bibr" target="#b85">[85]</ref> and exploring VLIW instructions <ref type="bibr" target="#b86">[86]</ref>, <ref type="bibr" target="#b87">[87]</ref>. However, the DSP chip instructions set targeted in this earlier work do not have much correspondence to a modern mobile DSP chip like the one considered in this work. The techniques presented in this work are all related to advances in SIMD instruction sets and properties of VLIW instruction execution. Recently, Ahmad et al. <ref type="bibr" target="#b3">[4]</ref> have reported a system that does instruction selection and code generation for the same instruction set as the one we have targeted. Their work is more general in considering arbitrary loop nests but does not address the global optimization problem. Moreover, their approach has a high compilation cost, and they report results on small kernels only -our experimental comparison shows better results for our system even on individual operators. The work from Vanhattum et al. <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b52">[52]</ref> also has similar focus (and limitations) but their target backend is different, making a direct experimental comparison infeasible. Next, Yang et al. have mapped a vision-related DNN to a chip that comprises several DSP processors, performing effective mapping to their vector instruction <ref type="bibr" target="#b1">[2]</ref>. However, their work has been applied to a single model and does not include a general compiler-based optimization framework. Prior to that, another system (based on Halide system) was extended to support DSP chips <ref type="bibr" target="#b0">[1]</ref>, but this work did not emphasize data layout issues. SIMD Optimizations. Compiler-driven code optimization and generation for SIMD <ref type="bibr" target="#b88">[88]</ref>, <ref type="bibr" target="#b89">[89]</ref>, <ref type="bibr" target="#b90">[90]</ref> goes back several decades. Earlier work was heavily driven by the fact that Intel SIMD extensions required operands of vector instructions to be contiguous <ref type="bibr" target="#b88">[88]</ref>, <ref type="bibr" target="#b91">[91]</ref>, <ref type="bibr" target="#b92">[92]</ref>. More advanced techniques in this area used polyhedral models to map arbitrary loop nests for SIMD execution <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref> or even consider irregular applications <ref type="bibr" target="#b93">[93]</ref>. Because of our target workloads, where there are relatively fewer options for the computations within one operator, but there can be a very long chain of operators, the challenges we address are related to global optimization, and not dealing with arbitrary loop nests. Previous work on global optimizations for SIMD <ref type="bibr" target="#b94">[94]</ref>, <ref type="bibr" target="#b95">[95]</ref> did not consider a comparable instruction set as ours, and therefore, SIMD instruction selection and associated data layout optimizations were not their focus. Recently, Chen et al. have developed VeGen <ref type="bibr" target="#b30">[31]</ref> that targets the growing diversity in available SIMD and vector instructions. The VeGen compiler extracts what they term as lane-level parallelism by finding the instruction most suitable for a loop (nest). This work, however, does not consider the possibility (and costs) of data transformation to use specific instructions, does not target instructions as complicated as the one we have handled, and there are no global optimizations in their work. In another recent work, a JIT compilation system was presented to use Intel SIMD advances for convolution operations <ref type="bibr" target="#b31">[32]</ref> this work, however, does not consider any layout or global optimizations. VLIW Instruction Packing. VLIW instruction scheduling with timing and resource constraints is a long-standing issue, and many solutions have been proposed for various DSP architectures (that are different from modern mobile DSPs), including advanced software pipelining <ref type="bibr" target="#b96">[96]</ref>, <ref type="bibr" target="#b97">[97]</ref>, <ref type="bibr" target="#b98">[98]</ref>, <ref type="bibr" target="#b99">[99]</ref>. Closely related to this work, Six et al. <ref type="bibr" target="#b59">[59]</ref> discussed a critical path based approach based on a variant of Coffman-Graham list scheduling <ref type="bibr" target="#b100">[100]</ref>. This approach is top-down by leveraging the heuristic that instructions with the longest latency path to the exit have priority. However, our scheduling is bottom-up by considering a heuristic of assigning higher priority to instructions that are on a critical path and can enable more instructions packing if they are packed early.</p><p>More importantly, compared with all existing efforts, GCD 2 categorizes data dependencies and tolerates soft dependencies with advanced hardware support, and focuses on a more domain-specific design for DNN accelerations on mobile DSP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION AND FUTURE WORK</head><p>This paper has presented a compilation system, GCD 2 , for efficiently mapping real-world complex DNN workloads on modern mobile DSP architectures. GCD 2 consists of three major optimizations including the development of matrix layout formats to support novel advanced SIMD instructions in the mobile DSP, a global SIMD optimization procedure that selects optimal SIMD instructions and associated layouts, and an SDA VLIW instruction packing that considers the effect of soft dependencies. GCD 2 is extensively evaluated with ten real-world complex DNNs on popular mobile DSPs. The results show that GCD 2 outperforms two cutting-edge end-to-end DNN execution frameworks supporting mobile DSPs by up to 6.0? and outperforms three established compilers that support efficient computation kernels execution on mobile DSPs by up to 4.5? because of the improved SIMD execution and optimized VLIW instruction scheduling. For certain DNNs, GCD 2 is unique in supporting the realtime execution of the model. For two of these ten models, GCD 2 implementation has, for the first, enabled execution on mobile DSPs. The overall compilation time is also justified. In the future, we plan to design and integrate a more advanced (or customized) Quantization approach <ref type="bibr" target="#b42">[42]</ref> to GCD 2 , and explore DSP-friendly operator fusion <ref type="bibr" target="#b63">[63]</ref> to further improve the performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>3 2 1 0 (c) 4 -</head><label>04</label><figDesc>column layout example (instruction: vrmpy).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Data Layouts to Support Usage of Varied SIMD Instructions for Matrix Multiplication. Each number denotes the linear storage offset of an element. A blue, yellow, and orange cell takes 1, 2, and 4 bytes, respectively. Left shows data storage, and right shows computation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Examples of Computational Graphs. Left and right show partial CGs in ResNet<ref type="bibr" target="#b33">[34]</ref> and TinyBERT<ref type="bibr" target="#b53">[53]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Two Examples of Packing Instructions with Soft Dependencies. Different colors show different VLIW execution pipeline stages (read in green, execute in orange, and write in blue). In (a), the second stage (Assign R1+R2 to R3) of the second instruction requires to wait for the completion of the first instruction, incurring packing penalty. A similar situation happens to (b) between Assign and Store.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: An Instruction Packing Example. The left part shows part of the pseudo assembly code for the innermost nested loop performing 2D Element-wise Addition: R = A + B + C, where A, B, and C are two-dimensional uint8arrays and R is a two-dimensional int16 array. v2:1 denotes a 16-bit register combining 2 8-bit registers v2 and v1. The middle part shows an IDG, in which, solid edges denote hard dependency, dot edges denote soft, and critical path is colored in red. Right shows the packing results from our solution and an sub-optimal solution that treats all soft dependencies as hard (soft to hard). N denotes an empty instruction slot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: System Workflow of GCD 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Performance Comparison of GCD 2 , Halide, TVM, and RAKE with Individual Kernels. Left shows the speedup and right shows the packet counts, both normalizing Halide as 1. Conv2D operators (from ResNet-50) are used. GCD b is a sub-optimal version of GCD 2 that contains tensor optimizations only without VLIW packing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>(a) and (b) show ENT-B0 RNT-50 FST WDSR PixOr 80</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: DSP Utilization and Memory Bandwidth Comparison. These results are as reported by Snapdragon Profiler [77], and normalized with GCD 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Layout Optimization Analysis. X-axis denotes the number of operators in the computational graph. The left figure shows the speedup over local optimal with different numbers of operators. The right figure shows the search time, and its y-axis is logarithmically scaled.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>2 Figure 11 :</head><label>211</label><figDesc>Figure 11: VLIW Scheduling Analysis. The version treating all soft dependencies as hard ones is used as the baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Unrolling Factor Analysis on a Single MatMul Kernel and on Multiple MatMul Kernels. The x-axis in the left figure denotes the unrolling factors. The right figure shows the performance comparison among the best settings of three unrolling strategies (Out, Mid, and GCD 2 ) on 8 operators (from O1 to O8). For comparison, it also shows versions w/o unrolling and w/ exhaustive search.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 13</head><label>13</label><figDesc>Figure13compares the total power consumption and energy efficiency of GCD 2 against TFLite and SNPE also</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Comparison of Total Power Consumption (left) and Energy Efficiency in Inference Frames/Watt (right). Three DSP frameworks and TFLite with GPU back-end on 4 representative DNNs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table II :</head><label>II</label><figDesc>Execution Latency w/ Different SIMD Instructions (and Layouts) for Matrix Multiplication C = A ? B. M, K, and N denote the dimension size of Matrix A (M ? K), B (K ? N), and C (M ? N), respectively. Execution latency and total data size with padding are normalized by vmpy for readability. Smaller numbers mean better latency or less padding. Bold ones denote the best case.</figDesc><table><row><cell>M</cell><cell>K</cell><cell>N</cell><cell cols="6">Execution Latency vmpy vmpa vrmpy vmpy vmpa vrmpy Total Data Size w/ Pad</cell></row><row><cell>32</cell><cell>32</cell><cell>32</cell><cell>1.00</cell><cell>0.79</cell><cell>0.63</cell><cell>1.00</cell><cell>0.56</cell><cell>0.33</cell></row><row><cell>64</cell><cell>64</cell><cell>64</cell><cell>1.00</cell><cell>0.69</cell><cell>0.76</cell><cell>1.00</cell><cell>0.60</cell><cell>0.60</cell></row><row><cell>96</cell><cell>96</cell><cell>96</cell><cell>1.00</cell><cell>1.06</cell><cell>0.89</cell><cell>1.00</cell><cell>1.00</cell><cell>0.82</cell></row><row><cell cols="3">128 128 128</cell><cell>1.00</cell><cell>1.10</cell><cell>1.23</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>s</figDesc><table><row><cell></cell><cell>Byte</cell><cell cols="2">2-Byte</cell><cell cols="2">4-Byte</cell><cell></cell><cell></cell><cell>v /v : i-th element in the vector register</cell></row><row><cell></cell><cell cols="2">High addr.</cell><cell></cell><cell>128</cell><cell cols="2">Low addr.</cell><cell></cell><cell>s : i-th element in the scalar register</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Vector pair</cell></row><row><cell></cell><cell>Vector</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Vector pair</cell><cell></cell><cell>Vector</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b) vmpa</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Figure 1: SIMD/Vector Multiply Instruction Examples in Mobile DSP Chip</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Byte</cell><cell>2-Byte</cell><cell>4-Byte</cell><cell></cell><cell cols="2">Primary computation</cell></row><row><cell>128</cell><cell cols="2">0 128 256 384 1 129 257 385 ? ? ? ? 1 1 1 1</cell><cell cols="2">0 4 8 12</cell><cell>? ? ? ? 1 129 257 385 0 128 256 384</cell><cell cols="3">Input vector 127126125124 ? 4 3 2 1 0 128 128</cell></row><row><cell></cell><cell cols="2">127 255 383 511</cell><cell cols="2">1 5 9 13</cell><cell>127 255 383 511</cell><cell cols="2">Splat one element</cell><cell>0 0 0 0</cell></row><row><cell>128</cell><cell cols="2">512 640 768 896 513 641 769 897 ? ? ? ?</cell><cell cols="2">2 6 10 14 3 7 11 15</cell><cell>512 640 768 896 ? ? ? ? 513 641 769 897</cell><cell cols="2">126 Output vector pair 124</cell><cell>?</cell><cell>2</cell><cell>0</cell></row><row><cell></cell><cell cols="2">639 767 895 1023</cell><cell></cell><cell></cell><cell>639 767 895 1023</cell><cell>127</cell><cell>125</cell><cell>?</cell><cell>3</cell><cell>1</cell></row><row><cell></cell><cell>Input</cell><cell></cell><cell></cell><cell>Weight</cell><cell>Output</cell><cell></cell><cell cols="2">64</cell></row><row><cell></cell><cell cols="8">(a) 1-column layout example (instruction: vmpy).</cell></row><row><cell></cell><cell>2</cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">128</cell></row><row><cell>64 64</cell><cell cols="2">0, 1 2, 3 ? 126, 127 254, 255 128, 129 130, 131 ? 256, 257 384, 385 258, 259 386, 387 ? ?</cell><cell cols="2">Reorder 3 7 11 15 0 4 8 12 2 6 10 14 1 5 9 13</cell><cell>? 0, 1 2, 3 ? 126, 127 254, 255 ? 128, 129 130, 131 ? 258, 259 386, 387 256, 257 384, 385</cell><cell cols="3">126 127 126 125 124 ? 4 3 2 1 0 124 ? 2 0 Output vector pair 255 254 253 252 ? 132 131 130 129 128 Weight scalar 2 1 Input vector pair 3 2 1 0</cell></row><row><cell></cell><cell cols="2">382, 383 510, 511</cell><cell></cell><cell></cell><cell>382, 383 510, 511</cell><cell>126</cell><cell>124</cell><cell>?</cell><cell>2</cell><cell>0</cell></row><row><cell></cell><cell>Input</cell><cell></cell><cell cols="2">Weight</cell><cell>Output</cell><cell></cell><cell cols="2">64</cell></row><row><cell></cell><cell cols="8">(b) 2-column layout example (instruction: vmpa).</cell></row><row><cell></cell><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">0, 1, 2, 3</cell><cell></cell><cell></cell><cell>0, 1, 2, 3</cell><cell></cell><cell cols="2">128</cell></row><row><cell>32</cell><cell cols="2">4, 5, 6, 7 ?</cell><cell cols="2">0 4 8 12</cell><cell>4, 5, 6, 7 ?</cell><cell cols="3">127 126 125 124 ? 4 3 2 1 0 Input vector</cell></row><row><cell></cell><cell cols="2">124, 125, 126, 127 128, 129, 130, 131</cell><cell cols="2">1 5 9 13 2 6 1014</cell><cell>124, 125, 126, 127 128, 129, 130, 131</cell><cell cols="2">Weight scalar</cell></row><row><cell>32</cell><cell cols="2">132, 133, 134, 135 ?</cell><cell cols="2">3 7 11 15</cell><cell>132, 133, 134, 135 ?</cell><cell cols="2">31 Output vector</cell><cell>?</cell><cell>0</cell></row><row><cell></cell><cell cols="2">252, 253, 254, 255</cell><cell></cell><cell></cell><cell>252, 253, 254, 255</cell><cell></cell><cell></cell><cell>32</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table III :</head><label>III</label><figDesc>SIMD Instructions Selected and Performance by RAKE<ref type="bibr" target="#b3">[4]</ref> and GCD 2 . Representative Conv2d kernels (w/ varied shapes, 7?7, 1?1, and 3?3) are from ResNet-50.</figDesc><table><row><cell cols="2">Conv2d properties</cell><cell>Instruction</cell><cell>Speedup</cell></row><row><cell cols="3">Input shape Weight shape Output shape RAKE Ours</cell><cell>Ours/RAKE</cell></row><row><cell>1x3x224x224 64x3x7x7</cell><cell cols="2">1x64x112x112 vrmpy vmpy</cell><cell>1.63x</cell></row><row><cell cols="2">1x64x56x56 64x64x1x1 1x64x56x56</cell><cell>vmpy vmpa</cell><cell>1.98x</cell></row><row><cell cols="3">1x128x28x28 128x128x3x3 1x128x28x28 vrmpy vmpy</cell><cell>2.06x</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Read ad from RF Load into R1</figDesc><table><row><cell></cell><cell cols="2">Execution pipelines</cell></row><row><cell></cell><cell>Stage: read</cell><cell>Stage: execute</cell><cell>Stage: write</cell></row><row><cell>R1 load(ad)</cell><cell></cell><cell></cell></row><row><cell>R3 R2 + R1</cell><cell>Read R2 from RF</cell><cell cols="2">Assign R1+R2 to R3 Write R3 to RF</cell></row><row><cell></cell><cell cols="2">(a) Read after loading</cell></row><row><cell>R3 R1 + R2</cell><cell cols="2">Read R1 from RF Assign R1 + R2 to R3</cell><cell>Write R3 to RF</cell></row><row><cell>store(R3, ad)</cell><cell>Read R2 from RF</cell><cell>Store R3</cell></row><row><cell></cell><cell>Read ad from RF</cell><cell></cell></row><row><cell></cell><cell cols="2">(b) Store after writing</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Algorithm 1 :</head><label>1</label><figDesc>Soft-dependency-aware VLIW Packing Func packing: instructions ? [Packet] 1 cfg ? build cfg(instructions) 2 all packets ? Stack() 3 foreach block in cfg.block do Instruction 20 all insts ? resource constraint(free insts, packet) 21 if all insts is empty then</figDesc><table><row><cell>4</cell><cell>idg ? build IDG(block)</cell></row><row><cell>5</cell><cell>free insts ? Set()</cell></row><row><cell>6</cell><cell>find free instruction(idg, free insts)</cell></row><row><cell>7</cell><cell>while free insts is not empty do</cell></row><row><cell>8</cell><cell>/ * Build critical path from IDG * / critical path ? get critical path(idg)</cell></row><row><cell>9</cell><cell>cur packet ? critical path[-1]</cell></row><row><cell>10</cell><cell>/ * Iterate all the free instruction * / while len(cur packet) ? 4 do</cell></row><row><cell></cell><cell>/ * Select the most profitable</cell></row><row><cell>11</cell><cell>instruction * / inst ? select instruction(free insts, cur packet)</cell></row><row><cell>12</cell><cell>find free instruction(idg, free insts)</cell></row><row><cell>13</cell><cell>if inst is None then</cell></row><row><cell>14</cell><cell>break</cell></row><row><cell>15</cell><cell>else</cell></row><row><cell>16</cell><cell>cur packet.add(inst)</cell></row></table><note><p>17 idg.remove(inst) 18 all packet.add(cur packet) 19 return all packets Func select instruction: free insts, packet ? 22 return NULL 23 hi lat ? highest latency(packet) 24 best ? NULL 25 foreach i in all insts do / * The criteria of profitability * / 26 i.score ? (i.order + i.pred) ?w -abs(hi lat -i.lat) ?(1w) 27 if soft dependency(i, packet) then 28 i.score ? i.score -p(i, packet) 29 if best is NULL or best.score ? i.score then 30 best ? i 31 return best 16-bit register combining 2 8-bit registers (v2 and v1) to store the addition result.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table IV :</head><label>IV</label><figDesc>Overall Performance Comparison among TFLite, SNPE, and GCD 2 on Mobile DSP. "-" means this model is not supported by the framework yet. OverT and OverS are the speedup of GCD 2 over TFLite, and SNPE, respectively. GCD 2 's overall compilation time for these models ranges from 5 minutes (WDSR-b) to 25 minutes (EfficientDet-d0).</figDesc><table><row><cell>Model</cell><cell>Type</cell><cell>Task</cell><cell cols="8">#MACS #Params #Operators TFLite (ms) SNPE (ms) GCD 2 (ms) OverT OverS</cell></row><row><cell>MobileNet-V3</cell><cell>2D CNN</cell><cell>Classification</cell><cell>0.22G</cell><cell>5.5M</cell><cell>193</cell><cell>7.5</cell><cell>6.2</cell><cell>4.0</cell><cell>1.9</cell><cell>1.6</cell></row><row><cell>EfficientNet-b0</cell><cell>2D CNN</cell><cell>Classification</cell><cell>0.40G</cell><cell>4M</cell><cell>254</cell><cell>9.1</cell><cell>9.2</cell><cell>6.0</cell><cell>1.5</cell><cell>1.5</cell></row><row><cell>ResNet-50</cell><cell>2D CNN</cell><cell>Classification</cell><cell>4.1G</cell><cell>25.5M</cell><cell>140</cell><cell>13.9</cell><cell>11.6</cell><cell>7.1</cell><cell>2.0</cell><cell>1.6</cell></row><row><cell>FST</cell><cell>2D CNN</cell><cell>Style transfer</cell><cell>161G</cell><cell>1.7M</cell><cell>64</cell><cell>935</cell><cell>870</cell><cell>211</cell><cell>4.4</cell><cell>4.1</cell></row><row><cell>CycleGAN</cell><cell>GAN</cell><cell>Image translation</cell><cell>186G</cell><cell>11M</cell><cell>84</cell><cell>450</cell><cell>366</cell><cell>181</cell><cell>2.5</cell><cell>2.0</cell></row><row><cell>WDSR-b</cell><cell>2D CNN</cell><cell>Super resolution</cell><cell>11.5G</cell><cell>22.2K</cell><cell>32</cell><cell>400</cell><cell>137</cell><cell>66.7</cell><cell>6.0</cell><cell>2.1</cell></row><row><cell>EfficientDet-d0</cell><cell>2D CNN</cell><cell>2D object detection</cell><cell>2.6G</cell><cell>4.3M</cell><cell>822</cell><cell>62.8</cell><cell>-</cell><cell>26</cell><cell>2.4</cell><cell>-</cell></row><row><cell>PixOr</cell><cell>2D CNN</cell><cell>3D object detection</cell><cell>8.8G</cell><cell>2.1M</cell><cell>150</cell><cell>43</cell><cell>26.4</cell><cell>11.7</cell><cell>3.7</cell><cell>2.3</cell></row><row><cell>TinyBERT</cell><cell>Transformer</cell><cell>NLP</cell><cell>1.4G</cell><cell>4.7M</cell><cell>211</cell><cell>-</cell><cell>-</cell><cell>12.2</cell><cell>-</cell><cell>-</cell></row><row><cell>Conformer</cell><cell cols="2">Transformer Speech recognition</cell><cell>5.6G</cell><cell>1.2M</cell><cell>675</cell><cell>-</cell><cell>-</cell><cell>65</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Speedup (geometric mean)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2.8</cell><cell>2.1</cell></row><row><cell cols="3">C0 C1 C2 C3 C4 C5 C6 C7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>2 achieves up to 1.51 TOPS for an individual layer in DNN inference. Considering the necessary data loading and memory latency costs involved, this value shows effective practical use of the hardware.</figDesc><table><row><cell>2.0</cell><cell cols="2">Local GCD 2 (13)</cell><cell>Global GCD 2 (17)</cell></row><row><cell>1.5 Speedup</cell><cell></cell><cell></cell><cell></cell></row><row><cell>1.0</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>10</cell><cell>15</cell><cell>20</cell><cell>25</cell></row><row><cell></cell><cell></cell><cell cols="2">(a) Speedup.</cell></row><row><cell>C. Impact of Opt. and Algorithmic Features</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Impact of Different Optimizations. To understand how</cell><cell></cell><cell></cell><cell></cell></row><row><cell>different optimizations (instruction and layout selection,</cell><cell></cell><cell></cell><cell></cell></row><row><cell>VLIW packing, and other optimizations) contribute towards</cell><cell></cell><cell></cell><cell></cell></row><row><cell>performance speedups, Figure 9 (a) studies the impact of</cell><cell></cell><cell></cell><cell></cell></row><row><cell>these optimizations with 5 representative models that cover</cell><cell></cell><cell></cell><cell></cell></row><row><cell>2D CNN, GAN, and Transformer (EfficientNet-B0 (ENT-B0),</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResNet-50 (RNT-50), FST, WDSR, and PixOr). We evaluate</cell><cell></cell><cell></cell><cell></cell></row><row><cell>each compiler-based optimization speedup incrementally over</cell><cell></cell><cell></cell><cell></cell></row><row><cell>our baseline (w/o proposed optimizations). Compared with</cell><cell></cell><cell></cell><cell></cell></row><row><cell>No opt, instruction and layout selection brings 1.4? to</cell><cell></cell><cell></cell><cell></cell></row><row><cell>2.9? gains, VLIW scheduling achieves additional 1.2? to</cell><cell></cell><cell></cell><cell></cell></row><row><cell>2.0? speedup, and finally, other optimizations (e.g., replacing</cell><cell></cell><cell></cell><cell></cell></row><row><cell>an expensive division operation with a database lookup) add</cell><cell></cell><cell></cell><cell></cell></row><row><cell>1.1? to 1.4? speedup. Figure 9 (b) and Figure 9 (c) further</cell><cell></cell><cell></cell><cell></cell></row><row><cell>reveal that instruction and layout selection also has the largest</cell><cell></cell><cell></cell><cell></cell></row><row><cell>impact on DSP utilization and memory bandwidth.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Instruction (and Layout) Selection Analysis. This section</cell><cell></cell><cell></cell><cell></cell></row><row><cell>justifies the choice we have made in performing global</cell><cell></cell><cell></cell><cell></cell></row><row><cell>layout selection. Specifically, we compare the algorithm</cell><cell></cell><cell></cell><cell></cell></row><row><cell>used in GCD 2 with two baselines -local optimal</cell><cell></cell><cell></cell><cell></cell></row><row><cell>and exhaustive search based global optimal solutions.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>The local optimal solution selects the layout with the best</cell><cell></cell><cell></cell><cell></cell></row><row><cell>performance independently for each operator, whereas the</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table V :</head><label>V</label><figDesc>Inference Speed and Energy Efficiency Comparison with ResNet-50 on EdgeTPU<ref type="bibr" target="#b78">[78]</ref> and NVIDIA Jetson Xavier<ref type="bibr" target="#b79">[79]</ref>. FPS is short for frames per second, and FPW represents for inference frames per Watt.</figDesc><table><row><cell>Platform</cell><cell>Device</cell><cell>FPS</cell><cell>Power</cell><cell>FPW</cell></row><row><cell>EdgeTPU [78]</cell><cell>Edge TPU (int8)</cell><cell>17.8</cell><cell>2 W</cell><cell>8.9</cell></row><row><cell cols="2">Jetson Xavier [79] GPU + DLA (fp16)</cell><cell>291</cell><cell>?30 W</cell><cell>9.7</cell></row><row><cell>Jetson Xavier [79]</cell><cell>GPU + DLA (int8)</cell><cell cols="2">1100 ?30 W</cell><cell>36.7</cell></row><row><cell>GCD 2</cell><cell>DSP (int8)</cell><cell>141</cell><cell>2.6 W</cell><cell>54.2</cell></row></table><note><p>executed on DSP ( * -DSP) on four representative DNN models ( EfficientNet-b0, ResNet-50, PixOr, and Cycle-GAN). As additional baseline, TFLite on a mobile GPU, Qualcomm Adreno 650 GPU on the same Snapdragon 865</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Qualcomm Snapdragon is one of the most popular SoC and many generations of Snapdragon are equipped with Hexagon DSPs. Although our presentation and evaluation is on Hexagon DSP, the work is generally applicable to other mobile DSPs as well, e.g., Cadence, which is the other major player in the mobile DSP market.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>Authorized licensed use limited to: Tsinghua University. Downloaded on January 01,2024 at 08:18:59 UTC from IEEE Xplore. Restrictions apply.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>This classification is independent of the traditional classification of dependencies into flow/RAW, output/WAW, and anti/WAR, though soft dependencies can only be RAW or WAR.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>According to the target microarchitecture design, each VLIW pipeline execution comprises three stages, read from Register File (RF), execute, and write to RF, though some of these stage can be be empty. Our explanations will assume that each stage is 1 clock cycle<ref type="bibr" target="#b58">[58]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>Mobile DSP processors (e.g., Hexagon DSPs) execute instructions within each VLIW packet in parallel, but without overlap between packets.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENT</head><p>The authors would like to thank the anonymous reviewers for their constructive comments and helpful suggestions. This work was supported in part by <rs type="funder">National Science Foundation (NSF)</rs> under the awards of CCF-2047516 (CAREER), <rs type="grantNumber">CCF-2146873</rs>, <rs type="grantNumber">CCF-2232813</rs>, <rs type="grantNumber">CCF-2146852</rs>, <rs type="grantNumber">CCF-2131509</rs>, <rs type="grantNumber">CCF-2034850</rs>, and <rs type="grantNumber">CCF-2007793</rs>, and <rs type="funder">Army Research Office/Army Research Laboratory</rs> via grant <rs type="grantNumber">W911-NF-20-1-0167</rs> to <rs type="institution">Northeastern University</rs>. Any errors and opinions are not those of the <rs type="funder">NSF</rs>, <rs type="funder">Army Research Office, or Department of Defense</rs>, and are attributable solely to the author(s).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_jJHQDpN">
					<idno type="grant-number">CCF-2146873</idno>
				</org>
				<org type="funding" xml:id="_xeNGgeg">
					<idno type="grant-number">CCF-2232813</idno>
				</org>
				<org type="funding" xml:id="_n9R9NhW">
					<idno type="grant-number">CCF-2146852</idno>
				</org>
				<org type="funding" xml:id="_n9pYWCY">
					<idno type="grant-number">CCF-2131509</idno>
				</org>
				<org type="funding" xml:id="_jjKEWhH">
					<idno type="grant-number">CCF-2034850</idno>
				</org>
				<org type="funding" xml:id="_Ua7ADHU">
					<idno type="grant-number">CCF-2007793</idno>
				</org>
				<org type="funding" xml:id="_DHvJ7NA">
					<idno type="grant-number">W911-NF-20-1-0167</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Extending halide to improve software development for imaging dsps</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vocke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Corporaal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jordans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Corvino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Architecture and Code Optimization (TACO)</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The evaluation of dcnn on vector-simd dsp</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="22" to="301" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vectorization for digital signal processors via equality saturation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vanhattum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bornholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sampson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="874" to="886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Vector instruction selection for digital signal processors using program synthesis</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B S</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Root</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kamil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cheung</surname></persName>
		</author>
		<idno type="DOI">10.1145/3503222.3507714</idno>
		<ptr target="https://doi.org/10.1145/3503222.3507714" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ser. ASPLOS 2022</title>
		<meeting>the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ser. ASPLOS 2022<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1004" to="1016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Tvm: An automated end-to-end optimizing compiler for deep learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="578" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Automatic kernel generation for volta tensor cores</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Bhaskaracharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Demouth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Grover</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.12645</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Vasilache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Zinenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Theodoridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Moses</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Verdoolaege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04730</idno>
		<title level="m">Tensor comprehensions: Framework-agnostic highperformance machine learning abstractions</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Swirl: High-performance many-core cpu code generation for deep neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Venkat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rusira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Truong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of High Performance Computing Applications</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1275" to="1289" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Taso: optimizing deep learning computation with automatic generation of graph substitutions</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Padon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Warszawski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aiken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM Symposium on Operating Systems Principles</title>
		<meeting>the 27th ACM Symposium on Operating Systems Principles</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="47" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ios: Inter-operator scheduler for cnn acceleration</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pekhimenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deftnn: Addressing bottlenecks for dnn execution on gpus via synapse vector elimination and near-compute data fission</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zamirai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Laurenzano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mahlke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 50th Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="786" to="799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A flexible approach to autotuning multipass machine learning compilers</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Phothilimthana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sabne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sarda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Angermueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burrows</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mandke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Farahani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 30th International Conference on Parallel Architectures and Compilation Techniques (PACT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deepx: A software accelerator for low-power deep learning inference on mobile devices</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Georgiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Forlivesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qendro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kawsar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 15th ACM/IEEE International Conference on Information Processing in Sensor Networks</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI 2016. USA: USENIX Association</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mnn: A universal and efficient inference engine</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Dhillon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Papailiopoulos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Sze</surname></persName>
		</editor>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deepcache: Principled cache for mobile deep vision</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Annual International Conference on Mobile Computing and Networking, ser. MobiCom &apos;18</title>
		<meeting>the 24th Annual International Conference on Mobile Computing and Networking, ser. MobiCom &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="129" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deepmon: Mobile gpu-based deep learning framework for continuous vision applications</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Balan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Annual International Conference on Mobile Systems, Applications, and Services (MobiSys)</title>
		<meeting>the 15th Annual International Conference on Mobile Systems, Applications, and Services (MobiSys)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="82" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deepsense: A unified deep learning framework for timeseries mobile sensing data processing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Abdelzaher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web, ser. WWW &apos;17. Republic and Canton of Geneva, CHE: International World Wide Web Conferences Steering Committee</title>
		<meeting>the 26th International Conference on World Wide Web, ser. WWW &apos;17. Republic and Canton of Geneva, CHE: International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="351" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mcdnn: An approximation-based execution framework for deep stream processing under resource constraints</title>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Philipose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wolman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Annual International Conference on Mobile Systems, Applications, and Services (MobiSys)</title>
		<meeting>the 14th Annual International Conference on Mobile Systems, Applications, and Services (MobiSys)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="123" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Profiling and optimizing deep learning inference on mobile gpus</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th ACM SIGOPS Asia-Pacific Workshop on Systems</title>
		<meeting>the 11th ACM SIGOPS Asia-Pacific Workshop on Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="75" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Asymo: scalable and efficient deep-learning inference on asymmetric mobile cpus</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Annual International Conference on Mobile Computing and Networking</title>
		<meeting>the 27th Annual International Conference on Mobile Computing and Networking</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="215" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">nn-meter: towards accurate latency prediction of deep-learning model inference on diverse edge devices</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services</title>
		<meeting>the 19th Annual International Conference on Mobile Systems, Applications, and Services</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="81" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Optimizing grouped convolutions on edge devices</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>O'boyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 31st International Conference on Application-specific Systems, Architectures and Processors</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="189" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deepear: robust smartphone audio sensing in unconstrained acoustic environments using deep learning</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Georgiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qendro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM international joint conference on pervasive and ubiquitous computing</title>
		<meeting>the 2015 ACM international joint conference on pervasive and ubiquitous computing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="283" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Morph: Flexible acceleration for 3d cnn-based video understanding</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Fletcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 51st Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="933" to="946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A learned performance model for tensor processing units</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Phothilimthana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mendis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sabne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burrows</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Towards a uniform template-based architecture for accelerating 2d and 3d cnns on fpga</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</title>
		<meeting>the 2018 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="97" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Akg: Automatic kernel generation for neural processing units using polyhedral transformations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<idno type="DOI">10.1145/3453483.3454106</idno>
		<ptr target="https://doi-org.proxy.wm.edu/10.1145/3453483.3454106" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation, ser. PLDI 2021</title>
		<meeting>the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation, ser. PLDI 2021<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1233" to="1248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Polyhedral-model guided loop-nest auto-vectorization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Trifunovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nuzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zaks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Rosen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 18th International Conference on Parallel Architectures and Compilation Techniques</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="327" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">When polyhedral transformations meet simd code generation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Veras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Franchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-N</forename><surname>Pouchet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sadayappan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th ACM SIGPLAN conference on Programming language design and implementation</title>
		<meeting>the 34th ACM SIGPLAN conference on Programming language design and implementation</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="127" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Vegen: a vectorizer generator for simd and beyond</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mendis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amarasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="902" to="914" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Anatomy of highperformance deep learning convolutions on simd architectures</title>
		<author>
			<persName><forename type="first">E</forename><surname>Georganas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Avancha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalamkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pabst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Heinecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC18: International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="830" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pixor: Real-time 3d object detection from point clouds</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7652" to="7660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Snpe</title>
		<author>
			<persName><surname>Qualcomm</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName><surname>Available</surname></persName>
		</author>
		<ptr target="https://developer.qualcomm.com/software/qualcomm-neural-processing-sdk" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Halide: A language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amarasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PLDI 2013</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="519" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Snapdragon 865</title>
		<author>
			<persName><surname>Qualcomm</surname></persName>
		</author>
		<ptr target="https://www.qualcomm.com/products/snapdragon-865-5g-mobile-platform" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<ptr target="https://www.qualcomm.com/products/snapdragon-820-mobile-platform" />
		<title level="m">Snapdragon 820</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Available</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Theoretical speed of hexagon dsp</title>
		<ptr target="https://en.wikipedia.org/wiki/QualcommHexagon" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Automatic generation of high-performance quantized machine learning kernels</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bornholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ceze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM/IEEE International Symposium on Code Generation and Optimization</title>
		<meeting>the 18th ACM/IEEE International Symposium on Code Generation and Optimization</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="305" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Bipointnet: Binary neural network for point clouds</title>
		<author>
			<persName><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.05501</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Hexagon nn library</title>
		<author>
			<persName><surname>Qualcomm</surname></persName>
		</author>
		<ptr target="https://developer.qualcomm.com/software/hexagon-dsp-sdk" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Mobisr: Efficient on-device super-resolution through heterogeneous mobile processors</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Venieris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dudziak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 25th Annual International Conference on Mobile Computing and Networking</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Quantization and training of neural networks for efficient integer-arithmetic-only inference</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kligys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2704" to="2713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Efficient execution of quantized deep learning models: A compiler approach</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Masuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10226</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Adabits: Neural network quantization with adaptive bit-widths</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2146" to="2156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">I-bert: Integer-only bert quantization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.01321</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Overflow aware quantization: Accelerating neural network inference by lowbit multiply-accumulate operations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="868" to="875" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A synthesis-aided compiler for dsp architectures (wip paper)</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vanhattum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bornholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sampson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 21st ACM SIGPLAN/SIGBED Conference on Languages, Compilers, and Tools for Embedded Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="131" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Tinybert: Distilling bert for natural language understanding</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.10351</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Optimal dnn primitive selection with partitioned boolean quadratic programming</title>
		<author>
			<persName><forename type="first">A</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gregg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 International Symposium on Code Generation and Optimization</title>
		<meeting>the 2018 International Symposium on Code Generation and Optimization</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="340" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Nearly optimal register allocation with pbqp</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hames</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scholz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Modular Languages Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Compiler optimization on vliw instruction scheduling for low power</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Design Automation of Electronic Systems (TODAES)</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="252" to="268" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Instruction scheduling using max-min ant system optimization</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kastner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM Great Lakes symposium on VLSI</title>
		<meeting>the 15th ACM Great Lakes symposium on VLSI</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="44" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Hexagon v66 manual</title>
		<author>
			<persName><surname>Qualcomm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Available: QualcommHexagonV66Programmer&apos;sReferenceManual</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Certified and efficient instruction scheduling: application to interlocked vliw processors</title>
		<author>
			<persName><forename type="first">C</forename><surname>Six</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boulm?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Monniaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OOPSLA</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Global instruction scheduling for superscalar machines</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rodeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGPLAN 1991 conference on Programming language design and implementation</title>
		<meeting>the ACM SIGPLAN 1991 conference on Programming language design and implementation</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="241" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Vectorization-aware loop unrolling with seed forwarding</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Porpodas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Petoumenos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>G?es</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Leather</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Compiler Construction</title>
		<meeting>the 29th International Conference on Compiler Construction</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Patdnn: Achieving real-time dnn execution on mobile devices with pattern-based weight pruning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="907" to="922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Dnnfusion: accelerating deep neural networks execution with advanced operator fusion</title>
		<author>
			<persName><forename type="first">W</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation</title>
		<meeting>the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="883" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Cocopie: enabling real-time ai on off-theshelf mobile devices via compression-compilation co-design</title>
		<author>
			<persName><forename type="first">H</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="62" to="68" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">"</forename><surname>Llvm</surname></persName>
		</author>
		<author>
			<persName><surname>Llvm</surname></persName>
		</author>
		<ptr target="https://llvm.org/" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Searching for mobilenetv3</title>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Wide activation for efficient and accurate image superresolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.08718</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Efficientdet: Scalable and efficient object detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">790</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Conformer: Convolution-augmented transformer for speech recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.08100</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Post-training quantization</title>
		<author>
			<persName><surname>Tensorflow</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/lite/performance/posttrainingquantization" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Dataset and study</title>
		<author>
			<persName><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2017-07">July 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research (IJRR)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Librispeech: An asr corpus based on public domain audio books</title>
		<author>
			<persName><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Snapdragon profiler</title>
		<author>
			<persName><surname>Qualcomm</surname></persName>
		</author>
		<ptr target="https://developer.qualcomm.com/software/snapdragon-profiler" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Available</note>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Snapdragon 820</title>
		<author>
			<persName><surname>Google</surname></persName>
		</author>
		<ptr target="https://coral.ai/docs/edgetpu/benchmarks/" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Measuring inference performance of machine-learning frameworks on edge-class devices with the mlmark benchmark</title>
		<author>
			<persName><forename type="first">P</forename><surname>Torelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bangale</surname></persName>
		</author>
		<ptr target="https://www.eembc.org/techlit/articles/MLMARK-WHITEPAPERFINAL-1.pdf" />
		<imprint>
			<date type="published" when="2021-04-05">5 April 2021</date>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Techincal Report</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Accpar: Tensor partitioning for heterogeneous deep learning accelerators</title>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="342" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">an optimizing compiler for the tms320c25 dsp chip</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Signal Processing Applicat</title>
		<meeting>Int. Conf. Signal essing Applicat</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page">689</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Cathedralii: A silicon compiler for digital signal processing</title>
		<author>
			<persName><forename type="first">H</forename><surname>De Man</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rabaey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Six</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Claesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Design &amp; Test of Computers</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="13" to="25" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Dsp processor/compiler co-design: a quantitative approach</title>
		<author>
			<persName><forename type="first">V</forename><surname>Zivojnovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schlager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Willems</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schoenen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Meyr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 9th International Symposium on Systems Synthesis</title>
		<meeting>9th International Symposium on Systems Synthesis</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="108" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Spl: A language and compiler for dsp algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Padua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="298" to="308" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Compiler based exploration of dsp energy savings by simd operations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Marwedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Drager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fettweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Leupers</surname></persName>
		</author>
		<idno>04EX753</idno>
	</analytic>
	<monogr>
		<title level="m">ASP-DAC 2004: Asia and South Pacific Design Automation Conference</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="page" from="839" to="842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">A retargetable vliw compiler framework for dsps with instruction-level parallelism</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rajagopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Takayama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1319" to="1328" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Enabling compiler flow for embedded vliw dsp processors with distributed register files</title>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-H</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-P</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 ACM SIGPLAN/SIGBED conference on Languages, compilers, and tools for embedded systems</title>
		<meeting>the 2007 ACM SIGPLAN/SIGBED conference on Languages, compilers, and tools for embedded systems</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="146" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Auto-vectorization of interleaved data for simd</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nuzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zaks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="132" to="143" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Exploiting vector instructions with generalized stream fusion</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mainland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Leshchinskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications of the ACM</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="83" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Program generation for small-scale linear algebra applications</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fabregat-Traver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bientinesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>P?schel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 International Symposium on Code Generation and Optimization</title>
		<meeting>the 2018 International Symposium on Code Generation and Optimization</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="327" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Efficient utilization of simd extensions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Franchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Ueberhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="409" to="425" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Optimizing data permutations for simd devices</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Padua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="118" to="131" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Exploiting recent simd architectural advances for irregular applications</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE/ACM International Symposium on Code Generation and Optimization</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="47" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Goslp: Globally optimized superword level parallelism framework</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mendis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amarasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OOPSLA</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Improving the effectiveness of searching for isomorphic chains in superword level parallelism</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tuck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 50th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="718" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Synthesis and optimization of digital circuits</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">De</forename><surname>Micheli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>McGraw Hill</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Instruction scheduling for clustered vliw dsps</title>
		<author>
			<persName><forename type="first">R</forename><surname>Leupers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 2000 International Conference on Parallel Architectures and Compilation Techniques</title>
		<meeting>2000 International Conference on Parallel Architectures and Compilation Techniques</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="291" to="300" />
		</imprint>
	</monogr>
	<note type="report_type">Cat. No. PR00622</note>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Formal verification of translation validators: a case study on instruction scheduling optimizations</title>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Tristan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Leroy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th annual ACM SIGPLAN-SIGACT symposium on Principles of programming languages</title>
		<meeting>the 35th annual ACM SIGPLAN-SIGACT symposium on Principles of programming languages</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="17" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">A simple, verified validator for software pipelining</title>
	</analytic>
	<monogr>
		<title level="j">ACM Sigplan Notices</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="83" to="92" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">From machine scheduling to vliw instruction scheduling</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>De Dinechin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ST Journal of Research</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
