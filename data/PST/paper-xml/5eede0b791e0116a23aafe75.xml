<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Temporal Graph Networks for Deep Learning on Dynamic Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Emanuele</forename><surname>Rossi</surname></persName>
							<email>erossi@twitter.com</email>
						</author>
						<author>
							<persName><forename type="first">Ben</forename><surname>Chamberlain</surname></persName>
							<email>bchamberlain@twitter.com</email>
						</author>
						<author>
							<persName><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
							<email>ffrasca@twitter.com</email>
						</author>
						<author>
							<persName><forename type="first">Davide</forename><surname>Eynard</surname></persName>
							<email>deynard@twitter.com</email>
						</author>
						<author>
							<persName><forename type="first">Federico</forename><surname>Monti</surname></persName>
							<email>fmonti@twitter.com</email>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
							<email>mbronstein@twitter.com</email>
						</author>
						<title level="a" type="main">Temporal Graph Networks for Deep Learning on Dynamic Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs) have recently become increasingly popular due to their ability to learn complex systems of relations or interactions arising in a broad spectrum of problems ranging from biology and particle physics to social networks and recommendation systems. Despite the plethora of different models for deep learning on graphs, few approaches have been proposed thus far for dealing with graphs that present some sort of dynamic nature (e.g. evolving features or connectivity over time). In this paper, we present Temporal Graph Networks (TGNs), a generic, efficient framework for deep learning on dynamic graphs represented as sequences of timed events. Thanks to a novel combination of memory modules and graph-based operators, TGNs are able to significantly outperform previous approaches being at the same time more computationally efficient. We furthermore show that several previous models for learning on dynamic graphs can be cast as specific instances of our framework. We perform a detailed ablation study of different components of our framework and devise the best configuration that achieves state-of-the-art performance on several transductive and inductive prediction tasks for dynamic graphs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the past few years, graph representation learning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b3">4]</ref> has produced a sequence of successes, gaining increasing popularity in machine learning. Graphs are ubiquitously used as models for systems of relations and interactions in many fields <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b52">53]</ref>, in particular, social sciences <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b42">43]</ref> and biology <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b17">18]</ref>. Learning on such data is possible using graph neural networks (GNNs) <ref type="bibr" target="#b25">[26]</ref> that typically operate by a message passing mechanism <ref type="bibr" target="#b3">[4]</ref> aggregating information in a neighborhood of a node and create node embeddings that are then used for node-wise classification <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b34">35]</ref> or edge prediction <ref type="bibr" target="#b71">[72]</ref> tasks.</p><p>The majority of methods for deep learning on graphs assume that the underlying graph is static. However, most real-life systems of interactions such as social networks or biological interactomes are dynamic. While it is often possible to apply static graph deep learning models <ref type="bibr" target="#b36">[37]</ref> to dynamic graphs by ignoring the temporal evolution, this has been shown to be sub-optimal <ref type="bibr" target="#b65">[66]</ref>, and in some cases, it is the dynamic structure that contains crucial insights about the system. Learning on dynamic graphs is relatively recent, and most works are limited to the setting of discrete-time dynamic graphs represented as a sequence of snapshots of the graph over time <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b70">71]</ref>. Few approaches support the inductive setting of generalizing to new nodes not seen during training <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b35">36]</ref>. Such approaches are unsuitable for interesting real world settings such as social networks, where dynamic graphs are continuous (i.e. edges can appear at any time) and evolving (i.e. new nodes join the graph continuously).</p><p>Contributions. In this paper, we first propose the generic inductive framework of Temporal Graph Networks (TGNs) operating on continuous-time dynamic graphs represented as a sequence of events, and show that many previous methods are specific instances of TGNs. Second, we propose a novel training strategy allowing the model to learn from the sequentiality of the data while maintaining highly efficient parallel processing. We show that this leads to an order of magnitude speed up over previous methods. Third, we perform a detailed ablation study of different components of our framework and analyze the tradeoff between speed and accuracy. Finally, we show state-of-the-art performance on multiple tasks and datasets in both transductive and inductive settings, while being much faster than previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Deep learning on static graphs. A static graph G = (V, E) comprises nodes V = {1, . . . , n} and edges E ? V ? V, which are endowed with features, denoted by v i and e ij for all i, j = 1, . . . , n, respectively. A typical graph neural network (GNN) creates an embedding z i of the nodes by learning a local aggregation rule of the form</p><formula xml:id="formula_0">z i = j?Ni h(m ij , v i ) m ij = msg(v i , v j , e ij ),</formula><p>which is interpreted as message passing from the neighbors j of i. Here, N i = {j : (i, j) ? E} denotes the neighborhood of node i and msg and h are learnable functions.</p><p>Dynamic Graphs. There exist two main classes of dynamic graphs. Discrete-time dynamic graphs (DTDG) are sequences of static graph snapshots taken at intervals in time. Continuos-time dynamic graphs (CTDG) are more general and can be represented as timed lists of events, which may include edge addition or deletion, node addition or deletion and node or edge feature transformations. In this paper, we do not consider deletion events.</p><p>Our temporal (multi-)graph is modeled as a sequence of time-stamped events G = {x(t 1 ), x(t 2 ), . . .}, representing addition or change of a node or interaction between a pair of nodes at times 0 ? t 1 ? t 2 ? . . .. An event x(t) can be of two types: 1) A node-wise event is represented by v i (t), where i denotes the index of the node and v is the vector attribute associated with the event. After its first appearance, a node is assumed to live forever and its index is used consistently for the following events. 2) An interaction event between nodes i and j is represented by a (directed) temporal edge e ij (t) (there might be more than one edge between a pair of nodes, so technically this is a multigraph). We denote by</p><formula xml:id="formula_1">V(T ) = {i : ?v i (t) ? G, t ? T } and E(T ) = {(i, j) : ?e ij (t) ? G, t ? T }</formula><p>the temporal set of vertices and edges, respectively, and by</p><formula xml:id="formula_2">N i (T ) = {j : (i, j) ? E(T )} the neighborhood of node i in time interval T . N k i (T ) denotes the k-hop neighborhood. A snapshot of the temporal graph G at time t is the (multi-)graph G(t) = (V[0, t], E[0, t]) with n(t) nodes.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Temporal Graph Networks</head><p>Following the terminology in <ref type="bibr" target="#b31">[32]</ref>, a neural model for dynamic graphs can be regarded as an encoder-decoder pair, where an encoder is a function that maps from a dynamic graph to node embeddings, and a decoder takes as input one or more node embeddings and makes a prediction based on these, e.g. node classification or edge prediction. The key contribution of this paper is a novel Temporal Graph Network (TGN) encoder applied on a continuous-time dynamic graph represented as a sequence of time-stamped events and producing, for each time t, the embedding of the graph nodes Z t) = (z 1 (t), . . . , z n(t) (t) . Top: using the embedding module to compute the temporal node embeddings and subsequently the loss function. Bottom: memory update from batch interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Core modules</head><p>Memory. The memory (state) of the model at time t consists of a vector s i (t) for each node i the model has seen so far. The memory of a node is updated when the node is involved in an event (e.g. interaction with an other node or node-wise change), and its purpose is to represent the history of a node in a compressed format. Thanks to this specific module, TGNs have the capability to memorize long term dependencies for each node in the graph.</p><p>In addition, a global memory can be added to the model to track the evolution of the entire temporal network. While we envisage the benefits such a memory could bring (e.g. information can easily travel long distances in the graph, nodes' memory can be updated w.r.t the changes in global state, easy graph-wise predictions based on global memory), such a direction has not been explored in this work and it is as such left to future research.</p><p>Message Function. For each event involving node i, a message is computed to update i's memory. In the case of an interaction event e ij (t) between nodes i and j at time t, two messages can be computed for the source and target nodes that respectively start and receive the interaction:</p><formula xml:id="formula_3">m i (t) = msg s s i (t -), s j (t -), t, e ij (t) , m j (t) = msg d s j (t -), s i (t -), t, e ij (t)<label>(1)</label></formula><p>Similarly, in case of a node-wise event v i (t), a single message can be computed for the node involved in the event:</p><formula xml:id="formula_4">m i (t) = msg n s i (t -), t, v i (t) .<label>(2)</label></formula><p>Here, s i (t -) is the memory of node i just before time t, and msg s , msg d and msg n are learnable message functions, e.g. MLPs. In all our experiments, we chose the message function as identity (id), which is simply the concatenation of the inputs, for the sake of simplicity.</p><p>Message Aggregator. Resorting to batch processing for efficiency reasons may lead to multiple events involving the same node i in the same batch. As each event generates a message in our formulation, we use a mechanism to aggregate messages m i (t 1 ), . . . , m i (t b ) for t 1 , . . . , t b ? t,</p><formula xml:id="formula_5">mi (t) = agg (m i (t 1 ), . . . , m i (t b )) .<label>(3)</label></formula><p>Here, agg is an aggregation function. While multiple choices can be considered for implementing this module (e.g. RNNs or attention w.r.t. the node memory), for the sake of simplicity we considered two efficient non-learnable solutions in our experiments: most recent message (keep only most recent message for a given node) and mean message (average all messages for a given node). We leave learnable aggregation as a future research direction.</p><p>Memory Updater. As previously mentioned, the memory of a node is updated upon each event involving the node itself:</p><formula xml:id="formula_6">s i (t) = mem mi (t), s i (t -) .<label>(4)</label></formula><p>For interaction events involving two nodes i and j, the memories of both nodes are updated after the event has happened. For node-wise events, only the memory of the related node is updated. Here, mem is a learnable memory update function, e.g. a recurrent neural network such as LSTM <ref type="bibr" target="#b28">[29]</ref> or GRU <ref type="bibr" target="#b8">[9]</ref>.</p><p>Embedding. The embedding module is used to generate the temporal embedding z i (t) of node i at any time t. The main goal of the embedding module is to avoid the so-called memory staleness problem <ref type="bibr" target="#b31">[32]</ref>. Since the memory of a node i is updated only when the node is involved in an event, it might happen that, in the absence of events for a long time (e.g. a social network user who stops using the platform for some time before becoming active again), i's memory becomes stale. While multiple implementations of the embedding module are possible, we use the form:</p><formula xml:id="formula_7">z i (t) = emb(i, t) = j?N k i ([0,t]) h (s i (t), s j (t), e ij , v i (t), v j (t)) ,</formula><p>where h is a learnable function. This includes many different formulations as particular cases:</p><formula xml:id="formula_8">Identity (id): emb(i, t) = s i (t)</formula><p>, which uses the memory directly as the node embedding.</p><p>Time projection (time):</p><formula xml:id="formula_9">emb(i, t) = (1 + ?t w) ? s i (t)</formula><p>, where w are learnable parameters, ?t is the time since the last interaction, and ? denotes element-wise vector product. This version of the embedding method was used in JODIE <ref type="bibr" target="#b35">[36]</ref>.</p><p>Temporal Graph Attention (attn): A series of L graph attention layers compute i's embedding by aggregating information from its L-hop temporal neighborhood. The input to the l-th layer is i's representation h</p><formula xml:id="formula_10">(l-1) i (t), the current timestamp t, i's neighborhood representation {h (l-1) 1 (t), . . . , h<label>(l-1) N</label></formula><p>(t)} together with timestamps t 1 , . . . , t N and features e i1 (t 1 ), . . . , e iN (t N ) for each of the considered interactions which form an edge in i's temporal neighborhood:</p><formula xml:id="formula_11">h (l) i (t) = MLP (l) (h (l-1) i (t) h(l) i (t)),<label>(5) h</label></formula><formula xml:id="formula_12">(l) i (t) = MultiHeadAttention (l) (q (l) (t), K (l) (t), V (l) (t)),<label>(6)</label></formula><formula xml:id="formula_13">q (l) (t) = h (l-1) i (t) ?(0),<label>(7)</label></formula><formula xml:id="formula_14">K (l) (t) = V (l) (t) = C (l) (t),<label>(8)</label></formula><formula xml:id="formula_15">C (l) (t) = [h (l-1) 1 (t) e i1 (t 1 ) ?(t -t 1 ), . . . , h (l-1) N (t) e iN (t N ) ?(t -t N )].<label>(9)</label></formula><p>Here, ?(?) represents a generic time encoding <ref type="bibr" target="#b65">[66]</ref>, is the concatenation operator and</p><formula xml:id="formula_16">z i (t) = emb(i, t) = h (L) i (t).</formula><p>Each layer amounts to performing multi-head-attention <ref type="bibr" target="#b59">[60]</ref> where the query (q (l) (t)) is a reference node (i.e. the target node or one of its L -1-hop neighbors), and the keys K (l) (t) and values V (l) (t) are its neighbors. Finally, an MLP is used to combine the reference node representation with the aggregated information. Differently from the original formulation of this layer (firstly proposed in TGAT <ref type="bibr" target="#b65">[66]</ref>) where no node-wise temporal features were used, in our case the input representation of each node h (0) j (t) = s j (t) + v j (t) and as such it allows the model to exploit both the current memory s j (t) and the temporal node features v j (t).</p><p>Temporal Graph Sum (sum): A simpler and faster aggregation over the graph:</p><formula xml:id="formula_17">h (l) i (t) = MLP (l) (h (l-1) i (t) h(l) i (t)),<label>(10) h</label></formula><formula xml:id="formula_18">(l) i (t) = j?Ni([0,t]) h (l-1) j (t) e ij ?(t -t j ).<label>(11)</label></formula><p>Here as well, ?(?) is a time encoding and</p><formula xml:id="formula_19">z i (t) = emb(i, t) = h (L) i (t).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training</head><p>Our TGN model can be trained for a variety of tasks such as future edge prediction (self-supervised setting) or node classification (semi-supervised setting). We present two possible training procedures Right: Advanced training strategy. m_raw(t) is the raw message generated by event e(t), t is the instant of time of the last event involving each node, and t -the one immediately preceding t.</p><formula xml:id="formula_20">!(#) !(# % ) &amp;(#) '() *)) '+' +', -<label>(#</label></formula><p>for TGNs while using the link prediction task as a simple example: provided a list of ordered timed interactions, the goal of the model is to predict the future interactions from those observed in the past.</p><p>Both training procedures are detailed in Algorithms 1 and 2, and Figure <ref type="figure" target="#fig_1">2</ref> depicts how TGN modules are combined.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> shows that interactions serve two purposes: 1) they are the training objective, 2) they are used to update the memory. While the interactions in a batch cannot be used to update the memory before predicting the same interactions (as this would leak information), reversing the order of the operations, i.e. predicting the interactions and computing the loss before updating the memory, causes all memory-related modules (Message Function, Message Aggregator, and Memory Updater) not to receive a gradient (Algorithm 1). Therefore, extra steps must be taken in order to train these modules.</p><p>Algorithm Basic training strategy. The simplest strategy keeps the same order of operations as Algorithm 1 (predict interactions, then update memory), but breaks every batch<ref type="foot" target="#foot_1">2</ref> of size b into k sub-batches of size b/k. The sub-batches are processed sequentially with their losses accumulated and backpropagation is only performed after the last sub-batch. If a node appears in two sub-batches, its memory in the second sub-batch will depend on the computation done by the memory-related modules in the first. Therefore, these modules will receive a gradient.</p><p>Advanced training strategy. While the basic training procedure is straightforward to implement, it presents two drawbacks: 1) it slows down the training, as each batch is not computed fully in parallel, 2) the only nodes that contribute to the memory-related modules' gradients are those with at least one interaction in multiple sub-batches. Therefore, these modules can still receive no gradient if sub-batches do not share any nodes, or the gradient can be heavily skewed towards a few nodes that appear multiple times, leading to biased update steps and ultimately to a sub-optimal local minimum for the overall training procedure.</p><p>The solution to this problem is to reverse the order of operations. Let ti be the time of node i's last interaction in its last sub-batch b i ( ti ). Instead of letting the memory be representative of the entire set of interactions involving i in the past, we store memory s i ( ti ), i.e. the state of i prior to the last subbatch b i ( ti ), together with the raw information we need to update s i ( ti ) with the interactions of b i ( ti ) (i.e. the set of raw update messages {(s i ( ti ), s j ( ti ), e ij (t), t) ?e ij (t) ? b i ( ti )} of i's interactions in b i ( ti )). At the beginning of each sub-batch, the model first updates the nodes' memories by computing and aggregating messages from the stored raw information (line 11 of Algorithm 2), then uses the updated memory to infer the embeddings and computes the loss function (Figure <ref type="figure" target="#fig_1">2 right</ref>). As a result, the loss function depends on a memory which has just been updated by its related modules. Moreover, all nodes involved in the computation of the embeddings (i.e. all source and target nodes and related neighbors) contribute to the gradients, ultimately producing more stable optimization and better local minima (Figure <ref type="figure" target="#fig_4">3b</ref>).</p><p>While the advanced training strategy is sufficient to train TGNs, it can also be combined with the basic strategy by breaking each batch into sub-batches. We investigate the speed vs accuracy tradeoff of different combinations of the two strategies in Section 5. Early models for learning on dynamic graphs focused on Discrete Time Dynamic Graphs (DTDG)s. Such approaches either aggregate graph snapshots and then apply static methods <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2]</ref>, assemble snapshots into tensors and factorize <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b38">39]</ref>, or encode each snapshot to produce a series of embeddings. In the latter case, the embeddings are either aggregated by taking a weighted sum <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b73">74]</ref>, fit to time series models <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b43">44]</ref>, used as components in RNNs <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b47">48]</ref>, or learned by imposing a smoothness constraint over time <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b49">50]</ref>. Another line of work encodes DTDGs by first performing random walks on an initial snapshot and then modifying the walk behaviour for subsequent snapshots <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b70">71]</ref>.</p><p>Only recently have Continuous Time Dynamic Graphs (CTDGs) been addressed. Several approaches use random walk models <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b2">3]</ref> that incorporate continuous time through constraints on transition probabilities. Sequence-based approaches for CTDGs <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b37">38]</ref> use RNNs to update representations of the source and destination node each time a new edge appears. Other recent works have focused on dynamic knowledge graphs <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>Most recent CTDG learning models can be interpreted as specific cases of our framework (see Table <ref type="table" target="#tab_2">1</ref>). For example, Jodie <ref type="bibr" target="#b35">[36]</ref> uses the time projection embedding module emb(i, t) = (1+?tw)?s i (t). TGAT <ref type="bibr" target="#b65">[66]</ref> is a specific case of TGN when the memory and its related modules are missing, and graph attention is used as the Embedding module. Finally, we note that TGN generalizes the Graph Networks (GN) model <ref type="bibr" target="#b3">[4]</ref> for static graphs (with the exception of the global block that we mentioned before), and thus the majority of existing message passing-type architectures.</p><p>For additional background, we refer the reader to surveys on general graph representation learning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b3">4]</ref> and the recent survey on dynamic graph learning <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Settings</head><p>Datasets. We use three datasets in our experiments: Wikipedia, Reddit <ref type="bibr" target="#b35">[36]</ref>, and Twitter. Reddit and Wikipedia are bipartite interaction graphs. In the Reddit dataset, users and sub-reddits are nodes, and an interaction occurs when a user writes a post to the sub-reddit. In the Wikipedia dataset, users and pages are nodes, and an interaction represents a user editing a page. In both aforementioned datasets, the interactions are represented by text features (of a post or page edit, respectively), and labels represent whether a user is banned. Both interactions and labels are time-stamped.</p><p>The Twitter dataset is a non-bipartite graph released as part of the 2020 RecSys Challenge <ref type="bibr" target="#b5">[6]</ref>. Nodes are users and interactions are retweets. The features of an interaction are a BERT-based <ref type="bibr" target="#b62">[63]</ref> vector representation of the text of the retweet. We use a subset of the original dataset formed by taking the largest connected component of the retweet graph and retaining only the nodes with the 5000 highest in degrees and 5000 highest out degrees. Dataset statistics together with more details are provided in the supplementary material.</p><p>Tasks. Our experimental setup closely follows <ref type="bibr" target="#b65">[66]</ref> and focuses on the tasks of future edge prediction and dynamic node classification. On the former, we use both the transductive and inductive settings. In the transductive task, we predict future links of nodes which were observed during training, whereas in the inductive tasks we predict future links of nodes never observed before. The transductive setting is used for node classification. We perform the same 70%-15%-15% chronological split as in <ref type="bibr" target="#b65">[66]</ref>.</p><p>Future Edge Prediction. The goal is to predict the probability of an edge occurring between two nodes at a given time. Our encoder is combined with a simple MLP decoder mapping from the concatenation of two node embeddings to the probability of the edge. For the Wikipedia and Reddit datasets, we use Adam optimizer with a learning rate of 0.0001, a batch size of 200 for both training, validation and testing, and early stopping with a patience of 5. For the Twitter dataset, the only change is the learning rate, which is set to 0.00005. We sample an equal amount of negatives to the positive interactions, and use average precision as reference metric. All results are averaged over 10 runs to obtain mean and standard deviation. Dynamic Node Classification. The task is to predict a binary label indicating whether a user was banned at a specific time. We pre-train our encoder on the future edge prediction task, then freeze it and combine it with a task-specific MLP decoder. We use the Adam optimizer with a learning rate of 0.0003 and a batch size of 100 for both training, validation and testing. The metric used is the ROC-AUC. All results are averaged over 10 runs to obtain mean and standard deviation.</p><p>Baselines. Our strong baselines are state-of-the-art approaches for continuous time dynamic graphs (CTDNE <ref type="bibr" target="#b46">[47]</ref>, Jodie <ref type="bibr" target="#b35">[36]</ref>, and TGAT <ref type="bibr" target="#b65">[66]</ref>) as well as state-of-the-art models for static graphs (GAE <ref type="bibr" target="#b33">[34]</ref>, VGAE <ref type="bibr" target="#b33">[34]</ref>, DeepWalk <ref type="bibr" target="#b50">[51]</ref>, Node2Vec <ref type="bibr" target="#b22">[23]</ref>, GAT <ref type="bibr" target="#b60">[61]</ref> and GraphSAGE <ref type="bibr" target="#b26">[27]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance</head><p>Results. Table <ref type="table" target="#tab_3">2</ref> presents the results on future edge prediction. Our model clearly outperforms the baselines by a large margin in both the transductive and the inductive setting on all datasets. The gap is particularly large on the Twitter dataset, where we outperfom the second-best method (TGAT) by over 25%. Table <ref type="table" target="#tab_4">3</ref> shows the results on dynamic node classification, where again our model obtains state-of-the-art results, with a large improvement over all other methods.</p><p>Speed. Due to the efficient parallel processing and the need for only one graph attention layer (see section 5.3 for the ablation study on the number of layers), our model is up to 3? faster than Jodie and about 19? faster than TGAT to complete a single epoch (see Figure <ref type="figure" target="#fig_4">3a</ref>), while requiring a similar number of epochs to converge.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Choice of Modules</head><p>We perform a detailed ablation study comparing different instances of our TGN framework. We are particular interested in the speed vs accuracy tradeoff resulting from the choice of modules and their combination. The variants we experiment with are reported in Table <ref type="table" target="#tab_2">1</ref> and their results are depicted in Figure <ref type="figure" target="#fig_4">3a</ref>.</p><p>Memory. We compare a model that does not make use of a memory (TGN-no-mem), with a model which uses memory (TGN-attn) but is otherwise identical. While TGN-att is about 3? slower, it vastly outperforms TGN-no-mem, confirming the importance of memory for effective learning on dynamic graphs, due to its ability to store long-term information about a node which is otherwise hard to capture.</p><p>Embedding Module. We compared models with different embedding modules (TGN-id, TGNtime, TGN-attn, TGN-sum). The first interesting insight is that projecting the embedding in time seems to slightly hurt, as shown by the fact that TGN-time underperforms TGN-id. Moreover, the ability to exploit the graph is crucial for performance: we note that all graph-based projections (TGN-attn, TGN-sum) outperform the graph-less TGN-id model by a large margin, with TGN-attn being the top performer at the expense of being slightly slower than the simpler TGN-sum.</p><p>Message Aggregator. We compared two models, one using the most last message aggregator (TGN-attn) and another a mean aggregator (TGN-mean-aggr) but otherwise the same. While TGNmean-aggr performs slightly better, it is more than 3? slower.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of layers.</head><p>While in TGAT having 2 layers is of fundamental importance for obtaining good performances (TGAT vs TGAT-1l), in TGN the presence of the memory makes it enough to use 1 layer to obtain very high performances (TGN-attn vs TGN-2l). This is probably because when accessing the memory of the 1-hop neighbors, we are indirectly accessing information from hops further away. Moreover, being able to use only 1 layer of graph attention speeds up the model dramatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Training Strategies</head><p>Table <ref type="table">4</ref> shows the configurations we experimented with for this ablation study, while figure <ref type="figure" target="#fig_4">3b</ref> presents the results. The TGN-id model makes only use of the memory (no embedding module) and therefore makes for a perfect testbed for training strategies related to the memory-related modules. Looking at the results with the TGN-id model, our proposed strategy of updating the memory at the start of the epoch clearly outperforms updating at the end. Interestingly, when using a graph attention embedding module (TGN-attn), the benefit of the advanced strategy shrinks. This is probably due to the fact that the embedding module is able is able to adapt to the random memory-related modules, effectively denoising the spurious behavior of the nodes' memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We introduce TGN, a generic framework for learning on continuous-time dynamic graphs. We obtain state-of-the-art results on several tasks and datasets while being faster than previous methods. Detailed ablation studies shows the importance of the memory and its related modules to store long-term information, as well as the importance of the graph-based embedding module to generate up-to-date node embeddings. We envision interesting applications of TGN in the fields of social sciences, recommender systems, and biological interaction networks, opening up a future research direction of exploring more advanced settings of our model and understanding the most appropriate domain-specific choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>Graph Neural Networks able to effectively process temporal graphs can potentially serve a variety of purposes in our society e.g. improved recommender systems that take into account the evolving nature of users on social networks or marketplaces, as well as better filtering mechanisms for the detection of unhealthy behaviors such as spam or coordinate manipulation. At the same time, due to the novelty of these approaches, the robustness of such architectures w.r.t. external adversarial attacks has not been validated yet in the literature. Additional studies will thus need to be realised to identify the potential risks and benefits that temporal graph neural networks may present when subjected to adversarial attacks, before these can be applied to sensitive personal data and extensively exploited in industrial applications. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Settings for Baselines</head><p>Our results for GAE <ref type="bibr" target="#b33">[34]</ref>, VGAE <ref type="bibr" target="#b33">[34]</ref>, DeepWalk <ref type="bibr" target="#b50">[51]</ref>, Node2Vec <ref type="bibr" target="#b22">[23]</ref>, GAT <ref type="bibr" target="#b60">[61]</ref> and GraphSAGE <ref type="bibr" target="#b26">[27]</ref>, CTDNE <ref type="bibr" target="#b46">[47]</ref> and TGAT <ref type="bibr" target="#b65">[66]</ref> are taken directly from the TGAT paper <ref type="bibr" target="#b65">[66]</ref>.</p><p>For Jodie <ref type="bibr" target="#b35">[36]</ref>, we implement our own version in PyTorch, as a specific case of our framework with the temporal embedding module, and the t-batch training algorithm.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Two flows of operations for processing a batch of time-stamped interactions using TGN. Top: using the embedding module to compute the temporal node embeddings and subsequently the loss function. Bottom: memory update from batch interactions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Two implementations of TGN with different memory updates. Left: Basic training strategy.Right: Advanced training strategy. m_raw(t) is the raw message generated by event e(t), t is the instant of time of the last event involving each node, and t -the one immediately preceding t.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Tradeoff between accuracy (test average precision in %) and speed (time per epoch in sec) of different models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>-s5 TGN-attn-e1 TGN-attn-e5 (b) Tradeoff between accuracy (test average precision in %) and speed (time per epoch in sec) of different training strategies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Ablation studies on the Wikipedia dataset for the transductive setting. Means and standard deviations (visualized as ellipses) were computed over 10 runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Previous models for deep learning on continuous-time dynamic graphs are specific case of our TGN framework. Shown are multiple variants of TGN used in our ablation studies. method (l,n) refers to graph convolution using l layers and n neighbors. ? uses t-batches. * uses uniform sampling of neighbors, while the default is sampling the most recent neighbors.</figDesc><table><row><cell></cell><cell cols="2">Mem. Mem. Update</cell><cell>Embedding</cell><cell cols="2">Mess. Agg. Mess. Func.</cell></row><row><cell>JODIE</cell><cell>node</cell><cell>RNN</cell><cell>time</cell><cell>- ?</cell><cell>id</cell></row><row><cell>TGAT</cell><cell>-</cell><cell>-</cell><cell>attn (2l, 20n)  *</cell><cell>-</cell><cell>-</cell></row><row><cell>TGN-attn</cell><cell>node</cell><cell>GRU</cell><cell>attn (1l, 10n)</cell><cell>last</cell><cell>id</cell></row><row><cell>TGN-2l</cell><cell>node</cell><cell>GRU</cell><cell>attn (2l, 10n)</cell><cell>last</cell><cell>id</cell></row><row><cell>TGN-no-mem</cell><cell>-</cell><cell>-</cell><cell>attn (1l, 10n)</cell><cell>-</cell><cell>id</cell></row><row><cell>TGN-time</cell><cell>node</cell><cell>GRU</cell><cell>time</cell><cell>last</cell><cell>id</cell></row><row><cell>TGN-id</cell><cell>node</cell><cell>GRU</cell><cell>id</cell><cell>last</cell><cell>id</cell></row><row><cell>TGN-sum</cell><cell>node</cell><cell>GRU</cell><cell>sum (1l, 10n)</cell><cell>last</cell><cell>id</cell></row><row><cell>TGN-mean</cell><cell>node</cell><cell>GRU</cell><cell>attn (1l, 10n)</cell><cell>mean</cell><cell>id</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Average Precision (%) for future edge prediction task in transductive and inductive settings. Mean and standard deviations are computed over 10 runs. GraphSAGE * 93.56 ? 0.3 91.09 ? 0.3 97.65 ? 0.2 96.27 ? 0.2 65.79 ? 0.6 60.13 ? 0.6 JODIE 94.33 ? 0.4 91.29 ? 0.5 96.36 ? 0.5 94.62 ? 0.5 62.05 ? 1.0 52.72 ? 1.6 TGAT 95.34 ? 0.1 93.99 ? 0.3 98.12 ? 0.2 96.62 ? 0.3 67.84 ? 0.6 62.21 ? 0.6 TGN-attn 98.64 ? 0.1 98.05 ? 0.1 98.80 ? 0.1 97.71 ? 0.1 93.66 ? 1.3 90.16 ? 2.4</figDesc><table><row><cell>Does not support</cell></row></table><note><p>* Static graph method. ?</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>ROC AUC % for the dynamic node classification. Mean and standard deviations are computed over 10 runs.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="4">Table 4: Different settings of combinations of</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">models and training strategies. #sb is number</cell></row><row><cell></cell><cell></cell><cell></cell><cell>of sub-batches.</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Wikipedia</cell><cell>Reddit</cell><cell>Setting</cell><cell>Model</cell><cell cols="2">Update #sb</cell></row><row><cell>GAE  *</cell><cell cols="2">74.85 ? 0.6 58.39 ? 0.5</cell><cell>TGN-id-s1</cell><cell>TGN-id</cell><cell>start</cell><cell>1</cell></row><row><cell>VAGE  *</cell><cell cols="2">73.67 ? 0.8 57.98 ? 0.6</cell><cell>TGN-id-s5</cell><cell>TGN-id</cell><cell>start</cell><cell>5</cell></row><row><cell>GAT  *</cell><cell cols="2">82.34 ? 0.8 64.52 ? 0.5</cell><cell>TGN-id-e1</cell><cell>TGN-id</cell><cell>end</cell><cell>1</cell></row><row><cell cols="3">GraphSAGE  *  82.42 ? 0.7 61.24 ? 0.6</cell><cell>TGN-id-e5</cell><cell>TGN-id</cell><cell>end</cell><cell>5</cell></row><row><cell>CTDNE</cell><cell cols="2">75.89 ? 0.5 59.43 ? 0.6</cell><cell cols="2">TGN-att-s1 TGN-att</cell><cell>start</cell><cell>1</cell></row><row><cell>JODIE</cell><cell cols="2">87.17 ? 0.5 59.50 ? 2.1</cell><cell cols="2">TGN-att-s5 TGN-att</cell><cell>start</cell><cell>5</cell></row><row><cell>TGAT</cell><cell cols="2">83.69 ? 0.7 65.56 ? 0.7</cell><cell cols="2">TGN-att-e1 TGN-att</cell><cell>end</cell><cell>1</cell></row><row><cell>TGN-attn</cell><cell cols="2">88.56 ? 0.3 68.63 ? 0.7</cell><cell cols="2">TGN-att-e5 TGN-att</cell><cell>end</cell><cell>5</cell></row></table><note><p>* Static graph method.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Statistics of the datasets used in the experiments.</figDesc><table><row><cell></cell><cell>Wikipedia</cell><cell>Reddit</cell><cell>Twitter</cell></row><row><cell># Nodes</cell><cell>9,227</cell><cell>11,000</cell><cell>8,926</cell></row><row><cell># Edges</cell><cell>157,474</cell><cell>672,447</cell><cell>130,865</cell></row><row><cell># Edge features</cell><cell>172</cell><cell>172</cell><cell>768</cell></row><row><cell># Edge features type</cell><cell>LIWC</cell><cell>LIWC</cell><cell>BERT</cell></row><row><cell>Timespan</cell><cell>30 days</cell><cell>30 days</cell><cell>7 days</cell></row><row><cell>Chronological Split</cell><cell cols="3">70%-15%-15% 70%-15%-15% 70%-15%-15%</cell></row><row><cell cols="2"># Nodes with dynamic labels 217</cell><cell>366</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Model Hyperparameters.</figDesc><table><row><cell></cell><cell>Value</cell></row><row><cell>Memory Dimension</cell><cell>172</cell></row><row><cell cols="2">Node Embedding Dimension 100</cell></row><row><cell cols="2">Time Embedding Dimension 100</cell></row><row><cell># Attention Heads</cell><cell>2</cell></row><row><cell>Dropout</cell><cell>0.1</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Preprint. Under review.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>By 'batch' we refer to what is sometime defined as mini-batch, i.e. a subset of the original dataset, which is used for mini-batch gradient descent.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_2"><p>For the sake of clarity, we use the same message function for both sources and destination.<ref type="bibr" target="#b2">3</ref> We denote with emb ? an embedding layer that operates on the updated version of the memory ?.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>The statistics of the three datasets used are reported in table <ref type="table">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyperparameters</head><p>For all the models and datasets we used the same hyperparameters, which had been found to work well in the TGAT paper <ref type="bibr" target="#b65">[66]</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An efficient algorithm for link prediction in temporal uncertain social networks</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">331</biblScope>
			<biblScope unit="page" from="120" to="136" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sampling-based algorithm for link prediction in temporal networks</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">374</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">evolve2vec: Learning network representations using temporal unfolding</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bastas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Semertzidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Axenopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Daras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia Modeling</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="447" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<title level="m">Relational inductive biases, deep learning, and graph networks</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Interaction networks for learning about objects, relations and physics</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4502" to="4510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Privacy-preserving recommender systems challenge on twitter&apos;s home timeline</title>
		<author>
			<persName><forename type="first">L</forename><surname>Belli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Ktena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lung-Yut-Fon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Portman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Deli?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.13715</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Gc-lstm: Graph convolution embedded lstm for dynamic link prediction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04206</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graph neural networks for icecube signal classification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Choma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gerhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Palczewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ronaghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prabhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bhimji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMLA</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Time series based link prediction</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silva</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B C</forename><surname>Prud?ncio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCNN</title>
		<imprint>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2012">2012</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">HyTE: Hyperplane-based temporally aware knowledge graph embedding</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2001">2001-2011, 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Combining temporal aspects of dynamic networks with node2vec for a more efficient dynamic link prediction</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">De</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Decuypere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mitrovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Baesens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">De</forename><surname>Weerdt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASONAM</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1234" to="1241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dynamic network embedding: An extended approach for skip-gram based network embedding</title>
		<author>
			<persName><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2086" to="2092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Temporal link prediction using matrix and tensor factorizations</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Dunlavy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Kolda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Acar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDD</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bombarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Relationship prediction in dynamic heterogeneous information networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Fard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="19" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deciphering interaction fingerprints from protein molecular surfaces using geometric deep learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gainza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="184" to="192" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning sequence encoders for temporal knowledge graph completion</title>
		<author>
			<persName><forename type="first">A</forename><surname>Garc?a-Dur?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Duman?i?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4816" to="4821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brubaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Poupart</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.03143</idno>
		<title level="m">Diachronic embedding for temporal knowledge graph completion</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Dyngem: Deep embedding method for dynamic graphs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kamra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11273,abs/1805.11273</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;16, KDD &apos;16</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Link prediction using time series of neighborhood-based node similarity scores</title>
		<author>
			<persName><forename type="first">?</forename><surname>G?ne?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?</forename><surname>G?nd?z-?g?d?c?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>?ataltepe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="147" to="180" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Evolutionary clustering and analysis of bibliographic networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASONAM</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="63" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05584</idno>
		<title level="m">Representation learning on graphs: Methods and applications</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semi-supervised graph embedding approach to dynamic link prediction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hisano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Springer Proceedings in Complexity</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="109" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11">Nov. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The time-series link prediction problem with applications in communication surveillance</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">INFORMS Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="286" to="303" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Link prediction in dynamic social networks by integrating different types of information</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M A</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="738" to="750" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Representation learning for dynamic graphs: A survey</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kobyzev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Poupart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">70</biblScope>
			<biblScope unit="page" from="1" to="73" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A particle-and-density based evolutionary clustering method for dynamic networks</title>
		<author>
			<persName><forename type="first">M.-S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="622" to="633" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Bayesian Deep Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Predicting dynamic embedding trajectory in temporal interaction networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;19</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1269" to="1278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The link-prediction problem for social networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liben-Nowell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Soc. Inf. Sci. Technol</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1019" to="1031" />
			<date type="published" when="2007-05">May 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.10627</idno>
		<title level="m">Streaming graph neural networks</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Embedding models for episodic knowledge graphs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Daxberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Web Semantics</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page">100490</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">dynnode2vec: Scalable dynamic network embedding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mahdavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khoshraftar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Big Data</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3762" to="3765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dynamic graph convolutional networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Manessi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Manzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page">107000</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rodol?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mannion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.06673</idno>
		<title level="m">Fake news detection on social media using geometric deep learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A novel time series link prediction method: Learning automata approach</title>
		<author>
			<persName><forename type="first">B</forename><surname>Moradabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Meybodi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica A: Statistical Mechanics and its Applications</title>
		<imprint>
			<biblScope unit="volume">482</biblScope>
			<biblScope unit="page" from="422" to="432" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning graph dynamics using deep neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Roe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IFAC-PapersOnLine</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="433" to="438" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dynamic network embeddings: From random walks to temporal random walks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Boaz Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Big Data</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1085" to="1092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<title level="m">Continuous-time dynamic network embeddings. In WWW &apos;18</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="969" to="976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Pareja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Domeniconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Suzumura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kanezashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Leisersen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10191</idno>
		<title level="m">Evolvegcn: Evolving graph convolutional networks for dynamic graphs</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Disease prediction using graph convolutional networks: Application to autism spectrum disorder and alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">S</forename><surname>Parisot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Ktena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ferrante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Guerrero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med Image Anal</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="117" to="130" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Node classification in dynamic social networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fletcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pechenizkiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AALTD</title>
		<imprint>
			<biblScope unit="page">54</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;14</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning human-object interactions by graph parsing neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="401" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">ncrna classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In KDD Workshop on Deep Learning on Graphs</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Dysat: Deep neural representation learning on dynamic graphs via self-attention networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="519" to="527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Structured sequence modeling with graph convolutional recurrent networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="page" from="362" to="373" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Temporal-relational classifiers for prediction in evolving domains</title>
		<author>
			<persName><forename type="first">U</forename><surname>Sharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="540" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Node embedding over temporal graphs</title>
		<author>
			<persName><forename type="first">U</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Radinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="4605" to="4612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Know-evolve: Deep temporal reasoning for dynamic knowledge graphs</title>
		<author>
			<persName><forename type="first">R</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3462" to="3471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Dyrep: Learning representations over dynamic graphs</title>
		<author>
			<persName><forename type="first">R</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Biswal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">U</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Hyperfoods: Machine intelligent mapping of cancer-beating molecules in foods</title>
		<author>
			<persName><forename type="first">K</forename><surname>Veselkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brew</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03771</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">An adaptive random walk sampling method on dynamic community detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="10" to="19" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nayyeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Alkhoury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Yazdi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.07893</idno>
		<title level="m">Temporal knowledge graph completion based on time series gaussian embedding</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Inductive representation learning on temporal graphs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Korpeoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Achan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Link prediction based on common-neighbors for dynamic social network</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Procedia Computer Science</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="82" to="89" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;18</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">3d graph convolutional networks with temporal graphs: A spatial information free framework for traffic forecasting</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.00919</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Link prediction with spatial and temporal consistency in dynamic networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3343" to="3349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Netwalk: A flexible deep embedding approach for anomaly detection in dynamic networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;18</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2672" to="2681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Link prediction based on graph neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Dynamic network embedding by modeling triadic closure process</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">A hybrid time-series link prediction framework for large social network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Chin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DEXA</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="345" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">What to do next: Modeling user behaviors by time-lstm</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="3602" to="3608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Modeling polypharmacy side effects with graph convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="457" to="466" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
