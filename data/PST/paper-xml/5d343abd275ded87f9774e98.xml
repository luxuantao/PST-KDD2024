<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transparent, Scrutable and Explainable User Models for Personalized Recommendation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Krisztian</forename><surname>Balog</surname></persName>
							<email>krisztianb@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Filip</forename><surname>Radlinski</surname></persName>
							<email>filiprad@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Shushan</forename><surname>Arakelyan</surname></persName>
							<email>shushan@isi.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<settlement>Google London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<settlement>Google London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">USC Information Sciences Institute Marina Del Rey</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Transparent, Scrutable and Explainable User Models for Personalized Recommendation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2EF1FC3D7AE2604FC924641BAC6BFB91</idno>
					<idno type="DOI">10.1145/3331184.3331211</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Recommendations</term>
					<term>explainability</term>
					<term>transparency</term>
					<term>scrutability</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most recommender systems base their recommendations on implicit or explicit item-level feedback provided by users. These item ratings are combined into a complex user model, which then predicts the suitability of other items. While effective, such methods have limited scrutability and transparency. For instance, if a user's interests change, then many item ratings would usually need to be modified to significantly shift the user's recommendations. Similarly, explaining how the system characterizes the user is impossible, short of presenting the entire list of known item ratings. In this paper, we present a new set-based recommendation technique that permits the user model to be explicitly presented to users in natural language, empowering users to understand recommendations made and improve the recommendations dynamically. While performing comparably to traditional collaborative filtering techniques in a standard static setting, our approach allows users to efficiently improve recommendations. Further, it makes it easier for the model to be validated and adjusted, building user trust and understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Information systems → Recommender systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The importance of explainable AI has been recognized in recent years <ref type="bibr" target="#b32">[33]</ref>. Recommender systems represent an important branch of AI research and the explainability of recommendations has attracted considerable attention <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b53">54]</ref>. Generally, explanations "seek to show how a recommended item relates to a user's preferences" <ref type="bibr" target="#b50">[51]</ref>. Explanations can serve a multiplicity of aims, including * Work was performed while the author was at Google.</p><p>• You like movies that are tagged as 'action', especially those that are tagged as 'violent', such as Aliens. • You like movies that are tagged as 'twist ending', such as A Pure Formality.</p><p>• You don't like movies that are tagged as 'adventure', unless they are tagged as 'thriller', such as Twister. • You like movies that are tagged as 'cheesy', such as Who Framed Roger Rabbit? • You like movies that are tagged as 'australia', such as Crocodile Dundee II. transparency (helping users to understand how the system works), justification (explaining individual recommendations), and scrutability (allowing users to tell the system if it is wrong) <ref type="bibr" target="#b48">[49]</ref>.</p><p>There is an important distinction between transparency and justifications <ref type="bibr" target="#b50">[51]</ref>. The former should give an honest account of how recommendations are selected, while the latter merely gives a plausible description that might be decoupled from the recommendation algorithm. Often, the underlying algorithm is too complex to be described in a human-interpretable manner (e.g., ensemble and deep learning models) or may involve technology that the system provider wishes to protect. Contemporary recommender systems therefore often opt for providing justifications <ref type="bibr" target="#b53">[54]</ref>, rather than offering genuine transparency.</p><p>Scrutability is also lacking in most recommender systems. There is usually little recourse to tell a system if it incorrectly inferred preferences, as the only instruments at the user's disposal are usually removing items from the history and modifying ratings made in the past. It is especially cumbersome to exclude an entire set of items (such as a specific genre in movie recommendations) when, for example, a user's interests shift.</p><p>Thus transparency and scrutability are closely tied together, yet one does not imply the other. So far there have only been preliminary attempts at making explanations both transparent and scrutable <ref type="bibr" target="#b18">[19]</ref>. With this paper, we aim to fill that gap. Our main research question is the following: How much recommendation accuracy would one need to sacrifice by making a recommender system both transparent and scrutable? We address this question by developing a recommendation approach that satisfies the following desiderata: (1) reveal to users how their preferences are generally understood (explainability); (2) faithfully represent and expose the reasoning behind the recommendation mechanism (transparency);</p><p>(3) provide users with a direct and meaningful way to revise their model <ref type="bibr">(scrutability)</ref>.</p><p>A fundamental difference between this work and prior research is that we take explainability to the level of user preferences, as opposed to that of item recommendations. That is, instead of explaining the user why a given item was recommended, we present an approach to provide a textual description that summarizes the system's understanding of the user's preferences. We allow the user to scrutinize this summary and thereby directly modify his or her user model, as illustrated in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>Session 3B: Interpretatibility and Explainability SIGIR <ref type="bibr">'19, July 21-25, 2019</ref>, Paris, France</p><p>Our proposed approach hinges on the notion of set-based preferences. People often reason about categories and groups of items when reasoning about possible recommendations to make <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14]</ref>. Since attaching labels to clusters of items is an inherently difficult problem (e.g., <ref type="bibr" target="#b6">[7]</ref>), we instead reverse the process and use tags to define sets. Thus, following related research <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b50">51]</ref>, a basic assumption made in this paper is that user preferences as well as items can be characterized by a set of tags or keywords. These tags may be provided by users (social tagging) or extracted automatically. Given explicit ratings of specific items, which is the most common way of eliciting preferences today, we infer set-based preferences by aggregating over items that are associated with a given tag. We also present a novel pairwise tag-interaction approach that leads to much more semantically rich tag-based preferences being modeled.</p><p>This set-based user preference model enables us to generate item recommendations in a transparent manner. For explaining preferences, we opt for sentence-level textual explanations as this provides scrutability, by letting users provide feedback on individual sentences. Any change to the user's preferences has an immediate impact, thereby endowing users with more direct control over the recommendations they receive.</p><p>In summary, we make the following contributions: (1) we present an efficient method for inferring set-based user preferences from ratings given to individual items based on the tags associated with those items; (2) we develop a simple, effective, and computationally efficient recommendation model that operates on item tags and set-based user preferences; (3) we propose a simple algorithm for generating natural language explanations of user preferences; (4) we show the value of such a transparent and scrutable model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Explainable recommendations refers to personalized recommendation algorithms that "not only provide the user with recommendations, but also make the user aware why such items are recommended" <ref type="bibr" target="#b53">[54]</ref>. In addition to improving user acceptance of recommendations (persuasiveness), explanations can serve a multiplicity of aims, such as inspiring the user's confidence in the system (trust), helping users make good decisions (effectiveness) as well as make decisions faster (efficiency) and increasing the ease of use of a system (satisfaction) <ref type="bibr" target="#b48">[49]</ref>. Various forms of explanations have been explored in prior work, including sentences <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b54">55]</ref>, tag/keyword clouds <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b52">53]</ref>, as well as different kinds of visualizations <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b46">47]</ref>. Gedikli et al. <ref type="bibr" target="#b16">[17]</ref> evaluate different explanation types and propose a set of guidelines for designing and selecting suitable explanations for recommender systems.</p><p>Generation Approaches. Another way to classify explainable recommendation research is by the model used for generating explanations <ref type="bibr" target="#b53">[54]</ref>. They can broadly be categorized into content-based, collaborative filtering, and hybrid approaches. Content-based methods attempt to match items to users based on features/attributes, such as genre or director in the movies domain. These approaches naturally lend themselves to intuitive explanations by listing content features/attributes that made an item appear in the recommendations <ref type="bibr" target="#b12">[13]</ref>. Instead of relying on content information, collaborative filtering techniques leverage the "wisdom of the crowds" and make recommendations based on patterns of ratings or usage <ref type="bibr" target="#b23">[24]</ref>. Two families of methods may be distinguished: neighborhood-based and model-based. Neighborhood-based methods estimate ratings by those made by like-minded users (user-based) or known ratings made by the user on similar items (item-based). Of the two, item-based methods often have better scalability and improved accuracy <ref type="bibr" target="#b39">[40]</ref>, and are also more amenable to explaining the reasons behind the recommendations, as "users are familiar with items previously preferred by them, but do not know those allegedly like-minded users" <ref type="bibr" target="#b23">[24]</ref>. Model-based methods capture salient characteristics of users and items by parameters learned from training data. Perhaps the most prominent are latent factor models based on matrix factorization <ref type="bibr" target="#b24">[25]</ref>, where user-item interactions are modeled as inner products in a lower dimensional space. Although collaborative filtering methods have achieved significant improvements over content-based methods in terms of accuracy, they are less intuitive to explain <ref type="bibr" target="#b53">[54]</ref>. Specifically, the challenge is that the meaning of each latent dimension is unknown. Zhang et al. <ref type="bibr" target="#b54">[55]</ref> propose to alleviate this problem by aligning each latent dimension with a particular explicit feature extracted from textual user reviews. Note that this work, along with many others <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b51">52]</ref>, generates explanations with the help of reviews written for items, which may not always be available. Others consider neighborhood-style explanations for matrix factorization ("users who are the most similar to you also liked the recommended item") and incorporate an "explainability regularizer" into the objective function <ref type="bibr" target="#b0">[1]</ref>. Neural models, which have recently attracted attention for explainable recommendations, also fall under the category of model-based approaches <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b42">43]</ref>. These leverage attention weights over words in user reviews to indicate which parts are relevant for the recommendation that was made. The recommendation model, however, is still a black box and "the explainability of the deep model itself also needs further exploration" <ref type="bibr" target="#b53">[54]</ref>.</p><p>It is important to note that "the underlying algorithm of a recommender engine will to a certain degree influence the types of explanations that can be generated" <ref type="bibr" target="#b48">[49]</ref>. Therefore, explainability is a main design decision for us. Our model would be classified as content-based, as user preferences are characterized as a set of model parameters which are inferred from item ratings and then used to predict how much the user would like an unseen item.</p><p>Transparency provides insights into how the recommendation process works, and is closely related to explainability. Indeed, one of the aims that explanations can serve is to provide transparency <ref type="bibr" target="#b48">[49]</ref>. A crucial difference is that transparency "should give an honest account of how the recommendations are selected and how the system works" <ref type="bibr" target="#b48">[49]</ref>, while justification merely provides a plausible reason that may be decoupled from the recommendation algorithm <ref type="bibr" target="#b50">[51]</ref>. Explanations can also help to make a system scrutable, that is, allow users to correct the system's reasoning or modify preferences in the user model <ref type="bibr" target="#b35">[36]</ref>. A preliminary attempt at transparent and scrutable explanations is presented in <ref type="bibr" target="#b18">[19]</ref>. There, the authors consider a similar item search scenario and provide explanations in the form of overlapping and difference tag clouds between a seed item and a recommended item. Users can then steer the recommendations by manipulating the tag clouds.</p><p>The utilization of tags for explainable recommendations has been particularly well studied in the movie domain <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b50">51]</ref>. Vig et al. <ref type="bibr" target="#b50">[51]</ref> use tags for explaining movie recommendations and evaluate different interface designs that show how user preferences relate Set of all tags T +/-Set of user tags liked/disliked by the user W User model (weighted set of tags) to items. One key observation was that displaying tag preference (the user's sentiment towards a tag) is more important than tag relevance (the weight of a tag for a given movie). They explain this as follows: "Users may prefer seeing tag preference because they are skeptical that a recommender system can accurately infer their preferences." Gedikli et al. <ref type="bibr" target="#b16">[17]</ref> find content-based tag cloud explanations effective and particularly well accepted by users. We also use tags to represent user preferences, but unlike existing work, we strive for natural language explanations instead of word clouds.</p><p>Set-Based Preferences. Many papers consider the case where preferences over sets are given, and used to infer preferences for individual items <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b43">44]</ref>. We investigate the reverse process, starting from item preferences and infering sets. Of the above, <ref type="bibr" target="#b6">[7]</ref> is most relevant. Users are presented with sets of items along with some tags (e.g. "based on a comic, dark hero, superhero"), and asked to indicate which sets match their interests. The authors show that users are able to complete the preference elicitation process more rapidly than with traditional item ratings. Other studies also suggest that it may be easier for users to express preferences implicitly in sets rather than item by item <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14]</ref>. Sharma et al. <ref type="bibr" target="#b43">[44]</ref> study how users' ratings on sets of items relate to their ratings on the constituent items. They find that for most users the rating on a set can be accurately approximated by the average rating of the items in that set. There is, however, a considerable user population that tends to over-or under-estimate set-level ratings, especially for sets that contain items with diverse ratings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MODELING USER PREFERENCES</head><p>This section describes our user model, which is based on set-based preferences. It is a core enabling component for providing transparent item recommendations and for generating scrutable textual explanations of user preferences.</p><p>For the ease of presentation, all our examples are from the domain of our evaluation dataset, movies, as this has been a fertile area of recommendation systems research both from an algorithmic and from an explainability perspective <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b50">51]</ref>. However, there is nothing domain specific in our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A Case for Set-based Preferences</head><p>Related work has shown that set-based preference elicitation can lead to an accurate model of user preferences and, consequently, to high-quality item recommendations <ref type="bibr" target="#b6">[7]</ref>. However, generating descriptions for sets is notoriously hard. Therefore, instead of first clustering items and then naming clusters (as per <ref type="bibr" target="#b6">[7]</ref>), we capture sets in terms of social tags, and generate explanations based on those sets. While we will be using tags for capturing sets, we note that sets could also be defined in alternative ways, e.g., using item attributes (movie genres, directors, etc.). We further acknowledge that there are known issues regarding the quality <ref type="bibr" target="#b40">[41]</ref> and redundancy <ref type="bibr" target="#b17">[18]</ref> of social tags. Some have suggested that certain types of tags are more suitable than others for generating explanations <ref type="bibr" target="#b50">[51]</ref>. The quality of item-tag assignments can thus have a significant impact on recommendation accuracy as well as on explanation quality. However, we regard this as a data quality issue, and beyond the scope of this paper; it does not affect our modeling in any way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Inferring Set-level Preferences</head><p>Let us assume that I is the set of all known items where each item i represents a single movie, TV series, TV show, or similar. Let us also assume that we have a set of items I * that are rated by a given target user u and r i ∈ [-1, 1] is the (normalized) rating of each item i ∈ I * . Rating values lower than 0 correspond to disliking the item, while values higher than 0 show liking. We also have a number of tags t, each of which represents a semantic set like "comedy film" or "movies directed by Christopher Nolan. " We refer to Table <ref type="table" target="#tab_0">1</ref> for the notation used throughout the paper. Note that we will be referring to ratings and preferences of a single user, therefore the subscript u is generally omitted for the ease of notation.</p><p>We infer set-level preferences from ratings given to individual items, taking the mean rating of items labeled with a given tag to be the tag's average appeal to the user:</p><formula xml:id="formula_0">rt = 1 |I * t | i ∈I * t r i ,<label>(1)</label></formula><p>where I * t is the set of items rated by the user and tagged with t. Note that we take the relationship between items and tags (referred to as tag relevance in <ref type="bibr" target="#b50">[51]</ref>) to be binary (as opposed to a value on a continuous scale). It means that we are assuming that all tags associated with an item describe that item equally well. The binary approach is motivated both by its conceptual simplicity and by the fact that weighted item-tag assignments are unavailable in the public dataset we are working with. A tag-weighted variant of Eq. ( <ref type="formula" target="#formula_0">1</ref>) would be a straightforward extension in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Modeling Pairwise Set Interactions</head><p>Prior work modeling user interests with tags has focused on tags that succinctly characterize a single set of items, e.g., "sciencefiction movies" or "movies starring Tom Cruise." However, reasoning about high-level preferences through single tags produces preferences that are rarely rich enough to capture realistic interests particularly well. For instance, consider the above example of science fiction movies. A wide variety of science fiction movies exist, and a user's preferences are rarely black and white for such a large class of items. It may well be that the user generally dislikes science fiction movies, while some may still appeal to her. For this reason, one of our key contributions is representing user preferences by interactions between pairs of tags.</p><p>To explain this concept further, consider Figure <ref type="figure">2</ref>. Here, the user does not like science fiction movies in general, but does like science fiction movies that are about space exploration. Our approach allows describing this situation as "You don't like science fiction movies unless they are about space exploration. " Note, that in this</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Science fiction</head><p>Movies about space exploration + -Figure <ref type="figure">2</ref>: Here, the user does not like science fiction movies in general, but does like those about space exploration. example it is not necessarily true that the user would also like documentaries or horror movies featuring space exploration. Note also that multi-set interactions could be modeled in a similar way. However, our goal is to produce scrutable models, and we hypothesize that pairwise interactions capture sufficient richness while still providing a unit that can be naturally scrutinized by a user. For instance, it would be natural to ask someone "What sort of science fiction movies do you like?", but not usually natural to ask "What sort of science fiction movies about space exploration do you like?".</p><p>We formalize and consider different interactions between sets, which are presented in Figure <ref type="figure" target="#fig_1">3</ref>. Each pairwise interaction can also be translated into a simple sentence that captures the meaning of the interaction. While language is naturally ambiguous, we provide a specific meaning to the base interactions allowed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">User Model</head><p>We define the user model as a weighted set W of tags. For a given tag t, w t denotes the preference of the target user for that tag, such that the absolute value expresses the strength of the preference, while the sign indicates the direction of preference (i.e., like/dislike).</p><p>The preferences for the individual values can then naturally be derived from the inferred tag ratings:</p><formula xml:id="formula_1">w t = rt -r µ ,<label>(2)</label></formula><p>where r µ corresponds to the neutral rating. This may be personalized by using a user-specific value, however, we use the objective neutral rating, i.e., a value of 0, for all users. <ref type="foot" target="#foot_0">1</ref> Given a tagged item corpus, pairwise tag interactions are encoded by introducing any required pairwise tags, as pseudo tags, allowing a weight to be inferred where pairwise interactions exist. In particular, for a pair of tags t a , t b encoding "You [don't] like t a connective t b , " and noting that whenever both tags apply then t a must also apply, we model:</p><formula xml:id="formula_2">w t a ,t b = rt a ,t b -w t a .<label>(3)</label></formula><p>Recall that our approach is designed with scrutability in mind. That is, the user can explicitly state a positive or negative preference for a given tag. In practical terms this simply means overwriting the estimated w t for that tag with a specific value, such as 1 or -1, or excluding preference on a tag by removing it from W.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">IDENTIFYING USER PREFERENCES</head><p>This section introduces our approach to selecting which tags and pairwise tags, referred to as statements hereinafter, should be included in a user model. The algorithm works as follows. A set of candidate statements C is generated. These are ranked based on utility to the user model, and the set S of selected statements is included in the user model. Before detailing each of these steps below, we define desirable properties of the set of selected statements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>++ +</head><p>You (don't) like first especially if second.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>---+ ++</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Desirable Properties</head><p>We define the following desirable properties for the generated summary: (1) we aim to provide a correct description, i.e., one that correctly describes the user and does not contradict ratings provided by the user;</p><p>(2) we aim to provide a complete description, i.e., covering as many preferences from the set of rated items I * as possible, and ideally not leaving any significant portion of the user's interests uncovered;</p><p>(3) we aim for precision to avoid generating generic or abstract descriptions, such as "you like TV shows. "</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Generating Candidate Statements</head><p>A candidate statement s ∈ C models either the user's preference about a single tag t or about interaction between a pair of tags (t a , t b ). We start with the simpler case of a single tag.</p><p>Single Tag Statements. Candidate statements are generated for any tag which satisfies a set of conditions parameterized by Θ:</p><p>• The tag applies to at least Θ k items rated by the user. I.e., it is generalizable. • R t , the set of ratings for t has a mean that is statistically significantly different from zero (using the Wilcoxon Signed Rank test), with p-value below Θ p . I.e., it indicates a significant preference. The utility of statement s t for a single tag t in the user model is the weight of the tag in the user model, corrected for coverage and significance:</p><formula xml:id="formula_3">U (s t ) = cov(s t ) • siд(s t ) • |w t | ,<label>(4)</label></formula><p>where cov(s t ) represents how many of the item-level user observations are covered by this statement, and siд(s t ) discounts statements over tags that are only weakly statistically significant. Specifically, the coverage is defined as the lesser of the fraction of items rated by the user that do, or do not, contain the tag:</p><formula xml:id="formula_4">cov(s t ) = min |I * t | |I * | , |I * | -|I * t | |I * | . (<label>5</label></formula><formula xml:id="formula_5">)</formula><p>The utility of statements for which the statistical strength is less than two standard errors is discounted by defining:</p><formula xml:id="formula_6">siд(s t ) = min 2, |w t | σ t |I * t |<label>(6)</label></formula><p>where σ t is the estimated variance of the ratings provided in R t .</p><p>Pairwise Tag Statements. For generating pairwise interactions, these requirements apply to the first tag, as well as to the second tag in the context of the first:</p><formula xml:id="formula_7">U (s t a ,t b ) = U (t a ) + cov(s t a ,t b ) • siд(s t a ,t b ) • |w t a ,t b |,<label>(7)</label></formula><p>Specifically, for a pairwise interaction we redefine the coverage as:</p><formula xml:id="formula_8">cov(s t a ,t b ) = min |I * t a ∩ I * t b | |I * t a | , |I * t a | -|I * t a ∩ I * t b | |I * t a | . (<label>8</label></formula><formula xml:id="formula_9">)</formula><p>Significance and weight are computed as before, but over</p><formula xml:id="formula_10">I * t a ∩ I * t b .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Selecting Statements</head><p>While statements can be generated in this way for all tags attached to items in the corpus, as well as for all pairs of tags that co-occur on a sufficient number of items, this would yield an inscrutable user model. Specifically, we started with the observation that it is often impractical for a user to review or update all items they have provided feedback about to validate a model. In this section we show how we select down from all the possible tags and pairwise tags to a smaller set that can be presented to a user. Recall that C is the set of all single tag and pairwise candidate statements for a given user. Inspired by the MMR algorithm <ref type="bibr" target="#b4">[5]</ref>, we perform a greedy selection over C to obtain a subset S k that captures a user's top-k preferences. In each round, we select a statement to add to S k by picking the one with highest incremental utility over those statements already selected. Thus, we select:</p><formula xml:id="formula_11">s * = arg max s ∈C U (s |S k )<label>(9)</label></formula><p>where</p><formula xml:id="formula_12">U (s |S k ) = cov(s |S k ) • siд(s) • |w s |,<label>(10)</label></formula><formula xml:id="formula_13">cov(s |S k ) = min |I * s /I S k | |I * | , |I * | -|I * s /I S k | |I * | ,<label>(11)</label></formula><p>defining I S k as the set of items in I * that influence one of the existing statements in S k . Below, in Section 7.2, we will evaluate how the value of k impacts recommendation quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Generating Textual Representations</head><p>Using the templates presented in Figure <ref type="figure" target="#fig_1">3</ref>, the entire model can be presented to the user in natural language. We also note that the intensity of the user's preferences can be accentuated using terms such as "like, " "love, " "hate, " "don't like, " etc. In our experiments, we only included two grades, namely "like" and "don't like. " Finally, when generating sentences to present to the user, we additionally select a representative example from the specific items the user has rated for each statement. Our preliminary experiments showed that this disambiguates the meaning of the tag(s) to the user, and grounds it in specific movies that the user knows, thus improving the user's understanding. An example of a single-tag statement would thus be "You don't like science fiction movies, such as The Day After Tomorrow. "</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">GENERATING ITEM RECOMMENDATIONS</head><p>This section introduces our set-based model for generating item recommendations. It may be classified as a content-based approach, which scores individual items by matching the tags assigned to them against the tag preferences of the user. It is by design a transparent and explainable process that can directly incorporate user feedback, without complex interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Set-based Model</head><p>Let L be a binary random variable that indicates whether item i is liked (ℓ + ) or disliked (ℓ -). Following the probability ranking principle in IR <ref type="bibr" target="#b38">[39]</ref>, items should be ranked according to the probability P(L = ℓ + |u, i), which is equivalent to ranking items based on the odds ratio:</p><formula xml:id="formula_14">O(L = ℓ + |u, i) = P(L = ℓ + |u, i) P(L = ℓ -|u, i) . (<label>12</label></formula><formula xml:id="formula_15">)</formula><p>Applying Bayes' rule and then decomposing the joint probability P(u, i |L), we get:</p><formula xml:id="formula_16">O(L = ℓ + |u, i) rank = P(u, i |L = ℓ + ) P(u, i |L = ℓ -) = P(u |i, L = ℓ + ) P(u |i, L = ℓ -) × P(L = ℓ + |i) P(L = ℓ -|i) .</formula><p>(13) There are two main components in this model. The user likelihood, P(u |i, L) is, intuitively, the probability that user u likes/dislikes item i. The term P(L|i) can be interpreted as the prior probability of item i being liked/disliked (by any user). It is worth pointing out the similarity between this model and negative query generation for document retrieval <ref type="bibr" target="#b27">[28]</ref>. Here, the user is the query that is being generated by the document (item). However, the estimation of the model's components, which we describe below, is very different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">User Likelihood</head><p>We start by discussing the estimation of P(u, i |L = ℓ + ) and P(u, i |L = ℓ -). To reduce the number of equations, we shall use the symbol • to denote either + or -. Assuming that tags for an item are sampled identically and independently, the user likelihood is estimated as:</p><formula xml:id="formula_17">P(u |i, L = ℓ • ) = t ∈T • u P(t |θ i ) w • t,u ,<label>(14)</label></formula><p>where P(t |θ i ) is the probability of tag t given the model of item i, T • u is the set of tags liked/disliked the user, and w • t,u are the corresponding (positive/negative) user-tag weights. We detail the estimation of these components below.</p><p>Modeling Items. Notice that Eq. ( <ref type="formula" target="#formula_17">14</ref>) resembles the query likelihood formula in ad hoc document retrieval. There is, however, an important difference. The probability of a given tag in the item's model, P(t |θ i ), should only depend on the presence/absence of that tag. It should not be influenced by what other tags are associated with the item. (Otherwise, the number of tags that are associated with an item would have an undesired influence on the ranking.) Therefore, we model each item as a single sample from a multiple-Bernoulli distribution, where each binary trial corresponds to the event that some tag is associated with the item or not.</p><p>Let each item i be represented by a vector ì w i ∈ {0, 1} |T| , where w t,i = 1 iff tag t is assigned to i. From this single sample, we wish to estimate a smoothed tag model θ i for the item. We assume a prior over the model, specifically a multiple-Beta distribution, which is the conjugate prior for the multiple-Bernoulli distribution. The probability of a tag given the item's model can then be written as:</p><formula xml:id="formula_18">P(t |θ i ) = w t,i + α t -1 α t + β t -1 ,<label>(15)</label></formula><p>where α t and β t are parameters of the model. Analogously to how smoothing is applied in language modeling for ad hoc document retrieval <ref type="bibr" target="#b31">[32]</ref>, we set:</p><formula xml:id="formula_19">α t = µ |I t | |I| + 1 β t = |I| |I t | + µ(1 - |I t | |I| ) -1 , (<label>16</label></formula><formula xml:id="formula_20">)</formula><p>where I is the set of all items, I t is the set of items tagged with t, and µ is the smoothing parameter. Plugging Eq. ( <ref type="formula" target="#formula_19">16</ref>) back into Eq. ( <ref type="formula" target="#formula_18">15</ref>) yields:</p><formula xml:id="formula_21">P(t |θ i ) = w t,i + µ |I t |/|I| µ + |I|/|I t | . (<label>17</label></formula><formula xml:id="formula_22">)</formula><p>Modeling Users. The other ingredient for the estimation of the user likelihood in Eq. ( <ref type="formula" target="#formula_17">14</ref>) is the positive and negative user-tag weights, w + t,u and w - t,u . Intuitively, these correspond to positive and negative weights in the user model W (cf. Sect. 3.4). There is, however, the requirement of explainability that we also wish to satisfy. Instead of considering all tags for which preference could be inferred, we restrict ourselves to those tags that were selected to describe the user's preferences, i.e., part of the selected statements, S k . Recall that statements are generated both for single tags and for pairwise tag interactions. Pairwise tags, nevertheless, are introduced as pseudo tags with appropriate relative weights (cf. Eq. ( <ref type="formula" target="#formula_2">3</ref>)), which means that those can also be treated as simple tags here.</p><p>Given the set of top-k user preference statements, we divide tags involved into those liked (T + ) and disliked (T -) by the user:</p><formula xml:id="formula_23">T + = {t : t ∈ S k , w t &gt; 0} T -= {t : t ∈ S k w t &lt; 0} , (<label>18</label></formula><formula xml:id="formula_24">)</formula><p>where w t is the user's preference for tag t according the user model (cf. Sect. <ref type="bibr">3.4)</ref>. By definition, T + and T -are mutually exclusive. Then, the positive and negative user-tag weights are defined as:</p><formula xml:id="formula_25">w + t,u = w t if t ∈ T + 0 otherwise w - t,u = -w t if t ∈ T - 0 otherwise . (<label>19</label></formula><formula xml:id="formula_26">)</formula><p>We refer to the above formulation as the fully transparent user model. Alternatively, one may use all tags from all candidate statements C to rank items (i.e., replacing S k with C in Eq. ( <ref type="formula" target="#formula_23">18</ref>)), while verbalizing only the top-k statements to the user. We call this a partially transparent model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Item Priors</head><p>So far, our model is purely content-based. However, it is also possible to leverage the wisdom of the crowds, that is, the rating of other users on a given item. This is achieved by setting:</p><formula xml:id="formula_27">P(L = ℓ + |i) P(L = ℓ -|i) ∝ n + i + 1 |U| + 1 -n - i ,<label>(20)</label></formula><p>where n + i and n - i are the number of users who liked and disliked the item, 2 respectively, and |U| is the total number of users. 2 Here, we use the neutral objective rating (i.e., 0.5) to decide if ratings given to this item in the training dataset correspond to likes or dislikes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Final Model</head><p>By taking the logarithm of Eq. ( <ref type="formula">13</ref>), we obtain:</p><formula xml:id="formula_28">log O(L = ℓ + |u, i) rank = log P(L = ℓ + |i) -log P(L = ℓ -|i) (21) + log P(u |i, L = ℓ + ) -log P(u |i, L = ℓ -) ,</formula><p>where the first two terms are item priors that are independent of the user and the last two terms express the probability of the user liking/disliking the given item. Substituting Eqs. ( <ref type="formula" target="#formula_17">14</ref>) and ( <ref type="formula" target="#formula_27">20</ref>), we obtain the following final ranking function:</p><formula xml:id="formula_29">log O(L = ℓ + |u, i) rank = log(n + i + 1) -log(|U| + 1 -n - i ) + t ∈T + w + t,u log P(t |θ i )<label>(22)</label></formula><p>-</p><formula xml:id="formula_30">t ∈T - w - t,u log P(t |θ i ) .</formula><p>When item priors are used, they may be seen as a collaborative filtering element, making the overall approach a hybrid method. In the absence of item priors, the model is a purely content-based one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTAL SETUP</head><p>To assess our model in terms of effectiveness and scrutability, we conduct both a traditional benchmark-style evaluation as well as a user study. We next provide a high level overview of the methodology and describe the evaluation dataset. Following this, we present the experiment design in detail and describe baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Measurement Approach</head><p>Two broad categories of evaluation methodologies are commonly distinguished in the literature for measuring recommendation quality <ref type="bibr" target="#b45">[46]</ref>: (1) rating prediction estimates the rating a user would assign to an item in the collection (often measured in terms of root mean square error) and (2) item recommendation (a.k.a. top-N recommendation), where a small set of items assessed for suitability as user recommendations (typically measured using rank-based metrics, such as precision or NDCG). While rating prediction has traditionally been more popular, it can only be measured with respect to observed ratings in the dataset. In practice, these are biased towards items users like or know <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b44">45]</ref>. Many real-world scenarios, instead, are concerned with suggesting a few specific items from all items in the catalog, and therefore should instead be modeled as ranking problems <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b45">46]</ref>. Accordingly, for a given user u, the output of the recommendation is a ranked list of the top-N highest scoring items for that user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Benchmark Dataset</head><p>We employ the MovieLens-20M (ML-20M) dataset, which has been extensively used in recommendation research <ref type="bibr" target="#b19">[20]</ref>. It describes users' movie preferences, expressed as 5-star ratings. The dataset also contains social tags that users have assigned to individual movies. These are typically single words or short phrases whose meaning and purpose is determined by the user. Tag quality is known to be mixed in this collection <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b50">51]</ref>. Therefore, we filter tags on multiple criteria. Following <ref type="bibr" target="#b50">[51]</ref>, we limit the vocabulary of tags to those that have been applied by at least five different users and to at least two different items. Further, we remove tags deemed inappropriate (e.g., adult content) or of low utility (e.g., "don't remember"). For a given item, we only keep tags that have been assigned by at least two users. Finally, we filter out movies that have fewer than two tags assigned to them. Table <ref type="table" target="#tab_2">2</ref> provides descriptive statistics on the original and filtered datasets.</p><p>We partition the dataset into train and test sets by randomly selecting 1000 users and sampling 20% of their ratings as test data. All other users and the remaining ratings of our test users constitute the training split. For each user, we generate a ranked list of 100 items. We report standard IR measures: mean average precision (MAP), mean reciprocal rank (MRR), and normalized discounted cumulative gain (NDCG). For measures that operate on binary relevance (MAP and MRR), following <ref type="bibr" target="#b25">[26]</ref>, items rated 4 stars or more are considered relevant. When relevance is graded (NDCG), the gain value is the star-rating minus 2, i.e., a 3-star item receives gain 1 and a 5-star item receives gain 3.</p><p>A major challenge in rank-based evaluation is that only some of the items users like are generally known. One evaluation strategy (referred to as the TestRatings methodology in <ref type="bibr" target="#b1">[2]</ref>) is to only consider items that have been rated by the user in the test set. This, however, "does not test the recommender's ability to identify interesting items from a large pool" <ref type="bibr" target="#b14">[15]</ref> and thus amounts to solving an easier problem, often considerably overestimating the true performance of a system. Instead, we consider all items rated by any user in the training set a potential candidate (except those already rated by the target user)-known as the TrainingItems methodology in <ref type="bibr" target="#b1">[2]</ref>. Items that have not been rated by the user are all assumed to be non-relevant. As there are many such items, some of which are possibly relevant, this leads to underestimated performance <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">User Study</head><p>We also design a user study to evaluate the transparency and scrutability of our model. Taking place on a crowdsourcing platform (restricted to US-based users), it was performed in two rounds, as illustrated in Figure <ref type="figure">4</ref>. First, a preference elicitation round is conducted, where users are asked to provide ratings on a number of movies. This data serves as input to the set-based model and to the subsequent summary generation. The second round is further subdivided into two steps. In step 2A, users are presented with a summary of their preferences, or more precisely, a number of sentences, which they are asked to inspect (scrutinize). In step 2B, users are given a set of movie recommendations to rate. We provide specific details regarding the data collection process below.</p><p>We took a stratified sample of 500 movies from the filtered MovieLens-20M dataset as our item collection. This sample is made up of (i) the top 150 movies by the number of ratings received and (ii) a random movie for each year between 1980 and 2014, and for each of the top 10 most popular genres, <ref type="foot" target="#foot_1">3</ref> that is not already in the top-150 set. For the random movies, we required each to be rated by at least 20 users (to avoid movies too far down in the long tail). We randomly split the set of movies into a preference elicitation set (400 movies) and a candidate set (100 movies). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Baselines</head><p>We compare against the following collaborative filtering methods, as implemented in the MyMediaLite recommender library <ref type="bibr" target="#b15">[16]</ref>: 4  • MostPopular: A simple non-personalized baseline that recommends the most popular items (i.e., those with the most ratings). • Item-kNN: Item-based k-Nearest Neighbors <ref type="bibr" target="#b39">[40]</ref>, a classical collaborative filtering algorithm that is usually a strong baseline. • WR-MF: Weighted Regularized Matrix Factorization <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b34">35]</ref> is a regularized version of singular value decomposition (SVD). It can be seen as a pointwise regression approach that learns latent factors by minimizing the square-loss. • BPR-MF: Bayesian Personalized Ranking <ref type="bibr" target="#b37">[38]</ref> is a matrix factorization model directly optimized for ranking. It is "built on the assumption that a user prefers an item with a positive feedback to an item without an observed feedback" <ref type="bibr" target="#b36">[37]</ref>. • BPR-SLIM: Sparse Linear Methods (SLIM) <ref type="bibr" target="#b33">[34]</ref> learn a sparse coefficient matrix for items solely from the user rating profiles by solving a regularized optimization problem. We employ a variant that is optimized for the BPR-Opt criterion <ref type="bibr" target="#b37">[38]</ref>, using Stochastic Gradient Ascent. All methods use the default configuration settings in MyMediaLite. MostPopular and Item-kNN operate on the original 5-star ratings. For matrix factorization and sparse linear methods, ratings are binarized (corresponding to the problem of learning from binary, positive-only feedback <ref type="bibr" target="#b49">[50]</ref>). Specifically, ratings scored by 4 stars and above are taken as positive feedback <ref type="bibr" target="#b25">[26]</ref>.</p><p>Since our set-based model operates on tags, we also consider a content-based approach, which calculates the similarity between a user's profile vector and an item's tag vector.</p><p>• Tags-cosine: Items are scored according to the cosine similarity between the item's tag vector ì w i and the user's preference vector ì w u . For a given tag t, the user's preference for t is given by:  where r µ is the neutral rating. We also consider another variant with the popularity prior (cf. Eq. ( <ref type="formula" target="#formula_27">20</ref>)) incorporated by way of multiplication.</p><formula xml:id="formula_31">w t,u = 1 |I * | i ∈I * t (r i -r µ ) • w t,i ,<label>(23)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RESULTS</head><p>We now present a detailed analysis of the performance of our approach both in terms of recommendation quality, and the extent to which it satisfies the desirable properties listed in Sect. 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Effectiveness</head><p>Our first research question concerns the effectiveness of item recommendations produced by our method. This is evaluated on the benchmark (Benchmark) and user study (UserStudy) datasets. We start by considering the standard Benchmark dataset and compare against all the baselines; see Table <ref type="table" target="#tab_3">4</ref>. We note that popularity is a relatively weak baseline, and the simple tags-cosine approach is reasonably competitive, particularly when movie scores are weighted with a popularity prior. Our set-based approach (also including the prior, and all candidate statements in C) does not perform best, but also does not perform dramatically more poorly than many reasonable baselines.</p><p>However, we particularly note that in this dataset, the vast majority of items that contribute to the performance of a model are not rated by users as shown in the %UJ column. This introduces the potential for bias, and the results for all methods must thus be treated as lower bound estimates for algorithm performance.</p><p>For the UserStudy, we consider three representative baselines: MostPopular, Item-kNN, and BPR-SLIM. Note that these are trained on the ML-20M collection. We use these to assess four specific variants of our set-based model.</p><p>First, we observe that the performance numbers in Table <ref type="table" target="#tab_4">5</ref> are generally much higher, as users have rated a much larger fraction of items. In particular, almost all items (%UJ &lt; 3%) in the top 5 positions for all the algorithms were judged by the user study participants. No more than 39% of items in the top 10 positions were unjudged (and %UJ &lt; 25% for any variant of the set-based model). This makes these results much more representative than those in Table <ref type="table" target="#tab_3">4</ref>.</p><p>As noted earlier, a scrutable model is one that has a limited number of statements, which could be presented to a user (with a possibility to revise/correct them). We consider an algorithm transparent if it only uses such a model, i.e., defined by S k , consisting of no more than k = 5 statements. We term partial transparency use of the model C, which, while larger, still selects down for tags and pairs of tags with significant coverage of the user's judged items.</p><p>On the UserStudy dataset, the set-based model performs best on three of the metrics, while the most popular model performs best  on MRR. As such, we also see the large role that the popularity prior plays, and the relatively small cost of full transparency. We will consider this effect in more detail next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Transparency</head><p>Our second research question asks about the effects of providing full transparency as opposed to partial transparency. According to Table <ref type="table" target="#tab_4">5</ref>, the relative difference between full and partial transparency is at most 31% without priors and less than 5% when using priors, for any metric. The differences, however, are statistically significant for all metrics except MRR (indicated by ‡, using a two-tailed paired t-test with a p-value threshold of 0.001). Figure <ref type="figure" target="#fig_4">5</ref> further studies the effect of transparency in detail, by varying the length of the user summary. We see that as the number of statements S k in the user model is increased, recommendation quality improves. While there is a substantial improvement as the model size increases from 5 to 15 statements, beyond this the performance increases more slowly. We also clearly see the effect of the popularity prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Scrutability</head><p>Finally, our third research question considers scrutability: Given user feedback on the generated summaries, what impact does it have on the recommendations? Are the inferred statements correctly capturing the participant's interests?</p><p>Table <ref type="table" target="#tab_5">6</ref> shows the distribution of user responses on the generated (top-5) statements. First, we note that, surprisingly, around 19% of two-tag statements and 27% of single-tag statements had the participant disagree with the example belonging to the correct statement. As the example was always rated by the participant, and had the relevant tags, there are two possibilities. Either the participant may disagree with the tag(s) being appropriate for the example. Alternatively, the participant's opinion about the example may have changed between the first and second stages of the user study. As the stages were conducted within a day of each other, we expect that this illustrates the aforementioned data quality concern with tags, where perhaps a significant fraction of tags do not universally represent the movies to which they are attached. Exploring the effect of tag quality is an important direction for understanding where the performance of tag-based algorithms is suboptimal.</p><p>Second, we note that only 34% and 57% of participants agree with the entire statements (ignoring the example). This suggests that there is a lot of potential for users to improve their inferred user model in tag-based approaches.</p><p>Table <ref type="table" target="#tab_6">7</ref> shows the effect of scrutability on recommendation quality. Specifically, Initial shows the performance of the set-based algorithm using the inital model (matching the full transparency results from Table <ref type="table" target="#tab_4">5</ref>). If statements from the fully transparent model We see that in the set-based model with priors, this improves overall recommendation performance. We therefore conclude that scrutability is in fact being achieved. At the same time, we note that without the popularity prior, the corrected models perform slightly worse than the initial ones. We attribute this to the fact that user feedback is not utilized to its full possible extent. In particular, we are only removing tag preferences when the user did not agree with a statement. It would also be possible to update the weights of tags that belonged to statements with which users agreed. It should be noted that the observed differences between Initial and Corrected are not statistically significant, they should thus be regarded as indicative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSIONS</head><p>The task we set out in this paper is develop recommendation approaches that are more transparent and scrutable. We presented a novel set-based recommendation model, which we showed to return recommendations that are comparable quality to state-ofthe-art recommendation algorithms despite being transparent and explainable. In a user study with very high coverage of rated movies, we demonstrated how the user model can be explicitly scrutinized by users, leading to improved recommendations. We also showed how the size of the model that must be scrutinized can be traded off against recommendation quality. This study suggests a number of avenues for future work. Beyond questions of tag quality, and non-binary tag weighting for set-based models, we also leave open the question of how best to benefit from inferred preferences that users have scrutinized and agree with.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example summary of a user's preferences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Pairwise set interactions allowed in the user model. First is the tag of the left set and second is that of the right set. +, -and N indicate positive, negative and neutral average user appeal, with double symbols indicating stronger signals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>4 http://www.mymedialite.net (version 3.11).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(no priors) Set-based (with priors)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Recommendation performance as a function of model size. Note the strong effect of popularity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Notation. (All ratings are w.r.t. a given target user.)</figDesc><table><row><cell>C</cell><cell cols="2">Set of candidate statements i</cell><cell>Item (i ∈ I)</cell></row><row><cell>I</cell><cell>Set of all items</cell><cell cols="2">rt Inferred rating of t</cell></row><row><cell>I  *</cell><cell>Items rated by the user</cell><cell cols="2">r µ Neutral rating threshold</cell></row><row><cell cols="4">I t I  *  t R t Set of all ratings from items from I  *  Items labeled with t r i Rating of item i Rated items labeled with t t Tag (t ∈ T) t</cell></row><row><cell cols="4">S k Top-k user preference statements selected</cell></row><row><cell>T</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>You (don't) like first especially if not second.</figDesc><table><row><cell>first</cell><cell>second</cell><cell>first</cell><cell>second</cell></row><row><cell></cell><cell></cell><cell>--</cell><cell>-</cell></row><row><cell>first</cell><cell>second</cell><cell>first</cell><cell>second</cell></row><row><cell></cell><cell cols="2">You (don't) like first unless second.</cell><cell></cell></row><row><cell>+</cell><cell>-</cell><cell>-</cell><cell>+</cell></row><row><cell>first</cell><cell>second</cell><cell>first</cell><cell>second</cell></row><row><cell></cell><cell cols="2">You (don't) like first if second.</cell><cell></cell></row><row><cell>N</cell><cell>+</cell><cell>N</cell><cell>-</cell></row><row><cell>first</cell><cell>second</cell><cell>first</cell><cell>second</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Characteristics of the MovieLens-20M dataset.</figDesc><table><row><cell></cell><cell cols="3">#Ratings #Users #Items #Tags</cell></row><row><cell>Original data</cell><cell cols="3">20 000 263 138 493 27 278 38 641</cell></row><row><cell cols="2">Filtered data used 17 710 309 133 638</cell><cell>5 839</cell><cell>5 543</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Benchmark item recommendation results. %UJ refers to the fraction of unjudged items in the top 10.</figDesc><table><row><cell>Method</cell><cell>MRR MAP NDCG %UJ</cell></row><row><cell>MostPopular</cell><cell>0.272 0.071 0.214 85%</cell></row><row><cell>Item-kNN</cell><cell>0.360 0.129 0.341 76%</cell></row><row><cell>BPR-MF</cell><cell>0.387 0.137 0.334 80%</cell></row><row><cell>WR-MF</cell><cell>0.439 0.155 0.349 78%</cell></row><row><cell cols="2">SoftMarginRanking-MF 0.275 0.082 0.240 86%</cell></row><row><cell>WeightedBPR-MF</cell><cell>0.265 0.073 0.207 89%</cell></row><row><cell>BPR-SLIM</cell><cell>0.441 0.158 0.359 77%</cell></row><row><cell>Tags-cosine</cell><cell>0.319 0.073 0.195 86%</cell></row><row><cell>Tags-cosine + priors</cell><cell>0.351 0.094 0.245 83%</cell></row><row><cell>Set-based</cell><cell>0.308 0.081 0.228 85%</cell></row><row><cell cols="2">set-based model, and supplement with items by the sum of recip-</cell></row><row><cell cols="2">rocal ranks at which they appear in other user models. Similarly</cell></row><row><cell cols="2">to the benchmark evaluation, when using binary relevance only</cell></row><row><cell cols="2">the liked items are considered relevant. When relevance is graded,</cell></row><row><cell cols="2">neutral (3-star) items have gain of 1, and liked items have gain of 3.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>User study item recommendation results.</figDesc><table><row><cell>Method</cell><cell cols="2">MRR MAP NDCG@5 NDCG@10</cell></row><row><cell>MostPopular</cell><cell>0.882 0.515 0.721</cell><cell>0.628</cell></row><row><cell>Item-kNN</cell><cell>0.479 0.245 0.452</cell><cell>0.364</cell></row><row><cell>BPR-SLIM</cell><cell>0.709 0.378 0.624</cell><cell>0.511</cell></row><row><cell>Set-based model</cell><cell></cell><cell></cell></row><row><cell>Full transp., no priors</cell><cell>0.710 0.393 0.543</cell><cell>0.499</cell></row><row><cell>Full transp., priors</cell><cell>0.835 0.529 0.748</cell><cell>0.643</cell></row><row><cell cols="3">Partial transp., no priors 0.748 0.516  ‡ 0.663  ‡ 0.648  ‡</cell></row><row><cell>Partial transp., priors</cell><cell cols="2">0.866 0.554  ‡ 0.782  ‡ 0.670  ‡</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Distribution of responses on the generated user preference statements.</figDesc><table><row><cell>Statements for tag interactions</cell><cell cols="2">Count Ratio</cell></row><row><cell>(1) [first] [interaction] [second] [example]</cell><cell>39</cell><cell>27%</cell></row><row><cell>(2) [first] [interaction] [second]</cell><cell>10</cell><cell>7%</cell></row><row><cell>(3) [first] [example]</cell><cell>33</cell><cell>23%</cell></row><row><cell>(4) [first]</cell><cell>14</cell><cell>10%</cell></row><row><cell>(5) [example]</cell><cell>44</cell><cell>31%</cell></row><row><cell>(6) None of the above</cell><cell>3</cell><cell>2%</cell></row><row><cell>Total</cell><cell cols="2">143 100%</cell></row><row><cell>Statements for single tags</cell><cell></cell><cell></cell></row><row><cell>(1) [first] [example]</cell><cell>154</cell><cell>37%</cell></row><row><cell>(2) [first]</cell><cell>82</cell><cell>20%</cell></row><row><cell>(3) [example]</cell><cell>150</cell><cell>36%</cell></row><row><cell>(4) None of the above</cell><cell>31</cell><cell>7%</cell></row><row><cell>Total</cell><cell cols="2">417 100%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Scrutability results for the set-based model. We report on the fully transparent variant (k=5). participants disagree are removed from the user model, and are replaced with the next candidate statements from C to yield a new full transparent recommendation model consisting of 5 statements, we obtain the performance listed as Corrected.</figDesc><table><row><cell>Method</cell><cell cols="3">MRR MAP NDCG@5 NDCG@10</cell></row><row><cell>Initial, no priors</cell><cell>0.710 0.393</cell><cell>0.543</cell><cell>0.499</cell></row><row><cell>Initial, w/ priors</cell><cell>0.835 0.529</cell><cell>0.748</cell><cell>0.643</cell></row><row><cell cols="2">Corrected, no priors 0.678 0.381</cell><cell>0.530</cell><cell>0.484</cell></row><row><cell cols="2">Corrected, w/ priors 0.855 0.532</cell><cell>0.751</cell><cell>0.645</cell></row><row><cell>with which the</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>It is known that some users tend to either underrate or overrate items<ref type="bibr" target="#b43">[44]</ref>, therefore, r µ may also be set to the mean user rating. However, in our experiments, this achieved inferior performance to using an objective neutral rating.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>These are: action, adventure, documentary, comedy, crime, drama, horror, romance, sci-fi, and thriller.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary of user preferences</head><note type="other">Set</note><p>Table <ref type="table">3</ref>: Illustration of scrutability options for a summary preference statement in our user study.</p><p>(1) You like movies that are tagged as 'action', especially those that are tagged as 'sword fight', such as The Princess Bride. (2) You like movies that are tagged as 'action', especially those that are tagged as 'sword fight'. (3) You like movies that are tagged as 'action', such as The Princess Bride. (4) You like movies that are tagged as 'action'.</p><p>(5) You like The Princess Bride. (6) None of the above.</p><p>In round one, users were asked to rate at least 20 and at most 80 movies, using a three-point rating scale, corresponding to 1-star (dislike), 3-star (neutral), and 5-star (like) ratings on a 5-star scale. To prevent workers from providing random ratings, we also requested them to specify, in a text input field, what they liked/disliked about the given movie. To incentivize workers, they were promised an invitation to a generously-compensated follow-up task (i.e., round two), subject to the quality of their responses. We performed a manual inspection of the text inputs provided, and filtered out users with low-quality responses. In this way, we collected two rounds of responses from 122 users.</p><p>Round two was divided into two steps. First, in 2A, users were asked to inspect and provide feedback on the top five summary sentences generated for them using Eq. ( <ref type="formula">9</ref>). To make user feedback actionable, we start with the complete statement, then remove segments progressively. Participants are asked to inspect the statements in order, and select the first that accurately describes their preferences. We refer to Table <ref type="table">3</ref> for a specific example. For each option, we interpret the choice by updating the user's model if the option is selected. For instance, if the user were to select the third option in Table <ref type="table">3</ref>, we would update their user model to set the weight of the pairwise tag (action,sword fight) to zero.</p><p>Finally, in 2B, we presented users with personalized movie recommendations to rate (selected from the candidate set). Note that we perform 2A and 2B in a single round to reduce user dropout. We thus need to anticipate possible changes to the user summaries. At the same time, we need to keep the number of items to rate reasonable, to avoid attention fatigue. Therefore, users are presented with 20 movies, pooled from the following sources: two baseline methods (ItemKNN and BPRSLIM), the initial set-based model (i.e., no scrutinization), and the set-based model with different user model variants, based on the anticipated changes. Specifically, for each summary statement and for each scrutability option, we create a corresponding updated model, then generate recommendations using that user model. To form the pool of items to rate, we take the top-5 recommendations of the two baselines and of the initial</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Explainable Matrix Factorization for Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Behnoush</forename><surname>Abdollahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olfa</forename><surname>Nasraoui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WWW Companion</title>
		<meeting>of WWW Companion</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Precision-oriented Evaluation of Recommender Systems: An Algorithmic Comparison</title>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Bellogin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Castells</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Cantador</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of RecSys&apos;11</title>
		<meeting>of RecSys&apos;11</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="333" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Preferences over Sets</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">I</forename><surname>Brafman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Domshlak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Shimony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI&apos;06</title>
		<meeting>of AAAI&apos;06</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1101" to="1106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Representing Preferences Among Sets</title>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Brewka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirosław</forename><surname>Truszczyński</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Woltran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI&apos;10</title>
		<meeting>of AAAI&apos;10</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="273" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The Use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries</title>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jade</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGIR&apos;98</title>
		<meeting>of SIGIR&apos;98</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="335" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Explainable Entity-based Recommendations with Knowledge Graphs</title>
		<author>
			<persName><forename type="first">Rose</forename><surname>Catherine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><surname>Mazaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxine</forename><surname>Eskénazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Poster Track of RecSys&apos;17</title>
		<meeting>of the Poster Track of RecSys&apos;17</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Using Groups of Items for Preference Elicitation in Recommender Systems</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">Maxwell</forename><surname>Shuo Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loren</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName><surname>Terveen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CSCW&apos;15</title>
		<meeting>of CSCW&apos;15</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1258" to="1269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Crowd-Based Personalized Natural Language Explanations for Recommendations</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">Maxwell</forename><surname>Shuo Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loren</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName><surname>Gilbert Terveen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of RecSys&apos;16</title>
		<meeting>of RecSys&apos;16</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="175" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to Rank Features for Recommendation over Multiple Categories</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Xu Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGIR&apos;16</title>
		<meeting>of SIGIR&apos;16</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="305" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Xu Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongteng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyuan</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><surname>Zha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10288</idno>
		<title level="m">Visually Explainable Recommendation</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Aspect-Aware Latent Factor Model: Rating Prediction with Ratings and Reviews</title>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohan</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WWW&apos;18</title>
		<meeting>of WWW&apos;18</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="639" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Performance of Recommender Algorithms on Top-n Recommendation Tasks</title>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Cremonesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Turrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of RecSys&apos;10</title>
		<meeting>of RecSys&apos;10</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="39" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semantics-Aware Content-Based Recommender Systems</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gemmis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pasquale</forename><surname>Lops</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cataldo</forename><surname>Musto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fedelucio</forename><surname>Narducci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Semeraro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recommender Systems Handbook</title>
		<editor>
			<persName><forename type="first">Francesco</forename><surname>Ricci</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lior</forename><surname>Rokach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bracha</forename><surname>Shapira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Paul</forename><forename type="middle">B</forename><surname>Kantor</surname></persName>
		</editor>
		<meeting><address><addrLine>US, Chapter</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="119" to="159" />
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning User Preferences for Sets of Objects</title>
		<author>
			<persName><forename type="first">Marie</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Eaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiri</forename><forename type="middle">L</forename><surname>Wagstaff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML&apos;06</title>
		<meeting>of ICML&apos;06</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="273" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sturgeon and the Cool Kids: Problems with Random Decoys for Top-N Recommender Evaluation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaibhav</forename><surname>Ekstrand</surname></persName>
		</author>
		<author>
			<persName><surname>Mahant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of FLAIRS&apos;17</title>
		<meeting>of FLAIRS&apos;17</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="639" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">MyMediaLite: A Free Recommender System Library</title>
		<author>
			<persName><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of RecSys&apos;11</title>
		<meeting>of RecSys&apos;11</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="305" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">How Should I Explain? A Comparison of Different Explanation Types for Recommender Systems</title>
		<author>
			<persName><forename type="first">Fatih</forename><surname>Gedikli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dietmar</forename><surname>Jannach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mouzhi</forename><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Hum.-Comput. Stud</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="367" to="382" />
			<date type="published" when="2014-04">2014. April 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Usage Patterns of Collaborative Tagging Systems</title>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">A</forename><surname>Golder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><forename type="middle">A</forename><surname>Huberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="198" to="208" />
			<date type="published" when="2006-04">2006. April 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generating Transparent, Steerable Recommendations from Textual Descriptions of Items</title>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">J</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Lamere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Maillet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susanna</forename><surname>Kirk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackie</forename><surname>Bourque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Wen</forename><surname>Mak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of RecSys&apos;09</title>
		<meeting>of RecSys&apos;09</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="281" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The MovieLens Datasets: History and Context</title>
		<author>
			<persName><forename type="first">F</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Interact. Intell</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2015-12">2015. Dec. 2015</date>
		</imprint>
	</monogr>
	<note>Syst. 5, 4, Article 19</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">TriRank: Reviewaware Explainable Recommendation by Modeling Aspects</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CIKM&apos;15</title>
		<meeting>of CIKM&apos;15</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1661" to="1670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Explaining Collaborative Filtering Recommendations</title>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">L</forename><surname>Herlocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CSCW&apos;00</title>
		<meeting>of CSCW&apos;00</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="241" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Collaborative Filtering for Implicit Feedback Datasets</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICDM&apos;08</title>
		<meeting>of ICDM&apos;08</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Advances in Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recommender Systems Handbook</title>
		<editor>
			<persName><forename type="first">Francesco</forename><surname>Ricci</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lior</forename><surname>Rokach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bracha</forename><surname>Shapira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Paul</forename><forename type="middle">B</forename><surname>Kantor</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer US</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="77" to="118" />
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Matrix Factorization Techniques for Recommender Systems</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009-08">2009. Aug. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Wisdom of the Better Few: Cold Start Recommendation via Representative Based Rating Elicitation</title>
		<author>
			<persName><forename type="first">Nathan</forename><forename type="middle">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangrui</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of RecSys&apos;11</title>
		<meeting>of RecSys&apos;11</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="37" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Coevolutionary Recommendation Model: Mutual Learning Between Ratings and Reviews</title>
		<author>
			<persName><forename type="first">Yichao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruihai</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Smyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WWW&apos;18</title>
		<meeting>of WWW&apos;18</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="773" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Query Likelihood with Negative Query Generation</title>
		<author>
			<persName><forename type="first">Yuanhua</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CIKM&apos;12</title>
		<meeting>of CIKM&apos;12</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1799" to="1803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Collaborative Prediction and Ranking with Non-random Missing Data</title>
		<author>
			<persName><forename type="first">Benjamin</forename><forename type="middle">M</forename><surname>Marlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of RecSys&apos;09</title>
		<meeting>of RecSys&apos;09</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="5" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hidden Factors and Hidden Topics: Understanding Rating Dimensions with Review Text</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of RecSys&apos;13</title>
		<meeting>of RecSys&apos;13</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="165" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Explanation in Recommender Systems</title>
		<author>
			<persName><forename type="first">David</forename><surname>Mcsherry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Rev</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="179" to="197" />
			<date type="published" when="2005-10">2005. Oct. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Formal Multiplebernoulli Models for Language Modeling</title>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGIR&apos;04</title>
		<meeting>of SIGIR&apos;04</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="540" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">AI, Explain Yourself</title>
		<author>
			<persName><forename type="first">Don</forename><surname>Monroe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="11" to="13" />
			<date type="published" when="2018-10">2018. Oct. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">SLIM: Sparse Linear Methods for Top-N Recommender Systems</title>
		<author>
			<persName><forename type="first">Xia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICDM&apos;11</title>
		<meeting>of ICDM&apos;11</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="497" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">One-Class Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Rong</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><forename type="middle">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajan</forename><surname>Lukose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Scholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICDM&apos;08</title>
		<meeting>of ICDM&apos;08</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="502" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Evaluating Recommender Systems from the User&apos;s Perspective: Survey of the State of the Art</title>
		<author>
			<persName><forename type="first">Pearl</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">User Modeling and User-Adapted Interaction</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="317" to="355" />
			<date type="published" when="2012-10">2012. Oct. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Item Group Based Pairwise Preference Learning for Personalized Ranking</title>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGIR&apos;14</title>
		<meeting>of SIGIR&apos;14</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1219" to="1222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">BPR: Bayesian Personalized Ranking from Implicit Feedback</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of UAI&apos;09</title>
		<meeting>of UAI&apos;09</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The Probability Ranking Principle in IR</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="294" to="304" />
			<date type="published" when="1977">1977. 1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Itembased Collaborative Filtering Recommendation Algorithms</title>
		<author>
			<persName><forename type="first">Badrul</forename><surname>Sarwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WWW&apos;01</title>
		<meeting>of WWW&apos;01</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="285" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The Quest for Quality Tags</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">Maxwell</forename><surname>Shilad Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Lapitz</surname></persName>
		</author>
		<author>
			<persName><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of GROUP&apos;07</title>
		<meeting>of GROUP&apos;07</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="361" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Tagommenders: Connecting Users to Items Through Tags</title>
		<author>
			<persName><forename type="first">Shilad</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WWW&apos;09</title>
		<meeting>of WWW&apos;09</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="671" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Interpretable Convolutional Neural Networks with Dual Local and Global Attention for Review Rating Prediction</title>
		<author>
			<persName><forename type="first">Sungyong</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of RecSys&apos;17</title>
		<meeting>of RecSys&apos;17</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="297" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning from Sets of Items in Recommender Systems</title>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">Maxwell</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of eKNOW&apos;17</title>
		<meeting>of eKNOW&apos;17</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="59" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Training and Testing of Recommender Systems on Data Missing Not at Random</title>
		<author>
			<persName><forename type="first">Harald</forename><surname>Steck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of KDD&apos;10</title>
		<meeting>of KDD&apos;10</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="713" to="722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Evaluation of Recommendations: Rating-prediction and Ranking</title>
		<author>
			<persName><forename type="first">Harald</forename><surname>Steck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of RecSys&apos;13</title>
		<meeting>of RecSys&apos;13</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="213" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Effective Explanations of Recommendations: User-centered Design</title>
		<author>
			<persName><forename type="first">Nava</forename><surname>Tintarev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judith</forename><surname>Masthoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of RecSys&apos;07</title>
		<meeting>of RecSys&apos;07</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="153" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Evaluating the Effectiveness of Explanations for Recommender Systems</title>
		<author>
			<persName><forename type="first">Nava</forename><surname>Tintarev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judith</forename><surname>Masthoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">User Modeling and User-Adapted Interaction</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="399" to="439" />
			<date type="published" when="2012-10">2012. Oct. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Explaining Recommendations: Design and Evaluation</title>
		<author>
			<persName><forename type="first">Nava</forename><surname>Tintarev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judith</forename><surname>Masthoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recommender Systems Handbook</title>
		<editor>
			<persName><forename type="first">Francesco</forename><surname>Ricci</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lior</forename><surname>Rokach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bracha</forename><surname>Shapira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Paul</forename><forename type="middle">B</forename><surname>Kantor</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer US</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="353" to="382" />
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Collaborative Filtering for Binary, Positiveonly Data</title>
		<author>
			<persName><forename type="first">Koen</forename><surname>Verstrepen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kanishka</forename><surname>Bhaduriy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Cule</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Goethals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGKDD Explor. Newsl</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2017-09">2017. Sept. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Tagsplanations: Explaining Recommendations using Tags</title>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shilad</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IUI&apos;09</title>
		<meeting>of IUI&apos;09</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="47" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Explainable Recommendation via Multi-Task Learning in Opinionated Text Data</title>
		<author>
			<persName><forename type="first">Nan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiling</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGIR&apos;18</title>
		<meeting>of SIGIR&apos;18</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Incorporating Phrase-level Sentiment Analysis on Textual Reviews for Personalized Recommendation</title>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WSDM&apos;15</title>
		<meeting>of WSDM&apos;15</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="435" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Explainable Recommendation: A Survey and New Perspectives</title>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.11192</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Explicit Factor Models for Explainable Recommendation Based on Phrase-level Sentiment Analysis</title>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoping</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGIR&apos;14</title>
		<meeting>of SIGIR&apos;14</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="83" to="92" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
