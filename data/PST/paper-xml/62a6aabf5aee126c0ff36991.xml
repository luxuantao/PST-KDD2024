<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NAGphormer: Neighborhood Aggregation Graph Transformer for Node Classification in Large Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-06-10">10 Jun 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jinsong</forename><surname>Chen</surname></persName>
							<email>chenjinsong@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Huazhong University of Science and Technology Wuhan</orgName>
								<address>
									<postCode>430074</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kaiyuan</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gaichao</forename><surname>Li</surname></persName>
							<email>gaichaolee@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Huazhong University of Science and Technology Wuhan</orgName>
								<address>
									<postCode>430074</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kun</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">NAGphormer: Neighborhood Aggregation Graph Transformer for Node Classification in Large Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-06-10">10 Jun 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2206.04910v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Transformers have demonstrated superiority on various graph learning tasks in recent years. However, the complexity of existing Graph Transformers scales quadratically with the number of nodes, making it hard to scale to graphs with thousands of nodes. To this end, we propose a Neighborhood Aggregation Graph Transformer (NAGphormer) that is scalable to large graphs with millions of nodes. Before feeding the node features into the Transformer model, NAGphormer constructs tokens for each node by a neighborhood aggregation module called Hop2Token. For each node, Hop2Token aggregates neighborhood features from each hop into a representation, and thereby produces a sequence of token vectors. Subsequently, the resulting sequence of different hop information serves as input to the Transformer model. By considering each node as a sequence, NAGphormer could be trained in a mini-batch manner and thus could scale to large graphs. NAGphormer further develops an attention-based readout function so as to learn the importance of each hop adaptively. We conduct extensive experiments on various popular benchmarks, including six small datasets and three large datasets. The results demonstrate that NAGphormer consistently outperforms existing Graph Transformers and mainstream Graph Neural Networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph Neural Networks (GNNs) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b32">33]</ref>, owing to the message passing mechanism <ref type="bibr" target="#b16">[17]</ref>, have been recognized as a type of powerful deep learning techniques for graph learning tasks <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b19">20]</ref> over the last decade. Though effective, however, message passing-based GNNs have a number of inherent limitations, including over-smoothing <ref type="bibr" target="#b3">[4]</ref> and over-squashing <ref type="bibr" target="#b0">[1]</ref> with the increment of the model depth, limiting their potential capability for graph representation learning.</p><p>Transformers <ref type="bibr" target="#b31">[32]</ref>, on the other hand, are well-known deep learning architectures that have shown superior performance in a variety of Euclidean data, such as natural languages <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25]</ref> and images <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b25">26]</ref>. Due to their great modeling capability, researchers have increasingly focused on generalizing Transformers to non-Euclidean data, such as to graphs <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b18">19]</ref>. In contrast, graph-structured data generally contains more complicated properties, including structural topology and edge features. These properties cannot be directly encoded into Transformers as the tokens.</p><p>Figure <ref type="figure">1</ref>: Model framework of NAGphormer. NAGphormer first uses a novel neighborhood aggregation module, Hop2Token, to construct a sequence for each node based on the tokens of different hops of neighbors. Then NAGphormer learns the node representation using the standard Transformer backbone. An attention-based readout function is developed to aggregate neighborhood information of different hops adaptively. In the end, an MLP-based module is adopted for label prediction.</p><p>Existing Graph Transformers have developed three strategies to address this issue <ref type="bibr" target="#b27">[28]</ref>. (I) To capture the structural feature of nodes, some researchers replace the positional encoding with Laplacian eigenvectors <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b23">24]</ref> or degree-related feature vectors <ref type="bibr" target="#b36">[37]</ref>. (II) In addition to the positional encoding, GNNs are used as auxiliary modules to enable the Transformer model to capture the structural information <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b29">30]</ref>. (III) Another approach is to introduce graph information bias into the attention score of each node pair, e.g. the shortest-path distance <ref type="bibr" target="#b36">[37]</ref>.</p><p>Despite effectiveness, existing Graph Transformers are typically designed for graph-level tasks with a small number of nodes. However, as for node classification that is a node-level task, a graph would have millions of nodes. Existing Graph Transformers have high computational complexity that rises quadratically with the number of nodes, restricting them to be applied directly to large-scale networks for node classification. Gophormer <ref type="bibr" target="#b41">[42]</ref> is a recent attempt that aims to reduce the training cost by sampling the ego-graphs of nodes. Nevertheless, such a sampling strategy is still time-consuming and limits the model to capture deeper structural information.</p><p>The construction of the pair-wise attention matrix for all the node pairs is the main drawback of current Graph Transformers. If no attention mask is implemented, such type of attention matrix can be regarded as the global information. But global information is not necessary for node classification. For instances, when there is a considerable distance between two nodes, the chance of them belonging to the same category is very small. Taking citation networks as an example, a paper in The Botanical Magazine is barely relevant to a paper analysing programming language. The distance between them could be considerable large in the network. As a result, information from a faraway node is rather useless, and even noisy with negative impact. Meanwhile, GNN-based studies <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b22">23]</ref> that aggregate neighborhood (or local) information, have shown superior performance on all kinds of node classification tasks. Consequently, neighborhood operation should be useful for Graph Transformers. Motivated by the above observation, We propose a novel model, termed Neighborhood Aggregation Graph Transformer (NAGphormer), for node classification. Unlike existing Graph Transformers that regard all nodes as fully-connected, NAGphormer first constructs tokens for each node by a novel neighborhood aggregation module called Hop2Token. The key idea behind Hop2Token is to aggregate neighborhood features from multiple hops and transform them into a representation, which can be thought as a token. To preserve the local information of nodes, Hop2Token constructs a sequence for each node based on the tokens of neighbors in different hops. The sequences are then fed into a Transformer-based module for learning the node representations. By treating each node as a sequence, NAGphormer can be trained in a mini-batch manner and hence can handle large graphs. Given the fact that the contributions of neighbors in different hops to the final node representation differ, NAGphormer provides an attention-based readout function so as to adaptively learn the importance of each hop.</p><p>The main contributions of this work are as follows:</p><p>? We propose Hop2Token, a novel neighborhood aggregation method that aggregates the neighborhood features from each hop into a node representation, resulting in a sequence of token vectors that preserves the neighborhood information for different hops.</p><p>? We propose a new Graph Transformer method, NAGphormer, for node classification. NAGphormer can be trained in a mini-batch manner depending on the output of Hop2Token, and therefore enable the Graph Transformers to handle large graphs.</p><p>? We further develop an attention-based readout function to adaptively learn the importance of different hops to continue to boost the model performance.</p><p>? Extensive experiments on popular datasets from small to large demonstrate the superiority of the proposed methodology over existing Graph Transformers and mainstream Graph Neural Networks.</p><p>2 Preliminaries</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Formulation</head><p>Let G = (V, E) denote an unweighted and undirected attributed network (graph), where</p><formula xml:id="formula_0">V = {v 1 , v 2 , ? ? ? , v n }, n = |V | is the number of nodes. Each node v ? V has a feature vector x v ? X,</formula><p>where X ? R n?d is the feature matrix describing the attribute information of nodes and d is the dimension of the feature vector. A ? R n?n represents the adjacency matrix and we can obtain the normalized adjacency matrix ? with the diagonal degree matrix D. Typical normalization for the adjacency matrix includes:</p><formula xml:id="formula_1">? = D -1/2 AD -1/2 [3] or ? = D -1 A [41].</formula><p>In this work, we focus on the node classification task in attributed networks. Concretely, the node classification task provides a labeled node set V l and an unlabeled node set V u . Let Y ? R n?c denote the label matrix where c is the number of classes. Given the labels Y V l , the goal is to predict the labels Y Vu for unlabeled nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Transformer</head><p>The Transformer encoder <ref type="bibr" target="#b31">[32]</ref> contains a sequence of Transformer layers, where each layer is comprised with a multi-head self-attention (MSA) and a position-wise feed-forward network. The MSA module is the critical component that aims to capture the semantic correlation between the input items (tokens). For simplicity, we use the single-head self-attention module for description. Suppose we have an input H ? R n?d for the self-attention module where n is the number of items and d is the hidden dimension. The self-attention module first projects H into three subspaces, named Q, K and V:</p><formula xml:id="formula_2">Q = HW Q , K = HW K , V = HW V ,<label>(1)</label></formula><p>where</p><formula xml:id="formula_3">W Q ? R d?d K , W K ? R d?d K and W V ? R d?d V</formula><p>are the projection matrices. And the output matrix is calculated as:</p><formula xml:id="formula_4">H = softmax QK ? d K V.<label>(2)</label></formula><p>The attention matrix QK captures the pair-wise similarity of the input tokens in the sequence. Specifically, it calculates the dot product between each token pair after projection. The softmax is applied row-wise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Proposed NAGphormer</head><p>In this section, we present the proposed NAGphormer in detail. To handle networks at scale, we first introduce a novel neighborhood aggregation module called Hop2Token, then we build NAGphormer together with structural encoding and attention-based readout function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 The Hop2Token algorithm</head><p>Input: Normalized adjacency matrix ?; Feature matrix X; Propagation step K Output: Sequences of all nodes X G 1: for k = 0 to K do 2:</p><p>for i = 0 to n do 3:</p><formula xml:id="formula_5">X G [i, k] = X[i]; 4:</formula><p>end for 5:</p><p>X = ?X; 6: end for 7: return Sequences of all nodes X G ;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Hop2Token</head><p>Aggregating information from neighboring nodes into a node representation is a crucial strategy in reasonably powerful Graph Neural Network (GNN) architectures. It addresses the issue that each node has a variable number of neighbors. To inherit the desirable properties, we design Hop2Token considering different hops of the neighborhood information. <ref type="figure">where d(v,</ref><ref type="figure">u</ref>) represents the distance of the shortest path between v and u. We define N 0 (v) = {v}, i.e., the 0-hop neighborhood of a node is the node itself. In Hop2Token, we transform the k-hop neighborhood N k (v) into a neighborhood embedding x k v with an aggregation operator ?. In this way, the k-hop representation of a node v can be expressed as:</p><formula xml:id="formula_6">Let N k (v) = {v ? V |d(v, u) ? k} be the k-hop neighborhood of a node v,</formula><formula xml:id="formula_7">x k v = ?(N k (v)).<label>(3)</label></formula><p>By this equation, we can calculate the neighborhood embeddings for variable hops of a node and further construct a sequence to represent its neighborhood information,</p><formula xml:id="formula_8">S v = (x 0 v , x 1 v , ..., x K v ). The sequence length of K is fixed as a hyperparameter. Assume x k v is a d-dimensional vector.</formula><p>The sequences of all nodes on graph G will construct a tensor X G ? R n?(K+1)?d . To better illustrate the implementation of Hop2Token, we decompose X G to a sequence S = (X 0 , X 1 , ? ? ? , X K ), where X k ? R n?d can be seen as the k-hop neighborhood matrix for the entire graph. Here we define X 0 as the original feature matrix X.</p><p>In practice, we apply a propagation process similar to the method in Chien et al. <ref type="bibr" target="#b8">[9]</ref> to obtain the sequence of K-hop neighborhood matrices. Given the normalized adjacency matrix ? (also known as transition matrix <ref type="bibr" target="#b15">[16]</ref>) and X, multiplying ? with X aggregates immediate neighborhood information. Applying this multiplication consecutively allows us to propagate information at larger distances. For example, we can access 2-hop neighborhood information by ?( ?X). Simply put, the k-hop neighborhood matrix can be described as:</p><formula xml:id="formula_9">X k = ?k X.<label>(4)</label></formula><p>For the normalization strategy, we choose ? = D -1/2 AD -1/2 due to its popularity <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>. The detailed implementation is drawn in Algorithm 1.</p><p>The advantages of Hop2Token is two-fold: 1) Hop2Token is a non-parametric method. It can be conducted offline before the model training. And the output of Hop2Token supports minibatch training, the model can handle graphs of arbitrary sizes, thus allowing the generalization of Graph Transformers to large-scale graphs. 2) Encoding k-hop neighborhood of a node into one representation is helpful for capturing the hop-wise semantic relevance, which is ignored in most GNNs <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b32">33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">NAGphormer for Node Classification</head><p>Figure <ref type="figure">1</ref> depicts the architecture of NAGphormer. Given an attributed graph, we first reconstruct the feature matrix into a structure-aware matrix. To this end, an additional matrix constructed from eigendecomposition is concatenated to the original feature matrix. Accordingly, the effective feature vector for node v is extended as x v ? R d . The detailed construction is described in Section 3.3.</p><p>Next, we assemble an aggregated neighborhood sequence as S v by applying Hop2Token. Each element of S v is a token and can be expanded as</p><formula xml:id="formula_10">(x 0 v , x 1 v , ..., x K v ), where x k v is calculated from x (k-1) u (u ? N k (v)) and x 0 v = x v .</formula><p>K is the predefined sequence length. Then we map to the hidden dimension d m of the Transformer with a learnable linear projection:</p><formula xml:id="formula_11">Z (0) = x 0 v E; x 1 v E; ? ? ? ; x K v E ,<label>(5)</label></formula><p>where E ? R d ?dm and Z (0) ? R (K+1)?dm .</p><p>Then, we feed the projected sequence into the Transformer encoder. The building blocks of the Transformer is described in Section 2.2, i.e., multi-head self-attention (MSA) and position-wise feed-forward network (FFN). We follow the implementation of the vanilla Transformer encoder described in <ref type="bibr" target="#b31">[32]</ref>, while LayerNorm (LN) is applied before each block <ref type="bibr" target="#b34">[35]</ref>. And the FFN consists of two layers with a GELU non-linearity.</p><formula xml:id="formula_12">Z ( ) = MSA LN Z ( -1) + Z ( -1) ,<label>(6)</label></formula><formula xml:id="formula_13">Z ( ) = FFN LN Z ( ) + Z ( ) ,<label>(7)</label></formula><p>where = 1, . . . , L implies the -th layer of the Transformer.</p><p>Finally, a novel readout function is applied to the output of the Transformer encoder. Through several Transformer layers, the corresponding output Z ( ) contains the embeddings for all neighborhoods.</p><p>It requires a readout function to aggregate the information of different neighborhoods into one embedding. Common readout functions include summation and mean <ref type="bibr" target="#b17">[18]</ref>. However, these methods ignore the importance of different neighborhoods. Inspired by GAT <ref type="bibr" target="#b32">[33]</ref>, we propose an attentionbased readout function to learn such importance by computing the attention coefficients between 0-hop neighborhood (i.e., the node itself) and every other neighborhood. Details are described as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implementation details</head><p>Structural encoding. Besides the attribute information of nodes, the structural information of nodes is also a crucial feature for graph learning tasks. We adopt the eigenvectors of the Laplacian matrix of the graph which have been widely used in spectral clustering <ref type="bibr" target="#b33">[34]</ref> for capturing the structural information of nodes. Specifically, we select the eigenvectors corresponding to the s smallest nontrivial eigenvalues to construct the structure matrix U ? R n?s <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b23">24]</ref>. Then we combine the original feature matrix X with the structure matrix U to preserve both attribute and structural information:</p><formula xml:id="formula_14">X = X U,<label>(8)</label></formula><p>where indicates the concatenation operator and X ? R n?(d+s) denotes the fused feature matrix.</p><p>Attention-based readout function. For the output matrix Z ? R (K+1)?dm of a node, Z 0 is the token representation of the node itself and Z k is its k-hop representation. We calculate the normalized attention coefficients for its k-hop neighborhood:</p><formula xml:id="formula_15">? k = exp((Z 0 Z i )W a ) K i=1 exp((Z 0 Z i )W a ) ,<label>(9)</label></formula><p>where W a ? R 1?2dm denotes the learnable projection and i = 1, . . . , K. Therefore, the readout function takes the correlation between each neighborhood and the node representation into account. The node representation is finally aggregated as follows:</p><formula xml:id="formula_16">Z out = Z 0 + K k=1 ? k Z k . (<label>10</label></formula><formula xml:id="formula_17">)</formula><p>4 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Datasets. We conduct experiments on nine widely used datasets of different scales, including six small-scale datasets and three relatively large-scale datasets. For small-scale datasets, we adopt Pubmed <ref type="bibr" target="#b28">[29]</ref>, CoraFull <ref type="bibr" target="#b30">[31]</ref>, Computer <ref type="bibr" target="#b30">[31]</ref>, Photo <ref type="bibr" target="#b30">[31]</ref>, CS <ref type="bibr" target="#b30">[31]</ref> and Physics <ref type="bibr" target="#b30">[31]</ref> from the Deep Graph Library (DGL) <ref type="foot" target="#foot_0">3</ref> . Since DGL does not provide identical data splits for these datasets, we apply 60%/20%/20% train/val/test splits for experiments. For large-scale datasets, we adopt AMiner-CS <ref type="bibr" target="#b14">[15]</ref>, Reddit <ref type="bibr" target="#b17">[18]</ref> and Amazon2M <ref type="bibr" target="#b7">[8]</ref> from <ref type="bibr" target="#b13">[14]</ref>. The data splits of these datasets follow the settings as Grand+ <ref type="bibr" target="#b13">[14]</ref>. The statistics on datasets are reported in Table <ref type="table" target="#tab_0">1</ref>.</p><p>Baselines. To evaluate the effectiveness of NAGphormer on the node classification task, we compare NAGphormer with nine representative baselines, including (I) three full-batch GNNs: GCN <ref type="bibr" target="#b21">[22]</ref>, GAT <ref type="bibr" target="#b32">[33]</ref> and APPNP <ref type="bibr" target="#b22">[23]</ref>; (II) three scalable GNNs: GraphSAINT <ref type="bibr" target="#b38">[39]</ref>, PPRGo <ref type="bibr" target="#b1">[2]</ref> and GRAND+ <ref type="bibr" target="#b13">[14]</ref>; (III) three Graph Transformers: GT <ref type="bibr" target="#b11">[12]</ref>, SAN <ref type="bibr" target="#b23">[24]</ref> and Graphormer <ref type="bibr" target="#b36">[37]</ref>.</p><p>Implementation details. Referring to the recommended settings in the official implementations, we perform hyperparameter tuning for each of the baselines. For model configuration of NAGphormer, we try the number of Transformer layers in {1,2,...,5}, the hidden dimension in {128, 512}, and the propagation steps in {2,3,...,12}. Parameters are optimized with AdamW <ref type="bibr" target="#b26">[27]</ref> optimizer, with the learning rate of 1e-4 and weight decay of 1e-3. The batch size is set to 2000. The training process is early stopped within 50 epochs. All experiments are conducted on a Linux server with 1 I9-9900k CPU, 1 RTX 2080TI GPU and 64G RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance of Node Classification on Small-scale Datasets</head><p>For small-scale datasets, we conduct ten trials with random seeds for each model and take the mean accuracy and standard deviation for comparison. Table <ref type="table" target="#tab_1">2</ref> reports the results. Overall, NAGphormer outperforms the baselines consistently on all the small-scale datasets, demonstrating the superiority of our proposed model. For the superiority over GNN-based methods, it is because NAGphormer utilizes our proposed Hop2Token and the Transformer model to capture the semantic relevance of different hop neighbors that is overlooked in most GNNs. Besides, the performance of NAGphormer also surpasses Graph Transformer-based methods, indicating that leveraging the local information is beneficial for node classification. Moreover, we observe that Transformer-based models show competitive performance among the six datasets, demonstrating that the Transformer architecture is competent for node classification task by introducing structural information. We also observe that Graphormer and SAN suffer from the out-of-memory error even in some small graphs, further demonstrating the necessity of designing an scalable Graph Transformer for node-level tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Performance of Node Classification on Large-scale Datasets</head><p>To verify the scalability of NAGphormer, we continue to compare NAGphormer with three scalable GNNs on three large-scale datasets. Due to the huge computational cost, existing Graph Transformers can not work on such large-scale datasets. We also run each model ten times with different random seeds and report the mean accuracy. The results are summarized in Table <ref type="table" target="#tab_2">3</ref>. We can observe that NAGphormer consistently outperform the scalable GNNs on all datasets, indicating that NAGphormer can better preserve the local information of nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>To analyze the importance of structural encoding and attention-based readout function, we perform a series of ablation studies on the Aminer-CS dataset. The results are summarized in Table <ref type="table" target="#tab_3">4</ref>.</p><p>Structural encoding. In order to investigate the effectiveness of structural encoding, we compare our proposed NAGphormer to its variant without the structural encoding module. The experimental results show that the structural encoding can significantly improve the performance of NAGphormer, demonstrating the necessity of structural encoding for node-level task.</p><p>Attention-based readout function. We conduct a comparative experiment between the proposed attention-based readout function Att. (Equation <ref type="formula" target="#formula_15">9</ref>) with previous readout functions: Single and Sum.</p><p>Similar to <ref type="bibr" target="#b18">[19]</ref>, the function of Single utilizes the corresponding representation of the node itself as the final output to predict labels. And Sum can be regarded as aggregating all information of the neighbors equally. From Table <ref type="table" target="#tab_3">4</ref>, we observe that Att. outperforms other readout functions, indicating the effectiveness of aggregating the information of neighbors in different hops adaptively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Parameter Study</head><p>To further evaluate the performance of NAGphormer, this subsection studies the influence of two key parameters: the number of propagation steps K and the number of Transformer layers L. Specifically, we perform the experiments on AMiner-CS, Reddit and Amazon2M by setting different values of K and L,</p><p>The influence of K. We select the number of propagation steps K from {4, 6, ..., 12} and fix L = 1. of the neighborhood information. Notably, the performance of NAGphormer is less competitive when K lies beyond a certain threshold (e.g. 10 in these datasets). It is reasonable as neighbors in large hop may contain more noisy information, such as noisy attribute information. The values of K are different for each dataset to achieve the best performance since different networks exhibit different neighborhood structures <ref type="bibr" target="#b22">[23]</ref>. In practice, we set K = 10 for AMiner-CS and Amazon2M. For Reddit, we set K = 8.</p><p>The influence of L. To a great extent, L directly determines the parameter size of the model. We vary L from 1 to 5 and fix k = 10. The results are shown in Figure <ref type="figure">2</ref>(b). Generally speaking, a smaller L can achieve a high accuracy while a larger L degrades the performance of NAGphormer. This scenario can attribute to that a larger L is easy to result in over-fitting. we set L = 3 for AMiner-CS. For Reddit and Amazon2M, we set L = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>In this section, we first review the progress of Graph Transformer based models with an emphasis on their strategies for structural information extraction, then briefly present the most recent works that generalize GNN to large-scale graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Graph Transformers</head><p>There are several works that make efforts to generalize transformer neural networks <ref type="bibr" target="#b31">[32]</ref> to graphstructured data. Similar to the positional encoding in the original Transformer, some notable works compress the graph structure into positional encoding to capture the topological information. Dwivedi et al. <ref type="bibr" target="#b11">[12]</ref> utilize Laplacian eigenvectors to represent positional encodings and fuse them with the raw attribute of nodes as the input of Transformer. Derived from <ref type="bibr" target="#b11">[12]</ref>, Devin et al. <ref type="bibr" target="#b23">[24]</ref> leverage the full spectrum of Laplacian matrix to learn the positional encodings. In addition to representing structural information by the eigenvectors, Ying et al. <ref type="bibr" target="#b36">[37]</ref> propose a spatial encoding method that model the structural similarity of node pairs based on their shortest path lengths. Wu et al. <ref type="bibr" target="#b18">[19]</ref> regard GNNs as an auxiliary module to extract local structural information of nodes and further feed them into the Transformer to learn long-range pairwise relationships. Besides, by modeling edge features in chemical and molecular graphs, Dwivedi et al. <ref type="bibr" target="#b11">[12]</ref> extend Graph Transformer to edge feature representation by injecting them into the self-attention module of Transformer.</p><p>Nevertheless, like the original Transformer, these aforementioned methods adopt the fully-connected attention mechanism upon all node pairs, in which the spatial complexity is quadratic with the number of nodes. Such high cost makes these methods hard to handle graph mining task on large-scale networks with millions of nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Scalable Graph Neural Networks</head><p>Graph Neural Networks (GNNs) have become a powerful technique to model graph-structured data.</p><p>Based on the message passing framework, typical GNNs <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b32">33]</ref> require the entire adjacency matrix as the input during training. In this way, when applying to large-scale graphs, the cost of training will be too high to afford. To generalize GNNs to large-scale networks and meanwhile reduce the training cost, a general approach is to design a sampling strategy. Existing strategies can be divided into two categories: node sampling-based methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b42">43</ref>] that aim at reducing the number of neighborhood nodes, and graph sampling-based methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b38">39]</ref> that dedicate to decrease the size of the input graph.</p><p>Besides the sampling strategy, recent studies have developed a variety of approximate propagation methods to accelerate the message passing process, such as Approximate Personalized PageRank <ref type="bibr" target="#b1">[2]</ref>, Localized Bidirectional Propagation <ref type="bibr" target="#b5">[6]</ref> and Generalized Forward Push <ref type="bibr" target="#b13">[14]</ref>.</p><p>However, by designing various sampling-based or approximation-based methods to reduce the training cost, these models will inevitably lead to information loss, and somehow restrict their performance on large-scale networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 (Figure 2 :</head><label>22</label><figDesc>Figure 2: Performance under different parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics on datasets.</figDesc><table><row><cell>Dataset</cell><cell>#Nodes</cell><cell cols="3">#Edges #Features #Classes</cell></row><row><cell>Pubmed</cell><cell>19,717</cell><cell>44,324</cell><cell>500</cell><cell>3</cell></row><row><cell>CoraFull</cell><cell>19,793</cell><cell>126,842</cell><cell>8,710</cell><cell>70</cell></row><row><cell>Computer</cell><cell>13,752</cell><cell>491,722</cell><cell>767</cell><cell>10</cell></row><row><cell>Photo</cell><cell>7,650</cell><cell>238,163</cell><cell>745</cell><cell>8</cell></row><row><cell>CS</cell><cell>18,333</cell><cell>163,788</cell><cell>6,805</cell><cell>15</cell></row><row><cell>Physics</cell><cell>34,493</cell><cell>495,924</cell><cell>8,415</cell><cell>5</cell></row><row><cell>AMiner-CS</cell><cell>593,486</cell><cell>6,217,004</cell><cell>100</cell><cell>18</cell></row><row><cell>Reddit</cell><cell cols="2">232,965 11,606,919</cell><cell>602</cell><cell>41</cell></row><row><cell cols="3">Amazon2M 2,449,029 61,859,140</cell><cell>100</cell><cell>47</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of all models in terms of mean accuracy ? stdev (%) on small-scale datasets. The best results appear in bold. OOM indicates the out-of-memory error.</figDesc><table><row><cell>Method</cell><cell>Pubmed</cell><cell>CoraFull</cell><cell>Computer</cell><cell>Photo</cell><cell>CS</cell><cell>Physics</cell></row><row><cell>GCN</cell><cell cols="6">84.65 ? 0.10 61.76 ? 0.14 89.65 ? 0.52 92.70 ? 0.20 92.92 ? 0.12 96.18 ? 0.07</cell></row><row><cell>GAT</cell><cell cols="6">85.77 ? 0.15 64.47 ? 0.18 90.78 ? 0.13 93.87 ? 0.11 93.61 ? 0.14 96.17 ? 0.08</cell></row><row><cell>APPNP</cell><cell cols="6">86.80 ? 0.11 65.16 ? 0.28 90.18 ? 0.17 94.32 ? 0.14 94.49 ? 0.07 96.54 ? 0.07</cell></row><row><cell cols="7">GraphSAINT 88.96 ? 0.16 67.85 ? 0.21 90.22 ? 0.15 91.72 ? 0.13 94.41 ? 0.09 96.43 ? 0.05</cell></row><row><cell>PPRGo</cell><cell cols="6">87.38 ? 0.11 63.54 ? 0.25 88.69 ? 0.21 93.61 ? 0.12 92.52 ? 0.15 95.51 ? 0.08</cell></row><row><cell>GRAND+</cell><cell cols="6">88.64 ? 0.09 71.37 ? 0.11 88.74 ? 0.11 94.75 ? 0.12 93.92 ? 0.08 96.47 ? 0.04</cell></row><row><cell>GT</cell><cell cols="6">88.79 ? 0.12 61.05 ? 0.38 91.18 ? 0.17 94.74 ? 0.13 94.64 ? 0.13 97.05 ? 0.05</cell></row><row><cell>Graphormer</cell><cell>OOM</cell><cell>OOM</cell><cell>OOM</cell><cell>92.74 ? 0.14</cell><cell>OOM</cell><cell>OOM</cell></row><row><cell>SAN</cell><cell>88.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>22 ? 0.15 59.01 ? 0.34 89.83 ? 0.16 94.86 ? 0.10 94.51 ? 0.15 OOM NAGphormer 89.17 ? 0.13 71.51 ? 0.13 91.22 ? 0.14 95.49 ? 0.11 95.75 ? 0.09 97.34 ? 0.03</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison of all models in terms of mean accuracy ? stdev (%) on large-scale datasets. The best results appear in bold.</figDesc><table><row><cell>Method</cell><cell>AMiner-CS</cell><cell>Reddit</cell><cell>Amazon2M</cell></row><row><cell>PPRGo</cell><cell cols="3">49.07 ? 0.19 90.38 ? 0.11 66.12 ? 0.59</cell></row><row><cell cols="4">GraphSAINT 51.86 ? 0.21 92.35 ? 0.08 75.21 ? 0.15</cell></row><row><cell>GRAND+</cell><cell cols="3">54.67 ? 0.25 92.81 ? 0.03 75.49 ? 0.11</cell></row><row><cell cols="4">NAGphormer 55.61 ? 0.22 93.52 ? 0.02 76.72 ? 0.27</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Ablation study results on Aminer-CS dataset.</figDesc><table><row><cell>Structual encoding</cell><cell cols="3">Readout function Sum Single Att. (Equation 10)</cell><cell>Accuracy (%)</cell><cell>Gain</cell></row><row><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>50.21</cell><cell>+ 5.40</cell></row><row><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell>52.53</cell><cell>+ 3.08</cell></row><row><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell>54.23</cell><cell>+ 1.38</cell></row><row><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>55.61</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>this work, we propose a powerful and scalable Graph Transformer, called NAGphormer, for the node classification task in attributed graphs. NAGphormer first utilizes Hop2Token to aggregate neighborhoods into a series of representations. Hop2Token can be seen as a preprocessing step before the model training. In this way, NAGphormer can be trained in a mini-batch manner since necessary local information is acquired in advance for each node. Benefited from batch training, NAGphormer can easily handle large-scale networks with millions of nodes. Then NAGphormer subsequently learns the node representations by a standard Transformer encoder. And an attention-based readout function is developed to aggregate the neighborhood information of different hops adaptively. Experiments on various datasets from small to large demonstrate the effectiveness of NAGphormer, outperforming representative Graph Transformers and Graph Neural Networks.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>https://docs.dgl.ai/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On the Bottleneck of Graph Neural Networks and its Practical Implications</title>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scaling Graph Neural Networks with Approximate Pagerank</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Blais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benedek</forename><surname>R?zemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spectral Networks and Locally Connected Networks on Graphs</title>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Measuring and Relieving the Over-smoothing Problem for Graph Neural Networks From the Topological View</title>
		<author>
			<persName><forename type="first">Deli</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scalable Graph Neural Networks via Bidirectional Propagation</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Simple and Deep Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive Universal Generalized PageRank Graph Neural Network</title>
		<author>
			<persName><forename type="first">Eli</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olgica</forename><surname>Milenkovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<title level="m">An Image is Worth 16x16 Words: Transformers for Image</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Recognition at Scale. In ICLR. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A Generalization of Transformer Networks to Graphs</title>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dwivedi</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<idno>CoRR abs/2012.09699</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Graph Neural Networks for Social Recommendation</title>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Yihong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference</title>
		<meeting>the Web Conference</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Wenzheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tinglin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeny</forename><surname>Kharlamov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.06389</idno>
		<title level="m">GRAND+: Scalable Graph Random Neural Networks</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Graph Random Neural Networks for Semi-supervised Learning on Graphs</title>
		<author>
			<persName><forename type="first">Wenzheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeny</forename><surname>Kharlamov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Diffusion Improves Graph Learning</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Gasteiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Wei?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural Message Passing for Quantum Chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Representing Long-Range Context for Graph Neural Networks with Global Attention</title>
		<author>
			<persName><forename type="first">Paras</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Graph Convolutional Networks Meet Markov Random Fields: Semi-supervised Community Detection in Attribute Networks</title>
		<author>
			<persName><forename type="first">Di</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxiao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weixiong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Node Similarity Preserving Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Derr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Web Search and Data Mining</title>
		<meeting>the ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semi-supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Predict then Propagate: Graph Neural Networks meet Personalized PageRank</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rethinking Graph Transformers with Spectral Attention</title>
		<author>
			<persName><forename type="first">Devin</forename><surname>Kreuzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>L?tourneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prudencio</forename><surname>Tossou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</title>
		<author>
			<persName><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno>CoRR abs/2103.14030</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Decoupled Weight Decay Regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Transformer for Graphs: An Overview from Architecture Perspective</title>
		<author>
			<persName><forename type="first">Erxue</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runfa</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yatao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kangfei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.08455</idno>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Query-driven Active Surveying for Collective Classification</title>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>London</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bert</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Umd</forename><surname>Edu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Mining and Learning with Graphs</title>
		<meeting>the International Workshop on Mining and Learning with Graphs</meeting>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Self-Supervised Graph Transformer on Large-Scale Molecular Data</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yatao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiyang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pitfalls of Graph Neural Network Evaluation. Relational Representation Learning Workshop</title>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">2018</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention Is All You Need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A Tutorial on Spectral Clustering</title>
		<author>
			<persName><forename type="first">Ulrike</forename><surname>Von</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luxburg</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On Layer Normalization in the Transformer Architecture</title>
		<author>
			<persName><forename type="first">Ruibin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunchang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tieyan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">How Powerful Are Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Do Transformers Really Perform Badly for Graph Representation</title>
		<author>
			<persName><forename type="first">Chengxuan</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianle</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Graph Convolutional Neural Networks for Web-scale Recommender Systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">GraphSAINT: Graph Sampling Based Inductive Learning Method</title>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><forename type="middle">K</forename><surname>Prasanna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Link Prediction Based on Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">An End-to-End Deep Learning Architecture for Graph Classification</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018-04">2018. 2018. 2018-04</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Jianan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaozhuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianlong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfang</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.13094</idno>
		<title level="m">Gophormer: Ego-Graph Transformer for Node Classification</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Layerdependent Importance Sampling for Training Deep and Large Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Difan</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yewen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
