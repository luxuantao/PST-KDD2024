<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IDEAL RATIO MASK ESTIMATION USING DEEP NEURAL NETWORKS FOR ROBUST SPEECH RECOGNITION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Arun</forename><surname>Narayanan</surname></persName>
							<email>narayaar@cse.ohio-state.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering † Center for Cognitive Science</orgName>
								<orgName type="institution">The Ohio State University</orgName>
								<address>
									<postCode>43210-1277</postCode>
									<settlement>Columbus</settlement>
									<region>OH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Deliang</forename><surname>Wang</surname></persName>
							<email>dwang@cse.ohio-state.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering † Center for Cognitive Science</orgName>
								<orgName type="institution">The Ohio State University</orgName>
								<address>
									<postCode>43210-1277</postCode>
									<settlement>Columbus</settlement>
									<region>OH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">IDEAL RATIO MASK ESTIMATION USING DEEP NEURAL NETWORKS FOR ROBUST SPEECH RECOGNITION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6C5AD2ECDF1DF5C09F9011CD617F1663</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Computational Auditory Scene Analysis</term>
					<term>instantaneous SNR</term>
					<term>noise robust ASR</term>
					<term>Aurora-4</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a feature enhancement algorithm to improve robust automatic speech recognition (ASR). The algorithm estimates a smoothed ideal ratio mask (IRM) in the Mel frequency domain using deep neural networks and a set of time-frequency unit level features that has previously been used to estimate the ideal binary mask. The estimated IRM is used to filter out noise from a noisy Mel spectrogram before performing cepstral feature extraction for ASR. On the noisy subset of the Aurora-4 robust ASR corpus, the proposed enhancement obtains a relative improvement of over 38% in terms of word error rates using ASR models trained in clean conditions, and an improvement of over 14% when the models are trained using the multi-condition training data. In terms of instantaneous SNR estimation performance, the proposed system obtains a mean absolute error of less than 4 dB in most frequency channels.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Noise robust speech recognition is a widely studied research problem with important practical applications <ref type="bibr">[1]</ref>. Several methods aim to extract robust features like RASTA PLP <ref type="bibr" target="#b3">[2]</ref> and AFE <ref type="bibr" target="#b4">[3]</ref>; but merely tuning feature extraction has achieved limited success. Therefore, techniques like model adaptation and feature enhancement are commonly used. Adaptation techniques, like MLLR <ref type="bibr" target="#b5">[4]</ref> and Vector Taylor series (VTS) based adaptation <ref type="bibr" target="#b6">[5,</ref><ref type="bibr" target="#b7">6]</ref>, try to modify the model parameters to match the test conditions better. Such methods are computationally expensive, and may additionally need adaptation data. In contrast, feature enhancement techniques try to remove noise from a given mixture without modifying the model parameters. Such methods are, therefore, computationally more efficient. Examples of feature enhancement techniques include missing feature reconstruction <ref type="bibr" target="#b8">[7]</ref>, Wiener filtering <ref type="bibr" target="#b9">[8]</ref>, and VTS-based enhancement <ref type="bibr" target="#b10">[9]</ref>.</p><p>A popular way to perform feature enhancement is by using computational auditory scene analysis (CASA) based algorithms to perform speech separation prior to recognition. Inspired by the remarkable robustness of human listeners, CASA aims to develop speech separation algorithms motivated by the principles of auditory scene analysis <ref type="bibr" target="#b11">[10]</ref>. A main goal of CASA is to estimate the ideal binary</p><p>The research described in this paper was supported in part by an AFOSR grant (FA9550-12-1-0130). mask (IBM) <ref type="bibr" target="#b12">[11]</ref>, which identifies each unit in a time-frequency (T-F) representation of the noisy signal as speech dominant or noise dominant. With the IBM as the computational goal, the task of separation reduces to a binary classification problem. The IBM has been used for performing feature enhancement (or noise suppression) in ASR by either using the direct masking approach <ref type="bibr" target="#b13">[12]</ref> or by performing reconstruction <ref type="bibr" target="#b8">[7]</ref>. In direct masking, the IBM is used as a binary gain function to attenuate the energy within the noise-dominant T-F units. In reconstruction, the speech energy within the noise dominant units is estimated using the information available in the speech dominant units.</p><p>The performance of both the above methods depend largely on the quality of IBM estimation. Supervised classification-based algorithms have been used to perform the task of IBM estimation for speech separation <ref type="bibr" target="#b14">[13,</ref><ref type="bibr" target="#b15">14]</ref>. Such algorithms extract features at the T-F unit level, and perform classification using learning machines like SVMs and deep neural networks (DNN). One of the goals of this study is to evaluate performance of such algorithms on a robust ASR task. In robust ASR, it has been noted in earlier studies that estimating the ideal ratio mask may result in better performance <ref type="bibr" target="#b16">[15]</ref> <ref type="foot" target="#foot_0">1</ref> . Therefore, we also study: 1) how can such supervised learning algorithms be adapted to estimate the IRM and 2) the potential of such algorithms in improving noise robust ASR performance.</p><p>The rest of the paper is organized as follows. In Section 2, we discuss prior work related to IRM estimation. Section 3 provides the system description. Evaluation results are presented in Section 4. We conclude in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PRIOR WORK</head><p>Soft masks have been used in several robust ASR studies <ref type="bibr" target="#b8">[7,</ref><ref type="bibr" target="#b17">16]</ref>. The values in a soft mask represent the probability of a T-F unit being speech dominant, and are typically used in a missing data framework to perform recognition. The masks are estimated either by applying a sigmoid function to the estimated a priori signal-tonoise-ratio (SNR) <ref type="bibr" target="#b17">[16]</ref>, or by using a Gaussian mixture model of speech to directly predict the posterior probability <ref type="bibr" target="#b8">[7]</ref>. In an alternative approach, Srinivasan et al. estimate the IRM by learning the relationship between the binaural cues of interaural time and level differences, and the instantaneous SNR <ref type="bibr" target="#b16">[15]</ref>. Note that instantaneous SNR is directly related to the IRM value at each T-F unit. They use the estimated IRM to perform feature enhancement and report improvements in ASR performance over using the estimated IBM.</p><p>Recently, van Hout and Alwan propose to estimate a smoothed ratio mask using noise power estimators and a median filter, which they use to perform feature enhancement in the log Mel spectral domain before cepstral transformation <ref type="bibr" target="#b18">[17]</ref>.</p><p>SNR estimation, which is a general task, has been widely studied in the context of speech enhancement. Typical algorithms estimate the a priori SNR which is used to obtain the gain at each T-F unit for enhancement <ref type="bibr" target="#b9">[8]</ref>. A supervised learning algorithm to estimate the instantaneous SNR was proposed by Tchorz and Kollmeier <ref type="bibr" target="#b19">[18]</ref>. Their system uses amplitude modulation spectrograms (AMS) as features and multi-layer perceptron (MLP) as the function estimator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SYSTEM DESCRIPTION</head><p>The proposed system uses a supervised learning algorithm to estimate the IRM. The following subsections describe how the desired target is set, what features are used, and how the mapping function is learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Target signal</head><p>Mathematically, the ideal ratio mask, which is closely related to the Wiener gain, is defined as follows:</p><formula xml:id="formula_0">IRM (m, c) = 10 (SN R(m,c)/10) 10 (SN R(m,c)/10) + 1</formula><p>,</p><formula xml:id="formula_1">and SN R(m, c) = 10 log 10 (x(m, c)/n(m, c)).</formula><p>Here, x(m, c) and n(m, c) denote the instantaneous speech and noise energy, respectively, at time frame m and frequency channel c. SN R(m, c) denotes the instantaneous SNR in dB. Instead of directly estimating the IRM, our system estimates the instantaneous SNR transformed using a tunable sigmoid function:</p><formula xml:id="formula_2">d(m, c) = 1 1 + exp(-α(SN R(m, c) -β)) .<label>(1)</label></formula><p>d(m, c) denotes the desired target while training. α controls the slope of the sigmoid, and β is the bias. By tuning α and β, we can control the range of SNR to focus on while training the system.</p><p>In our experiments we set α to roughly have a 35 dB SNR span 2 centered at β, which is set to -6 dB. β corresponds to the threshold commonly used to define the IBMs <ref type="bibr" target="#b20">[19]</ref>. The SNR to target mapping based on these chosen values is shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>During testing, the output of the system is mapped back to the corresponding IRM values so that they can be used as a filter to perform noise suppression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Features</head><p>We perform mask estimation in the Mel spectral domain, which is a commonly used front-end to perform feature enhancement for ASR. To extract features, the pre-emphasized input signal is first filtered using a 26-channel Mel filterbank that spans frequencies from 50 Hz to 7 kHz. The filterbank is implemented using sixth order butterworth filters. The filter output in each channel is then used to extract the following T-F unit level features: 13 dimensional RASTA filtered PLP cepstral coefficients with delta and acceleration components, 31 dimensional Mel frequency cepstral coefficients (MFCC), 15 dimensional AMS features, and 6 dimensional pitch-based features along 2 We define SNR span as the difference between the instantaneous SNRs corresponding to the desired target values of 0.95 and 0.05. with their time and frequency delta components. While calculating these features the hop size is set to 10 msec; the frame size depends on the feature type -20 msec frames are used for RASTA PLPs, MFCCs, and pitch-based features, and 32 msec frames are used for AMS features (see <ref type="bibr" target="#b21">[20]</ref> for detailed descriptions of how these features are extracted). We use this group of features as it has been found to work well for IBM estimation <ref type="bibr" target="#b21">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Supervised learning</head><p>Following the supervised IBM estimation algorithm proposed in <ref type="bibr" target="#b22">[21]</ref>, we use deep neural networks to learn the function that maps the extracted features to the desired target (see Eq. 1). We take a two stage approach. In the first stage, 26 DNNs are trained, one for each frequency channel, using the features described above. The DNN training schedule includes an unsupervised pre-training phase and a supervised back-propagation phase, each consisting of 100 epochs <ref type="bibr" target="#b23">[22]</ref>. The cross-entropy learning criterion is used during back-propagation. Each DNN has 103 input nodes corresponding to the feature dimensionality, 2 hidden layers each with 200 nodes, and an output layer with 1 node.</p><p>The DNNs learn the function using locally obtained features at each T-F unit, and do not directly make use of the information available in the neighboring units. Therefore, in the second stage, we learn MLPs with 1 hidden layer to smooth the output of the DNN. The MLPs are also trained for each frequency channel. They use the output of the DNNs in a neighborhood surrounding each T-F unit as input, and are trained to re-estimate the same targets as the DNNs. The neighborhood, which was chosen based on ASR performance on a development set, consists of a window of 9 frequency channels and 11 time-frames. The number of nodes in the hidden layer of the MLPs is fixed to 100. Like in the first stage, they are trained for 100 epochs using the cross-entropy learning criterion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental setup</head><p>The proposed system is evaluated using the Aurora-4 dataset <ref type="bibr" target="#b24">[23]</ref>, which is based on the Wall Street Journal corpus <ref type="bibr" target="#b25">[24]</ref>. The DNNs and the MLPs are trained using the noisy utterances from the multicondition training set. These utterances were created by mixing speech with 6 noise types at SNRs ranging from 10 dB to 20 dB. Of the 2676 utterances in the set, 2100 sentences are used to train the system and the rest are used for cross validation and early stopping. The clean and noise signals comprising each mixture is used to set the desired target for training using Eq. 1. For evaluating performance, we use the reduced noisy test sets of the corpus. It consists of 166 clean utterances mixed with the same 6 noise types at SNRs ranging from 5 dB to 15 dB.</p><p>The ASR system is implemented using the HTK toolkit <ref type="bibr" target="#b26">[25]</ref>. The recognition module consists of state tied, word-internal triphones modeled as 3-state HMMs. The observation probability of each state is modeled as a mixture of 16 diagonal Gaussians. The standard bigram language model and the CMU pronunciation dictionary are used during decoding. As features, we use 12th order MFCCs along with their delta and acceleration components. The features are mean and variance normalized at the utterance level to improve robustness. During testing, the noisy signals are filtered using the estimated IRM in the Mel spectral domain before cepstral transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Instantaneous SNR estimation</head><p>We first present the instantaneous SNR estimation performance of the proposed system. The proposed 2-stage system is compared with the following alternatives: 1) a 1-stage system that directly uses the output of the DNN without any smoothing, 2) a 1-stage system that directly estimates the IRM rather than the targets as defined by Eq. 1 (IRM-direct), 3) a system similar to the one proposed by Tchorz and Kollmeier <ref type="bibr" target="#b19">[18]</ref> (TK-AMS). TK-AMS concatenates the AMS features calculated at the T-F unit level to obtain a frame-level feature (dimensionality: 15×26=390). A single DNN is then trained to simultaneously estimate the outputs corresponding to the 26 frequency channels. The architecture of the DNN is the same as used by the proposed system, except that the input and the output layers now consist of 390 and 26 nodes, respectively. The output of each of these systems is converted to decibels to evaluate performance. The ground truth instantaneous SNR values and the estimates are restricted to the range of -15 dB to 10 dB; any estimate out of this range is rounded to these boundary values.</p><p>The mean absolute error averaged across the 6 noise conditions is shown in Fig. <ref type="figure" target="#fig_1">2</ref>. On average, the 1-stage system gives a mean error of 3.0 dB. Smoothing the output further improves the average error by 0.3 dB. Estimating the IRM directly worsens performance by around 1 dB compared to the 2-stage system, which shows the utility of using the sigmoid function to transform the SNRs. TK-AMS produces an average error of 3.7 dB.</p><p>It is interesting to note that the average error for every frequency channel is below 4 dB for the proposed 2-stage algorithm. It performs worst in babble noise and airport noise conditions, where the mean error across all channels is around 3 dB. As expected, it performs the best in the relatively stationary car noise conditions with an average error of 2.3 dB. It can be observed from the figure that the performance of all algorithms drops at higher frequency channels. This is expected since the high frequency region contains more unvoiced speech, which has noise-like characteristics, making it difficult to distinguish it from actual noise. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">ASR performance</head><p>We use both the clean and multi-condition (MC) training sets to train two ASR systems. In clean conditions they produce a word error rate (WER) of 8% and 10.4%, respectively. The performance of the tested feature enhancement algorithms are shown in Table <ref type="table">1</ref>. The baseline performance corresponds to recognizing noisy speech directly without any enhancement. This results in an average WER of 29% when using models trained in clean conditions, and 19.3% using MC training.</p><p>Apart from the systems described before, we also present results obtained using an IBM estimation algorithm (IBM-direct). The IBM-direct system uses binary targets, instead of ratio targets as used by the proposed algorithm, during training. The binary targets are obtained by applying a threshold to the instantaneous SNR at -6 dB. It uses DNNs trained similarly to the proposed 1-stage system (i.e., without any smoothing). The IBM-direct system is most similar to the one proposed in <ref type="bibr" target="#b22">[21]</ref>, which has been shown to perform well for speech separation. The direct masking approach is used to perform feature enhancement when the estimated IBM is used.</p><p>As can be seen, using models trained in clean conditions, the proposed 2-stage system obtains an average WER of 17.9%, 0.7 percentage points better than the 1-stage system and 3.6 percentage points better than IBM-direct. Clearly, estimating the IRM seems more appropriate for the task of ASR. Both 1-stage and 2-stage systems outperform IRM-direct and TK-AMS. It is worth emphasizing that the 2-stage system obtains a large improvement of 11.1 percentage points when compared to the noisy baseline. The difference in performance is not as dramatic when the ASR models are trained using the MC set. The 2-stage system obtains an improvement of 0.5 percentage points over the 1-stage system. The remaining systems obtain similar WERs on average. Compared to the noisy baseline, the 2-stage system obtains an improvement of 2.8 percentage points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>We have proposed a feature enhancement algorithm for improving noise robustness of ASR systems. The algorithm estimates a smoothed ideal ratio mask in the Mel spectrogram domain using deep neural networks, which is used to filter out noise before cepstral transformation. Large improvements were obtained on the Aurora-4 robust ASR task using the proposed system. It is also observed that better ASR performance is obtained using the estimated ratio mask compared to the estimated binary mask. We note that the noise types used in the test set are seen during training. Therefore, an interesting issue for future study is how well the system generalizes to unseen conditions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The instantaneous SNR mapping function that is used to set the desired target during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Instantaneous SNR estimation performance in the Mel spectral domain. 26 frequency channels span frequencies from 50 Hz to 7 kHz.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The IBM can be thought of as a binary approximation to the IRM.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We thank Yuxuan Wang for very helpful discussions and for providing the DNN training scripts.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Word error rates on the noisy subset of the Aurora4 corpus. The proposed systems are denoted as One-stage and Two-stage. RI stands for relative improvement with respect the noisy baseline</title>
	</analytic>
	<monogr>
		<title level="j">System Test set Car Babble Restaurant Street Airport Train Average RI Clean Training Noisy</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
		<title level="m">Techniques for Noise Robustness in Automatic Speech Recognition</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Raj</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Singh</surname></persName>
		</editor>
		<meeting><address><addrLine>West Sussex, UK</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">RASTA processing of speech</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="578" to="589" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Distributed speech recognition; Advanced front-end feature extraction algorithm; Compression algorithms</title>
		<author>
			<persName><surname>Etsi</surname></persName>
		</author>
		<idno>ES 202 050 V1.1.4</idno>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>Speech processing transmission and quality aspects (STQ)</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Maximum likelihood linear transformations for HMM-based speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J F</forename><surname>Gales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer speech and language</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="75" to="98" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A vector taylor series approach for environment-independent speech recognition</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting>the IEEE International Conference on Acoustics, Speech, and Signal Processing</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="733" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A unified framework of HMM adaptation with joint compensation of additive and convolutive distortions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer, Speech, and Language</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="389" to="405" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Missing-feature approaches in speech recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="101" to="116" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Loizou</surname></persName>
		</author>
		<title level="m">Speech Enhancement: Theory and Practice</title>
		<meeting><address><addrLine>Boca Raton, Florida</addrLine></address></meeting>
		<imprint>
			<publisher>CRC Press</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improvements to VTS feature enhancement</title>
		<author>
			<persName><forename type="first">J</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting>the IEEE International Conference on Acoustics, Speech, and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="4677" to="4680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m">Computational Auditory Scene Analysis: Principles, Algorithms, and Applications</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Brown</surname></persName>
		</editor>
		<meeting><address><addrLine>Hoboken, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley/IEEE Press</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On ideal binary masks as the computational goal of auditory scene analysis</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech Separation by Humans and Machines</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Divenyi</surname></persName>
		</editor>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer Academic</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="181" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Nothing doing: Re-evaluating missing feature ASR</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fosler-Lussier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<idno>OSU-CISRC-7/11-TR21</idno>
		<ptr target="ftp://ftp.cse.ohio-state.edu/pub/tech-report/2011/" />
		<imprint>
			<date type="published" when="2011">2011</date>
			<pubPlace>Columbus, Ohio, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science and Engineering, The Ohio State University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Speech segregation based on sound localization</title>
		<author>
			<persName><forename type="first">N</forename><surname>Roman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2236" to="2252" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A Bayesian classifer for spectrographic mask estimation for missing feature speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="379" to="393" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Binary and ratio time-frequency masks for robust speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Roman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1486" to="1501" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Soft decisions in missing data techniques for robust automatic speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Josifovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Cooke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Green</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Spoken Language Processing</title>
		<meeting>the International Conference on Spoken Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="373" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A novel approach to soft-mask estimation and log-spectral enhancement for robust speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Van Hout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alwan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting>the IEEE International Conference on Acoustics, Speech, and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="4105" to="4108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SNR estimation based on amplitude modulation analysis with applications to noise suppression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tchorz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kollmeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="184" to="192" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Speech intelligibility in background noise with ideal binary time-frequency masking</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Kjems</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Boldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lunner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="page" from="2336" to="2347" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exploring monaural features for classification-based speech segregation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="270" to="279" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Towards scaling up classificationbased speech separation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Analysis of the Aurora large vocabulary evalutions</title>
		<author>
			<persName><forename type="first">N</forename><surname>Parihar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Picone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Speech Communication and Technology</title>
		<meeting>the European Conference on Speech Communication and Technology</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="337" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">DARPA TIMIT acoustic phonetic continuous speech corpus</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Garofolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Lamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Fiscus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Pallett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Dahlgren</surname></persName>
		</author>
		<ptr target="http://www.ldc.upenn.edu/Catalog/LDC93S1.html" />
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The HTK Book</title>
		<author>
			<persName><forename type="first">S</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Evermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kershaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Odell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ollason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Valtchev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Woodland</surname></persName>
		</author>
		<ptr target="http://htk.eng.cam.ac.uk" />
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Cambridge University Publishing Department</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
