<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust Color Guided Depth Map Restoration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiaogang</forename><surname>Chen</surname></persName>
							<email>xg.chen@live.com</email>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Yang</surname></persName>
							<email>jieyang@sjtu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Qiang</forename><surname>Wu</surname></persName>
							<email>qiang.wu@uts.edu.au</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Ministry of Education for System Control and Information Processing</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">College of Communication and Art De-sign</orgName>
								<orgName type="institution">University of Shanghai for Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Computing and Communications</orgName>
								<orgName type="institution">Univer-sity of Technology</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Robust Color Guided Depth Map Restoration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8D06CD39EAF482752AD7D59587DF6B8B</idno>
					<idno type="DOI">10.1109/TIP.2016.2612826</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2016.2612826, IEEE Transactions on Image Processing This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2016.2612826, IEEE Transactions on Image Processing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T10:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>color guided depth map restoration</term>
					<term>preserve depth discontinuity</term>
					<term>suppress texture copy artifacts</term>
					<term>ToF</term>
					<term>Kinect</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One of the most challenging issues in color guided depth map restoration is the inconsistency between color edges in guidance color images and depth discontinuities on depth maps. This makes the restored depth map suffer from texture copy artifacts and blurring depth discontinuities. To handle this problem, most state-of-the-art methods design complex guidance weight based on guidance color images and heuristically make use of the bicubic interpolation of the input depth map. In this paper, we show that using bicubic interpolated depth map can blur depth discontinuities when the upsampling factor is large and the input depth map contains large holes and heavy noise.</p><p>In contrast, we propose a robust optimization framework for color guided depth map restoration. By adopting a robust penalty function to model the smoothness term of our model, we show that the proposed method is robust against the inconsistency between color edges and depth discontinuities even when we use simple guidance weight. To the best of our knowledge, we are the first to solve this problem with a principled mathematical formulation rather than previous heuristic weighting schemes. The proposed robust method performs well in suppressing texture copy artifacts. Moreover, it can better preserve sharp depth discontinuities than previous heuristic weighting schemes. Through comprehensive experiments on both simulated data and real data, we show promising performance of the proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>D EPTH maps captured by modern depth cameras such as Kinect and Time-of-Flight (ToF) are usually of low resolution. They are also contaminated by missing depth values and noise. However, many applications require accurate and high resolution depth maps such as object reconstruction, robot navigation and automotive driver assistance. To facilitate the use of depth data, tremendous efforts have been spent on the restoration of depth maps captured by modern depth cameras. As stated by Yang et al. <ref type="bibr" target="#b37">[38]</ref>, for ToF depth maps, their problems are of low resolution and noise. The corresponding restoration mainly focuses on enlarging the spatial resolution and smoothing the noise. For Kinect depth maps, their problems are the missing depth values and noise. The corresponding restoration mainly focuses on filling the "hole" formed with missing depth values and smoothing the noise.</p><p>Yang et al. <ref type="bibr" target="#b37">[38]</ref> also pointed out that the degradation of these two kinds of depth maps could be formulated with the same formulation. Although the restoration tasks of these two kinds of depth maps are different, their analysis <ref type="bibr" target="#b37">[38]</ref> shows that they can be formulated as the same problem. This implies that most methods for ToF depth map restoration are also applicable to Kinect depth map restoration. For example, the work of Yang et al. <ref type="bibr" target="#b37">[38]</ref> can be used for both ToF and Kinect depth map restoration. The work of Park et al. <ref type="bibr" target="#b25">[26]</ref> which was proposed for ToF depth map restoration has also been proved to be applicable to Kinect depth map restoration in their extended work <ref type="bibr" target="#b26">[27]</ref>.</p><p>Inspired by the work of texture synthesis <ref type="bibr" target="#b5">[6]</ref>, Max et al. <ref type="bibr" target="#b21">[22]</ref> first presented an algorithm to synthetically increase the resolution of a single depth map by using a generic database of high resolution local patches. They casted the task of selecting the right candidate from the database at each location on the target depth map as a Markov Random Fields (MRF) labeling problem. Their work was further extended by Li et al. <ref type="bibr" target="#b16">[17]</ref> with more regularization terms such as label coherency term and self-similarity term. Hornacek et al. <ref type="bibr" target="#b10">[11]</ref> proposed to upsample a single depth map without any external database. They upsampled the depth map by identifying and merging patch correspondences within the input depth map itself. Their work exploited patch-wise scene self-similarity across depth with a new 3D variant of patch match. These methods tend to fail to cope with large upsampling factors and scenes with complex structures.</p><p>Another research direction of depth map restoration is to fuse multiple frames of depth measurements into one depth map. Schuon et al. <ref type="bibr" target="#b30">[31]</ref> combined multiple low resolution depth maps captured with different camera centers with an optimization framework that incorporated ToF sensor characteristics. Hahne et al. <ref type="bibr" target="#b7">[8]</ref> combined depth maps taken with different exposure time in order to produce high quality depth maps. By modeling the original HR range as an MRF, Rajagopalan et al. <ref type="bibr" target="#b29">[30]</ref> proposed a Bayesian framework to fuse several low resolution depth images. These methods are based on the assumption that the scene is static. They are more practical for static scenes rather than for dynamic scenes.</p><p>There are also research interests in developing guided restoration schemes which restore the depth map with the guidance of the registered (aligned) color image. Joint Bilateral Upsampling (JBU) <ref type="bibr" target="#b13">[14]</ref> extended the Bilateral Filter (BF) <ref type="bibr" target="#b35">[36]</ref> for depth upsampling where the bilateral weights are based on the guidance color image. Camplani et al. <ref type="bibr" target="#b0">[1]</ref> proposed to restore Kinect depth maps with a variant of JBU. To achieve sharper depth discontinuities, Liu et al. <ref type="bibr" target="#b17">[18]</ref> evaluated pixel dissimilarities based on the geodesic distance instead of the Euclidean distance in JBU <ref type="bibr" target="#b13">[14]</ref>. Min et al. <ref type="bibr" target="#b23">[24]</ref> proposed to enhance the quality of depth map with a novel weighted mode filter. Very recently, a new edge preserving filter named guided image filter <ref type="bibr" target="#b9">[10]</ref> was introduced and used to perform guided upsampling. Their work was further improved by Lu et al. <ref type="bibr" target="#b19">[20]</ref> and Tan et al. <ref type="bibr" target="#b34">[35]</ref> to preserve sharp depth discontinuities and spatial variation. Diebel et al. <ref type="bibr" target="#b1">[2]</ref> performed the restoration using Markov Random Fields (MRF) with a pairwise appearance consistency data term and an image guided smoothness term. Park et al. <ref type="bibr" target="#b25">[26]</ref> proposed an optimization framework which regularized the depth map with a nonlocal structure regularization term in order to maintain fine details and structures. Ham et al. <ref type="bibr" target="#b8">[9]</ref> cast color guided depth upsampling as a graph-based transduction problem. Yang et al. <ref type="bibr">[37] [38]</ref> performed color guided depth map restoration using a color guided Auto-Regressive (AR) model. Sparse representation models <ref type="bibr" target="#b12">[13]</ref> [16] <ref type="bibr" target="#b6">[7]</ref> were also proposed to learn the statistical dependency between color and depth in a scene. When compared with the first two categories of methods, this category of methods can produce promising restoration quality for larger upsampling factors and are also not restricted to static scenes.</p><p>The fundamental assumption of color guided depth map restoration is that there exists joint occurrences between depth discontinuities and the corresponding color image edges. However, when depth discontinuities are inconsistent with the color edges, these methods may have the following two problems: (I) texture copy artifacts in smooth depth regions when the corresponding color image regions are highly textured, (I-I) blurring depth discontinuities when the corresponding color edges are weak (or even no corresponding color edges). To handle these two issues, most recent methods proposed to design complex guidance weight based on guidance color images and heuristically took the bicubic interpolation of the input depth map into account, for example, the definition of the AR coefficient in the color guided AR model proposed by Yang et al. <ref type="bibr" target="#b37">[38]</ref>, the weight of the smoothness term in the weighted least squares model proposed by Park et al. <ref type="bibr" target="#b25">[26]</ref> and the RGB-D structure similarity in the sparse model proposed by Kwon et al. <ref type="bibr" target="#b15">[16]</ref>. However, complex guidance weight does not always help to improve the upsampling quality while its computational cost is quite heavy. Also, the bicubic interpolation of the input depth map becomes unreliable especially when the upsampling factor is large (e.g., 8×) and the input depth map contains large holes and heavy noise. This is the case as the resolution of modern ToF depth cameras (e.g., SwissRanger SR4000) is rather low and their depth maps usually contain heavy noise. Kinect depth maps can also contain large holes when the object contains large black areas. In such cases, their methods often blur depth discontinuities on the restored depth map.</p><p>The method proposed in this paper belongs to the color guided depth map restoration scheme. We attempt to alleviate the above two issues. We propose a robust optimization framework for color guided depth map restoration. By adopting a robust penalty function to model the smoothness term of our model, we show that the proposed method is robust against the inconsistency between color edges and depth discontinuities even when we use simple guidance weight. To the best of our knowledge, it is the first time to solve this problem with a principled mathematical formulation rather than the previous heuristic weighting schemes. The proposed method performs well in suppressing texture copy artifacts. Moreover, it can better preserve sharp depth discontinuities than previous heuristic weighting schemes such as the AR model <ref type="bibr" target="#b37">[38]</ref> and the model proposed by Park et al. <ref type="bibr" target="#b25">[26]</ref> especially for real data.</p><p>The rest of this paper is organized as follows: in Sec. II, we briefly present the Weighted Least Square (WLS) model <ref type="bibr" target="#b22">[23]</ref> and other choices of guidance weighting schemes. Then we compare and analyze these weighting schemes in terms of handling the inconsistency between color edges and depth discontinuities. We propose our robust model and its parameter adaptation based on a robust smoothness measurement in Sec. III. Experimental results and comparison with other state-of-the-art methods are shown in Sec. IV. We draw the conclusion of this paper in Sec. V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK AND MOTIVATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The weighted least squares model</head><p>Given a degraded depth map and the aligned high resolution color image, the degraded depth map can be restored through the Weighted Least Squares (WLS). The WLS is a basic optimization framework that can be applied to many image processing tasks <ref type="bibr">[3] [25]</ref>. Recently, Min et al. <ref type="bibr" target="#b22">[23]</ref> showed that the WLS could also be applied to color guided depth map restoration. The WLS is formulated as:</p><formula xml:id="formula_0">D H = arg min D { ∑ i∈Ω 0 (D i -D 0 i ) 2 + α ∑ i∈Ω ∑ j∈N (i) ω c i,j (D i -D j ) 2 } (1)</formula><p>where D 0 represents the valid measurements that have been mapped to the high resolution coordinate Ω. Ω 0 represents the corresponding coordinate of D 0 . N (i) is the neighborhood of i inside the square patch centered at i<ref type="foot" target="#foot_0">1</ref> . We define ω c i,j as:</p><formula xml:id="formula_1">ω c i,j = exp ( - |i -j| 2 2σ 2 s ) exp ( - ∑ k∈C |I k i -I k j | 2 3 × 2σ 2 c )<label>(2)</label></formula><p>where C = {R, G, B} or C = {Y, U, V } represents different channels of the guidance color image I. σ c and σ s are constants defined by the user.</p><p>This WLS model cannot handle the case where color edges are inconsistent with depth discontinuities. It has the following two problems: (I) When the homogeneous depth region corresponds to a highly textured color area, this region suffers from texture copy artifacts on the restored depth map. (II) When the depth discontinuity corresponds to a weak color edge (or no color edge), it will be blurred on the restored depth map. Taking depth map upsampling for example, Fig. <ref type="figure" target="#fig_1">1(d</ref>) illustrates these two problems on 8× upsampling.</p><p>To handle the above two issues, most state-of-the-art methods adopt two strategies: adopting more complicated color guidance weight and making use of the bicubic interpolation of the input depth map. For example, the Auto-Regressive (AR) coefficient in the color guided AR model <ref type="bibr" target="#b37">[38]</ref> consists of the combination of the "shape-based" guidance weight based on the guidance color image and the bilateral weight based on the bicubic interpolated input depth map. The guidance weight in the model proposed by Park et al. <ref type="bibr" target="#b25">[26]</ref> consists the segmentation, color information and edge saliency of the guidance color image as well as the bicubic interpolated input depth map. The RGB-D structure similarity in the sparse model proposed by Kwon et al. <ref type="bibr" target="#b15">[16]</ref> also adopted the bicubic interpolated input depth map. Promising results were reported in their papers in handling the above two problems. To show the effectiveness of these strategies, we extend the WLS model in Eq. ( <ref type="formula">1</ref>) by replacing its guidance weight in Eq. ( <ref type="formula" target="#formula_1">2</ref>) with the AR coefficient in the AR model <ref type="bibr" target="#b37">[38]</ref>. The AR coefficient is defined as:</p><formula xml:id="formula_2">ai,j = a D i,j a I i,j , a D i,j = exp ( - | Di -Dj| 2 2σ<label>2 1</label></formula><p>)</p><formula xml:id="formula_3">a I i,j = exp ( - ∑ k∈C ∥Bi • (P k i -P k j )∥ 2 3 × 2σ 2 2 )<label>(3)</label></formula><p>where a D i,j is the depth term defined on D which is the bicubic interpolation of the input depth map. a I i,j is the color term defined on the guidance color image I. σ 1 and σ 2 are user defined constants. P k i denotes an operator that extracts a ω × ω patch centered at i in color channel k, "•" represents the element-wise multiplication. The bilateral filter kernel B i is defined as:</p><formula xml:id="formula_4">Bi(i, j) = exp ( - |i -j| 2 2σ<label>2</label></formula><p>3</p><p>) exp</p><formula xml:id="formula_5">( - ∑ k∈C |I k i -I k j | 2 2σ 2 4 )<label>(4)</label></formula><p>where σ 3 and σ 4 are user defined constants.</p><p>By replacing ω c i,j in Eq. ( <ref type="formula">1</ref>) with a i,j in Eq. ( <ref type="formula" target="#formula_3">3</ref>), we obtain another WLS model and we denote it as Extended WLS (EWLS) in this paper. The EWLS can have much better performance than the WLS on handling the inconsistency between color edges and depth discontinuities, i.e. suppressing texture copy artifacts and preserving depth discontinuities. Fig. <ref type="figure" target="#fig_0">1</ref>(e) shows illustrations for 8× upsampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison and analysis of different weighting schemes</head><p>The weight a i,j in Eq. ( <ref type="formula" target="#formula_3">3</ref>) consists of two terms: the depth term a D i,j and the color term a I i,j . Now we show which term contributes to the performance improvement. To this end, we perform another two experiments: (I) Only use the color term a I i,j in Eq. ( <ref type="formula" target="#formula_3">3</ref>) to replace the guidance weight ω c i,j in Eq. ( <ref type="formula">1</ref>). We denote this experiment as Experiment I. (II) Replace the guidance weight ω c i,j in Eq. ( <ref type="formula">1</ref>) with</p><formula xml:id="formula_6">a i,j = ω c i,j a D i,j . ω c i,j</formula><p>is defined in Eq. ( <ref type="formula" target="#formula_1">2</ref>). a D i,j is defined in Eq. ( <ref type="formula" target="#formula_3">3</ref>). We denote this experiment as Experiment II. Then we perform four comparisons: (I) comparison between Experiment I and the WLS defined in Eq. ( <ref type="formula">1</ref>), (II) comparison between Experiment II and the EWLS defined in Sec. II-A, (III) comparison between Experiment I and the EWLS, (IV) comparison between Experiment II and the WLS. The first two comparisons show whether a I i,j defined in Eq. ( <ref type="formula" target="#formula_3">3</ref>) has better performance than ω c i,j defined in Eq. ( <ref type="formula" target="#formula_1">2</ref>). The last two comparisons show whether guidance weight with additional a D i,j defined in Eq. ( <ref type="formula" target="#formula_3">3</ref>) contributes to the performance improvement. For Experiment I, depth discontinuities are severely blurred and texture copy artifacts are quite obvious. For Experiment II, depth discontinuities are properly preserved and texture copy artifacts are also well suppressed. Then we perform the four experimental comparisons mentioned above. Based on the visual comparison, we have the following two observations: (I) The first two comparisons show that the guidance weight a I i,j defined in Eq. ( <ref type="formula" target="#formula_3">3</ref>) makes little performance improvement over the bilateral guidance weight ω c i,j defined in Eq. ( <ref type="formula" target="#formula_1">2</ref>). (II) The last two comparisons show that when the guidance weight makes use of the bicubic interpolation of the input depth map, i.e., containing a D i,j , there is large performance improvement when compared with the results of the guidance weight that is only based on the color image. When the guidance weight contains a D i,j , texture copy artifacts are well suppressed and depth discontinuities are also better preserved. To further validate our observations, we show quantitative comparisons on the simulated ToF dataset and Kinect datasets used in this paper in terms of average Mean Absolute Error (MAE) in Fig. <ref type="figure" target="#fig_2">2</ref>. As shown in the figure, quantitative comparisons also validate the above two observations.  defined in Eq. ( <ref type="formula" target="#formula_1">2</ref>). (e) The kernel computed on the color image based on the a I i,j defined in Eq. ( <ref type="formula" target="#formula_3">3</ref>). (f) The kernel computed on the bicubic interpolation in (c) based on a D i,j defined in Eq. ( <ref type="formula" target="#formula_3">3</ref>). (g) The kernel computed on the updated depth map in the proposed method. (h) Combination of (d) and (f) which is used in Experiment II defined in Sec. II. (i) Combination of (e) and (f) used in the EWLS, i.e. a i,j in Eq. ( <ref type="formula" target="#formula_3">3</ref>). (j) Combination of (d) and (g) used in the proposed method. The upsampling results of (k) the EWLS and (l) the proposed method. Depth maps in the first row are simulated data. Depth maps in the second row are real data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. The drawback of making use of the bicubic interpolated input depth map</head><p>In fact, to make use of the bicubic interpolation of the input depth is to make use of the information from the depth map itself. However, simply making use of the information from the bicubic interpolation is far from good enough. Fig. <ref type="figure" target="#fig_3">3</ref> shows the corresponding kernel of (I) ω c i,j used in the original WLS, (II) a I i,j defined in Eq. ( <ref type="formula" target="#formula_3">3</ref>), (III) a D i,j defined in Eq. ( <ref type="formula" target="#formula_3">3</ref>), (IV) a i,j = ω c i,j a D i,j used in Experiment II, (V) a i,j = a D i,j a I i,j used in the EWLS. It can be observed that depth discontinuities on the bicubic interpolation of the noisy low resolution input are quite blurring as shown in Fig. <ref type="figure" target="#fig_3">3(b)</ref>. Kernel a D i,j based on the bicubic interpolation is also quite blurring as shown in Fig. <ref type="figure" target="#fig_3">3(f)</ref>. When combined with this kernel, kernel a i,j = ω c i,j a D i,j in Fig. <ref type="figure" target="#fig_3">3</ref>(h) and kernel a i,j = a D i,j a I i,j in Fig. <ref type="figure" target="#fig_3">3</ref>(i) are also blurred. As a result, depth discontinuities on the final restored depth map are also blurred as shown in Fig. <ref type="figure" target="#fig_3">3(k</ref>). In fact, as shown in the second row in Fig. <ref type="figure" target="#fig_3">3</ref>, the quality of real data is much worse than simulated data. Blurring depth discontinuities are not the only problem of the bicubic interpolated depth map. As shown in Fig. <ref type="figure" target="#fig_3">3(b)</ref>, pixels belong to the background are assigned to depth values of the foreground. As a result, kernels in Fig. <ref type="figure" target="#fig_3">3</ref>(f), (h) and (i) are not only blurred but also incorrect. This results in the bad restoration shown in Fig. <ref type="figure" target="#fig_3">3(k)</ref>. From results of both the simulated data and real data, it shows that a D i,j based on the bicubic interpolation is not good enough to preserve sharp depth discontinuities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE PROPOSED METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The robust model</head><p>The smoothness term of the WLS in Eq. ( <ref type="formula">1</ref>) is modeled with L 2 norm. The L 2 norm is known to be not robust against outliers in relevant research areas such as image denoising <ref type="bibr" target="#b3">[4]</ref>  <ref type="bibr" target="#b27">[28]</ref>. In the previous sections, we also show it is not robust against the inconsistency between color edges and depth discontinuities in color guided depth map restoration.</p><p>In this section, we propose to penalize the smoothness term with a robust penalty function instead of L 2 norm. The proposed optimization framework is defined as:</p><formula xml:id="formula_7">D H = arg min D { ∑ i∈Ω 0 (D i -D 0 i ) 2 + α ∑ i∈Ω ∑ j∈N (i) ω c i,j ψ(|D i -D j | 2 )}<label>(5)</label></formula><p>where ψ(•) is defined as:</p><formula xml:id="formula_8">ψ ( x 2 ) = 2µ 2 ( 1 -exp ( - x 2 2µ<label>2</label></formula><p>))</p><p>where µ is a user given parameter. We will present a data driven approach in Sec. III-D to properly adapt µ according to the smoothness property of depth maps.</p><p>As our analysis shown in Sec. II, the color guidance weight a I i,j in Eq. ( <ref type="formula" target="#formula_3">3</ref>) makes little contribution to the performance improvement. Note that a I i,j is based on the patch to patch difference. It is much more time-consuming than ω c i,j in Eq. ( <ref type="formula" target="#formula_1">2</ref>) which is based on the pixel to pixel difference. We thus adopt ω c i,j as the color guidance weight in our model. In fact, Eq. ( <ref type="formula" target="#formula_9">6</ref>) has long been used as a robust penalty function in relevant areas such as image denoising <ref type="bibr" target="#b28">[29]</ref> and optical flow <ref type="bibr" target="#b38">[39]</ref> to preserve sharp edges. In this paper, we introduce it to the color guided depth map restoration to handle the inconsistency between color edges and depth discontinuities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Analysis of the model</head><p>Now we analyze why the proposed model is robust against the inconsistency between color edges and depth discontinuities. First, we present the normal equation of our model:</p><formula xml:id="formula_10">(Di -D 0 i ) + 2α ∑ j∈N (i) ω c i,j di,j(Di -Dj) = 0, i ∈ Ω (7)</formula><p>where we define:</p><formula xml:id="formula_11">di,j = ψ ′ (|Di -Dj| 2 ) = exp ( - |Di -Dj| 2 2µ 2 )<label>(8)</label></formula><p>The closed-form solution to Eq. ( <ref type="formula">7</ref>) is not available. Thus it should be solved iteratively. If we keep d i,j as constant in each iteration where</p><formula xml:id="formula_12">d n i,j = ψ ′ (|D n i -D n j | 2 )</formula><p>for iteration n+1, then Eq. ( <ref type="formula">7</ref>) becomes the normal equation of the following iteratively re-weighted least squares optimization framework:</p><formula xml:id="formula_13">D n+1 = arg min D { ∑ i∈Ω 0 (D i -D 0 i ) 2 + α ∑ i∈Ω ∑ j∈N (i) ω i,j (D i -D j ) 2 } (<label>9</label></formula><formula xml:id="formula_14">)</formula><p>where ω i,j = ω c i,j d n i,j . Now we compare the guidance weight ω i,j in Eq. ( <ref type="formula" target="#formula_13">9</ref>) with the guidance weight ω c i,j of WLS in Eq. ( <ref type="formula" target="#formula_1">2</ref>) and the guidance weight a i,j of the EWLS in Eq. (3). The ω c i,j only based on the guidance color image results in blurring depth discontinuities and texture copy artifacts. The a i,j based on the guidance color image and the bicubic interpolation of the input can suppress texture copy artifacts but depth discontinuities are still blurred. The ω i,j is based on not only the guidance color image but also the newly updated depth map in each iteration. The quality of the newly updated depth map is much better than that of the bicubic interpolated depth map. This helps our model to not only suppress texture copy artifacts but also preserve sharper depth discontinuities than the EWLS. This is also validated in Fig. <ref type="figure" target="#fig_0">1</ref>(h), Fig. <ref type="figure" target="#fig_3">3</ref>(j) and Fig. <ref type="figure" target="#fig_3">3(l)</ref> where the proposed method results in both the sharp kernel and sharp depth discontinuities on the restored depth maps. Note that the proposed method is especially effective for real data as shown in Fig. <ref type="figure" target="#fig_3">3</ref>. Fig. <ref type="figure" target="#fig_2">2(e</ref>) also shows the comparison between our method and Experiment II (using bicubic interpolation of the input depth map) on different datasets in terms of average MAE. The improvement of the average MAE also validates the effectiveness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. The numerical solution</head><p>We start from the normal equation of our model in Eq. ( <ref type="formula">7</ref>). If we keep d i,j as constant in each iteration where</p><formula xml:id="formula_15">d n i,j = ψ ′ (|D n i -D n j | 2</formula><p>) for iteration n + 1, then we can solve it through its fixed point equation. It is defined as:</p><formula xml:id="formula_16">D n+1 i = D 0 i + 2α ∑ j∈N (i) ω c i,j d n i,j D n j 1 + 2α ∑ j∈N (i) ω c i,j d n i,j , i ∈ Ω<label>(10)</label></formula><p>We initialize Eq. ( <ref type="formula" target="#formula_16">10</ref>) with the bicubic interpolation of the input depth map. As the bicubic interpolation has already been close to the "groundtruth", we thus assume that it satisfies the condition of the fixed point equation. We iteratively update the depth map through Eq. ( <ref type="formula" target="#formula_16">10</ref>) to get the final output. According our experimental results, it works well for satisfying results.</p><p>In fact, we can solve the proposed model in Eq. ( <ref type="formula" target="#formula_7">5</ref>) by iteratively solving the normal equation in Eq. ( <ref type="formula">7</ref>) in each iteration. Eq. ( <ref type="formula">7</ref>) is a linear system which can be solved by modern solvers such as Preconditioned Conjugate Gradient (PCG) <ref type="bibr" target="#b14">[15]</ref>. However, one of the most challenging problems is that solving this linear system needs a large amount of memory to store the affinity matrix when the size of the neighborhood system N (i) becomes large (e.g., <ref type="bibr">19 × 19)</ref>. It cannot be accomplished without a computer of large memory. On the contrary, the memory cost of Eq. ( <ref type="formula" target="#formula_16">10</ref>) is quite little even for the 19×19 neighborhood system. It can be accomplished even with a laptop of 4GB memory. According to our experimental results, both these two methods can get similar results. We thus solve our model through Eq. <ref type="bibr" target="#b9">(10)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Parameter adaptation based on local relative smoothness</head><p>The parameter µ in Eq. ( <ref type="formula" target="#formula_9">6</ref>) is an important parameter in our model. We denote µ as the bandwidth of our model in this paper. Small bandwidth can well preserve depth discontinuities but performs worse in smoothing noise. On the contrary, large bandwidth performs better in smoothing noise but can blur depth discontinuities. A better choice of bandwidth based on the local smoothness of the depth map can result in both better noise smoothing and depth discontinuity preserving. To this end, we first adopt our previous work <ref type="bibr" target="#b18">[19]</ref> to measure the local smoothness of the depth map as follows:</p><formula xml:id="formula_17">ρi = ∑ s∈Nρ(x min ) xs ∑ t∈Nρ(xmax) xt , i ∈ Ω<label>(11)</label></formula><p>where x min and x max are the minimal and maximal depth values inside N (i). N (i) is the neighborhood of pixel i. N ρ (x min )/N ρ (x max ) denotes a small patch of radius r ρ × r ρ centered at the pixel whose value is x min /x max . This makes ρ i robust against the noise. We set r ρ = r where r is the radius of N (i). ρ i is denoted as the relative smoothness of a given depth patch in this paper. In our experiments, we iteratively update the relative smoothness based on the newly updated depth map in Eq. ( <ref type="formula" target="#formula_16">10</ref>) in each iteration.</p><p>After we get the local relative smoothness of the depth map, we can properly adapt the bandwidth for each pixel according to ρ i (i ∈ Ω). However, unlike our previous work <ref type="bibr" target="#b18">[19]</ref> which only divided a depth map into homogenous regions and depth discontinuities, we find that depth discontinuities should be explored on more fine-grained levels. Our experiments show  <ref type="formula" target="#formula_18">12</ref>). (f) Illustration of the relative smoothness and (g) the corresponding adapted bandwidth based on Eq. ( <ref type="formula" target="#formula_18">12</ref>). (h) Quantitative comparison between the results restored with bandwidth adaptation and without adaptation in terms of average MAE.</p><p>that not all depth discontinuities favor small bandwidth µ. Weak depth discontinuities favor smaller bandwidth while strong depth discontinuities favor larger bandwidth. If we use small bandwidth for strong depth discontinuities, there will be multiple layers around the discontinuities. We illustrate this in Fig. <ref type="figure" target="#fig_4">4</ref> with 8× upsampling examples. Fig. <ref type="figure" target="#fig_4">4(c)</ref> shows the results restored with our method using µ = 10 (large bandwidth) where fine details and weak discontinuities in the second row are heavily blurred. Fig. <ref type="figure" target="#fig_4">4(d)</ref> shows the results restored with our method using µ = 4 (small bandwidth) where fine details and weak discontinuities are properly preserved. However, the strong discontinuities in the first row result in multiple layers which are "blurred". Thus, depth discontinuities of different scales on the depth map should be treated differently: weak depth discontinuities need smaller bandwidth. On the contrary, strong depth discontinuities need larger bandwidth. Luckily, the value of the proposed relative smoothness in Eq. ( <ref type="formula" target="#formula_17">11</ref>) can properly reflect the strength of depth discontinuities: strong depth discontinuities have smaller µ than weak depth discontinuities.</p><p>After we have made all this clear, we can properly adapt the bandwidth µ for each pixel i ∈ Ω according to the corresponding relative smoothness ρ i . Here we show one possible parameter setting that can yield satisfying results in our experiments. Assuming all the depth values on the depth map are in interval [0, 255], then we adopt the bandwidth by directly thresholding the relative smoothness as follows:</p><formula xml:id="formula_18">µi =      10, if ρi ≥ 0.96, 4, if 0.8 ≤ ρi &lt; 0.96, 7, if 0.7 ≤ ρi &lt; 0.8, 10, if ρi &lt; 0.7, i ∈ Ω (<label>12</label></formula><formula xml:id="formula_19">)</formula><p>The first inequality in Eq. ( <ref type="formula" target="#formula_18">12</ref>) means that the depth map N (i) locates in homogeneous region which needs large bandwidth to smooth the noise. The second to the fourth inequalities mean that N (i) contains a depth discontinuity. As the relative smoothness becomes smaller, the strength of the discontinuity becomes stronger and the bandwidth should become larger based on our analysis above. Our experimental results show that this simple thresholding is enough for satisfying results. Note that the value of µ i in Eq. ( <ref type="formula" target="#formula_18">12</ref>) is based on the assumption that depth values are in interval [0, 255]. If depth values on the depth map are not in [0, 255], it should be normalized into [0, 255]. However, the condition of ρ i on the right side of Eq. ( <ref type="formula" target="#formula_18">12</ref>) does not depend on either the range of depth values or the noise on the depth map since we always have ρ i ∈ (0, 1]. If the noise on the depth map becomes larger, to guarantee the accuracy of Eq. ( <ref type="formula" target="#formula_18">12</ref>), the radius r ρ of Eq. ( <ref type="formula" target="#formula_17">11</ref>) should become larger to make ρ i more robust against the noise. Fig. <ref type="figure" target="#fig_4">4</ref>(e) shows the results by our method with the proposed parameter adaptation. It shows that both the strong depth discontinuities in the first row and the weak depth discontinuities in the second row are well preserved. To further validate the effectiveness of the proposed parameter adaptation, we perform experiments on both ToF and Kinect datasets. Quantitative comparison in terms of average MAE between our method with parameter adaptation and without parameter adaptation is shown in Fig. <ref type="figure" target="#fig_4">4</ref>(h). The parameter adaptation also helps to achieve lower average MAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we test the proposed method on both simulated and real ToF and Kinect datasets. Parameters are set as follows: α in Eq. ( <ref type="formula" target="#formula_7">5</ref>) is set as 0.95. r of the neighborhood N (i)(i ∈ Ω) is set as 9. The bandwidth of the robust penalty function in Eq. ( <ref type="formula" target="#formula_9">6</ref>) is set as Eq. <ref type="bibr" target="#b11">(12)</ref>. Depth values of all the depth maps are firstly normalized into [0, 255] when performing the restoration and then normalized back to their own depth range for quantitative comparison. All the parameters remain the same for all the test datasets. We also show visual and quantitative comparison between the proposed method and other state-of-the-art methods including Joint Bilateral Upsampling (JBU) <ref type="bibr" target="#b13">[14]</ref>, Joint Geodesic Upsampling (JGU) <ref type="bibr" target="#b17">[18]</ref>, the nonlocal means regularized weighted least squares <ref type="bibr" target="#b25">[26]</ref> that we denote as NLM-WLS, the Joint Intensity and Depth (JID) co-sparse analysis model <ref type="bibr" target="#b12">[13]</ref>, the Total Generalized Variation (TGV) guided depth upsampling <ref type="bibr" target="#b4">[5]</ref>, the Weighted Least Squares (WLS) <ref type="bibr" target="#b22">[23]</ref> and the color guided Auto-Regressive (AR) model <ref type="bibr" target="#b37">[38]</ref> 2 . 2 We re-implement the AR model with C language based on the author provided MATLAB source code. The author provided source code is available here: http://cs.tju.edu.cn/faculty/likun/projects/depth recovery/index.htm. Our implementation is available here:https://github.com/wliusjtu/ Adaptive-Auto-regressive-Model-for-Guided-Depth-Map-Restoration. Our implementation is much faster and can handle much larger depth maps.    GF <ref type="bibr" target="#b9">[10]</ref> JGU <ref type="bibr" target="#b17">[18]</ref> JBU <ref type="bibr" target="#b13">[14]</ref> NLM-WLS <ref type="bibr" target="#b25">[26]</ref> JID <ref type="bibr" target="#b12">[13]</ref> AR <ref type="bibr" target="#b37">[38]</ref> WLS   (e) the WLS <ref type="bibr" target="#b22">[23]</ref>, (f) the AR model <ref type="bibr" target="#b37">[38]</ref> and (g) the proposed method. (h) Bandwidth maps of the proposed method. Regions in red boxes are highlighted. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experiments on ToF depth maps upsampling</head><p>We first test our method on ToF depth map upsampling with the simulated ToF dataset. Two kinds of degradation are simulated: downsampling and downsampling with noise. The noisy downsampled depth maps are from the dataset provided by Yang et al. <ref type="bibr" target="#b37">[38]</ref>. Four umpsampling factors are tested in this paper: 2 × /4 × /8 × /16×.</p><p>Table <ref type="table" target="#tab_0">I</ref> shows the quantitative comparison of upsampling results of noise free depth maps in terms of MAE. Both our method and the TGV <ref type="bibr" target="#b4">[5]</ref> outperform other methods in terms of lower MAE. Fig. <ref type="figure">5</ref> shows illustration of 8× upsampling. The corresponding error maps are shown in Fig. <ref type="figure">6</ref> where we show our methods performs better in preserving sharp depth discontinuities than other methods.</p><p>Table <ref type="table" target="#tab_1">II</ref> shows the quantitative comparison of noisy simulated ToF upsampling results. Our method outperforms other compared state-of-the-art methods with lowest MAE. As shown in the highlighted regions, when the edges in the guidance color image are weak, the corresponding depth discontinuities on the upsampled depth map by WLS <ref type="bibr" target="#b22">[23]</ref> are severely blurred. This is clearly illustrated on the corresponding error maps in Fig. <ref type="figure" target="#fig_7">8(c</ref>). Errors are quite large around the depth discontinuities. Note that there are also noticeable errors    <ref type="bibr" target="#b12">[13]</ref>, (e) the WLS <ref type="bibr" target="#b22">[23]</ref>, (f) the AR model <ref type="bibr" target="#b37">[38]</ref> and (g) the proposed method. (h) Bandwidth maps of the proposed method. Regions in red boxes are highlighted. The depth map in the first row is from Kinect dataset 2. The depth map in the second row is from Kinect dataset 1. in the homogeneous regions in Fig. <ref type="figure" target="#fig_7">8(c</ref>) which reflect texture copy artifacts on the upsampled depth map. Results of the NLM-WLS <ref type="bibr" target="#b25">[26]</ref> in Fig. <ref type="figure" target="#fig_6">7(c</ref>) and Fig. <ref type="figure" target="#fig_7">8</ref>(a) are similar with that of the WLS <ref type="bibr" target="#b22">[23]</ref>. The AR model <ref type="bibr" target="#b37">[38]</ref> can properly suppress texture copy artifacts. However, depth discontinuities are still blurred. This is because the depth term in its AR coefficient is only based on the bicubic interpolation of the noisy low resolution input. On the contrary, our method can properly preserve sharp depth discontinuities. Our results have much smaller errors around the depth discontinuities in Fig. <ref type="figure" target="#fig_7">8(e)</ref> than that of the AR model in Fig. <ref type="figure" target="#fig_7">8(d)</ref>. Moreover, our method can preserve fine details, for example, the "small holes" in the rings as highlighted in the first row of Fig. <ref type="figure" target="#fig_6">7(g)</ref>. Bandwidth maps of our method shown in Fig. <ref type="figure" target="#fig_6">7</ref>(h) also well correspond to the property of the corresponding depth maps. Weak depth discontinuities are assigned to small bandwidth value (the ones in blue) while strong depth discontinuities are assigned to large bandwidth value (the ones in red). This helps our method to properly preserve depth discontinuities of different scales and fine details as shown in the highlighted regions.</p><p>We also test our method on the real ToF dataset <ref type="bibr" target="#b4">[5]</ref> which contains three depth maps: books, devil and shark. The noisy  <ref type="bibr" target="#b25">[26]</ref>, (d) the JID <ref type="bibr" target="#b12">[13]</ref>, (e) the WLS <ref type="bibr" target="#b22">[23]</ref>, (f) the AR model <ref type="bibr" target="#b37">[38]</ref> and (g) the proposed method. (h) Bandwidth maps of the proposed method. Regions in red boxes are highlighted. The depth maps in the first two rows are captured by Kinect v1 from the NYU Kinect dataset <ref type="bibr" target="#b32">[33]</ref>. The depth maps in the last two rows are captured by Kinect v2 from the SUN RBGD dataset <ref type="bibr" target="#b33">[34]</ref>. low resolution depth maps are captured by a PMD Nano ToF camera delivering a 120 × 160 dense depth map and IR amplitude image. The intensity image is captured by a CMOS camera with the sensor size of 810 × 610 pixels. The upsampling factor is about 6.25×. Also, this dataset provides groundtruth measurements which are generated by using a structured light scanner with high accuracy. Table <ref type="table" target="#tab_2">III</ref> shows the quantitative comparison of our method with other methods in terms of MAE. The MAE is measured in mm between the upsampled results and the corresponding groundtruth. Fig. <ref type="figure" target="#fig_9">9</ref> shows visual comparison. We also show the bicubic interpolation of the input in Fig. <ref type="figure" target="#fig_9">9(c</ref>). As shown in the figure, the bicubic interpolation is quite noisy and also contains jaggy depth discontinuities, for example, the highlighted "rock" in the first row. As a result, depth discontinuities in the results of AR model <ref type="bibr" target="#b37">[38]</ref> are not only blurred but also jaggy. This problem is well handled by our method as show in Fig. <ref type="figure" target="#fig_9">9</ref>(g). The corresponding error maps are shown in Fig. <ref type="figure" target="#fig_10">10</ref> for better comparison. Unlike the simulated results in Fig. <ref type="figure" target="#fig_7">8(d</ref>), the AR model cannot well smooth the noise in homogeneous regions as shown in Fig. <ref type="figure" target="#fig_10">10(d)</ref>. This is also due to the heavy noise in the bicubic interpolation. Bandwidth maps in Fig. <ref type="figure" target="#fig_9">9</ref>(h) also show that our parameter adaptation scheme is robust against noise and can well handle real data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiments on Kinect depth maps restoration</head><p>We also test our method on Kinect depth map restoration. Two simulated Kinect datasets are used in this paper: we reuse the dataset in <ref type="bibr" target="#b20">[21]</ref> that we denote as Kinect Dataset 1. There are eight depth maps in Dataset 1 which are shown in the first row of Fig. <ref type="figure" target="#fig_11">11</ref>. We also reuse the simulated Kinect dataset in <ref type="bibr" target="#b37">[38]</ref>. We denote this dataset as Kinect Dataset 2. This dataset contains six depth maps. However, depth maps in this dataset are noise free and only contain structure missing depth values. According to the analysis by Khoshelham et al. <ref type="bibr" target="#b11">[12]</ref>, the noise level on Kinect depth maps is proportional to the square of depth values. So we add additive Gaussian noise to the depth maps where the variance of noise at each pixel is proportional to the square of noise free depth value. This kind of simulation was also adopted by Park et al. <ref type="bibr" target="#b25">[26]</ref>. According to the analysis by Shen et al. <ref type="bibr" target="#b31">[32]</ref>, Kinect depth maps should also contain random missing depth values other than structure missing depth values, so we also add random missing values to the depth maps. This kind of simulation is also adopted by Yang et al. <ref type="bibr" target="#b36">[37]</ref>. The final simulated Kinect depth maps in Kinect dataset 2 are shown in the second row of Fig. <ref type="figure" target="#fig_11">11</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Comparison on 8× upsampling. (a) The groundtruth depth maps. (b) The color images. (c) The bicubic interpolation of the noisy low resolution depth maps. Results obtained by (d) the WLS defined in Eq. (1), (e) the EWLS defined in Sec. II, (f) Experiment I and (g) Experiment II defined in Sec. II, (h) the proposed method.</figDesc><graphic coords="3,48.96,56.55,514.21,135.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 (</head><label>1</label><figDesc>Fig.1(f) shows the experimental results of Experiment I and Fig.1(g) shows the experimental results of Experiment II. For Experiment I, depth discontinuities are severely blurred and texture copy artifacts are quite obvious. For Experiment II, depth discontinuities are properly preserved and texture copy artifacts are also well suppressed. Then we perform the four experimental comparisons mentioned above. Based on the visual comparison, we have the following two observations: (I) The first two comparisons show that the guidance weight a I i,j defined in Eq. (3) makes little performance improvement over the bilateral guidance weight ω c i,j defined in Eq. (2). (II) The last two comparisons show that when the guidance weight makes use of the bicubic interpolation of the input depth map, i.e., containing a D i,j , there is large performance improvement when compared with the results of the guidance weight that is only based on the color image. When the guidance weight contains a D i,j , texture copy artifacts are well suppressed and depth discontinuities are also better preserved. To further validate our observations, we show quantitative comparisons on the simulated ToF dataset and Kinect datasets used in this paper in terms of average Mean Absolute Error (MAE) in Fig.2. As shown in the figure, quantitative comparisons also validate the above two observations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Quantitative comparisons on simulated ToF dataset and Kinect datasets in terms of average MAE. Comparison between (a) the WLS and Experiment I, (b) the EWLS and Experiment II, (c) the WLS and Experiment II, (d) Experiment I and the EWLS. (e) Experiment II and our method. Experiment I and Experiment II are defined in Sec. II. Comparisons in (a) and (b) show whether a I i,j defined in Eq. (3) has better performance than ω c i,j defined in Eq. (2). Comparisons in (c) and (d) show whether a D i,j defined in Eq. (3) contributes to the performance improvement.</figDesc><graphic coords="4,48.96,226.85,251.15,231.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Kernel comparison. (a) The color images. (b) The bicubic interpolation of the noisy low resolution depth maps. (c) The newly updated depth map in the proposed method. Kernels with respect to the central pixel labeled with the red box. (d) The kernel computed on the color image based on the ω c i,j</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. (a) The color images. (b) The groundtruth depth maps. Depth maps restored by our method using (c) large bandwidth µ = 10, (d) small bandwidth µ = 4 and (e) adapted bandwidth based on Eq. (12). (f) Illustration of the relative smoothness and (g) the corresponding adapted bandwidth based on Eq. (12). (h) Quantitative comparison between the results restored with bandwidth adaptation and without adaptation in terms of average MAE.</figDesc><graphic coords="6,48.96,56.62,514.27,104.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Fig. 5. 8× upsampling of simulated ToF data without noise. (a) Color images. (b) The groundtruth. Upsampling Results of (c) the NLM-WLS [26], (d) the TGV<ref type="bibr" target="#b4">[5]</ref>, (e) the WLS<ref type="bibr" target="#b22">[23]</ref>, (f) the AR model<ref type="bibr" target="#b37">[38]</ref> and (g) the proposed method. (h) Bandwidth maps of the proposed method. Regions in red boxes are highlighted.</figDesc><graphic coords="7,48.96,438.06,513.95,90.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. 8× upsampling of noisy simulated ToF data. (a) Color images. (b) The groundtruth. Upsampling Results of (c) the NLM-WLS<ref type="bibr" target="#b25">[26]</ref>, (d) the TGV<ref type="bibr" target="#b4">[5]</ref>, (e) the WLS<ref type="bibr" target="#b22">[23]</ref>, (f) the AR model<ref type="bibr" target="#b37">[38]</ref> and (g) the proposed method. (h) Bandwidth maps of the proposed method. Regions in red boxes are highlighted.</figDesc><graphic coords="8,48.96,56.63,514.14,176.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Corresponding error maps of the results in Fig. 7. (a) the NLM-WLS [26], (b) the TGV [5], (c) the WLS [23], (d) the AR model<ref type="bibr" target="#b37">[38]</ref> and (e) the proposed method.</figDesc><graphic coords="8,48.96,275.70,514.08,270.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Fig. further shows visual comparison of 8× upsampling results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Experimental results of real ToF data. (a) Intensity images. (b) The groundtruth. (c) The bicubic interpolation of the input depth maps. UpsamplingResults of (d) the TGV<ref type="bibr" target="#b4">[5]</ref>, (e) the WLS<ref type="bibr" target="#b22">[23]</ref>, (f) the AR model<ref type="bibr" target="#b37">[38]</ref> and (g) the proposed method. (h) Bandwidth maps of the proposed method. Regions in red boxes are highlighted.</figDesc><graphic coords="9,48.96,61.70,514.19,162.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Corresponding error maps of the results in Fig. 9 (a) The bicubic interpolation of the input depth maps. (b)The TGV<ref type="bibr" target="#b4">[5]</ref>, (c) the WLS<ref type="bibr" target="#b22">[23]</ref>, (d) the AR model<ref type="bibr" target="#b37">[38]</ref> and (e) the proposed method.</figDesc><graphic coords="9,48.96,284.11,514.13,252.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 .</head><label>11</label><figDesc>Fig.11. Simulated Kinect datasets used in this paper. Kinect dataset 1 in the first row are from<ref type="bibr" target="#b20">[21]</ref>. Kinect dataset 2 in the second row are slightly changed by adding noise and random missing depth values to the original data in<ref type="bibr" target="#b37">[38]</ref>.</figDesc><graphic coords="9,48.96,587.30,514.15,125.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Corresponding error maps of the results in Fig. 12. (a) the NLM-WLS [26], (b) the JID<ref type="bibr" target="#b12">[13]</ref>, (c) the WLS<ref type="bibr" target="#b22">[23]</ref>, (d) the AR model<ref type="bibr" target="#b37">[38]</ref> and (e) the proposed method.</figDesc><graphic coords="10,48.96,290.42,514.18,274.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Experimental results of real Kinect data. (a) Color images. (b)The degraded depth maps. Restored results of (c) the NLM-WLS<ref type="bibr" target="#b25">[26]</ref>, (d) the JID<ref type="bibr" target="#b12">[13]</ref>, (e) the WLS<ref type="bibr" target="#b22">[23]</ref>, (f) the AR model<ref type="bibr" target="#b37">[38]</ref> and (g) the proposed method. (h) Bandwidth maps of the proposed method. Regions in red boxes are highlighted. The depth maps in the first two rows are captured by Kinect v1 from the NYU Kinect dataset<ref type="bibr" target="#b32">[33]</ref>. The depth maps in the last two rows are captured by Kinect v2 from the SUN RBGD dataset<ref type="bibr" target="#b33">[34]</ref>.</figDesc><graphic coords="11,48.96,56.80,513.86,336.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I QUANTITATIVE</head><label>I</label><figDesc>COMPARISON ON THE NOISE FREE SIMULATED TOF DATA. RESULTS ARE EVALUATED IN MAE AND THE BEST RESULTS ARE IN BOLD.</figDesc><table><row><cell></cell><cell></cell><cell>Art</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Book</cell><cell></cell><cell></cell><cell cols="2">Dolls</cell><cell></cell><cell></cell><cell cols="2">Laundry</cell><cell></cell><cell></cell><cell cols="2">Moebius</cell><cell></cell><cell></cell><cell cols="2">Reindeer</cell></row><row><cell></cell><cell>2×</cell><cell>4×</cell><cell>8×</cell><cell>16×</cell><cell>2×</cell><cell>4×</cell><cell>8×</cell><cell>16×</cell><cell>2×</cell><cell>4×</cell><cell>8×</cell><cell>16×</cell><cell>2×</cell><cell>4×</cell><cell>8×</cell><cell>16×</cell><cell>2×</cell><cell>4×</cell><cell>8×</cell><cell>16×</cell><cell>2×</cell><cell>4×</cell><cell>8×</cell><cell>16×</cell></row><row><cell>JBU [14]</cell><cell cols="3">0.67 0.97 1.64</cell><cell>3.39</cell><cell>0.3</cell><cell>0.4</cell><cell>0.59</cell><cell>1.12</cell><cell cols="3">0.37 0.45 0.62</cell><cell>1.08</cell><cell>0.4</cell><cell cols="2">0.55 0.88</cell><cell>1.76</cell><cell>0.31</cell><cell>0.4</cell><cell>0.6</cell><cell>1.11</cell><cell>0.44</cell><cell cols="2">0.59 0.88</cell><cell>1.65</cell></row><row><cell>JGU [18]</cell><cell>0.66</cell><cell>1</cell><cell>1.76</cell><cell>3.59</cell><cell>0.3</cell><cell>0.4</cell><cell>0.61</cell><cell>1.18</cell><cell cols="3">0.37 0.45 0.64</cell><cell>1.16</cell><cell>0.4</cell><cell cols="2">0.57 0.96</cell><cell>1.87</cell><cell>0.3</cell><cell>0.4</cell><cell>0.62</cell><cell>1.17</cell><cell>0.44</cell><cell cols="2">0.61 0.97</cell><cell>1.84</cell></row><row><cell>NLM-WLS [26]</cell><cell cols="3">0.67 0.81 1.44</cell><cell>3.33</cell><cell cols="3">0.27 0.32 0.53</cell><cell>1.1</cell><cell cols="3">0.35 0.37 0.54</cell><cell>1.04</cell><cell cols="2">0.36 0.44</cell><cell>0.8</cell><cell>1.76</cell><cell>0.29</cell><cell cols="2">0.33 0.54</cell><cell>1.09</cell><cell>0.38</cell><cell cols="2">0.46 0.77</cell><cell>1.66</cell></row><row><cell>TGV [5]</cell><cell cols="2">0.51 0.74</cell><cell>1.29</cell><cell>2.8</cell><cell cols="3">0.19 0.29 0.45</cell><cell>0.83</cell><cell cols="2">0.22 0.33</cell><cell>0.57</cell><cell>0.96</cell><cell>0.29</cell><cell>0.4</cell><cell>0.76</cell><cell>1.55</cell><cell>0.2</cell><cell cols="2">0.31 0.51</cell><cell>1.03</cell><cell>0.3</cell><cell>0.41</cell><cell>0.71</cell><cell>1.45</cell></row><row><cell>WLS [23]</cell><cell cols="3">0.53 0.94 1.69</cell><cell>3.28</cell><cell>0.2</cell><cell cols="2">0.38 0.62</cell><cell>1.12</cell><cell>0.23</cell><cell>0.4</cell><cell>0.63</cell><cell>1.06</cell><cell cols="2">0.29 0.51</cell><cell>0.9</cell><cell>1.71</cell><cell>0.21</cell><cell cols="2">0.39 0.66</cell><cell>1.15</cell><cell>0.32</cell><cell cols="2">0.54 0.89</cell><cell>1.57</cell></row><row><cell>AR [38]</cell><cell cols="3">0.56 1.12 2.21</cell><cell>4.62</cell><cell cols="3">0.27 0.47 0.89</cell><cell>1.92</cell><cell cols="3">0.29 0.55 1.02</cell><cell>1.91</cell><cell cols="3">0.37 0.62 1.23</cell><cell>2.45</cell><cell>0.25</cell><cell cols="2">0.51 0.99</cell><cell>1.98</cell><cell>0.75</cell><cell cols="2">0.69 1.33</cell><cell>2.67</cell></row><row><cell>Ours</cell><cell cols="2">0.45 0.73</cell><cell>1.3</cell><cell>2.48</cell><cell cols="2">0.19 0.31</cell><cell>0.51</cell><cell>0.93</cell><cell>0.22</cell><cell>0.3</cell><cell>0.51</cell><cell>1</cell><cell>0.27</cell><cell>0.4</cell><cell>0.68</cell><cell>1.46</cell><cell>0.19</cell><cell>0.33</cell><cell>0.5</cell><cell>1</cell><cell>0.3</cell><cell>0.43</cell><cell>0.68</cell><cell>1.33</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II QUANTITATIVE</head><label>II</label><figDesc>COMPARISON ON THE NOISY SIMULATED TOF DATA. RESULTS ARE EVALUATED IN MAE AND THE BEST RESULTS ARE IN BOLD.</figDesc><table><row><cell></cell><cell></cell><cell>Art</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Book</cell><cell></cell><cell></cell><cell cols="2">Dolls</cell><cell></cell><cell></cell><cell cols="2">Laundry</cell><cell></cell><cell></cell><cell cols="2">Moebius</cell><cell></cell><cell></cell><cell cols="2">Reindeer</cell></row><row><cell></cell><cell>2×</cell><cell>4×</cell><cell>8×</cell><cell>16×</cell><cell>2×</cell><cell>4×</cell><cell>8×</cell><cell>16×</cell><cell>2×</cell><cell>4×</cell><cell>8×</cell><cell>16×</cell><cell>2×</cell><cell>4×</cell><cell>8×</cell><cell>16×</cell><cell>2×</cell><cell>4×</cell><cell>8×</cell><cell>16×</cell><cell>2×</cell><cell>4×</cell><cell>8×</cell><cell>16×</cell></row><row><cell>GF [10]</cell><cell cols="3">1.91 2.23 3.08</cell><cell>4.87</cell><cell cols="3">0.84 1.12 1.73</cell><cell>2.82</cell><cell cols="3">0.84 1.11 1.69</cell><cell>2.71</cell><cell cols="2">1.01 1.31</cell><cell>2.0</cell><cell>3.33</cell><cell>0.92</cell><cell cols="2">1.19 1.78</cell><cell>2.84</cell><cell>1.06</cell><cell cols="2">1.32 1.98</cell><cell>3.31</cell></row><row><cell>JBU [14]</cell><cell cols="3">1.59 2.06 3.11</cell><cell>5.08</cell><cell cols="3">0.82 1.24 2.04</cell><cell>3.19</cell><cell>0.81</cell><cell>1.2</cell><cell>1.98</cell><cell>3.05</cell><cell cols="3">0.94 1.38 2.25</cell><cell>3.67</cell><cell>0.89</cell><cell cols="2">1.28 2.05</cell><cell>3.18</cell><cell>0.95</cell><cell cols="2">1.36 2.24</cell><cell>3.67</cell></row><row><cell>JGU [18]</cell><cell cols="2">1.33 1.81</cell><cell>2.9</cell><cell>4.92</cell><cell cols="3">0.79 1.24 2.05</cell><cell>3.17</cell><cell>0.8</cell><cell cols="2">1.23 2.01</cell><cell>3.05</cell><cell cols="3">0.88 1.36 2.23</cell><cell>3.65</cell><cell>0.82</cell><cell cols="2">1.25 2.03</cell><cell>3.15</cell><cell>0.91</cell><cell cols="2">1.37 2.26</cell><cell>3.68</cell></row><row><cell>NLM-WLS [26]</cell><cell cols="3">1.66 2.47 3.44</cell><cell>5.55</cell><cell cols="3">1.19 1.47 2.06</cell><cell>3.1</cell><cell cols="3">1.19 1.56 2.15</cell><cell>3.04</cell><cell cols="3">1.34 1.73 2.41</cell><cell>3.85</cell><cell>1.2</cell><cell>1.5</cell><cell>2.13</cell><cell>2.95</cell><cell>1.26</cell><cell cols="2">1.65 2.46</cell><cell>3.66</cell></row><row><cell>JID [13]</cell><cell cols="3">1.69 2.98 3.68</cell><cell>5.99</cell><cell cols="3">1.53 2.71 3.04</cell><cell>4.39</cell><cell cols="3">1.54 2.71 2.94</cell><cell>3.90</cell><cell cols="3">1.45 2.72 3.16</cell><cell>4.63</cell><cell>1.55</cell><cell cols="2">2.72 2.94</cell><cell>4.34</cell><cell>1.65</cell><cell cols="2">2.80 3.13</cell><cell>4.63</cell></row><row><cell>TGV [5]</cell><cell>0.8</cell><cell cols="2">1.21 2.01</cell><cell>4.59</cell><cell cols="3">0.61 0.88 1.21</cell><cell>2.19</cell><cell cols="3">0.66 0.96 1.38</cell><cell>2.88</cell><cell cols="3">0.61 0.87 1.36</cell><cell>3.06</cell><cell>0.57</cell><cell cols="2">0.77 1.23</cell><cell>2.74</cell><cell>0.61</cell><cell>0.85</cell><cell>1.3</cell><cell>3.41</cell></row><row><cell>AR [38]</cell><cell>1.17</cell><cell>1.7</cell><cell>2.93</cell><cell>5.32</cell><cell cols="3">0.98 1.22 1.74</cell><cell>2.89</cell><cell cols="3">0.97 1.21 1.71</cell><cell>2.74</cell><cell>1</cell><cell cols="2">1.31 1.97</cell><cell>3.43</cell><cell>0.95</cell><cell>1.2</cell><cell>1.79</cell><cell>2.82</cell><cell>1.07</cell><cell>1.3</cell><cell>2.03</cell><cell>3.34</cell></row><row><cell>WLS [23]</cell><cell cols="3">1.25 1.73 2.59</cell><cell>4.01</cell><cell>0.74</cell><cell>1.1</cell><cell>1.45</cell><cell>2.13</cell><cell cols="3">0.85 1.21 1.68</cell><cell>2.13</cell><cell cols="3">0.83 1.17 1.65</cell><cell>2.62</cell><cell>0.8</cell><cell cols="2">1.18 1.67</cell><cell>2.27</cell><cell>0.84</cell><cell cols="2">1.15 1.58</cell><cell>2.43</cell></row><row><cell>Ours</cell><cell cols="3">0.71 1.06 1.72</cell><cell>3.13</cell><cell cols="3">0.57 0.78 1.13</cell><cell>1.68</cell><cell cols="3">0.64 0.87 1.21</cell><cell>1.73</cell><cell cols="3">0.54 0.77 1.12</cell><cell>1.98</cell><cell>0.55</cell><cell cols="2">0.76 1.15</cell><cell>1.71</cell><cell>0.57</cell><cell>0.8</cell><cell>1.14</cell><cell>1.88</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III QUANTITATIVE</head><label>III</label><figDesc>COMPARISON ON REAL TOF DATASET. THE ERROR IS CALCULATED AS MAE TO THE MEASURED GROUNDTRUTH IN M M . THE BEST RESULTS ARE IN BOLD.</figDesc><table><row><cell></cell><cell>Bicubic</cell><cell>GF [10]</cell><cell>JGU [18]</cell><cell>JBU [14]</cell><cell>NLM-WLS [26]</cell><cell>TGV [5]</cell><cell>AR [38]</cell><cell>WLS [23]</cell><cell>Ours</cell></row><row><cell>Books</cell><cell>16.23mm</cell><cell>15.55mm</cell><cell>15.76mm</cell><cell>15.86mm</cell><cell>14.31mm</cell><cell>12.8mm</cell><cell>14.37mm</cell><cell>13.87mm</cell><cell>12.82mm</cell></row><row><cell>Devil</cell><cell>17.78mm</cell><cell>16.1mm</cell><cell>16.28mm</cell><cell>16.32mm</cell><cell>15.36mm</cell><cell cols="2">14.97mm 15.41mm</cell><cell>15.36mm</cell><cell>14.5mm</cell></row><row><cell>Shark</cell><cell>16.66mm</cell><cell>17.1mm</cell><cell>17.25mm</cell><cell>17.32mm</cell><cell>15.88mm</cell><cell cols="2">15.53mm 16.27mm</cell><cell>15.88mm</cell><cell>14.92mm</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV QUANTITATIVE</head><label>IV</label><figDesc>COMPARISON ON SIMULATED KINECT DATASETS. RESULTS ARE EVALUATED IN TERMS OF AVERAGE MAE OF ALL THE DATA. THE BEST RESULTS ARE IN BOLD.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We have slightly adjusted the 4-connected/8-connected neighborhood system in the original model to a larger square support N (i) of radius r. However, the mechanism remains the same.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_1"><p>TRANSACTIONS ON IMAGE PROCESSING, VOL. XX, NO. XX, XXXX</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>WEI LIU et.al: ROBUST COLOR GUIDED DEPTH MAP RESTORATION</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1057-7149 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p><p>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI  10.1109/TIP.2016.2612826, IEEE Transactions on Image Processing WEI LIU et.al: ROBUST COLOR GUIDED DEPTH MAP RESTORATION</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This research is supported by 973 Plan, China (No. 2015CB856004) and NSFC, China (No:61572315 and 61503250).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>GF <ref type="bibr" target="#b9">[10]</ref> JBU <ref type="bibr" target="#b13">[14]</ref> JGU <ref type="bibr" target="#b17">[18]</ref> NLM-WLS <ref type="bibr" target="#b25">[26]</ref> JID <ref type="bibr" target="#b12">[13]</ref> TGV <ref type="bibr" target="#b4">[5]</ref> WLS <ref type="bibr" target="#b22">[23]</ref> AR <ref type="bibr">[</ref> Table <ref type="table">IV</ref> shows the quantitative comparison of simulated Kinect restoration results in terms of average MAE. The proposed method also yields the lowest average MAEs on the two datasets. Fig. <ref type="figure">12</ref> shows visual comparison. Our method performs well in preserving sharp depth discontinuities. The corresponding error maps are shown in Fig. <ref type="figure">13</ref> for better comparison. As shown in the error maps, our method clearly outperforms other methods with much smaller errors.</p><p>Real Kinect data are also used to test the proposed method. As the first generation Kinect (Kinect v1) is different from the second generation Kinect (Kinect v2), the real Kinect data used in this paper include depth maps captured by Kinect v1 and Kinect v2. We reuse the Kinect v1 data in <ref type="bibr" target="#b37">[38]</ref> which are from the NYU dataset <ref type="bibr" target="#b32">[33]</ref> and are shown in the first two rows of Fig. <ref type="figure">14</ref>. The Kinect v2 data are from the SUN RGBD dataset <ref type="bibr" target="#b33">[34]</ref> which are shown in the last two rows of Fig. <ref type="figure">14</ref>. Both results of WLS <ref type="bibr" target="#b22">[23]</ref>, NLM-WLS <ref type="bibr" target="#b25">[26]</ref> and AR model <ref type="bibr" target="#b37">[38]</ref> suffer from blurring depth discontinuities as shown in highlighted regions. The inpainting based method JID <ref type="bibr" target="#b12">[13]</ref> can preserve sharp depth discontinuities but they are quite jaggy. Our method can properly rectify the jaggy depth discontinuities. Depth discontinuities in our results are also quite sharp.</p><p>Table <ref type="table">V</ref> shows the computational time of different methods on different datasets. Our method is slower than WLS <ref type="bibr" target="#b22">[23]</ref> and NLM-WLS <ref type="bibr" target="#b25">[26]</ref>. However, it is faster than the AR model <ref type="bibr" target="#b37">[38]</ref> and much faster than TGV <ref type="bibr" target="#b4">[5]</ref> and JID <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we propose a robust framework for color guided depth map restoration. The proposed method is robust against the inconsistency between color edges and depth discontinuities. Through both mathematical analysis and experimental results, we show that the proposed method can both suppress texture copy artifacts and preserve sharp depth discontinuities. Experimental results on both simulated and real ToF and Kinect data have shown the promising performance of the proposed method.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Depth-color fusion strategy for 3-D scene modeling with kinect. Cybernetics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Camplani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mantecon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Salgado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1560" to="1571" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An application of Markov random fields to range sensing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Diebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Edge-preserving decompositions for multi-scale tone and detail manipulation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Farbman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fattal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">67</biblScope>
			<date type="published" when="2008">2008</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast and robust multiframe super resolution. Image processing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Farsiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1327" to="1344" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image guided depth upsampling using anisotropic total generalized variation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ferstl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Reinbacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rüther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Markov random fields for super-resolution and texture synthesis</title>
		<author>
			<persName><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Markov Random Fields for Vision and Image Processing</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Guided depth upsampling via a cosparse analysis model</title>
		<author>
			<persName><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops (CVPRW), 2014 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="738" to="745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exposure fusion for time-of-flight imaging</title>
		<author>
			<persName><forename type="first">U</forename><surname>Hahne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alexa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1887" to="1894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Depth superresolution by transduction</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1524" to="1535" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Guided image filtering</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1397" to="1409" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Depth super resolution by rigid body self-similarity in 3D</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hornacek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gelautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Accuracy and resolution of kinect depth data for indoor mapping applications</title>
		<author>
			<persName><forename type="first">K</forename><surname>Khoshelham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">O</forename><surname>Elberink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1437" to="1454" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A joint intensity and depth co-sparse analysis model for depth map super-resolution</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kiechle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hawe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kleinsteuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1545" to="1552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Joint bilateral upsampling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Uyttendaele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">96</biblScope>
			<date type="published" when="2007">2007</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient preconditioning of laplacian matrices for computer graphics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fattal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">142</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Data-driven depth map refinement via multi-scale sparse representation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="159" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Similarity-aware patchwork assembly for depth image super-resolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Joint geodesic upsampling of depth images</title>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Taguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Variable bandwidth weighting for texture copy artifacts suppression in guided depth upsampling. Circuits and Systems for Video Technology</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cross-based local multipoint filtering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Depth enhancement via low-rank matrix completion</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3390" to="3397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Patch based synthesis for single depth image super-resolution</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fast global image smoothing based on weighted least squares. Image Processing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5638" to="5653" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Depth video enhancement based on weighted mode filtering</title>
		<author>
			<persName><forename type="first">D</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1176" to="1190" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cost aggregation and occlusion handling with WLS in stereo matching</title>
		<author>
			<persName><forename type="first">D</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1431" to="1442" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">High quality depth map upsampling for 3D-ToF cameras</title>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">High-quality depth map upsampling and completion for RGB-D cameras. Image Processing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5559" to="5572" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Scale-space and edge detection using anisotropic diffusion. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="629" to="639" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Generalised nonlocal image smoothing</title>
		<author>
			<persName><forename type="first">L</forename><surname>Pizarro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mrázek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Didas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grewenig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="87" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Resolution enhancement of PMD range maps</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rajagopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhavsar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wallhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="304" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Lidarboost: Depth superresolution for ToF 3D shape scanning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Schuon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Layer depth denoising and completion for structured-light RGB-D cameras</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><forename type="middle">S</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013-06">June 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from RGBD images</title>
		<author>
			<persName><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sun RGB-D: A RGB-D scene understanding benchmark suite</title>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multipoint filtering with local polynomial approximation and range guidance</title>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Pham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2941" to="2948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Bilateral filtering for gray and color images</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manduchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Depth recovery using an adaptive color-guided auto-regressive model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">color-guided depth recovery from RGB-D data using an adaptive auto-regressive model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3443" to="3458" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Wei Liu received the B.Eng degree from the department of Automation of Xi&apos;an Jiaotong University in 2012, Currently he is a Ph.D candidate of Shanghai Jiao Tong University at the Institute of Image Processing and Pattern Recognition under the supervision of professor Jie Yang. His current research interests mainly include image/video denoise, image deblur and depth image restoration</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Xiaogang Chen received the Ph.D. degree from</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="368" to="388" />
			<date type="published" when="2011">2011. 2013</date>
			<pubPlace>Shanghai, China; Shanghai</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Shanghai Jiao Tong University ; University of Shanghai for Science and Technology</orgName>
		</respStmt>
	</monogr>
	<note>International Journal of Computer Vision. as an Assistant Professor. His current research interests mainly focus on image and video processing</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">He has led many research projects (e.g.,National Science Foundation, 863 National High Tech. Plan), had one book published in Germany, and authored more than 200 journal papers. His major research interests are object detection and recognition, data fusion and data mining, and medical image processing</title>
	</analytic>
	<monogr>
		<title level="m">His research outcomes have been published in many premier international conferences including ECCV, CVPR, ICIP, and ICPR and the major international journals such as IEEE TIP</title>
		<meeting><address><addrLine>Germany; China; Harbin, China; PR, TIP, TCSVT, TSMC-B, CVIU, IVC, PRL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
		<respStmt>
			<orgName>Jie Yang received his PhD from the Department of Computer Science, Hamburg University ; Shanghai Jiao Tong University ; IEEE TCSVT, IEEE TIFS, PR, PRL, Signal Processing, Signal Processing Letter. Dr</orgName>
		</respStmt>
	</monogr>
	<note>Neurocomputing and EURASIP Journal on Image and Video Processing</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
