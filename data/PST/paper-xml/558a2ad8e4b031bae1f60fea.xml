<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TOWARDS AUTOMATIC EXTRACTION OF EXPRESSIVE ELEMENTS FROM MOTION PICTURES: TEMPO</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Brett</forename><surname>Adams</surname></persName>
							<email>fadamsb@cs.curtin.edu.au</email>
						</author>
						<author>
							<persName><forename type="first">Chitra</forename><surname>Dorai</surname></persName>
							<email>dorai@watson.ibm.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Yorktown Heights GPO Box U1987</orgName>
								<orgName type="institution">Curtin University of Technology</orgName>
								<address>
									<postBox>P.O. Box 704</postBox>
									<postCode>6845, 10598</postCode>
									<settlement>Perth</settlement>
									<region>W. Australia New York</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Svetha</forename><surname>Venkatesh</surname></persName>
							<email>svethag@cs.curtin.edu.au</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science y</orgName>
								<orgName type="institution">IBM T. J. Watson Research Center</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TOWARDS AUTOMATIC EXTRACTION OF EXPRESSIVE ELEMENTS FROM MOTION PICTURES: TEMPO</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">256EFFDEEB30350C89D5DD1BFD8B19C4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes a unique computational approach to extraction of expressive elements of motion pictures for deriving high level semantics of stories portrayed, thus enabling better video annotation and interpretation systems. This approach, motivated and directed by the existing cinematic conventions known as film grammar, as a first step towards demonstrating its effectiveness, uses the attributes of motion and shot length to define and compute a novel measure of tempo of a movie. Tempo flow plots are defined and derived for four full-length movies and edge analysis is performed leading to the extraction of dramatic story sections and events signaled by their unique tempo. The results confirm tempo as a useful attribute in its own right and a promising component of semantic constructs such as tone or mood of a film.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Though a great deal of work has been done in low level content based analysis of video, automatic high-level semantic analysis of its content is just beginning. We propose a unique approach, inspired by existing cinematic conventions, also known as film grammar <ref type="bibr" target="#b12">[12]</ref>, to computationally determine the expressive elements of motion pictures conveyed by the manipulation of editing, lighting, camera movements, color, etc., for high level video understanding and appreciation. The rules of film grammar are found more in history of use, than in an abstract predefined set of rules, and elucidate on the relationships that exist between the many cinematic techniques employed by directors worldwide and their intended meaning and emotional impact on viewers. Our project, guided by this grammar, focuses on the extraction of high-level semantics associated with the expressive elements and the form of story narration in films. It differs from many recent approaches in that while others have sought to model very specific events occurring in a specific domain, our research attempts to understand the "expressiveness" of the medium and the thematic units (high-paced section, tranquil event, etc.) highlighted by the expressions, that are pervasive regardless of the domain of the story.</p><p>One concept, often employed with film understanding is pace or tempo that gives a sense of a story's experienced time. Tempo is defined by <ref type="bibr" target="#b2">[2]</ref> as being influenced "in three ways: by the actual speed and rhythm of movement and cuts within the film, by the accompanying music, and by the content of the story". Sobchack says that " <ref type="bibr">[Tempo]</ref> is usually created chiefly by the rhythm of editing and by the pace of motion within the frame" ( <ref type="bibr">[12, p. 103]</ref>). This paper is concerned with the automatic extraction of tempo from a film, and proposes an elegant tempo/pace detection technique based on two relatively simple computable features; shot length and motion from digital movies and videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PREVIOUS WORK</head><p>Research in the area of content-based video analysis has sought to use several attributes, especially of motion and shot characteristics with varying measures and methods in different domains. The solutions tend towards falling into 1 of 2 (somewhat overlapping) categories: The first class of approaches <ref type="bibr">[7,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b4">4]</ref> seeks to extract as much information as possible from the video source. All further processing is then founded on the basis of formulating various image and audio similarity measures. Shots that are similar in terms of the multitude of extracted features are merged. Further investigation is then optionally carried out on the discovered "semantic units" in terms of some a priori temporal model(s).</p><p>The second category involves discriminating between predefined categories of scene/film based upon a careful selection of low level features that map well to high level semantics for the given categorization problem. The focus here is on spotting "useful", albeit limited features as opposed to completely reassembling the full spatio-temporal nature of the video contents for a specific task such as shot or scene labeling <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b14">14]</ref>, genre discrimination <ref type="bibr" target="#b5">[5]</ref>, and sports events extraction <ref type="bibr" target="#b11">[11]</ref>.</p><p>Our research is somewhere between the above two approaches. The expressive element, tempo derived in this paper, can be seen to be both high-level and fundamental (therefore widely applicable), yet manifest in such a way as to be computationally tractable. Our work moves away from single frame/shot study to across shot analysis for extraction of story form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DEFINITION OF TEMPO</head><p>Tempo or pace is a term that is broadly and often interchangeably used in video appreciation and therefore in this paper as well. A helpful definition in this context might be "rate of performance or delivery." Tempo/pace carries with it the important notions of time and speed and its definition reflects the complexity of the domain to which it is applied. A runner has a simple velocity, music has a tempo and rhythm, a time signature that speaks to beat and bar. Video can be quite complex including both of the above at once.</p><p>How is tempo made manifest in film? More precisely, how does a director manipulate time and speed in a film to create a desired tempo? One way is by using the cinematic technique of montage. Montage, also known as editing, is "a dialectical process that creates a third meaning out of the adjacent shots" and has the ability to "bend the time line of a film" <ref type="bibr">[9, p. 183,185]</ref>. Essentially, the director controls the speed at which a viewer's attention is directed and thus impacts on her appreciation of the tempo of a piece of video.</p><p>A second way that tempo is manifest in film is through the level of motion or dynamics. Both camera motion and object motion impact on a viewer's estimation of the pace of a video. This is because motion, like montage, can influence the viewer's attention with more or less haste and strength.</p><p>There are many other elements which feed into this concept of tempo, music being another major contributor (along with the story). We will limit our consideration of tempo/pace to the factors of montage and motion in this paper for the following reasons: (i) The characteristic features of both montage and motion lend themselves well to automatic extraction. (ii) They, together, form the major contribution to pace <ref type="bibr">[12, p. 103</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">COMPUTATIONAL APPROACH TO TEMPO DETECTION IN MOVIES</head><p>Our proposed technique is predicated on the notion that film sections of differing pace will leave distinct marks on the attributes of shot length and motion, and hence may be detected and classified based on those fundamental primitives. First we describe the methods used to measure motion and shot length in a film. We then combine them in a novel fashion, initially to form a simple two label classification, and finally to yield a continuous measure of pace for the course of the film. The input to our analysis is a compressed movie or TV program in MPEG-1 format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Extracting Camera Motion and Shot Length</head><p>Extraction of camera pan and tilt between successive frames was performed on the input video stream with software implementing the qualitative motion estimation algorithm of <ref type="bibr" target="#b13">[13]</ref>. The raw pan and tilt computed were then filtered of anomalous values and smoothed with a sliding window. An index of shot boundaries (specifically cuts) is created by means of the commercial software WebFlix. Although imperfect, it has been found overall, to do an adequate job of automatic shot detection. The generated shot index is output as a series of start and stop frames. Shots of length smaller than 10 frames (under half a second) are merged, as they are deemed to be false positives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Tempo Clustering and Validation</head><p>As a preliminary validation of our approach founded on motion and shot length, we carried out a clustering exercise with data from three mainstream movies; Titanic (TT), Lethal Weapon 2 (LW2), and The Colour Purple (CP). First, a manual list of story sections was compiled for each movie and a pace classification of "fast" or "slow" was assigned to each section that exhibited unmistakable pace of either of those categories. The motion and shot statistics of each of these sections of film was then automatically determined using the methods described, normalized (see Section 4.3), and a single 2D data point for each section was generated. Each point ranged in shot length from 1000 to 5000 frames. The motion feature of each point undergoes a standard normalization; shot length however, uses overall shot median in normalization (see <ref type="bibr" target="#b1">[1]</ref> for a discussion of shot length distribution, normalization, and associated implications). Figure <ref type="figure" target="#fig_0">1</ref> shows the plot of motion magnitude versus shot length of about 49 points drawn from the data computed for the three movies. The plot shows the fast sections, for the most part, clustered tightly in the top left of the plot. The slow sections, in contrast, are spread over a much broader area. Half of this data was then randomly selected and used to generate a decision tree classifier with the C4.5 tree generation software <ref type="bibr" target="#b10">[10]</ref>, and the remainder comprising of 26 points was used to test the built tree. The following results were observed: The result-  ing tree classifier shown in Figure <ref type="figure" target="#fig_1">2a</ref>, generates decision boundaries on the feature space shown in 2b. Samples that reach the "fast" leaf are essentially above average motion and below the shot median (plus 1 second). In other words, sections with much movement and fast edits are classified as "fast". Otherwise a label of "slow" is given. We found that 23 sections were given the correct label while only 3 were misclassified demonstrating that the generated classifier does well with the cases provided, and it distinguishes well between slow or fast pace. However, the boundaries between fast and slow sections break down upon the addition of the remainder of data from a given film (i.e. sections that are neither decidedly fast nor slow). This is due, in part, to the fact that the sections that produce a single data point are of the order of 1000's of frames. If the sections are too short we risk anomalous results; too long and we risk smoothing over subsections of markedly different tempo. Thus the issue of resolution with this kind of classification scheme is a problem. It is also due to the fact that an objective decision as to a section's absolute pace is difficult. It is much easier to say "faster" than "fast" for example, and decisions can be effected by non-pace factors such as the emotional content of the section under consideration which relate more to higher semantic constructs such as tone or mood.</p><p>A more desirable tempo indicator would address the resolution issue and offer a more intuitive feel for the pace of a section within the context of the given film (ie. would offer more relational information than a simple binary ordinal classification). The following section outlines one solution which has both of these desired attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Novel Tempo/Pace Function</head><p>We propose a novel pace function based on motion and shot length. The average motion magnitude is computed for each shot, where the motion magnitude is simply the absolute value of the sum of the pan and tilt values for a given frame pair. Shot length, in frames (assuming a 25 frame/s rate), is also calculated for each shot.</p><p>In addition to the per shot data, the 1st and 2nd statistical moments (mean, and standard deviation, ) of these features are calculated for the entire film, along with the overall shot median, meds.</p><p>Tempo is then defined as:</p><formula xml:id="formula_0">P(n) = (meds ; s(n)) s + (m(n) ; m) m (1)</formula><p>where s refers to shot length in frames, m to motion magnitude, and n to shot number. The weights and , are given values of 1, effectively assuming that both shot length and motion contribute equally to the perception of pace for a given film. Other weighting schemes are under our investigation. This function is then smoothed with a Gaussian filter. Besides smoothing the data this process is desired for two reasons. First it reflects our knowledge that directors generally do not continue to make drastic pace changes in single or small numbers of shots, unless motivated by rare narrative requirements. Secondly it also helps, in a very simple fashion, mimic the process of human perception of pace in that pace has a certain inertia to it due to memory retention of preceding shots. That is, pace is a function of a neighbourhood of shots. As anticipated, the amount of smoothing changes the resolution of the tempo indication and correspondingly, the level at which pace features may be extracted. Figure <ref type="figure" target="#fig_2">3</ref> is a plot of P(n) for Titanic. The zero axis in this plot may be roughly considered as the average pace mark for the film. The first half of the plot encompasses the day before Titanic sinks, up to the point where the iceberg strikes. The second half of the plot depicts from that time until the ship sinks, and is conspicuous by the marked increase in pace (staying above the reference pace mostly) accompanying it. Smoothing factor used for this movie overview is 100. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Tempo/Pace Detector for Story Sections and Events</head><p>Given this continuous measure of tempo there are many features that one might extract that would be useful. We have initially chosen to locate the edges of the function P(n) as it is a relatively straight forward task, and more importantly is a good indicator of important events. Significant pace changes often occur across the boundary of story elements, and are often precipitated by events of high dramatic import in the story. Edge analysis is performed to determine locations of these changes.</p><p>Edges of the pace function are detected using Deriche's recursive filtering algorithm <ref type="bibr" target="#b3">[3]</ref>. This multi-scale edge detection algorithm is parameterised by , which determines the slope of the target edges. Larger detects edges of smaller slope (more gradual change) and vice versa. A threshold ( ) is applied to the resultant output of the algorithm to filter edges; the higher the threshold the fewer and larger the edges detected, and vice versa. The parameters used for the edge detection process are as follows: (i) = 3 0 , high ( 1:7 (of edge output)): to locate significant, gradual pace transitions, (ii) = 3 0 , low ( 1 ): to locate all gradual pace transitions (large and small), (iii) = 0 :5, high ( 2 ): to locate significant, sharp pace transitions, and (iv) = 0 :5, low ( 0:8 ): to locate all sharp pace transitions (large and small).</p><p>Thus, four rounds of edge detection were applied to each film examined. Large pace transitions are targeted with a high threshold, and the resulting edges are designated "Story sections". This label is somewhat arbitrary as large transitions do not always indicate a change of story element and vice versa, however, it is useful in terms of presenting the results of the edge detection process. Small pace transitions are accordingly called "Events" due to the fact that such transitions are generally associated with localized events as opposed to changes of the order of story element size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTAL RESULTS</head><p>Results from only Titanic, one of the four movies analyzed are presented in detail due to space restrictions. Titanic is a love story centered around the event of the sinking of the Titanic. Figure <ref type="figure" target="#fig_3">4</ref> shows the pace plot of a section of the movie (from the 3rd class party up to the point where the iceberg hits) with located edges indicated for each of the 4 sigma/threshold combinations used, and Table <ref type="table">1</ref> matches each automatically discovered edge to a brief description of the story section bounded by, or the dramatic event coinciding with the discovered edges.</p><p>Consider Table <ref type="table">1</ref>. The first large gradual edge reported occurs at the transition between the story elements, A and B, labelled as "1st Class Dinner" and "3rd Class Party" respectively. The differ-  <ref type="table">1</ref>: Labelled story sections and events identified from tempo changes in Titanic (cf. Figure <ref type="figure" target="#fig_3">4</ref>).</p><p>ence between the lives of the 1st and 3rd class people is a dominant theme throughout the movie and is expressed here by the stiffly formal nature of the former contrasted with the exuberance of the latter. As such it marks a large change in the pace which is duly signaled by our algorithm. The next large gradual edge occurs at the transition to the next story element, C, labelled "The Next Day". This is a negative edge and marks the change of tempo that occurs as Rose is seen back in her 1st class life. On a finer scale of sharp edges, the sharp negative edge, j, "Tense Wait after Iceberg Seen" occurs as the initial flurry at the sight of the iceberg dies and the crew wait to see whether the Titanic will clear it or not. The next sharp positive edge, k, "Iceberg Hits" coincides with the actual impact of the iceberg and the ramp up in tempo as the resulting damage is graphically portrayed.</p><p>Overall the computation of P(n) and subsequent edge detection succeeded in discovering nearly all actual distinct tempo changes. The resulting list of located edges in all four movies (see </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">DISCUSSION AND CONCLUSIONS</head><p>While this measure of pace is normalized, an analysis of the overall film motion and shot statistics also provides a (very) approximate indication of the genre of the movie such as action, romance, etc., or at least its relationship pace wise to another movie. The details of this aspect of our study can be referred to in our research report. The tempo function and its edges offer a rough feel for the pace changes in a movie from a quick glance. The experiment with Titanic indicates, for example, that there is a marked tempo change that occurs about halfway through the movie. This change coincides with the advent of the iceberg, no less. Relative pace levels of different sections of the movie can also be determined from our results. This is an advantage of our graduated pace measure over a classification scheme of a few discrete levels. Our measure may be subsequently reduced to labels if desired by different policies.</p><p>Taking advantage of the chosen domain of films this work has sought to lay the framework of film grammar over the video to be analyzed, in particular the notion of tempo/pace as expressed by the indicators of shot length and motion, to produce a novel continuous measure of tempo. Our results have demonstrated that, to a large degree, the expressive element of tempo is extractable. It has also been shown that tempo is a desirable and useful attribute to analyze as it offers pointers to higher level semantic constructs such as dramatic events and important story elements in films.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A two class tempo categorization of movie data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A tempo decision tree and corresponding decision boundaries in the feature space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A plot of the pace function for Titanic.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Results of edge detection on pace flow and corresponding story sections and events from Titanic.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Gradual Edge, Sigma: 30 Sharp Edge, Sigma: 0.5</head><label></label><figDesc></figDesc><table><row><cell cols="2">Story Element detected (high thresh)</cell></row><row><cell>A 1st class dinner</cell><cell>Day before sinking</cell></row><row><cell>B 3rd class party</cell><cell>Rose in trouble</cell></row><row><cell>C The next day</cell><cell>Rose has to decide</cell></row><row><cell>D Calm before the storm</cell><cell>Rose chooses Jack</cell></row><row><cell>E Titanic is sinking</cell><cell>Jack and Rose chased</cell></row><row><cell>F</cell><cell>Calm before the storm</cell></row><row><cell>G</cell><cell>Titanic is sinking</cell></row><row><cell>Event detected (low thresh)</cell><cell></cell></row><row><cell cols="2">a Jack grabs Rose for D&amp;M Dancing begins</cell></row><row><cell>b Jack sketches Rose</cell><cell>Jack partners with Rose</cell></row><row><cell>c Jack and Rose chased!</cell><cell>Rose stands on toes</cell></row><row><cell>d</cell><cell>Dancing again</cell></row><row><cell>e</cell><cell>Rose chooses Jack</cell></row><row><cell>f</cell><cell>At the bow of Titanic</cell></row><row><cell>g</cell><cell>Present day to Rose and</cell></row><row><cell></cell><cell>Jack back at room</cell></row><row><cell>h</cell><cell>Jack sketches Rose</cell></row><row><cell>i</cell><cell>From sketch to pres. day</cell></row><row><cell>j</cell><cell>Tense wait after iceberg</cell></row><row><cell></cell><cell>seen</cell></row><row><cell>k</cell><cell>Iceberg hits</cell></row><row><cell>Table</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 )</head><label>2</label><figDesc>serves as a useful and reliable index into the dramatic development and narration of the story.</figDesc><table><row><cell>Movie</cell><cell cols="3">Edges Found Missed Edges False Pos.</cell></row><row><cell>Titanic</cell><cell>68</cell><cell>5</cell><cell>7</cell></row><row><cell>LW2</cell><cell>17</cell><cell>4</cell><cell>1</cell></row><row><cell>Lost World, JP</cell><cell>19</cell><cell>3</cell><cell>0</cell></row><row><cell>Colour Purple</cell><cell>18</cell><cell>4</cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results of tempo detection on four movies.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Role of shot length in characterizing tempo and dramatic story sections in motion pictures</title>
		<author>
			<persName><forename type="first">B</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dorai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000-04">April 2000</date>
			<publisher>IBM Working Document</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Britannica</surname></persName>
		</author>
		<title level="m">Encyclopedia Britannica Online</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recursively implementing the Gaussian and it&apos;s derivatives</title>
		<author>
			<persName><forename type="first">R</forename><surname>Deriche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP&apos;92</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="263" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Interactive content-based retrieval in video databases using fuzzy classification and relevance feedback</title>
		<author>
			<persName><forename type="first">A</forename><surname>Doulamis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Doulamis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kollias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Multimedia Computing and Systems</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Automatic recognition of film genres</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lienhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Effelsberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<pubPlace>Germany</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Mannheim</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An extensible spatial-temporal model for semantic video segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hammoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fontaine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IFMCP &apos;98</title>
		<meeting><address><addrLine>Anchorage, Alaska</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Visualgrep: A systematic method to compare and retrieve video sequences</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lienhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Effelsberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
		<respStmt>
			<orgName>The University of Mannheim</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving the spatialtemporal clue based segmentation by the use of rhythm</title>
		<author>
			<persName><forename type="first">W</forename><surname>Mahdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fontaine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second European Conference, ECDL &apos;98</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">How to read a film: The Art, Technology, Language, History and Theory of Film and Media</title>
		<author>
			<persName><forename type="first">J</forename><surname>Monaco</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981">1981</date>
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">C4.5: Programs for Machine Learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Quinlan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Morgan Kaufmann Publishers</publisher>
			<pubPlace>San Mateo, California</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Closed-world tracking</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Intille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Bobick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Computer Vision</title>
		<meeting>the 5th International Conference on Computer Vision<address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995-06">June 1995</date>
			<biblScope unit="page" from="672" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An introduction to film</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sobchack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sobchack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scot, Foresman and Company</title>
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Qualitative extraction of camera parameters</title>
		<author>
			<persName><forename type="first">M</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hosie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bayesian modeling of video editing and structure: Semantic features for video summarization and browsing</title>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lippman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP&apos;98</title>
		<meeting><address><addrLine>Chicago</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Content-based retrieval of video data by the grammar of film</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yoshitaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hirakawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ichikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1997 IEEE Symposium on Visual Languages</title>
		<meeting><address><addrLine>Capri, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
