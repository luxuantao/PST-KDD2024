<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">When Do Graph Neural Networks Help with Node Classification: Investigating the Homophily Principle on Node Distinguishability</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-04-25">25 Apr 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sitao</forename><surname>Luan</surname></persName>
							<email>sitao.luan@mail</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chenqing</forename><surname>Hua</surname></persName>
							<email>chenqing.hua@mail</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Minkai</forename><surname>Xu</surname></persName>
							<email>minkai@</email>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qincheng</forename><surname>Lu</surname></persName>
							<email>qincheng.lu@mail</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiaqi</forename><surname>Zhu</surname></persName>
							<email>jiaqi.zhu@mail</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiao-Wen</forename><surname>Chang</surname></persName>
							<email>chang@cs</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Fu</surname></persName>
							<email>fujie@mila.quebec</email>
						</author>
						<author>
							<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Doina</forename><surname>Precup</surname></persName>
							<email>dprecup@cs.mcgill.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">When Do Graph Neural Networks Help with Node Classification: Investigating the Homophily Principle on Node Distinguishability</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-04-25">25 Apr 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2304.14274v1[cs.SI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Homophily principle, i.e., nodes with the same labels are more likely to be connected, was believed to be the main reason for the performance superiority of Graph Neural Networks (GNNs) over Neural Networks (NNs) on Node Classification (NC) tasks. Recently, people have developed theoretical results arguing that, even though the homophily principle is broken, the advantage of GNNs can still hold as long as nodes from the same class share similar neighborhood patterns [29], which questions the validity of homophily. However, this argument only considers intra-class Node Distinguishability (ND) and ignores inter-class ND, which is insufficient to study the effect of homophily. In this paper, we first demonstrate the aforementioned insufficiency with examples and argue that an ideal situation for ND is to have smaller intra-class ND than inter-class ND. To formulate this idea and have a better understanding of homophily, we propose Contextual Stochastic Block Model for Homophily (CSBM-H) and define two metrics, Probabilistic Bayes Error (PBE) and Expected Negative KL-divergence (ENKL), to quantify ND, through which we can also find how intra-and inter-class ND influence ND together. We visualize the results and give detailed analysis. Through experiments, we verified that the superiority of GNNs is indeed closely related to both intra-and inter-class ND regardless of homophily levels, based on which we define Kernel Performance Metric (KPM). KPM is a new non-linear, feature-based metric, which is tested to be more effective than the existing homophily metrics on revealing the advantage and disadvantage of GNNs on synthetic and real-world datasets.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph Neural Networks (GNNs) have gained popularity in recent years as a powerful tool for graph-based machine learning tasks. By combining graph signal processing and convolutional neural networks, various GNN architectures have been proposed <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b18">19]</ref>, and have been shown to outperform traditional neural networks in tasks such as node classification (NC), graph classification, link prediction, and graph generation. The success of GNNs is believed to be rooted in the homophily assumption <ref type="bibr" target="#b31">[32]</ref>, which states that connected nodes tend to have similar attributes <ref type="bibr" target="#b13">[14]</ref>, providing extra useful information to the aggregated features over the original node features. This relational inductive bias is thought to be a major contributor to the superior performance of GNNs over traditional neural networks in various tasks <ref type="bibr" target="#b3">[4]</ref>. On the other hand, the lack of homophily, i.e., heterophily, is considered as the main cause of the inferiority of GNNs on heterophilic graphs, because nodes from different classes are connected and mixed, which can lead to indistinguishable node embeddings, making the classification task more difficult for GNNs <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b27">28]</ref>. Numerous models have been proposed to address the heterophily challenge recently <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b25">26]</ref> Recently both empirical and theoretical studies indicate that the relationship between homophily and GNN performance is more complicated than "homophily wins, heterophily loses" <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b25">26]</ref>. For example, the authors in <ref type="bibr" target="#b28">[29]</ref> stated that, as long as nodes within the same class share similar neighborhood patterns, their embeddings will be similar after aggregation. They provided experimental evidence and theoretical analysis, and concluded that homophily may not be necessary for GNNs to distinguish nodes. The paper <ref type="bibr" target="#b25">[26]</ref> studied homophily/heterophily from post-aggregation node similarity perspective and found that heterophily is not always harmful, which is consistent with <ref type="bibr" target="#b28">[29]</ref>. Besides, the authors have proposed to use high-pass filter to address some heterophily cases, which is adopted in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b4">5]</ref> as well. They have also proposed aggregation homophily, which is a linear feature-independent performance metric and is verified to be better at revealing the performance advantages and disadvantages of GNNs than the existing homophily metrics <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b23">24]</ref>. Moreover, <ref type="bibr" target="#b5">[6]</ref> has investigated heterophily from a neighbor identifiable perspective and stated that heterophily can be helpful for NC when the neighbor distributions of intra-class nodes are identifiable.</p><p>Inspite that the current literatures on studying homophily principle provide the profound insights, they are still deficient: 1. <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b5">6]</ref> only consider intra-class node distinguishability (ND), but ignore inter-class ND; 2. <ref type="bibr" target="#b25">[26]</ref> does not show when and how high-pass filter can help with heterophily problem; 3. There is a lack of a non-linear, feature-based performance metric which can leverage richer information to provide an accurate threshold value to indicate whether GNNs or NNs are needed on certain task.</p><p>To address those issues, in this paper: 1. We first demonstrate that, to comprehensively study the impact of homophily on ND , one needs to consider intra-and inter-class ND together and an ideal case is to have smaller intra-class ND than inter-class ND; 2. To formulate this idea, we propose Contextual Stochastic Block Model for Homophily (CSBM-H) as a graph generative model, which has an explicit parameter to control homophily, class variances parameters to control intra-class ND and center distance to control inter-class ND; 3. To quantify ND of CSBM-H, we propose two metrics, Probabilistic Bayes Error (PBE) and Expected Negative KL-divergence (ENKL), through which we can analytically study how intra-and inter-class ND influence ND together. We visualize the PBE and ENKL of original (full-pass filtered) features, low-pass (LP) filtered features and high-pass (HP) filtered features at different homophily levels, give detailed analysis and discuss how class variances and node degree will influence ND; 4. In practice, we test that the performance superiority of GNNs is indeed closely related to whether intra-class ND is smaller than inter-class ND, regardless of homophily levels, based on which we propose Kernel Performance Metric (KPM), a new non-linear, feature-based metric. Experiments show KPM is more effective than the existing homophily metrics on predicting the performance of GNNs versus NNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>In this section, we provide an overview of the notation and background knowledge used throughout the paper. We use bold font for vectors (e.g.,v) and define an undirected connected graph G = (V, E), where V is the set of nodes with a total of N elements, E is the set of edges without self-loops. A is the symmetric adjacency matrix with A i,j = 1 if there is an edge between nodes i and j, otherwise A i,j = 0. We also define D as the diagonal degree matrix of the graph, with D i,i = d i = j A i,j . The neighborhood set of a node i, denoted as N i , is defined as N i = {j : e ij ? E}. A graph signal is a vector in R N , whose i-th entry is a feature of node i. Additionally, we use X ? R N ?F to denote the feature matrix, whose columns are graph signals and whose i-th row X i,: = x T i is the feature vector of node i. The label encoding matrix Z ? R N ?C , where C is the number of classes, has its i-th row Z i,: as the one-hot encoding of the label of node i. For simplicity, we denote z i = arg max Z i,: ? {1, 2, . . . C}. The indicator function 1 B equals 1 when event B happens and 0 otherwise.</p><p>For node i, j ? V, if z i = z j , then they are considered as intra-class nodes; if z i = z j , then they are considered to be inter-class nodes. Similarly, an edge e i,j ? E, is considered to be an intra-class edge if z i = z j , and an inter-class edge if z i = z j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph Convolutional Neural Network (GCN)</head><p>The Graph Convolutional Network (GCN) <ref type="bibr" target="#b19">[20]</ref> utilizes the renormalization trick to the affinity matrix, where ?sym = D-1/2 ? D-1/2 , ? ? A + I and D ? D + I. GCN then uses ?sym in each layer of the network as follows,</p><formula xml:id="formula_0">Y = softmax( ?sym ReLU( ?sym XW 0 ) W 1 ),<label>(1)</label></formula><p>where W 0 ? R F0?F1 and W 1 ? R F1?O are learnable parameter matrices. For simplicity, we denote y i = arg max Y i,: ? {1, 2, . . . C}. The Random walk renormalized matrix ?rw = D-1 ? can also be applied to GCN, which is essentially a mean aggregator commonly used in some spatial-based GNNs <ref type="bibr" target="#b14">[15]</ref>. To bridge spectral and spatial methods, we use ?rw in the theoretical analysis, and self-loops are not added to the adjacency matrix to maintain consistency with previous literature <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b25">26]</ref>. Experimental results with symmetric renormalized matrix will be provided in Appendix.</p><p>To address the heterophily challenge, high-pass (HP) filter <ref type="bibr" target="#b10">[11]</ref>, such as I -?rw , is often used to replace low-pass (LP) filter <ref type="bibr" target="#b29">[30]</ref> ?rw in GCN <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b25">26]</ref>. In this paper, we use ?rw and I -?rw as the LP and HP operators, respectively. The LP and HP filtered feature matrices are represented as H = ?rw X and H HP = (I -?rw )X. For simplicity, we denote h i = H T i,: , h HP i = (H HP i,: ) T . A network that includes the feature aggregation step according to graph structure is called graphaware (G-aware) model, e.g., GCN <ref type="bibr" target="#b19">[20]</ref>, SGC <ref type="bibr" target="#b38">[39]</ref>; A network that does not use graph structure is called graph-agnostic (G-agnostic) model, such as Multi-Layer Perceptron with 2 layers (MLP-2) and MLP-1. A G-aware model is often coupled with a G-agnostic model because when we remove the aggregation step in G-aware model, it becomes exactly the same as its coupled G-agnostic model, e.g., GCN is coupled with MLP-2 and SGC is coupled with MLP-1. To measure if the G-aware models can outperform its coupled G-agnostic model before training, a lot of homophily metrics have been proposed and we will introduce the most commonly used ones in the following subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Homophily Metrics for GNNs with Low-pass Filter</head><p>The homophily metric is a way to describe the relation between node labels and graph structure. There are four commonly used homophily metrics: edge homophily <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b41">42]</ref>, node homophily <ref type="bibr" target="#b33">[34]</ref>, class homophily <ref type="bibr" target="#b23">[24]</ref> and aggregation homophily <ref type="bibr" target="#b25">[26]</ref>, defined as follows:</p><formula xml:id="formula_1">H edge (G) = {e uv | e uv ? E, Z u,: = Z v,: } |E| , H node (G) = 1 |V| v?V H v node = 1 |V| v?V {u | u ? N v , Z u,: = Z v,: } d v , H class (G) = 1 C -1 C k=1 h k - {v | Z v,k = 1} N + , where h k = v?V {u | Z v,k = 1, u ? N v , Z u,: = Z v,: } v?{v|Z v,k =1} d v , H agg (G) = S agg S( ?, Z) , S agg S( ?, X) = 1 |V| ? v Mean u {S( ?, X) Zu,:=Zv,: v,u } ? Mean u {S( ?, X) Zu,: =Zv,: v,u }<label>(2</label></formula><p>) where H v node is the local homophily value for node v; [a] + = max(0, a); h k is the class-wise homophily metric <ref type="bibr" target="#b23">[24]</ref>; Mean u ({?}) takes the average over u of a given multiset of values or variables and S( ?, X) = ?X( ?X) T is the post-aggregation node similarity matrix. These metrics all fall within the range of [0, 1], with a value closer to 1 indicating strong homophily and imply that G-aware models are more likely to outperform its coupled G-agnostic model, and vice versa. However, the current homophily metrics are all linear, feature-independent metrics which fail to give an accurate prediction of the superiority of G-aware models and cannot provide a threshold value <ref type="bibr" target="#b25">[26]</ref> for the superiority.</p><p>3 Analysis of Homophily on Node Distinguishability (ND)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motivation</head><p>The Problem in Current Literature Recent research has shown that heterophily does not always negatively impact the embeddings of intra-class nodes, as long as their neighborhood patterns "corrupt in the same way" <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b5">6]</ref>. For example, in Figure <ref type="figure" target="#fig_0">1</ref>, nodes {1,2} both have heterophilic neighborhood patterns and only connect to inter-class nodes, but their heterophilic patterns are the same. As a result, their aggregated features will still be similar and they can be classified into the same class. However, this is not always the truth if we forget to discuss inter-class ND, e.g., for nodes {1,2,3} in Figure <ref type="figure" target="#fig_0">1</ref>, we can see that node 3 also has the same neighborhood pattern as nodes {1,2}, which means the inter-class ND will be lost after aggregation. This highlights the necessity for careful consideration of both intra-and inter-class ND when evaluating the impact of homophily on the performance of GNNs and an ideal case would be node {1,2,4} where we have smaller intra-class "distance" than inter-class "distance". We will formulate the above idea in this section and verify if it really relates to the performance of GNNs in section 4. In order to have more control over the assumptions made about the node embeddings, we consider the Contextual Stochastic Block Model (CSBM) <ref type="bibr" target="#b8">[9]</ref>. It is a generative model that is commonly used to create graphs and node features, and it has been widely adopted to study the behavior of GNNs <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b37">38]</ref>. To investigate the impact of homophily on ND, the authors in <ref type="bibr" target="#b28">[29]</ref> simplify CSBM to the two-normal setting, where the node features X and are assumed to be sampled from two normal distributions and intra-and inter-class edges are generated according to two separate parameters. This simplification does not lose much information about CSBM, but it does not include an explicit homophily parameter to study homophily directly and intuitively, it does not include class variances parameters to study intra-class ND and it does not rigorously define and quantify ND.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">CSBM-H and Optimal Bayes Classifier</head><p>In this section, we introduce the Contextual Stochastic Block Model for Homophily/Heterophily (CSBM-H), which is a variation of CSBM that incorporates an explicit homophily parameter h for the two-normal setting and also has class variance parameters ? 2 0 , ? 2 1 to describe the inner-class ND. We then derive the the optimal Bayes classifier (CL Bayes ) and Expected Negative KL-divergence (ENKL) for CSBM-H, based on which we can quantify ND for CSBM-H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CSBM-H(?</head><formula xml:id="formula_2">0 , ? 1 , ? 2 0 I, ? 2 1 I, d 0 , d 1 , h)</formula><p>The generated graph consists of two disjoint sets of nodes, i ? C 0 and j ? C 1 , corresponding to the two classes. The features of each node are generated independently, with x i generated from N (? 0 , ? 2 0 I) and x j generated from N (? 1 , ? 2 1 I), where ? 0 , ? 1 ? R F h and F h is the dimension of the embeddings. The degrees of nodes in C 0 and C 1 are d 0 , d 1 ? N respectively. For i ? C 0 , its neighbors are generated by independently sampling from h ? d 0 intra-class nodes and (1 -h) ? d 0 inter-class nodes. The neighbors of j ? C 1 are generated in the same way. As a result, the original, LP and HP filtered features are as follows,</p><formula xml:id="formula_3">i ? C 0 : x i ? N (? 0 , ? 2 0 I); h i ? N ( ?0 , ?2 0 I), h HP i ? N ?HP 0 , (? HP 0 ) 2 I , j ? C 1 : x j ? N (? 1 , ? 2 1 I); h j ? N ( ?1 , ?2 1 I), h HP j ? N ?HP 1 , (? HP 1 ) 2 I ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_4">?0 = h(? 0 -? 1 ) + ? 1 , ?1 = h(? 1 -? 0 ) + ? 0 , ?HP 0 = (1 -h)(? 0 - ? 1 ), ?HP 1 = (1 -h)(? 1 -? 0 ), ?2 0 = (h(? 2 0 -? 2 1 )+? 2 1 ) d0 , ?2 1 = (h(? 2 1 -? 2 0 )+? 2 0 ) d1 , (? HP 0 ) 2 = ? 2 0 + (h(? 2 0 -? 2 1 )+? 2 1 ) d0 , (? HP 1 ) 2 = ? 2 1 + (h(? 2 1 -? 2 0 )+? 2 0 ) d1 .</formula><p>Inter-class ND The squared Euclidean distance between two means can describe the inter-class ND and we have</p><formula xml:id="formula_5">d 2 X = ? 0 -? 1 2 2 , d 2 H = ?0 -?1 2 2 = (2h -1) 2 d 2 X , d 2 HP = ?HP 0 -?HP 1 2 2 = 4(1 -h) 2 d 2 X .<label>(4)</label></formula><p>As h varies between 0 and 1, d 2 H is a U-shaped curve<ref type="foot" target="#foot_0">1</ref> that follows a quadratic function and d 2 HP is monotonically decreasing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intra-class ND</head><p>The variance of each class can describe the intra-class ND. When comparing variances, if ? 2 0 &lt; ? 2 1 , we refer to C 0 as the low variation class and C 1 as the high variation class. It is observed that LP filter will reduce the class variances and HP filter will increase the class variance regardless of the homophily value.</p><p>To quantify the ND of CSBM-H and investigate how it is related to inter-class and intra-class ND, we first compute the optimal Bayes classifier in the following theorem. The theorem is about x, but the results are applicable to h and h HP when the parameters are replaced according to Equation <ref type="formula" target="#formula_3">3</ref>.</p><formula xml:id="formula_6">Theorem 1. Suppose ? 2 0 = ? 2 1 and ? 2 0 , ? 2 1 &gt; 0, the optimal Bayes Classifier (CL Bayes ) for CSBM-H (? 0 , ? 1 , ? 2 0 I, ? 2 1 I, d 0 , d 1 , h) is CL Bayes (x i ) = 1, ?(x i ) ? 0.5 0, ?(x i ) &lt; 0.5 , and ?(x i ) = P(y i = 1|x i ) = 1 1 + exp (Q(x i )) , where Q(x i ) = ax T i x i + b T x i + c, a = 1 2 1 ? 2 1 -1 ? 2 0 , b = ?0 ? 2 0 -?1 ? 2 1 , c = ? T 1 ?1 2? 2 1 - ? T 0 ?0 2? 2 0 + ln n0? F h 1 n1? F h 0 . Proof. See Appendix A.</formula><p>Advantages of CL Bayes Over <ref type="bibr" target="#b28">[29]</ref> The authors in <ref type="bibr" target="#b28">[29]</ref> classify the nodes by a fixed linear classifier which only depends on the centers of the two distributions ? 0 , ? 1 . As h changes, the centers will also be different, and a well-defined ND should be able to capture this variation. However, the fixed linear classifier in <ref type="bibr" target="#b28">[29]</ref> cannot capture the movement of distributions and thus, is not qualified to measure ND for different h. Besides, we cannot investigate how variances ?<ref type="foot" target="#foot_1">2</ref> 0 , ? 2 1 and node degree d 0 , d 1 affect ND with the fixed classifier in <ref type="bibr" target="#b28">[29]</ref>. Remark The Bayes classifier (CL Bayes ) for multiple categories (&gt; 2) can be computed by stacking multiple expectation terms using similar methods as in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12]</ref>. We do not discuss the more complicated settings in this paper.</p><p>In the following subsection, we will define two methods to quantify ND of CSBM-H, one is based on CL Bayes , which is a precise measure but hard to be explainable; another is based on KL-divergence, which can give us more intuitive understanding of how intra-and inter-class ND will impact ND together at different homophily levels. These two measurements can also be used together to analyze the effect of LP and HP filters on ND.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Measure Node Distinguishability of CSBM-H</head><p>The Bayes error rate (BE) is the probability of a node being mis-classified when the true class probabilities given the predictors are known <ref type="bibr" target="#b15">[16]</ref>. It can be used to measure the distinguishability of node embeddings and the BE for CL Bayes is defined as follows, Definition 1 (Bayes Error Rate). The Bayes error rate <ref type="bibr" target="#b15">[16]</ref> for CL Bayes is defined as</p><formula xml:id="formula_7">BE = E x [P(y|CL Bayes (x) = y)] = E x [1 -P(CL Bayes (x))].</formula><p>Specifically, the BE for CSBM-H can be written as</p><formula xml:id="formula_8">BE = P (x ? C 0 ) (1 -P(CL Bayes (x) = 0|x ? C 0 )) + P(x ? C 1 ) (1 -P(CL Bayes (x) = 1|x ? C 1 )) .</formula><p>(5) In order to estimate the above value, we define Probabilistic Bayes Error (PBE). Probabilistic Bayes Error (PBE) The random variable in each dimension of x i is independently normally distributed. As a result, the quadratic function of these random variables, Q(x i ), follows a generalized ? 2 distribution 2 (See the calculation in the appendix C). Specifically,</p><formula xml:id="formula_9">For i ? C 0 , Q(x i ) ? ?2 (w 0 , F h , ? 0 ) + ?; j ? C 1 , Q(x j ) ? ?2 (w 1 , F h , ? 1 ) + ?,</formula><p>where</p><formula xml:id="formula_10">w 0 = a? 2 0 , w 1 = a? 2 1 , F h is the degree of freedom, ? 0 = ( ?0 ?0 + b 2a?0 ) T ( ?0 ?0 + b 2a?0 ), ? 1 = ( ?1 ?1 + b 2a?1 ) T ( ?1 ?1 + b 2a?1 ) and ? = c -b T b 4a .</formula><p>Then, by using the Cumulative Distribution Function (CDF) of ?2 , we can calculate the predicted probabilities directly as,</p><formula xml:id="formula_11">P(CL Bayes (x) = 0|x ? C 0 ) = P(Q(x) &gt; 0|x ? C 0 ) = 1 -CDF ?2 (w0,F h ,?0) (-?), P(CL Bayes (x) = 1|x ? C 1 ) = P(Q(x) ? 0|x ? C 1 ) = CDF ?2 (w1,F h ,?1) (-?). Suppose we have a balanced prior distribution P(x ? C 0 ) = P(x ? C 1 ) = 1 2 . Then, the PBE is computed as, CDF ?2 (w0,F h ,?0) (-?) + 1 -CDF ?2 (w1,F h ,?1) (-?) 2</formula><p>To investigate the impact of homophily on the ND of LP filtered and HP filtered embeddings, we just need to replace ? 0 , ? 2 0 , ? 1 , ? 2 1 with ?0 , ?2 0 , ?1 , ?2 1 and ?HP 0 , (? HP 0 ) 2 , ?HP 1 , (? HP 1 ) 2 as equation 3. PBE can be numerically calculated and visualized to show the relation between h and ND precisely. However, we do not have an analytic expression for PBE, which makes it less explainable and intuitive. To address this issue, we define another metric for ND in the following paragraphs.</p><p>Expected Negative KL-divergence (ENKL) KL-divergence is a statistical measure of how a probability distribution P is different from another distribution Q <ref type="bibr" target="#b7">[8]</ref>. It offers us a tool to define an explainable ND measure, Expected Negative KL-divergence, as follows. Definition 2 (Expected Negative KL-divergence). The expected negative KL-divergence between two distributions P (x), Q(x) is defined as</p><formula xml:id="formula_12">ENKL(P, Q) = E x [-D KL ] = -P(x ? P )E x?P ln P (x) Q(x) -P(x ? Q)E x?Q ln Q(x) P (x)</formula><p>From the above definition, the ENKL for the two-normal setting in CSBM-H can be computed by</p><formula xml:id="formula_13">ENKL(CSBM-H) = -d 2 X ( 1 4? 2 1 + 1 4? 2 0 ) Negative Normalized Distance - F h 4 (? 2 + 1 ? 2 -2)</formula><p>Negative Variance Ratio <ref type="bibr" target="#b5">(6)</ref> where</p><formula xml:id="formula_14">d 2 X = (? 0 -? 1 ) T (? 0 -? 1 ), ? = ?0</formula><p>?1 and since we assume ? 2 0 &lt; ? 2 1 , we have 0 &lt; ? &lt; 1. The smaller ENKL a CSBM-H has, the more distinguishable the inter-class nodes are.</p><p>From equation 6 we can see that, ENKL mainly relies on two terms which are related to both intraand inter-class ND: Expected Negative Normalized Distance (ENND) and the Negative Variance Ratio (NVR). ENND depends on how large is the inter-class ND d 2 X compared with the normalization term</p><formula xml:id="formula_15">1 4? 2 1 + 1 4? 2 0</formula><p>determined by intra-class ND, i.e., if inter-class ND is large and intra-class ND is small, then ENND is small which means the nodes are more distinguishable and vice versa; NVR depends on how different the two intra-class NDs are, i.e., when the intra-class ND of high-variation class is significantly larger than that of low-variation class (? is close to 0), NVR is small which means the nodes are more distinguishable and vice versa. Given the aforementioned measures, we can now  </p><formula xml:id="formula_16">0 = [-1, 0], ? 1 = [0, 1], ? 2 0 = 1, ? 2 1 = 2, d 0 = 5, d 1 = 5</formula><p>, the curve for LP filtered features h is bell-shaped, indicating that when the homophily value is extremely low or high, the aggregated node embeddings become more distinguishable than at medium levels of homophily. The curve for h HP is monotonically increasing, which means that the high-pass filter works better in heterophily areas than in homophily areas. Moreover, we can see that x, h, and h HP will get the lowest PBE and ENKL in different homophily intervals, which we refer to as the "FP zone (black)", "LP zone (green)", and "HP zone (red)". This indicates that LP filter works better at very low and very high homophily intervals (two ends), HP filter works better at low to medium homophily interval <ref type="foot" target="#foot_2">3</ref> , the original features works betters at medium to high homophily area.</p><p>In the existing literature, people are interested in discussing how does node degree relate to the effect of homophily <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b39">40]</ref>. In the following subsection, we will take a deeper look at how node degree and class variances influence the homophily-ND curves and the three zones by conducting ablation study over ? 2 1 , ? 2 2 , d 0 , d 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablation Study on CSBM-H</head><p>Increase the Variance of High-variation Class (? 2 0 = 1, ? 2 1 = 5) From Figure <ref type="figure" target="#fig_3">3</ref>, it is observed that as the variance in C 1 increases and the variance between C 0 and C 1 becomes more imbalanced, the PBE and ENKL of the three curves all go up which means the node embeddings become less distinguishable under HP, LP and FP filters. The significant shrinkage of the LP and HP zones and the expansion of the X zone indicates that the original features are more robust to imbalanced variances especially in the low heterophily area, which can be reflected by the NVR in Figure <ref type="figure" target="#fig_3">3 (d)</ref>. In other words, h HP is less sensitive to ? than x and h in low homophily area.</p><p>Increase the Variance of Low-variation Class (? 2 0 = 1.9, ? 2 1 = 2) As shown in Figure <ref type="figure" target="#fig_12">8</ref> in Appendix E, when the variance in C 0 increases and the variance between C 0 and C 1 becomes more balanced, PBE and ENKL curves go up which means the node embeddings become less distinguishable. The H, HP and the X zones almost stays the same because the magnitude of NVR becomes too small that it almost has no effect to ND as shown in Figure <ref type="figure" target="#fig_12">8 (d)</ref>.</p><p>Interestingly, we found the change of variances only cause little difference of the 3 zones in ENND and the movement of 3 zones mainly comes from NVR <ref type="foot" target="#foot_3">4</ref> and HP filter is less sensitive to ? changes in low homophily area. This insensitivity will have significant impact to the 3 zones when ? is close to 0 and have trivial effect when ? is close to 1 because the magnitude of NVR is too small.  This leads to a substantial expansion of LP zone and shrinkage of FP and HP zone. This is mainly due to the decrease of ENND of LP filters and the decrease of its NVR in low homophily area also plays an important role.</p><p>Increase the Node Degree of Low-variation Class (d 0 = 25, d 1 = 5) From Figure <ref type="figure" target="#fig_4">4</ref>, we have the similar observation as when we increase the node degree of how-variation class. The difference is that the expansion of LP zone and shrinkage of FP and HP zones are not as significant as before.</p><p>We can see that increasing node degree helps a lot for LP filter, especially for high-variation class, while HP filter less sensitive to the change of node degree. This can be found from ?2 0 , ?2 1 , (? HP 0 ) 2 , (? HP 1 ) 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">More General Theoretical Analysis</head><p>In this subsection, we aim to gain a deeper understanding of how LP and HP affect node distinguishability in a broader context beyond the two-normal settings. To be consistent with previous literature, we follow the assumptions outlined in <ref type="bibr" target="#b28">[29]</ref>, which are: (1) The features of node i are sampled from feature distribution F zi , i.e.,, x i ? F zi , with mean ? zi ; (2) Dimensions of x i are independent to each other; (3) Each dimension in feature x i is bounded by a specific range, i.e.,, a ? x i,k ? b; (4) For node i, the labels of its neighbors are independently sampled from neighborhood distribution D zi and repeated for d i times. We refer to a graph that follows the above assumptions as G = {V, E, {F c , c ? C} , {D c , c ? C}}, where C = {1, . . . , C} is the label set; (b -a) 2 reflects how variation the features are. Instead of describing ND by the distance between the node embedding and its expectation as <ref type="bibr" target="#b28">[29]</ref>, i.e., h i -E(h i ) 2 , which only considers the embeddings within the same class and has been shown to be insufficient for ND, in the following theorem, we investigate ND by studying how significant is the intra-class embedding distance smaller than the inter-class embedding distance, which is a better way to describe ND and is formulated as follows, Theorem 2. Suppose the above assumptions (1-4) all hold. For node i, j, v ? V, suppose z i = z j and z i = z v , then for constants t x , t h , t HP that satisfy</p><formula xml:id="formula_17">t x ? ? F h D x (i, j), t h ? ? F h D h (i, j), t HP ? ? F h D HP (i, j) we have P x i -x j 2 ? x i -x v 2 + t x ? 2F h exp - (D x (v, j) -tx ? F h ) 2 V x (v, j) , P( h i -h j 2 ? h i -h v 2 + t h ) ? 2F h exp - (D h (v, j) -t h ? F h ) 2 V h (v, j) , P( h HP i -h HP j 2 ? h HP i -h HP v 2 + t HP ) ? 2F h exp ? ? ?- D HP (v, j) -tHP ? F h 2 V HP (v, j) ? ? ?,<label>(7)</label></formula><p>where</p><formula xml:id="formula_18">D x (v, j) = ? zv -? zj 2 , V x (v, j) = (b -a) 2 , D h (v, j) = ?zv -?zj 2 , V h (v, j) = 1 2d v + 1 2d j (b -a) 2 , D HP (v, j) = ? zv -?zv -? zj -?zj 2 , V HP (v, j) = 1 + 1 2d v + 1 2d j (b -a) 2 , ?zv = u?N (v) E zu?Dz v , xu?Fz u 1 d v x u .</formula><p>We can see that, the probability upper bound mainly depends on a distance term (inter-class ND) and normalized variance term (intra-class ND). The normalized variance term of HP filter is less sensitive to the changes of node degree than that of LP filter because there is an additional 1 in the constant term. Moreover, we show that the distance term of HP filter actually depends on the relative center distance which is a novel discovery. As shown in Figure <ref type="figure" target="#fig_6">6</ref>, when homophily decreases, the aggregated centers will move away from the original centers, and the relative center distance (purple) will get larger which means the embedding distance of nodes from different classes will have larger probability to be big. This explains how HP filter work for some heterophily cases. Overall, in a more general setting with weaker assumptions, we can see that ND is also described by the intra-and inter-class ND terms together rather than intra-class ND only, which is consistent with CSBM-H.  The above results investigate the effect of homophily on ND mostly from theoretical perspective. In this section, we will conduct experiments to verify whether the effect of homophily on GNNs performance really relates to its effect on ND. If a strong relation can be verified, then it indicates that we can design a new metric based on ND, besides homophily metrics introduced in section 2.2, to test the superiority of G-aware models without training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Hypothesis Testing of Inter-class v.s. Intra-class Distance</head><p>To test whether "intra-class embedding distance is smaller than the inter-class embedding distance" strongly relates to the superiority of G-aware models to their coupled G-agnostic models in practice, we conduct the following hypothesis testing.</p><p>Experimental Setup We first train two G-aware models GCN, SGC-1 and their coupled G-agnostic models MLP-2 and MLP-1 with fine-tuned hyperparameters. For each trained model, we calculate the pairwise Euclidean distance of the embeddings in output layers, e.g., we get ED(GCN) for GCN, where ED(GCN) i,j is the embedding distance between node i, j in the output layer of the trained GCN. Next, we compute the proportion of nodes whose average intra-class node distance is significantly smaller 5 than inter-class node distance. e.g., we obtain Prop(GCN) for GCN. By training the models multiple times, we obtain a couple of Prop samples for each model. And to compare the Prop values, we set up the following 3 hypotheses:</p><formula xml:id="formula_19">H 0 : Prop(G-aware model) = Prop(G-agnostic model); H 1 : Prop(G-aware model) &gt; Prop(G-agnostic model); H 2 : Prop(G-aware model) &lt; Prop(G-agnostic model)</formula><p>Specifically, we compare GCN v.s. MLP-2 and SGC-1 v.s. MLP-1 on 9 widely used benchmark datasets with different homophily values for 100 times. In each time, we randomly split the data into train/validation/test sets with a ratio of 60%/20%/20%. With the 100 samples, we conduct T-test for the means of two independent samples of scores, and obtain the corresponding p-values. The test results and model performance comparisons are shown in Table <ref type="table" target="#tab_0">1</ref>.</p><p>We can observe that, in most cases (except for GCN v.s. MLP-2 on PubMed), when H 1 significantly holds, G-aware models will outperform the G-agnostic models; when H 2 significantly holds, G-aware models will underperform the G-agnostic models. This indicates that the superiority of G-aware models is closely related to their ability to reduce the intra-class embedding distance relative to inter-class embedding distance and vice versa, no matter the homophily levels. This reminds us that there exists a better performance metric for GNNs than homophily metrics. Additionally, the p-values provide a threshold value, such as p ? 0.05, that can determine whether the aggregation step is beneficial or not. This property is not present in existing homophily metrics.</p><p>However, one limitation of this method is that it requires training the models to obtain the p-values, which can make it less practical in certain situations. To overcome this issue, in the next subsection, we propose a kernel-based performance metric that can provide a p-value without the need for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Kernel Performance Metric</head><p>Kernel method utilizes a pairwise similarity function K(x i , x j ) to measure how closely related two node embeddings are, without the need for any training process <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b32">33]</ref>. A higher value of K(x i , x j ) indicates a smaller distance between the embeddings of nodes x i and x j and vice versa. To capture the feature-based non-linear node similarity, we use the kernel based on Neural Network Gaussian Process (NNGP) <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b30">31]</ref>. Specifically, we consider the activation function ?(x) = ReLU(x) and have</p><formula xml:id="formula_20">KNNGP(xi, xj) = E w?N (0,I) ?(w T xi)?(w T xj) = 1 2? x T i xj ? -? x T i xj xi 2 xj 2 + xi 2 2 xj 2 2 -(x T i xj) 2</formula><p>where ?(x) = arccos(x) is the dual activation function of ReLU. Additionally, when ?(x) = exp (ix), we have</p><formula xml:id="formula_21">K(x i , x j ) = exp (-1 2 x i -x j<label>2</label></formula><p>2 ), which is closely related to the Euclidean 5 By saying "significantly smaller", we mean that by hypothesis testing, the nodes with p-values smaller than 0.05 is considered as "significantly smaller". distance of node embeddings tested in section 4.1, further emphasizing the strong relationship between embedding distances and kernel similarities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>After obtaining K X</head><p>NNGP and K H NNGP , we calculate the proportion of nodes whose average intraclass similarity is significantly higher than inter-class similarity, resulting in Prop(K X NNGP ) and Prop(K H NNGP ). To estimate these values, we randomly divide the data into 60%/20%/20% parts for 100 iterations and only use the 60% training data to calculate Prop(K X NNGP ) and Prop(K H NNGP ). We then test the following three hypotheses:</p><formula xml:id="formula_22">H 0 : Prop(K H NNGP ) = Prop(K X NNGP ); H 1 : Prop(K H NNGP ) &gt; Prop(K X NNGP ); H 2 : Prop(K H NNGP ) &lt; Prop(K X NNGP ).</formula><p>As seen in Table <ref type="table" target="#tab_0">1</ref>, the p-values can provide a threshold value, such as 0.05, to predict whether aggregated features are better than the original features for node classification. Even with an outlier, e.g., Squirrel, the p-value is still very small for H 0 v.s. H 1 . Thus, we use the p-value as the kernel performance metric and Table <ref type="table" target="#tab_1">2</ref> summarizes its advantages over the existing metrics <ref type="foot" target="#foot_4">6</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we demonstrate that we need to consider both intra-and inter-class node embeddings in order to study the impact of homophily on node distinguishability. To simplify the analysis, we propose CSBM-H, where we can directly quantify the node distinguishability by the BE of the CL Bayes . We study the node distinguishability of the aggregated and diversified features through PBE on all homophily levels and state how the PBE changes for different class size, node degree and class variances settings. We then conduct experiments and hypothesis testing to explore how the relation between intra-and inter-class node embeddings relates to the superiority of G-aware models over G-agnostic models. Based on observation, we propose kernel homophily, which can accurately reveal the superiority and inferiority of G-aware models without training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proof of Theorem 1</head><p>Theorem 1. Optimal Bayes Classifier (CL Bayes ) for CSBM-H(?</p><formula xml:id="formula_23">0 , ? 1 , ? 2 0 I, ? 2 1 I, d 0 , d 1 , h) is CL Bayes (y i = 1|x i ) = 1 1 + exp ax T i x i + b T x i + c CL Bayes (y i = 0|x i ) = exp ax T i x i + b T x i + c 1 + exp ax T i x i + b T x i + c where a = 1 2 1 ? 2 1 -1 ? 2 0 , b = ?0 ? 2 0 -?1 ? 2 1 , c = ? T 1 ?1 2? 2 1 - ? T 0 ?0 2? 2 0 + ln n0? F h 1 n1? F h 0 .</formula><p>Proof. The prior for the Bayes classifier is</p><formula xml:id="formula_24">P r(y i = 0) = n 0 n 0 + n 1 , P r(y i = 1) = n 1 n 0 + n 1 CL Bayes (y i = 1|x i ) = P r(y i = 1, x i ) P r(x i ) = P r(y i = 1)P r(x i |y i = 1) P r(y i = 0)P r(x i |y i = 0) + P r(y i = 1)P r(x i |y i = 1) = 1 1 + P r(yi=0)P r(xi|yi=0) P r(yi=1)P r(xi|yi=1) = 1 1 + n0(2?) -F h /2 det (? 2 0 I) -1/2 exp -1 2? 2 0 (xi-?0) T (xi-?0) n1(2?) -F h /2 det (? 2 1 I) -1/2 exp -1 2? 2 1 (xi-?1) T (xi-?1) = 1 1 + n0? -F h 0 n1? -F h 1 exp -1 2? 2 0 (x i -? 0 ) T (x i -? 0 ) + 1 2? 2 1 (x i -? 1 ) T (x i -? 1 ) = 1 1 + n0? -F h 0 n1? -F h 1 exp -1 2? 2 0 (x T i x i -2? T 0 x i + ? T 0 ? 0 ) + 1 2? 2 1 (x T i x i -2? T 1 x i + ? T 1 ? 1 ) = 1 1 + exp ( 1 2? 2 1 -1 2? 2 0 )x T i x i + ( ?0 ? 2 0 -?1 ? 2 1 ) T x i + ? T 1 ?1 2? 2 1 - ? T 0 ?0 2? 2 0 + ln n0? F h 1 n1? F h 0 Lemma 1. Let x i = X i,:</formula><p>and suppose each dimension of x i are independent, then</p><formula xml:id="formula_25">P h i -h j 2 ? t ? F h k=1 P |h i,k -h j,k | ? t ? F h Proof. If h i -h j 2 ? t, then at least for one k ? {1, . . . , F h }, the inequality |h i,k -h j,k | ? t ? F h</formula><p>holds. Hence, we have</p><formula xml:id="formula_26">P h i -h j 2 ? t ? P F h k=1 |h i,k -h j,k | ? t ? F h ? F h k=1 P |h i,k -h j,k | ? t ? F h</formula><p>Lemma 2. (Heoffding Lemma) Let X be any real-valued random variable such that a ? X ? b almost surely, i.e., with probability one. Then, for all ? ? R,</p><formula xml:id="formula_27">E e ?X ? exp ?E[X] + ? 2 (b -a) 2<label>8</label></formula><p>Theorem 2. For node i, j, v ? V, suppose z i = z j and z i = z v , then for constants t x , t h , t HP that satisfy</p><formula xml:id="formula_28">t x ? ? F h D x (i, j), t h ? ? F h D h (i, j), t HP ? ? F h D HP (i, j) we have P x i -x j 2 ? x i -x v 2 + t x ? 2F h exp - (D x (i, j) -tx ? F h ) 2 V x (i, j) , P( h i -h j 2 ? h i -h v 2 + t h ) ? 2F h exp - (D h (i, j) -t h ? F h ) 2</formula><p>V h (i, j) ,</p><formula xml:id="formula_29">P( h HP i -h HP j 2 ? h HP i -h HP v 2 + t HP ) ? 2F h exp ? ? ?- D HP (i, j) -tHP ? F h 2 V HP (i, j) ? ? ?,<label>(8)</label></formula><p>where</p><formula xml:id="formula_30">D x (i, j) = ? zi -? zj 2 , V x (i, j) = (b -a) 2 , D h (i, j) = ?zi -?zj 2 , V h (i, j) = 1 2d i + 1 2d j (b -a) 2 , D HP (i, j) = ? zi -?zi -? zj -?zj 2 , V HP (i, j) = 1 + 1 2d i + 1 2d j (b -a) 2 , ?zi = v?N (i) E zv?Dz i , xv?Fz v 1 d i x v .</formula><p>Proof. Part for filter:</p><formula xml:id="formula_31">Let h i,k = 1 di u?N (i), zu?Dz i , x u,k ?F zu ,k x u,k and ?zi,k = E [h i,k ] = E 1 di x u,k . Since we have h i -h j 2 - h i -h v 2 ? h i -h j -(h i -h v ) 2 , then P h i -h j 2 ? h i -h v 2 + t h = P h i -h j 2 -h i -h v 2 ? t h ? P h i -h j -(h i -h v ) 2 ? t h = P h v -h j ) 2 ? t h</formula><p>We will calculate the upper bound of P h v -h j 2 ? t h in the following part. To do this, we first compute the upper bound of P (h v,k -h j,k ? t).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Suppose t &gt; ?</head><p>F h ?zv?zj and for any s ? 0, we have</p><formula xml:id="formula_32">P (h v,k -h j,k ? t) = P (exp (s(h v,k -h j,k )) ? exp (st)) ? exp (-st)E [exp (s(h v,k -h j,k ))] (Markov Inequality) = exp (-st)E ? ? ? ? ? ? ? exp ? ? ? ? ? ? ? s d v u?N (v), zu?Dz v , x u,k ?F zu ,k x u,k ? ? ? ? ? ? ? ? ? ? ? ? ? ? E ? ? ? ? ? ? ? exp ? ? ? ? ? ? ? -s d j u?N (j), zu?Dz j , x u,k ?F zu,k x u,k ? ? ? ? ? ? ? ? ? ? ? ? ? ? (Independency) = exp (-st) u?N (v), zu?Dz v , x u,k ?F zu ,k E exp s d v x u,k u?N (j), zu?Dz j , x u,k ?F zu ,k E exp -s d j x u,k<label>(Independency)</label></formula><p>? exp (-st) With the same steps, we have</p><formula xml:id="formula_33">u?N (v), zu?Dz v , x u,k ?F zu ,k exp s d v E [x u,k ] + (b -a) 2 s 2 8d 2 i u?N (j), zu?Dz j , x u,k ?Fz u,k exp -s d j E [x u,k ] + (b -a) 2 s 2 8d 2 j (Hoeffding's lemma) = exp (-st) exp (b -a) 2 s 2 8d 2 v exp ? ? ? ? ? ? ? sE ? ? ? ? ? ? ? 1 d v u?N (v), zu?Dz v , x u,k ?F zu,k x u,k ? ? ? ? ? ? ? ? ? ? ? ? ? ? exp (b -a) 2 s 2 8d 2 j exp ? ? ? ? ? ? ? -sE ? ? ? ? ? ? ? 1 d j u?N (j), zu?Dz j , x u,k ?F zu ,k x u,k ? ? ? ? ? ? ? ? ? ? ? ? ? ? = exp ( (b -a) 2 8d v + (b -a) 2 8d j )s 2 + (? zv,k -?zj,k -t)s ? exp ( (b -a) 2 8d v + (b -a) 2 8d j )s 2 + ( ?zv,k -?zj,k -t)s Since t &gt; ? F h ?zv -?zj , so when s = - (| ?zv,k -?z j ,k |-t)</formula><formula xml:id="formula_34">P (h v,k -h j,k ? -t) = P (h j,k -h v,k ? t) ? exp ? ? - ( ?zv -?zj 2 -t) 2 (b-a) 2 2dv + (b-a) 2 2dj ? ?</formula><p>Combined together we have</p><formula xml:id="formula_35">P h v,k -h j,k ? t ? 2 exp ? ? - ( ?zv -?zj 2 -t) 2 (b-a) 2 2dv + (b-a) 2 2dj ? ?</formula><p>From the previous lemma, we have</p><formula xml:id="formula_36">P h v -h j 2 ? t ? F h k=1 P |h v,k -h j,k | ? t ? F h ? 2F h exp ? ? - ( ?zv -?zj 2 -t ? F h ) 2 (b-a) 2 2dv + (b-a) 2 2dj ? ?<label>(9)</label></formula><p>A.1 Theoretical Results for High-pass Filter</p><p>Proof. The proof for HP filter is similar to that of LP filter.</p><p>Let h HP i = x i -h i , which is the HP filtered signal. Since we have</p><formula xml:id="formula_37">h HP i -h HP j 2 -h HP i -h HP v 2 ? h HP i -h HP j -(h HP i -h HP v ) 2 , then P h HP i -h HP j 2 ? h HP i -h HP v 2 + t = P h HP i -h HP j 2 -h HP i -h HP v 2 ? t ? P h HP i -h HP j -(h HP i -h HP v ) 2 ? t h = P h HP v -h HP j ) 2 ? t</formula><p>We will calculate the upper bound of P h HP v -h HP j 2 ? t in the following part. To do this, we first compute the upper bound of P h HP v,k -h HP j,k ? t .</p><p>Since t ? ? F h ? v -?v? j -?j 2 , for s ? 0 and the k-th element of h HP v and h HP j , we have</p><formula xml:id="formula_38">P h HP v,k -h HP j,k ? t = P (x v,k -h v,k -x j,k + h j,k ? t) = P (exp (s(x v,k -h v,k -x j,k + h j,k )) ? exp (st)) ? exp (-st)E [exp (s(x v,k -h v,k -x j,k + h j,k ))] (Markov Inequality) = exp (-st) ? E [exp (sx v,k )] ? E [exp (-sx j,k )] ? E ? ? ? ? ? ? ? exp ? ? ? ? ? ? ? - s d v u?N (v), zu?Dz v , x u,k ?F zu ,k x u,k ? ? ? ? ? ? ? ? ? ? ? ? ? ? E ? ? ? ? ? ? ? ? exp ? ? ? ? ? ? ? ? s d i u?N (j), zu?Dz j , x u,k ?Fz u,k x u,k ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? (Independency) ? exp (-st)E [exp (sx v,k )] E [exp (-sx j,k )] u?N (v), zu?Dz v , x u,k ?F zu ,k E exp -s d v x u,k u?N (j), zu?Dz j , x u,k ?Fz u,k E exp s d j x u,k ? exp (-st) exp s? v,k + (b -a) 2 s 2 8 exp -s? j,k + (b -a) 2 s 2 8 exp -s d v E[x u,k ] + (b -a) 2 s 2 8d 2 i exp s d j E[x u,k ] + (b -a) 2 s 2 8d 2 j (Hoeffding's lemma) = exp ( (b -a) 2 4 + (b -a) 2 8d v + (b -a) 2 8d j )s 2 + ? v,k -? j,k -(? v,k -?j,k ) -t s ? exp ( (b -a) 2 4 + (b -a) 2 8d v + (b -a) 2 8d j )s 2 + ? v,k -? j,k -(? v,k -?j,k ) -t s Since t ? ? F h ? v -?v -? j -?j 2 , then when s = - |? v,k -? j,k -( ?v,k -?j,k )|-t ( (b-a) 2 2 + (b-a) 2 4dv + (b-a) 2 4d j )</formula><p>&gt; 0, we get the tightest bound and</p><formula xml:id="formula_39">exp - ? v,k -? j,k -(? v,k -?j,k ) -t 2 (1 + 1 2dv + 1 2dj )(b -a) 2 ? exp ? ? ?- ? v -?v -? j -?j 2 -t 2 (1 + 1 2dv + 1 2dj )(b -a) 2 ? ? ? Then P h HP v,k -h HP j,k ? t ? 2 exp ? ? ?- ? v -?v -? j -?j 2 -t 2 (1 + 1 2dv + 1 2dj )(b -a) 2 ? ? ?</formula><p>From the previous lemma and theorem, we have </p><formula xml:id="formula_40">P h HP v -h HP j 2 ? t ? F h k=1 P h HP v,k -h HP j,k ? t ? F h ? 2F h exp ? ? ?- ? v -?v -? j -?j 2 -t ? F h 2 (1 + 1 2dv + 1 2dj )(b -a) 2 ? ? ?<label>(</label></formula><formula xml:id="formula_41">P (x) = N (? 0 , ? 2 0 I), Q(x) = N (? 1 , ? 2 1 I) Then, the KL-divergence between P (x) and Q(x) is D KL (P ||Q) = P (x) ln P (x) Q(x) dx = E x?P (x) ln P (x) Q(x) = E x?P (x) ln ? F h 1 ? F h 0 exp - 1 2 ? -2 0 (x -? 0 ) (x -? 0 ) + 1 2 ? -2 1 (x -? 1 ) (x -? 1 ) = F h ln ? 1 ? 0 + E x?P (x) - 1 2 ? -2 0 (x -? 0 ) (x -? 0 ) + 1 2 ? -2 1 (x -? 1 ) (x -? 1 ) = F h ln ? 1 ? 0 - F h 2 + E x?P (x) 1 2 ? -2 1 (x -? 1 ) (x -? 1 ) = F h ln ? 1 ? 0 - F h 2 + F h ? 2 0 2? 2 1 + (? 0 -? 1 ) T (? 0 -? 1 ) 2? 2 1 = F h ln ? 1 ? 0 - F h 2 + d 2 M (x, Q(x)|x ? P (x))</formula><p>where d 2 M is the squared Mahalanobis distance. In the same way, we have</p><formula xml:id="formula_42">D KL (Q||P ) = F h ln ? 0 ? 1 - F h 2 + F h ? 2 1 2? 2 0 + (? 0 -? 1 ) T (? 0 -? 1 ) 2? 2 0 Suppose P(x ? P ) = P(x ? Q) = 1 2 , then we have D ENKL (P, Q) = -E x [D KL ] = -P(x ? P )E x?P ln P (x) Q(x) -P(x ? Q)E x?Q ln Q(x) P (x) = - F h 4 (? 2 + 1 ? 2 -2) -d 2 E ( 1 4? 2 1 where d 2 E = (? 0 -? 1 ) T (? 0 -? 1 )</formula><p>, which is the squared Euclidean distance between the means of the two distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Noncentral ? 2 distribution C.1 An Introduction</head><p>Noncentral ? 2 distribution Let (X 1 , X 2 , . . . , X i , . . . , X k ) be k independent, normally distributed random variables with means ? i and unit variances. Then the random variable</p><formula xml:id="formula_43">k i=1 X 2 i ? ? 2 (k, ?)</formula><p>is distributed according to the noncentral ? 2 distribution. It has two parameters (k, ?): k which specifies the number of degrees of freedom (i.e., the number of X i ), and ? which is the sum of the squared mean of the random variables X i :</p><formula xml:id="formula_44">? = k i=1 ? 2 i .</formula><p>? is sometimes called the noncentrality parameter.</p><p>Generalized ? 2 distribution The generalized ? 2 variable can be written as a linear sum of independent noncentral ? 2 variables and a normal variable:</p><formula xml:id="formula_45">? = i w i Y i + X, Y i ? ? 2 (k i , ? i ) , X ? N m, s<label>2</label></formula><p>Here the parameters are the weights w i , the degrees of freedom k i and non-centralities ? i of the constituent ? 2 , and the normal parameters m and s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Quadratic Function</head><p>For i ? C 0 , let x i = ? 0 y i + ? 0 , y i is the standard normal variable</p><formula xml:id="formula_46">Q(x i ) = ax T i x i + b T x i + c = a(x i + b 2a ) T (x i + b 2a ) + c - b T b 4a = a(? 0 y i + ? 0 + b 2a ) T (? 0 y i + ? 0 + b 2a ) + c - b T b 4a = a? 2 0 (y i + ? 0 ? 0 + b 2a? 0 ) T (y i + ? 0 ? 0 + b 2a? 0 ) + c - b T b 4a = a? 2 0 F h l=1 y 2 i,l + c - b T b 4a = a? 2 0 ? 2 (F h , ?) + c - b T b 4a where F h is the degree of freedom, ? = ( ?0 ?0 + b 2a?0 ) T ( ?0 ?0 + b 2a?0 ). Then CDF(x) = P(Q(x i ) ? x) = P r(a? 2 0 ? 2 (F h , ?) ? x -c + b T b 4a ) = P r(? 2 (F h , ?) ? 1 a? 2 0 x - c a? 2 0 + b T b 4a 2 ? 2 0 ) = CDF ? 2 (F h ,?) ( 1 a? 2 0 x - c a? 2 0 + b T b 4a 2 ? 2 0 )</formula><p>For j ? C 1 and H i,: , H j,: , we can apply the same computation.</p><p>Suppose ? 2 0 = ? 2 1 = 0, we have a = 0, then Q(</p><formula xml:id="formula_47">x 1 ) = b T x i + c follows a normal distribution Q(x i ) ? N (b T ? 0 + c, ? 2 0 b T b) Then CDF(x) = P(Q(x i ) ? x) = P(b T ? 0 + c + ? 2 0 b T bN (0, 1) ? x) = P(N (0, 1) ? x -(b T ? 0 + c) ? 2 0 b T b ) D A Discussion of (Imbalanced) Prior Distribution Let P(x ? C 0 ) = n0 n1+n0 = p 0 , P(x ? C 1 ) = n1 n1+n0 = p 1 , p 0 + p 1 = 1 and ? = ?0 ?1</formula><p>, which is the ratio of standard deviation and 0 ? ? ? 1, then an estimation of D EKL is</p><formula xml:id="formula_48">D EKL (P, Q) = p 0 D KL (P ||Q) + p 1 D KL (Q||P ) = F h ln ?(p 1 -p 0 ) + F h 2 (p 0 ? 2 + p 1 ? 2 -1) + d 2 E ( p 0 2? 2 1 + p 1 2?<label>2 0</label></formula><p>) where d 2 E = (? 0 -? 1 ) T (? 0 -? 1 ), which is the square Euclidean distance between the means of the two distributions. Thus, the Probabilistic Bayes Error (PBE) is defined as, n 0 CDF ?2 (w0,F h ,?0) (-?) + n  Prior Distribution p 0 , p 1 In Figure <ref type="figure" target="#fig_10">7</ref>, we can observe the impact of class size imbalance on the PBE curves of h and h HP . As the size of the low variation class increases, we can see that the PBE curve of h decreases, and the peak of the curve becomes a plateau. This is because, as the size of the low variation class increases, all nodes become indistinguishable, resulting in the CL Bayes predicting all nodes to be in the same class. Additionally, as the size of the low variation class increases, the HP zone expands and the LP zone shrinks, particularly when h is close to 0. Conversely, as the size of the high variation class increases, the HP zone shrinks and the LP zone expands, particularly when h is close to 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E More Figures of CSBM-H F More Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 Experiments on Synthetic Datasets</head><p>To test how close is the relation between Prop(K X NNGP ) and Prop(K H NNGP ) relates to the performance gain of the aggregation step across different homophily levels, we conduct experiments with the synthetic graphs. This process is similar to the one proposed by <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Generation &amp; Experimental Setup</head><p>We generated a total of 280 graphs to study the impact of edge homophily on the performance of graph-aware models. We used 28 different levels of edge homophily, ranging from 0.005 to 0.95, and generated 10 graphs for each homophily level. Each graph consisted of 5 classes, with 400 nodes in each class. For nodes in each class, we randomly generated 800 intra-class edges and [ 800 Hedge(G) -800] inter-class edges, and assigned features to the nodes using the CiteSeer dataset. We then randomly split the nodes into train/validation/test sets in a 60%/20%/20% ratio. We trained GCN, SGC-1, MLP-2, and MLP-1 models on the synthetic graphs with fine-tuned hyperparameters. For each edge homophily level H edge (G), we computed the average test accuracy of the 4 models, as well as Prop(K NNGP (H)) and Prop(K NNGP (X)). These results were visualized in Figure <ref type="figure" target="#fig_15">10</ref>.      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of intra-and interclass node distinguishability.</figDesc><graphic url="image-1.png" coords="4,345.60,105.02,158.39,115.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Visualization of CSBM-H ? 0 = [-1, 0], ? 1 = [0, 1], ? 2 0 = 1, ? 2 1 = 2, d 0 = 5, d 1 = 5) investigate the impact of homophily on ND through the lens of PBE and ENKL. Specifically, in the standard CSBM-H setting as shown in Figure 2 with ? 0= [-1, 0], ? 1 = [0, 1], ? 2 0 = 1, ? 2 1 = 2, d 0 = 5, d 1 = 5, the curve for LP filtered features h is bell-shaped, indicating that when the homophily value is extremely low or high, the aggregated node embeddings become more distinguishable than at medium levels of homophily. The curve for h HP is monotonically increasing, which means that the high-pass filter works better in heterophily areas than in homophily areas. Moreover, we can see that x, h, and h HP will get the lowest PBE and ENKL in different homophily intervals, which we refer to as the "FP zone (black)", "LP zone (green)", and "HP zone (red)". This indicates that LP filter works better at very low and very high homophily intervals (two ends), HP filter works better at low to medium homophily interval 3 , the original features works betters at medium to high homophily area.</figDesc><graphic url="image-4.png" coords="6,327.69,376.84,110.88,66.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparison of CSBM-H with ? 2 0 = 1, ? 2 1 = 5.</figDesc><graphic url="image-6.png" coords="7,110.49,73.63,95.04,69.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparison of CSBM with different d 0 = 5, d 1 = 25 setups.</figDesc><graphic url="image-14.png" coords="7,110.49,452.99,95.04,69.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comparison of CSBM with different d 0 = 25, d 1 = 5 setups.Increase the Node Degree of High-variation Class (d 0 = 5, d 1 = 25) From Figure4, it can be observed that as the node degree of the high-variation class increases, the PBE and ENKL curves of FP and HP filters almost stay the same while the curves of LP filters go down with a large margin. This leads to a substantial expansion of LP zone and shrinkage of FP and HP zone. This is mainly due to the decrease of ENND of LP filters and the decrease of its NVR in low homophily area also plays an important role.</figDesc><graphic url="image-17.png" coords="7,440.89,453.97,118.80,68.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Demonstration of how HP filter captures the relative center distance.</figDesc><graphic url="image-18.png" coords="9,345.60,80.67,158.39,81.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>( 2 (</head><label>2</label><figDesc>?zv,k?zj,k -t)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>10) ? v -?v? j -?j 2 indicates that it has strong relation to the relative center movement. B Expected Negative KL-divergence Proposition 1. (KL-divergence Between Two Gaussian Distributions) Suppose we have</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>1 1 - 1 D 2 E ( 11 ) 1 )d 2 H ( 12 )</head><label>112111212</label><figDesc>CDF ?2 (w1,F h ,?1) (-?) n 0 + n ESM (x) = F h p 0 where p 0 = n0 n0+n1 , p 1 = n1 n0+n1 , d 2 E = (? 0 -? 1 ) T (? 0 -? 1 ) Aggregated Features H D ESM (h) = F h p 0 ?2Diversified Features H HP D ESM (h HP ) = F h p 0 (? HP 0 ) 2 (? HP 1 ) 2 + p 1 n0 = 500, n1 = 100 (b) n0 = 100, n1 = 500</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Comparison of CSBM with different p 0 , p 1 setups.</figDesc><graphic url="image-19.png" coords="19,110.49,188.64,198.00,144.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Comparison of CSBM-H with ? 2 0 = 1.9, ? 2 1 = 2.</figDesc><graphic url="image-23.png" coords="20,110.49,233.48,198.00,117.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Comparison of CSBM-H with ? 2 0 = 2.5, ? 2 1 = 5.</figDesc><graphic url="image-26.png" coords="20,110.49,566.09,198.00,115.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: H NNGP (H), H NNGP (X) and performance comparison of GCN and MLP-2 and SGC and MLP-1</figDesc><graphic url="image-27.png" coords="21,207.00,72.00,198.00,139.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>Cornell</cell><cell>Wisconsin</cell><cell>Texas</cell><cell>Film</cell><cell>Chameleon</cell><cell>Squirrel</cell><cell>Cora</cell><cell>CiteSeer</cell><cell>PubMed</cell></row><row><cell>Baseline</cell><cell>Edge Homophily</cell><cell>0.5669</cell><cell>0.4480</cell><cell>0.4106</cell><cell>0.3750</cell><cell>0.2795</cell><cell>0.2416</cell><cell>0.8100</cell><cell>0.7362</cell><cell>0.8024</cell></row><row><cell>Homophily</cell><cell>Node Homophily</cell><cell>0.3855</cell><cell>0.1498</cell><cell>0.0968</cell><cell>0.2210</cell><cell>0.2470</cell><cell>0.2156</cell><cell>0.8252</cell><cell>0.7175</cell><cell>0.7924</cell></row><row><cell>Metrics</cell><cell>Class Homophily</cell><cell>0.0468</cell><cell>0.0941</cell><cell>0.0013</cell><cell>0.0110</cell><cell>0.0620</cell><cell>0.0254</cell><cell>0.7657</cell><cell>0.6270</cell><cell>0.6641</cell></row><row><cell></cell><cell>Aggregation Homophily</cell><cell>0.8032</cell><cell>0.7768</cell><cell>0.694</cell><cell>0.6822</cell><cell>0.61</cell><cell>0.3566</cell><cell>0.9904</cell><cell>0.9826</cell><cell>0.9432</cell></row><row><cell></cell><cell>H0 vs H1 p-value</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>1.00</cell></row><row><cell></cell><cell>H0 vs H2 p-value</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell><cell>0.00</cell></row><row><cell>SGC-1 vs MLP-1</cell><cell>ACC SGC ACC MLP-1</cell><cell cols="9">70.98 ? 8.39 70.38 ? 2.85 83.28 ? 5.43 25.26 ? 1.18 64.86 ? 1.81 47.62 ? 1.27 85.12 ? 1.64 79.66 ? 0.75 85.5 ? 0.76 93.77 ? 3.34 93.87 ? 3.33 93.77 ? 3.34 34.53 ? 1.48 45.01 ? 1.58 29.17 ? 1.46 74.3 ? 1.27 75.51 ? 1.35 86.23 ? 0.54</cell></row><row><cell></cell><cell>Diff Acc</cell><cell>-22.79</cell><cell>-23.49</cell><cell>-10.49</cell><cell>-9.27</cell><cell>19.85</cell><cell>18.45</cell><cell>10.82</cell><cell>4.15</cell><cell>-0.73</cell></row><row><cell></cell><cell>H0 vs H1 p-value</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>1.00</cell></row><row><cell></cell><cell>H0 vs H2 p-value</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell><cell>0.00</cell></row><row><cell>GCN vs MLP-2</cell><cell>ACC GCN ACC MLP-2</cell><cell cols="9">82.46 ? 3.11 75.5 ? 2.92 91.30 ? 0.70 93.87 ? 3.33 92.26 ? 0.71 38.58 ? 0.25 46.72 ? 0.46 31.28 ? 0.27 76.44 ? 0.30 76.25 ? 0.28 86.43 ? 0.13 83.11 ? 3.2 35.51 ? 0.99 64.18 ? 2.62 44.76 ? 1.39 87.78 ? 0.96 81.39 ? 1.23 88.9 ? 0.32</cell></row><row><cell></cell><cell>Diff Acc</cell><cell>-8.84</cell><cell>-18.37</cell><cell>-9.15</cell><cell>-3.07</cell><cell>17.46</cell><cell>13.48</cell><cell>11.34</cell><cell>5.14</cell><cell>2.47</cell></row><row><cell>NNGP</cell><cell>H0 vs H1 p-value</cell><cell>1.00</cell><cell>0.99</cell><cell>0.99</cell><cell>1.00</cell><cell>0.00</cell><cell>0.11</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>KPM</cell><cell>H0 vs H2 p-value</cell><cell>0.00</cell><cell>0.01</cell><cell>0.01</cell><cell>0.00</cell><cell>1.00</cell><cell>0.89</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell></row></table><note><p>P-value of the hypothesis testing and model performance comparison 4 Empirical Study of Node Distinguishability</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Property comparisons of homophily metrics</figDesc><table><row><cell>Homophily</cell><cell>Linear or Non-</cell><cell>Feature</cell><cell cols="2">No Test Threshold</cell></row><row><cell>Metrics</cell><cell cols="3">linear Similarity Dependency Labels</cell><cell>Value</cell></row><row><cell>Node</cell><cell>linear</cell><cell></cell><cell></cell></row><row><cell>Edge</cell><cell>linear</cell><cell></cell><cell></cell></row><row><cell>Class</cell><cell>linear</cell><cell></cell><cell></cell></row><row><cell>Aggregation</cell><cell>linear</cell><cell></cell><cell></cell></row><row><cell>Kernel</cell><cell>non-linear</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>This is consistent with the empirical results found in<ref type="bibr" target="#b25">[26]</ref> that the relation between GNN performance and homophily value is a U-shaped curve.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>A gentle introduction to the generalized ? 2 distribution can be found in the appendix C.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>This verifies the conjecture made in<ref type="bibr" target="#b25">[26]</ref> saying that high-pass filter cannot address all kinds of heterophily and only works well for certain heterophily cases.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>To verify this, we increase ? 2 0 and ? 2 1 proportionally. From Figure9in Appendix E, relative sizes of the FP, LP, and HP areas remain similar.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>To test how close is kernel performance metric relates to the performance gain of the aggregation step, we did a synthetic experiments. See details in Appendix F.1</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Steeg</surname></persName>
		</author>
		<author>
			<persName><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On exact computation with an infinitely wide neural net</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Graph convolution for semi-supervised classification: Improved linear separability and out-of-distribution generalization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Baranwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fountoulakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jagannath</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06966</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Relational inductive biases, deep learning, and graph networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00797</idno>
		<title level="m">Beyond low-frequency information in graph convolutional networks</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Exploiting neighbor effect: Conv-agnostic gnns framework for graphs with heterophily</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.11200</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adaptive universal generalized pagerank graph neural network</title>
		<author>
			<persName><forename type="first">E</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Milenkovic</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">I-divergence geometry of probability distributions and minimization problems. The annals of probability</title>
		<author>
			<persName><forename type="first">I</forename><surname>Csisz?r</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<biblScope unit="page" from="146" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Contextual stochastic block models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Montanari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mossel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A probabilistic theory of pattern recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Devroye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gy?rfi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lugosi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Graph structured data viewed through a fourier lens</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Ekambaram</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
		<respStmt>
			<orgName>University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Strong universal consistency of neural network classifiers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Farag?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lugosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1146" to="1151" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Garriga-Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Aitchison</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.05587</idno>
		<title level="m">Deep convolutional networks as shallow gaussian processes</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Graph representation learning</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Artifical Intelligence and Machine Learning</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="159" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno>arXiv, abs/1706.02216</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The elements of statistical learning: data mining, inference, and prediction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bernnet: Learning arbitrary graph spectral filters via bernstein approximation</title>
		<author>
			<persName><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Kernel methods in machine learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">High-order pooling for graph neural networks with tensor decomposition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rabusseau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.11691</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>arXiv, abs/1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Pattern recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Koutroumbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Theodoridis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00165</idno>
		<title level="m">Deep neural networks as gaussian processes</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Qian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.07308</idno>
		<title level="m">Finding global homophily in graph neural networks when meeting heterophily</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hohne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-N</forename><surname>Lim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01404</idno>
		<title level="m">New benchmarks for learning on non-homophilous graphs</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.05641</idno>
		<title level="m">Is heterophily a real nightmare for graph neural networks to do node classification? arXiv preprint</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.07606</idno>
		<title level="m">Revisiting heterophily for graph neural networks</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Break the ceiling: Stronger multi-scale deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02174</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Complete the missing half: Augmenting aggregation filtering with diversification for graph convolutional networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.08844</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.06134</idno>
		<title level="m">Is homophily a necessity for graph neural networks? arXiv preprint</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Maehara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09550</idno>
		<title level="m">Revisiting graph neural networks: All we have is low-pass filters</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G D G</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rowland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.11271</idno>
		<title level="m">Gaussian process behaviour in wide deep neural networks</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Birds of a feather: Homophily in social networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mcpherson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Smith-Lovin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of sociology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="415" to="444" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Foundations of machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05287</idno>
		<title level="m">Geom-gcn: Geometric graph convolutional networks</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Synthetic graph generation to benchmark graph learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tsitsulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rozemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Palowitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.01376</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>arXiv, abs/1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Acmp: Allen-cahn message passing for graph neural networks with particle phase transition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.05437</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.11311</idno>
		<title level="m">Understanding non-linearity in graph neural networks from the bayesian-inference perspective</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H D</forename><surname>Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07153</idno>
		<title level="m">Simplifying graph convolutional networks</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Two sides of the same coin: Heterophily and oversmoothing in graph convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koutra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06462</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lipka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koutra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.13566</idno>
		<title level="m">Graph neural networks with heterophily</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Beyond homophily in graph neural networks: Current limitations and effective designs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
