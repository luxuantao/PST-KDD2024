<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TransO: a knowledge-driven representation learning method with ontology information constraints</title>
				<funder ref="#_NzrgfKX">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
				<funder ref="#_YCjNAEm">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhao</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Liu</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
							<email>wangx@tju.edu.cn</email>
							<idno type="ORCID">0000-0001-9651-0651</idno>
						</author>
						<author>
							<persName><forename type="first">Pengkai</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yuxin</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jianxin</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chengfei</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ziyu</forename><surname>Guan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yinghui</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<affiliation>
								<orgName>1 College of Intelligence and Computing, Tianjin University, </orgName>
								<address><addrLine>Xin Wang Tianjin, China Tianjin, China Yinchuan, China Tianjin, China</addrLine></address>
							</affiliation>
						</author>
						<author>
							<affiliation>
								<orgName> 2 Tianjin Key Laboratory of Cognitive Computing and Application, </orgName>
							</affiliation>
						</author>
						<author>
							<affiliation>
								<orgName> 3 Financial Big Data Laboratory, Bank of Shizuishan, </orgName>
							</affiliation>
						</author>
						<author>
							<affiliation>
								<orgName> 4 Tianjin International Engineering Institute, Tianjin University,</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TransO: a knowledge-driven representation learning method with ontology information constraints</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1007/s11280-022-01016-3</idno>
					<note type="submission">Received: 15 October 2021 / Revised: 18 January 2022 / Accepted: 21 January 2022</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Knowledge graph</term>
					<term>Representation learning</term>
					<term>Ontology information</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Representation learning techniques for knowledge graphs (KGs) are crucial for constructing knowledge-driven decisions in complex network data application scenarios. Most existing methods focus mainly on structured information, ignoring the important value of rich ontology information constraints and complements, however, ontology information is the key for building knowledge-driven decision-making processes. In this paper, we propose a novel ontology information constrained knowledge representation learning model, TransO, which can efficiently model relations explicitly and seamlessly incorporate rich ontology information to improve model performance and maintain low model complexity. Moreover, specific constraint strategies are proposed for entity types, relations, and hierarchical information to effectively implement reasoning and completion of KGs and construct knowledge-driven decisions that are more consistent with the logic of human knowledge in complex network applications. The experimental tasks of link prediction and triple classification are performed on two public datasets. The experimental results demonstrate the effectiveness of our proposed method with better performance than state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Complex networks in the real world are inherently heterogeneous, such as financial transaction networks, social networks, biological networks, and so on. In these networks, there are many different types of nodes and different types of semantic relations <ref type="bibr" target="#b18">[19]</ref>. In recent years, with the rapid development of the field of artificial intelligence (AI), heterogeneous network representation learning technology has played an important role in various machine learning tasks, such as recommendation systems, search engines, knowledge graphs, information retrieval, and so on <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. As the analysis and mining of heterogeneous networks have gradually received attention from researchers, academia and industry have found that the traditional heterogeneous network representation learning methods are insufficient in complex real-world tasks <ref type="bibr" target="#b3">[4]</ref>. The topological structure information and attribute information in complex networks are highly nonlinear and have coupling and interaction complex relations with each other, and it is difficult to make better decisions by capturing only the network structure information <ref type="bibr" target="#b12">[13]</ref>. At the same time, traditional methods generally cannot explicitly model relations between individuals, and cannot effectively embed the rich semantic information in heterogeneous networks. However, the relation and semantic information between individuals is essential for decision-making and applications in the real world <ref type="bibr" target="#b2">[3]</ref>.</p><p>As an important cornerstone of next-generation AI, the powerful representation and reasoning capabilities of KGs enable better direct modeling of coupled and interacting relations <ref type="bibr" target="#b10">[11]</ref>. Furthermore, KG is a general category of heterogeneous networks, and relations between nodes in heterogeneous networks can be represented using KGs <ref type="bibr" target="#b29">[30]</ref>, that is, each edge can be expressed in the form of a triple (head entity, relation, tail entity) <ref type="bibr" target="#b8">[9]</ref>. Therefore, the representation learning techniques of KGs are crucial for building knowledgedriven decisions in complex network data application scenarios. Knowledge representation learning (KRL) aims to learn a function that maps the entities and relations in the KG to a continuous vector space and captures the structural information and semantic information contained in the KG <ref type="bibr" target="#b24">[25]</ref>. Compared with traditional heterogeneous network representation learning, knowledge representation learning has advantages in complex data scenarios. It usually uses parametric algebraic operators to explicitly model relations between edges, and the heterogeneous characteristics of complex networks can also be effectively embedded at the same time. Furthermore, due to the existence of a large number of entities and relation types, knowledge representation learning focuses more on the design of scoring functions based on triples, which is different from the traditional heterogeneous network representation learning that focuses on the design of meta-path or meta-graph <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b33">34]</ref>. In complex application scenarios, the representation learning technology of KGs can help analyze deeper hidden information and make more accurate scientific decisions.</p><p>Knowledge triples are an effective representation of structured data that emphasizes the relationship and semantic information in complex networks. Most knowledge representation learning methods only focus on the representation of structured information in the triples, while ignoring the important ontology information. The rich ontology information embedded in KGs is crucial for enhancing decision-making in complex network data scenarios and is a key for building knowledge-driven decision processes. It is well known that KGs are usually incomplete <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22]</ref>, and ontology information can constrain and supplement the knowledge embedding process to make the learning results of KG representation more accurate. The hierarchical information in ontology information can also help explore more complex and deeper potential relations, which is beneficial for decision-making under complex network data. Moreover, the constraints and complements of ontology information can also reduce unnecessary computation footprint, and improve the efficiency of representation learning. In summary, ontology information has an important constraint and complementary role in the knowledge representation learning process, which can reason about the incompleteness of the KG and construct knowledge-driven decisions that are more consistent with the logic of human knowledge. Therefore, it can be recognized that ontology information constrained knowledge representation learning method is a necessary and challenging research issue for improving decision-making in a complex network. To address this issue, the contributions of this paper can be summarized as follows:</p><p>(1) We propose a novel ontology information constrained knowledge representation learning model, TransO, which can explicitly model relations and seamlessly incorporate rich ontology information in KGs to improve model performance and maintain low model complexity.</p><p>(2) The specific constraint strategies are proposed for entity types, relations, and hierarchical information, respectively, which can effectively achieve reasoning and completion of KGs and construct knowledge-driven decisions that are more consistent with the logic of human knowledge in complex network applications. (3) The experimental tasks of link prediction and triple classification were conducted on two public datasets, and the experimental results have demonstrated the effectiveness of our proposed method with better performance than state-of-the-art methods. Ablation study and parameter analysis experiments have demonstrated that the proposed model can seamlessly incorporate ontology information to enhance performance while maintaining good stability.</p><p>The rest of this paper is organized as follows. Section 2 briefly reviews classical related work on knowledge representation learning. Section 3 describes the details of the proposed knowledge representation learning model. Section 4 focuses on the details of the experimental part, including experimental datasets, baseline methods, parameter settings, evaluation of experimental tasks and analysis of results, ablation study, and parameter analysis. Finally, a brief conclusion of the work is presented in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>In this section, we will introduce the classic representative KRL models. According to the difference of score function, the existing KRL models can be roughly divided into three groups, i.e., Translation distance models ( ), Semantic matching models ( ), and Ontology models ( ). The first group measures the plausibility of a fact as the distance between the two entities, usually after a translation carried out by the relation. The second group uses a similarity-based score function to measure the credibility of facts by matching the latent semantics of the entities and relations contained in the vector space representation. And the third group incorporates more supplementary information, such as entity type and hierarchical structure, to enhance the expressiveness of the translation distance models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Translation distance models</head><p>Translation distance models consider the relation as a translation operation from the head entity to the tail entity and measure the authenticity of the triple by calculating the Euclidean distance between the head entity and the tail entity vector after translation. Among them, TransE <ref type="bibr" target="#b1">[2]</ref>, TransH <ref type="bibr" target="#b23">[24]</ref>, TransR <ref type="bibr" target="#b13">[14]</ref>, and TransD <ref type="bibr" target="#b9">[10]</ref> are the most representative models.</p><p>TransE <ref type="bibr" target="#b1">[2]</ref> is the most representative translation distance model, which represents relations as translations operating on the low-dimensional embedding of the entities. TransE has the advantage of fewer parameters and lower computational complexity and is more suitable for 1-to-1 relations. However, for more complex relations such as 1-to-N, N-to-1, and N-to-N, there are limitations due to its simplistic modeling approach. Subsequently, many improved models based on TransE have emerged to address this problem. To preserve the mapping properties of these complex relations, TransH <ref type="bibr" target="#b23">[24]</ref> addresses the shortcomings of TransE by modeling a relation as a hyperplane with a translation operation that allows each entity to have a different representation under different relations. However, it still constructs entities and relations in the same semantic space, thus limiting the expressive power. TransR <ref type="bibr" target="#b13">[14]</ref> models entities and relations in entity space and multiple relation spaces and transforms them in the corresponding relation spaces, solving the problem of weak representation in the same space. However, it has the defects of more parameters and higher computational complexity, and it is also unreasonable to relate the projection matrix to relations only. TransD <ref type="bibr" target="#b9">[10]</ref> constructs a dynamic mapping matrix for each entity-relation pair to capture the diversity of relations and entities. In addition, it simplifies TransR by decomposing the projection matrix into the product of two vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Semantic matching models</head><p>The semantic matching model utilizes a similarity-based scoring function to measure the plausibility of facts by matching the latent semantics of the entities and relations embodied in the vector space representation. Among them, RESCAL <ref type="bibr" target="#b16">[17]</ref>, DistMult <ref type="bibr" target="#b30">[31]</ref>, HolE <ref type="bibr" target="#b15">[16]</ref>, ComplEx <ref type="bibr" target="#b20">[21]</ref>, and SimplE <ref type="bibr" target="#b11">[12]</ref> are the simplest and widely used models.</p><p>RESCAL <ref type="bibr" target="#b16">[17]</ref> uses vectors and matrices to represent entities and relations, respectively, capturing the internal interactions of the triples through custom scoring functions. However, as the dimension of the relation matrix increases, the computational overhead becomes very high. To solve the computational overhead problem, DistMult <ref type="bibr" target="#b30">[31]</ref> simplifies the relation matrix by restricting it to a diagonal matrix. The oversimplified model can only handle symmetric relations and is not powerful enough for general KGs. HolE <ref type="bibr" target="#b15">[16]</ref> combines the representational ability of RESCAL with the efficiency of DistMult. HolE employs circular correlation to create compositional representations that capture rich interactions while remaining computationally efficient, easy to train, and very scalable. ComplEx <ref type="bibr" target="#b20">[21]</ref> is an extension of DistMult, introducing a vector representation of complex numbers based on DistMult to better model asymmetric relations. The SimplE <ref type="bibr" target="#b11">[12]</ref> is an enhanced version of the Canonical Polyadic tensor decomposition method that allows the two entities corresponding vectors to be learned separately and can encode background knowledge into the embedding using parameter sharing. The SimplE can be viewed as a bilinear model with full expressiveness and few redundant parameters compared to other models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Ontology models</head><p>Translation distance models and semantic matching models provide a good paradigm for knowledge representation learning, but these two types of methods ignore external information such as text descriptions, type constraints, and hierarchical structures. Ontology models can enhance knowledge representation learning performance by integrating semantic information into the knowledge embedding process. SSE <ref type="bibr" target="#b5">[6]</ref> captures the semantic correlation between entities by imposing a smoothness assumption, whereas SSE assumes that an entity can only belong to one type, ignoring the internal connection between types. TransC <ref type="bibr" target="#b14">[15]</ref> models the hierarchical relation between types by constraining the positions of spheres. JOIE <ref type="bibr" target="#b7">[8]</ref> employs both cross-view and intraview models to learn the multiple facets of KGs, so that the learned representation contains the original structural information and the hierarchical associations between entities and the concepts. However, TransC and JOIE only consider types information, i.e., ignoring the rich semantics of relations in ontology. TKRL <ref type="bibr" target="#b25">[26]</ref> regards hierarchical types as projection matrices for entities, and utilizes the type information as relation-specific type constraints. HRS <ref type="bibr" target="#b32">[33]</ref> considers relations in KGs as a three-layer hierarchical relation structure, which clusters similar relations as the top layer, and splits coarse-grained relations into finegrained sub-relations as the bottom layer. TransRHS <ref type="bibr" target="#b31">[32]</ref> trains a vector and a sphere for each relation, and represents the relation hierarchical structure in terms of the relative position of the vector and the sphere.</p><p>Most existing ontology models incorporate single ontology information in the knowledge embedding process to enhance the performance of knowledge representation learning, and cannot seamlessly embed all available ontology information. In the real world, KGs are usually incomplete and single ontology information has a limited effect on complementing the KG. Our proposed model can seamlessly incorporate all available ontology information in the knowledge embedding process to fully complement the KG and improve the decision-making capability in complex scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our approach</head><p>In this section, we introduce several basic definitions separately and then describe in detail the ontology semantic information that is commonly found in KGs, i.e., type information, relation constraint information, and hierarchical structure information. Subsequently, the overall framework and ontology information constraint details of the knowledge-driven ontology information constraints representation learning model TransO are presented. Finally, the training process of the TransO model is described and the model time complexity and space complexity are analyzed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>This subsection will introduce the relevant background knowledge in detail, including knowledge representation learning and knowledge reasoning rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Knowledge representation learning</head><p>Given a symbol G = (E, R, C, T) represents the knowledge graph. Among them, E repre- sents the set of entities, R represents the set of relations, C represents the set of types, and T represents the set of triples, that is,</p><formula xml:id="formula_0">T = {(h, r, t) | h, t ? E, r ? R} ? E ? R ? E . The knowl- edge representation learning aims to design a scoring function { f r (h, t) ? E ? R ? E ? ? }</formula><p>to measure the truth of the triple (h, r, t) in the embedding space. Generally, most existing knowledge representation learning models perform the embedding task using facts from the knowledge graph to maximize the score of the fact triples (h, r, t) ? G , while mini- mizing the score of (h, r, t) ? G drives the resulting vectors to be compatible with these fact triples. Moreover, knowledge representation learning transforms entities and relations into representation vectors , , ? ? k in a low-dimensional continuous embedding vector space, where k is the dimension of the embedding vector and k ? |E|.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Knowledge reasoning</head><p>The rich semantic information in the knowledge graph can be standardized by RDFS in terms of ontology information, specifically entity types information rdf:type, relation constraints information rdfs:domain and rdfs:range. Hierarchical structure information, respectively, hierarchical type information rdfs:subClassOf and hierarchical relation information rdfs:subPropertyOf. Ontology information has important constraints and complements to the knowledge representation learning process, allowing basic knowledge reasoning and completion to improve decision-making in complex network scenarios. Specifically, we set e, h, t ? E as entity meta-variables, r, r s , r p ? R as relation meta- variables, and c, c eh , c et , c s , c p ? C as type meta-variables in the knowledge graph. The con- straints and complements of ontology information to the knowledge representation process can be standardly formalized by the following rules as:</p><p>-Relation information has constraints for head entity types and tail entity types.</p><p>-Hierarchical information can be very effective in complementing the potential information in the KG.</p><p>After the ontology information is constrained and supplemented, the KG is generally more complete than the original one. The ontology information constraints and supplementary rules enable basic knowledge reasoning, which can mine deeper potential information and make more logical decisions in complex network data scenarios. Ontology information constraints and supplementary information can construct the basic reasoning rules, and four basic simple reasoning rules are given in this paper:</p><formula xml:id="formula_1">(h, r, t) ? (r, , c eh ) ? (h, , c eh ) (h, r, t) ? (r, , c et ) ? (t, , c et ) (c, , c s ) ? (c s , , c p ) ? (c, , c p ) (r, , r s ) ? (r s , , r p ) ? (r, , r p ) (e, , c) ? (c, , c p ) ? (e, , c p ) (h, r, t) ? (r, , r p ) ? (h, r p , t) (h, r, t) ? (r, , r p ) ? (r p , , c) ? (h, , c) (h, r, t) ? (r, , r p ) ? (r p , , c) ? (t, , c)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Overall framework</head><p>Knowledge graph representation learning is a key technique to improve more efficient and accurate decision-making in complex data scenarios, so we propose a novel knowledgedriven ontology information constrained representation learning model TransO. The translation distance model has been proven to be efficient and accurate idea <ref type="bibr" target="#b1">[2]</ref>, so the training objective of TransO model is + ? . The scoring function of the overall framework of the TransO model consists of two parts: the basic triple scoring function and the ontology information constrained triple scoring function. The basic triple scoring function is:</p><p>Let the entity projection matrix be , and the head and tail entities after the ontology information constraints are where the head entity projection matrix ? , and the tail entity projection matrix ? . Let the relation projection matrix be , and the relations after the ontology information constraints are After the setting of the projection matrix, the other part of the ontology information constrained triple scoring function is:</p><formula xml:id="formula_2">(1) f (1) (h, r, t) = ? + -? 1?2 (2) = , =<label>(3)</label></formula><formula xml:id="formula_3">= (4) f (2) (h, r, t) = ? ? + -? ?1?2</formula><p>Fig. <ref type="figure">1</ref> The overall framework diagram of the TransO model</p><p>The overall framework diagram of the TransO model is shown in Figure <ref type="figure">1</ref>. Specifically, the TransO model maps the general vector representation of entities by the entity projection matrix so that their vectors are scaled in each dimension, thus achieving different properties for the same entity represented on different relations. In addition, the TransO model introduces a projection matrix for each relation, which is used to keep the general representations of the entities and the corresponding vectors of the relations to satisfy + ? . It is worth stating that if only the entity vectors are pro- jected without any treatment of the vector representation of the relations, it will slow down the convergence of the whole model. Moreover, ontology information constraints will manipulate and process the projection matrix, and the introduction of the projection matrix helps to embed ontology information to improve the model representation learning capability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ontology information constraints</head><p>Ontology information constraints are mainly divided into relation and type constraints, and hierarchical structure information constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Relation and type constraints</head><p>It is well known that entities have rich type information, and considering entity type information in the knowledge embedding process to supplement the modeling process will be beneficial to the decision-making in complex scenarios. The entity projection matrix is constructed using the type projection matrix , so as to realize the knowledge representation of embedded entity type ontology information ? , as follows:</p><p>For real-world triple data, specific relations constrain the semantic information of entity type rdf:type, which is rdfs:domain and rdfs:range in the ontology information. There are many types of entity William Shakespeare, such as Poet, Writer, and Award Nominee, as shown in Figure <ref type="figure" target="#fig_0">2</ref>. In the triple (William Shakespeare, works written, Romeo and Juiet), obviously, the entity William Shakespeare type Writer is more related to this triple, that is, rdfs:domain should constrain the entity William Shakespeare type mainly represents the characteristics and attributes of Writer. Similarly, in the triple (Oscar, award, William Shakespeare), the Award Nominee type of William Shakespeare is more related to this triple. In other words, rdfs:range should constrain the type of entity William Shakespeare mainly to be Award Nominee.</p><p>As shown in Figure <ref type="figure">3</ref>, we process and analyze the widely used public real-world knowledge graph data FB15K <ref type="bibr" target="#b25">[26]</ref>, which reveals a valuable conclusion: although each entity has many types, the constraining effect of rdfs:domain and rdfs:range makes  <ref type="figure">3</ref> The frequency density histogram of explicit entity types for the public real-world dataset FB15K. Among them, 89.51% of the head entities have more than 40% implicit types, and 91.59% of the tail entities have more than 40% implicit types many entity types not appear. The ontology information constraint of rdfs:domain and rdfs:range makes basically all entities have a certain number of implicit types with no information value. For example, about 90% of the triples in FB15K have more than 40% implicit entity types. If we remove these useless implicit types when building and training the model, and keep only the explicit types that can complement the knowledge embedding, we can not only reduce the noise interference of useless information but also greatly reduce the model computation time and memory consumption.</p><p>In the knowledge embedding process, incorporating ontology information rdfs:domain and rdfs:range constraints will greatly improve the efficiency and accuracy of the decision-making process in complex heterogeneous data scenarios. More often, we consider that there is a difference in the importance of entity types exhibited in different data scenarios. Therefore, in this paper, we define an importance weight to implement the role of ontology information rdfs:domain and rdfs:range to constrain entity types:</p><p>where count(?) is a counting function to calculate the frequency of entity type occurrences. It is worth stating that this importance weight is more flexible and can also be calculated using other functions, such as the attention mechanism. In this paper, the counting function is used to satisfy a certain logic and at the same time facilitate the calculation.</p><p>The constraints on the ontology information rdfs:domain are achieved through the head entity type importance weight operation type projection matrix .</p><p>where is the projection matrix corresponding to the i-th type of the entity, and c rh denotes the set of types that the head entity can only represent because of the constraints of the ontology information rdfs:domain.</p><p>Similarly, the constraints on the ontology information rdfs:range are achieved through the tail entity type importance weight operation type projection matrix .</p><p>where is the projection matrix corresponding to the i-th type of the entity, and c rt denotes the set of types that the tail entity can only represent because of the constraints of the ontology information rdfs:range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Hierarchical structure constraints</head><p>Drawing on the two hierarchical type encoder ideas of Recursive Hierarchy Encoder and Weighted Hierarchy Encoder in the TKRL model <ref type="bibr" target="#b25">[26]</ref>, we designed hierarchical encoding strategies for hierarchical types rdfs:subClassOf constraints and hierarchical relations rdfs:subPropertyOf constraints together into the knowledge embedding model. ( <ref type="formula">6</ref>)</p><formula xml:id="formula_4">c i = i ? (c i ) ? n i=1 i ? (c i ) , i = ? 1, c i ? c e 0, c i ? c e (7) = n ? i=1 c i , c e = c rh (8) = n ? i=1 c i , c e = c rt</formula><p>Entity types have a different hierarchical structure that can be constrained by the ontology information rdfs:subClassOf, as follows:</p><p>where m is the number of layers for type c in the hierarchical structure, (i) is the projection matrix of the i-th sub-type c (i) .</p><p>In the KG, relations also have a hierarchical structure that can be constrained by the ontology information rdfs:subPropertyOf, and relations with different hierarchical depths should have different importance, as follows:</p><p>where m is the number of layers for relation r in the hierarchical structure, ( ) is the projection matrix of the i-th sub-relation r (i) , and i is the corresponding weight of r (i) . We design a proportional-declined weighting strategy between r (i) and r (i+1) as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Let</head><p>? m i=1 i = 1 and ? (0, 0.5) . The strategy indicates that the more precise and deeper sub-relation r (i) will receive a higher weight value i and play a greater role in influencing , which is also more consistent with the logic of human knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model training</head><p>The training process of TransO model requires the participation of negative examples, given a positive example triple (h, r, t) ? . Then (h ? , r, t ? ) ? ? denotes the negative exam- ple triple. Accordingly, f(h, r, t) is used to denote the score of the scoring function for the positive triple, and f (h ? , r, t ? ) is used to denote the score of the scoring function for the negative triple. The negative triple is specifically defined as</p><p>The first part of the TransO model loss function is the basic triple loss function as</p><p>The second part of the TransO model loss function is the ontology information constrained triple loss function as</p><p>The total loss function of the TransO model is the sum of the two parts of the loss function of the basic triple and the ontology information constrained triple, as follows:</p><formula xml:id="formula_5">(9) = m ? i=1 (i) = (1) (2) ? ? ? (m) (10) = m ? i=1 i (i) = 1 (1) + 2 (2) + ? + m (m) (11) i ? i+1 = (1 -) ? (12) ? = (h ? , r, t) | h ? ? E ? (h, r, t ? ) | t ? ? E (13) L 1 = ? (h,r,t)? ? (h ? ,r,t ? )? ? [ 1 + f (1) (h, r, t) -f (1) (h ? , r, t ? )] + (14) L 2 = ? (h,r,t)? ? (h ? ,r,t ? )? ? [ 2 + f (2) (h, r, t) -f (2) (h ? , r, t ? )] + (15) L = L 1 + L 2</formula><p>The training process of the TransO model is shown in Algorithm 1. The inputs of TransO include the training set , the entity set E, the relation set R, and the type set C. Besides, some hyperparameters should be given, i.e., the dimension of embedding d, the iteration times of the training process N, the learning rate , and two margin values 1 and 2 . As shown in the formula ( <ref type="formula">13</ref>) and ( <ref type="formula">14</ref>), the whole model is trained based on margin loss, which keeps the negative sample away from the positive sample great than 1 or 2 . In the training process of TransO, the vectors of entities and relations are first initialized using the normal distribution N 0, 1 d . And and are initialized via the normal distribution N 1, 1 d . Then the N epoch process of the model is entered. Before each epoch, the vectors of entities and relations will be normalized. batch, the sample subset batch is first obtained by randomly sampling from the training set . For each positive triple in , a negative triple is generated by replacing the head entity or the tail entity, and the negative triple will be added to the corrupted triple set ? . Then the loss functions ( <ref type="formula">13</ref>) and ( <ref type="formula">14</ref>) corresponding to batch and ? batch will be optimized by mini-batch gradient descent (mini-batch GD). The above process is repeated N times, and the training process of TransO is completed. The TransO model introduces a diagonal matrix for each type and each relation, while the depth of the hierarchical structure is usually much smaller than the other parameters of the model, so the space complexity of the TransO model can be equated to O(nd + md + ld) , where n denotes the number of entities, m denotes the number of relations, and l denotes the number of types. In addition, the entities and relations are still represented by a d-dimensional vector, and the projection matrix is diagonal, so there is no increase in the time complexity of the model during the computation, and the time complexity of the TransO model can be equated to O(d) . The difference between TransO and TransE in terms of time complexity and space complexity is small, incorporating ontology information into knowledge representation learning at the cost of lower Spatio-temporal complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Complexity analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we compare the performance of the link prediction task and triple classification task with several state-of-the-art knowledge representation learning methods on two public benchmark datasets to validate the effectiveness of our proposed method. In addition, ablation study and parameter analysis experiments verify the stability of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>In this research, we evaluate our proposed model on two publicly available benchmark datasets, FB15K <ref type="bibr" target="#b25">[26]</ref> and Sport <ref type="bibr" target="#b31">[32]</ref>, which are constructed from Freebase <ref type="bibr" target="#b0">[1]</ref> and NELL <ref type="bibr" target="#b22">[23]</ref>, respectively. It should be explained that the FB15K and Sport datasets contain important ontology information needed for our proposed model, while FB15K does not have rdfs:subClassOf (sc), rdfs:subPropertyOf (sp) ontology information. The detailed statistical information of the dataset is presented in Table <ref type="table" target="#tab_2">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Baseline methods</head><p>A total of 12 state-of-the-art KRL models are used as baseline methods to fully demonstrate the effectiveness of our proposed models. These comparison methods are classified into three major categories: Translation distance models , Semantic matching models , and Ontology models . Specifically, the Translation distance models include TransE <ref type="bibr" target="#b1">[2]</ref>, TransH <ref type="bibr" target="#b23">[24]</ref>, TransR <ref type="bibr" target="#b13">[14]</ref>, and TransD <ref type="bibr" target="#b9">[10]</ref>; the Semantic matching models include RESCAL <ref type="bibr" target="#b16">[17]</ref>, DistMult <ref type="bibr" target="#b30">[31]</ref>, HolE <ref type="bibr" target="#b15">[16]</ref>, ComplEx <ref type="bibr" target="#b20">[21]</ref>, and SimplE <ref type="bibr" target="#b11">[12]</ref>; the Ontology models include TKRL <ref type="bibr" target="#b25">[26]</ref>, TransC <ref type="bibr" target="#b14">[15]</ref>, and TransRHS <ref type="bibr" target="#b31">[32]</ref>. It is worth stating that our proposed model belongs to Ontology model.</p><p>For fair model comparison, all models are trained at the same embedding dimension d = 100 . All experimental results are implemented by the open-source tool OpenKE <ref type="bibr" target="#b6">[7]</ref> using default parameters, and some of the experimental results are obtained from published papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Experimental parameter settings</head><p>We trained the proposed model with mini-batch GD. For both datasets, the embedding dimension d is tuned from {50, 100, 150, 200} and the margin 2 is tuned from {0.25, 0.5, 0.75, 1.0} . Besides, the batch size b is set to be | |?m . For the Sport dataset, the margin 1 is tuned from {0.5, 1.0, 1.5, 2.0} and TransO is trained with a learning rate = 0.005 , epoch size N = 500 , and mini-batch m = 10 . For the FB15K dataset, the margin It is well known that the training process of the KRL model needs to generate negative triples from positive triples to accelerate the convergence of the model, so negative sample collection is essential in this paper. Currently, there are two mainstream strategies to generate negative triples, namely "unif" and "bern". The "unif" strategy replaces the head entity or the tail entity randomly, meaning that the head and tail entities are replaced with equal probability. In this process, a negative triple is created by replacing the head entity (tail entity) of a positive triple with an arbitrary entity in the entity set. The current knowledge base is generally incomplete, so the triples obtained by the "unif" strategy are likely to be positive triples, which means that false negative triples are introduced into the training process. The "bern" strategy replaces the head entity or the tail entity using different probabilities depending on the mapping properties of the relation, i.e., 1-to-N, N-to-1, and N-to-N. In short, the "bern" strategy prefers to replace the head entity if the relation is 1-to-N; the "bern" strategy prefers to replace the tail entity if the relation is N-to-1. With the "bern" strategy, the probability of generating false negative triples is reduced. Our proposed model and all baseline methods performed experiments using the two mainstream strategies separately, and the experimental results of all baseline methods show only the best experimental results of the two strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Link prediction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Evaluation</head><p>Link Prediction aims to predict the missing h or t for a data triple (h, r, t), i.e., predict t given (h, r) or predict h given (r, t). Rather than requiring one best answer, this task emphasizes more on ranking a set of candidate entities from the KG. For each test triple (h, r, t) ? T , we remove the head or tail entity and replace it by every entity e ? E in a KG, and rank these entities in ascending order according to the score calculated by scoring function f (1) (h, r, t) + f (2) (h, r, t) . Let rank h (h, r, t) be the ranking of (h, r, t) among all head entity replacements, and rank t (h, r, t) denotes a similar ranking tail entity replace- ments. Two mainstream evaluation metrics are used by us to assess the performance of the link prediction task, namely, the mean reciprocal rank of all correct entities (MRR) and the proportion of correct entities that rank no larger than N (Hits@N). Specifically, MRR is the mean of the reciprocal rank:</p><p>Hits@N measures the proportion of triples in T that rank among the top t after corrupting both head entities and tail entities.</p><p>It should be noted that when we construct corrupted triplets, some of the constructed triples happen to be in the original knowledge graph, and the scores of such triples are naturally ranked in the top position. However, this affects the accuracy of the evaluation of model effectiveness, so we can consider eliminating such corrupted triplets from the training sets, validation sets, and testing sets to ensure fairness of the evaluation. The evaluation without the above filtering operation is called the "raw" setting, and the evaluation with the filtering operation is called the "filter" setting. In this paper, the MRR metric performs experiments for both settings, and the Hits@N metric takes the best experimental result of the two settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Results</head><p>The experimental results are shown in Table <ref type="table" target="#tab_3">3</ref>. TransO achieved optimal results on both the FB15K dataset and the Sport dataset, which fully demonstrated the effectiveness of the proposed model. On the FB15K dataset, TransO improves the MRR metric by 2.12% on average and the Hits@N metric by 2.19% on average over the optimal baseline; on the Sport dataset, TransO improves the MRR metric by 3.20% on average and the Hits@N metric by 3.92% on average over the optimal baseline. Compared with the experimental results of the FB15K dataset, it can be seen that TransO not only has better performance on the Sport dataset but also has a more significant performance improvement than the optimal baseline. Analyzing from the perspective of the dataset, the FB15K dataset cannot construct the projection</p><formula xml:id="formula_6">(16) MRR = 1 2 * |T| ? (h,r,t)?T 1 rank h (h, r, t) + 1 rank t (h, r, t)</formula><p>matrix of entities and relations well due to the lack of two important ontology information of subClassOf and subPropertyOf, which causes a part of semantic information loss. In contrast, the Sport dataset has complete semantic information, which can help TransO accurately construct the projection matrix, supplement more ontology information constraints, so it achieves a better performance improvement.</p><p>Compared with the and models, the models achieved better experimental results overall. The overall performance advantage of the models on the FB15K dataset is not as obvious as that on the Sport dataset, which is still because the Sport dataset contains richer ontology information than the FB15K dataset. The richer ontology information complements the information of the models more adequately, so the model enhancement effect is more obvious. Moreover, TransO can seamlessly incorporate more ontology information than other models, so it still achieves optimal results among the models with superior overall performance. All these experimental results demonstrate the high value of seamlessly incorporating ontology information. It also shows that more ontology information constraints can better enable reasoning to complement incomplete knowledge graphs to enhance the performance of knowledge representation. Besides, it can be seen from the experimental results that the "bern" strategy is generally better than the "unif" strategy; the "filter" setting is usually superior to the "raw" setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Triple classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Evaluation</head><p>Triple classification is a binary classification task to determine whether a triple (h, r, t) is correct. Negative triples are required for the evaluation of triple classification. Hence, we construct some negative triples following the same setting in <ref type="bibr" target="#b19">[20]</ref>. There are as many true triples as negative triples in both valid and test sets. The classification strategy is conducted as follows: we set different relation-specific thresholds r for each relation. For a triple (h, r, t), if the score f(h, r, t) is below r , the triple is then predicted to be positive and otherwise negative. The optimal threshold r of each relation can be obtained by maximizing the classification accuracy on the valid sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Results</head><p>As can be seen from Table <ref type="table" target="#tab_4">4</ref>, TransO achieved the best experimental results in both FB15K and Sport, with an improvement of 0.54% and 1.43% over the optimal baseline, respectively. It seems that the enhancement of TransO on FB15K is not as great as it is on the Sport dataset. From the perspective of data, the lack of two important ontology information, subClassOf and subPropertyOf, leads to a situation similar to the link prediction task, which can not help TransO construct the projection matrix accurately. In contrast, the richer ontology information complements the semantic needed for TransO more adequately, so the effect on Sport is more obvious. On the whole, the models generally perform better than the models and models. The experimental results illustrate that ontology information can be used to infer facts that do not exist in the knowledge graph, thus providing a powerful reasoning capability for the knowledge graph. Furthermore, among the models, our proposed model TransO achieves better performance than the other models, indicating that TransO can seamlessly integrate more ontology information to improve performance. The experimental results show that the "bern" strategy is more reasonable than the "unif" strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation study</head><p>In this subsection, we conduct an ablation study on the Sport dataset and the FB15K dataset to determine the importance of each component in TransO. On the Sport Evaluating the results on the two datasets will help to ascertain the individual and cumulative effect of each ontology information in this paper. The specific results are shown in Figure <ref type="figure" target="#fig_2">4</ref>. The experimental results from the Sport dataset show that removing any of the ontology information leads to performance degradation. Removing the type constraints and hierarchical type ontology information reduces the MRR metric by 3.78% and the Hits@N metric by 3.17% on average. Removing hierarchical relation ontology information reduces the MRR metric by 2.27% and the Hits@N metric by an average of 3.83%. Removing all ontology information, the MRR metric is reduced by 18.41% and the Hits@N metric is reduced by 13.92% on average. The experimental results from the FB15K dataset show that removing the ontology information reduces the MRR metric by 3.15% and the Hits@N metric by 4.54% on average. All of these experimental results validate the importance of ontology information, and the seamless incorporation of ontology information can enhance the performance of knowledge representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Parameter analysis</head><p>In this subsection, we analyze the effect of important hyperparameters in the proposed model. The specific hyperparameters studied are: the number of epochs N, the embedding dimension d, the margin value 1 of L 1 and the margin value 2 of L 2 . All parameter analyses are performed on the link prediction task on the Sport dataset and the FB15K dataset, respectively, with MRR and Hits@N as the performance evaluation metrics of the model.</p><p>The experimental results of the analysis of epoch number N are shown in Figure <ref type="figure">5</ref>. From the results of the evaluation metrics MRR and Hits@N, it can be seen that the proposed model TransO improves with increasing N on the Sport dataset and basically remains stable after about 200 iterations; it improves with increasing N on the FB15K dataset and basically remains stable after about 400 iterations. The experimental results of parameter analysis on both datasets show that the TransO model does not show any obvious overfitting phenomenon after reaching the optimum with increasing iterations, which indicates that the proposed model has good robustness.</p><p>The experimental results of dimensional analysis are shown in Figure <ref type="figure">6</ref>, which show the sensitivity of the proposed model to the parameter of representation learning dimension. From the experimental results of the Sport dataset, the embedding dimension d of the proposed model has little effect on the Hits@N metric and has a slight influence on the MRR metric; from the experimental results of the FB15K dataset, the performance of The L 1 and L 2 of the proposed model can be well trained by the margin-based loss function method. This method has some characteristics that need to be explained: if the margin is too large, the loss of the model will be large and the loss will be difficult to converge to 0, even leading to non-convergence, but it can distinguish similar samples with more confidence. If the margin is too small, the model is well trained and the loss can easily converge to 0, but it is more difficult to distinguish between more similar samples. The Sport dataset has less entity and type information, so there are more similar samples, which need to set a larger margin. At the same time, the small size of the Sport dataset makes it possible to fit quickly even if a large margin is set. In contrast, the FB15K dataset has more entity and type information, each entity has multiple types, and the entities are more evenly distributed. Setting a smaller margin can better distinguish the similar samples and at the same time can reduce the training difficulty. It is worth explaining that our proposed model can distinguish entities well from entities of the same type after the type projection matrix, which reflects the advantages of the proposed model. We selected the optimal parameter settings from the range of important parameters of the proposed model TransO after performance analysis of important parameters on the Sport dataset and the FB15K dataset. Specifically, as shown in Table <ref type="table" target="#tab_5">5</ref>, the optimal parameter settings for the Sport dataset are: N = 500, d = 100, 1 = 2, 2 = 0.5 ; the optimal parameter settings for the FB15K dataset are: N = 500, d = 100, 1 = 0.25, 2 = 0.25.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Knowledge graphs, as the latest achievement of symbolism, have become an important cornerstone of next-generation artificial intelligence because of their powerful representation and reasoning capabilities with rich ontology information. Studying the representation learning techniques of KGs has important academic value and industrial application prospects for constructing knowledge-driven decisions in complex network data application scenarios. In this paper, we propose a novel knowledge representation learning method, TransO, which can seamlessly embed ontology information to enhance model performance. More, different encoding strategies are designed for entity types, relations, and hierarchical information, which can effectively achieve knowledge reasoning and completion to construct knowledge-driven decisions that are more consistent with the logic of human knowledge. The experimental results on public datasets have fully demonstrated the superiority of our proposed model with better performance than the state-of-the-art methods. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2</head><label>2</label><figDesc>Fig. 2 Schematic diagram of the connection between entity types and relations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1</head><label></label><figDesc>is tuned from {0.25, 0.5, 0.75, 1.0} and TransO is trained with a learning rate = 0.01 , epoch size N = 500 , and mini-batch m = 100 . The optimal parameter settings for the Sport dataset: {N = 500, d = 100, = 0.005, m = 10, 1 = 2, 2 = 0.5} ; the optimal parameter set- tings for the FB15K dataset: {N = 500, d = 100, = 0.01, m = 100, 1 = 0.25, 2 = 0.25} . These parameters are used by both link prediction and triple classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4</head><label>4</label><figDesc>Fig. 4 Study on the Sport dataset and the FB15K dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 Fig. 6</head><label>56</label><figDesc>Fig. 5 Results of different epochs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7</head><label>7</label><figDesc>Fig. 7 Results of different margins 1 and 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-3.png" coords="12,49.59,80.58,340.20,279.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>compares the classical knowledge representation learning model with the TransO model proposed in this paper in terms of time complexity and space complexity.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>Comparison of the complexity of TransO model and other knowledge representation models</figDesc><table><row><cell>Model</cell><cell>Space Complexity</cell><cell>Time Complexity</cell></row><row><cell>TransE [2]</cell><cell>O(nd + md)</cell><cell>O(d)</cell></row><row><cell>TransH [24]</cell><cell>O(nd + md)</cell><cell>O(d)</cell></row><row><cell>TransR [14]</cell><cell>O(nd + mdk)</cell><cell>O(dk)</cell></row><row><cell>TransD [10]</cell><cell>O(nd + mk)</cell><cell>O(max(d, k))</cell></row><row><cell>TransM [5]</cell><cell>O(nd + mdk)</cell><cell>O(d)</cell></row><row><cell>RESCAL [17]</cell><cell>O(nd + md 2 )</cell><cell>O(d 2 )</cell></row><row><cell>DistMult [31]</cell><cell>O(nd + md)</cell><cell>O(d)</cell></row><row><cell>HolE [16]</cell><cell>O(nd + md)</cell><cell>O(d log d)</cell></row><row><cell>ComplEx [21]</cell><cell>O(nd + md)</cell><cell>O(d)</cell></row><row><cell>SimplE [12]</cell><cell>O(nd + md)</cell><cell>O(d)</cell></row><row><cell>TransRHS [32]</cell><cell>O(nd + md)</cell><cell>O(d)</cell></row><row><cell>TransO</cell><cell>O(nd + md + ld)</cell><cell>O(d)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>Statistics of datasets</figDesc><table><row><cell cols="2">Dataset Rel</cell><cell>Ent</cell><cell cols="2">Con Train</cell><cell>Valid</cell><cell>Test</cell><cell>type</cell><cell>sc</cell><cell cols="3">sp domain range</cell></row><row><cell cols="4">FB15K 1,345 14,951 571</cell><cell cols="5">483,142 50,000 59,071 66,242 -</cell><cell>-</cell><cell>1,345</cell><cell>1,345</cell></row><row><cell>Sport</cell><cell>4</cell><cell>1,039</cell><cell>44</cell><cell>1,349</cell><cell>358</cell><cell>358</cell><cell>1,039</cell><cell cols="2">42 4</cell><cell>4</cell><cell>4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>Evaluation results on link prediction. , , and correspond to Translation distance models, Semantic matching models, and Ontology models, respectivelyThe best results are in bold and the second-best results are underlined</figDesc><table><row><cell>M</cell><cell>FB15K</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Sport</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>MRR</cell><cell></cell><cell cols="2">Hits@N(%)</cell><cell></cell><cell>MRR</cell><cell></cell><cell cols="2">Hits@N(%)</cell><cell></cell></row><row><cell>Model</cell><cell>Filter</cell><cell>Raw</cell><cell>1</cell><cell>3</cell><cell>10</cell><cell>Filter</cell><cell>Raw</cell><cell>1</cell><cell>3</cell><cell>10</cell></row><row><cell>TransE</cell><cell cols="2">0.463 0.222</cell><cell cols="8">29.7 57.8 74.9 0.644 0.417 53.3 73.8 80.0</cell></row><row><cell>TransH</cell><cell>0.416</cell><cell cols="9">0.255 28.7 49.2 64.4 0.540 0.368 40.2 65.5 73.3</cell></row><row><cell>TransR</cell><cell cols="2">0.346 0.198</cell><cell cols="8">21.8 40.4 58.2 0.423 0.304 28.0 52.6 65.6</cell></row><row><cell>TransD</cell><cell cols="2">0.405 0.251</cell><cell cols="2">21.5 47.9</cell><cell cols="6">77.3 0.443 0.319 27.2 57.6 69.5</cell></row><row><cell>RESCAL</cell><cell cols="2">0.354 0.189</cell><cell cols="6">23.5 40.9 58.7 0.071 0.058 4.0</cell><cell>7.9</cell><cell>12.2</cell></row><row><cell>DistMult</cell><cell cols="2">0.158 0.114</cell><cell>8.3</cell><cell cols="7">16.5 32.0 0.463 0.341 39.8 49.1 58.3</cell></row><row><cell>HolE</cell><cell cols="2">0.348 0.196</cell><cell cols="8">27.9 45.5 60.4 0.269 0.184 22.4 26.8 35.4</cell></row><row><cell>ComplEx</cell><cell cols="2">0.281 0.192</cell><cell cols="8">16.9 32.2 50.9 0.511 0.365 43.0 55.1 66.2</cell></row><row><cell>SimplE</cell><cell cols="2">0.360 0.189</cell><cell cols="8">27.6 47.3 63.8 0.495 0.380 42.7 52.0 62.8</cell></row><row><cell>TKRL</cell><cell cols="2">0.401 0.223</cell><cell cols="8">20.1 52.5 74.3 0.676 0.430 59.8 71.5 77.3</cell></row><row><cell>TransC</cell><cell cols="2">0.405 0.216</cell><cell cols="8">21.9 54.4 70.7 0.682 0.446 61.7 73.3 79.1</cell></row><row><cell>TransRHS</cell><cell cols="2">0.469 0.236</cell><cell cols="8">30.2 58.2 75.2 0.781 0.483 71.8 78.4 80.1</cell></row><row><cell>TransO(unif)</cell><cell cols="2">0.432 0.238</cell><cell cols="8">29.2 51.9 68.4 0.592 0.388 43.7 73.5 82.8</cell></row><row><cell cols="3">TransO(bern) 0.476 0.262</cell><cell cols="8">31.0 59.5 78.6 0.797 0.504 73.0 81.6 84.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>Evaluation results on triple classification. , , and</cell><cell>M</cell><cell>Accuracy(%)</cell><cell></cell></row><row><cell>correspond to Translation distance models, Semantic</cell><cell>Model</cell><cell>FB15K</cell><cell>Sport</cell></row><row><cell>matching models, and Ontology models, respectively. The best results are in bold and the second-best results are</cell><cell>TransE TransH TransR</cell><cell>79.8 79.9 82.1</cell><cell>89.2 86.8 85.6</cell></row><row><cell>underlined</cell><cell>TransD</cell><cell>88.0</cell><cell>86.1</cell></row><row><cell></cell><cell>RESCAL</cell><cell>73.1</cell><cell>57.8</cell></row><row><cell></cell><cell>DistMult</cell><cell>84.9</cell><cell>85.7</cell></row><row><cell></cell><cell>HolE</cell><cell>77.4</cell><cell>77.3</cell></row><row><cell></cell><cell>ComplEx</cell><cell>85.5</cell><cell>86.3</cell></row><row><cell></cell><cell>SimplE</cell><cell>85.2</cell><cell>86.5</cell></row><row><cell></cell><cell>TKRL</cell><cell>89.5</cell><cell>90.7</cell></row><row><cell></cell><cell>TransC</cell><cell>92.7</cell><cell>89.1</cell></row><row><cell></cell><cell>TransRHS</cell><cell>79.8</cell><cell>90.9</cell></row><row><cell></cell><cell>TransO(unif)</cell><cell>89.3</cell><cell>91.8</cell></row><row><cell></cell><cell>TransO(bern)</cell><cell>93.2</cell><cell>92.2</cell></row></table><note><p>dataset, we design four versions of TransO for analysis. The version information is as follows: (1) TransO-L 1 : a base version that does not incorporate any ontology information. (2) TransO-L 1 + Type : a version that incorporates some ontology information without considering hierarchical relation structure ontology information. (3) TransO-L 1 + Relation : a version that incorporates some ontology informa- tion without considering type constraints and hierarchical type ontology information. (4) TransO-L 1 + L 2 : a version that incorporates all ontology information, i.e., the final version of TransO. On the FB15K dataset, only two sets of experiments were designed because of the lack of hierarchical type information and hierarchical relation information: (1) TransO-L 1 : a base version that does not incorporate any ontology information. (2) TransO-L 1 + L 2 : a version that incorporates rdf:type, rdfs:domain, and rdfs:range ontology information versions, i.e., all ontology information of the dataset, is the final version of TransO.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc>Parameter scope on the Sport dataset and the FB15K dataset</figDesc><table><row><cell></cell><cell>Sport</cell><cell></cell><cell>FB15K</cell><cell></cell></row><row><cell>P</cell><cell>Scope</cell><cell>Best</cell><cell>Scope</cell><cell>Best</cell></row><row><cell>N</cell><cell>{0 -500}</cell><cell>500</cell><cell>{0 -500}</cell><cell>500</cell></row><row><cell>d</cell><cell>{50, 100, 150, 200}</cell><cell>100</cell><cell>{50, 100, 150, 200}</cell><cell>100</cell></row><row><cell>1</cell><cell>{0.5, 1.0, 1.5, 2.0}</cell><cell>2</cell><cell>{0.25, 0.5, 0.75, 1.0}</cell><cell>0.25</cell></row><row><cell>2</cell><cell>{0.25, 0.5, 0.75, 1.0}</cell><cell>0.5</cell><cell>{0.25, 0.5, 0.75, 1.0}</cell><cell>0.25</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work is supported by the <rs type="funder">National Key R&amp;D Program of China</rs> (<rs type="grantNumber">2020AAA0108504</rs>) and the <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">61972275</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_NzrgfKX">
					<idno type="grant-number">2020AAA0108504</idno>
				</org>
				<org type="funding" xml:id="_YCjNAEm">
					<idno type="grant-number">61972275</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declarations</head><p>Conflicts of interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Freebase: A collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data, SIGMOD&apos;08</title>
		<meeting>the 2008 ACM SIGMOD International Conference on Management of Data, SIGMOD&apos;08</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Neural Information Processing Systems, NIPS&apos;13</title>
		<meeting>the 26th International Conference on Neural Information Processing Systems, NIPS&apos;13</meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Target-aware holistic influence maximization in spatial social networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Effective deep attributed network representation learning with topology adapted smoothing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics PP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Transition-based knowledge graph embedding with relational mapping properties</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Pacific Asia Conference on Language, Information and Computing, PACLIC&apos;14</title>
		<meeting>the 28th Pacific Asia Conference on Language, Information and Computing, PACLIC&apos;14</meeting>
		<imprint>
			<publisher>Chulalongkorn University</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="328" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sse: Semantically smooth embedding for knowledge graphs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="884" to="897" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Openke: An open toolkit for knowledge embedding</title>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP&apos;18</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP&apos;18</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="139" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Universal representation learning of knowledge bases by jointly embedding instances and ontological concepts</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD&apos;19</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD&apos;19</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1709" to="1719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Path-enhanced explainable recommendation with knowledge graphs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World Wide Web</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1769" to="1789" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding via dynamic mapping matrix</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, ACL&apos;15</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, ACL&apos;15</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="687" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A survey on knowledge graphs: Representation, acquisition, and applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Marttinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Simple embedding for link prediction in knowledge graphs</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd Conference on Neural Information Processing Systems, NeurIPS&apos;18</title>
		<meeting>the 32nd Conference on Neural Information Processing Systems, NeurIPS&apos;18</meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4284" to="4295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep attributed network representation learning of complex coupling and interaction</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">212</biblScope>
			<biblScope unit="page">106618</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, AAAI&apos;15</title>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence, AAAI&apos;15</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2181" to="2187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Differentiating concepts and instances for knowledge graph embedding</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP&apos;18</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP&apos;18<address><addrLine>Brussels; Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1971" to="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Holographic embeddings of knowledge graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI&apos;16</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence, AAAI&apos;16</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1955" to="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning, ICML&apos;11</title>
		<meeting>the 28th International Conference on Machine Learning, ICML&apos;11</meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Open-world knowledge graph completion with multiple interaction attention</title>
		<author>
			<persName><forename type="first">L</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World Wide Web</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="419" to="439" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Datatype-aware knowledge graph representation learning in hyperbolic space</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference On Information &amp; Knowledge Management, CIKM&apos;21</title>
		<meeting>the 30th ACM International Conference On Information &amp; Knowledge Management, CIKM&apos;21</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1630" to="1639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Neural Information Processing Systems, NIPS&apos;13</title>
		<meeting>the 26th International Conference on Neural Information Processing Systems, NIPS&apos;13</meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bouchard</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on International Conference on Machine Learning, ICML&apos;16</title>
		<meeting>the 33rd International Conference on International Conference on Machine Learning, ICML&apos;16</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adaptive knowledge subgraph ensemble for robust and trustworthy knowledge graph completion</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World Wide Web</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="471" to="490" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Knowledge base completion using embeddings and rules</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Artificial Intelligence, IJCAI&apos;15</title>
		<meeting>the 24th International Conference on Artificial Intelligence, IJCAI&apos;15</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1859" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, AAAI&apos;14</title>
		<meeting>the Twenty-Eighth AAAI Conference on Artificial Intelligence, AAAI&apos;14</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1112" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding: A survey of approaches and applications</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2724" to="2743" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Representation learning of knowledge graphs with hierarchical types</title>
		<author>
			<persName><forename type="first">R</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Joint Conference on Artificial Intelligence, IJCAI&apos;16</title>
		<meeting>the 25th International Joint Conference on Artificial Intelligence, IJCAI&apos;16</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2965" to="2971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A survey on heterogeneous network representation learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page">107936</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Dynamic network embedding survey</title>
		<author>
			<persName><forename type="first">G</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kong</surname></persName>
		</author>
		<idno>arxiv: abs/ 2103. 15447</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Interpretable and efficient heterogeneous graph convolutional network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Heterogeneous network representation learning: A unified framework with survey and benchmark</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W T</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the third International Conference on Learning Representations</title>
		<meeting>the third International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Transrhs: A representation learning method for knowledge graphs with relation hierarchical structure</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI&apos;20</title>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI&apos;20</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2987" to="2993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding with hierarchical relation structure</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP&apos;18</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP&apos;18</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3198" to="3207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Semantic-aware heterogeneous information network embedding with incompatible meta-paths</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
