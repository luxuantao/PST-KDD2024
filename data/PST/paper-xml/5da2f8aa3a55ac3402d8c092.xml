<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Second-order Attention Network for Single Image Super-Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tao</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate School at Shenzhen</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">PCL Research Center of Networks and Communications</orgName>
								<orgName type="institution">Peng Cheng Laboratory</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianrui</forename><surname>Cai</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yongbing</forename><surname>Zhang</surname></persName>
							<email>zhang.yongbing@sz.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School at Shenzhen</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate School at Shenzhen</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">PCL Research Center of Networks and Communications</orgName>
								<orgName type="institution">Peng Cheng Laboratory</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
							<email>cslzhang@comp.polyu.edu.hk</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<settlement>Alibaba Group</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Second-order Attention Network for Single Image Super-Resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, deep convolutional neural networks (CNNs) have been widely explored in single image super-resolution (SISR) and obtained remarkable performance. However, most of the existing CNN-based SISR methods mainly focus on wider or deeper architecture design, neglecting to explore the feature correlations of intermediate layers, hence hindering the representational power of CNNs. To address this issue, in this paper, we propose a second-order attention network (SAN) for more powerful feature expression and feature correlation learning. Specifically, a novel trainable second-order channel attention (SOCA) module is developed to adaptively rescale the channel-wise features by using second-order feature statistics for more discriminative representations. Furthermore, we present a non-locally enhanced residual group (NLRG) structure, which not only incorporates non-local operations to capture long-distance spatial contextual information, but also contains repeated local-source residual attention groups (LSRAG) to learn increasingly abstract feature representations. Experimental results demonstrate the superiority of our SAN network over state-of-the-art SISR methods in terms of both quantitative metrics and visual quality.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Single image super-resolution (SISR) <ref type="bibr" target="#b5">[5]</ref> has recently received much attention. In general, the purpose of SISR is to produce a visually high-resolution (HR) output from its low-resolution (LR) input. However, this inverse problem is ill-posed since multiple HR solutions can map to any LR input. Therefore, a great number of SR methods have been proposed, ranging from early interpolation-based <ref type="bibr" target="#b37">[37]</ref> and model-based <ref type="bibr" target="#b4">[4]</ref>, to recent learning-based methods <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b39">39]</ref>.</p><p>The early developed interpolated-based methods (e.g., bilinear and bicubic methods) are simple and efficient but limited in applications. For more flexible SR methods, more advanced model-based methods are proposed by exploiting powerful image priors, such as non-local similarity prior <ref type="bibr" target="#b34">[34]</ref> and sparsity prior <ref type="bibr" target="#b4">[4]</ref>. Although such model-based methods are flexible to produce relative high-quality HR images, they still suffer from some drawbacks: (1) such methods often involve a time-consuming optimization process; <ref type="bibr" target="#b2">(2)</ref> the performance may degrade quickly when image statistics are biased from the image prior.</p><p>Deep convolution neural networks (CNNs) have recently achieved unprecedented success in various problems <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b25">25]</ref>. The powerful feature representation and end-to-end training paradigm of CNN makes it a promising approach to SISR. In the last several years, a flurry of CNN-based SISR methods have been proposed to learn a mapping function from an interpolated or LR input to its corresponding HR output. By fully exploiting the image statics inherent in training datasets, CNNs have achieved state-of-the-art results in SISR <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b38">38]</ref>. Although considerable progress has been achieved in image SR, existing CNNbased SR models are still faced with some limitations: (1) most of CNN-based SR methods do not make full use of the information from the original LR images, thereby resulting in relatively-low performance; (2) most existing CNNbased SR models focus mainly on designing a deeper or wider network to learn more discriminative high-level features, while rarely exploiting the inherent feature correlations in intermediate layers, thus hindering the representational ability of CNNs.</p><p>To address these problems, we propose a deep secondorder attention network (SAN) for more powerful feature expression and feature correlation learning. Specifically, we 1 (a) HR (b) FSRCNN (c) LapSRN <ref type="bibr" target="#b14">[14]</ref> (d) SRMD <ref type="bibr" target="#b36">[36]</ref> (e) EDSR <ref type="bibr" target="#b36">[36]</ref> (f) DBPN <ref type="bibr" target="#b20">[20]</ref> (g) RDN <ref type="bibr" target="#b6">[6]</ref> (h) Ours</p><p>Figure <ref type="figure">1</ref>. Zoom visual results for 4Ã— SR on "img 092" from Urban100. Our method obtains better visual quality and recovers more image details compared with other state-of-the-art SR methods propose a second-order channel attention (SOCA) mechanism for better feature correlation learning. Our SOCA adaptively learns feature inter-dependencies by exploiting second-order feature statistics instead of first-order ones. Such SOCA mechnism makes our network focus on more informative feature and improve discriminative learning ability. Moreover, a non-locally enhanced residual group (NLRG) structure is presented to further incorporates nonlocal operations to capture long-distance spatial contextual information. By stacking the local-source residual attention groups (LSRAG) structure, we can exploit the information from the LR images and allow the abundant low-frequency information to be bypassed. As shown in Fig. <ref type="figure">1</ref>, our method obtains better visual quality and recovers more image details compared with other state-of-the-art SR methods. In summary, the main contributions of this paper are listed as follows:</p><p>â€¢ We propose a deep second-order attention network (SAN) for accurate image SR. Extensive experiments on public datasets demonstrate the superiority of our SAN over state-of-the-art methods in terms of both quantitive and visual quality.</p><p>â€¢ We propose second-order channel attention (SOCA) mechanism to adaptively rescale features by considering feature statistics higher than first-order. Such SOCA mechanism allows our network to focus on more informative features and enhance discriminative learning ability. Besides, we also utilize an iterative method for covariance normalization to speed up the training of our network.</p><p>â€¢ We propose non-locally enhanced residual group (NLRG) structure to build a deep network, which further incorporates non-local operations to capture spatial contextual information, and share-source residual group structure to learn deep features. Besides, the share-source residual group structure through sharesource skip connections could allow more abundant information from the LR input to be bypassed and ease the training of the deep network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>During the past decade, a plenty of image SISR methods have been proposed in the computer vision community, including interpolation-based <ref type="bibr" target="#b37">[37]</ref>, model-based <ref type="bibr" target="#b34">[34]</ref>, and CNN-based methods <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b38">38]</ref>. Due to space limitation, we here briefly review works related to CNN-based SR methods and attention mechanism, which is close to our method. CNN-based SR models. Recently, CNN-based methods have been extensively studied in image SR, due to their strong nonlinear representational power. Generally, such methods cast SR as an image-to-image regression problem, and learn an end-to-end mapping from LR to HR directly. Most existing CNN-based methods mainly focus on designing a deeper or wider network structure <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b38">38]</ref>. For example, Dong et al. <ref type="bibr" target="#b2">[ 2]</ref> first introduced a shallow three-layer convolutional network (SRCNN) for image SR, which achieves impressive performance. Later, Kim et al. designed deeper VDSR <ref type="bibr" target="#b12">[12]</ref> and DRCN <ref type="bibr" target="#b13">[13]</ref> with more than 16 layers based on residual learning. To further improve the performance, Lim et al. <ref type="bibr" target="#b20">[20]</ref> proposed a very deep and wide network EDSR by stacking modified residual blocks. The significant performance gain indicates the depth of representation plays a key role in image SR. Other recent works like MemNet <ref type="bibr" target="#b30">[30]</ref> and RDN <ref type="bibr" target="#b39">[39]</ref>, are based on dense blocks <ref type="bibr" target="#b10">[10]</ref> to form deep networks and focus on utilizing all the hierarchical features from all the convolutional layers. In addition to focusing on increasing the depth of the network, some other networks, such as NLRN <ref type="bibr" target="#b22">[22]</ref> and RCAN <ref type="bibr" target="#b38">[38]</ref>, improve the performance by considering feature correlations in spatial or channel dimension. Attention mechanism. Attention in human perception generally means that human visual systems adaptively process visual information and focus on salient areas <ref type="bibr" target="#b16">[16]</ref>. In recent years, several trials have embeded attention processing to improve the performance of CNNs for various tasks, such as image and video classification tasks <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b33">33]</ref>. Wang et al. <ref type="bibr" target="#b33">[ 33]</ref> proposed non-local neural network to incorporate non-local operations for spatial attention in video classification. On the contrary, Hu et al. <ref type="bibr" target="#b9">[ 9]</ref> proposed SENet to exploit channel-wise relationships to achieve significant performance gain for image classification.</p><p>Recently, SENet was introduced to deep CNNs to further improve SR performance <ref type="bibr" target="#b38">[38]</ref>. However, SENet only explores first-order statistics (e.g., global average pooling), while ignoring the statistics higher than first-order, thus hindering the discriminative ability of the network. In im-age SR, features with more high-frequency information are more informative for HR reconstruction. To this end, we propose a deep second-order attention network (SAN) by exploring second-order statistics of features.</p><p>3. Second-order Attention Network (SAN)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network Framework</head><p>As shown in Fig. <ref type="figure" target="#fig_0">2</ref>, our SAN mainly consists of four parts: shallow feature extraction, non-locally enhanced residual group (NLRG) based deep feature extraction, upscale module, and reconstruction part. Given I LR and I SR as the input and output of SAN. As explored in <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b39">39]</ref>, we apply only one convolutional layer to extract the shallow feature F 0 from the LR input</p><formula xml:id="formula_0">F 0 = H SF (I LR ),<label>(1)</label></formula><p>where H SF (â€¢) stands for convolution operation. Then the extracted shallow feature F 0 is used for NLRG based deep feature extraction, which thus produces the deep feature as</p><formula xml:id="formula_1">F DF = H NLRG (F 0 ),<label>(2)</label></formula><p>where H NLRG represents the NLRG based deep feature extraction module, which consists of several non-local modules to enlarge receptive field and G local-source residual attention group (LSRAG) modules (see Fig. <ref type="figure" target="#fig_0">2</ref>). So our proposed NLRG obtains very deep depth and thus provides very large receptive field size. Then the extracted deep feature F DF is upscaled via the upscale module via</p><formula xml:id="formula_2">F â†‘ = H â†‘ (F DF ),<label>(3)</label></formula><p>where H â†‘ (â€¢) and F â†‘ are a upscale module and upscaled feature respectively. There are some choices to act as upscale part, such as transposed convolution <ref type="bibr" target="#b3">[3]</ref>, ESPCN <ref type="bibr" target="#b28">[28]</ref>.</p><p>The way of embedding upscaling feature in the last few layers obtains a good trade off between computational burden and performance, and thus is preferable to be used in recent CNN-based SR models <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b39">39]</ref>. The upscaled feature is then mapped into SR image via one convolution layer</p><formula xml:id="formula_3">I SR = H R (F â†‘ )=H SAN (I LR ),<label>(4)</label></formula><p>where H R (â€¢), H â†‘ (â€¢) and H SAN are the reconstruction layer, upscale layer and the function of SAN, respectively. Then SAN will be optimized with a certain loss function. Some loss functions have been widely used, such as L 2 <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b30">30]</ref>, L 1 <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b39">39]</ref>, perceptual losses <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b26">26]</ref>. To verify the effectiveness of our SAN, we adopt the same loss functions as previous works (e.g., L 1 loss function). Given a training set with N LR images and their HR counterparts denoted by {I i LR , I i HR } N i=1 , the goal of training SAN is to optimize the L 1 loss function:</p><formula xml:id="formula_4">L(Î˜) = 1 N N i=1 ||H SAN (I i LR ) âˆ’ I i HR || 1 ,<label>(5)</label></formula><p>where Î˜ denotes the parameter set of SAN. The loss function is optimized by stochastic gradient descent algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Non-locally Enhanced Residual Group (NLRG)</head><p>We now show our non-locally enhanced residual group (NLRG) (see Fig. <ref type="figure" target="#fig_0">2</ref>), which consists of several region-level non-local (RL-NL) modules and one share-source residual group (SSRG) structure. The RL-NL exploits the abundant structure cues in LR features and the self-similarities in HR nature scenes. The SSRG is composed of G local-source residual attention groups (LSRAG) with share-source skip connections (SSC). Each LSRAG further contains M simplified residual blocks with local-source skip connection, followed by a second-order channel attention (SOCA) module to exploit feature interdependencies.</p><p>It has been verified that stacking residual blocks is helpful to form a deep CNN in <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b39">39]</ref>. However, very deep network built in such way would suffer from training difficulty and performance bottleneck due to the problem of gradient vanishing and exploding in deep network. Inspired by the work in <ref type="bibr" target="#b15">[15]</ref>, we propose local-source residual attention group (LSRAG) as the fundamental unit. It is known that simply stacking repeated LSRAGs would fail to obtain better performance. To address this issue, the share-source skip connection (SSC) is introduced in NLRG to not only facilitate the training of our deep network, but also to bypass abundant low-frequency information from LR images. Then a LSRAG in the g-th group is represented as:</p><formula xml:id="formula_5">F g = W SSC F 0 + H g (F gâˆ’1 ),<label>(6)</label></formula><p>where W SSC denotes the weight to the convolution layer, and is initialized as 0, and then gradually learns to assign more weight to the shallow feature. The bias term is omitted for simplicity. H g (â€¢) is the function of the g-th LSRAG. F g , F gâˆ’1 denote the input and output of the g-th LSRAG. The deep feature is then obtained as:</p><formula xml:id="formula_6">F DF = W SSC F 0 + F G .<label>(7)</label></formula><p>Such SSRG structure can not only ease the flow of information across LSRAGs, but also make it possible to train very deep CNN for image SR with high performance.</p><p>Region-level non-local module (RL-NL). The proposed NLRG also exploits the abundant structure cues in LR features and the self-similarities in HR nature scenes by RL-NL modules plugged before and after the SSRG. The nonlocal neural network <ref type="bibr" target="#b33">[33]</ref> is proposed to capture the computation of long-range dependencies throughout the entire image for high-level tasks. However, traditional globallevel non-local operations may be limited for some reasons: 1) global-level non-local operations require unacceptable computational burden, especially when the size of feature is large; 2) it is empirically shown that non-local operations at a proper neighborhood size are preferable for low-level tasks (e.g., image super-resolution) <ref type="bibr" target="#b22">[22]</ref>. Thus for feature with higher spatial resolution or degradation, it is natural to perform region-level non-local operations. For such reasons, we divide the feature map into a grid of regions (see Fig. <ref type="figure" target="#fig_0">2</ref>, the k Ã— k RL-NL indicates the input feature is first divided into a grid of k 2 blocks with the same size.), each of which is then processed by the subsequent layers.</p><p>After non-local operations, the feature representation is non-locally enhanced before fedding into the subsequent layers via exploiting the spatial correlations of features.</p><p>Local-source residual attention group (LSRAG). Due to our share-source skip connections, the abundant lowfrequency information can be bypassed. To go a further step to residual learning, we stack M simplified residual blocks to form a basic LSRAG. The m-th residual block (see Fig. <ref type="figure" target="#fig_0">2</ref>)intheg-th LSRAG can be represented as</p><formula xml:id="formula_7">F g,m = H g,m (F g,mâˆ’1 ),<label>(8)</label></formula><p>where H g,m (â€¢) denotes the function of m-th residual block in g-th LSRAG, and F g,mâˆ’1 , F g,m are the corresponding input and output. To make our network focus on more informative features, a local-source skip connection is used to produce the block output via</p><formula xml:id="formula_8">F g = W g F gâˆ’1 + F g,M ,<label>(9)</label></formula><p>where W g is the corresponding weight. Such local-source and share-source skip connections allow more abundant low-frequency information to be bypassed during training.</p><p>For more discriminative representations, we propose SOCA mechnism embedded at the tail of each LSRAG. Our SOCA mechnism learns to adaptively rescale channel-wise features by considering second-order statistics of features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Second-order Channel Attention (SOCA)</head><p>Most previous CNN-based SR models do not consider the feature interdependencies. To utilize such information, SENet <ref type="bibr" target="#b9">[9]</ref> was introduced in CNNs to rescale the channelwise features for image SR. However, SENet only exploits first-order statistics of features by global average pooling, while ignoring statistics higher than first-order, thus hindering the discriminative ability of the network. On the other hand, recent works <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b21">21]</ref> have shown that second-order statistics in deep CNNs are more helpful for more discriminative representations than first-order ones.</p><p>Inspired by the above observations, we propose a second-order channel attention (SOCA) module to learn feature interdependencies by considiering second-order statistics of features. Now we will describe how to exploit such second-order information next.</p><formula xml:id="formula_9">Covariance normalization. Given a H Ã— W Ã— C feature map F =[ f 1 , â€¢â€¢â€¢ , f C ]</formula><p>with C feature maps with size of H Ã— W . We reshape the feature map to a feature matrix X with s = WH features of C-dimension. Then the sample covariance matrix can be computed as</p><formula xml:id="formula_10">Î£ = X ÄªX T ,<label>(10)</label></formula><p>where Äª = 1 s (I âˆ’ 1 s 1), I and 1 are the s Ã— s identity matrix and matrix of all ones, respectively.</p><p>It is shown in <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b19">19]</ref> that covariance normalization plays a critical role for more discriminative representations. For this reason, we first perform covariance normalization for the obtained covariance matrix Î£, which is symmetric positive semi-definite and thus has eigenvalue decomposition (EIG) as follows</p><formula xml:id="formula_11">Î£ = UÎ›U T , (<label>11</label></formula><formula xml:id="formula_12">)</formula><p>where U is an orthogonal matrix and Î› = diag(Î» 1 , â€¢â€¢â€¢ ,Î» C ) is diagonal matrix with eigenvalues in non-increasing order. Then convariance normalization can be converted to the power of eigenvalues:</p><formula xml:id="formula_13">Å¶ = Î£ Î± = UÎ› Î± U T , (<label>12</label></formula><formula xml:id="formula_14">)</formula><p>where Î± is a positive real number, and</p><formula xml:id="formula_15">Î› Î± = diag(Î» Î± 1 , â€¢â€¢â€¢ ,Î» Î± C ). When Î± =1</formula><p>, there is no normalization; when Î±&lt;1, it nonlinearly shrinks the eigenvalues larger than 1.0 and streches those less than 1.0. As explored in <ref type="bibr" target="#b19">[19]</ref>, Î± =1/2 works well for more discriminative representations. Thus, we set Î± =1/2 in the following. Channel attention. The normalized covariance matrix characterizes the correlations of channel-wise features. We then take such normalized covariance matrix as a channel descriptor by global covariance pooling. As illustrated in Fig. <ref type="figure" target="#fig_0">2</ref>, let Å¶ =[ y 1 , â€¢â€¢â€¢ , y C ], the channel-wise statistics z âˆˆ R CÃ—1 can be obtained by shrinking Å¶. Then the c-th dimension of z is computed as</p><formula xml:id="formula_16">z c = H GCP (y c )= 1 C C i y c (i),<label>(13)</label></formula><p>where H GCP (â€¢) denotes the global covariance pooling function. Compared with the commonly used first-order pooling (e.g., global average pooling), our global covariance pooling explores the feature distribution and captures the feature statistics higher than first-order for more discriminative representations.</p><p>To fully exploit feature interdependencies from the aggregated information by global covariance pooling, we apply a gating mechanism. As explored in <ref type="bibr" target="#b9">[9]</ref>, the simple sigmoid function can serve as a proper gating function</p><formula xml:id="formula_17">w = f (W U Î´(W D z)),<label>(14)</label></formula><p>where W D and W U are the weight set of convolution layer, which set channel dimension of features to C/r and C, respectively. f (â€¢) and Î´(â€¢) are the function of sigmoid and RELU. Finally, we obtain the channel attention map w to rescale the input</p><formula xml:id="formula_18">fc = w c â€¢ f c ,<label>(15)</label></formula><p>where w c and f c denote the scaling factor and feature map in the c-th channel. With such channel attention, the residual component in the LSRAG is rescaled adaptively. As is shown above, covariance normalization plays a vital role in our SOCA. However, such covariance normalization relies heavily on eigenvalue decoomposition, which is not well supported on GPU platform, thus leading to inefficient training. To solve this issue, as explored in <ref type="bibr" target="#b18">[18]</ref>, we also apply a fast matrix normalization method based on Newton-Schulz iteration <ref type="bibr" target="#b8">[8]</ref>. In the next section, we briefly describe the covariance normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Covariance Normalization Acceleration</head><p>To date, fast implementation of EIG on GPU is still an open problem. Inspired by <ref type="bibr" target="#b18">[18]</ref>, we utilize Newton-Schulz iteration to speed up the computation of covariance normalization. Specifically, from Equ. <ref type="bibr" target="#b11">(11)</ref>, the Î£ has square root as Î£ 1/2 = Y = Udiag(Î» 1/2 i )U T .G i v e nY 0 = Î£, Z 0 = I, for n =1 , â€¢â€¢â€¢ ,N, as shown in <ref type="bibr" target="#b18">[18]</ref>, the Newton-Schulz iteration is then updated alternately as follows:</p><formula xml:id="formula_19">Y n = 1 2 Y nâˆ’1 (3I âˆ’ Z nâˆ’1 Y nâˆ’1 ), Z n = 1 2 (3I âˆ’ Z nâˆ’1 Y nâˆ’1 )Z nâˆ’1 .<label>(16)</label></formula><p>After enough iterations, Y n and Z n quadratically converges to Y and Y âˆ’1 . Such iterative operation is suitable for parallel implementation on GPU. In practice, one can achieve approximate solution with few iterations, e.g., no more than 5 iterations in our method. Since Newton-Schulz iteration only converges locally, to guarantee the convergence, we pre-normalize Î£ first via</p><formula xml:id="formula_20">Î£ = 1 tr(Î£) Î£,<label>(17)</label></formula><p>where tr(Î£)= C i Î» i denotes the trace of Î£. In such case, it can be inferred that the ||Î£ âˆ’ I|| 2 equals to the largest singular value of (Î£ âˆ’ I), i.e., 1 âˆ’ Î»i i Î»i) less than 1, which thus satisfies the convergence condition.</p><p>After Newton-Schulz iteration, we apply a postcompensation procedure to compensate the data magnitude caused by pre-normalization, thus producing the final normalized covariance matrix Å¶ = tr(Î£)Y N .</p><p>(18)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Implementations</head><p>We set LSRAG number as G =2 0in the SSRG structure, and embed RL-NL modules (k =2 ) at the head and tail of SSRG. In each LSRAG, we use m =1 0residual blocks plus single SOCA module at the tail. In SOCA module, we use 1 Ã— 1 convolution filter with reduction ratio r =1 6 . For other convolution filter outside SOCA, the size and number of filter are set as 3 Ã— 3 and C =6 4 , respectively. For upscale part H â†‘ (â€¢), we follow the works in <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b39">39]</ref> and apply ESPCNN <ref type="bibr" target="#b28">[28]</ref> to upscale the deep features, followed by one final convolution layer with three filters to produce color images (RGB channels).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Discussions</head><p>Difference to Non-local RNN (NLRN). NLRN <ref type="bibr" target="#b22">[22]</ref> introduces non-local operations to capture long-distance spatial contextual information in image restoration. There are some differences between NLRN and our SAN. First, NLRN embeds non-local operations in a recurrent neural network (RNN) for image restoration, while our SAN incorporates non-local operations in deep convolutional neural network (CNN) framework for image SR. Second, NLRN only considers spatial feature correlations between each location and its neighborhood, but ignores the channelwise feature correlations. While our SAN mainly focuses on learning such channel-wise feature correlations with second-order statistics of features for more powerful representational ability. Difference to Residual Dense Network (RDN). We summarize the main differences between RDN <ref type="bibr" target="#b39">[39]</ref> and our SAN. The first one is the design of basic block. RDN mainly combines dense blocks with local feature fusion by using local residual learning, while our SAN is built on the basis of residual blocks. The second one is the way of enhancing discriminative ability of the network. Channel attention <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b38">38]</ref> has been shown to be effective for better discriminative representations. However, RDN does not consider such information, but pays attention to exploiting the hierarchical features from all the convolutional layers. On the contrary, our SAN heavily relies on channel attention for better discriminative representations. Thus, we propose second-order channel attention (SOCA) mechanism to effectively learn channel-wise feature interdependencies. Difference to Residual Channel Attention Network (RCAN). Zhang et al. <ref type="bibr" target="#b38">[38]</ref> proposed a residual in residual structure to form a very deep network. RCAN is close to our SAN, and the main differences lie in the following aspects. First, RCAN consists of several residual groups with long skip connections. While, SAN stacks repeated residual groups through share-source skip connections, which allows more abundant low-frequency information to be bypassed. Second, RCAN can only exploit the contextual information in a local receptive field, but is unable to exploit the information outside of the local region. While SAN can alleviate this problem by incorporating non-local operations to not only capture long-distance spatial contextual information, but enlarge the receptive field. Third, to enhance the discriminative ability of the network, RCAN only considers channel attention based first-order feature statistics by global average pooling. While our SAN learns channel attention based on second-order feature statistics.</p><p>To the best of our knowledge, it is the first attempt to investigate the effect of such attention based on second-order feature statistics for image SR. More analysis about the effect of such attention mechanism are shown next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Setup</head><p>Following <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b38">38]</ref>, we use 800 high-resolution images from DIV2K dataset <ref type="bibr" target="#b31">[31]</ref> as training set. For testing, we adopt 5 standard benchmark datasets: Set5, Set14, BSD100, Urban100 and Manga109, each of which has different characteristics. We carry out experiments with Bicubic (BI) and Blur-downscale (BD) degradation models <ref type="bibr" target="#b36">[36]</ref>. All the SR results are evaluated by PSNR and SSIM metrics on Y channel of transformed YCbCr space.</p><p>During training, we augment the training images by randomly rotating 90 â€¢ , 180 â€¢ , 270 â€¢ and horizontally flipping. In each min-batch, 8 LR color patches with size 48 Ã— 48 are provided as inputs. Our model is trained by ADAM optimizor with Î² 1 =0 .9,Î² 2 =0 .99, and Îµ =1 0 âˆ’8 . The learning rate is initialized as 10 âˆ’4 and then reduced to half every 200 epochs. Our proposed SAN has been implemented on the Pytorch framework <ref type="bibr" target="#b23">[23]</ref> on an Nvidia 1080Ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>As discussed in Section 3, our SAN contains two main components including non-locally enhanced residual group (NLRG) and second-order channel attention (SOCA). Non-locally Enhanced Residual Group (NLRG). To verify the effectiveness of different modules, we compare NLRG with its variants trained and tested on Set5 dataset. The specific performance is listed in Table <ref type="table" target="#tab_0">1</ref>.</p><p>Base refers to a very basic baseline which only contains the convolution layers with 20 LSRAGs and 10 residual blocks in each LSRAG, thus resulting in deep network with over 400 convolution layers. As in <ref type="bibr" target="#b38">[38]</ref>, we also add long and short skip connections in Base model. From Table 1 we can see that Base reaches PSNR=32.00 dB on Set5 (Ã—4). Results from R a to R e verify the effectiveness of individual module, since the module used alone improves the performance over Base model. Specifically, R a and R b that add a single RL-NL in shallow (before SSRG) or deep layers (after SSRG) obtain similar SR results and outperform Base, which verifies the effectiveness of RL-NL. When share-source skip connection (SSC) is added alone (R c ), the performance can be improved from 32.00 dB to 32.07 dB. The main reason lies in that share-source skip connections allows more abundant low-frequency information from the LR images to be bypassed. When both of R a and R b are used (leading to R f ), the performance can be further improved. It is found more RL-NL modules cannot obtain much better performance than R f in our method, and thus we apply R f in our method to balance the performance and efficiency. Second-order channel attention (SOCA). We also show the effect of our SOCA from the results of R d ,R e ,R h and R i . Specifically, R d means that channel attention is based on first-order feature statistics by global average pooling, thus leading to first-order channel attention (FOCA). R e means that channel attention is based on second-order feature statistics, thus leading to our second-order channel attention (SOCA). It can be found that both of R d and R e obtain better performance than methods of R a to R c with-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results with Bicubic Degradation (BI)</head><p>To test the effectiveness of our SAN, we compare our SAN with 11 state-of-the-art CNN-based SR methods: SR-CNN [1], FSRCNN <ref type="bibr" target="#b3">[3]</ref>, VDSR <ref type="bibr" target="#b12">[12]</ref>, LapSRN <ref type="bibr" target="#b14">[14]</ref>, Mem-Net <ref type="bibr" target="#b30">[30]</ref>, EDSR <ref type="bibr" target="#b20">[20]</ref>, SRMD <ref type="bibr" target="#b36">[36]</ref>, NLRN <ref type="bibr" target="#b22">[22]</ref>, DBPN <ref type="bibr" target="#b6">[6]</ref>, RDN <ref type="bibr" target="#b39">[39]</ref> and RCAN <ref type="bibr" target="#b38">[38]</ref>. As in <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b38">38]</ref>, we also adopt self-ensemble method to further improve our SAN denoted as SAN+. All the quantitative results for various scaling factors are reported in Table <ref type="table" target="#tab_1">2</ref>. Compared with other methods, our SAN+ performs the best results on all the datasets on various scaling factors. Without self-ensemble, SAN and RCAN obtain very similar results and outperform other methods. This is mainly because both of them adopt channel attention to learn feature interdependencies, thus making the network focus on more informative features.</p><p>Compared with RCAN, our SAN obtains better results for datasets (e.g., such as Set5, Set14 and BSD100) with rich texture information, while obtaining a little worse results for datasets(e.g., Urban100 and Manga109) with rich repeated edge information. It is known that textures are highorder patterns and have more complex statistic characteristics, while edges are first-order patterns that can be extracted by first-order gradient operators. Thus our SOCA based on second-order feature statistics works better on images with more high-order information (e.g., textures).</p><p>Visual quality. We also show the zoomed results of various methods in Fig. <ref type="figure" target="#fig_1">3</ref>, from which we can see that most compared SR models cannot reconstruct the lattices accurately and suffer from serious blurring artifact. In contrast, our SAN obtains sharper results and recovers more highfrequency details, such as high contrast and sharp edges. Take "img 076" for example, most compared methods output heavy blurring artifacts. The early developed bicubic, SRCNN, FSRCNN and LapSRN even lose the main structure. More recent methods (e.g., EDSR, DBPN and RDN) can recover the main outlines but fail to recover more image details. Compared with the ground-truth, RCAN and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results with Blur-downscale Degradation (BD)</head><p>Following <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b39">39]</ref>, we also compare various SR methods on image with blur-down degradation (BD) model. We compare our method with 8 state-of-the-art SR methods: SPMSR <ref type="bibr" target="#b24">[24]</ref>, SRCNN <ref type="bibr" target="#b2">[2]</ref>, FSRCNN <ref type="bibr" target="#b3">[3]</ref>, VDSR <ref type="bibr" target="#b12">[12]</ref>, IR-CNN <ref type="bibr" target="#b35">[35]</ref>, SRMD <ref type="bibr" target="#b36">[36]</ref>, RDN <ref type="bibr" target="#b39">[39]</ref>, and RCAN <ref type="bibr" target="#b38">[38]</ref>. All the results on 3Ã— are shown in Table <ref type="table" target="#tab_2">3</ref>, from which we can observe that our SAN achieves consistently better performance than other methods even without self-ensemble. Specifically, the PSNR gain of SAN over RDN is up to 0.4 dB on Urban100 and Manga109 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Model Size Analyses</head><p>The Table <ref type="table" target="#tab_3">4</ref> shows the performance and model size of recent deep CNN-based SR methods. Among these methods, MemNet and NLRG contain much less parameters at the cost of performance degradation. Instead, our SAN has lless parameters than RDN and RCAN, but obtains higher performance, which implies that our SAN can have a good trade-off between performance and model complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We propose a deep second-order attention network (SAN) for accurate image SR. Specifically, the non-locally enhanced residual group (NLRG) structure allows SAN to capture the long-distance dependencies and structural information by embedding non-local operations in the network. Meanwhile, NLRG allows abundant low-frequency information from the LR images to be bypassed through sharesource skip connections. In addition to exploiting the spatial feature correlations, we propose second-order channel attention (SOCA) module to learn feature interdependencies by global covariance pooling for more discriminative representations. Extensive experiments on SR with BI and BD degradation models show the effectiveness of our SAN in terms of quantitative and visual results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Framework of the proposed second-order attention network (SAN) and its sub-modules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Visual comparison for 4Ã— SR with BI model on Urban100 dataset. The best results are highlighted</figDesc><graphic url="image-22.png" coords="7,101.45,315.19,113.74,99.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Effects of different modules. We report the best PSNR (dB) values on Set5 (4Ã—) in 5.6 Ã— 10 5 iterations.</figDesc><table><row><cell></cell><cell>Base</cell><cell>Ra</cell><cell>R b</cell><cell>Rc</cell><cell>R d</cell><cell>Re</cell><cell>R f</cell><cell>Rg</cell><cell>R h</cell><cell>Ri</cell></row><row><cell>RL-NL(before SSRG)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RL-NL(after SSRG)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>share-source skip connection (SSC)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>First-order channel attention (FOCA)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Second-order channel attention (SOCA)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>32.00</cell><cell cols="9">32.04 32.06 32.07 32.12 32.16 32.08 32.10 32.14 32.20</cell></row><row><cell></cell><cell>HR</cell><cell></cell><cell>Bicubic</cell><cell></cell><cell cols="2">SRCNN [2]</cell><cell cols="2">FSRCNN [3]</cell><cell></cell><cell>LapSRN [14]</cell></row><row><cell></cell><cell>PSNR/SSIM</cell><cell></cell><cell cols="2">17.02/0.7101</cell><cell cols="2">18.39/0.8023</cell><cell cols="2">18.21/0.7994</cell><cell></cell><cell>18.66/0.8406</cell></row><row><cell>Urban100 (4Ã—):</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>img 067</cell><cell>EDSR [20]</cell><cell></cell><cell>DBPN [6]</cell><cell></cell><cell cols="2">RDN [39]</cell><cell cols="2">RCAN [38]</cell><cell></cell><cell>SAN</cell></row><row><cell></cell><cell>21.17/0.9052</cell><cell></cell><cell cols="2">20.31/0.8910</cell><cell cols="2">20.87/0.9023</cell><cell cols="2">21.29/0.9127</cell><cell cols="2">21.34/0.9081</cell></row><row><cell></cell><cell>HR</cell><cell></cell><cell>Bicubic</cell><cell></cell><cell cols="2">SRCNN [2]</cell><cell cols="2">FSRCNN [3]</cell><cell></cell><cell>LapSRN [14]</cell></row><row><cell></cell><cell>PSNR/SSIM</cell><cell></cell><cell cols="2">21.59/0.6325</cell><cell cols="2">22.5619/0.7316</cell><cell cols="2">22.0382/0.6807</cell><cell></cell><cell>22.03/0.6948</cell></row><row><cell>Urban100 (4Ã—):</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>img 076</cell><cell>EDSR [20]</cell><cell></cell><cell>DBPN [6]</cell><cell></cell><cell cols="2">RDN [39]</cell><cell cols="2">RCAN [38]</cell><cell></cell><cell>SAN</cell></row><row><cell></cell><cell>23.95/0.7750</cell><cell></cell><cell cols="2">23.21/0.7455</cell><cell cols="2">24.08/0.7801</cell><cell cols="2">24.30/0.7896</cell><cell cols="2">24.53/0.7925</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative results with BI degradation model.</figDesc><table><row><cell>Method</cell><cell>Set5 PSNR/ SSIM</cell><cell>Set14 PSNR/ SSIM</cell><cell>BSD100 PSNR/ SSIM</cell><cell>Urban100 PSNR/ SSIM</cell><cell>Manga109 PSNR/ SSIM</cell></row><row><cell>Bicubic</cell><cell cols="5">2 33.66/ .9299 30.24/ .8688 29.56/ .8431 26.88/ .8403 30.80/ .9339</cell></row><row><cell cols="6">SRCNN 2 36.66/ .9542 32.45/ .9067 31.36/ .8879 29.50/ .8946 35.60/ .9663</cell></row><row><cell cols="6">FSRCNN 2 37.05/ .9560 32.66/ .9090 31.53/ .8920 29.88/ .9020 36.67/ .9710</cell></row><row><cell>VDSR</cell><cell cols="5">2 37.53/ .9590 33.05/ .9130 31.90/ .8960 30.77/ .9140 37.22/ .9750</cell></row><row><cell cols="6">LapSRN 2 37.52/ .9591 33.08/ .9130 31.08/ .8950 30.41/ .9101 37.27/ .9740</cell></row><row><cell cols="6">MemNet 2 37.78/ .9597 33.28/ .9142 32.08/ .8978 31.31/ .9195 37.72/ .9740</cell></row><row><cell>EDSR</cell><cell cols="5">2 38.11/ .9602 33.92/ .9195 32.32/ .9013 32.93/ .9351 39.10/ .9773</cell></row><row><cell>SRMD</cell><cell cols="5">2 37.79/ .9601 33.32/ .9159 32.05/ .8985 31.33/ .9204 38.07/ .9761</cell></row><row><cell>NLRN</cell><cell cols="5">2 38.00/ .9603 33.46/ .9159 32.19/ .8992 31.81/ .9246 âˆ’âˆ’/ âˆ’âˆ’</cell></row><row><cell>DBPN</cell><cell cols="5">2 38.09/ .9600 33.85/ .9190 32.27/ .9000 32.55/ .9324 38.89/ .9775</cell></row><row><cell>RDN</cell><cell cols="5">2 38.24/ .9614 34.01/ .9212 32.34/ .9017 32.89/ .9353 39.18/ .9780</cell></row><row><cell>RCAN</cell><cell cols="5">2 38.27/ .9614 34.11/ .9216 32.41/ .9026 33.34/ .9384 39.43/ .9786</cell></row><row><cell>SAN</cell><cell cols="5">2 38.31/ .9620 34.07/ .9213 32.42/ .9028 33.10/ .9370 39.32/ .9792</cell></row><row><cell>SAN+</cell><cell cols="5">2 38.35/ .9619 34.44/ .9244 32.50/ .9038 33.73/ .9416 39.72/ .9797</cell></row><row><cell>Bicubic</cell><cell cols="5">3 30.39/ .8682 27.55/ .7742 27.21/ .7385 24.46/ .7349 26.95/ .8556</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Quantitative results with BD degradation model. Best and second best results are highlighted and underlined .8308 26.38/ .7271 26.33/ .6918 23.52/ .6862 25.46/ .8149 SPMSR 3 32.21/ .9001 28.89/ .8105 28.13/ .7740 25.84/ .7856 29.64/ .9003 SRCNN 3 32.05/ .8944 28.80/ .8074 28.13/ .7736 25.70/ .7770 29.47/ .8924 FSRCNN 3 26.23/ .8124 24.44/ .7106 24.86/ .6832 22.04/ .6745 23.04/ .7927 VDSR 3 33.25/ .9150 29.46/ .8244 28.57/ .7893 26.61/ .8136 31.06/ .9234 IRCNN 3 33.38/ .9182 29.63/ .8281 28.65/ .7922 26.77/ .8154 31.15/ .9245 SRMD 3 34.01/ .9242 30.11/ .8364 28.98/ .8009 27.50/ .8370 32.97/ .9391 RDN 3 34.58/ .9280 30.53/ .8447 29.23/ .8079 28.46/ .8582 33.97/ .9465 RCAN 3 34.70/ .9288 30.63/ .8462 29.32/ .8093 28.81/ .8645 34.38/ .9483 SAN 3 34.75/ .9290 30.68/ .8466 29.33/ .8101 28.83/ .8646 34.46/ .9487 SAN+ 3 34.86/ .9297 30.77/ .8481 29.39/ .8112 29.03/ .8674 34.76/ .9501</figDesc><table><row><cell>Method</cell><cell>Set5 PSNR/ SSIM</cell><cell>Set14 PSNR/ SSIM</cell><cell>BSD100 PSNR/ SSIM</cell><cell>Urban100 PSNR/ SSIM</cell><cell>Manga109 PSNR/ SSIM</cell></row><row><cell>Bicubic</cell><cell>3 28.78/</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Computational and parameter comparison (2Ã— Set5).</figDesc><table><row><cell></cell><cell>EDSR</cell><cell>MemNet</cell><cell>NLRG</cell><cell>DBPN</cell><cell>RDN</cell><cell>RCAN</cell><cell>SAN</cell></row><row><cell>Para.</cell><cell>43M</cell><cell>677k</cell><cell>330k</cell><cell>10M</cell><cell cols="2">22.3M 16M</cell><cell>15.7M</cell></row><row><cell>PSNR</cell><cell>38.11</cell><cell>37.78</cell><cell>38.00</cell><cell>38.09</cell><cell>38.24</cell><cell>38.27</cell><cell>38.31</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work is supported in part by the National Natural Science Foundation of China under Grant 61771273, the R&amp;D Program of Shenzhen under Grant JCYJ20180508152204044, and the research fund of PCL Future Regional Network Facilities for Large-scale Experiments and Applications (PCL2018KP001).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<idno>31.74/ .8893 28.26/ .7723 27.40/ .7281 25.50/ .7630</idno>
	</analytic>
	<monogr>
		<title level="j">MemNet</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2016. 1, 2, 3, 7, 8</date>
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Accelerating the super-resolution convolutional neural network</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2016. 3, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image deblurring and super-resolution by adaptive sparse domain selection and adaptive regularization</title>
		<author>
			<persName><forename type="first">Weisheng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning low-level vision</title>
		<author>
			<persName><forename type="first">Egon</forename><forename type="middle">C</forename><surname>William T Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Owen</forename><forename type="middle">T</forename><surname>Pasztor</surname></persName>
		</author>
		<author>
			<persName><surname>Carmichael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="47" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep backprojection networks for super-resolution</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norimichi</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2007">2018. 2, 3, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Functions of matrices: theory and computation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nicholas</surname></persName>
		</author>
		<author>
			<persName><surname>Higham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>SIAM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2006">2018. 2, 4, 5, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyoung</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2008">2016. 1, 2, 3, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deeplyrecursive convolutional network for image super-resolution</title>
		<author>
			<persName><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyoung</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate superresolution</title>
		<author>
			<persName><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2007">2017. 1, 2, 3, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Fast and accurate image super-resolution with deep laplacian pyramid networks</title>
		<author>
			<persName><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.01992</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName><forename type="first">Christof</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ernst</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Photorealistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferenc</forename><surname>HuszÃ¡r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Towards faster training of global covariance pooling networks by iterative matrix square root normalization</title>
		<author>
			<persName><forename type="first">Peihua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qilong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zilin</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Is second-order information helpful for large-scale visual recognition</title>
		<author>
			<persName><forename type="first">Peihua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qilong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyoung</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
				<imprint>
			<date type="published" when="2007">2017. 2, 3, 5, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bilinear cnn models for fine-grained visual recognition</title>
		<author>
			<persName><forename type="first">Tsung-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aruni</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Non-local recurrent network for image restoration</title>
		<author>
			<persName><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bihan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2007">2018. 2, 4, 5, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A statistical prediction model based on sparse representations for single image super-resolution</title>
		<author>
			<persName><forename type="first">Tomer</forename><surname>Peleg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Enhancenet: Single image super-resolution through automated texture synthesis</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>SchÃ¶lkopf</surname></persName>
		</author>
		<author>
			<persName><surname>Hirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Image classification with the fisher vector: Theory and practice</title>
		<author>
			<persName><forename type="first">Jorge</forename><surname>SÃ¡nchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<idno>IJCV. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferenc</forename><surname>HuszÃ¡r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Andrew P Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehan</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Image superresolution via deep recursive residual network</title>
		<author>
			<persName><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Memnet: A persistent memory network for image restoration</title>
		<author>
			<persName><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyan</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Methods and results</title>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyoung</forename><surname>Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semi-coupled dictionary learning with applications to image super-resolution and photo-sketch synthesis</title>
		<author>
			<persName><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Abhinav Gupta, and Kaiming He. Non-local neural networks</title>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Single image superresolution with non-local means and steering kernel regression</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4544" to="4556" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning deep cnn denoiser prior for image restoration</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning a single convolutional super-resolution network for multiple degradations</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2008">2018. 1, 2, 6, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An edge-guided image interpolation algorithm via directional filtering and data fusion</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2008">2018. 1, 2, 6, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2008">2018. 1, 2, 3, 5, 6, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
