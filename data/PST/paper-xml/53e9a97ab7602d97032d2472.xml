<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SVD based initialization: A head start for nonnegative matrix factorization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">C</forename><surname>Boutsidis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
								<address>
									<postCode>12180</postCode>
									<settlement>Troy</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">E</forename><surname>Gallopoulos</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Computer Engineering and Informatics Department</orgName>
								<orgName type="institution">University of Patras</orgName>
								<address>
									<postCode>GR-26500</postCode>
									<settlement>Patras</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SVD based initialization: A head start for nonnegative matrix factorization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3FB6FD93BA42677602E69D781EB8667E</idno>
					<idno type="DOI">10.1016/j.patcog.2007.09.010</idno>
					<note type="submission">Received 9 February 2007; received in revised form 3 August 2007; accepted 20 September 2007</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>NMF</term>
					<term>Sparse NMF</term>
					<term>SVD</term>
					<term>Nonnegative matrix factorization</term>
					<term>Singular value decomposition</term>
					<term>Perron-Frobenius</term>
					<term>Low rank</term>
					<term>Structured initialization</term>
					<term>Sparse factorization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe Nonnegative Double Singular Value Decomposition (NNDSVD), a new method designed to enhance the initialization stage of nonnegative matrix factorization (NMF). NNDSVD can readily be combined with existing NMF algorithms. The basic algorithm contains no randomization and is based on two SVD processes, one approximating the data matrix, the other approximating positive sections of the resulting partial SVD factors utilizing an algebraic property of unit rank matrices. Simple practical variants for NMF with dense factors are described. NNDSVD is also well suited to initialize NMF algorithms with sparse factors. Many numerical examples suggest that NNDSVD leads to rapid reduction of the approximation error of many NMF algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Nonnegative matrix factorization (NMF), that is the approximation 1 of a (usually nonnegative) matrix, A ∈ R m×n , as a product of nonnegative factors, say W ∈ R m×k + and H ∈ R k×n + , for some selected k, has become a useful tool in a large variety of applications, and the scientific literature and software tools on the subject and variants thereof are rapidly expanding; see e.g. Refs. <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref> and description of specific software packages in Refs. <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. As usual, we denote by A B the componentwise inequality i,j i,j for all elements of (equisized matrices) A, B. For convenience, following Ref. <ref type="bibr" target="#b19">[20]</ref> we denote by R m×n + the set of all m × n nonnegative matrices. The motivation behind NMF is that besides the dimensionality reduction sought in many applications, the underlying data ensemble is nonnegative and can be better modeled and interpreted by means of nonnegative factors. For instance, image collections are frequently stored as matrices of nonnegative elements, where each column encodes one image of the collection. The application of NMF, then, would produce a dictionary of k basis images as columns of factor W, and the nonnegative coefficients for the linear combination of these images that reconstructs an approximation of the originals in factor H. In text mining under the vector space model, document collections are stored as term-document matrices of nonnegative elements, each matrix column encoding one document. Each column of W corresponds to a basic document, and the resulting k documents can be additively combined using coefficients from H to reconstruct an approximation of the document collection as well as for other applications profiting from dimensional reduction such as clustering <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>. In the above cases, NMF provides a framework for learning parts of images and semantic features of text. Another area of application is space situational alertness, in which data collections consist of spectral reflectance data of a space object containing essential information regarding the materials composing it. Each column of the matrix represents one such spectral measurement. After the NMF, factor W contains spectral signatures that aid in detecting the type of constituent materials for the space object, and H contains coefficients that help in the computation of the proportional amounts in which these materials appear in the object <ref type="bibr" target="#b20">[21]</ref>. In all cases, the additive nature of the factorization has been proposed as an important aid in interpretation.</p><p>Our target NMF problem is as follows:</p><p>Given A ∈ R m×n + , and natural number k &lt; min(m, n), compute W ∈ R m×k + and H ∈ R k×n + that solve min W 0,H 0 (A, WH), where (A, WH) : R m×n × R m×n → R + is some suitable distance metric.</p><p>In the NMF algorithms combined with the initialization proposed in this paper, the distance metrics will be the Frobenius norm A -WH F , or modifications thereof, or the generalized Kullback-Leibler divergence D(A||WH). All are used extensively in the literature, and are directly related to more general metrics <ref type="bibr" target="#b0">[1]</ref>.</p><p>As defined, the NMF problem is a more general instance of the case where we demand factors whose product exactly equals the matrix (ignoring roundoff, as we will do throughout this paper). Such nonnegative decompositions were a central topic already in early treatments of nonnegative matrices (see e.g. Ref. <ref type="bibr" target="#b19">[20]</ref>). In general, there is no guarantee that an exact nonnegative factorization exists for arbitrary k. It is known, however, that if A 0, then there exists a natural number (called nonnegative rank) and nonnegative W and H having that number as rank so that A = WH holds exactly (cf. Ref. <ref type="bibr" target="#b21">[22]</ref>.) Furthermore, NMF is a nonconvex optimization problem with inequality constraints and iterative methods become necessary for its solution (see e.g. Refs. <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>). Unfortunately, current NMF algorithms typically converge slowly and then at local minima.</p><p>There exist many algorithms for approximating matrices using nonnegative factors (Ref. <ref type="bibr" target="#b10">[11]</ref> plays pivotal role, cf. the survey <ref type="bibr" target="#b20">[21]</ref>). Popular and used by many researchers as basis for further developments are two algorithms proposed in Ref. <ref type="bibr" target="#b24">[25]</ref>. These rely on an iterative multiplicative or additive correction of initial guesses for the pair of factors (W, H ). The algorithms were proven to converge monotonically and can be interpreted as diagonally rescaled gradient descent methods; cf. Refs. <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26]</ref>. Another important issue addressed by researchers is the incorporation of further constraints appropriate for the problem; see e.g. <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15]</ref>. One generic constraint is sparsity: It is frequently important to minimize the number of features used in reconstruction, e.g. in sparse linear representation, in the case of signal processing <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref>. Ref. <ref type="bibr" target="#b6">[7]</ref>, for example, uses a sparsity metric and describes an algorithm to satisfy it in the factors.</p><p>Due to the iterative nature of all NMF algorithms, the initialization of the pair of factors (W, H ) is cited in the literature as an important component in the design of successful NMF methods. In this paper we focus on this issue. With few exceptions (see Refs. <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>) most NMF algorithms in the literature use random nonnegative initialization for (W, H ). Iterates converge to a local minimum, so it becomes necessary to run several instances of the algorithm using different random initializations and then select the best solution. Because NMF can be viewed as a bound optimization problem, it is also likely to suffer from slow convergence <ref type="bibr" target="#b23">[24]</ref>. Therefore, the overall process can become quite expensive. As a step toward the development of overall faster algorithms for NMF, we propose a novel initialization strategy that is based on singular value decomposition (SVD) and has the following features: (i) it can be readily combined with all available NMF algorithms; (ii) in its basic form, contains no randomization and therefore converges to the same solution for any given algorithm; (iii) it rapidly provides an approximation with error almost as good as that obtained via the deployment of alternative initialization schemes when these run to convergence. We call the proposed initialization strategy NNDSVD (Nonnegative Double Singular Value Decomposition) to underline the fact that it is based on two SVD processes: One to create the rank-k approximation, followed by a "small" SVD on each of the positive sections of each of the factors. Because of property (iii) it is expected to be especially useful whenever the application constrains the maximum time interval for result delivery. NNDSVD depends on an interesting property (Lemma 1 and Theorem 2) concerning the behavior of unit rank matrices; to the best of our knowledge, this fact remained unnoticed until now, and could have interesting applications in other domains. We show one family of nonnegative matrices for which NNDSVD returns an exact NMF (Proposition 7). The basic algorithm also admits an interesting modification, we name 2-step NNDSVD, that has the potential to provide initialization at an even lower cost.</p><p>In the sequel, given any vector or matrix variable X, its "positive section", X + 0, will be defined to be the vector or matrix of same size that contains the same values as X there where X has nonnegative elements and 0 elsewhere. The "negative section" of X will be the matrix X -= X + -X, where again X -0. It follows immediately that any vector or matrix can be written as X = X + -X -, and if X 0 then X -= 0. MATLABlike notation is followed when necessary. The notions "positive" and "negative", of course, are slight misnomers, since we are really referring to nonnegative and absolute value of nonpositive values respectively. We prefer them, however, and let the handling of zero values be made clear by context. When we seek sparse factors we will refer to "sparse NMF". We occasionally refer to "dense NMF" when we need to underline that we do not seek sparsity. The paper is organized as follows: Section 2 discusses initialization in the context of related work and describes the properties of NNDSVD. Section 3 illustrates the use and performance of the method in a variety of cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SVD-based initializations</head><p>Most research papers to date discussing NMF algorithms mention the need to investigate good initialization strategies (see e.g. Ref. <ref type="bibr" target="#b20">[21]</ref>) but, in the absence of any additional information about the problem, initialize the elements of the pair (W, H ) with nonnegative random values. In some cases, only one of the factors (e.g. W) is initialized (as random) while the other is chosen to satisfy certain constraints, possibly obtained after solving an optimization problem using the initial values for the former. In the sequel, we would be referring to "initialization of (W, H )" to mean initialization of either or both factors, but would be more specific whenever necessary.</p><p>Because of the nature of the underlying optimization problem, repeated runs of any of these algorithms with different initializations will be necessary and will lead to different answers. Before proceeding, we need to clarify what we mean by "good initialization strategy". Two possible answers are: (i) one that leads to rapid error reduction and faster convergence; (ii) one that leads to better overall error at convergence. We concentrate on the former objective while noting that a satisfactory answer to the latter remains elusive.</p><p>Because NMF is a constrained low rank matrix approximation, we seek the initialization strategy amongst alternative low rank factorization schemes. Indeed, one of the few published algorithms for nonrandom initialization (see Refs. <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>), relies on a method (see Ref. <ref type="bibr" target="#b31">[32]</ref>) that provides low rank approximation via clustering. Specifically, spherical k-means (Skmeans) is used to partition the columns of A into k clusters, selecting the normalized centroid representative vector (named "concept vector") for each cluster and using that vector to initialize the corresponding column of W. Depending on the NMF algorithm used subsequently, H can either be random or be computed as arg min H 0 A -WH F . It was shown in Refs. <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref> that only few iterations of this clustering algorithm are sufficient and that the scheme, we call CENTROID in the sequel, leads to faster error reduction than random initialization. Specifically, numerical experiments in Ref. <ref type="bibr" target="#b30">[31]</ref> showed that the method, at some overhead for the clustering phase, can save several expensive NMF update steps.</p><p>In our quest that eventually led to the framework proposed in this paper, we first explored initializations inspired by the aforementioned original ideas of Refs. <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref> for structured initialization. Specifically, we deployed an SVD analogue (cf. Ref. <ref type="bibr" target="#b32">[33]</ref>) to the low rank approximation methods used in CEN- TROID. We first clustered the columns of A into k groups and then initialized (W, H ) using nonnegative left and right singular vectors corresponding to the maximum singular value of each group. Their existence is guaranteed by Perron-Frobenius theory; see also Ref. <ref type="bibr" target="#b33">[34]</ref>. Results were mixed: sometimes the SVD approach would outperform CENTROID, sometimes not. We were thus motivated to consider alternative approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">NNDSVD initialization</head><p>We next present a method for initialization that turns out to be quite effective. We start from the basic property of the SVD, by which, every matrix A ∈ R m×n of rank r min(m, n) can be expressed as the sum of r leading singular factors A = r j =1 j u j v j , where 1 • • • r &gt; 0 are the nonzero singular values of A and {u j , v j } r j =1 the corresponding left and right singular vectors. Then, for every k r, the optimal rankk approximation of A with respect to the Frobenius norm, say A (k) , is readily available from the sum of the first k factors (cf. Ref. <ref type="bibr">[35,</ref> Schmidt and Eckart-Young theory]), that is</p><formula xml:id="formula_0">A (k) := k j =1 j C (j ) = arg min rank(G) k A -G ,<label>(1)</label></formula><p>where C (j ) = u j v j . We assume, from now on, that A is nonnegative. Our approach uses a modification of expansion (1) that will produce a nonnegative approximation of A and provide, in the same time, effective initial values for (W, H ). In particular, every unit rank matrix C (j ) is approximated by its nonnegative section C (j ) + ; subsequently, (W, H ) are initialized from selected singular triplets of C (j )</p><formula xml:id="formula_1">+ . The factors C (j )</formula><p>+ possess special properties that play a key role in our algorithm. As we will show:</p><p>• Their rank is at most 2 because of the "set to zero with small rank increment" property (Lemma 1). • They are the best nonnegative approximations of C (j ) in terms of the Frobenius norm (cf. Lemma 5). • There exist corresponding singular vectors that are nonnegative and are readily available from the singular triplets</p><formula xml:id="formula_2">{ j , u j , v j } of A (cf. Theorem 2).</formula><p>In summary, NNDSVD can be described as follows: (i) Compute k leading singular triplets of A; (ii) form the unit rank matrices {C (j ) } k j =1 obtained from singular vector pairs; (iii) extract their positive section and respective singular triplet information; (iv) use them to initialize (W, H ). The two SVD's, in steps (i) and (iii), motivated the naming of NNDSVD. On the other hand, we will show that because of special properties of C (j ) + , Steps (ii) and (iii) can be implemented at very low cost. Using the notation introduced thus far, we show the Lemma that is central in our discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 1. Consider any matrix</head><formula xml:id="formula_3">C ∈ R m×n such that rank(C) = 1, and write C = C + -C -. Then rank(C + ), rank (C -) 2.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof. From the rank assumption we can write</head><formula xml:id="formula_4">C = xy = (x + -x -)(y + -y -) = (x + y + + x -y -) -(x + y -+ x -y + ).</formula><p>All factors are nonnegative; moreover, for each x, y, the nonzero values of the positive section are situated at locations that are complementary than the nonzeros of the corresponding negative section. Consequently, each nonzero element of C is obtained from exactly one term from the terms on the right. Therefore, C + = x + y + + x -y -and C -= x + y -+ x -y + and the rank of each is at most 2. It is worth noting, as an alternate algebraic proof, that we can also write C = XTY, where (in MATLAB notation)</p><formula xml:id="formula_5">X := [x + , x -], Y = [y + ; y -] and T = [1, -1; -1, 1]. Note that T is unit rank. The expression C = C + -C -amounts to decomposing T = I -J , where J = [0, 1; 1, 0] so that C = XY -XJY and C + = XY, C -= XJY, each of which has rank at most 2.</formula><p>The result, albeit simple, is quite remarkable: It tells us that if we zero out all negative values of a unit rank matrix, the resulting matrix will have rank 2 at most. We thus call the above "set to zero with small rank increment" property. It is worth noting and easy to verify that matrices of rank(C) &gt; 1 do not share a similar property. For example consider the matrix</p><formula xml:id="formula_6">C = XY , where X, Y ∈ R 6×2 are X = ⎛ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝ 1 2 -3 -4 2 3 -4 -5 -5 6 6 -7 ⎞ ⎟ ⎟ ⎟ ⎟ ⎟ ⎠ and Y = ⎛ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝ 2 1 3 -2 4 4 -5 -5 6 6 -6 7 ⎞ ⎟ ⎟ ⎟ ⎟ ⎟ ⎠ . Although rank(C) = 2, rank(C + ) = 5. Also rank(C -) = 5.</formula><p>Because C + is nonnegative, its maximum left and right singular vectors will also be nonnegative from Perron-Frobenius theory. The next theorem says that the remaining, trailing, singular vectors are also nonnegative. Furthermore, because of the special structure of C + , its singular value expansion is readily available.</p><p>Theorem 2. Let C ∈ R m×n have unit rank, so that C = xy for some x ∈ R m , y ∈ R n . Let also x± := x ± / x ± , ŷ± := y ± / y ± be the normalized positive and negative sections of x and y, and ± = x ± y ± and ± = x ± y ∓ . Then the unordered singular value expansions of C + and C -are</p><formula xml:id="formula_7">C + = + x+ ŷ + + -x-ŷ -and C -= + x+ ŷ -+ -x-ŷ + .</formula><p>(</p><formula xml:id="formula_8">)<label>2</label></formula><p>The maximum singular triplet of</p><formula xml:id="formula_9">C + is ( + , x+ , ŷ+ ) if + = max( x + y + , x -y -), otherwise it is ( -, x-, ŷ-). Sim- ilarly, the maximum singular triplet of C -is ( + , x+ , ŷ-) if + = max( x + y -, x -y + ) else it is ( -, x-, ŷ+ ).</formula><p>Proof. By construction, each pair of vectors x ± and y ± have their nonzero values at complementary locations, therefore x -x + =0 and y -y + =0 and each of the matrices X := [ x+ , x-] and Y := [ ŷ+ , ŷ-] is orthogonal. Terms ± are nonnegative, therefore the result follows by the uniqueness of the singular value expansion. Similarly for the decomposition of C -.</p><p>The above result establishes nonnegativity for all singular vectors corresponding to nontrivial singular values of C ± . There is an immediate connection of the decompositions introduced in Theorem 2 with the concept of nonnegative rank, already mentioned in the Section 1. <ref type="bibr" target="#b21">[22]</ref>). The nonnegative rank, rank + (A), of A ∈ R m×n + is the smallest number of nonnegative unit rank matrices into which a matrix can be decomposed additively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 3 (Gregory and Pullman</head><p>Nonnegative rank is difficult to compute (see e.g. Ref. <ref type="bibr" target="#b35">[36]</ref>). It generally holds, however, that rank(A) rank + (A) min (m, n) (cf. Ref. <ref type="bibr" target="#b21">[22]</ref>). As shown in Ref. <ref type="bibr" target="#b35">[36]</ref>, when rank(A) 2, then rank + (A) = rank(A). Combining with our previous results, we can provide precise estimates regarding the nonnegative ranks of C ± .</p><formula xml:id="formula_10">Corollary 4. (i) rank + (C ± ) 2. (ii) rank + (C ± ) = rank(C ± ).</formula><p>(iii) If C contains both positive and negative elements, then The call psvd(A, k) computes the k leading singular triplets of A, e.g. MATLAB's svds. Functions pos and neg extract the positive and negative sections of their argument:</p><formula xml:id="formula_11">[A p ] = pos(A) returns A p = (A &gt; = 0). * A; and [A n ] = neg(A) returns (A &lt; 0). * (-A). rank + (C ± ) = 2. (iv) If C 0 (resp. C 0) then rank + (C + ) = 1 (resp. rank + (C -) = 1).</formula><p>Parts (i), (iii) and (iv) follow directly from the unit rank assumption for C and Theorem 2. Part (ii) follows from the aforementioned result in Ref. <ref type="bibr" target="#b35">[36]</ref>. Theorem 2, however, provides an explicit construction for the decomposition and suggests a cheap way to compute it. The next lemma is a straightforward consequence of the definition of the Frobenius norm.</p><formula xml:id="formula_12">Lemma 5. Let C ∈ R m×n . Then C + = arg min G∈R m×n + C - G F .</formula><p>Therefore, the best (in terms of the Frobenius norm) nonnegative approximation of each unit rank term C (j ) = u (j ) (v (j ) ) would be the corresponding C (j ) + . The preceding results constitute the theoretical foundation of NNDSVD. Based on these, the method can be implemented as in Table <ref type="table" target="#tab_0">1</ref>. Note that the method first approximates each of the first k terms in the unit rank singular factor expansion of A by means of their positive sections. These new factors have rank at most 2. Then, each of these factors is approximated by its maximum singular triplet which is then used to initialize (W, H ). Note that Step 3 is applied from j = 2 onwards since the leading singular triplet is nonnegative and can be readily used to initialize the first column (resp. row) of W (resp. H).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">NNDSVD approximation</head><p>From the preceding results, it becomes possible bound the error corresponding to the initial factors (W, H ) obtained by NNDSVD, specifically, the Frobenius norm of the residual, R = A -WH. Denote by { j } r j =1 the nonzero singular values of A in nonincreasing order and by { j (C + ), x j (C + ), y j (C + )} the singular triplets of C + . From Lemma 1, rank(C + ) 2, therefore there are only two nontrivial triplets that we index, as usual by j = 1, 2. We write, A = A (k) + E (k) , where E (k) := r j =k+1 j u j v j , therefore</p><formula xml:id="formula_13">A (k) = 1 C (1) + k j =2 j C (j ) = 1 C (1) + k j =2 j C (j ) + - k j =2 j C (j ) - = 1 C (1) + k j =2 j 1 (C (j ) + )x 1 (C (j ) + )(y 1 (C (j ) + )) + Ê,</formula><p>where</p><formula xml:id="formula_14">Ê := k j =2 j 2 (C (j ) + )x 2 (C (j ) + )(y 2 (C (j ) + )) - k j =2 j C (j ) -.</formula><p>The NNDSVD algorithm (Table <ref type="table" target="#tab_0">1</ref>) selects (W, H ) so that</p><formula xml:id="formula_15">WH = 1 C (1) + k j =2 j 1 (C (j ) + )x 1 (C (j ) + )(y 1 (C (j ) + )) = A (k) -Ê.</formula><p>Therefore, A -WH = E (k) + Ê and thus</p><formula xml:id="formula_16">E (k) F R F E (k) F + Ê F ,<label>(3)</label></formula><p>so that Ê F measures the deviation from the optimal unconstrained approximation (Eq. ( <ref type="formula" target="#formula_0">1</ref>)). Now, for each j,</p><formula xml:id="formula_17">x 1 (C (j ) + )(y 1 (C (j ) + )) F = 1 since x 1 (C (j ) + ), y 1 (C (j )</formula><p>+ ) are singular vectors and have unit length. Furthermore,</p><formula xml:id="formula_18">C (j ) + 2 F + C (j ) -<label>2</label></formula><formula xml:id="formula_19">F = C (j ) 2 F = 1, therefore both C (j )</formula><p>± F 1. These lead to the following result: Proposition 6. Given A ∈ R m×n + , and the pair (W, H ) initialized by NNDSVD, then the Frobenius norm of R = A -WH is bounded as follows:</p><formula xml:id="formula_20">E (k) F R F E (k) F + Ê F ,<label>(4)</label></formula><p>where</p><formula xml:id="formula_21">Ê F k j =2</formula><p>( 2 (C (j )</p><formula xml:id="formula_22">+ ) + 1) j 2 k j =2 j .<label>(5)</label></formula><p>Even though the upper bound is very loose (e.g. it may become larger than the trivial upper bound A F obtained when (W, H ) are initialized as all zero) it establishes that the residual is bounded. Of far greater interest is that, in practice, only few iterations are sufficient for NNDSVD to drive the initial residual down to a magnitude that is very close to the one we would have obtained had we applied the underlying NMF algorithm with random initialization but for many more iterations.</p><p>The above analysis helps us also bound the error in modified versions of NNDSVD that will be described in the next section. These rely on initializing using the pair (W f , H f ), where <ref type="figure">(W,</ref><ref type="figure">H</ref> ) are as before and E W , E H are structured perturbations so that their nonzero elements occur at positions that are complementary to those of W and H, respectively. Also, max( E H F , E W F ) . Then, because all columns of W and rows of H have unit length,</p><formula xml:id="formula_23">W f := W + E W , H f := H + E H ,</formula><formula xml:id="formula_24">A -W f H f F = A -WH -W E H -E W H -E W E H F A -WH F + ( W F + H F ) = E F + 2 √ k. (<label>6</label></formula><formula xml:id="formula_25">)</formula><p>Note that the first term on the right side was bounded in Proposition 6. We finally show that there are matrices for which NNDSVD is able to return their exact decomposition into nonnegative unit rank factors. To do this, we consider matrices that admit the orthogonal nonnegative factorization described in Refs. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b36">37]</ref>. In this category belong, for instance, block diagonal matrices where each diagonal block is unit rank and generated by nonnegative vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition 7. Let</head><formula xml:id="formula_26">A = W A DH A ∈ R m×n + , where W A ∈ R m×k + , H A ∈ R k×n + , D ∈ R k×k are nonnegative and D diago- nal. Let also W A , H A be orthogonal, so that W A W A =H A H A = I . Then, if NNDSVD is applied to compute rank-k factors, it ini- tializes: (i) with the exact values, W = W A D 1/2 , H = D 1/2 H A when k = k; (ii)</formula><p>with the pair (W, H ) that returns the minimum error in Frobenius norm, when k &lt; k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof. By construction, A = W</head><formula xml:id="formula_27">A DH A is the (compact) SVD of A.</formula><p>If this is written as sum of k, rank-1 terms, each resulting from the product of the corresponding column of W A , row of H A , and diagonal term of D, then NNDSVD will compute the elements exactly since all terms are nonnegative, so their positive sections are identical to the terms themselves. The result for k &lt; k trivially follows by the optimal approximation property (cf.1) of partial SVD with respect to Frobenius norm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Dense variants: NNDSVDa and NNDSVDar</head><p>As described, one feature of NNDSVD is that it obtains initial columns and rows for (W, H ) from the leading singular vectors of the positive section of each one of the first k singular factors of A. All, except the maximum singular vectors are likely to contain positive as well as negative elements. Therefore, the initial (W, H ) are likely to contain a number of zeros commensurate to the latter. In some cases, e.g. when we seek sparse NMF (cf. Ref. <ref type="bibr" target="#b6">[7]</ref>, discussion in Section 3 and Fig. <ref type="figure" target="#fig_5">9</ref>), this is desirable, especially in view of the fact that some NMF algorithms retain the same sparsity in the iterates that was present in the initial (W, H ). In the dense case, however, a large number of zeros may become undesirable, as will be illustrated when we compare the performance of all methods (Fig. <ref type="figure" target="#fig_6">10</ref>). In particular, it will be seen that in those cases, even though the basic algorithm initially provides rapid error reductions, eventually leads to worse error than RANDOM. It was worth noting that such a behavior was also observed for some algorithms described in Ref. <ref type="bibr" target="#b30">[31]</ref>. To address this problem, we deploy two slightly modified variants of the basic algorithm. In these, we perturb the zero values in the original (W, H ); in particular, Table <ref type="table">2</ref> Nonnegative unit rank approximation of arbitrary matrix <ref type="bibr" target="#b26">[27]</ref> </p><formula xml:id="formula_28">Input: Matrix C ∈ R m×n Output: Nonnegative g ∈ R m + , h ∈ R m</formula><p>+ so that C ≈ gh 1. Compute the largest singular triplet of C: [ , u, v] 2. Set g = u + , h = v + where u + , v + are the nonnegative sections of u, v; for j = 1, . . . , 3. Compute g = Ch/ h h and set g = g + ; 4. Compute h = g C/g g and set h = h + ; end variant NNDSVDa sets all zeros equal to the average of all elements of A; we denote this by mean(A). 2 Variant NNDSVDar sets each zero element equal to a random value chosen from a uniform distribution in [0, mean(A)/100]. Both variants incur no appreciable overhead on the basic initialization and lead to error bounds such as in Eq. ( <ref type="formula" target="#formula_24">6</ref>). Moreover, the user has control over the number of zero elements that are perturbed and can exercise this judiciously to satisfy (A, WH) ≈ (A, W f H f ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Discussion and extensions</head><p>We next discuss and evaluate a seemingly similar initialization option that comes to mind naturally and is also SVD-based. This would be to set to zero the positions with negative values in each {u j , v j } pair of the singular value expansion of A and use multiples of these vectors to initialize (W, H ). This straightforward method can be interpreted using our framework: In particular, each C (j ) = u j v j , therefore from Theorem 2, and dropping for simplicity the index j till the end of this paragraph, each one of the two addends on the right of C + =u + v + +u -v - is a scalar multiple of a singular factor of C + . We remind that u = (u) + -(u) -and v = (v) + -(v) -. Therefore, this initialization is equivalent to approximating each C (actually C (j ) + ) by u + v + , whereas NNDSVD picks this or u -v -, depending on the magnitude of the corresponding ± 's (cf. Theorem 2). We conclude that NNDSVD is preferable since it leads to equal or smaller error contribution from each term.</p><p>The aforementioned approach is also closely related to another iterative algorithm, discussed in Ref. <ref type="bibr" target="#b26">[27]</ref>, for the nonnegative, unit rank approximation of arbitrary matrices. The algorithm, tabulated in Table <ref type="table">2</ref>, initializes two vectors with the positive sections of the leading left and right singular vectors of the matrix and then iterates for a certain number of steps. We now show that when the input matrix is any one of the unit rank terms C = uv corresponding to j &gt; 1 (actually C (j ) and j &gt; 1), the algorithm will make no progress, but will return as approximations the positive sections computed in Step 2. Then g = u + , h = v + and in Step 3, g = uv h/ h h, where h = v + , since (C) = 1. It follows that</p><formula xml:id="formula_29">g = uv v + /v + v + = u(v + -v -) v + /v + v + = u because v -v + = 0.</formula><p>2 We deviate slightly from MATLAB notation, where we must use mean(mean(A)).</p><p>Therefore, the final value entered in g will be u + , so there will be no change between steps. Similarly, the new value of h will be the original v + . Therefore, the approximations returned when the above process is applied to an arbitrary unit rank matrix will be u + and v + .</p><p>We finally sketch an extension of NNDSVD, we call 2-step NNDSVD, that can be especially useful when it becomes difficult or expensive to compute all leading k singular triplets of A. From Theorem 2 we know that not only the maximum but also the trailing singular triplet, ( 2 (C (j )</p><formula xml:id="formula_30">+ ), x 2 (C (j ) + ), y 2 (C (j )</formula><p>+ )), has strictly nonnegative components. Therefore, NNDSVD could be modified as follows: For j = 2, . . . until all k columns and rows of (W, H ) are filled, if the rank of C + (j ) is 1, initialize column j of W and row j of H with scalar multiples of the maximum left and right singular vectors of C (j ) + as is done in the original algorithm. If, however, the rank is 2, then columns and rows 2j, 2j + 1 of W and H are initialized with scalar multiples of x 1 (C (j )</p><formula xml:id="formula_31">+ ), x 2 (C (j ) + ) and y 1 (C (j ) + ) , y 2 (C (j )</formula><p>+ ) , respectively. If, for example, k is odd and all C (2) , . . . , C (k+1)/2 have rank-2, then these factors are enough to produce a nonnegative initialization for (W, H ). Note that using this approach, all singular vectors generating C (j ) + participate in the initialization hence the reconstruction is exact. 2-step NNDSVD leads to a different upper bound for the residual.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corollary 8. Given A ∈ R m×n</head><p>+ , and the pair (W, H ) initialized as in 2-step NNDSVD, then</p><formula xml:id="formula_32">E (k/2) F R F E (k/2) F + k/2 j =2 j . (<label>7</label></formula><formula xml:id="formula_33">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Beyond initialization</head><p>NNDSVD can be readily combined with any existing NMF algorithm. The ones selected for this paper are tabulated in Table <ref type="table" target="#tab_1">3</ref>. The first two (MM and AD) correspond to the additive and multiplicative updates proposed in Ref. <ref type="bibr" target="#b24">[25]</ref>. MM uses</p><formula xml:id="formula_34">H ← H. * ((W A)./(W WH)), W ← W. * ((AH )./(WHH )), (<label>8</label></formula><formula xml:id="formula_35">)</formula><p>where . * and ./ denote element by element multiplication and division, respectively. These updates do not increase the Frobenius norm of the residual A-WH F . We refer to the literature for a full description of the design and update formulas for the remaining methods in Table <ref type="table" target="#tab_1">3</ref>.</p><p>We finally mention that when the data matrix is symmetric, we might be seeking symmetric (e.g. A ≈ H H ) or weighted symmetric (e.g. A = H ZH ) nonnegative factorizations; see e.g. Ref. <ref type="bibr" target="#b36">[37]</ref>. Noting that matrix symmetry is inherited by the positive section, NNDSVD can be adapted to generate a symmetric initialization. Costs become lower because all necessary values are derived from the eigendecomposition rather than the SVD of A and only half the factors need be computed. </p><formula xml:id="formula_36">(A, WH) = A -WH F AD Additive [25] (A, WH) = D(A||WH) CNMF a Multiplicative [38] (A, WH) = 0.5( A -WH 2 F + W 2 F + H 2 F ) GD-CLS b</formula><p>Multipl./alternating least sq. <ref type="bibr" target="#b27">[28]</ref> (A, WH) = A -WH F nmfsc Sparse NMF from nmfpack <ref type="bibr" target="#b6">[7]</ref> (A, WH) = 0.5 A -WH 2  [46], also displayed in Fig. <ref type="figure" target="#fig_3">4</ref>. b Hyperspectral data collected from the U.S. Geological Survey (USGS) Digital Spectral Library. See also Refs. <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b20">21]</ref> regarding the spectral unmixing application.</p><p>c The dataset was described and used in Ref. <ref type="bibr" target="#b6">[7]</ref>. d Images (kindly provided by Prof. R.J. Plemmons and taken at the U.S. Air Force Maui Space Center) are from the space shuttle Columbia on its tragic final orbit, before disintegration upon re-entry in February 2003. Three sample images are depicted in Fig. <ref type="figure">1</ref>.</p><p>e Image datasets (SHUTTLE, IRIS, CBCL and NATURAL IMAGES) are vectorized so that each column corresponds to one image. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.">Computational costs</head><p>The two major computational steps of NNDSVD are (i) computing k largest singular triplets, and (ii) computing the maximum singular triplet of the positive section of each singular factor in the singular expansion of A. When appropriate (e.g. for large sparse data), we assume that the (partial) SVD is computed by means of some iterative algorithm; see e.g. Refs. <ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref>. Any improvement in algorithms that compute the above two steps will reduce the runtime of NNDSVD. A rough estimate of the cost of the first step above is O(kmn) for dense A. Hidden, in this notation, is a factor that depends on the number of iterations to convergence and which is unknown a priori. The other step can be performed very effectively, without ever computing explicitly the rank-2 matrices C (j )  (Theorem 2) at cost O(m+n). Thus the overall cost for NNDSVD on dense data is O(kmn). The asymptotic cost of structured CENTROID initialization <ref type="bibr" target="#b30">[31]</ref>, which relies on Skmeans, is also O(kmn), though the leading constants in the expressions are typically smaller. Since any initialization method is eventually linked with an NMF algorithm, for fairness we measure and take into account the initialization cost in terms of "it-eration equivalents" of the ensuing NMF, that is the number of iterations, say d, in the factorization algorithm, that could have been performed at the time it takes to initialize. We thus set d = 0 for RANDOM. In all NMF algorithms, update formulas were written so as to enforce a sequencing of operations that was appropriate for the problem dimensions. For example, since all datasets had k&gt; min(m, n), the update formulas computed W (H H ) rather than (WH)H . It is worth noting that depending on the dimensions and selected sequencing, the runtime differences can be significant. Therefore, the design of a MATLAB implementation, must enforce the sequencing by means of parentheses in the algebraic expression, or else, the default (left-to-right) will be used.</p><formula xml:id="formula_37">+ : specifically, if rank(C + ) = 2,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Numerical experiments</head><p>We ran several experiments with NMF algorithms (Fig. <ref type="figure">1</ref>) and datasets tabulated in Tables <ref type="table" target="#tab_1">3</ref> and<ref type="table" target="#tab_2">4</ref>. Table <ref type="table" target="#tab_3">5</ref> shows the figure number corresponding to results for specific   algorithm-dataset combinations. Each plot depicts the value of the corresponding objective function (see Table <ref type="table" target="#tab_1">3</ref> for details) vs. number of iterations. It also displays the value selected for parameter k and the measured value of d that must be taken into account when evaluating the results. When comparing the residual error curves of RANDOM, CENTROID and NNDSVD, we must shift appropriately: In particular, we must compare the errors at iteration j of NNDSVD and iteration j + d NNDSVD of RANDOM (the subscript distinguishes the d's). Similarly for CENTROID. Finally, when comparing the errors in CENTROID and NNDSVD, we must compare iteration j of the latter with iteration j + (d NNDSVDd CENTROID ) of the former. The platform used was a 2.0 GHz Pentium IV with 1024 MB RAM running Windows. Codes were written in MATLAB 7.0.1. We have been using a variety of methods to compute the partial SVD. In this paper, we used PROPACK <ref type="bibr" target="#b41">[42]</ref>; this MATLAB library is based on Lanczos bidiagonalization with partial reothogonalization and provides a fast alternative to MATLAB's native svds. Care is required when using any of these functions so that they are forced to return nonnegative leading singular vectors, since MATLAB can also return entirely nonpositive leading singular vectors. Their product is, of course, nonnegative. This is not sufficient for NNDSVD, because it utilizes the positive section of these vectors. In that case, nonpositive vectors return zero values. Therefore, to be cautious we use the absolute values of the leading singular vectors. A design choice we need to mention is that in CENTROID, H was initialized as random. This choice was dictated by the cost of intrinsic (lsqnonneg) and other off-the-shelf MAT-LAB functions (nnls and codes from Ref. <ref type="bibr" target="#b42">[43]</ref>) for solving the nonnegative least squares problem necessary to produce H from W. Specifically, their runtime was too high to make them viable as components of an initialization method. Furthermore we used an internally developed implementation of Skmeans clustering. The initializations used in the experiments are listed below. We note that the figures were selected after extensive experimentation with initializations, algorithms and datasets and selection of representative results.</p><p>RANDOM Initialize the pair (W, H ) to random using MAT-LAB's rand. CENTROID Initialize W as in Ref. <ref type="bibr" target="#b30">[31]</ref> and H as random.</p><p>Skmeans ran for 10 iterations. NNDSVD As specified in Table <ref type="table" target="#tab_0">1</ref>. NNDSVDa Perturbed NNDSVD using mean(A) (cf. Section 2.3). NNDSVDar Perturbed NNDSVD using values in [0, mean(A)/ 100] (cf. Section 2.3).</p><p>Fig. <ref type="figure" target="#fig_2">3</ref> shows experiments with MM. Fig. <ref type="figure">5</ref> shows the progress of the approximation when using data set IRIS (Fig. <ref type="figure" target="#fig_3">4</ref>) after RANDOM, NNDSVD and NNDSVDar initializations. The figure displays the basis images resulting from matricizing the columns of matrix W after the specified number of iterations. A noteworthy result is that different initializations lead to different basis images.  Figs. <ref type="figure">6</ref><ref type="figure">7</ref><ref type="figure">8</ref>show results for algorithms AD, CNMF and GD-CLS, respectively. Fig. <ref type="figure" target="#fig_5">9</ref> depicts results with the sparse algorithm. In this case we used basic NNDSVD and no variants, as justified in Section 2.3. The plots also show the selected values of parameters (sW, sH) that determine the desired sparsity level for (W, H ).</p><p>As alluded in Section 1, in the image datasets of Table <ref type="table" target="#tab_2">4</ref>, the data matrix A is constructed by stacking in columns the vectorized images in the collection, as is common practice in the NMF literature. The columns of W correspond to "basic images". Each image of the collection (column of A) is then approximated as a linear combination of the basis images using as reconstruction coefficients the elements in the corresponding column of H. Figs. <ref type="figure" target="#fig_1">2</ref> and<ref type="figure">5</ref>, for example, show the resulting basis images for the SHUTTLE and IRIS datasets. Dataset CLASSIC3 is a term-document matrix used as benchmark in text mining, hence each column of W encodes a basis document. Finally, in dataset USGS, each column encodes a "spectral signature" that becomes useful in spectral unmixing; cf. Refs. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30]</ref> for more information regarding these and other similar datasets in the context of NMF.</p><p>Overall, even taking into account the aforementioned d-shifts in iterations, the plots confirm that NNDSVD-type initializations are very fast. They also outperform RANDOM in all test cases, and generally appear to be a better choice (except when combined with GD-CLS) than CENTROID. They also need less time to reduce the NMF objective function, the improvements being most dramatic after only few iterations of the NMF algorithm as is evidenced by the pronounced "knee" shape of the corresponding error curves depicted in the figures, as well as in the visualization for dataset IRIS in Fig. <ref type="figure">5</ref>. Besides the gains in computational efficiency, sometimes algorithms based on NNDSVD appear to lead to smaller error than RANDOM and CENTROID, even at convergence; see e.g. Fig. <ref type="figure" target="#fig_5">9</ref>. One more feature that differentiates NNDSVD from RANDOM and CENTROID is that the basic method is deterministic. Finally, NNDSVD appears to be the first initialization scheme that comes with provable theoretical guarantees for the approximation error (Proposition 6).</p><p>Assuming that the reader proceeds with caution, since our evidence is entirely experimental and the heuristics are based solely on the sparsity structure of the dataset, the above experiments provide some guidance toward the selection of the most suitable NNDSVD variant. Specifically, if we seek some sparsity constraints, the basic NNDSVD algorithm appears well suited, since it tends to generate sparse factors. Otherwise, it is probably better to use one of NNDSVDa and NNDSVDar. In our experiments, NNDSVDar tends to return somewhat superior results than NNDSVDa. Nevertheless, after very extensive experiments, no algorithm emerged as an overall winner, so the final decision rests upon the user's experience with the variants' performance on the data under study. 3 . Fig. <ref type="figure" target="#fig_6">10</ref> illustrates the performance of all initialization methods used in this paper combined with algorithm CNMF. With dataset SHUTTLE (left), all methods reach about the same final error, though NNDSVDar has better performance and a pronounced knee behavior. With dataset CLASSIC3 (right), however, the error in RANDOM eventually catches up. As mentioned earlier, this behavior was the motivation behind the design of the NNDSVD variants; indeed, not only NNDSVDar has the most pronounced knee behavior but also results in the smallest final error.</p><p>We conclude that NNDSVD provides initial values that enable the followup NMF algorithms significant reduction of the initial residual after very few iterations and at low overall cost, at levels that are comparable to the residual obtained after running the algorithm to convergence with any of the three basic initializations. We also mention some further issues that have 3 After submission, we became aware of very recent work in Ref. <ref type="bibr" target="#b45">[47]</ref>,</p><p>including extensive experiments with some of the variants of NNDSVD described herein as well as others. We expect these to be useful to the potential user of our double SVD approach for initializing NMF.</p><p>arisen in the course of this investigation, such as the study and generalization of the property in Lemma 1, the application of clustering with NNDSVD in the spirit of CENTROID and the performance of the method in the context of distance metrics such as those in Ref. <ref type="bibr" target="#b0">[1]</ref>, that can be better tailored to prior knowledge about the data. Finally, it is worth reminding that even though NNDSVD frequently leads to errors that are comparable or superior to those achieved by random initialization, we did not provide any guarantees regarding the quality of the local minimum reached by the subsequent NMF algorithm. If the computed solution is unsatisfactory, one might conclude that NNDSVD hinders progress toward a better local minimum because its basic form is deterministic and does not allow for multiple runs. To the extent that the effectiveness of such a strategy to escape from local minima is justified, one could opt to use multiple runs of NNDSVDar, the randomized variant of NNDSVD, or simply repeatedly run the NMF algorithm with random initialization keeping track and finally comparing all results, including those obtained with NNDSVD. Clearly, the field is open for research contributions coupling deterministic initialization strategies with NMFs leading to even smaller approximation errors!</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fa</head><label></label><figDesc>Stands for constrained NMF. b Stands for gradient descent with constrained least squares.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Basis images for dataset SHUTTLE using Algorithm MM for k = 4.</figDesc><graphic coords="7,310.57,538.74,232.92,54.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Algorithm MM for datasets CLASSIC3, USGS, SHUTTLE and IRIS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Dataset of nine human eye iris images.</figDesc><graphic coords="8,97.12,515.57,141.84,106.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .Fig. 6 .Fig. 7 .</head><label>567</label><figDesc>Fig. 5. Progress of approximation on iris dataset using MM: Random (left); NNDSVDar (center); NNDSVD (right). Rows correspond to 0, 20, 40, 60 and 80 iterations using k = 3.</figDesc><graphic coords="9,139.07,71.64,307.44,123.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Sparse NMF algorithm, nmfsc, for dataset CLASSIC3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. NNDSVD, NNDSVDa, NNDSVDar and CENTROID on CNMF ( = = 0.5) for datasets SHUTTLE and CLASSIC3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>NNDSVD initialization of nonnegative matrix, in MATLAB notationInputs: Matrix A ∈ R m×n + , integer k &lt; min(m, n). Rank-k nonnegative factors W ∈ R m×k + , H ∈ R k×n</figDesc><table><row><cell>Output:</cell></row></table><note><p>+ . 1. Compute the largest k singular triplets of A: [U, S, V ] = psvd(A, k) 2. Initialize W (:, 1) = sqrt(S(1, 1)) * U(:, 1) and H (1, :) = sqrt(S(1, 1)) * V (:, 1) for j = 2 : k 3. x = U(:, j); y = V (:, j); 4. xp = pos(x); xn = neg(x); yp = pos(y); yn = neg(y); 5. xpnrm = norm(xp); ypnrm = norm(yp); mp = xpnrm * ypnrm; 6. xnnrm = norm(xn); ynnrm = norm(yn); mn = xnnrm * ynnrm; 7. if mp &gt; mn, u = xp/xpnrm; v = yp/ypnrm; sigma = mp; else u = xn/xnnrm; v = yn/ynnrm; sigma = mn; end 8. W (:, j) = sqrt(S(j, j ) * sigma) * u and H (j, :) = sqrt(S(j, j ) * sigma) * v ; end</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="2">NMF algorithms used in this paper</cell><cell></cell><cell></cell></row><row><cell>Name</cell><cell>Comments</cell><cell>Ref.</cell><cell>Distance metric (A, WH)</cell></row><row><cell>MM</cell><cell>Multiplicative</cell><cell>[25]</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>Datasets for experiments</cell><cell></cell><cell></cell></row><row><cell>Name</cell><cell>Matrix size</cell><cell>Comments</cell></row><row><cell>CLASSIC3</cell><cell>4299 × 3891</cell><cell>As specified in Ref. [32] using TMG [44]</cell></row><row><cell>IRIS a,e</cell><cell>10 800 × 9</cell><cell>9 human eye iris images of size 90 × 120</cell></row><row><cell>CBCL e</cell><cell>361 × 2429</cell><cell>2429 faces of size 19 × 19 from Ref. [45]</cell></row><row><cell>USGS b</cell><cell>256 × 500</cell><cell>(Hyperspectral) 500 spectra measured at 256 wavelengths</cell></row><row><cell>NATURAL IMAGES c,e</cell><cell>262 144 × 10</cell><cell>10 images of natural scenes of size 512 × 512</cell></row><row><cell>SHUTTLE d,e</cell><cell>16 384 × 16</cell><cell>16 shuttle images of size 128 × 128</cell></row></table><note><p>a Iris images from Ref.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell cols="3">Algorithm-dataset combinations and pointers to figures with results</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>CLASSIC3</cell><cell>USGS</cell><cell>SHUTTLE</cell><cell>CBCL</cell><cell>IRIS</cell><cell>NATURAL IMAGES</cell></row><row><cell>MM</cell><cell>3</cell><cell>3</cell><cell>3</cell><cell></cell><cell>3</cell></row><row><cell>AD</cell><cell></cell><cell></cell><cell></cell><cell>6</cell><cell></cell><cell>6</cell></row><row><cell>CNMF</cell><cell>7</cell><cell>7</cell><cell>7</cell><cell></cell><cell>7</cell></row><row><cell>GD-CLS</cell><cell></cell><cell></cell><cell></cell><cell>8</cell><cell></cell><cell>8</cell></row><row><cell>nmfsc</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>9 Fig. 1. Sample space shuttle Columbia images.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Fig. 8. Algorithm GD-CLS with = 0.01 for datasets CBCL and NATURAL IMAGES.</figDesc><table><row><cell></cell><cell>x 10 4</cell><cell></cell><cell cols="2">CBCL, k=5</cell><cell></cell><cell></cell><cell>x 10 4</cell><cell></cell><cell cols="2">Natural Images, k=4</cell><cell></cell></row><row><cell></cell><cell>3.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2.3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">RANDOM, d=0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RANDOM, d=0</cell></row><row><cell></cell><cell>3.2</cell><cell></cell><cell></cell><cell></cell><cell cols="2">CENTROID, d=4</cell><cell>2.2</cell><cell></cell><cell></cell><cell></cell><cell cols="2">CENTROID, d=7</cell></row><row><cell></cell><cell>3.1</cell><cell></cell><cell></cell><cell></cell><cell cols="2">NNDSVDar, d=4</cell><cell>2.1</cell><cell></cell><cell></cell><cell></cell><cell cols="2">NNDSVDar, d=7</cell></row><row><cell>objective function</cell><cell>2.7 2.8 2.9 3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>objective function</cell><cell>1.8 1.9 2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.7</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>200</cell><cell>400</cell><cell>600</cell><cell>800</cell><cell>1000</cell><cell>0</cell><cell>200</cell><cell>400</cell><cell>600</cell><cell>800</cell><cell>1000</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">iterations</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">iterations</cell><cell></cell></row><row><cell></cell><cell>x 10 5</cell><cell cols="4">CLASSIC3, k=10, sW=0.5, sH=0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2.61</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RANDOM, d=0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2.6</cell><cell></cell><cell></cell><cell></cell><cell>CENTROID, d=5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>NNDSVD, d=5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>objective function</cell><cell>2.57 2.58 2.59</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2.56</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2.55</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2.54</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>20</cell><cell>40</cell><cell>60</cell><cell>80</cell><cell>100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">iterations</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Discussions with and original experiments of our colleague, Dimitris Zeimpekis, were very helpful. We also thank him for providing us with MATLAB code for Skmeans. We thank Professors Robert J. Plemmons for his invaluable advice and encouragement, Daniel Szyld for his comments on an early version of this manuscript and Petros Drineas for many helpful observations. We are also grateful to Stefan Wild for providing us useful details about and codes from his original papers <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>. Finally, we thank the reviewers of the paper for their insightful comments. Research was supported in part by the University of Patras K. Karatheodori Grant no. B120.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generalized nonnegative matrix approximations with Bregman divergences</title>
		<author>
			<persName><forename type="first">I</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NIPS</title>
		<meeting>the NIPS<address><addrLine>Vancouver</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="283" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Optimality, computation, and interpretation of nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Diele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Plemmons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ragni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004-10">October 2004</date>
		</imprint>
	</monogr>
	<note>unpublished preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Orthogonal nonnegative matrix trifactorizations for clustering</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="126" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">When does non-negative matrix factorization give a correct decomposition into parts?</title>
		<author>
			<persName><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Stodden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Eldén</surname></persName>
		</author>
		<title level="m">Matrix Methods in Data Mining and Pattern Recognition</title>
		<meeting><address><addrLine>SIAM, Philadelphia, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Non-negative sparse coding</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Workshop on Neural Networks for Signal Processing</title>
		<meeting>the IEEE Workshop on Neural Networks for Signal Processing<address><addrLine>Martigny, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="557" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Non-negative matrix factorization with sparseness constraints</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1457" to="1469" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning the parts of objects by non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">401</biblScope>
			<biblScope unit="page" from="788" to="791" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast Newton-type methods for the least squares nonnegative matrix approximation problem</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 SIAM Conference on Data Mining</title>
		<meeting>the 2007 SIAM Conference on Data Mining<address><addrLine>SIAM, Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="343" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Marinetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Finesso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Marsilio</surname></persName>
		</author>
		<title level="m">Matrix factorization methods: application to thermal NDT/E, NDT&amp;E Int</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="611" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Positive matrix factorization: a non-negative factor model with optimal utilization of error estimates of data values</title>
		<author>
			<persName><forename type="first">P</forename><surname>Paatero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Tapper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Environmetrics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="111" to="126" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">One-sided non-negative matrix factorization and nonnegative centroid dimension reduction for text classification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Text Mining 2006 Workshop held with 6th SIAM International Conference on Data Mining (SDM 2006)</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Berry</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Castellanos</surname></persName>
		</editor>
		<meeting>the Text Mining 2006 Workshop held with 6th SIAM International Conference on Data Mining (SDM 2006)<address><addrLine>SIAM, Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Text mining using nonnegative matrix factorizations</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pauca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shahnaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Plemmons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th SIAM Conference on Data Mining</title>
		<meeting><address><addrLine>Orlando, FL, SIAM, Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-04">April 2004</date>
			<biblScope unit="page" from="452" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Nonnegative matrix factorization for rapid recovery of constituent spectra in magnetic resonance chemical shift imaging of the brain</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sajda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stoyanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shungu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Parra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1453" to="1465" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Document clustering using nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Shahnaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pauca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Plemmons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Process. Manage</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="373" to="386" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Document clustering based on non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGIR</title>
		<meeting>the 26th ACM SIGIR<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="267" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Two-dimensional non-negative matrix factorization for face representation and recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICCV&apos;05 Workshop on Analysis and Modeling of Faces and Gestures (AMFG&apos;05)</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting>the ICCV&apos;05 Workshop on Analysis and Modeling of Faces and Gestures (AMFG&apos;05)<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3723</biblScope>
			<biblScope unit="page" from="350" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">NMFLAB MATLAB toolbox for nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cichocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zdunek</surname></persName>
		</author>
		<ptr target="www.bsp.brain.riken.jp/ICALAB/nmflab.html/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<author>
			<persName><forename type="first">A</forename><surname>Pascual-Montano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Carmona-Saez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chagoyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tirado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carazo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascual-Marqui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">bioNMF: a versatile tool for nonnegative matrix factorization in biology</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">366</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Plemmons</surname></persName>
		</author>
		<title level="m">Nonnegative Matrices in the Mathematical Sciences</title>
		<meeting><address><addrLine>SIAM, Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Algorithms and applications for approximate nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Browne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Langville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Pauca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Plemmons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Stat. Data Anal</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="155" to="173" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semiring rank: Boolean rank and nonnegative rank factorization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Combin. Inf. System Sci</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="223" to="233" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Nonlinear programming</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bertsekas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<pubPlace>Athena Scientific, Belmont, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On the convergence of bound optimization algorithms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th Conference in Uncertainty in Artificial Intelligence (UAI &apos;03)</title>
		<meeting>the 19th Conference in Uncertainty in Artificial Intelligence (UAI &apos;03)<address><addrLine>Los Altos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="509" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Algorithms for non-negative matrix factorizations</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="556" to="562" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Existing and new algorithms for nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>University Texas at Austin</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">K-SVD and its non-negative variant for dictionary design</title>
		<author>
			<persName><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SPIE Conference</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Papadakis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Laine</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Unser</surname></persName>
		</editor>
		<meeting>the SPIE Conference</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">5914</biblScope>
			<biblScope unit="page" from="327" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unmixing spectral data for sparse low-rank non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">V</forename><surname>Pauca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Plemmons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Giffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hamada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Amos Technical Conference Maui</title>
		<meeting>the Amos Technical Conference Maui</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning sparse features for classification by mixture models</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Lett</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="155" to="161" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Seeding non-negative matrix factorizations with the spherical k-means clustering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wild</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>University of Colorado, Department of Applied Mathematics</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s Thesis</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improving non-negative matrix factorizations through structured intitialization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Curry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dougherty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2217" to="2232" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Concept decompositions for large sparse text data using clustering</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Modha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="143" to="175" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">CLSI: a flexible approximation scheme from clustered term-document matrices</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zeimpekis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gallopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th SIAM International Conference on Data Mining</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Kargupta</surname></persName>
		</editor>
		<meeting>the 5th SIAM International Conference on Data Mining<address><addrLine>SIAM, Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="631" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On reduced rank nonnegative matrix factorization for symmetric nonnegative matrices</title>
		<author>
			<persName><forename type="first">M</forename><surname>Catral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Plemmons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra Appl</title>
		<imprint>
			<biblScope unit="volume">393</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="126" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Sun</surname></persName>
		</author>
		<title level="m">Matrix Perturbation Theory</title>
		<meeting><address><addrLine>Boston</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Nonnegative ranks, decompositions, and factorizations of nonnegative matrices</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Rothblum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra Appl</title>
		<imprint>
			<biblScope unit="volume">190</biblScope>
			<biblScope unit="page" from="149" to="168" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On the equivalence of nonnegative matrix factorization and spectral clustering</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th SIAM International Conference on Data Mining</title>
		<meeting>the 5th SIAM International Conference on Data Mining<address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="606" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Nonnegative matrix factorization for spectral data analysis</title>
		<author>
			<persName><forename type="first">V</forename><surname>Pauca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Piper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Plemmons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra Appl</title>
		<imprint>
			<biblScope unit="volume">416</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="47" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">IRBL: an implicitly restarted block Lanczos method for large-scale Hermitian eigenproblems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Baglama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Calvetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Reichel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1650" to="1677" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Large scale singular value decomposition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Berry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Supercomput. Appl</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="13" to="49" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">V</forename><surname>Loan</surname></persName>
		</author>
		<title level="m">Matrix Computations</title>
		<meeting><address><addrLine>Baltimore</addrLine></address></meeting>
		<imprint>
			<publisher>The Johns Hopkins University Press</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
	<note>third ed.</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">PROPACK: a software package for the symmetric eigenvalue problem and singular value problems on Lanczos and Lanczos bidiagonalization with partial reorthogonalization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Larsen</surname></persName>
		</author>
		<ptr target="http://soi.stanford.edu/∼rmunk/PROPACK/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title/>
		<ptr target="http://www.mathworks.com/matlabcentral/fileexchange" />
	</analytic>
	<monogr>
		<title level="j">Mathworks MATLAB file exchange</title>
		<imprint/>
	</monogr>
	<note>accessed 3.1.07</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><surname>Tmg</surname></persName>
		</author>
		<ptr target="http://scgroup.hpclab.ceid.upatras.gr/scgroup/Projects/TMG/" />
		<title level="m">MATLAB Toolbox for generating term-document matrices from text collections</title>
		<imprint>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<ptr target="http://cbcl.mit.edu/cbcl/software-datasets/FaceData2.html" />
		<title level="m">CBCL face database # 1</title>
		<imprint>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Automated gene classification using nonnegative matrix factorization on biomedical literature</title>
		<author>
			<persName><forename type="first">K</forename><surname>Heinrich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007-05">May 2007</date>
			<pubPlace>Knoxville</pubPlace>
		</imprint>
		<respStmt>
			<orgName>The University of Tennessee</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. Thesis</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">As of September 2006, he is a Graduate Ph.D. student in Computer Science Department at Rensselaer Polytechnic Institute. About the Author-EFSTRATIOS GALLOPOULOS is a Professor at the University of Patras. Between 1985 and 1995 he held positions at UIUC and UCSB. His Ph.D. (computer science) is from the University of Illinois at Urbana-Champaign and his B.Sc. (1st class) from Imperial College</title>
	</analytic>
	<monogr>
		<title level="m">His research is on scientific computing, problem solving environments and parallel computing</title>
		<imprint>
			<date type="published" when="1983">1983. 2006</date>
		</imprint>
	</monogr>
	<note>He received a bachelor degree in computer science and engineering from the University of Patras in July</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
