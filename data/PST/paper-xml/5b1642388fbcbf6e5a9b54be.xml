<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reinforcement Learning for Relation Classification from Noisy Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jun</forename><surname>Feng</surname></persName>
							<email>feng-j13@mails.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">National Lab. for Information Science and Technology Dept. of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Lab. of Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
							<email>aihuang@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">National Lab. for Information Science and Technology Dept. of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Lab. of Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Li</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
							<email>yangya@zju.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Lab. for Information Science and Technology Dept. of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Lab. of Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Microsoft</forename><surname>Research</surname></persName>
						</author>
						<title level="a" type="main">Reinforcement Learning for Relation Classification from Noisy Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing relation classification methods that rely on distant supervision assume that a bag of sentences mentioning an entity pair are all describing a relation for the entity pair. Such methods, performing classification at the bag level, cannot identify the mapping between a relation and a sentence, and largely suffers from the noisy labeling problem. In this paper, we propose a novel model for relation classification at the sentence level from noisy data. The model has two modules: an instance selector and a relation classifier. The instance selector chooses high-quality sentences with reinforcement learning and feeds the selected sentences into the relation classifier, and the relation classifier makes sentencelevel prediction and provides rewards to the instance selector. The two modules are trained jointly to optimize the instance selection and relation classification processes. Experiment results show that our model can deal with the noise of data effectively and obtains better performance for relation classification at the sentence level.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Relation classification, aiming to categorize semantic relations between two entities given a plain text, is an important problem in natural language processing, particularly for knowledge graph completion and question answering. Most existing works for relation classification adopt supervised learning approaches, either based on traditional handcrafted features <ref type="bibr" target="#b12">(Mooney and Bunescu 2005;</ref><ref type="bibr" target="#b24">Zhou et al. 2005)</ref> or based on the features automatically generated by deep neural networks <ref type="bibr" target="#b22">(Zeng et al. 2014;</ref><ref type="bibr">dos Santos, Xiang, and Zhou 2015)</ref>, but all require high-quality annotated data.</p><p>In order to obtain large-scale training data, distant supervision <ref type="bibr" target="#b11">(Mintz et al. 2009</ref>) was proposed by assuming that if two entities have a relation in a given knowledge base, all sentences that contain the two entities will mention that relation. Although distant supervision is effective to label data automatically, it suffers from the noisy labeling problem. Taking the triple (Barack Obama, BornIn, United States) as an example, the noisy sentence "Barack Obamba is the 44th president of the United State" will be regarded as a positive instance by distant supervision and a BornIn relation is assigned to this sentence, although the sentence does not describe the relation BornIn at all. To address the issue of noisy labeling, previous studies adopt multi-instance learning to consider the noises of instances <ref type="bibr" target="#b13">(Riedel, Yao, and McCallum 2010;</ref><ref type="bibr" target="#b5">Hoffmann et al. 2011;</ref><ref type="bibr" target="#b15">Surdeanu et al. 2012;</ref><ref type="bibr" target="#b23">Zeng et al. 2015;</ref><ref type="bibr" target="#b9">Lin et al. 2016;</ref><ref type="bibr" target="#b6">Ji et al. 2017)</ref>. In these studies, the training and test process is proceeded at the bag level, where a bag contains noisy sentences mentioning the same entity pair but possibly not describing the same relation. As a result, previous studies suffer from two limitations: 1) Unable to handle the sentence-level prediction; 2) Sensitive to the bags with all noisy sentences which do not describe a relation at all.</p><p>To better explain the first limitation, we show an example in Figure <ref type="figure" target="#fig_0">1</ref>. Bag-level prediction can find the two relations "EmployedBy" and "BornIn" between the entity pair "Barack Obama" and "United States". However, sentencelevel prediction is able to further map each relation to the corresponding sentences. As for the second limitation, for each bag, previous bag-level methods retain at least one sentence, even if all the sentences in a given bag are noisy (not describing the relation). Such bags, produced by distant supervision, are quite common. For instance, our investigation on a widely used dataset<ref type="foot" target="#foot_0">1</ref> shows that 53% out of 100 sample bags have no sentences that describe the relation. Such noisy bags will definitely decrease the performance of relation classification.</p><p>In this paper, to handle the above two limitations, we propose a novel relation classification model consisting of two modules: instance selector and relation classifier. By having an explicit instance selector 2 , we are able to first select highquality sentences from a sentence bag, and then predict a relation at the sentence level by the relation classifier. To handle the second limitation, our instance selector will filter the entire bag if all sentences are labeled incorrectly. The major challenge here is how to train the two modules jointly, particularly when the instance selector has no explicit knowledge about which sentences are labeled incorrectly.</p><p>We address this challenge by casting the instance selection task as a reinforcement learning problem <ref type="bibr" target="#b16">(Sutton and Barto 1998)</ref>. Intuitively, although we do not have an explicit supervision for the instance selector, we can measure the utility of the selected sentences as a whole. Thus, the instance selection process has the following two properties: first, trial-and-error-search, meaning that the instance selector attempts to choose some sentences and obtain feedback (or reward) on the quality of the selected sentences from the relation classifier; second, the feedback from the relation classifier can be obtained only when we finish the instance selection process, which is typically delayed. These two properties naturally inspire us to utilize reinforcement learning techniques.</p><p>Our contributions in this work include: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Relation classification is a common task in natural language processing. Many approaches have been developed, particularly with supervised methods <ref type="bibr" target="#b12">(Mooney and Bunescu 2005;</ref><ref type="bibr" target="#b24">Zhou et al. 2005;</ref><ref type="bibr" target="#b21">Zelenko, Aone, and Richardella 2003)</ref>. However, such supervised methods heavily rely on highquality labeled data.</p><p>Recently, neural models have been widely applied to relation classification <ref type="bibr" target="#b22">(Zeng et al. 2014;</ref><ref type="bibr">dos Santos, Xiang, and Zhou 2015;</ref><ref type="bibr" target="#b12">Mooney and Bunescu 2005;</ref><ref type="bibr" target="#b20">Yang et al. 2016)</ref> including convolutional neural networks, recursive neural network <ref type="bibr" target="#b3">(Ebrahimi and Dou 2015;</ref><ref type="bibr" target="#b10">Liu et al. 2015)</ref>, and long short-term memory network <ref type="bibr" target="#b12">(Miwa and Bansal 2016;</ref><ref type="bibr" target="#b19">Xu et al. 2015;</ref><ref type="bibr" target="#b12">Miwa and Bansal 2016)</ref>. In <ref type="bibr" target="#b18">(Wang et al. 2016)</ref>, two levels of attention is proposed in order to better discern patterns in heterogeneous contexts for relation classification.</p><p>In general, a large amount of labeled data are required to train neural models, which is quite expensive. To address this issue, distant supervision was proposed (Mintz et   2 Instance is referred to a sentence in this paper. <ref type="bibr">al. 2009)</ref> by assuming that all sentences that mention two entities of a fact triple describe the relation in the triple. In spite of the success of distance supervision, such methods suffer from the noisy labeling issue. To alleviate this issue, many studies formulated relation classification as a multi-instance learning problem <ref type="bibr" target="#b13">(Riedel, Yao, and McCallum 2010;</ref><ref type="bibr" target="#b5">Hoffmann et al. 2011;</ref><ref type="bibr" target="#b15">Surdeanu et al. 2012;</ref><ref type="bibr" target="#b23">Zeng et al. 2015)</ref>. In <ref type="bibr" target="#b9">(Lin et al. 2016;</ref><ref type="bibr" target="#b6">Ji et al. 2017;</ref><ref type="bibr">Tianyu Liu and Sui 2017)</ref>, a sentence-level attention mechanism over multiple instances was proposed and incorrect sentences can be down-weighted. However, such multiinstance learning models all predict relations at the bag level but not at the sentence level, and they can not deal with the bags in which all sentences are not describing a relation at all. There are other approaches to reduce the noise of distant supervision using active learning <ref type="bibr" target="#b14">(Sterckx et al. 2014</ref>) and negative patterns <ref type="bibr" target="#b17">(Takamatsu, Sato, and Nakagawa 2012)</ref>.</p><p>Previous methods are all at the bag level but not at the sentence level and as such, they cannot find the exact mapping between a relation and a sentence. Furthermore, these methods are unable to handle the bags in which all the sentences are not describing the relation. To address these issues, we propose a new framework which first selects correct sentences in the framework of reinforcement learning <ref type="bibr" target="#b16">(Sutton and Barto 1998;</ref><ref type="bibr" target="#b13">Narasimhan, Yala, and Barzilay 2016)</ref> and then predicts relations from each sentence in the cleansed data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology</head><p>We propose a new relation classification framework, which is able to select correct sentences from noisy data for better relation classification. The proposed framework can predict relations at the sentence level from the cleansed data, rather than at the bag level. Sentence-level prediction is more friendly to the tasks that need to comprehend sentences such as question answering and semantic parsing.</p><p>Our framework consists of two key modules: the instance selector which selects correct sentences from noisy data, and the relation classifier which predicts relation and updates its parameters with cleaned data. The two modules interacts with each other during the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem Definition</head><p>Formally, we decompose the task of relation classification into two sub-problems in this paper: instance selection and relation classification.</p><p>We formulate the instance selection problem as follows: given a set of &lt;sentence, relation label&gt; pairs as X = {(x 1 , r 1 ), (x 2 , r 2 ), . . . , (x n , r n )}, where x i is a sentence associated with two entities (h i , t i ) and r i is a noisy relation label produced by distant supervision. The goal is to determine which sentence truly describes the relation and should be selected as a training instance.</p><p>The relation classification problem is formulated as follows: given a sentence x i and the mentioned entity pair (h i , t i ), the goal is to predict the semantic relation r i in x i . Essentially, the model estimates the probability: </p><formula xml:id="formula_0">p Φ (r i |x i , h i , t i ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overview</head><p>The proposed model is based on a reinforcement learning framework and consists of two components: the instance selector and the relation classifier. In the instance selector, each sentence x i has a corresponding action a i to indicate whether or not x i will be selected as a training instance for relation classification. The state s i is represented by the current sentence x i , the already chosen sentences among {x 1 , • • • , x i−1 }, and the entity pair h i and t i in sentence x i . The instance selector samples an action given the current state according to a stochastic policy. For the relation classifier, it adopts a convolutional architecture to automatically determine the semantic relation for an entity pair in a given sentence. The instance selector distills the training data to the relation classifier to train the convolutional neural network. Meanwhile, the relation classifier gives feedback to the instance selector to refine its policy function. Figure <ref type="figure" target="#fig_1">2</ref> gives an illustration of how the proposed framework works.</p><p>With the help of the instance selector, our method directly filters out noisy sentences. Unlike reducing the weights of noisy sentences <ref type="bibr" target="#b9">(Lin et al. 2016)</ref> or retaining one sentence in a bag <ref type="bibr" target="#b22">(Zeng et al. 2014</ref>), our method is better at dealing with noisy data. The relation classifier is trained and tested at the sentence level on the cleansed data, whereas previous models treat the sentence bag as a whole and predict relation at the bag level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instance Selector</head><p>We cast instance selection as a reinforcement learning problem. The instance selector is the agent, who interacts with the environment that consists of data and the relation classifier. The agent follows a policy to decide which action (choosing the current sentence or not) at each state (consisting of the current sentence, the chosen sentence set, and the entity pair), and then receive a reward from the relation classifier at the terminal state when all the selections are made.</p><p>As aforementioned, we can obtain a delayed reward from the relation classifier only when the selection on all the training instances are finished. Thus, we can only update the pol-icy function once for each scan of the entire training data, which is obviously inefficient. To obtain more feedbacks and to make the training process more efficiently, we split the training sentence instances X = {x 1 , . . . , x n } into N bags B = {B 1 , B 2 , . . . , B N } and compute a reward when we finish data selection in a bag. Each bag corresponds to a distinct entity pair, and each bag B k is a sequence of sentences</p><formula xml:id="formula_1">{x k 1 , x k 2 , . . . , x k |B k | }</formula><p>with the same relation label r k , however, the relation label is noisy. We define the action as selecting a sentence or not according to a policy function. The reward is computed once the selection decisions are completed on one bag. When the training process of the instance selector is completed, we merge all the selected sentences in each bag to obtain a cleansed dataset X. Then, the cleansed data will be used to train the relation classifier at the sentence level.</p><p>We will introduce (i.e., state, action, and reward) as follows. To be clear, we will omit the superscript k which denotes the bag index. Thus, the formulation hereafter is based on only one bag. State. The state s i represents the current sentence, the already selected sentences, and the entity pair when making decision on the i-th sentence of the bag B. We represent the state as a continuous real-valued vector F (s i ), which encodes the following information: 1) The vector representation of the current sentence, which is obtained from the nonlinear layer of the CNN for relation classification; 2) The representation of the chosen sentence set, which are the average of the vector representations of all chosen sentences; 3) The vector representations of the two entities in a sentence, obtained from a pre-trained knowledge graph embedding table. Action. We define an action a i ∈ {0, 1} to indicate whether the instance selector will select the i-th sentence of the bag B or not. We sample the value of a i by its policy function π Θ (s i , a i ), where Θ is the parameters to be learned. In this work, we adopt a logistic function as the policy function:</p><formula xml:id="formula_2">π Θ (s i , a i ) = P Θ (a i |s i ) = a i σ(W * F (s i ) + b) + (1 − a i )(1 − σ(W * F (s i ) + b)) (1)</formula><p>where F (s i ) is the state feature vector, and σ(.) is the sigmoid function with the parameter Θ = {W , b}.</p><p>Reward. The reward function is an indicator of the utility of the chosen sentences. For certain bag B = {x 1 , . . . , x |B| }, we sample an action for each sentence, to determine whether the current sentence should be selected or not. We assume that the model has a terminal reward when it finishes all the selection. Therefore we only receive a delayed reward at the terminal state s |B|+1 . The reward is zero at other states. Therefore, the reward is defined as follows:</p><formula xml:id="formula_3">r(s i |B) = ⎧ ⎨ ⎩ 0 i &lt; |B| + 1 1 | B| xj ∈ B log p(r|x j ) i = |B| + 1 (2)</formula><p>where B is the set of selected sentences, which is a subset of B, and r is the relation label of bag B. As shown in Figure <ref type="figure" target="#fig_1">2</ref>, p(r|x j ) is calculated by the relation classifier which is given by a CNN model. For the special case B = ∅, we set the reward as the average likelihood of all sentences in the training data, which enables our instance selector to exclude noisy bag effectively. Note that the relation classifier is at the sentence-level since it computes p(r|x) for each sentence. The reward is computed on a new bag of sentences selected by the instance selector. Essentially, the above reward evaluates the overall utility of all the actions made by the policy. It supervises the instance selector to maximize the average likelihood of the chosen instances, which makes the objective function of the instance selector consistent with the relation classifier.</p><p>In the selection process, not only the final action contributes to this reward, but also all the previous actions do. Therefore, this reward is delayed, and can be handled very well by reinforcement learning techniques <ref type="bibr" target="#b16">(Sutton and Barto 1998)</ref>.</p><p>Optimization. For a bag B, we aim to maximize the expected total reward. More formally, our objective function is defined as</p><formula xml:id="formula_4">J(Θ) = V Θ (s 1 |B) = E s1,a1,s2,...,si,ai,si+1... [ |B|+1 i=0 r(s i |B)] (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>where</p><formula xml:id="formula_6">a i ∼ π Θ (s i , a i ), s i+1 ∼ P (s i+1 |s i , a i ).</formula><p>The transition functions P (s i+1 |s i , a i ) are equal to 1, since the state s i+1 is fully determined by the state s i and a i . V Θ is the value function, and V Θ (s 1 |B) represents the expected future total reward that we can obtain by starting at certain state s 1 following policy π Θ (s i , a i ).</p><p>According to the policy gradient theorem <ref type="bibr" target="#b16">(Sutton et al. 1999</ref>) and the REINFORCE algorithm (Williams 1992), we compute the gradient in the following way. For each bag B, we sample an action for each state sequentially according to the current policy. We then get a sampled trajectory {s 1 , a 1 , s 2 , a 2 , ..., s |B| , a |B| , s |B|+1 } and a corresponding terminal reward r(s |B|+1 |B). Since we only have a non-zero terminal reward, the value function is the same for all states from s 1 to s |B| , namely v i = V (s i |B) = r(s |B|+1 |B), for i = 1, 2, ..., |B|. We update the current policy using the following gradient:</p><formula xml:id="formula_7">Θ ← Θ + α |B| i=1 v i ∇ Θ log π Θ (s i , a i ) (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation classifier</head><p>In the relation classifier, we adopt a CNN architecture to predict relations. The CNN network has an input layer, a convolution layer, a max pooling layer and a non-linear layer from which the representation is used for relation classification.</p><p>Input layer. For each sentence x, we represent it as a list of vectors x = (w 1 , w 2 , . . . , w m ). Each representation vector consists of two parts: one is the word embedding; the other is the position embedding. Word embeddings are obtained from word2vec 3 , and the dimension is d w . Similar to  <ref type="bibr" target="#b22">(Zeng et al. 2014)</ref>, we use d p -dimensional position embeddings, which are vector representations of the relative distances from the current word respectively to the head or tail entities in this sentence. We concatenate the word and position embeddings of each word to form a new vector w i (w i ∈ R d , and d = d w + 2 × d p ), and then input these vectors to the CNN model. CNN. In order to obtain high-level and abstractive representation of the raw input of a sentence, we apply a CNN structure for relation classification. This can be briefly described as below:</p><formula xml:id="formula_8">Θ = τ Θ + (1 − τ )Θ Φ = τ Φ + (1 − τ )Φ end</formula><formula xml:id="formula_9">L = CNN(x) (5)</formula><p>where x is the input vectors as described in the input layer and L ∈ R d s is the output of the max pooling layer. In this structure, there is a convolution layer, and a max pooling layer. The convolution operation is performed on 3 consecutive words, and the number of feature maps d s is set to 230, the same as the setting of <ref type="bibr" target="#b9">(Lin et al. 2016)</ref>. Hence, the convolution parameters are</p><formula xml:id="formula_10">W f ∈ R d s ×(3d) and b f ∈ R d s .</formula><p>Then, the probability for relation prediction p(r|x; Φ) is given as follows:</p><formula xml:id="formula_11">p(r|x; Φ) = sof tmax(W r * tanh(L) + b r ) (6)</formula><p>where W r ∈ R nr×d s and b r ∈ R nr are parameters in the fully-connected layer, n r is the total number of relation types, and</p><formula xml:id="formula_12">Φ = {W f , b f , W r , b r }.</formula><p>The key difference between our relation classifier and other studies lies in that our classifier performs relation classification at the sentence level. The input to the relation classifier in other studies is a bag of sentences. Instead, the input to ours is just one sentence, since we already filter out noisy sentences with the instance selector. Loss function. Given the selected training set { X} provided by the instance selector, we define the objective function of the relation classifier using cross-entropy as follows:</p><formula xml:id="formula_13">J (Φ) = − 1 | X| | X| i=1 log p(r i |x i ; Φ) (7)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Training</head><p>As the instance selector and the relation classifier are correlated mutually, we train them jointly.The complete joint training process is described in Algorithm 1. To optimize the policy network in the instance selector, we use a Monto-Carlo based policy gradient method (Williams 1992), which favors actions with high sampled reward. To optimize the CNN component, we use a gradient descent method to minimize the objective function (i.e., Eq. 7). We pre-train the model before the joint training process starts. We first pretrain the CNN in the relation classifier, and then pre-train the policy function by computing the reward with the pretrained CNN, while the parameters of the CNN model are frozen. At last, we jointly train the instance selector and the relation classifier. We found such a pre-training strategy is quite crucial for our method, which is also widely recommended by many other reinforcement learning studies <ref type="bibr" target="#b0">(Bahdanau et al. 2016)</ref>. Algorithm 2 presents the details of the joint training process. The relation classifier provides a mechanism of computing the rewards of the selected sentences to refine the instance selector. The instance selector chooses high-quality data by excluding wrongly labeled sentences to better train the relation classifier. In order to have a stable update, we take advantage of a target policy network and a target CNN with parameter sets Θ and Φ respectively, similar to <ref type="bibr" target="#b8">(Lillicrap et al. 2015)</ref>. The parameters in the target networks are updated much more slowly than the original ones. We update Θ and Φ by linear interpolation:</p><formula xml:id="formula_14">Θ ← (1 − τ )Θ + τ Θ and Φ ← (1 − τ )Φ + τ Φ, where τ</formula><p>1 is a hyper-parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Experiment Setup</head><p>Dataset. To evaluate our model, we adopted a widely used dataset<ref type="foot" target="#foot_1">4</ref> generated by the sentences in NYT<ref type="foot" target="#foot_2">5</ref> and developed by <ref type="bibr" target="#b13">(Riedel, Yao, and McCallum 2010)</ref>. There are 522,611 sentences, 281,270 entity pairs, and 18,252 relational facts in the training data; and 172,448 sentences, 96,678 entity pairs and 1,950 relational facts in the test data. Among the data, there are 39,528 unique entities and 53 unique relations from Freebase including a special relation NA that signifies no relation between two entities in a sentence.</p><p>Word and entity embedding. We adopted word2vec to train the word embeddings on the NYT corpus. For entity embedding, we implemented the TransE model <ref type="bibr" target="#b1">(Bordes et al. 2013</ref>) and trained it on a set of Freebase fact triples whose entities have been mentioned in the training and test data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence-Level Relation Classification</head><p>As discussed previously, the key difference between our method and other models lies in that our method can perform sentence-level relation classification. We conducted manual evaluation on relation classification in this section. Evaluation settings. We predicted a relation label for each sentence, instead of for each bag. For example, the task in Figure <ref type="figure" target="#fig_0">1</ref> needs to map the first sentence to relation "BornIn" and the second sentence to "EmployedBy".</p><p>Since the data obtained from distant supervision are noisy, we randomly chose 300 sentences and manually labeled the relation type for each sentence to evaluate the classification performance. We adopted accuracy and macro-averaged F 1 as the evaluation metric. Baselines. We adopted three state-of-the-art baselines: over the sentences in a bag and thus can down weight noisy sentences in a bag. CNN is a sentence-level model that is trained directly on noisy data. For bag-level models (CNN+Max and CNN+ATT), the training process is the same as the referenced papers. During test, each sentence is treated as a bag and a relation is predicted for each bag. In this scenario, the bag-level relation prediction is exactly the same as the sentence-level prediction. All the baselines were implemented with the source codes released by <ref type="bibr" target="#b7">(Li et al. 2016)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results. Results in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instance Selection</head><p>We then evaluated the effectiveness of our instance selector from several aspects. First, we evaluated whether the selected data by our instance selector are better for relation classification. Second, we justified the accuracy of selection decision in the selector by manually checking the decisions on sentences. Third, we compared the proposed RL selection strategy in our selector with greedy selection. Last, we assessed whether the selector has the ability of filtering those bags that contain all noisy sentences. Relation classification on selected data. To measure the quality of the selected data by our instance selector, we performed relation classification experiments on the selected data. We first used our instance selector to select the high-quality sentences from the original data. Then, we trained two state-of-the-art models, CNN and CNN+ATT with two settings. One setting is to train them on the original data, named as CNN(Original) and CNN+ATT(Original). The other setting is to train them on the selected data, which are named as CNN(Selected) and CNN+ATT(Seleted). We compared the performance of CNN(Original) (CNN+ATT(Original)) with CNN(Selected) (CNN+ATT(Selected)) on the relation classification task.</p><p>The results are compared under the held-out evaluation configuration <ref type="bibr" target="#b11">(Mintz et al. 2009</ref>) which provides an approximate measure of relation classification without expensive  human annotations. The held-out evaluation compares the predicted relational fact from the test data with the facts in Freebase, but it does not consider the mapping between a relational fact and a sentence. As shown in Figure <ref type="figure" target="#fig_3">3</ref> and Figure <ref type="figure" target="#fig_4">4</ref>, the models trained on the selected data achieve much better performance than the counterparts trained on the original dataset. The results also indicate our instance selector has the ability of filtering out noisy sentences and distilling high-quality sentences, resulting better classification performance. Accuracy of instance selection decision. To assess how accurate the decision is by the instance selector, we manually checked each sentence selected and rejected by the instance selector in a sampled dataset. For each sentence, the instance selector makes a correct decision if the sentence's label is correct and our instance selector selects it as a training instance, or, if its label is wrong and our instance selector rejects it. Otherwise, we judged that the instance selector makes a wrong decision.</p><p>Specifically, we sampled 300 sentences from the training data. Our instance selector chooses 64 sentences as the training instances, among which 45 sentences are correctly selected. The selector also rejects 236 instances, and 177 of them are noisy instances (not describing the relation). To summarize, the accuracy of our instance selector is (45 + 177)/300 = 74%, which demonstrates the effectiveness of our instance selector. Different instance selection strategies. To show the ne-Bag I (Entity Pair: fabrice santor, france; Relation:/people/person/nationality) CNN+RL CNN+ATT CNN+Max though not without some struggle, federer, the world 's top-ranked player, advanced to the fourth round with a thrilling, victory over the crafty fabrice santoro of france, who is ranked 76th. 1 0.60 0 in his quarterfinal , nalbandian overwhelmed unseeded fabrice santoro of france 1 0.39 1 fabrice santoro, 33 , of france finally reached the quarterfinals in a major on his 54th attempt by defeating the 11th-seeded spaniard david ferrer 1 0.01 0</p><p>Bag II (Entity Pair: jonathan littel, france; Relation:/people/person/nationality) jonathan littell, a new york-born writer whose french-language novel about a murderous and degenerate officer has been the sensation of the french publishing season, on monday became the first american to win france's most prestigious literary award, the prix goncourt 0 0.89 1 after a languid intercontinental auction that stretched for more than a week, the american rights to jonathan littell's novel les bienveillantes, which became a publishing sensation in france, have been sold to harpercollins, the publisher confirmed yesterday.  During the experiments, we kept the relation classifier untouched while replacing the RL selection by greedy selection. The number of selected instances N is the same as the RL strategy. As shown in Figure <ref type="figure" target="#fig_6">5</ref>, the performance of our instance selector is much better than the greedy strategy on the held-out evaluation. The results show that our RL strategy is reasonable and effective. Noisy bag filtering. As previous methods cannot filter the bags with all noisy sentences, we validated the ability of our model to filter bags with all noisy sentences. We randomly selected 100 deleted sentence bags and find that 86% of the bags consist of all noisy sentences. This indicates that our instance selector can exclude the noisy sentences effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case Study</head><p>Table <ref type="table">2</ref> shows two bag examples for instance selection. The first bag has three correct sentences. The second bag has two noisy sentences. It is clearly show that our model can do better instance selection than both instance-weighting with CNN+ATT and maximum likelihood selection with CNN+Max. The second example indicates that our model is able to filter bags with all noisy sentences while other methods fail to do so. Further, our solution for instance selection can be generalized to other tasks that employ noisy data or distant supervision. For instance, a possible attempt might be to perform sentiment classification on noisy data <ref type="bibr" target="#b4">(Go, Bhayani, and Huang 2009)</ref>. We leave this as our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion and Future Work</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Bag-level: Relations are mapped to a bag of sentences, each of which contains the same entity pair; Sentence-level: Each sentence is mapped to a specific relation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overall process. The instance selector chooses sentences according to a policy function, and then the selected sentences are used to train a better relation classifier. The instance selector updates its parameters, with a reward computed from the relation classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Model pre-training. As described in Algorithm 2, we pretrained the relation classifier and instance selector before the joint training process. As the reward is calculated based on the CNN model in the relation classifier, we first pre-trained the CNN model on the entire training data. Then, we fixed the parameters of the CNN model and pre-trained the policy function in the instance selector where the reward is obtained from the fixed CNN model. Parameter setting. Similar to previous studies, we tuned our model using three-fold cross validation. For the parameters of the instance selector, we set the dimension of entity embedding as 50, the learning rate as 0.02/0.01 at the pretraining stage and joint training stage respectively. The delay coefficient τ is 0.001. For the parameters of the relation classifier, the word embedding dimension d w = 50 and the position embedding dimension d p = 5. The window size of the convolution layer l is 3. The learning rate of the instance selector is α = 0.02 both at the pre-training and joint training stage. The batch size is fixed to 160. The training episode number L = 25. We employed a dropout strategy with a probability of 0.5 during the training of the CNN component.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparison between the CNN model trained on the original and selected data.</figDesc><graphic url="image-1.png" coords="6,355.32,53.71,133.67,125.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparison between the CNN+ATT model trained on the original and selected data.</figDesc><graphic url="image-3.png" coords="6,355.32,213.49,133.67,125.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Instance selection examples by different models. For CNN+RL and CNN+Max, 1 or 0 means the sentence is selected or not. For CNN+ATT, the value is the attention weight.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comparison of instance selection with reinforcement learning against greedy selection.</figDesc><graphic url="image-5.png" coords="7,89.82,225.73,133.70,125.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>In this paper, we propose a novel model for sentence-level relation classification from noisy data using a reinforcement learning framework. The model consists of an instance selector and a relation classifier. The instance selector chooses high-quality data for the relation classifier. The relation classifier predicts relation at the sentence level and provides rewards to the selector as a weak signal to supervise the instance selection process. Extensive experiments demonstrate that our model can filter out the noisy sentences and perform sentence-level relation classification better than state-of-theart baselines from noisy data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Episode number L. Training data B = {B 1 , B 2 , . . . , B N }. A CNN and a policy network model parameterized by Φ and Θ, respectively Initialize the target networks as: Φ = Φ, Θ = Θ for episode l = 1 to L do Shuffle B to obtain the bag sequence B = {B 1 , B 2 , . . . , B N } foreach B |B|+1 |B) end Update Φ in the CNN model Update the weights of the target networks:</figDesc><table><row><cell>ALGORITHM 1: Overall Training Procedure</cell></row><row><cell>1. Initialize the parameters of the CNN model of relation</cell></row><row><cell>classifier and the policy network of instance selector with</cell></row><row><cell>random weights respectively</cell></row><row><cell>2. Pre-train the CNN model to predict relation ri given the</cell></row><row><cell>sentence xi by maximizing log p(ri|xi)</cell></row><row><cell>3. Pre-train the policy network by running Algorithm 2</cell></row><row><cell>with the CNN model fixed.</cell></row><row><cell>4. Run Algorithm 2 to jointly train the CNN model and the</cell></row><row><cell>policy network until convergence</cell></row><row><cell>ALGORITHM 2: Reinforcement Learning Algo-</cell></row><row><cell>rithm for the Instance Selector</cell></row></table><note>3 https://code.google.com/p/word2vec/ Input: k ∈ B do Sample instance selection actions for each data instance in B k with Θ : (To be clear, we omit the superscript k below) A = {a1, . . . , a |B| }, ai ∼ π Θ (si, ai) Compute delayed reward r(s |B|+1 |B) Update the parameter Θ of instance selector: Θ ← Θ + α i vi∇Θ log πΘ(si, ai), where vi = r(s</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>• CNN (Zeng et al. 2014) is a sentence-level classification model. It does not consider the noisy labeling problem. • CNN+Max (Zeng et al. 2015) is a bag-level classification model. It assumes that there is one sentence describing the relation in a bag. It chooses the most correct sentence in each bag. • CNN+ATT (Lin et al. 2016) is also a bag-level model, similar to CNN+Max. It adopts a sentence-level attention Performance on sentence-level relation classification.</figDesc><table><row><cell>Method</cell><cell cols="2">Macro F1 Accuracy</cell></row><row><cell>CNN</cell><cell>0.40</cell><cell>0.60</cell></row><row><cell>CNN+Max</cell><cell>0.06</cell><cell>0.34</cell></row><row><cell>CNN+ATT</cell><cell>0.29</cell><cell>0.56</cell></row><row><cell>CNN+RL(ours)</cell><cell>0.42</cell><cell>0.64</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Table 1 reveal the following observations. • CNN+RL obtains superior performance than CNN, indicating that filtering noisy data by instance selection benefits the task. • CNN+RL outperforms CNN+Max and CNN+ATT remarkably. It shows the effectiveness of instance selection with reinforcement learning. • The sentence-level models (CNN and CNN+RL) perform much better than the bag-level models (CNN+Max and CNN+ATT), indicating that bag-level models do not perform well for sentence-level prediction.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">http://iesl.cs.umass.edu/riedel/ecml/ The Thirty-Second AAAI Conference on Artificial Intelligence </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1">http://iesl.cs.umass.edu/riedel/ecml/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2">New York Times, a widely used text corpus.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was partly supported by the National Science Foundation of China under grant No.61272227/61332007.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">An actorcritic algorithm for sequence prediction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.07086</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Classifying relations by ranking with convolutional neural networks</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="626" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Chain based RNN for relation classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1244" to="1249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Twitter sentiment classification using distant supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Go</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bhayani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>Cs224n Project Report</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Knowledge-based weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction with sentence-level attention and entity descriptions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3060" to="3066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for dialogue generation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1192" to="1202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.02971</idno>
		<title level="m">Continuous control with deep reinforcement learning</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural relation extraction with selective attention over instances</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2124" to="2133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A dependency-based neural network for relation classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="285" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">End-to-end relation extraction using lstms on sequences and tree structures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Bunescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2005">2016. 2005</date>
			<biblScope unit="page" from="171" to="178" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving information extraction by acquiring external evidence with reinforcement learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.07954</idno>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2016. 2010</date>
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Modeling relations and their mentions without labeled text</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Using active learning and semantic clustering for noise reduction in distant supervision</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sterckx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Develder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
	<note>In 4e Workshop on Automated Base Construction at NIPS2014 (AKBC-2014</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-instance multi-label learning for relation extraction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Policy gradient methods for reinforcement learning with function approximation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<publisher>MIT press Cambridge</publisher>
			<date type="published" when="1998">1998. 1999</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Reinforcement learning: An introduction</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A softlabel method for noise-tolerant distantly supervised relation extraction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Takamatsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nakagawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<editor>
			<persName><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kexiang</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Sui</surname></persName>
		</editor>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012">2012. 2017. 17911796</date>
			<biblScope unit="page" from="721" to="729" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="1992">2016. 1992</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="229" to="256" />
		</imprint>
	</monogr>
	<note>Relation classification via multi-level attention cnns</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Classifying relations via long short term memory networks along shortest dependency paths</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1785" to="1794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A position encoding convolutional neural network based on dependency tree for relation classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="65" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Kernel methods for relation extraction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zelenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Aone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Richardella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1083" to="1106" />
			<date type="published" when="2003-02">2003. Feb</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction via piecewise convolutional neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="17" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Exploring various knowledge in relation extraction</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="427" to="434" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
