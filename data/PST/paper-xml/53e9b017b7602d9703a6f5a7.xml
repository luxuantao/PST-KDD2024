<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Learning from Temporal Coherence in Video</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
							<email>hmobahi2@uiuc.edu</email>
						</author>
						<author>
							<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
							<email>collober@nec-labs.com</email>
						</author>
						<author>
							<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
							<email>jasonw@nec-labs.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<postCode>61801</postCode>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">NEC Labs America</orgName>
								<address>
									<addrLine>4 Independence Way</addrLine>
									<postCode>08540</postCode>
									<settlement>Princeton</settlement>
									<region>NJ</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Learning from Temporal Coherence in Video</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work proposes a learning method for deep architectures that takes advantage of sequential data, in particular from the temporal coherence that naturally exists in unlabeled video recordings. That is, two successive frames are likely to contain the same object or objects. This coherence is used as a supervisory signal over the unlabeled data, and is used to improve the performance on a supervised task of interest. We demonstrate the effectiveness of this method on some pose invariant object and face recognition tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the availability of ever increasing computing power, large-scale object recognition is slowly becoming a reality. Several massive databases have been introduced for that purpose. Huge hand-labeled datasets like NORB <ref type="bibr" target="#b15">(LeCun et al., 2004</ref>) can be constructed e.g. by considering a few objects and varying the pose (using a rotating platform) as well as lighting conditions, the presence of clutter and the background. Scaling to more classes has been shown to be possible using click-through data from search engines, like in the recent 80 million tiny images <ref type="bibr" target="#b22">(Torralba et al., 2008)</ref> database. Nevertheless, labeling images remains expensive, which motivates research into ways of leveraging the cheap and basically infinite source of unlabeled images available in the digital world.</p><p>Classical semi-supervised learning <ref type="bibr" target="#b9">(Chapelle et al., 2006)</ref> and transduction <ref type="bibr" target="#b23">(Vapnik, 1995)</ref>  beled and unlabeled data, which assume each unlabeled example belongs to one of the labeled classes that are considered. If the unlabeled data is coming from heterogeneous sources then in general no class membership assumptions can be made (the unlabeled image might not belong to any of the considered classes), and one cannot rely on these methods. There are however cases where unlabeled data has a useful underlying structure which can be exploited. One example is video, a source of images constrained by temporal coherence: two successive frames are very likely to contain similar contents and represent the same concept classes. Each object in the video is also likely to be subject to small transformations, such as translation, rotation or deformation over neighboring frames. This sequential data thus provides a signal from which to learn a representation invariant to these changes. In this paper we propose a learning method that can leverage temporal coherence in video to boost the performance of object recognition tasks.</p><p>We choose a deep convolutional network architecture <ref type="bibr" target="#b14">(LeCun et al., 1998)</ref>, well suited for large-scale object recognition. We propose a training objective with a temporal coherence regularizer added to a typical training error minimizing objective, resulting in a modified backpropagation rule. While this paper focuses on object recognition and the use of video, the proposed algorithm can be applied to other sequential data with temporal coherence, and when minimizing other choices of loss.</p><p>We report experimental results on a visual object recognition task, COIL100 <ref type="bibr" target="#b16">(Nayar et al., 1996)</ref>, where the goal is to recognize objects regardless of their pose. We show that in this case, temporal coherence in video acts as a very good regularizer, and that, without using hand-crafted or strongly engineered features one can produce models that compete with state-of-the-art hand-designed methods like VTU <ref type="bibr" target="#b24">(Wersing &amp; Körner, 2003)</ref>. We then go on to investigate how the choice of video source can affect this result, i.e. we measure the relative performance depending on the source of the video. To do this we recorded our own datasets of real rotating 3D objects, of differing degrees of similarity to COIL, and show how this unlabeled supplementary video also improves performance on our supervised classification task. We also report experiments on the ORL face dataset <ref type="bibr" target="#b20">(Samaria &amp; Harter, 1994)</ref>, with similar results.</p><p>From a biological point of view, several authors <ref type="bibr" target="#b12">(Hinton &amp; Sejnowski, 1999;</ref><ref type="bibr" target="#b0">Becker, 1996a)</ref> argue that pure supervised learning is a poor model of how animals really learn. Learning from temporal coherence in sequence data, e.g. in audio and video, on the other hand provides a natural, abundant source of data which seems a more biologically plausible signal than used in most current machine learning tasks. In this respect, we believe this is an important direction of research, and this work proposes a simple and intuitive solution to this problem.</p><p>The rest of the paper is organized as follows. Section 2 describes our neural network architecture for incorporating temporal coherence for object recognition, and Section 3 describes previous related work. Section 4 presents experimental results, and Section 5 concludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Exploiting Video Coherence with CNNs</head><p>Our choice of architecture is a convolutional neural network (CNN) <ref type="bibr" target="#b14">(LeCun et al., 1998)</ref>. A CNN performs a chain of filters and resolution reduction steps as shown in Figure <ref type="figure" target="#fig_2">1</ref>. This structure imposes a hardwired prior knowledge which is advantageous for visual recognition tasks compared to fully connected networks due to several reasons. First, it takes the topology of 2D data into account, as opposed to converting it to a long 1D vector. Second, the locality of filters significantly reduces the number of connections and henceforth parameters to be learned, reducing overfitting problems. Finally, the resolution reduction operation provides better tolerance against slight distortions in translation, scale or rotation. CNNs have justified themselves in many visual recognitions tasks including handwritten digit recognition <ref type="bibr" target="#b14">(LeCun et al., 1998)</ref> and face detection <ref type="bibr" target="#b17">(Osadchy et al., 2007)</ref>.</p><p>We now formally describe this architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Convolution and Subsampling</head><p>Each convolution layer C l in our architecture described in Figure <ref type="figure" target="#fig_2">1</ref> operates a linear K l × K l filtering over Two examples are input, one for each copy, and the comparison between them is used to train θ, i.e. using the temporal coherence regularizer in equation ( <ref type="formula">4</ref>).</p><formula xml:id="formula_0">N l−1 image input planes z l−1 1...N l−1 of size D l−1 × D l−1 .</formula><p>It outputs an arbitrary chosen number N l of planes z l 1...N l , where the value at position (i, j) in the p th plane is computed as follows:</p><formula xml:id="formula_1">z l p (i, j) = b l p + q K l s=1 K l t=1 w l p,q,s,t z l−1 q (i−1+s, j−1+t) ,</formula><p>where the biases b l p and the filter weights w l p,q,s,t are trained by backpropagation. The output plane size is</p><formula xml:id="formula_2">D l × D l , where D l = D l−1 − K l + 1.</formula><p>Subsampling layers S l simply apply a K l ×K l smoothing over each input planes:</p><formula xml:id="formula_3">z l p (i, j) = b p + w p K l s=1 K l t=1 z l−1 p (i − 1 + s, j − 1 + t) ,</formula><p>where the parameters b l p and w l p are also trained by backpropagation.</p><p>A non-linearity function like tanh(•) is applied after each convolution and subsampling layer. A final classical fully-connected layer outputs one value per class in the considered task. To interpret these values as probabilities, we add a "softmax" layer which computes:</p><formula xml:id="formula_4">Pp = exp(z l−1 p ) q exp(z l−1 q )</formula><p>.</p><p>(1)</p><p>We suppose we are given a set of training examples {(x n , y n )} n=1...N , where x n represents a twodimensional input image, and y n a label. We then minimize the negative log-likelihood L(θ) = parameters θ of the network:</p><formula xml:id="formula_5">L(θ) = − N n=1 log P θ (y n |x n ) = − N n=1 log Pθ,yn (x n ) (2)</formula><p>We use stochastic gradient descent <ref type="bibr" target="#b5">(Bottou, 1991)</ref> optimization for that purpose. Random examples (x, y) are sampled from the training set. After computation of the gradient ∂L(θ)/∂θ, a gradient descent update is applied:</p><formula xml:id="formula_6">θ ←− θ − λ ∂L(θ, x, y) ∂θ ,<label>(3)</label></formula><p>where λ is a carefully chosen learning rate (e.g., choosing the rate which optimizes the training error).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Leveraging Video Coherence</head><p>As highlighted in the introduction, video coherence ensures that consecutive images in a video are likely to represent the same scene. It is also natural to enforce the representation of input images in the deep layers of the neural network to be similar if we know that the same scene is represented in the input images.</p><p>We consider now two images x 1 and x 2 , and their corresponding generated representation z l θ (x 1 ) and z l θ (x 2 ) in the l th layer. We exploit the video coherence property by enforcing z l θ (x 1 ) and z l θ (x 2 ) to be close (in the L 1 norm) if the two input images are consecutive video images. If the two input images are not consecutive frames, then we push their representations apart. This corresponds to minimizing the following cost:</p><formula xml:id="formula_7">L coh (θ, x 1 , x 2 ) = (4)        ||z l θ (x 1 ) − z l θ (x 2 )|| 1 , if x 1 , x 2 consecutive max(0, δ − ||z l θ (x 1 ) − z l θ (x 2 )|| 1 ), otherwise</formula><p>where δ is the size of the margin, a hyperparameter chosen in advance, e.g. δ = 1.</p><p>Algorithm 1 Stochastic Gradient with Video Coherence.</p><formula xml:id="formula_8">Input: Labeled data (x n , y n ), n = 1, ...N , unla- beled video data x n , n = N + 1, ...N + U repeat Pick a random labeled example (x n , y n ) Make a gradient step to decrease L(θ, x n , y n ) Pick a random pair of consecutive images x m , x n in the video Make a gradient step to decrease L coh (θ, x m , x n ) Pick a random pair of images x m , x n in the video Make a gradient step to decrease L coh (θ, x m , x n ) until Stopping criterion is met</formula><p>In our experiments, we enforced video coherence as described in (4) on the (M −1) th layer of our M -layer network, i.e. on the representation yielded by the successive layers of the network just before the final softmax layer (1). The reasoning behind this choice is that the L 1 distance we use may not be appropriate for the log probability representation in the last layer, although in principle we could apply this coherence regularization at any layer l. In practice, minimizing (4) for all pairs of images is achieved by stochastic gradient descent over a "siamese network" architecture <ref type="bibr" target="#b7">(Bromley et al., 1993)</ref>: two networks sharing the same parameters θ compute the representation for two sampled images x 1 and x 2 as shown in Figure <ref type="figure" target="#fig_1">2</ref>. The gradient of the cost (4) with respect to θ is then computed and updated in the same way as in (3).</p><p>The optimization of the object recognition task (2) and the video coherence (4) is done simultaneously. That is, we minimize:</p><formula xml:id="formula_9">N n=1 L(θ, x n , y n ) + γ m,n L coh (θ, x m , x n )</formula><p>with respect to θ.</p><p>In order to limit the number of hyper-parameters, we gave the same weight to each task, i.e. γ = 1, and minimization is then achieved by alternating stochastic updates from each of the two tasks. Further, the distribution of consecutive versus non-consecutive frames used to minimize L coh (•) presented during stochastic gradient descent will also affect learning. Here, again we simplify things by presenting an equal number of each, as described in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Previous and Related Work</head><p>Temporal Coherence Learning Wiskott and Sejnowski (2002) learn invariant (slowly varying) features from unsupervised video based on a reconstruction loss. This can then be used for a supervised task but is not trained at the same time.</p><p>The work of <ref type="bibr" target="#b0">Becker (1996a;</ref><ref type="bibr" target="#b2">1999)</ref>, which is probably the most related work to ours, explores the use of temporal context using a fully connect neural network algorithm which introduces extra neurons, called contextual gating units, and a Hebbian update rule for clustering based on context ("competitive learning") This method was applied to rotating objects (faces) and showed improvements over not taking into account the temporal context. In comparison, our work does not introduce new network architectures, we instead introduce a natural choice of regularizer for taking advantage of temporal coherence that can be applied to any choice of network. We chose to apply our method to a state-of-the art deep convolutional network. <ref type="bibr" target="#b3">Becker and Hinton (1992;</ref><ref type="bibr" target="#b1">1996b</ref>) also introduced the IMAX method that maximizes the mutual information between different output units which can be applied to learning spacial or temporal coherency. However, this method has a number of drawbacks including a "tendency to become trapped in poor local minima" and that "learning is very slow" (unless specific tricks are used) due to the small gradients induced by their criterion, as reported by the authors. In contrast, our method is highly scalable and can be easily trained on millions of examples, and we observe improved generalization whenever we applied it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semi-Supervised Learning</head><p>Classical Semisupervised learning methods utilize unlabeled examples coming from the same distribution (and hence classes) as the labeled data, and can be realized by either shallow architectures (e.g. kernelized methods) or deep ones. There are many variants of each type, see e.g. <ref type="bibr" target="#b9">(Chapelle et al., 2006)</ref>.</p><p>Two main methods are transductive inference and graph-based approaches. Transductive methods like TSVMs <ref type="bibr" target="#b23">(Vapnik, 1995)</ref> involve maximizing the mar-gin (confidence) on a set of unlabeled examples which come from the same distribution as the training data. Several authors argue that this makes an assumption that the decision rule lies in a region of low density <ref type="bibr" target="#b10">(Chapelle &amp; Zien, 2003)</ref>. Graph-based methods use the unlabeled data by constructing a graph based on a chosen similarity metric, e.g. one builds edges between k-nearest neighbors. For example, Laplacian SVM <ref type="bibr" target="#b4">(Belkin et al., 2005)</ref> works by directly regularizing for a two-class SVM that ||f (x) − f (x )|| 2 should be small for two examples x and x connected in the graph. <ref type="bibr" target="#b25">(Weston et al., 2008)</ref> presents a similar approach for neural networks. Further, <ref type="bibr" target="#b11">(Chopra et al., 2005)</ref> applied a siamese network similar to ours but for a fully supervised (not semi-supervised) face similarity task (not using video). See also <ref type="bibr" target="#b6">(Bowling et al., 2005)</ref> for an embedding algorithm using the actions of a robot which seems related to our work. Finally, we note that many graph-based approaches are also used in an unsupervised rather than supervised setup <ref type="bibr" target="#b21">(Tenenbaum et al., 2000;</ref><ref type="bibr" target="#b19">Roweis &amp; Saul, 2000)</ref>.</p><p>In contrast to TSVMs, our method does not make a strong assumption that the class labels of objects in the unlabeled video have to belong to the training classes, and we show experimentally in Section 4 that our method takes advantage of examples coming from differing classes.</p><p>Graph methods on the other hand suffer from two further problems: (1) building the graph is computationally burdensome for large-scale tasks, (2) they make an assumption that the decision rule lies in a region of low density with respect to the distance metric chosen for k-nearest neighbors.</p><p>Our method does not rely on the low density assumption at all. To see this, consider uniform twodimensional data where the class label is positive if it is above the y-axis, and negative if it is below. A nearest-neighbor graph gives no information about the class label, or equivalently there is no margin to optimize for TSVMs. However, if sequence data (analogous to a video) only has data points with the same class label in consecutive frames then this would carry information. Further, no computational cost is associated with collecting video data for computing (4), in contrast to building neighbor graphs. Realistically, in high dimensional spaces nearest neighbors can also perform poorly, e.g. in the pixel space of images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We consider two types of experiment, object and face recognition, detailed in Sections 4.1 and 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Object Recognition</head><p>We considered several datasets, described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Datasets</head><p>COIL100 We use the COIL100 image dataset developed at Columbia University as our main recognition task <ref type="bibr" target="#b16">(Nayar et al., 1996)</ref>. This set contains color pictures of 100 objects, each 72x72 pixels; some examples are shown in Figure <ref type="figure" target="#fig_3">3</ref>. There are 72 different views for every object, i.e. there are 7200 images in total. The images were obtained by placing the objects on a turntable and taking a shot for each 5 degree turn. The images were clipped and passed through illumination normalization.</p><p>COIL100-Like We created a video dataset of auxiliary images to complement COIL100. The idea is to provide images that are similar to, but not the same objects as, the ones in COIL100. This will enable us to measure the success of our method when the unlabeled video comes from a different distribution. For this purpose we collected 4 types of objects that are also present in COIL100: namely fruits, cars, cups, and cans with 10 objects per type (see Figure <ref type="figure" target="#fig_4">4</ref>).</p><p>Similar to COIL100, we recorded video utilizing a turntable. As we wanted to record a continuous stream of video, in we removed and placed each object, where the operator is wearing a black glove which can be seen in the video. The idea is that our setup mimics that of a child holding a toy or other object and rotating it in her hand, and in this way learning about image transformations. The turntable makes 4 revolutions per minute and the recording rate is 24 fps. We downsampled the video so that two successive frames give a 5 degree rotation of the objects.</p><p>Animal Set We also created a video dataset containing objects rather dissimilar to COIL100. This will enable us to measure the success of our method when the unlabeled video shares no objects in common with the supervised task of interest. To do this, we collected a set of toy animals, consisting of 60 toys of different types such as horse, duck, cow, sheep, deer, dog, cat, pig, mouse, rabbit and different types of birds. The data was collected in the same way as for COIL100-Like. Some examples are given in Figure <ref type="figure" target="#fig_5">5</ref>.</p><p>Although both COIL100 and our video are in color, we convert them to gray scale so that recognition only depends on the shape of the objects. This will enable us to compare our results with other works who also use only the shape information, which is indeed a harder task than using color information as well.    Both the datasets we collected, COIL100-Like and Animal Set are available at http://ml.nec-labs.com/ download/data/videoembed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Methods</head><p>We compare our CNN architecture summarized in Figure <ref type="figure" target="#fig_2">1</ref> against previously used methods. These include Support Vector Machines (SVM) using a polynomial kernel <ref type="bibr" target="#b18">(Roobaert &amp; Hulle, 1999)</ref>, a nearest neighbor classifier on the direct images <ref type="bibr" target="#b24">(Wersing &amp; Körner, 2003</ref>) (Nearest Neighbor), an eigenspace plus spline recognition model <ref type="bibr" target="#b16">(Nayar et al., 1996)</ref> (Eigen Spline), a SpinGlass Markov Random Field (SpinGlass MRF) <ref type="bibr" target="#b8">(Caputo et al., 2002)</ref>, and a hierarchical view-tuned network for visual recognition tasks (VTU) <ref type="bibr" target="#b24">(Wersing &amp; Körner, 2003)</ref>.</p><p>Linear SVM and Nearest Neighbor Classifier are wellknown, so we briefly describe the other methods. Spin Glass MRF essentially uses an energy function inspired by models of physics of disordered systems. The eigenspace and spline method first obtains the eigenspace of training images (all classes together). It then projects training samples onto a subset of eigenvectors with the top eigenvalues. In the reduced space, a spline interpolation is applied to the samples of each class separately. The recognition phase projects a test point to the reduced space and assigns it to the class with closest spline.</p><p>The VTU method builds a hierarchy of biologically inspired feature detectors. The method applies Gabor filters at four orientations, followed by spatial pooling. It then learns receptive field profiles using a special type of sparse coding algorithm with invariance constraints. VTU has shown to achieve very good results, but there is a lot of manual engineering of knowledge in the algorithm as well as a lot of tuning parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3.">Results</head><p>The setup of our experiments is as follows. First, we use a standard CNN without utilizing any temporal information to establish a baseline for our contribution. We then explore three scenarios based on the source of the unlabeled video. These sources are COIL100 objects, COIL100-Like, and our Animal Set. In all of these scenarios, the labeled training and testing data for the supervised task belong to COIL.</p><p>For comparability with the settings available from other studies on COIL100, we choose two experimental setups. These are (i) when all 100 objects of COIL are considered in the experiment and (ii) when only 30 labeled objects out of 100 are studied (for both training and testing). In either case, 4 out of 72 views (at 0, 90, 180, and 270 degrees) per object are used for training, and the rest of the 68 views are used for testing. All of our reported numbers are based on averaging the classification rate on the test data over 10 training runs. The results are given in Table <ref type="table" target="#tab_0">1</ref>.</p><p>The first experiment uses only the labeled examples, and no unlabeled video, for training (Standard CNN). The performance (test set accuracy) is 71.49% and 84.88% for 100 and 30 objects respectively. This is slightly better than SVM but worse than VTU.</p><p>Next, we treat COIL100 as a continuous unlabeled video sequence of rotating objects with 72 consecutive frames per each object (after 72 frames the continuous video switches object). For the 100 object result, the test set is hence part of the unlabeled video (a so-called "transductive" setting). Here we obtained 92.25% accuracy (videoCNN V:COIL100) which is much higher than the best alternative method (VTU).</p><p>A natural question is what happens if we do not have access to test data during training, i.e. the setting is a typical semi-supervised situation rather than a "transductive" setting. To explore this, we used 30 objects as the primary task, i.e. 4 views of each object in this set were used for training, and the rest for test. The other 70 objects were treated as an unlabeled video sequence (again, images of each object were put in consecutive frames of a video sequence). Training with 4 views of 30 objects (labeled data) and 72 views of 70 objects (unlabeled video sequence) resulted in an accuracy of 95.03% on recognizing 68 views of the 30 objects (videoCNN V:COIL"70") This is about 5% above VTU's performance.</p><p>So far the unlabeled sequence was from the same dataset as the training and test. To investigate whether unlabeled video recordings of a large amount of video of some other (possibly similar) objects also improve recognition rates on the supervised task of interest, we then considered our datasets COIL100-Like and Animal Set.</p><p>For the following experiment, we consider all 100 objects in COIL (4 training, 68 for testing of each object). The performance when leveraging the COIL-Like video and Animal Set videos are 79.77% (videoCNN V:COIL100-Like) and 78.67% (videoCNN V:Animal Set) respectively. Compared to the state-of-the art VTU method, the first number is slightly better and the second slightly worse. However, in both cases we have managed to match the state-of-the-art whilst avoiding a strongly engineered solution for this task by utilizing learning from unlabeled video. Compared to a plain CNN without using unlabeled video, these are improvements of about 7% and 8%. This indicates that although use of similar objects has a larger improvement, the difference between the gain obtained by COIL-Like and the Animal Set is relatively small. This is important because it opens up the possibility that the CNN structure has the ability to learn pose invariance abstractly, without relying on the actual object set used for training.</p><p>The drop in performance when using the COIL-Like set as unlabeled video (videoCNN V:COIL-Like) rather than objects from COIL100 itself (videoCNN V:COIL100) is probably partly due to the change in camera and environment parameters. However, our results indicate that using unlabeled auxiliary video is still always beneficial compared to not using video, even when the objects in the auxiliary video are not similar to those of the primary task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Face Recognition</head><p>We also report a simple experiment on AT&amp;T's ORL face database <ref type="bibr" target="#b20">(Samaria &amp; Harter, 1994)</ref>, which consists of 10 different gray scale images for each of the 40 distinct subjects, taken at different times and with varying lighting and facial expressions (open / closed eyes, smiling / not smiling). See Figure <ref type="figure" target="#fig_6">6</ref> for examples.</p><p>The images were placed in a "video" sequence by concatenating 40 segments, one for each subject, ordering according to the (arbitrary) numbering system in the dataset. Note this is a "transductive" setup, where the labeled train and test images are part of the video (training examples are evenly spaced). We labeled k =1,2 or 5 images per subject and compared to the baselines Nearest Neighbor, PCA, LDA and MRF <ref type="bibr" target="#b13">(Huang et al., 2004)</ref>. We used the same CNN as in Section 4.1 and rescaled the images from 92 × 112 to 72 × 72 pixels for that purpose, and otherwise performed no special pre-processing. The results given in Table <ref type="table" target="#tab_1">2</ref> again indicate that learning significantly benefits from unlabeled video through temporal coherence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work we proposed a deep learning algorithm for visual object recognition exploiting the temporal coherence in video. Video acts a pseudo-supervisory signal that improves the internal representation of images by preserving translations in consecutive frames. This should be beneficial for many supervised tasks, and huge collections of data can be obtained without human annotation.</p><p>In our method, labeled and unlabeled data are trained on simultaneously; temporal coherence of unlabeled data acts as a regularizer for the supervised task. Potentially, one can learn representations that are invariant to pose, illumination, background or clutter, deformations (e.g. facial expressions) or occlusions with appropriate videos. Further, our method might be useful for non-visual tasks as well where sequence information has structure, e.g. speaker verification to name one possibility.</p><p>We conducted several experiments to evaluate our approach by considering several choices of video dataset. Our result suggests that strong improvements can be achieved when the unlabeled data comes from the same dataset that labeled data comes from. When the two sets come from different datasets, use of unlabeled data is still beneficial, where probably the more similar the objects are, the more beneficial the data is.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>are machinelearning classification techniques able to handle la-Appearing in Proceedings of the 26 th International Conference on Machine Learning, Montreal, Canada, 2009. Copyright 2009 by the author(s)/owner(s).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Siamese Architecture: the parameters θ are shared between the two identical copies of the network.Two examples are input, one for each copy, and the comparison between them is used to train θ, i.e. using the temporal coherence regularizer in equation (4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. A Convolutional Neural Network (CNN) performs a series of convolutions and subsamplings given the raw input image until it finally outputs a vector of predicted class labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Dataset 1: Examples of the 100 objects from COIL100, each of which has 72 different poses.</figDesc><graphic url="image-1.png" coords="5,328.19,67.08,192.18,123.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Dataset 2: Examples of 40 COIL100-Like objects, each of which is provided with 72 different poses, as a video stream. This video was collected to provide similar sensory data as provided in the COIL dataset.</figDesc><graphic url="image-26.png" coords="5,324.04,440.42,200.42,103.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Dataset 3: Examples of 60 animals from our Animal Set, comprising of animals such as horses, ducks, deer and rabbits. Again, 72 poses are provided for each animal as a video stream.</figDesc><graphic url="image-27.png" coords="5,326.44,610.13,196.00,80.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Dataset 4: Examples from the ORL face dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Test Accuracy on COIL100 in various settings. Both 30 and 100 objects were used following<ref type="bibr" target="#b24">(Wersing &amp; Körner, 2003)</ref>. Our temporal coherence respecting algorithm videoCNN, with various choices of video, outperforms a standard CNN and other baselines.</figDesc><table><row><cell>Method</cell><cell cols="2">30 objects 100 objects</cell></row><row><cell>Nearest Neighbor</cell><cell>81.8</cell><cell>70.1</cell></row><row><cell>SVM</cell><cell>84.9</cell><cell>74.6</cell></row><row><cell>SpinGlass MRF</cell><cell>82.79</cell><cell>69.41</cell></row><row><cell>Eigen Spline</cell><cell>84.6</cell><cell>77.0</cell></row><row><cell>VTU</cell><cell>89.9</cell><cell>79.1</cell></row><row><cell>Standard CNN</cell><cell>84.88</cell><cell>71.49</cell></row><row><cell>videoCNN V:COIL100</cell><cell>-</cell><cell>92.25</cell></row><row><cell>videoCNN V:COIL"70"</cell><cell>95.03</cell><cell>-</cell></row><row><cell>videoCNN V:COIL-Like</cell><cell>-</cell><cell>79.77</cell></row><row><cell>videoCNN V:Animal</cell><cell>-</cell><cell>78.67</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Test Accuracy Performance on ORL Faces with k labeled examples per subject.</figDesc><table><row><cell>Method</cell><cell>k=1</cell><cell>k=2</cell><cell>k=5</cell></row><row><cell>Nearest Neighbor</cell><cell cols="3">69.07 81.08 94.64</cell></row><row><cell>PCA</cell><cell cols="3">56.43 71.19 88.31</cell></row><row><cell>LDA</cell><cell>-</cell><cell cols="2">68.84 88.87</cell></row><row><cell>MRF</cell><cell cols="3">51.06 68.38 86.95</cell></row><row><cell>Standard CNN</cell><cell cols="3">71.83 82.58 94.05</cell></row><row><cell cols="4">videoCNN V:ORL 90.35 94.77 98.86</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning Temporally Persistent Hierarchical Representations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Becker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="1996">1996a</date>
			<biblScope unit="page" from="824" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mutual information maximization: models of cortical self-organization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Becker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Network: Computation in Neural Systems</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="7" to="31" />
			<date type="published" when="1996">1996b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Implicit Learning in 3D Object Recognition: The Importance of Temporal Context</title>
		<author>
			<persName><forename type="first">S</forename><surname>Becker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="347" to="374" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Self-organizing neural network that discovers surfaces in random-dot stereograms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">355</biblScope>
			<biblScope unit="page" from="161" to="163" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On manifold regularization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics (AISTATS)</title>
				<meeting>the Tenth International Workshop on Artificial Intelligence and Statistics (AISTATS)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stochastic gradient learning in neural networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neuro-Nîmes 91</title>
				<meeting>Neuro-Nîmes 91<address><addrLine>Nimes, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page">C2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghodsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wilkinson</surname></persName>
		</author>
		<title level="m">Action respecting embedding. International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Signature verification using a siamese time delay neural network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bentz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Pattern Recognition and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="page">669</biblScope>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A spin-glass markov random field for 3-d object recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Niemann</surname></persName>
		</author>
		<idno>LME-TR-2002- 01</idno>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>Institut fur Informatik, Universitat Erlangen Nurnberg</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Semisupervised learning. Adaptive computation and machine learning</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zien</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, Mass; USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification by Low Density Separation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1633" to="1640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning a Similarity Measure Discriminatively, with Application to Face Verification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition Conference</title>
				<meeting>Computer Vision and Pattern Recognition Conference</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Unsupervised Learning: Foundations of Neural Computation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sejnowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A hybrid face recognition method using markov random fields</title>
		<author>
			<persName><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Pattern Recognition, 17th International Conference on (ICPR&apos;04)</title>
				<meeting>the Pattern Recognition, 17th International Conference on (ICPR&apos;04)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="157" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gradient-Based Learning Applied to Document Recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
				<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning methods for generic object recognition with invariance to pose and lighting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition Conference</title>
				<meeting>Computer Vision and Pattern Recognition Conference</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Real-time focus range sensor</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Noguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1186" to="1198" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Synergistic face detection and pose estimation with energy-based models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Osadchy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page" from="1197" to="1215" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">View-based 3d object recognition with support vector machines</title>
		<author>
			<persName><forename type="first">D</forename><surname>Roobaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M V</forename><surname>Hulle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Neural Networks for Signal Processing</title>
				<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="77" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Nonlinear Dimensionality Reduction by</title>
		<author>
			<persName><forename type="first">S</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Locally Linear Embedding. Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Parameterisation of a stochastic model for human face identification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Samaria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Harter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2nd IEEE Workshop on Applications of Computer Vision</title>
				<meeting>2nd IEEE Workshop on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="138" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A Global Geometric Framework for Nonlinear Dimensionality Reduction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="2319" to="2323" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">80 million tiny images: A large data set for nonparametric object and scene recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1958" to="1970" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The nature of statistical learning theory</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>Second edition</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning optimized features for hierarchical models of invariant recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wersing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Körner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1559" to="1599" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep learning via semi-supervised embedding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rattle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1168" to="1175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Slow feature analysis: Unsupervised learning of invariances</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wiskott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="715" to="770" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
