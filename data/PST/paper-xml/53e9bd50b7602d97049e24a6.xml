<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Beyond sparsity: The role of L 1 -optimizer in pattern classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2011-08-30">30 August 2011</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jian</forename><surname>Yang</surname></persName>
							<email>jianyang@caltech.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<postCode>210094</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Computation and Neural Systems</orgName>
								<orgName type="institution">California Institute of Technology</orgName>
								<address>
									<postCode>91125</postCode>
									<settlement>Pasadena</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
							<email>cslzhang@comp.polyu.edu.hk</email>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Biometric Research Centre</orgName>
								<orgName type="department" key="dep2">Department of Computing</orgName>
								<orgName type="institution">Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Kowloon, Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yong</forename><surname>Xu</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Bio-Computing Research Centre</orgName>
								<orgName type="institution">Shenzhen Graduate School of Harbin Institute of Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jing-Yu</forename><surname>Yang</surname></persName>
							<email>yangjy@mail.njust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<postCode>210094</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<postCode>210094</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Beyond sparsity: The role of L 1 -optimizer in pattern classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2011-08-30">30 August 2011</date>
						</imprint>
					</monogr>
					<idno type="MD5">1D8E7BE390F56FBC39F928E3A6FBB067</idno>
					<idno type="DOI">10.1016/j.patcog.2011.08.022</idno>
					<note type="submission">Received 28 September 2010 Received in revised form 23 July 2011 Accepted 22 August 2011</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Sparse representation Pattern classification Classifier Feature extraction</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The newly-emerging sparse representation-based classifier (SRC) shows great potential for pattern classification but lacks theoretical justification. This paper gives an insight into SRC and seeks reasonable supports for its effectiveness. SRC uses L 1 -optimizer instead of L 0 -optimizer on account of computational convenience and efficiency. We re-examine the role of L 1 -optimizer and find that for pattern recognition tasks, L 1 -optimizer provides more classification meaningful information than L 0 -optimizer does. L 0 -optimizer can achieve sparsity only, whereas L 1 -optimizer can achieve closeness as well as sparsity. Sparsity determines a small number of nonzero representation coefficients, while closeness makes the nonzero representation coefficients concentrate on the training samples with the same class label as the given test sample. Thus, it is closeness that guarantees the effectiveness of the L 1 -optimizer based SRC. Based on the closeness prior, we further propose two kinds of class L 1 -optimizer classifiers (CL 1 C), the closeness rule based CL 1 C (C-CL 1 C) and its improved version: the Lasso rule based CL 1 C (L-CL 1 C). The proposed classifiers are evaluated on five databases and the experimental results demonstrate advantages of the proposed classifiers over SRC in classification performance and computational efficiency for large sample size problems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>''Sparse (or sparsity)'' becomes a popular term in neuroscience, information theory and signal processing and related areas in the past decade <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref>. Vinje and Gallant's studies suggested that primary visual cortex (area V1) uses a sparse code to efficiently represent natural scenes. The receptive fields function forms a sparse representation of the visual world during natural vision <ref type="bibr" target="#b0">[1]</ref>. Olshausen and Field <ref type="bibr" target="#b1">[2]</ref> and Serre <ref type="bibr" target="#b2">[3]</ref> revealed that the firing of the neurons with respect to a given input image is typically highly sparse if these neurons are viewed as an overcomplete dictionary of base signal elements at each visual stage. All of these findings form a physiological basis for sparse coding and sparse representation.</p><p>Sparse coding and sparse representation has recently aroused intensive interest pattern recognition and computer vision area. Labusch et al. <ref type="bibr" target="#b10">[11]</ref> presented a simple sparse-coding strategy for digit recognition and achieved state-of-the-art results on the MNIST benchmark. Zhou et al. <ref type="bibr" target="#b11">[12]</ref> presented a sparse principal component analysis (SPCA), which uses the Lasso (elastic net) to produce modified principal components with sparse loadings and yields encouraging results for regular multivariate data and gene expression arrays. Subsequently, different formulations of SPCA and sparse linear discriminant analysis have been developed <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref>. Cai et al. <ref type="bibr" target="#b15">[16]</ref> suggested a sparse projection over graph and showed its power for document classification. Qiao et al. <ref type="bibr" target="#b16">[17]</ref> put forward a sparse preserving projection technique and demonstrated its effectiveness for face recognition. Actually, Qiao et al.'s sparse preserving projection can be viewed as a special case of L 1 -graph under a general dimensionality reduction framework <ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref>. Recently, Wright et al. presented a sparse representation based classification method and successfully applied it to recognize human faces with varying lighting condition, occlusion and disguise <ref type="bibr" target="#b20">[21]</ref>. In addition, Wright et al. <ref type="bibr" target="#b19">[20]</ref> reviewed other sparse representation methods that were applied to different vision tasks such as image super-resolution <ref type="bibr" target="#b21">[22]</ref>, image denoising and inpainting <ref type="bibr" target="#b22">[23]</ref>, signal and image classification <ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref>, etc. In most of these applications, using sparsity as a prior leads to state-of-the-art results.</p><p>This paper focuses on sparse representation based classification. The basic idea of Wright et al.'s sparse representation based classification (SRC) method is to represent a given test sample as a sparse linear combination of all training samples; the sparse nonzero representation coefficients are supposed to concentrate on the training samples with the same class label as the test sample. The sparsest solution can be sought by solving the L 0 -optimization problem. However, solving L 0 -optimization problem is NP hard Contents lists available at SciVerse ScienceDirect journal homepage: www.elsevier.com/locate/pr and even difficult to approximate <ref type="bibr" target="#b27">[28]</ref>. Recent development in the emerging theory of sparse representation and compressed sensing <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b4">5]</ref> reveals that finding the solution of the L 0 optimization problem is equivalent to finding the solution of the L 1 optimization problem for certain dictionaries. The L 1 -optimizer is therefore used instead of the L 0 -optimizer in SRC.</p><p>Regarding SRC, a fundamental problem is: when one uses all classes of training samples to represent a given test sample, why does the small number of nonzero representation coefficients concentrate on the homo-class training samples? Wright and Ma <ref type="bibr" target="#b30">[31]</ref> and Wright et al. <ref type="bibr" target="#b19">[20]</ref> addressed the extended L 1 -minimization model based error correction problem and interpreted why accurate recovery of sparse signals is possible even if the corruption error is almost dense. But the fundamental problem mentioned remains open, just as said in <ref type="bibr" target="#b19">[20]</ref> ''-the striking discriminative power of the sparse representation still lacks rigorous mathematical justification''. In this paper, our intention is to seek some reasonable supports for SRC. We begin with an example of the two-class handwritten numerical recognition problem in which the L 0 -solution fails while the L 1 -Solution succeeds for classification. This fact indicates that the sparest representation gained by the L 0 -optimizer is not sufficient for classification. Conversely, the L 1 -optimizer may not achieve the sparest solution, but achieves the meaningful solution for correct classification. We then introduce the closeness theory to reveal the connection of the L 1 -solution to classification. The L 1 -norm of nonzero weights can provide a metric to measure the degree of closeness between the testing sample and its support training samples, while the L 0 -norm cannot. The effectiveness of SRC is due to the closeness prior: the homo-class representation leads to the minimal L 1 -norm of nonzero weights. The physical meaning of minimizing L 1 -norm of weights becomes clearer if a weight-sumto-one constraint is imposed onto the L 1 -optimizer, i.e., searching for the support training samples such that their centroid is closest to the given test sample in the sense of L 1 -norm.</p><p>We further introduce the theory of (global) neighborliness and local neighborliness of quotient polytope associated with a dictionary, and use it to in-depth analyze the role of L 1 -optimizer in pattern recognition. In global neighborliness cases where the quotient polytope associated with the dictionary formed by all training samples is t-neighborly, L 1 -optimizer achieves both sparsity and closeness globally. In such cases, L 1 -solution equals to L 0 -solution, i.e., the globally sparsest solution. This sparsest solution determines the set of support training samples that is closest to the given testing sample. In local neighborliness cases where the quotient polytope associated with the dictionary formed by class training samples is t-neighborly, L 1 -optimizer achieves sparsity locally and closeness globally. In such cases, L 1 -solution is a locally sparse solution, possibly not the globally sparsest solution, but it is the solution which is most meaningful for classification. Beyond neighborliness, the degree of sparsity of L 1 -solution cannot be guaranteed, but its effectiveness for classification can still be guaranteed, i.e., the L 1 -solution determines the set of support training samples that is closest to the given testing sample.</p><p>Based on the closeness analysis, we present two class L 1 -optimizer classifiers (CL 1 C). To this end, we first provide theoretical, geometrical and computational justifications for supporting the class training samples based representation. We then present the closeness rule based CL 1 C (C-CL 1 C), which uses the closeness (i.e., the L 1 -norm of the representation coefficients) as a criterion to make a decision. A normalized version of C-CL 1 C is obtained based on geometrical meaning of the solution of the constrained L 1 -optimizer. To overcome the limitation of C-CL 1 C, which restricts the testing sample to lie on faces of the class polytopes and only suits for large sample size problems, we further present the Lasso rule based CL 1 C (L-CL 1 C) and its normalized version. To test the proposed classifiers, we finally use four databases which involve different recognition tasks: the AR database for gender recognition, the CENPARMI database for handwritten numeral Recognition, the NUST603 database for handwritten Chinese character recognition, the Extended Yale B database for face recognition. The experimental results demonstrate the effectiveness of the proposed classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Outline of sparse representation-based classifier</head><p>Suppose there are c known pattern classes. Let A i be the matrix formed by the training samples of Class i, i.e., A i ¼ ½y i1 ,y i2 ,. . .,</p><formula xml:id="formula_0">y iM i A R NÂM i , where M i is the number of training samples of Class i. Let us define a matrix A ¼ ½A 1 ,A 2 ,. . .,A c A R NÂM , where M ¼ P c i ¼ 1 M i . The matrix A is obviously composed of entire training samples.</formula><p>Given a test sample y, we represent y in a overcomplete dictionary whose basis vectors are training sample themselves, i.e., y ¼Aw. This system of linear equation is underdetermined if N oM. The idea of sparse representation based classification is motivated by the following observation: a valid test sample y can be sufficiently represented using only the training samples from the same class. The representation is naturally sparse if training sample size is large enough. The sparser the recovered representation coefficient vector w is, the easier it will be to accurately determine the identity of the test sample y <ref type="bibr" target="#b20">[21]</ref>.</p><p>The sparsest solution to y¼Aw can be sought by solving the following optimization problem:</p><formula xml:id="formula_1">ðL 0 Þ ŵ0 ¼ arg min:w: 0 , subject to Aw ¼ y,<label>ð1Þ</label></formula><p>where :U: 0 denotes the L 0 -norm, which counts the number of nonzero entries in a vector. Solving L 0 optimization problem in Eq. ( <ref type="formula" target="#formula_1">1</ref>), however, is NP hard and extremely time-consuming. Fortunately, recent research efforts reveal that for certain dictionaries, if the solution ŵ0 is spare enough, finding the solution of the L 0 optimization problem is equivalent to finding the solution to the following L 1 optimization problem <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>:</p><formula xml:id="formula_2">ðL 1 Þ ŵ1 ¼ argmin:w: 1 , subject to Aw ¼ y:<label>ð2Þ</label></formula><p>This problem can be solved in polynomial time by standard linear programming algorithms <ref type="bibr" target="#b32">[33]</ref>. A more efficient algorithm, e.g., the homotopy algorithm which has a computational complexity that is linear to the size of the training set, is available recently <ref type="bibr" target="#b33">[34]</ref>.</p><p>After obtaining the sparsest solution ŵ1 , we can design a sparse representation based classifier (SRC) in terms of the class reconstruction residual. Specifically, for Class i, let d i : R N -R N be the characteristic function that selects the coefficients associated with the ith class. For wAR N , d i (w) is a vector whose only nonzero entries are the entries in w that are associated with Class i. Using only the coefficients associated with the ith class, one can reconstruct a given test sample y as ŷi ¼ Ad i ð ŵ1 Þ. The corresponding class reconstruction residual is defined by</p><formula xml:id="formula_3">r i ðyÞ ¼ :yÀ ŷi : 2 ¼ :yÀAd i ð ŵ1 Þ: 2 :<label>ð3Þ</label></formula><p>The SRC decision rule is: if r l ðyÞ ¼ min i r i ðyÞ, y is assigned to Class l.</p><p>For convenience, the training samples (or basis vectors) associated with nonzero representation coefficients are called the support training samples (or support basis vectors) in the remainder of the paper, which is in spirit consistent with the concept of support vectors in support vector machine (SVM) literature <ref type="bibr" target="#b31">[32]</ref>.</p><p>Finally, it should be stressed that SRC relies on the following assumption to guarantee the sparsity of the representation of a test sample: In all of our analysis in reminder of the paper, we always assume that the above assumption holds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">A classification example: L 0 solution fails while L 1 solution succeeds</head><p>The idea of sparse representation based classification implies that the identity of a test sample can be accurately determine, as long as the solution is sufficient sparse. However, this is not always the case. Sometimes, a test sample is misclassified even if the solution is extremely sparse. This problem becomes prominent when there exists a class formed by parts of objects among many classes of objects.</p><p>For example, in handwritten (or printed) numerical recognition problems, ''0'' can be viewed as a part of ''8''. For simplicity, let us consider a two-class problem which contains the samples of ''0'' and ''8''. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, a sample of ''8'', Y, can be represented extremely sparsely by samples of same class, Y 1 , Y 2 and Y 3 . At the same time, Y can be represented extremely sparsely as well by samples of the other class (''0'' class), X 1 , X 2 . Specifically, Y can be represented in the following ways:</p><formula xml:id="formula_4">ðHomo-class representationÞ Y ¼ 1 3 Y 1 þ 1 3 Y 2 þ 1 3 Y 3 þ 0Y 4 þ Á Á Á þ0Y M 1 ¼ A ŵ1 ,<label>ð4Þ</label></formula><formula xml:id="formula_5">ðHetero-class representationÞ Y ¼ 1X 1 þ1X 2 þ 0X 3 þ 0X 4 þ Á Á Á þ0X M 2 ¼ A ŵ0 :<label>ð5Þ</label></formula><formula xml:id="formula_6">Let A ¼ ½Y 1 ,. . .,Y M1 ,X 1 ,. . .,X M2 .</formula><p>If one uses L 0 -optimizer in Eq. ( <ref type="formula" target="#formula_1">1</ref>) to seek for the optimal solution, the sparsest representation coefficient vector is</p><formula xml:id="formula_7">ŵ0 ¼ 0,. . .,0 |fflfflffl{zfflfflffl} M 1 ,1,1,0,. . .,0 |fflfflfflfflfflfflffl{zfflfflfflfflfflfflffl} M 2<label>2 6 4 3 7 5</label></formula><p>T :</p><p>here, the L 0 -norm of ŵ0 is 2. In this case, the SRC makes a wrong decision and assign a sample of ''8'', Y, to the class of ''0''. This example shows us that natural sparsity itself cannot guarantee correct classification. More specifically, for a given sample, one uses the L 0 -optimizer to obtain the optimal solution. Even if the optimal solution is sufficiently sparse, the resulting representation still cannot garantee the correctness of classification.</p><p>In order to achieve correct classification results, based on the SRC decision rule, it is necessary to make the sparse nonzero representation coefficients of a sample concentrate on the homoclass samples. The L 0 -optimizer is not qualified for this, although it suffices to recover the sparse representation of a sample.</p><p>For the same two-class numerical classification problem mentioned above, if we use L 1 -optimizer in Eq. ( <ref type="formula" target="#formula_2">2</ref>) instead of L 0 -optimizer to seek for the optimal solution, the sparsest representation coefficient vector is   coefficients of a sample on the homo-class samples. It is this power that makes L 1 solution more effective than L 0 solution for pattern recognition. In the following sections, we provide theoretical analysis on L 0 -optimizer and L 1 -optimizer, reveal the role of L 1 -optimizer in pattern recognition and further show why L 1 -optimizer based classifier is effective for pattern classification.</p><formula xml:id="formula_8">ŵ1 ¼ 1=3,1=3,1=3,0,. . .,0 |fflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflffl{zfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflffl} M 1 ,0,. . .,</formula><formula xml:id="formula_9">2 Y X 1 Y 1 Y 2 Y 3 (Y 3 ) (Y 2 ) (Y 1 ) (X 2 ) (X 1 ) (Y)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>Why L 1 solution is more effective than L 0 solution for classification?</p><p>In this section, we will provide an intuitive interpretation for why L 1 -optimizer can recover classification meaningful representation. Here, we do not address the problem whether L 1 solution is sparse or not (actually this is another problem we will address in the next section). Rather, we focus on the function of L 1 -optimizer, i.e., minimizing the L 1 -norm of nonzero representation coefficients (weights) and reveal its connections to pattern classification.</p><p>For a given test sample y, the objective function of L 1 -optimizer is :w: 1 , which provides a metric to measure the magnitude of the nonzero reconstruction weights under the constraint of Aw¼y. The minimization of :w: 1 is apt to select the set of support training samples associated with the smallest nonzero reconstruction weights in the sense of the L 1 -norm, among all candidate sets of samples which can produce the representation y¼Aw. Whereas, the objective function of L 0 -optimizer, minimizing :w: 0 , does not provide this weight-selecting mechanism, except for determining the degree of sparsity, i.e., the minimum number of support training samples for representing y. For instance, if two set of support vectors can represent the test sample with the same degree of sparsity, the L 1 -optimizer has the ability to choose the set with minimal L 1 -norm of nonzero weights, whereas the L 0 -optimizer does not have this ability. In other words, L 1 -optimizer is more informative than L 0 -optimizer, since its objective function provides a mechanism for support vector selection, i.e., selecting the support training samples to represent a given test sample with the minimal ''representation cost''.</p><p>In the following, we aim to reveal the intuitive connection between the minimal L 1 -norm of nonzero representation weights and classification. To this end, we first give the following assumption: L 1 -Prior (Closeness Prior) For a given testing sample, using only the homo-class support training samples to represent it can give rise to the minimal representation weights (coefficients) in the sense of the L 1 -norm.</p><p>The reason that calls L 1 -Prior the Closeness Prior lies in two aspects. First, each sample should be naturally represented by the homo-class support training samples, thus a sample is closed in the homo-class sample set. Second, the magnitude of representation weights determines the degree of closeness between a testing sample and the set of support training samples used to represent it. The minimal representation weights imply that a testing sample is closest to the set of support training samples.</p><p>For the two-class numerical classification example mentioned above, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>, the test sample Y is very close to the sample set {Y 1 , Y 2 , Y 3 } of Class ''8'', but far away from the sample set {X 1 , X 2 } of Class ''0'', noticing that the representation weights of the former, as a whole, is much smaller than that of the later in sense of L 1 -norm. This closeness provides very important information for classification. Although the test sample Y can be represented most sparsely by X 1 and X 2 , it is far away from these two samples, so Y is not likely to belong to the class of X 1 and X 2 .</p><p>Actually, the physical meaning of minimal L 1 -norm of nonzero representation weights (i.e., :w: 1 ) becomes clearer if we put a weight-sum-to-one constraint on the L 1 -optimizer. The constraint can eliminate the effect of rotation and rescaling. By adding this constraint, the L 1 -optimizer becomes</p><formula xml:id="formula_10">ðConstrained L 1 Þ ŵ1 ¼ argmin:w: 1 , subject to Aw ¼ y and 1 T w ¼ 1,<label>ð6Þ</label></formula><p>where 1 is an M-dimensional column vector in which every element is 1. Since 1 T w¼1 is a linear equation, it easy to integrate it with Aw¼y, forming a new system of linear equations. Thus, Eq. ( <ref type="formula" target="#formula_10">6</ref>) is equivalent to the following augmented L 1 -optimizer:</p><formula xml:id="formula_11">ðConstrained L 1 Þ ŵ1 ¼ arg min:w: 1 , subject to Aw ¼ y,<label>ð7Þ</label></formula><p>where</p><formula xml:id="formula_12">A ¼ A 1 T , y ¼ y 1 :</formula><p>In this way, the algorithm for L 1 -optimizer can be directly used to resolve the constrained L 1 -optimizer. Assuming ŵ1 is the obtained optimal solution, y ¼ A ŵ1 is actually a weighted mean of the support training samples since 1 T ŵ1 ¼ 1. Now, looking back at the two constraints of the constrained L 1 -optimizer in Eq. ( <ref type="formula" target="#formula_10">6</ref>) from a new viewpoint, we can see that the given sample y lies on the following hyperplane:</p><formula xml:id="formula_13">F ¼ z ¼ X M j ¼ 1 w j x j 9 X M j ¼ 1 w j ¼ 1 8 &lt; : 9 = ; ,<label>ð8Þ</label></formula><p>here we rewrite all training samples in A as x 1 ,x 2 ,. . .,x M . If the solution of the constrained L 1 -optimizer is sparse, without loss of generality, let the first K training samples are support basis vectors.</p><p>Then, the given sample y actually lies on a (KÀ 1)-dimensional hyperplane</p><formula xml:id="formula_14">F ¼ z ¼ X K j ¼ 1 w j x j 9 X K j ¼ 1 w j ¼ 1 8 &lt; : 9 = ; :<label>ð9Þ</label></formula><p>Assuming that x 1 ,x 2 ,. . .,x K belong to the same class, the above hyperplane can be viewed as a local (K À1)-dimensional patch (local face) on the class manifold.</p><p>If we choose a reference point on the hyperplane as the origin, for example, using the centroid (the mean of the support training samples) x ¼ 1=K P K j ¼ 1 x j as the origin, the constraint P K j ¼ 1 w j ¼ 1 can be removed and the hyperplane in Eq. ( <ref type="formula" target="#formula_14">9</ref>) is equivalently expressed as <ref type="bibr" target="#b34">[35]</ref> </p><formula xml:id="formula_15">F ¼ zÀx ¼ X K j ¼ 1 w j x ! j 8 &lt; : 9 = ; , where x ! j ¼ x j Àx:<label>ð10Þ</label></formula><p>Now, let us consider the problem in the new coordinate system whose axes are x ! 1 ,. . ., x</p><formula xml:id="formula_16">! K and origin is x. If x ! 1 ,. . ., x ! K</formula><p>are unitary vectors, for the test sample y on the hyperplane F, the L 1 -norm of its corresponding weights, :w: 1 , is actually the L 1 -distance from y to the origin x, i.e., the mean (centroid) of the support training samples, as shown in Fig. <ref type="figure">2</ref> where K¼ 3. From this point, we know that the implication of :w: implies to search for the support training samples such that their centroid is closest to the given test sample.</p><p>In a word, if a given test sample complies with the Closeness Prior, L 1 -optimizer can concentrate the nonzero sparse representation coefficients of a test sample onto its homo-class support samples such that their centroid is closest to the test sample in the sense of L 1 -norm. This provides an underlying justification for the effectiveness of the L 1 -optimizer based sparse representation classifier.</p><p>In addition, from the example in Section 3, we can conclude that sparsity itself pays more attention to the local reconstruction, while closeness focuses on the global similarity. This global similarity is critical for pattern classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Analysis of the role of L 1 -optimizer in pattern recognition</head><p>In this section, we first outline the existing theoretical results on the equivalence between the L 0 and L 1 problems which was developed by Donoho <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>, then use the theory of neighborliness to address the uniqueness question and finally present the sufficient and necessary condition for the L 0 -L 1 equivalence in Section 5.1. In Section 5.2, we apply the L 0 -L 1 equivalence to analyze the role of L 1 -optimizer in pattern recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Fundamentals</head><p>For a given, general dictionary A and signal y, the equivalence between the L 0 problem and the L 1 problem involves the following two questions <ref type="bibr" target="#b35">[36]</ref>:</p><p>(1) Uniqueness: having the solution of the L 1 problem, under which conditions can we guarantee that this is also the solution of the L 0 problem? (2) Equivalence: knowing the solution of the L 0 problem, what are the conditions under which L 1 is guaranteed to lead to the exact same solution? This question is called L 1 /L 0 equivalence.</p><p>The Uniqueness question has been answered by the following theorem <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>: Theorem 1. (Uniqueness Theorem): given a general dictionary A, given its corresponding Spark value s, and given a signal y, the solution w of the L 1 problem is also the solution of the L 0 problem if :w: 0 r s=2.</p><p>Theorem 1 involves a concept of the Spark value. Given a matrix A, s¼Spark(A) is defined as the largest possible number such that every sub-set of s columns from A are linearly independent, and at least one sub-set of sþ1 columns of A are linearly dependent.</p><p>The equivalence question has been addressed by Donoho <ref type="bibr" target="#b36">[37]</ref> based on the ideas from the theory of convex polytopes. The related concept of quotient polytope corresponding to a dictionary A and its neighborliness are given below: Definition 1. Let a i denote the ith column of a d Â n matrix A. A quotient polytope P associated to A is defined as the convex hull of the 2n points (7a i , i ¼ 1,. . .,n) in R d . The 2n points 7a i are called vertices of P. P is centrosymmetric and is called (centrally) k-neighborly if every subset of k þ1 points not including an antipodal pair spans a face of P.</p><p>For any point on a k-dimensional face of a closed, convex polytope P, its representation is unique, and vice versa. Formally, the following lemma holds <ref type="bibr" target="#b36">[37]</ref>:</p><formula xml:id="formula_17">Lemma 1. (Unique Representation) Consider a k-face F k (P) and suppose that F is a k-simplex. Let xAF. Then (a)</formula><p>x has a unique representation as a convex combination of vertices of P; (b) this representation places nonzero weights only on vertices of F. Conversely, suppose that F is a k-dimensional closed convex subset of P with properties (a) and (b) for every xAF. Then F is a k-simplex and a k-face of P.</p><p>Actually, the concept of neighborliness can be understood from the polytope map point of view. Let C CR n be the n-dimensional cross-polytope, characterized as the convex hull of the signed unit basis vectors 7e i with i ¼ 1,. . .,n and as the L 1 ball in R n , i.e.,</p><formula xml:id="formula_18">:w: 1 r 1:<label>ð11Þ</label></formula><p>After being transformed by the matrix A, the cross-polytope C is mapped into a convex polytope P¼AC. Neighborliness means that the l-faces of P are simply images under A of the l-faces of C. Specifically, we have the following Lemma Lemma 2. (Alternate Form of Neighborliness) <ref type="bibr" target="#b36">[37]</ref> Suppose that the centrosymmetric polytope P ¼AC has 2n vertices and is k-neighborly. Then</p><formula xml:id="formula_19">8l ¼ 0,. . .,kÀ1, 8F A F l ðCÞ, AF A F l ðPÞ:<label>ð12Þ</label></formula><p>Conversely, suppose that Eq. ( <ref type="formula" target="#formula_19">12</ref>) holds; then P¼AC has 2n vertices and is k-neighborly.</p><p>Combining Lemmas 1 and 2, we know that if P is k-neighborly, there exists a one-to-one mapping from l-faces of the cross-polytope C to l-faces of the quotient polytope P, where lok. For each point w on l-faces of the cross-polytope C, there is a unique, corresponding point y on l-faces of the quotient polytope P such that y¼Aw, and vice versa. Fig. <ref type="figure">3</ref>(a) gives an example of k-neighborliness (k¼3), where each point on l-faces of C is mapped into a unique point on l-faces of P (lo3). Fig. <ref type="figure">3</ref>(b) shows an example of non-k-neighborliness (k¼3), where there exist two points on l-faces of C that are mapped into a point y. The point y must be an interior point of P from Lemma 1, since its representation is not unique. The image of the point w 1 or w 2 on l-faces of C is not on l-faces of P, so P is not k-neighborly (k¼3) from Lemma 2. Donoho <ref type="bibr" target="#b36">[37]</ref> has connected the neighborliness to the question of L 1 /L 0 equivalence: Theorem 2. (Equivalence from Neighborliness) Let A be a d Â n matrix, don. The quotient polytope P has 2n vertices and is k-neighborly if and only if w 0 is the unique optimal solution of the L 1 problem whenever y¼Aw 0 has a solution w 0 with at most k nonzeros.</p><formula xml:id="formula_20">C ∈ R 3 P ∈ R 3 w y C ∈ R 3 P ∈ R 3</formula><p>From the above results on neighborliness, an upper bound on the sparsity level at which L 1 -optimizer can solve L 0 problem has been obtained <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b36">37]</ref> Corollary 1. Let P be a centrosymmetric d-polytope with dZ2 and n Zdþ2. If P is k-neighborly, we have</p><formula xml:id="formula_21">k r½ðd þ1Þ=3,<label>ð13Þ</label></formula><p>here, we will go one step further and connect the neighborliness to the question of Uniqueness. To this end, let us first present the following lemma:</p><p>Lemma 3. Let P be a quotient polytope associated to a d Â n dictionary A. If P is k-neighborly, then s¼Spark(A)Z2k.</p><p>The Proof of Lemma 3 is given in appendix. From Theorem 1 and Lemma 3, we have Theorem 3. (Uniqueness from Neighborliness): given a general dictionary A and a signal y, if the associated quotient polytope P is k-neighborly, any solution of the L 1 problem with at most k nonzeros is also the solution of the L 0 problem.</p><p>Combining Theorems 2 and 3, we obtain the following theorem: Theorem 4. (L 0 -L 1 Equivalence) Let P be a centrosymmetric polytope associated to a d Â n dictionary A. P is k-neighborly if and only if the L 0 problem is equivalent to the L 1 problem, that is, for every w 0 with at most k nonzeros, if it is the solution of the L 0 problem, it must be the unique solution of the L 1 problem, and vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">The role of L 1 -optimizer in pattern recognition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Achieving both sparsity and closeness globally in global neighborliness cases</head><p>Now, we discuss about the L 0 -L 1 Equivalence for a special dictionary A formed by all training samples in SRC. The dictionary associated quotient polytope P is the convex hull of the 2M vertices ( 7x ij ) corresponding to M training samples. From the Theorem 4, we know that if P is k-neighborly, for every solution of the L 1 problem with at most k nonzeros, it must be the unique solution of the L 0 problem. Conversely, for every solution of the L 0 problem with at most k nonzeros, solving the L 1 problem can exactly recover this sparest solution.</p><p>From the analysis in Section 4, we know that for a given test sample y, the objective function of L 1 -problem is to select the set of support training samples associated with the smallest nonzero reconstruction weights in the sense of the L 1 -norm from all candidate sets of samples which can produce the representation y¼Aw. As a result, the degree of closeness between the testing sample and the set of support training samples is minimal. If the number of support training samples is no more than k, this set of support training samples can also provide the sparsest representation of y provided that A-associated quotient polytope P is k-neighborly. In summary, L 1 -optimizer can achieve both sparsity and closeness globally if P is k-neighborly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">Achieving sparsity locally and closeness globally in local neighborliness cases</head><p>For the dictionary A formed by all training samples from different classes in SRC, however, the associated quotient polytope P is not necessarily (globally) k-neighborly. Although Donoho <ref type="bibr" target="#b28">[29]</ref> has shown that for most large underdetermined systems of linear equations, the miminal L 1 -norm solution is also the sparsest solution, it should be noted that this conclusion was drawn based on the assumption that columns of a dictionary are sampled independent and identically-distributed (iid) from the uniform distribution on the unit sphere S dÀ 1 (see Theorem 4.2 in <ref type="bibr" target="#b36">[37]</ref> for details). However, for SRC, the columns of A are generally not iid random vectors because they are sampled from different classes with different distributions. So, this conclusion is not suitable for the dictionary used in SRC. In other words, the dictionary formed all training samples from different classes may have a small probability to be k-neighborly.</p><p>If A is not k-neighborly, a test sample might be the interior point of the associated quotient polytope P. An interior point of the quotient polytope P has two or more different original images on faces of the cross-polytope C, as shown in Fig. <ref type="figure">3(b</ref>). To avoid this many-to-one mapping, a possible way is to split the polytope P into a number of small polytopes such that the interior points exist on faces of the generated small polytopes. That is, when A is not k-neighborly, we would rather look at the associated quotient polytope P locally than globally. This idea connects to the concept of local neighborliness <ref type="bibr" target="#b36">[37]</ref>: Definition 2. Given a d Â n matrix A and its associated quotient polytope P, let I denotes the subset of m columns of A and A I denotes the matrix formed by this subset of m columns. If A I -associated quotient polytope P I is k-neighborly, we call P is locally k-neighborly.</p><p>There are two justifications for supporting local neighborliness in SRC:</p><p>First, local neighborliness implies that the support vectors (corresponding to nonzeros) are collected from a subset of columns of the dictionary A. For classification purposes, we would like a test sample to be represented by the samples of the same class. Therefore, it is reasonable to choose the subset I as the set of the training samples of the same class.</p><p>Second, the local quotient polytope P I is more likely to be k-neighborly if the subset I is composed of training samples of the same class. This is because columns of A I are independent and identically-distributed random vectors since they are sampled from one class. From Theorem 4.2 in <ref type="bibr" target="#b36">[37]</ref>, we know P I have a large probability to be k-neighborly.</p><p>Based on the above analysis, for the dictionary A ¼ ½A 1 ,A 2 ,. . .,A c composed of all training samples of c classes in SRC, we can split its associated quotient polytope P into c small local ones, P 1 ,P 2 ,. . .,P c , which are, respectively, associated with A 1 ,A 2 ,. . .,A c , where A i is the matrix composed of the training samples of Class i, i ¼ 1,. . .,c. If every P i is k-neighborly, for a given testing sample y, the L 1 -optimizer can recover the local sparsest solution w i from the L 0 -L 1 Equivalence. That is, L 1 -optimizer can achieve the sparsity locally. Based on the set of c local sparsest solutions w 1 ,w 2 ,. . .,w c , we can get the global L 1 -optimal solution w 1 ¼ argmin i :w i : 1 , which is not necessarily the global sparest solution w 0 ¼ argmin i :w i : 0 , but the sparse one which yields the set of support training samples closest to the given test sample. That is, L 1 -optimizer can achieve the closeness globally.</p><p>Looking back at the numerical classification example in Section 3, the quotient polytope P associated with the dictionary A ¼ ½Y 1 ,. . .,Y M 1 ,X 1 ,. . .,X M 2 is not k-neighborly (k¼2 here). The reason is that the representation of the given sample point of ''8'', Y, is not unique. Thus, Y must be interior point of the quotient polytope P from Lemma 1. Note that Y¼ A ŵ1 from Eq. ( <ref type="formula" target="#formula_4">4</ref>), where the point ŵ1 is on a face of the cross-polytope C. However, the image of ŵ1 , Y, is not on a face of the quotient polytope P. From Lemma 2, we know that A is not k-neighborly (k¼2). Let us divide P into two parts, P 1 and P 2 , which are quotient polytopes corresponding to A 1 ¼ ½Y 1 ,. . .,Y M 1 and A 2 ¼ ½X 1 ,. . .,X M 2 , respectively. If P 1 and P 2 are both k-neighborly, we can use L 1 -optimizer to recover the local sparsest solutions ŵ1 and ŵ0 for A 1 and A 2 . Then, the global L 1 -solution is ŵ1 , which is the solution yielding the closest support training samples to Y, but not the global sparsest solution ŵ0 .</p><p>In summary, the phenomenon of local neighborliness may commonly occur in pattern recognition problems. In such a case, locally, L 1 -optimizer achieves the same solution with L 0 -optimizer, but globally, the solution of L 1 -optimizer might not be that of L 0 -optimizer. L 1 -optimizer achieves sparsity locally and closeness globally. The local sparsity implies that the global L 1 -solution is a local sparse solution but not necessarily the globally sparsest. The global closeness means that the global L 1 -solution is a solution most meaningful for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3.">Achieving closeness still beyond neighborliness</head><p>In real world pattern recognition problems, however, we cannot even guarantee the local neighborliness. Specifically, we cannot ensure each local quotient polytope P associated with class training sample matrix A i to be k-neighborly. Further, even if local neighborliness can be guaranteed, the sparsity level k is strictly limited. Corollary 1 shows the upper bound of k is [(dþ1)/3] for neighborliness. This means that for a solution of L 0 problem with more than [(dþ1)/3] nonzeros, L 1 -optimizer may fail to recover this solution.</p><p>This above fact is somewhat disappointing, from the viewpoint of sparsity recovery. However, from the viewpoint of classification, this limitation of L 1 -optimizer is insignificant. The L 1 solution is classification meaningful, even if it is not as sparse as expected. This classification meaningfulness is due to the closeness, an inherent characteristic of the solution, i.e., searching for supporting training samples which are closest to the given test sample in the sense of L 1 -norm. First, based on the analysis in Section 5.2, we know that the training sample matrix A associated quotient polytope P is more likely to be locally k-neighborly. Specifically, the class training sample matrix A I associated class quotient polytope P I is more likely to be k-neighborly because its columns are sampled from one class thus they are apt to be independent and identicallydistributed random vectors. The local neighborliness supports class training samples based representation from the sparsity point of view.</p><p>Second, the geometric meaning becomes clearer if the class training samples based representation is adopted. Obviously, the training samples of a class lie on the associated class quotient polytope P I . For a given testing sample y, finding the support training samples of the class to represent it is geometrically equivalent to finding a face of P I such that using its all vertices to represent y leads to the minimal representation coefficients in the sense of L 1 -norm (i.e., :w: 1 ). If P I is k-neighborly, this representation is the sparest. Based on the analysis in Section 4, we know that :w: 1 is actually the L 1 -distance from y to the centroid of all vertices of the face. Therefore :w: 1 determines a geometric distance from y to the class. Based on the sample-toclass distances, we can classify the sample to the closest class (i.e., the class with minimal distance). The above geometric interpretation supports class training samples based representation from the closeness point of view.</p><p>Third, when the training sample size is very large, the Global L 1 -optimizer classifier (GL 1 C) encounters a large-scale L 1 optimization problem. The Class L 1 -optimizer classifier (CL 1 C) means that we can solve the problem instead by dividing the large-scale problem into c (the number of classes) relative small-scale problems. Therefore, CL 1 C has the advantage of dealing with large-scale problems over GL 1 C from the computational point of view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Class L 1 -optimizer classifier with the closeness rule</head><p>Assume there are enough training samples per class (i.e., Assumption 1 holds) and A i is the matrix formed by the training samples of Class i. For a given testing sample y, we use the training samples of Class i to represent it and obtain the representation coefficients by solving the following problem:</p><formula xml:id="formula_22">ðL 1 Þ w i ¼ argmin:w: 1 , subject to A i w ¼ y:<label>ð14Þ</label></formula><p>After getting all representation coefficient vectors w 1 ,. . .,w c corresponding to all classes, we use the closeness (i.e., the L 1 -norm of the representation coefficients) as a criterion to yield a decision rule: if w l ¼ min i :w i : 1 , then y belongs to Class l. This is the original version of the closeness rule based Class L 1 -optimizer classifier (C-CL 1 C).</p><p>We now use the geometric interpretation given in Section 4 to further refine the original C-CL 1 C. To this end, we enforce the sum-to-one constraint 1 T w ¼1 to the L 1 -optimizer in Eq. ( <ref type="formula" target="#formula_22">14</ref>) and have the constrained L 1 -optimizer</p><formula xml:id="formula_23">ðConstrained L 1 Þ w i ¼ argmin:w: 1 , subject to A i w ¼ y,<label>ð15Þ</label></formula><p>where</p><formula xml:id="formula_24">A ¼ A 1 T , y ¼ y 1 ,</formula><p>by solving the problem, we obtain the representation coefficient vector w i and the corresponding support training samples of Class i. Without loss of generality, assume that x i1 ,x i2 ,. . .,x iK are support training samples. We use the mean of these support training samples, x i ¼ 1=K P K j ¼ 1 x ij , as the origin to center the data locally and then use x i1 Àx i ,. . .,x iK Àx i as axes to form the local coordinate system. To make the coordinate of a point meaningful, we need to normalize the axes to be unitary vectors. Note that the representation coefficient vector w i is calculated based on the original axes x i1 Àx i ,. . .,x iK Àx i . So, the normalized representation coefficient vector w i based on the normalized axes is</p><formula xml:id="formula_25">w i ¼ ½w i 1 :x i1 Àx i :,. . .,w i K :ðx iK Àx i Þ:, 0,. . .,0 T ,<label>ð16Þ</label></formula><p>:w i : 1 is geometrically the L 1 -distance from y to the origin in the local coordinate system of Class i. This distance leads to a classification rule: if w l ¼ min i :w i : 1 , then y belongs to Class l. We call this the normalized closeness rule based class L 1 -optimizer classifier (NC-CL 1 C).</p><p>We finally provide the geometric interpretation for the decision rule of NC-CL 1 C. For every class quotient polytope, we seek a face of it on which the test sample y may lie, noticing that the vertices of the face are determined by the solution of Eq. ( <ref type="formula" target="#formula_23">15</ref>).</p><p>Then, to determine which polytope the test sample belongs to, we compare the L 1 distances from the test sample to the centroids of the faces of class polytopes, i.e., the magnitude of :w i : 1 . We know that a face of a class polytope is a convex hull of its vertices. The smaller :w i : 1 is, the larger possibility the test sample belongs to the convex hull (face) of the class polytope.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Class L 1 -optimizer classifier with the Lasso rule</head><p>From the geometric interpretation, we know C-CL 1 C (or NC-CL 1 C) restrict the testing sample to lie on faces of the class polytopes, as shown in Fig. <ref type="figure">2</ref>. This restriction is generally too strict and even infeasible when there are not enough training sample per class. Here we remove this restriction and allow the testing sample point not on faces of the class polytopes, as shown in Fig. <ref type="figure">4</ref>. We seek the face of a class polytope that is nearest to the given testing sample. This nearness between a testing sample and a face can be measured by two criterions. One is the residual criterion, which characterizes the distance between the test sample point and its image (reconstruction point) on the face, and the other is the closeness criterion (L 1 -norm of the reconstruction coefficients), which characterizes the distance between the image of the test sample point and the centroid of the face. These two criterions can be integrated into the Lasso criterion <ref type="bibr" target="#b11">[12]</ref> as follows:</p><formula xml:id="formula_26">ðLassoÞ w i ¼ arg min LðwÞ ¼ :A i wÀy: 2 2 þ l:w: 1 ,<label>ð17Þ</label></formula><p>where l is a non-negative parameter. Obviously, if we restrict the testing sample to lie on faces of the class polytopes, i.e., A i w¼y, the Lasso criterion becomes the closeness criterion.</p><p>Solving the Lasso and obtaining the representation coefficient vector w i corresponding to Class i, i ¼ 1,. . .,c, we use the Lasso function L(w) as a measure to yield the decision rule: if Lðw l Þ ¼ min i Lðw i Þ, then y belongs to Class l. This forms the original version of the Lasso rule based Class L 1 -optimizer classifier (L-CL 1 C). Now, we consider how to refine the Lasso rule and embed the sum-to-one constraint 1 T w ¼1 to it. Let</p><formula xml:id="formula_27">A ¼ A a1 T , y ¼ y a ,</formula><p>where a40. The constrained Lasso criterion is defined by ðConstrained LassoÞ w i ¼ arg min:A i wÀy:</p><formula xml:id="formula_28">2 2 þ l:w: 1 :<label>ð18Þ</label></formula><p>Since :A i wÀy:</p><formula xml:id="formula_29">2 2 ¼ :A i wÀy: 2 2 þ a 2 :1 T wÀ1: 2 2 ,</formula><p>if we set a large enough, the solution of the constrained Lasso naturally satisfies 1 T w¼ 1. Based on the solution of Eq. ( <ref type="formula" target="#formula_28">18</ref>) and the determined support training samples, we further obtain the normalized representation coefficient vector w i , as shown in Eq. ( <ref type="formula" target="#formula_25">16</ref>). Then, the normalized Lasso distance is defined by</p><formula xml:id="formula_30">Lðw i Þ ¼ :A i w i Ày: 2 2 þ l:w i : 1 :<label>ð19Þ</label></formula><p>The geometric meaning of the normalized Lasso distance is shown in Fig. <ref type="figure">4</ref>.</p><p>The normalized Lasso distance leads to a decision rule: if Lðw l Þ ¼ min i Lðw i Þ, then y belongs to Class l. We call this the normalized Lasso rule based class L 1 -optimizer classifier (NL-CL 1 C).</p><p>Finally, we would like to explain why we use the Lasso criterion in our classification model from the regularization point of view. The Lasso criterion is the sum of two terms: the first is the square reconstruction residual term, and the second term is a L 1 regularization term which is introduced to avoid overfitting. If the regularization term is neglected, to minimize the Lasso criterion leads to a standard least-square regression problem. Its solution is geometrically the distance from the point y to the hyperplane spanned by the training samples <ref type="bibr" target="#b38">[39]</ref>. However, sometimes this distance is unreliable and leads to misclassification. For example, in the two-class case where there are two training sample points per class, as shown in Fig. <ref type="figure">5</ref>, the two points span a line provided that the sum-to-one constraint is enforced. A test sample x, which belongs to Class 1, is misclassified because it is closer to the line spanned by training samples of Class 2. In such a case, we notice that the reconstruction weights are very large because the image of x, x 0 , is far away from the centroid of y 1 and y 2 . Adding the regularization term and minimizing the regularized distance can pull the image of x closer to the centroid of y 1 and y 2 , and therefore rectify the distance between the testing sample x and its image. The rectified distance gives rise to a correct classification.</p><p>Here, the role of L 1 regularization is twofold. First, it results in a sparse solution which produces a local characterization for a given testing sample. This solution determines a small number of support training samples, which forms a local ''patch'' of the class manifold. Particularly when the class training sample matrix A i associated quotient polytope P i is t-neighborly, the local patch forms a face of the polytope P i . Second, it helps rectify the distance between the testing sample and the face spanned by the support training samples. Actually, the regularization term itself also provides a meaningful distance between the image of the testing sample and the centroid of the face. The two distances are integrated into the Lasso distance, which provides a robust measure between the testing sample and the class manifold.</p><p>It should be mentioned that L 2 regularization can also help rectify the distance between the testing sample and the hyperplane spanned by the support training samples <ref type="bibr" target="#b34">[35]</ref>. But, it cannot give rise to a local characterization. Specifically, if we use the L 2 regularization term instead in Eq. ( <ref type="formula" target="#formula_26">17</ref>) or <ref type="bibr" target="#b17">(18)</ref>, the solution of the model is dense. To obtain a local measure, one may appeal to the K nearest neighbor searching, which results in a series of local classification methods such as the nearest neighbor line (K¼2) <ref type="bibr" target="#b39">[40]</ref>, the nearest neighbor plane (K¼3) <ref type="bibr" target="#b40">[41]</ref> and the K-local hyperplane <ref type="bibr" target="#b34">[35]</ref>. However, how to choose the proper parameter K for these kinds of methods is a difficult problem. Generally we choose a common K for every class. This is not a good strategy since </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class 2</head><p>Class 1</p><p>x'' x'</p><p>x 2 x 1 x y 2 y 1 Fig. <ref type="figure">5</ref>. An example where the minimum least-square distance leads to misclassification.</p><p>K represents the local dimension of class manifold which might be different for different local faces of different class manifold. In contrast, L 1 regularization can automatically determine the local dimension by counting nonzeros in the solution of Lasso.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Experiment on the AR database for gender recognition</head><p>The AR face <ref type="bibr" target="#b41">[42]</ref> contains over 4000 color face images of 126 people, including frontal views of faces with different facial expressions, lighting conditions and occlusions. The images of 110 persons including 55 males and 55 females are selected and used in our experiment. The pictures of each person were taken in two sessions (separated by two weeks) and each section contains 7 color images without occlusions. The face portion of each image is manually cropped and then normalized to 50 Â 45 pixels. The sample images of one male and female are shown in Fig. <ref type="figure" target="#fig_5">6</ref>.</p><p>In our experiment, images of the first 25 males and 25 females were used for training, and images of the remaining 30 males and 30 females for testing. Since there are 14 images per person, the total number of training samples is 700 (each class with 350 samples). We use PCA to reduce the dimension of each image to be D, where D varies from 10 to 100 with an interval of 10. In the D-dimensional PCA-transformed space, SRC <ref type="bibr" target="#b20">[21]</ref> and the proposed class L 1 -optimizer classifiers (CL 1 C) including the closeness rule based CL 1 C (C-CL 1 C), the normalized closeness rule based CL 1 C (NC-CL 1 C), the Lasso rule based CL 1 C (L-CL 1 C) and the normalized Lasso rule based CL 1 C (NL-CL 1 C) are employed for classification. The nearest neighbor classifier is also used to provide a baseline. Note that here in SRC, C-CL 1 C and NC-CL 1 C, the matlab function ''l1eq_pd'' from the l 1 -magic <ref type="bibr" target="#b42">[43]</ref> is used to calculate the sparse representation coefficients. In L-CL 1 C and NL-CL 1 C, the matlab function ''l1_ls'' provided by Kim et al. <ref type="bibr" target="#b43">[44]</ref> is used. The parameter l in Lasso is chosen as 0.05 in L-CL 1 C and 0.01 in NL-CL 1 C. The recognition rate curve of each classifier versus the variation of dimensions is shown in Fig. <ref type="figure" target="#fig_8">7</ref>. The maximal recognition rate of each classifier and the corresponding dimension are listed in Table <ref type="table">1</ref>.</p><p>From Fig. <ref type="figure" target="#fig_8">7</ref> and Table <ref type="table">1</ref>, we can see that the proposed class L 1 -optimizer classifiers, C-CL 1 C and L-CL 1 C, improve the performance of the global SRC. The normalized class L 1 -optimizer classifiers NC-CL 1 C and NL-CL 1 C can further improve the performance. The two classifiers consistently outperform the NN classifier and SRC, irrespective of the variation of dimensions. SRC does not perform well on this database, even worse than the NN classifier. In addition, we notice that the Lasso rule based class L 1 -optimizer classifier L-CL 1 C improve the performance of the closeness rule based class L 1 -optimizer classifier C-CL 1 C. However, their performance difference becomes insignificant after normalization: NC-CL 1 C achieve comparable results with NL-CL 1 C in this experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Experiment on the CENPARMI database for handwritten numeral recognition</head><p>The experiment was done on Concordia University CENPARMI handwritten numeral database. The database contains 6000 samples of 10 numeral classes (each class has 600 samples). Some samples of ''0'' from the CENPARMI database are shown in Fig. <ref type="figure" target="#fig_11">8</ref>.</p><p>In our first experiment, we choose the first 200 samples of each class for training, the remaining 400 samples for testing. Thus, the total number of training samples is 2000 while the total number of testing samples is 4000. PCA is used to transform the original 121-dimensional Legendre moment features <ref type="bibr" target="#b44">[45]</ref> into D-dimensional features, where D varies from 10 to 80 with an interval of 10. Based on the PCA-transformed features, the nearest neighbor classifier, SRC, C-CL 1 C, NC-CL 1 C, L-CL 1 C and NL-CL 1 C are employed for classification. The parameter l is chosen as 0.01 in L-CL 1 C and NL-CL 1 C. The recognition rate each classifier corresponding to the variation of dimensions is shown in Fig. <ref type="figure" target="#fig_9">9</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>The maximal recognition rates (%) of each classifier for gender recognition on the AR database and the corresponding dimensions. The recognition rate of each classifier as the dimension is 50 and the corresponding total CPU time (CPU: 2.33 GHz, RAM: 2.48 GB) are listed in Table <ref type="table" target="#tab_1">2</ref>. Table <ref type="table" target="#tab_1">2</ref> shows that NC-CL 1 C achieves comparable results with SRC, but the former is much faster than the later. The total CPU time of NC-CL 1 C is only 1/7 of that of SRC, noticing that both classifiers use the same Matlab function (i.e., ''l1eq_pd'' from the l 1 -magic <ref type="bibr" target="#b42">[43]</ref>) to calculate the sparse representation coefficients.</p><p>To provide more insights into the Lasso rule based class L 1 -optimizer classifiers, we would like to observe the representation coefficient vector of each testing sample with respect to each class. We calculate the number of nonzeros in the representation coefficient vector. Note that here the representation coefficient bigger than 10 À 3 is thought of as nonzero. The number of nonzeros represents the local dimension of the class manifold. The mean and standard deviation of the number of nonzeros corresponding to all testing samples across all classes are calculated and listed in Table <ref type="table" target="#tab_2">3</ref>. In addition, the means and standard deviations of the number of nonzeros corresponding to all testing samples via homo-class sample representation and hetero-class sample representation are respectively calculated and listed in Table <ref type="table" target="#tab_2">3</ref>. Table <ref type="table" target="#tab_2">3</ref> shows us that the nonzero representation coefficients are quite different for different testing samples and different classes. In general, the homo-class representation of a testing sample yields much less nonzero representation coefficients than the hetero-class representation in the average sense. From classification point of view, for a given testing sample and a class, we find a local face of the class manifold that is closest to the sample. The dimension of the local face is generally different for different testing samples. The Lasso criterion, due to its L 1 regularization term, provides a mechanism to evaluate the local dimension adaptively.</p><p>The K-local hyperplane classifier <ref type="bibr" target="#b34">[35]</ref> uses the L 2 regularization rather than the L 1 regularization. It does not have the ability to evaluate the local dimension K of the class manifold automatically. To address this problem, we generally assume local dimension is identical and determine a proper K by experiments. This K is obviously not theoretically optimal. Fig. <ref type="figure" target="#fig_2">10</ref> shows the recognition rate curse of the K-local hyperplane classifier with the variation of the parameter K. The maximal recognition rate is 95.1%. This result indicates that the K-local hyperplane classifier is effective, but not as good as the Lasso rule based classifiers L-CL 1 C and NL-CL 1 C. This is understandable since a common K for all testing samples and all classes is suboptimal. In addition, we find that when K ¼31, the K-local hyperplane classifier achieves a recognition rate of 94.9%, which is very close to the maximal recognition rate. Notice that 31 is the approximate local dimension estimated by the average number of nonzeros in the solution of Lasso (as shown in Table <ref type="table" target="#tab_2">3</ref>).    Finally, we let the number of training samples per class vary from 100 to 500 with an interval of 100, and use the remaining samples for test in the experiment. The recognition rate and the average CPU time (s) consumed for one test sample of each classifier is illustrated in Fig. <ref type="figure" target="#fig_13">11</ref>. Fig. <ref type="figure" target="#fig_13">11(</ref>  ) both use the same matlab function ''l1eq_pd'' from the l 1 -magic <ref type="bibr" target="#b42">[43]</ref> to calculate the sparse representation coefficients. This means that the local class L 1 -optimizer classifiers have computational advantage over the global L 1 -optimizer classifier SRC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Experiment on the NUST603 database for handwritten Chinese character recognition</head><p>The experiment was performed on the NUST603 handwritten Chinese character database which was built in Nanjing University of Science and Technology. The database contains 19 groups of Chinese characters that are collected from bank checks, each group with 400 samples. Some images from the NUST603HW database are shown in Fig. <ref type="figure" target="#fig_3">12</ref>.</p><p>In our experiment, we let the number of training samples per class vary from 100 to 300 with an interval of 50, and use the remaining samples for test. Similar to the experimental methodology adopted in Section 7.2, PCA is used to transform the original 128-dimensionl peripheral feature vectors <ref type="bibr" target="#b45">[46]</ref>  It is evident that here we achieve consistent results with the last experiment in Section 7.2. The Lasso rule, which combines the residual criterion and the closeness criterion, demonstrates its advantage again. L-CL 1 C and NL-CL 1 C consistently outperforms other classifiers, irrespective of the variation of training sample size. NC-CL 1 C achieves similar (or better) results as SRC when the number of training samples per class is over 150, but as the number of training samples per class is 100, SRC performs better. This result shows again that NC-CL 1 C need enough training samples to guarantee its performance. The local class L 1 -optimizer classifiers, including </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Experiment on the Extended Yale B database for face recognition</head><p>The Extended Yale B face database <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48]</ref> contains 38 human subjects under 9 poses and 64 illumination conditions. The 64 images of a subject in a particular pose are acquired at camera frame rate of 30 frames/s, so there is only small change in head pose and facial expression for those 64 images. All frontal-face images marked with P00 are used in our experiment, and each is resized to 42 Â 48 pixels. Some sample images of one person are shown in Fig. <ref type="figure" target="#fig_6">14</ref>.</p><p>In the first experiment, we use histogram equalization as a preprocessing step to alleviate the effect of illuminations on images. The first 32 images of each subject are used for training, and the remaining for test. PCA (Eigenfaces <ref type="bibr" target="#b48">[49]</ref>), LDA (Fisherfaces <ref type="bibr" target="#b49">[50]</ref>) and LPP (Laplacianfaces <ref type="bibr" target="#b50">[51]</ref>) are used to extract 100-dimensional features. To avoid overfitting, we perform LDA and LPP in the 200-dimensional PCA-transformed space. Finally, the nearest   <ref type="table" target="#tab_3">4</ref>. Table <ref type="table" target="#tab_3">4</ref> shows that the proposed classifiers L-CL 1 C and NL-CL 1 C achieve similar recognition results with SRC for face recognition. But, the former ones are more than 5 times faster than the latter.</p><p>In the second experiment, we remove the histogram equalization step and just normalize image vectors to be unit vectors in preprocessing. Obviously, in this case, the face recognition problem becomes more challenging. We use the same experimental procedure as above to test the four classifiers for three feature extraction methods. The results are shown in Table <ref type="table" target="#tab_4">5</ref>. We can see that the performance of the NN classifier highly depends on what feature extraction method is used. It performs much worse than the other classifiers with respect to unsupervised feature extraction methods, such as PCA and LPP. Conversely, all L 1 -optimizer classifiers, L-CL 1 C, NL-CL 1 C and SRC, are insensitive to feature extraction methods adopted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.">Experiment using the PIE database for face recognition</head><p>The CMU PIE face database contains 68 subjects with over 40,000 face images <ref type="bibr" target="#b51">[52]</ref>. Images of each person were taken across 13 different poses, under 43 different illumination conditions, and with 4 different expressions. Here we use a subset containing images of pose C05 (a nearly frontal pose) of 68 persons, each with 49 images. All images are manually aligned, cropped and resized to be 64 Â 64 pixels <ref type="bibr" target="#b52">[53]</ref> in our experiment.</p><p>Here, we only preprocess each image by normalizing image vectors to be unit vectors. The first 25 images of each subject are used for training, and the remaining for test. We use PCA, LDA and LPP for feature extraction and obtain 150 features for face representation. To avoid overfitting, we perform LDA and LPP in the 200-dimensional PCA-transformed space. The nearest neighbor classifier, SRC, L-CL 1 C and NL-CL 1 C are employed for classification.</p><p>The parameter l in L-CL 1 C and NL-CL 1 C is chosen as 0.05. The recognition rate of four classifiers corresponding to three feature extraction methods are listed in Table <ref type="table" target="#tab_5">6</ref>. The results in Table <ref type="table" target="#tab_5">6</ref> are basically consistent with those in Table <ref type="table" target="#tab_4">5</ref>. We can see that the proposed classifiers L-CL 1 C and NL-CL 1 C achieve comparable results with SRC. The performance of the NN classifier highly depends on what feature extraction method is used. Its recognition rate is almost 20% lower than those of the other classifiers with respect to LPP. In contrast, the performance of all L 1 -optimizer Classifiers is much more robust to the change of feature extraction methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusions and discussions</head><p>We provide an insight into SRC and re-recognize the role of L 1 -optimizer: using L 1 -optimizer instead of L 0 -optimizer is central for pattern classification. L 1 -optimizer kills two birds with one stone: achieving sparsity<ref type="foot" target="#foot_4">2</ref> and closeness simultaneously in (global or local) neighborliness cases. Sparsity determines a small number support training samples to represent a given test sample, while closeness makes the nonzero representation coefficients concentrate on the homo-class training samples. Sparsity benefits for local reconstruction, while closeness helps for global similarity. By combining sparsity and closeness together, the solution of L 1 -optimizer yields a geometrically meaningful measure for classification.</p><p>We propose two kinds of class L 1 -optimizer classifiers (CL 1 C), the closeness rule based ones (C-CL 1 C and NC-CL 1 C) and the Lasso rule based ones (L-CL 1 C and NL-CL 1 C). The former can be viewed as a special case of the latter. If the number of training sample size per class is large enough, NC-CL 1 C achieve similar (or even better) performance as SRC but with much lower computational cost. So, in this case, if one cares more about the classification speed, we recommend using NC-CL 1 C since it is the fastest L 1 -optimizer classifier. However, in most real world pattern recognition problems, there is only limited number of training samples available, as shown in our experiments. In such general cases, we recommend using NL-CL 1 C due to its robust performance, as demonstrated across all of our experiments. Besides, its speed is also acceptable; it is significantly faster than SRC when the number of training samples is relatively large.</p><p>Finally, we would like to specify some distinctions and connections between our work and Wright and Ma's work <ref type="bibr" target="#b30">[31]</ref>. Our work focuses on the basic SRC with the standard L 1 -optimizer model as shown in Eq. (2), while Wright's work <ref type="bibr" target="#b30">[31]</ref> focuses on the general SRC with the extended L 1 -optimizer model as follows: ½ ŵ, ê ¼ arg min:w: 1 þ :e: 1 , subject to Awþe¼y. Our work is to provide reasonable supports for L 1 -optimizer based classifier: its discriminative power can be guaranteed, even if the L 1 -solution could be denser than L 0 -solution, whereas Wright's work is to provide theoretical justifications for error correction ability of the extended L 1 -optimizer: it can work well, even if the error vector e is nearly dense. There is one thing in common in both works: sparsity is not a necessary condition any more: both weight vector and error vector could be dense to some degree. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Assumption 1 .</head><label>1</label><figDesc>(Large Sample Size Assumption): there are sufficient number of training samples for each class, such that any test sample can be sufficiently represented using only the training samples from the same class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>X</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustration of the two-class handwritten numerical recognition problem, where the dots represent samples of ''8'', while the triangles represent samples of ''0''.</figDesc><graphic coords="3,94.64,225.05,396.41,60.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1 Fig. 2 .</head><label>12</label><figDesc>Fig.2. Illustration of the geometric meaning of :w: 1 : the L 1 -distance from y to the origin x in the coordinate system formed by x ! 1 ,. . ., x ! K , here K¼ 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2 Fig. 3 .</head><label>23</label><figDesc>Fig. 3. Illustration of k-neighborliness and non-k-neighborliness of P¼ AC from the mapping point of view. (a) P is k-neighborly (k¼ 3) and (b) P is not k-neighborly (k¼3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>6 .</head><label>6</label><figDesc>Class L 1 -optimizer classifier 6.1. Justifications for local class based classification Wright et al.'s SRC method represents a testing sample across all training samples, thus can be called Global L 1 -optimizer classifier. Here, we will present two local class L 1 -optimizer classifiers, which represent a testing sample by training samples belonging to every class. Three justifications for supporting the class training samples based representation are given below.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>1 yFig. 4 .</head><label>14</label><figDesc>Fig.<ref type="bibr" target="#b3">4</ref>. The geometric meaning of the normalized Lasso distance is the weighted combination of the two distances: the L 2 distance between the test sample point y and its image y 0 on the face of the class polytopes, and the L 1 distance between the image y 0 and the centroid of the face.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. Samples images of one male and female in the AR database.</figDesc><graphic coords="9,112.77,620.49,360.00,109.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The recognition rate of each classifier for gender recognition on the AR database versus the variation of dimensions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9</head><label>9</label><figDesc>Fig. 9 shows that the Lasso rule based class L 1 -optimizer classifiers (L-CL 1 C and NL-CL 1 C) consistently outperform the closeness rule based class L 1 -optimizer classifiers (C-CL 1 C and NC-CL 1 C) and SRC, irrespective of the variation of dimensions. This means that removing the restriction of the testing sample point on faces of the class manifold helps improve the classification performance. All of the five classifiers achieve (or nearly achieve) their maximal performance when the dimension reaches 50. As the dimension becomes larger, the performance of C-CL 1 C, NC-CL 1 C and SRC begins to decline, while the performance of L-CL 1 C and NL-CL 1 C keeps invariant or slightly increasing. This implies that the Lasso rule based class L 1 -optimizer classifiers are</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. The recognition rate of each classifier for handwritten numeral recognition on the CENPARMI database versus the variation of dimensions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Some samples in CENPARMI database.</figDesc><graphic coords="10,44.44,195.96,247.32,209.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>a) shows that the Lasso rule based class L 1 -optimizer classifiers (L-CL 1 C or NL-CL 1 C) achieve best recognition rate among all methods, irrespective of the variation of training sample size. NC-CL 1 C consistently outperforms C-CL 1 C, which implies that normalization does help improve the performance of the closeness rule based classifier. NC-CL 1 C achieves very close results to those of SRC when the number of training samples per class is over 200. However, when the number of training samples per class is not enough, NC-CL 1 C and C-CL 1 C do not perform well. For instance, when the class training sample size is 100, as shown in Fig. 11, both methods achieve lower recognition rate than SRC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 11 (</head><label>11</label><figDesc>Fig. 11(b) shows that the closeness rule based classifiers (C-CL 1 C and NC-CL 1 C) are the fastest among all classifiers. The CPU time difference between SRC and all CL 1 C classifiers (including C-CL 1 C, NC-CL 1 C, L-CL 1 C and NL-CL 1 C) become more and more significant with the increase of training sample size. SRC and C-CL 1 C (or NC-CL 1 C) both use the same matlab function ''l1eq_pd'' from the l 1 -magic [43] to calculate the sparse representation coefficients. This means that the local class L 1 -optimizer classifiers have computational advantage over the global L 1 -optimizer classifier SRC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>into 50-dimensional features. Based on the PCA-transformed features, the nearest neighbor classifier, SRC, C-CL 1 C, NC-CL 1 C, L-CL 1 C and NL-CL 1 C are employed for classification. The parameter l is chosen as 0.05 in L-CL 1 C and NL-CL 1 C. The recognition rate and the average CPU time curves of each classifier are illustrated in Fig. 13.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 10 .Fig. 11 .</head><label>1011</label><figDesc>Fig. 10. The recognition rate curve of the K-local hyperplane classifier versus the variation of K-neighbor parameter K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 12 .Fig. 13 .</head><label>1213</label><figDesc>Fig. 12. Some samples in NUST603HW database.</figDesc><graphic coords="12,122.58,58.64,360.00,168.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Samples of a person under different illuminations in the extended Yale B face database.</figDesc><graphic coords="12,122.59,529.84,360.00,64.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>The recognition rates (%) of each classifier for handwritten numeral recognition on the CENPARMI database and the corresponding total CPU time.</figDesc><table><row><cell>Classifier</cell><cell>NN</cell><cell>SRC</cell><cell>C-CL 1 C</cell><cell>NC-CL 1 C</cell><cell>L-CL 1 C</cell><cell>NL-CL 1 C</cell></row><row><cell>Recognition rate</cell><cell>88.3</cell><cell>93.0</cell><cell>92.4</cell><cell>93.1</cell><cell>96.2</cell><cell>96.0</cell></row><row><cell>CPU time (s)</cell><cell>8.92 Â 10 1</cell><cell>7.38 Â 10 3</cell><cell>1.05 Â 10 3</cell><cell>1.03 Â 10 3</cell><cell>4.29 Â 10 3</cell><cell>5.60 Â 10 3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>The mean and standard deviation (std) of the number of nonzero representation coefficients.</figDesc><table><row><cell>Homo-class</cell><cell>Hetero-class</cell><cell>All</cell></row><row><cell>19.7568 75.5557</cell><cell>32.29197 8.2891</cell><cell>31.03847 8.8919</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>The recognition rates (%) of three classifiers with respect to three feature extraction methods for face recognition on the Extended Yale B database with histogram equalization.</figDesc><table><row><cell>Classifier</cell><cell>NN</cell><cell>SRC</cell><cell>L-CL 1 C</cell><cell>NL-CL 1 C</cell></row><row><cell>PCA</cell><cell>95.9</cell><cell>98.4</cell><cell>98.4</cell><cell>98.5</cell></row><row><cell>LDA</cell><cell>98.9</cell><cell>98.8</cell><cell>99.0</cell><cell>99.0</cell></row><row><cell>LPP</cell><cell>96.4</cell><cell>98.7</cell><cell>98.4</cell><cell>98.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc>The recognition rates (%) of three classifiers with respect to three feature extraction methods for face recognition on the Extended Yale B database without histogram equalization.</figDesc><table><row><cell>Classifier</cell><cell>NN</cell><cell>SRC</cell><cell>L-CL 1 C</cell><cell>NL-CL 1 C</cell></row><row><cell>PCA</cell><cell>85.8</cell><cell>94.7</cell><cell>95.1</cell><cell>95.1</cell></row><row><cell>LDA</cell><cell>95.9</cell><cell>94.3</cell><cell>95.0</cell><cell>94.8</cell></row><row><cell>LPP</cell><cell>89.8</cell><cell>94.2</cell><cell>94.1</cell><cell>94.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6</head><label>6</label><figDesc>The recognition rates (%) of three classifiers with respect to three feature extraction methods for face recognition on the PIE database.</figDesc><table><row><cell>Classifier</cell><cell>NN</cell><cell>SRC</cell><cell>L-CL 1 C</cell><cell>NL-CL 1 C</cell></row><row><cell>PCA</cell><cell>83.6</cell><cell>96.1</cell><cell>98.0</cell><cell>97.9</cell></row><row><cell>LDA</cell><cell>98.3</cell><cell>99.3</cell><cell>99.1</cell><cell>99.1</cell></row><row><cell>LPP</cell><cell>77.9</cell><cell>97.4</cell><cell>97.3</cell><cell>97.4</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Note that here for simplicity and being easily understood, we use the original samples to directly represent Y, as shown in Eqs. (4) and<ref type="bibr" target="#b4">(5)</ref>. If we normalize Y 1 , Y</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>, Y</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>, X 1 , X 2 to be unit vectors before the representation, we still have the same classification result.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p>J. Yang et al. / Pattern Recognition 45 (2012) 1104-1118</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_4"><p>Note that the solution of L 1 -optimizer is sparse but not necessarily the sparest solution of L 0 -optimizer in local neighborliness case.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank the anonymous reviewers for their critical and constructive comments and suggestions. This work was partially supported by the Program for New Century Excellent Talents in University of China, the NUST Outstanding Scholar Supporting Program, the National Science Foundation of China under Grant nos. 60973098 and 90820306, National Science Fund for Distinguished Young Scholars, and the Hong Kong RGC General Research Fund.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Appendix. Proof of <ref type="bibr">Lemma 3</ref> Proof. If Spark(A)r2k À 1, there exists a subset of 2k columns from A which is linearly dependent. Without loss of generality, suppose that a 1 ,a 2 ,. . .,a 2k are linearly dependent. Then, there exists a set of scalars t 1 ,t 2 ,. . .,t 2k , not all zero, such that</p><p>Without loss of generality, we assume that the problem is scaled so that :w 1 : 1 ¼ 1 and :w 2 : 1 ¼ 1. Letting :w 1 : 0 ¼ l 1 and :w 2 : 0 ¼ l 2 , it is evident that l 1 rk and l 2 rk. Thus, w 1 and w 2 are on faces of the cross-polytope C. However, y¼Aw 1 ¼Aw 2 are not on faces of the quotient polytope P from Lemma 1, since the representation is not unique. Actually, y is an interior point of P. From Lemma 2, it derives that P is not k-neighborly.</p><p>Therefore, if P is k-neighborly, Spark(A) Z2k must hold. &amp;</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sparse coding and decorrelation in primary visual cortex during natural vision</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Vinje</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Gallant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">287</biblScope>
			<biblScope unit="issue">5456</biblScope>
			<biblScope unit="page" from="1273" to="1276" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sparse coding of sensory inputs</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Field</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Opinion in Neurobiology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="481" to="487" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<title level="m">Learning a Dictionary of Shape-Components in Visual Cortex: Comparison with Neurons, Humans and Machines</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>Ph.D. Dissertation, MIT</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Robust uncertainty principles: exact signal reconstruction from highly incomplete frequency information</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cand Es</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Romberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="489" to="509" />
			<date type="published" when="2006-02">2006. February</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Near optimal signal recovery from random projections: universal encoding strategies?</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cand Es</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5406" to="5425" />
			<date type="published" when="2006-12">2006. December</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Compressed sensing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1289" to="1306" />
			<date type="published" when="2006-04">2006. April</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An introduction to compressive sampling</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Wakin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="21" to="30" />
			<date type="published" when="2008-03-21">2008. 21 March</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Matching pursuits with time-frequency dictionaries</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3397" to="3415" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The K-SVD: an algorithm for designing of overcomplete dictionaries for sparse representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4311" to="4322" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Morphological component analysis: an adaptative thresholding strategy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bobin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Starck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fadili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Moudden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2675" to="2681" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Simple method for high-performance digit recognition based on sparse coding</title>
		<author>
			<persName><forename type="first">K</forename><surname>Labusch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Barth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Martinetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<title level="m">Sparse Principle Component Analysis</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
		<respStmt>
			<orgName>Statistics Department, Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A direct formulation for sparse PCA using semidefinite programming</title>
		<author>
			<persName><forename type="first">A</forename><surname>Aspremont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">El</forename><surname>Ghaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R G</forename><surname>Lanckriet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date>December</date>
			<biblScope unit="page">2004</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spectral bounds for sparse PCA: exact and greedy algorithms</title>
		<author>
			<persName><forename type="first">B</forename><surname>Moghaddam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generalized spectral bounds for sparse LDA</title>
		<author>
			<persName><forename type="first">B</forename><surname>Moghaddam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML &apos;06: Proceedings of the 23rd International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="641" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<title level="m">Proceedings of AAAI Conference on Artificial Intelligence (AAAI-08)</title>
		<meeting>AAAI Conference on Artificial Intelligence (AAAI-08)<address><addrLine>Chicago, Illinois</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-07">July 2008</date>
		</imprint>
	</monogr>
	<note>Sparse projections over graph</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sparsity preserving projections with applications to face recognition</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Y</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="331" to="341" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by sparse representation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIAM International Conference on Data Mining</title>
		<meeting>SIAM International Conference on Data Mining<address><addrLine>Sparks, Nevada, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Graph embedding and extension: a general framework for dimensionality reduction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="40" to="51" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sparse representation for computer vision and pattern recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of IEEE</title>
		<imprint>
			<date type="published" when="2009-03">2009. March</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robust face recognition via sparse representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="227" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image superresolution as sparse representation of raw patches</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning multiscale sparse representations for image and video restoration</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM MMS</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="214" to="241" />
			<date type="published" when="2008-04">2008. April</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sparse representation for signal classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Aviyente</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning discriminative dictionaries for local image analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Supervised dictionary learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in NIPS</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Linear spatial pyramid matching using sparse coding for image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On the approximability of minimizing nonzero variables or unsatisfied relations in linear systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Amaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">209</biblScope>
			<biblScope unit="page" from="237" to="260" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">For most large underdetermined systems of linear equations the minimal l1-norm solution is also the sparsest solution</title>
		<author>
			<persName><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications on Pure and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="797" to="829" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Stable signal recovery from incomplete and inaccurate measurements</title>
		<author>
			<persName><forename type="first">E</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Romberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications on Pure and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1207" to="1223" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dense error correction via l1-minimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3540" to="3560" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">The Nature of Statistical Learning Theory</title>
		<meeting><address><addrLine>N.Y.</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Atomic decomposition by basis pursuit</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saunders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="129" to="159" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Fast Solution of L1 Norm Minimization Problems when the Solution May Be Sparse</title>
		<author>
			<persName><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tsaig</surname></persName>
		</author>
		<ptr target="http://www-stat.stanford.edu/$donoho/reports.htmlS" />
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">K-local hyperplane and convex distance nearest neighbor algorithms</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Maximal Sparsity Representation Via l1 Minimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<ptr target="http://www-stat.stanford.edu/$donoho/reports.htmlS" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Neighborly Polytopes and Sparse Solution of Underdetermined Linear Equations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
		<respStmt>
			<orgName>Department. of Statistics, Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Diagrams for centrally symmetric polytopes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mcmullen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Shephard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematika</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="123" to="138" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Minimal local reconstruction error measure based discriminant feature extraction and classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2008)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2008)<address><addrLine>Alaska</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06">June, 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Face recognition using the nearest feature line method</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juwei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="439" to="443" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Discriminant waveletfaces and nearest feature classifiers for face recognition</title>
		<author>
			<persName><forename type="first">J.-T</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1644" to="1649" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>Machine Intelligence</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benavente</surname></persName>
		</author>
		<idno>#24</idno>
		<title level="m">The AR Face Database</title>
		<imprint>
			<date type="published" when="1998-06">June 1998</date>
		</imprint>
	</monogr>
	<note type="report_type">CVC Technical Report</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">l1-Magic: Recovery of Sparse Signals Via Convex Programming</title>
		<author>
			<persName><forename type="first">E</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Romberg</surname></persName>
		</author>
		<ptr target="http://www.acm.caltech.edu/l1magic/S" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A method for large-scale l1-regularized least squares</title>
		<author>
			<persName><forename type="first">S.-J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lustig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gorinevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="606" to="617" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">On image analysis by moments</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">X</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pawlak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="254" to="266" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Speeding up Chinese character recognition in an automatic document reading system</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1601" to="1612" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">From few to many: illumination cone models for face recognition under variable lighting and pose</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Georghiades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="643" to="660" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Acquiring linear subspaces for face recognition under variable lighting</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Driegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="684" to="698" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Face recognition using eigenfaces</title>
		<author>
			<persName><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="586" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Eigenfaces vs. Fisherfaces: recognition using class specific linear projection</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Hespanha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriengman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="711" to="720" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Face recognition using Laplacianfaces</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="328" to="340" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The CMU pose, illumination, and expression database</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bsat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1615" to="1618" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Spectral regression for efficient regularized subspace learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE 11th International Conference on Computer Vision (ICCV 2007)</title>
		<meeting>IEEE 11th International Conference on Computer Vision (ICCV 2007)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">From 2007, he has been a professor in the School of Computer Science and Technology of NUST. Now, he is a Visiting Associate at California Institute of Technology. He is the author of more than 50 scientific papers in pattern recognition, computer vision and the related areas. His journal papers have been cited more than 1100 times in the ISI Web of Science</title>
	</analytic>
	<monogr>
		<title level="m">Jian Yang received the B.S. degree in mathematics from the Xuzhou Normal University in 1995, the M.S. degree in applied mathematics from the Changsha Railway University in 1998 and the Ph</title>
		<imprint>
			<date type="published" when="2004">2004 to 2006. 2006 to 2007</date>
		</imprint>
		<respStmt>
			<orgName>Postdoctoral Fellow in Department of Hong Kong Polytechnic University ; Postdoctoral Fellow in Department of Computer Science of New Jersey Institute of Technology.</orgName>
		</respStmt>
	</monogr>
	<note>D. degree in computer science from the Nanjing University of Science and Technology (NUST) in 2002. and 2400 times in the Web of Scholar Google. Currently, he is an</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">D. degrees in Automatic Control Theory and Engineering from Northwestern Polytechnical University, Xi&apos;an, PR China, respectively</title>
	</analytic>
	<monogr>
		<title level="m">Lei Zhang received the B.S. degree in 1995 from Shenyang Institute of Aeronautical Engineering, Shenyang, PR China, the M.S. and Ph</title>
		<imprint>
			<date type="published" when="1998">1998. 2001. 2001 to 2002. January 2003 to January 2006. January 2006</date>
		</imprint>
		<respStmt>
			<orgName>The Hong Kong Polytechnic University ; Department of Electrical and Computer Engineering, McMaster University ; The Hong Kong Polytechnic University</orgName>
		</respStmt>
	</monogr>
	<note>Pattern Recognition, Multisensor Data Fusion and Optimal Estimation Theory</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">he worked at Shenzhen graduate school, Harbin Institute of Technology (HIT) as a postdoctoral research fellow. Now he is an associate professor at Shenzhen graduate school, HIT. He also acts as a research assistant researcher at the Hong Kong Polytechnic University from August</title>
		<author>
			<orgName type="collaboration">Yong Xu received his B.S. and M.S.</orgName>
		</author>
		<imprint>
			<date type="published" when="2005-05">May 2005 to April 2007. 2007. June 2008</date>
		</imprint>
	</monogr>
	<note>His current interests include pattern recognition, biometrics and machine learning. He has published more than 40 scientific papers</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">From 1982 to 1984 he was a visiting scientist at the Coordinated Science Laboratory, University of Illinois at Urbana-Champaign. From 1993 to 1994 he was a visiting professor at the Department of Computer Science, Missuria University. And in 1998, he acted as a visiting professor at Concordia University in Canada. He is currently a professor and Chairman in the department of Computer Science at NUST. He is the author of over 300 scientific papers in computer vision, pattern recognition and artificial intelligence. He has won more than 20 provincial awards and national awards. His current research interests are in the areas of pattern recognition</title>
		<author>
			<persName><forename type="first">Jing-Yu</forename></persName>
		</author>
		<imprint>
			<pubPlace>Nanjing, China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Yang received the B.S. Degree in Computer Science from Nanjing University of Science and Technology (NUST)</orgName>
		</respStmt>
	</monogr>
	<note>robot vision, image processing, data fusion and artificial intelligence</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
