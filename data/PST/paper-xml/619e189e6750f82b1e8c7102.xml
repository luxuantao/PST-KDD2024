<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LARGE-SCALE REPRESENTATION LEARNING ON GRAPHS VIA BOOTSTRAPPING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-11-04">4 Nov 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Shantanu</forename><surname>Thakoor</surname></persName>
							<email>&lt;thakoor@google.com&gt;.1</email>
							<affiliation key="aff0">
								<address>
									<country>Georgia Tech</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Deepmind</forename><surname>Corentin</surname></persName>
							<affiliation key="aff0">
								<address>
									<country>Georgia Tech</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tallec</forename><surname>Deepmind</surname></persName>
							<affiliation key="aff0">
								<address>
									<country>Georgia Tech</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mohammad</forename><surname>Gheshlaghi</surname></persName>
							<affiliation key="aff0">
								<address>
									<country>Georgia Tech</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Azar</forename><surname>Deepmind</surname></persName>
							<affiliation key="aff0">
								<address>
									<country>Georgia Tech</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mehdi</forename><surname>Azabou</surname></persName>
							<affiliation key="aff0">
								<address>
									<country>Georgia Tech</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eva</forename><forename type="middle">L</forename><surname>Dyer</surname></persName>
							<affiliation key="aff0">
								<address>
									<country>Georgia Tech</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Georgia</forename><surname>Tech</surname></persName>
							<affiliation key="aff0">
								<address>
									<country>Georgia Tech</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rémi</forename><surname>Munos</surname></persName>
							<affiliation key="aff0">
								<address>
									<country>Georgia Tech</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
							<affiliation key="aff0">
								<address>
									<country>Georgia Tech</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michal</forename><surname>Valko</surname></persName>
							<affiliation key="aff0">
								<address>
									<country>Georgia Tech</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LARGE-SCALE REPRESENTATION LEARNING ON GRAPHS VIA BOOTSTRAPPING</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-11-04">4 Nov 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2102.06514v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-supervised learning provides a promising path towards eliminating the need for costly label information in representation learning on graphs. However, to achieve state-of-the-art performance, methods often need large numbers of negative examples and rely on complex augmentations. This can be prohibitively expensive, especially for large graphs. To address these challenges, we introduce Bootstrapped Graph Latents (BGRL) -a graph representation learning method that learns by predicting alternative augmentations of the input. BGRL uses only simple augmentations and alleviates the need for contrasting with negative examples, and is thus scalable by design. BGRL outperforms or matches prior methods on several established benchmarks, while achieving a 2-10x reduction in memory costs. Furthermore, we show that BGRL can be scaled up to extremely large graphs with hundreds of millions of nodes in the semi-supervised regime -achieving state-ofthe-art performance and improving over supervised baselines where representations are shaped only through label information. In particular, our solution centered on BGRL constituted one of the winning entries to the Open Graph Benchmark -Large Scale Challenge at KDD Cup 2021, on a graph orders of magnitudes larger than all previously available benchmarks, thus demonstrating the scalability and effectiveness of our approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graphs provide a powerful abstraction for complex datasets that arise in a variety of applications such as social networks, transportation networks, and biological sciences <ref type="bibr" target="#b19">(Hamilton et al., 2017;</ref><ref type="bibr" target="#b11">Derrow-Pinion et al., 2021;</ref><ref type="bibr" target="#b53">Zitnik &amp; Leskovec, 2017;</ref><ref type="bibr" target="#b6">Chanussot et al., 2021)</ref>. Despite recent advances in graph neural networks (GNNs), when trained with supervised data alone, these networks can easily overfit and may fail to generalize <ref type="bibr" target="#b40">(Rong et al., 2019)</ref>. Thus, finding ways to form simplified representations of graph-structured data without labels is an important yet unsolved challenge.</p><p>Current state-of-the-art methods for unsupervised representation learning on graphs <ref type="bibr" target="#b47">(Veličković et al., 2019;</ref><ref type="bibr" target="#b36">Peng et al., 2020;</ref><ref type="bibr" target="#b20">Hassani &amp; Khasahmadi, 2020;</ref><ref type="bibr" target="#b52">Zhu et al., 2020b;</ref><ref type="bibr">a;</ref><ref type="bibr" target="#b50">You et al., 2020)</ref> are contrastive. These methods work by pulling together representations of related objects and pushing apart representations of unrelated ones. For example, current best methods <ref type="bibr" target="#b52">Zhu et al. (2020b)</ref> and <ref type="bibr" target="#b51">Zhu et al. (2020a)</ref> learn node representations by creating two augmented versions of a graph, pulling together the representation of the same node in the two graphs, while pushing apart every other node pair. As such, they inherently rely on the ability to compare each object to a large number of negatives.</p><p>In the absence of a principled way of choosing these negatives, this can require computation and memory quadratic in the number of nodes. In many cases, the generation of a large number of negatives poses a prohibitive cost, especially for large graphs.</p><p>In this paper, we introduce a scalable approach for self-supervised representation learning on graphs called Bootstrapped Graph Latents (BGRL). Inspired by recent advances in self-supervised learning in vision <ref type="bibr" target="#b16">(Grill et al., 2020)</ref> , BGRL learns node representations by encoding two augmented versions Z(1,i) H(2,i)</p><p>Figure <ref type="figure">1</ref>: Overview of our proposed BGRL method. The original graph is first used to derive two different semantically similar views using augmentations T1,2. From these, we use encoders E θ,φ to form online and target node embeddings. The predictor p θ uses the online embedding H1 to form a prediction Z1 of the target embedding H2. The final objective is then computed as the cosine similarity between Z1 and H2, flowing gradients only through Z1. The target parameters φ are updated as an exponentially moving average of θ.</p><p>of a graph using two distinct graph encoders: an online encoder, and a target encoder. The online encoder is trained through predicting the representation of the target encoder, while the target encoder is updated as an exponential moving average of the online network. Critically, BGRL does not require contrasting negative examples, and thus can scale easily to very large graphs.</p><p>Our main contributions are:</p><p>• We introduce Bootstrapped Graph Latents (BGRL), a graph self-supervised learning method that effectively scales to extremely large graphs and outperforms existing methods, while using only simple graph augmentations and not requiring negative examples (Section 2). • We show that contrastive methods face a trade-off between peak performance and memory constraints, due to their reliance on negative examples (Section 4.2). Due to having time and space complexity scaling only linearly in the size of the input, BGRL avoids the performancememory trade-off inherent to contrastive methods altogether. BGRL provides performance competitive with the best contrastive methods, while using 2-10x less memory on standard benchmarks (Section 3). • We show that leveraging the scalability of BGRL allows making full use of the vast amounts of unlabeled data present in large graphs via semi-supervised learning. In particular, we find that efficient use of unlabeled data for representation learning prevents representations from overfitting to the classification task, and achieves significantly higher, state-of-the-art performance. This was critical to the success of our solution at KDD Cup 2021 in which our BGRL-based solution was awarded one of the winners, on the largest publicly available graph dataset, of size 360GB consisting of 240 million nodes and 1.7 billion edges (Section 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BOOTSTRAPPED GRAPH LATENTS</head><p>2.1 BGRL COMPONENTS BGRL builds representations through the use of two graph encoders, an online encoder E θ and a target encoder E φ , where θ and φ denote two distinct sets of parameters. We consider a graph G = (X, A), with node features X ∈ R N ×F and adjacency matrix A ∈ R N ×N . BGRL first produces two alternate views of G: G 1 = ( X 1 , A 1 ) and G 2 = ( X 2 , A 2 ), by applying stochastic graph augmentation functions T 1 and T 2 respectively. The online encoder produces an online representation from the first augmented graph, H 1 := E θ ( X 1 , A 1 ); similarly the target encoder produces a target representation of the second augmented graph, H 2 := E φ ( X 2 , A 2 ). The online representation is fed into a node-level predictor p θ that outputs a prediction of the target representation, Z 1 := p θ ( H 1 ).</p><p>BGRL differs from prior bootstrapping approaches such as BYOL <ref type="bibr" target="#b16">(Grill et al., 2020)</ref> in that it does not use a projector network. Unlike vision tasks, in which a projection step is used by BYOL for dimensionality reduction, common embedding sizes are quite small for graph tasks and so this is not a concern in our case. In fact, we observe that this step can be eliminated altogether without loss in performance (Appendix B).</p><p>The augmentation functions T 1 and T 2 used are simple, standard graph perturbations previously explored <ref type="bibr" target="#b50">(You et al., 2020;</ref><ref type="bibr" target="#b52">Zhu et al., 2020b)</ref>. We use a combination of random node feature masking and edge masking with fixed masking probabilities p f and p e respectively. More details and background on graph augmentations is provided in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">BGRL UPDATE STEP</head><p>Updating the online encoder E θ : The online parameters θ (and not φ), are updated to make the predicted target representations Z 1 closer to the true target representations H 2 for each node, by following the gradient of the cosine similarity w.r.t. θ, i.e.,</p><formula xml:id="formula_0">(θ, φ) = − 2 N N −1 i=0 Z (1,i) H (2,i) Z (1,i) H (2,i) (1) θ ← optimize(θ, η, ∂ θ (θ, φ)),</formula><p>(2) where η is the learning rate and the final updates are computed from the gradients of the objective with respect to θ only, using an optimization method such as SGD or Adam <ref type="bibr" target="#b29">(Kingma &amp; Ba, 2015)</ref>. In practice, we symmetrize this loss, by also predicting the target representation of the first view using the online representation of the second.</p><p>Updating the target encoder E φ : The target parameters φ are updated as an exponential moving average of the online parameters θ, using a decay rate τ , i.e.,</p><formula xml:id="formula_1">φ ← τ φ + (1 − τ )θ,<label>(3)</label></formula><p>Figure <ref type="figure">1</ref> visually summarizes BGRL's architecture.</p><p>Note that although the objective (θ, φ) has undesirable or trivial solutions, BGRL does not actually optimize this loss. Only the online parameters θ are updated to reduce this loss, while the target parameters φ follow a different objective. This non-collapsing behavior even without relying on negatives has been studied further <ref type="bibr" target="#b46">(Tian et al., 2021)</ref>. We provide an empirical analysis of this behavior in Appendix A, showing that in practice BGRL does not collapse to trivial solutions and (θ, φ) does not converge to 0.</p><p>Scalable non-contrastive objective: Here we note that a contrastive approach would instead encourage Z (1,i) and H (2,j) to be far apart for node pairs (i, j) that are dissimilar. In the absence of a principled way of choosing such dissimilar pairs, the naïve approach of simply contrasting all pairs {(i, j) | i = j}, scales quadratically in the size of the input. As BGRL does not rely on this contrastive step, BGRL scales linearly in the size of the graph, and thus is scalable by design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">COMPUTATIONAL COMPLEXITY ANALYSIS</head><p>We provide a brief description of the time and space complexities of the BGRL update step, and illustrate its advantages compared to previous strong contrastive methods such as GRACE <ref type="bibr" target="#b52">(Zhu et al., 2020b)</ref>, which perform a quadratic all-pairs contrastive computation at each update step. The same analysis applies to variations of the GRACE method such as GCA <ref type="bibr" target="#b51">(Zhu et al., 2020a)</ref>.</p><p>Consider a graph with N nodes and M edges, and simple encoders E that compute embeddings in time and space O(N + M ). This property is satisfied by most popular GNN architectures such as convolutional <ref type="bibr" target="#b31">(Kipf &amp; Welling, 2017)</ref>, attentional <ref type="bibr">(Veličković et al., 2018)</ref>, or message-passing <ref type="bibr" target="#b14">(Gilmer et al., 2017)</ref> networks. BGRL performs four encoder computations per update step (twice for the target and online encoders, and twice for each augmentation) plus a node-level prediction step; GRACE performs two encoder computations (once for each augmentation), plus a node-level projection step. Both methods backpropagate the learning signal twice (once for each augmentation), and we assume the backward pass to be approximately as costly as a forward pass. We ignore the cost for computation of the augmentations in this analysis. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL ANALYSIS</head><p>We present an extensive empirical study of performance and scalability, showing that BGRL is effective across a wide range of settings from frozen linear evaluation to semi-supervised learning, and both when performing full-graph training and training on subsampled node neighborhoods. We give results across a range of dataset scales and encoder architectures including convolutional, attentional, and message-passing neural networks.</p><p>We analyze the performance of BGRL on a set of 7 standard transductive and inductive benchmark tasks, as well as in the very high-data regime by evaluating on the MAG240M dataset <ref type="bibr" target="#b25">(Hu et al., 2021)</ref>. We present results on medium-sized datasets where contrastive objectives can be computed on the entire graph (Section 4.1), on larger datasets where this objective must be approximated (Section 4.2), and finally on the much larger MAG240M dataset designed to test scalability limits (Section 4.3), showing that BGRL improves performance across all scales of datasets. In Appendix C, we show that BGRL achieves state-of-the-art performance even in the low-data regime on a set of 4 small-scale datasets. Dataset sizes are summarized in Table <ref type="table" target="#tab_0">2</ref> and described further in Appendix E.</p><p>Evaluation protocol: In most tasks, we follow the standard linear-evaluation protocol on graphs <ref type="bibr" target="#b47">(Veličković et al., 2019)</ref>. This involves first training each graph encoder in a fully unsupervised manner and computing embeddings for each node; a simple linear model is then trained on top of these frozen embeddings through a logistic regression loss with 2 regularization, without flowing any gradients back to the graph encoder network. In the more challenging MAG240M task, we extend BGRL to the semi-supervised setting by combining our self-supervised representation learning loss together with a supervised loss. We show that BGRL's bootstrapping objective obtains state-of-the-art performance in this hybrid setting, and even improves further with the added use of unlabeled data for representation learning -properties which have not been previously demonstrated by prior works on self-supervised representation learning on graphs.</p><p>Implementation details including model architectures and hyperparameters are provided in Appendix F. Algorithm implementation and experiment code for most tasks can be found at https://github.com/nerdslab/bgrl , while code for our solution on MAG240M has been open-sourced as part of the KDD Cup 2021 <ref type="bibr" target="#b0">(Addanki et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">PERFORMANCE AND EFFICIENCY GAINS WHEN SCALABILITY IS NOT A BOTTLENECK</head><p>We first evaluate our method on a set of 5 recent real-world datasets -WikiCS, Amazon-Computers, Amazon-Photos, Coauthor-CS, Coauthor-Physics -in the transductive setting. Note that these are challenging medium-scale datasets specifically proposed for rigorous evaluation of semi-supervised node classification methods <ref type="bibr" target="#b33">(Mernyei &amp; Cangea, 2020;</ref><ref type="bibr" target="#b42">Shchur et al., 2018)</ref>, but are almost all small enough that constrastive approaches such as GRACE <ref type="bibr" target="#b52">(Zhu et al., 2020b)</ref> can compute their quadratic objective exactly. Thus, these experiments present a comparison of BGRL with prior methods in the idealized case where scalability is not a bottleneck. We show that even in this steelmanned setting, our method outperforms or matches prior methods while requiring a fraction of the memory costs.</p><p>We primarily compare BGRL against GRACE, a recent strong contrastive representation learning method on graphs. We also report performances for other commonly used self-supervised graph methods from previously published results <ref type="bibr" target="#b38">(Perozzi et al., 2014;</ref><ref type="bibr" target="#b47">Veličković et al., 2019;</ref><ref type="bibr" target="#b36">Peng et al., 2020;</ref><ref type="bibr" target="#b20">Hassani &amp; Khasahmadi, 2020;</ref><ref type="bibr" target="#b51">Zhu et al., 2020a)</ref>, as well as Random-Init <ref type="bibr" target="#b47">(Veličković et al., 2019)</ref>, a baseline using embeddings from a randomly initialized encoder, thus measuring the quality of the inductive biases present in the encoder model. We use a 2-layer GCN model <ref type="bibr" target="#b31">(Kipf &amp; Welling, 2017)</ref> as our graph encoder E, and closely follow models, architectures, and graph-augmentation settings used in prior works <ref type="bibr" target="#b51">(Zhu et al., 2020a;</ref><ref type="bibr" target="#b47">Veličković et al., 2019;</ref><ref type="bibr" target="#b52">Zhu et al., 2020b)</ref>. Table <ref type="table">3</ref>: Performance measured in terms of classification accuracy along with standard deviations. Our experiments, marked as , are over 20 random dataset splits and model initializations. The other results are taken from previously published reports. OOM indicates running out of memory on a 16GB V100 GPU. We report the best result for GCA out of the proposed GCA-DE, GCA-PR, and GCA-EV models.</p><p>In Table <ref type="table">3</ref>, we report results of our experiments on these standard benchmark tasks. We see that even when scalability does not prevent the use of contrastive objectives, BGRL performs competitively both with our unsupervised and fully supervised baselines, achieving state-of-the-art performance in 4 of the 5 datasets. Further, as noted in Table <ref type="table">1</ref>, BGRL achieves this despite using 2-10x less memory.</p><p>BGRL provides this improvement in memory-efficiency at no cost in performance, demonstrating a useful practical advantage over prior methods such as GRACE.</p><p>Effect of more complex augmentations: In addition to the original GRACE method, we also highlight GCA, a variant of it that has the same learning objective but trades off more expressive but expensive graph augmentations for better performance. However, these augmentations often take time cubic in the size of the graph, or are otherwise cumbersome to implement on large graphs. As we focus on scalability to the high-data regime, we primarily restrict our comparisons to the base method GRACE, which uses the same simple, easily scalable augmentations as BGRL. Nevertheless, for the sake of completeness, in Table <ref type="table" target="#tab_2">4</ref> we investigate the effect of these complex augmentations with BGRL.</p><p>We see that BGRL obtains equivalent performance with both simple and complex augmentations, while GCA requires more expensive augmentations for peak performance. This indicates that BGRL can safely rely on simple augmentations when scaling to larger graphs without sacrificing performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">SCALABILITY-PERFORMANCE TRADE-OFFS FOR LARGE GRAPHS</head><p>When scaling up to large graphs, it may not be possible to compare each node's representation to all others. In this case, a natural way to reduce memory is to compare each node with only a subset of nodes in the rest of the graph. To study how the number of negatives impacts performance in this case, we propose an approximation of GRACE's objective called GRACE-SUBSAMPLING, where instead of contrasting every pair of nodes in the graph, we subsample k nodes randomly across the graph to use as negative examples for each node at every gradient step. Note that k = 2 is the asymptotic equivalent of BGRL in terms of memory costs, as BGRL always only compares each node with itself across both views, i.e., BGRL faces no such computational difficulty or design choice in scaling up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EVALUATING ON OGBN-ARXIV DATASET</head><p>To study the tradeoff between performance and complexity we consider a node classification task on a much larger dataset, from the OGB benchmark <ref type="bibr" target="#b23">(Hu et al., 2020a)</ref>, ogbn-arXiv. In this case, GRACE cannot run without subsampling (on a GPU with 16GB of memory). Considering the increased difficulty of this task, we slightly expand our model to use 3 GCN layers, following the baseline model provided by <ref type="bibr" target="#b23">Hu et al. (2020a)</ref>. As there has not been prior work on applying GNN-based unsupervised approaches to the ogbn-arXiv task, we implement and compare against two representative contrastivelearning approaches, DGI and GRACE. In addition, we report results from <ref type="bibr" target="#b23">Hu et al. (2020a)</ref> for node2vec <ref type="bibr" target="#b17">(Grover &amp; Leskovec, 2016</ref>) and a supervised-learning baseline. We report results on both validation and test sets, as is convention for this task since the dataset is split based on a chronological ordering.</p><p>Our results, summarized in Table <ref type="table" target="#tab_3">5</ref>, show that BGRL is competitive with the supervised learning baseline. Further, we note that the performance of GRACE-SUBSAMPLING is very sensitive to the parameter k-requiring a large number of negatives to match the performance of BGRL. Note that BGRL far exceeds the performance of GRACE-SUBSAMPLING with k = 2, its asymptotic equivalent in terms of memory; and that larger values of k lead to out-of-memory errors on a 16GB GPU. These results suggest that the performance of contrastive methods such as GRACE may suffer due to approximations to their objectives that must be made when scaling up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EVALUATING ON PROTEIN-PROTEIN INTERACTION DATASET</head><p>Next, we consider the Protein-Protein Interaction (PPI) task-a more challenging inductive task on multiple graphs where the gap between the best self-supervised methods and fully supervised methods is (still) significantly large, due to 40% of the nodes missing feature information. In addition to simple mean-pooling propagation rules from GraphSage-GCN <ref type="bibr" target="#b19">(Hamilton et al., 2017)</ref>, we also consider Graph Attention Networks (GAT, <ref type="bibr">Veličković et al., 2018)</ref> where each node aggregates features from its neighbors non-uniformly using a learned attention weight. It has been shown <ref type="bibr">(Veličković et</ref>  2018) that GAT improves over non-attentional models on this dataset when trained in supervised settings, but these models have thus far not been able to be trained to a higher performance than non-attentional models through contrastive techniques.</p><p>We report our results in  (Figure <ref type="figure">2</ref>), but also qualitatively different behaviors in the GAT models being trained. In Figure <ref type="figure" target="#fig_1">3</ref>, we examine the internals of GAT models trained through both BGRL and GRACE by analyzing the entropy of the attention weights learned. For each training node, we compute the average entropy of its attention weights across all GAT layers and attention heads, minus the entropy of a uniform attention distribution as a baseline. We see that GAT models learned using GRACE, particularly when subsampling few negative examples, tend to have very low attention entropies and perform poorly. On the other hand, BGRL is able to train the model to have meaningful attention weights, striking a balance between the low-entropy models learned through GRACE and the maximum-entropy uniform-attention distribution. This aligns with recent observations <ref type="bibr" target="#b28">(Kim &amp; Oh, 2021;</ref><ref type="bibr" target="#b48">Wang et al., 2019)</ref> that auxiliary losses must be chosen carefully for the stability of GAT attention weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">SCALING TO EXTREMELY LARGE GRAPHS</head><p>To further test the scalability and evaluate the performance of BGRL in the very high-data regime, we consider the MAG240M node classification task <ref type="bibr" target="#b25">(Hu et al., 2021)</ref>. As a single connected graph of 360GB with over 240 million nodes (of which 1.4 million are labeled) and 1.7 billion edges, this dataset is orders of magnitude larger than previously available public datasets, and poses a significant scaling challenge. Since the test labels for this dataset are (still) hidden, we report performance based on validation accuracies in our experiments. Implementation and experiment details are in Appendix G.</p><p>To account for the increased scale and difficulty of the classification task on this dataset, we make a number of changes to our learning setup. First, since we can no longer perform full-graph training due to the sheer size of the graph, we thus adopt the Neighborhood Sampling strategy proposed by <ref type="bibr" target="#b19">Hamilton et al. (2017)</ref> to sample a small number of central nodes at which our loss is to be applied, and sampling a fixed size neighborhood around them. Second, we use more expressive Message Passing Neural Networks <ref type="bibr" target="#b14">(Gilmer et al., 2017)</ref> as our graph encoders. Finally, as we are interested in pushing performance on this competition dataset, we make use of the available labels for representation learning and shift from evaluating on top of a frozen representation, to semi-supervised training by combining both supervised and self-supervised signals at each update step. We emphasize that these are significant changes from the standard small-scale evaluation setup for graph representation learning methods studied previously, and more closely resemble real-world conditions in which these algorithms would be employed. In Figure <ref type="figure">4</ref>, we see that BGRL used as an auxiliary signal is able to learn faster and significantly improve final performance over fully supervised learning on this challenging task. Considering the difficulty of this task and the small gap in final performance between winning entries in the KDD Cup 2021 contest, this is a significant improvement. On the other hand, GRACE-SUBSAMPLING provides much lower benefits over fully supervised learning, possibly due to no longer being able to sample sufficiently many negatives over the whole graph. Here we used k = 256 negatives, the largest value we were able to run without running out of memory.</p><p>We further show that we can leverage the high scalability of BGRL to make use of the vast amounts of unlabeled data present in the dataset. Since labeled nodes form only 0.5% of the graph, unlabeled data offers a rich self-supervised signal for learning better representations and ultimately improving performance on the supervised task. In Figure <ref type="figure">5</ref>, we consider adding some number of unlabeled nodes to each minibatch of data, and examine the effect on performance as this ratio of unlabeled to labeled data in each batch increases. Thus at each step, we apply the supervised loss on only the labeled nodes in the batch, and BGRL on all nodes. Note that a ratio of 0 corresponds to the case where we apply BGRL as an auxiliary loss only to the training nodes, already examined in Figure <ref type="figure">4</ref>. We observe a dramatic increase in both stability and peak performance as we increase this ratio, showing that BGRL can utilize the unlabeled nodes effectively to shape higher quality representations and prevent early overfitting to the supervised signal. This effect shows steady improvement as we increase this ratio from 1x to 10x unlabeled data, where we stop due to resource constraints on running ablations on this large-scale graph -however, this trend may continue to even higher ratios, as the true ratio of unlabeled to labeled nodes present in the graph is 99x.</p><p>This result of 73.89% is state-of-the-art for this dataset for the highest single-model performance (i.e., without ensembling) -the OGB baselines report a score of 70.02% <ref type="bibr" target="#b25">(Hu et al., 2021)</ref> while the KDD Cup 2021 contest first place solution reported a score of 73.71% before ensembling.</p><p>KDD Cup 2021: Our solution using BGRL to shape representations, utilizing unlabeled data in conjunction with a supervised signal for semi-supervised learning, was awarded as one of the winners of the MAG240M track at OGB-LSC <ref type="bibr" target="#b0">(Addanki et al., 2021)</ref>. It achieved a final position of second overall, achieving 75.19% accuracy on the test set. The first and third place solutions achieved 75.49% and 74.60% respectively. Although differences in many other factors such as model architectures, feature engineering, ensembling strategies, etc. prevent a direct comparison<ref type="foot" target="#foot_0">1</ref> between these solutions, these results serve as a strong empirical evidence for the effectiveness of BGRL for learning representations on extremely large scale datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>Early methods in the area relied on random-walk objectives such as DeepWalk <ref type="bibr" target="#b38">(Perozzi et al., 2014)</ref> and node2vec <ref type="bibr" target="#b17">(Grover &amp; Leskovec, 2016)</ref>. Even though the graph neural networks (GNNs) inductive bias aligns with these objectives <ref type="bibr" target="#b49">(Wu et al., 2019;</ref><ref type="bibr" target="#b47">Veličković et al., 2019;</ref><ref type="bibr" target="#b31">Kipf &amp; Welling, 2017)</ref>, composing GNNs and random-walks does not work very well and can even degrade performance <ref type="bibr" target="#b19">(Hamilton et al., 2017)</ref>. Earlier combinations of GNNs and self-supervised learning involve Embedding Propagation (García-Durán &amp; Niepert, 2017), Variational Graph Autoencoders <ref type="bibr" target="#b30">(Kipf &amp; Welling, 2016)</ref> and Graph2Gauss <ref type="bibr">(Bojchevski &amp; Günnemann, 2018)</ref>. <ref type="bibr" target="#b24">Hu et al. (2020b)</ref> leverages BERT <ref type="bibr" target="#b12">(Devlin et al., 2019)</ref> for representation learning in graph-structured inputs. <ref type="bibr" target="#b24">Hu et al. (2020b)</ref> assumes specific graph structures and uses feature masking objectives to shape representations.</p><p>Recently, contrastive methods effective on images have also been adapted to graphs using GNNs. This includes DGI <ref type="bibr" target="#b47">(Veličković et al., 2019)</ref>  <ref type="bibr" target="#b52">(Zhu et al., 2020b;</ref><ref type="bibr">a)</ref> that rely on more complex data-adaptive augmentations. GraphCL <ref type="bibr" target="#b50">(You et al., 2020)</ref> adapts SimCLR to learn graph-level embeddings using a contrastive objective. MVGRL <ref type="bibr" target="#b20">(Hassani &amp; Khasahmadi, 2020)</ref> generalizes CMC <ref type="bibr" target="#b45">(Tian et al., 2020)</ref> to graphs. Graph Barlow Twins <ref type="bibr" target="#b3">(Bielak et al., 2021)</ref> presents a method to learn representations by minimizing correlation between different representation dimensions.</p><p>Concurrent works DGB (Che et al., 2020) and SelfGNN (Kefato &amp; Girdzijauskas, 2021), like BGRL, adapt BYOL <ref type="bibr" target="#b16">(Grill et al., 2020)</ref> for graph representation learning. However, BGRL differs from these works in the following ways:</p><p>• We show that BGRL scales to and attains state-of-the-art results on the very high-data regime on the 360GB MAG240M graph with 240 million nodes. These large-scale results are unprecedented in the graph self-supervised learning literature and demonstrate a high degree of scalability.</p><p>• We show that BGRL is effective even when training on sampled subgraphs instead of the full graph, whereas DGB and SelfGNN only consider full-graph training in small-to-medium scale datasets.</p><p>• We provide an extensive analysis of the performance-computation trade-off of BGRL versus contrastive methods, showing that BGRL can be more efficient in terms of computation and memory usage as it requires no negative examples, whereas SelfGNN and DGB only consider the metric of task performance.</p><p>• We show that BGRL is effective when performing semi-supervised training, providing further gains when leveraging both labeled data and unlabeled data. This is a significant result that has not been demonstrated in graph representation learning methods using neural networks prior to our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A BGRL DOES NOT CONVERGE TO TRIVIAL SOLUTIONS</head><p>In Figure <ref type="figure">6</ref> we show the BGRL loss curve throughout training for all the datasets considered. As we see, the loss does not converge to zero, indicating that the training does not result in a trivial solution.</p><p>In Figure <ref type="figure">7</ref> we plot the spread of the node embeddings, i.e., the standard deviation of the representations learned across all nodes, divided by the average norm. As we see, the embeddings learned across all datasets have a standard deviation that is a similar order of magnitude as the norms of the embeddings themselves, further indicating that the training dynamics do not converge to a constant solution.</p><p>Further, Figure <ref type="figure" target="#fig_3">8</ref> shows that the embeddings do not collapse to zero or blow up as training progresses. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B ABLATIONS ON PROJECTOR NETWORK</head><p>As noted in Section 2, BGRL does not use a projector network, unlike both BYOL and GRACE. Prior works such as GRACE use a projector network to prevent the embeddings from becoming completely invariant to the augmentations used -however in BGRL, the predictor network can serve the same purpose. On the other hand, BYOL relies this for dimensionality reduction, to simplify the task of the predictor p θ , as it is challenging to directly predict very high-dimensional embeddings. required for large-scale vision tasks like ImageNet <ref type="bibr" target="#b10">(Deng et al., 2009)</ref>. Here we empirically verify that even in our most challenging, large-scale task of MAG240M, the projector network is not needed and only slows down learning. In Figure <ref type="figure" target="#fig_4">9</ref> we can see that adding the projector network leads to both slower learning and a lower final performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C COMPARISON ON SMALL DATASETS</head><p>We perform additional experiments on 4 commonly used small datasets, (Cora, CiteSeer, PubMed, and DBLP) <ref type="bibr" target="#b41">(Sen et al., 2008;</ref><ref type="bibr" target="#b4">Bojchevski &amp; Günnemann, 2017)</ref> and show that BGRL's bootstrapping mechanism still performs well in the low-data regime, even attaining a new state of the art performance on two of the datasets.</p><p>Note that Table <ref type="table" target="#tab_7">7</ref> reports results averaged over 20 random dataset splits, as has been followed in <ref type="bibr" target="#b52">Zhu et al. (2020b)</ref>, instead of using the standard fixed splits for these datasets which are known to be easy to unreliable for evaluating GNN methods <ref type="bibr" target="#b42">(Shchur et al., 2018)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D GRAPH AUGMENTATION FUNCTIONS</head><p>Generating meaningful augmentations is a much less explored problem in graphs than in other domains such as vision. Further, since we work over entire graphs, complex augmentations can be very expensive to compute and will impact all nodes at once. Our contributions are orthogonal to this problem, and we primarily consider only the standard graph augmentation pipeline that has been used in previous works on representation learning <ref type="bibr" target="#b50">(You et al., 2020;</ref><ref type="bibr" target="#b52">Zhu et al., 2020b)</ref>.</p><p>In particular, we consider two simple graph augmentation functions -node feature masking and edge masking. These augmentations are graph-wise: they do not operate on each node independently, and instead leverage graph topology information through edge masking. This contrasts with transformations used in BYOL, operate on each image independently. First, we generate a single random binary mask of size F , each element of which follows a Bernoulli distribution B(1 − p f ), and use it to mask features of all nodes in the graph (i.e., all nodes have the same features masked). Empirically, we found that performance is similar for using different random masks per node or sharing them, and so we use a single mask for simplicity. In addition to this node-level attribute transformation, we also compute a binary mask of size E (where E is the number of edges in the original graph), each element of which follows a Bernoulli distribution B(1 − p e ), and use it to mask edges in the augmented graph. To compute our final augmented graphs, we make use of both augmentation functions with different hyperparameters for each graph, i.e. p f1 and p e1 for the first view, and p f2 and p e2 for the second view.</p><p>Beyond these standard augmentations, in Section 4.1 we also consider more complex adaptive augmentations proposed by prior works <ref type="bibr" target="#b51">(Zhu et al., 2020a)</ref> which use various heuristics to mask different features or edges with different probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E DATASET DETAILS</head><p>WikiCS<ref type="foot" target="#foot_1">2</ref> This graph is constructed from Wikipedia references, with nodes representing articles about Computer Science and edges representing links between them. Articles are classified into 10 classes based on their subfield, and node features are the average of GloVE <ref type="bibr" target="#b37">(Pennington et al., 2014)</ref> embeddings of all words in the article. This dataset comes with 20 canonical train/valid/test splits, which we use directly.</p><p>Amazon Computers, Amazon Photos<ref type="foot" target="#foot_2">3</ref> These graphs are from the Amazon co-purchase graph <ref type="bibr">(McAuley et al., 2015)</ref> with nodes representing products and edges being between pairs of goods frequently purchased together. Products are classified into 10 (for Computers) and 8 (for Photos) classes based on product category, and node features are a bag-of-words representation of a product's reviews. We use a random split of the nodes into (10/10/80%) train/validation/test nodes respectively as these datasets do not come with a standard dataset split.</p><p>Coauthor CS, Coauthor Physics<ref type="foot" target="#foot_3">4</ref> These graphs are from the Microsoft Academic Graph <ref type="bibr">(Sinha et al., 2015)</ref>, with nodes representing authors and edges between authors who have co-authored a paper. Authors are classified into 15 (for CS) and 5 (for Physics) classes based on the author's research field, and node features are a bag-of-words representation of the keywords of an author's papers. We again use a random (10/10/80%) split for these datasets.</p><p>ogbn-arXiv: This is another citation network, where nodes represent CS papers on arXiv indexed by the Microsoft Academic Graph <ref type="bibr">(Sinha et al., 2015)</ref>. In our experiments, we symmetrize this graph and thus there is an edge between any pair of nodes if one paper has cited the other. Papers are classified into 40 classes based on arXiv subject area. The node features are computed as the average word-embedding of all words in the paper, where the embeddings are computed using a skip-gram model <ref type="bibr" target="#b34">(Mikolov et al., 2013)</ref> over the entire corpus.</p><p>PPI<ref type="foot" target="#foot_4">5</ref> is a protein-protein interaction network <ref type="bibr" target="#b53">(Zitnik &amp; Leskovec, 2017;</ref><ref type="bibr" target="#b19">Hamilton et al., 2017)</ref>, comprised of multiple ( <ref type="formula">24</ref>) graphs each corresponding to different human tissues. We use the standard dataset split as 20 graphs for training, 2 for validation, and 2 for testing. Each node has 50 features computed from various biological properties. This is a multilabel classification task, where each node can possess up to 121 labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F IMPLEMENTATION DETAILS</head><p>In all experiments, we use the AdamW optimizer <ref type="bibr" target="#b29">(Kingma &amp; Ba, 2015;</ref><ref type="bibr" target="#b18">Gugger &amp; Howard, 2018)</ref> with weight decay set to 10 −5 , and all models initialized using Glorot initialization <ref type="bibr" target="#b15">(Glorot &amp; Bengio, 2010)</ref>. The BGRL predictor p θ used to predict the embedding of nodes across views is fixed to be a Multilayer Perceptron (MLP) with a single hidden layer. The decay rate τ controlling the rate of updates of the BGRL target parameters φ is initialized to 0.99 and gradually increased to 1.0 over the course of training following a cosine schedule. Other model architecture and training details vary per dataset and are described further below. The augmentation hyperparameters p f1,2 and p e1,2 are reported below.</p><p>Graph Convolutional Networks Formally, the GCN propagation rule <ref type="bibr" target="#b31">(Kipf &amp; Welling, 2017)</ref> for a single layer is as follows,</p><formula xml:id="formula_2">GCN i (X, A) = σ D − 1 2 A D − 1 2 XW i ,<label>(4)</label></formula><p>where A = A + I is the adjacency matrix with self-loops, D is the degree matrix, σ is a non-linearity such as ReLU, and W i is a learned weight matrix for the i'th layer.</p><p>Mean Pooling Rule Formally, the Mean Pooling <ref type="bibr" target="#b19">(Hamilton et al., 2017)</ref> rule for a single layer is given by: MP</p><formula xml:id="formula_3">i (X, A) = σ( D −1 AXW i )<label>(5)</label></formula><p>As proposed by <ref type="bibr" target="#b47">Veličković et al. (2019)</ref>, our exact encoder in inductive experiments E is a 3-layer mean-pooling network with skip connections. We use a layer size of 512 and PReLU <ref type="bibr">(He et al., 2015)</ref> activation. Thus, we compute:</p><formula xml:id="formula_4">H 1 = σ(MP 1 (X, A)) (6) H 2 = σ(MP 2 (H 1 + XW skip , A)) (7) E(X, A) = σ(MP 3 (H 2 + H 1 + XW skip , A))<label>(8)</label></formula><p>Graph Attention Networks The GAT layer <ref type="bibr">(Veličković et al., 2018)</ref> consists of a learned matrix W that transforms each node features. We then use self-attention to compute attention coefficient for a pair of nodes i and j as e ij = a(h i , h j ). The attention function a is computed as LeakyReLU(a[Wh i ||Wh j ]), where a is a learned matrix transforming a pair of concatenated attention queries into a single scalar attention logit. The weight of the edge between nodes i and j is computed as α ij = softmax j (e ij ). We follow the architecture proposed by <ref type="bibr">Veličković et al. (2018)</ref>, including a 3-layer GAT model (with the first 2 layers consisting of 4 heads of size 256 each and the final layer size 512 with 6 output heads), ELU activation <ref type="bibr" target="#b9">(Clevert et al., 2016)</ref>, and skip-connections in intermediate layers.</p><p>Model architectures As described in Section 4, we use GCN <ref type="bibr" target="#b31">(Kipf &amp; Welling, 2017)</ref> encoders in our experiments on the smaller transductive tasks, while on the inductive task of PPI we use MeanPooling encoders with residual connections. The BGRL predictor p θ is implemented as a mutilayer perceptron (MLP). We also used stabilization techniques like batch normalization <ref type="bibr" target="#b26">(Ioffe &amp; Szegedy, 2015)</ref>, layer normalization <ref type="bibr" target="#b1">(Ba et al., 2016)</ref>, and weight standardization <ref type="bibr" target="#b39">(Qiao et al., 2019)</ref>. The decay rate use for statistics in the batch normalization is fixed to 0.99. We use PReLU activation <ref type="bibr">(He et al., 2015)</ref> in all experiments except those using a GAT encoder, where we use the ELU activation <ref type="bibr" target="#b9">(Clevert et al., 2016)</ref>. In all our models, at each layer including the final layer, we apply first the batch/layer normalization as applicable, then the activation function. Table <ref type="table" target="#tab_8">8</ref> describes hyperparameter and architectural details for most of our experimental setups with BGRL.</p><p>In addition to these standard settings, we perform additional experiments on the PPI dataset using a GAT <ref type="bibr">(Veličković et al., 2018)</ref> model as the encoder. When using the GAT encoder on PPI, we use 3 attention layers -the first two with 4 attention heads of size 256 each, and the final with 6 attention heads of size 512, following a very similar model proposed by <ref type="bibr">Veličković et al. (2018)</ref>. We concatenate the attention head outputs for the first 2 layers, and use the mean for the final output. We also use the ELU activation <ref type="bibr" target="#b9">(Clevert et al., 2016)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Augmentation parameters</head><p>The hyperparameter settings for graph augmentations, as well as the sizes of the embeddings and hidden layers, very closely follow previous work <ref type="bibr" target="#b52">(Zhu et al., 2020b;</ref><ref type="bibr">a)</ref> on all datasets with the exception of ogbn-arXiv. On this dataset, since there has not been prior work on applying self-supervised graph learning methods, we provide the hyperparameters we found through a small grid search.</p><p>Optimization settings We perform full-graph training at each gradient step on all small-scale experiments, with the exception of experiments using GAT encoders on the PPI dataset. Here, due to memory constraints, we perform training with a batch size of 1 graph. Since the PPI dataset consists of multiple smaller, disjoint subgraphs, we do not have to perform any node subsampling at training time.</p><p>We use Glorot initialization <ref type="bibr" target="#b15">(Glorot &amp; Bengio, 2010)</ref> the AdamW optimizer <ref type="bibr" target="#b29">(Kingma &amp; Ba, 2015;</ref><ref type="bibr" target="#b18">Gugger &amp; Howard, 2018)</ref> with a base learning rate η base and weight decay set to 10 −5 . The learning rate is annealed using a cosine schedule over the course of learning of n total total steps with an initial warmup period of n warmup steps. Hence, the learning rate at step i is computed as</p><formula xml:id="formula_5">η i i×η base nwarmup if i ≤ n warmup , η base × 1 + cos (i−nwarmup)×π ntotal−nwarmup × 0.5 if n warmup ≤ i ≤ n total .</formula><p>We fix n total to be 10,000 total steps and n warmup to 1,000 warmup steps, with the exception of experiments on the GAT encoder that requires using a batch size of 1 graph on the PPI dataset. In this case, we increase the number of total steps to 20,000 and warmup to 2,000 steps.</p><p>The target network parameters φ are initialized randomly from the same distribution of the online parameters θ but with a different random seed. The decay parameter τ is also updated using a cosine schedule starting from an initial value of τ base = 0.99 and is computed as</p><formula xml:id="formula_6">τ i 1 − (1 − τ base ) 2 × cos i × π n total + 1 .</formula><p>These annealing schedules for both η and τ follow the procedure used by <ref type="bibr" target="#b16">Grill et al. (2020)</ref>.</p><p>Frozen linear evaluation of embeddings In the linear evaluation protocol, the final evaluation is done by fitting a linear classifier on top of the frozen learned embeddings without flowing any gradients back to the encoder. For the smaller datasets of WikiCS, Amazon Computers/Photos, and Coauthor CS/Physics, we use an 2 -regularized LogisticRegression classifier from Scikit-Learn <ref type="bibr" target="#b35">(Pedregosa et al., 2011)</ref> using the 'liblinear' solver. We do a hyperparameter search over the regularization strength to be between {2 −10 , 2 −9 , . . . 2 9 , 2 10 }.</p><p>For larger PPI and ogbn-arXiv datasets, where the liblinear solver takes too long to converge, we instead perform 100 steps of gradient descent using AdamW with learning rate 0.01, with a smaller hyperparameter search on the weight decay between {2 −10 , 2 −8 , 2 −6 , . . . 2 6 , 2 8 , 2 10 }.</p><p>In all cases, we 2 -normalize the frozen learned embeddings over the entire graph before fitting the classifier on top. OGB-LSC MAG240M Dataset: This is a heterogeneous graph introduced for the KDD Cup 2021 <ref type="bibr" target="#b25">(Hu et al., 2021)</ref>, comprised of 121 million academic papers, 122 million authors, and 26 thousand institutions. Papers are represented by 768-dimensional BERT embeddings <ref type="bibr" target="#b12">(Devlin et al., 2019)</ref>, and the task is to classify arXiv papers into one of 153 categories, where 1% of the paper nodes are from arXiv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G MAG240M EXPERIMENT DETAILS</head><p>Message Passing Neural Networks encoders: We use a bi-directional version of the standard MPNN <ref type="bibr" target="#b14">(Gilmer et al., 2017)</ref> architectures with 4 message passing steps, a hidden size of 256 at each layer, with node and edge update functions represented by Multilayer Perceptrons (MLPs) with 2 hidden layers of size 512 each.</p><p>Node Neighborhood Sampling: Since we can no longer perform full-graph training, we sample a batch size of 1024 central nodes split across 8 devices, and subsample a fixed-size neighborhood for each. Specifically, we sample a depth-2 neighborhood with different numbers of neighbors sampled per layer depending on the type (paper, author, institution) of each neighbor. We sample up to 80 papers and 20 authors for each paper; and 40 papers and 10 institutions per author.</p><p>Other hyperparamters: We use edge masking probability p e of 0.2 and feature masking probability p f of 0.4 for each augmentation. We use a higher decay rate τ , starting at 0.999 and decayed to 1.0 with a cosine schedule. We use AdamW optimizer with a weight decay of 10 −5 , and a learning rate starting at 0.01 and annealed to 0 over the course of learning with a warmup step equal to 10% the period of learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Histogram of GAT attention entropies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure4: Performance on MAG240M using BGRL or GRACE-SUBSAMPLING as an auxiliary signal, averaged over 5 seeds and run for 50k steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 :</head><label>8</label><figDesc>Figure 6: BGRL Loss</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 :</head><label>9</label><figDesc>Figure9: Performance on OGB-LSC MAG240M task, averaged over 5 seeds, testing effect of using projector network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Full</head><label></label><figDesc>implementation and experiment code has been open-sourced as part of the KDD Cup 2021. Key implementation details and hyperparameter descriptions are reproduced below.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Thus the total time and space complexity per update step for BGRL is 6C encoder (M + N ) + 4C prediction N + C BGRL N , compared to 4C encoder (M + N ) + 4C projection N + C GRACE N 2 for GRACE, where C • are constants depending on architecture of the different components. Table1shows an empirical comparison of BGRL and GRACE's computational requirements on a set of benchmark tasks. Statistics of datasets used in our experiments.</figDesc><table><row><cell>Dataset</cell><cell cols="3">Amazon Photos WikiCS</cell><cell cols="3">Amazon Computers Coauthor CS Coauthor Phy</cell></row><row><cell>#Nodes</cell><cell>7,650</cell><cell></cell><cell>11,701</cell><cell>13,752</cell><cell></cell><cell>18,333</cell><cell>34,493</cell></row><row><cell>#Edges</cell><cell cols="2">119,081</cell><cell>216,123</cell><cell>245,861</cell><cell></cell><cell>81,894</cell><cell>247,962</cell></row><row><cell>BGRL Memory</cell><cell cols="2">0.47 GB</cell><cell cols="2">0.63 GB 0.58 GB</cell><cell></cell><cell>2.86 GB</cell><cell>5.50 GB</cell></row><row><cell cols="3">GRACE Memory 1.81 GB</cell><cell cols="2">3.82 GB 5.14 GB</cell><cell></cell><cell>11.78 GB</cell><cell>OOM</cell></row><row><cell cols="7">Table 1: Comparison of computational requirements on a set of standard benchmark graphs. OOM indicates</cell></row><row><cell cols="4">ruuning out of memory on a 16GB V100 GPU.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Task</cell><cell></cell><cell>Nodes</cell><cell cols="2">Edges Features</cell><cell>Classes</cell></row><row><cell>WikiCS</cell><cell></cell><cell>Transductive</cell><cell></cell><cell>11,701</cell><cell>216,123</cell><cell>300</cell><cell>10</cell></row><row><cell cols="3">Amazon Computers Transductive</cell><cell></cell><cell>13,752</cell><cell>245,861</cell><cell>767</cell><cell>10</cell></row><row><cell>Amazon Photos</cell><cell></cell><cell>Transductive</cell><cell></cell><cell>7,650</cell><cell>119,081</cell><cell>745</cell><cell>8</cell></row><row><cell>Coauthor CS</cell><cell></cell><cell>Transductive</cell><cell></cell><cell>18,333</cell><cell>81,894</cell><cell>6,805</cell><cell>15</cell></row><row><cell cols="2">Coauthor Physics</cell><cell>Transductive</cell><cell></cell><cell>34,493</cell><cell>247,962</cell><cell>8,415</cell><cell>5</cell></row><row><cell>ogbn-arxiv</cell><cell></cell><cell>Transductive</cell><cell cols="2">169,343</cell><cell>1,166,243</cell><cell>128</cell><cell>40</cell></row><row><cell>PPI (24 graphs)</cell><cell></cell><cell>Inductive</cell><cell></cell><cell>56,944</cell><cell>818,716</cell><cell>50 121 (multilabel)</cell></row><row><cell>MAG240M</cell><cell></cell><cell cols="4">Transductive 244,160,499 1,728,364,232</cell><cell>768</cell><cell>153</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Eigenvector centrality 92.95 ± 0.13 95.73 ± 0.03 87.54 ± 0.49 92.24 ± 0.21 Comparison of BGRL and GCA for simple versus complex augmentation heuristics on four benchmark graphs. For GCA, we report the numbers provided in their original paper.</figDesc><table><row><cell cols="2">Method Augmentation</cell><cell>Co.CS</cell><cell>Co.Phy</cell><cell>Am. Comp.</cell><cell>Am. Photos</cell></row><row><cell>BGRL</cell><cell>Standard</cell><cell cols="4">93.31 ± 0.13 95.73 ± 0.05 90.34 ± 0.19 93.17 ± 0.30</cell></row><row><cell></cell><cell>Degree centrality</cell><cell cols="4">93.34 ± 0.13 95.62 ± 0.09 90.39 ± 0.22 93.15 ± 0.37</cell></row><row><cell></cell><cell>Pagerank centrality</cell><cell cols="4">93.34 ± 0.11 95.59 ± 0.09 90.45 ± 0.25 93.13 ± 0.34</cell></row><row><cell></cell><cell cols="5">Eigenvector centrality 93.32 ± 0.15 95.62 ± 0.06 90.20 ± 0.27 93.03 ± 0.39</cell></row><row><cell>GCA</cell><cell>Standard</cell><cell>92.</cell><cell></cell><cell></cell></row></table><note>93 ± 0.01 95.26 ± 0.02 86.25 ± 0.25 92.15 ± 0.24 Degree centrality 93.10 ± 0.01 95.68 ± 0.05 87.85 ± 0.31 92.49 ± 0.09 Pagerank centrality 93.06 ± 0.03 95.72 ± 0.03 87.80 ± 0.23 92.53 ± 0.16</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Performance on the ogbn-arXiv task measured in terms of classification accuracy along with standard deviations. Our experiments, marked as , are averaged over 20 random model initializations. Other results are taken from previously published reports. OOM indicates running out of memory on a 16GB V100 GPU.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>al., Performance on the PPI task measured in terms of Micro-F1 across the 121 labels along with standard deviations. Our experiments, marked as , are averaged over 20 random model initializations. Other results are taken from previously published reports.</figDesc><table><row><cell>PPI</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Table6, showing that BGRL is competitive with GRACE when using the simpler MeanPooling networks. Applying BGRL to a GAT model, results in a new state-of-the-art performance, improving over the MeanPooling network. On the other hand, the GRACE contrastive loss is unable to improve performance of a GAT model over the non-attentional MeanPooling encoder. We observe that the approximation of the contrastive objective results not only in lower accuracies</figDesc><table><row><cell>Micro f1</cell><cell>0.600 0.625 0.650 0.675 0.700</cell><cell></cell><cell>1000 1500 2000</cell><cell cols="2">Grace sampling 16 Grace sampling 32 Grace sampling 64 Grace sampling 128 Grace sampling 256 Grace sampling all BGRL</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0 0.525 0.550 0.575</cell><cell>2500 5000 7500 10000 12500 15000 17500 Training Step Grace sampling 16 Grace sampling 32 Grace sampling 64 Grace sampling 128 BGRL Grace sampling all Grace sampling 256</cell><cell>0 500</cell><cell>6</cell><cell>5</cell><cell>4</cell><cell>3 Entropy</cell><cell>2</cell><cell>1</cell><cell>0</cell></row><row><cell cols="3">Figure 2: PPI task performance, averaged over 20 seeds.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Evaluation on small datasets. Results averaged over 20 dataset splits and model initializations.</figDesc><table><row><cell></cell><cell>Cora</cell><cell>CiteSeer</cell><cell>PubMed</cell><cell>DBLP</cell></row><row><cell>GRACE</cell><cell cols="4">83.02 ± 0.89 71.63 ± 0.64 86.06 ± 0.26 84.08 ± 0.29</cell></row><row><cell>BGRL</cell><cell cols="4">83.83 ± 1.61 72.32 ± 0.89 86.03 ± 0.33 84.07 ± 0.23</cell></row><row><cell cols="2">Supervised 82.8</cell><cell>72.0</cell><cell>84.9</cell><cell>82.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>, and skip connections in the intermediate attention layers, as suggested byVeličković et al. (2018). Hyperparameter settings for unsupervised BGRL learning.</figDesc><table><row><cell>Dataset</cell><cell cols="6">WikiCS Am. Computers Am. Photos Co. CS Co. Physics ogbn-arXiv</cell><cell>PPI</cell></row><row><cell>p f,1</cell><cell>0.2</cell><cell>0.2</cell><cell>0.1</cell><cell>0.3</cell><cell>0.1</cell><cell>0.0</cell><cell>0.25</cell></row><row><cell>p f,2</cell><cell>0.1</cell><cell>0.1</cell><cell>0.2</cell><cell>0.4</cell><cell>0.4</cell><cell>0.0</cell><cell>0.00</cell></row><row><cell>pe,1</cell><cell>0.2</cell><cell>0.5</cell><cell>0.4</cell><cell>0.3</cell><cell>0.4</cell><cell>0.6</cell><cell>0.30</cell></row><row><cell>pe,2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.1</cell><cell>0.2</cell><cell>0.1</cell><cell>0.6</cell><cell>0.25</cell></row><row><cell>η base</cell><cell>5 • 10 −4</cell><cell>5 • 10 −4</cell><cell>10 −4</cell><cell>10 −5</cell><cell>10 −5</cell><cell>10 −2</cell><cell>5 • 10 −3</cell></row><row><cell>embedding size</cell><cell>256</cell><cell>128</cell><cell>256</cell><cell>256</cell><cell>128</cell><cell>256</cell><cell>512</cell></row><row><cell>E hidden sizes</cell><cell>512</cell><cell>256</cell><cell>512</cell><cell>512</cell><cell>256</cell><cell>256, 256</cell><cell>512, 512</cell></row><row><cell>p θ hidden sizes</cell><cell>512</cell><cell>512</cell><cell>512</cell><cell>512</cell><cell>512</cell><cell>256</cell><cell>512</cell></row><row><cell>batch norm</cell><cell>Y</cell><cell>Y</cell><cell>Y</cell><cell>Y</cell><cell>Y</cell><cell>N</cell><cell>N</cell></row><row><cell>layer norm</cell><cell>N</cell><cell>N</cell><cell>N</cell><cell>N</cell><cell>N</cell><cell>Y</cell><cell>Y</cell></row><row><cell>weight standard.</cell><cell>N</cell><cell>N</cell><cell>N</cell><cell>N</cell><cell>N</cell><cell>Y</cell><cell>N</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">For example, the first place solution used a much larger set of 30 ensembled models compared to our 10, and exclusively relied on architectural improvements to improve performance without using self-supervised learning.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://github.com/pmernyei/wiki-cs-dataset/raw/master/dataset</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://github.com/shchur/gnn-benchmark/tree/master/data/npz</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">https://github.com/shchur/gnn-benchmark/tree/master/data/npz</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">https://s3.us-east-2.amazonaws.com/dgl.ai/dataset/ppi.zip</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Large-scale graph representation learning with very deep gnns and self-supervision</title>
		<author>
			<persName><forename type="first">Ravichandra</forename><surname>Addanki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreea</forename><surname>Deac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Godwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Keck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wai</forename><surname>Lok Sibon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacklynn</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Stott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Thakoor</surname></persName>
		</author>
		<author>
			<persName><surname>Velickovic</surname></persName>
		</author>
		<idno>CoRR, abs/2107.09422</idno>
		<ptr target="https://arxiv.org/abs/2107.09422" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>ArXiv, abs/1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mutual information neural estimation</title>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Ishmael Belghazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aristide</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Rajeshwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v80/belghazi18a.html" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Graph barlow twins: A self-supervised representation learning framework for graphs</title>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bielak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Kajdanowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<idno>CoRR, abs/2106.02466</idno>
		<ptr target="https://arxiv.org/abs/2106.02466" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03815</idno>
		<title level="m">Deep gaussian embedding of graphs: Unsupervised inductive learning via ranking</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep gaussian embedding of graphs: Unsupervised inductive learning via ranking</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1ZdKJ-0W" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Self-supervised graph representation learning via bootstrapping</title>
		<author>
			<persName><forename type="first">Lowik</forename><surname>Chanussot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammed</forename><surname>Shuaibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgane</forename><surname>Riviere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Heras-Domingo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caleb</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.1021/acscatal.0c04525</idno>
		<idno type="arXiv">arXiv:2011.05126</idno>
		<ptr target="http://dx.doi.org/10.1021/acscatal.0c04525.FeihuChe" />
	</analytic>
	<monogr>
		<title level="j">ACS Catalysis</title>
		<idno type="ISSN">2155-5435</idno>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="6059" to="6072" />
			<date type="published" when="2020">May 2021. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Open catalyst 2020 (oc20) dataset and community challenges</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v119/chen20j.html" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/fcbc95ccdd551da181207c0c1400c655-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName><forename type="first">Djork-Arné</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1511.07289" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">09</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">ETA prediction with graph neural networks in google maps</title>
		<author>
			<persName><forename type="first">Austin</forename><surname>Derrow-Pinion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Hester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Nunkesser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seongjae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueying</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brett</forename><surname>Wiltshire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishal</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<idno>CoRR, abs/2108.11482</idno>
		<ptr target="https://arxiv.org/abs/2108.11482" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://www.aclweb.org/anthology/N19-1423" />
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter</title>
				<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning graph representations with embedding propagation</title>
		<author>
			<persName><forename type="first">Alberto</forename><surname>García-Durán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>ISBN 9781510860964</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Artificial Intelligence and Statistics</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaohan</forename><surname>Daniel Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Valko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge discovery and Data Mining</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adamw and super-convergence is now the fastest way to train neural nets</title>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<ptr target="https://www.fast.ai/2018/07/02/adam-weight-decay/" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/hash/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Contrastive multi-view representation learning on graphs</title>
		<author>
			<persName><forename type="first">Kaveh</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hosein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khasahmadi</forename></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v119/hassani20a.html" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.123</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2015.123" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision, USA, 2015. ISBN 9781467383912</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Bklr3j0cKX" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/fb60d411a5c5b72b2e7d3527cfc84fd0-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Strategies for pre-training graph neural networks</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HJlWWJSFDH" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Ogb-lsc: A large-scale challenge for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maho</forename><surname>Nakata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v37/ioffe15.html" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Self-supervised graph neural networks without explicit negative sampling</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zekarias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarunas</forename><surname>Kefato</surname></persName>
		</author>
		<author>
			<persName><surname>Girdzijauskas</surname></persName>
		</author>
		<idno>CoRR, abs/2103.14958</idno>
		<ptr target="https://arxiv.org/abs/2103.14958" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">How to find your friendly neighborhood: Graph attention design with self-supervision</title>
		<author>
			<persName><forename type="first">Dongkwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alice</forename><surname>Oh</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Wi5KUNlqWty" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SJU4ayYgl" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Image-based recommendations on styles and substitutes</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Targett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinfeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Hengel</surname></persName>
		</author>
		<idno type="DOI">10.1145/2766462.2767755</idno>
		<ptr target="https://doi.org/10.1145/2766462.2767755" />
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;15, 2015. ISBN 9781450336215</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Wiki-cs: A wikipedia-based benchmark for graph neural networks</title>
		<author>
			<persName><forename type="first">Péter</forename><surname>Mernyei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cȃtȃlina</forename><surname>Cangea</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName><forename type="first">Tomás</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1301.3781" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Graph Representation Learning via Graphical Mutual Information Maximization</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minnan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghua</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3366423.3380112</idno>
		<ptr target="https://doi.org/10.1145/3366423.3380112" />
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="259" to="270" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
		<ptr target="https://www.aclweb.org/anthology/D14-1162" />
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
		<author>
			<persName><surname>Deepwalk</surname></persName>
		</author>
		<idno type="DOI">10.1145/2623330.2623732</idno>
		<ptr target="http://dx.doi.org/10.1145/2623330.2623732" />
		<title level="m">ACM SIGKDD International Conference on Knowledge discovery and Data Mining</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Weight standardization</title>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno>CoRR, abs/1903.10520</idno>
		<ptr target="http://arxiv.org/abs/1903.10520" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dropedge: Towards deep graph convolutional networks on node classification</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pitfalls of graph neural network evaluation. Relational Representation Learning Workshop</title>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">An overview of microsoft academic service (mas) and applications</title>
		<author>
			<persName><forename type="first">Arnab</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darrin</forename><surname>Eide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo-June (paul)</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1145/2740908.2742839</idno>
		<ptr target="https://doi.org/10.1145/2740908.2742839" />
	</analytic>
	<monogr>
		<title level="m">International Conference on World Wide Web, 2015. ISBN 9781450334730</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Infograph: Unsupervised and semisupervised graph-level representation learning via mutual information maximization</title>
		<author>
			<persName><forename type="first">Fan-Yun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1lfF2NYvH" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Contrastive multiview coding</title>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58621-8_45</idno>
		<idno>https://doi.org/10.1007/978-3-030-58621-8_</idno>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. Graph attention networks</title>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58621-8_45</idno>
		<idno>CoRR, abs/2102.06810</idno>
		<ptr target="https://openreview.net/forum?id=rJXMpikCZ" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2021. 2018</date>
		</imprint>
	</monogr>
	<note>Understanding self-supervised learning dynamics without contrastive pairs</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep graph infomax</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rklz9iAcKQ" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Improving graph attention networks with large margin-based constraints</title>
		<author>
			<persName><forename type="first">Guangtao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno>ArXiv, abs/1910.11945</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/3fe230348e9a12c13120749e3f9fa4cd-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Graph contrastive learning with adaptive augmentation</title>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<idno>ArXiv, abs/2010.14945</idno>
		<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Deep graph contrastive representation learning</title>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<idno>ArXiv, abs/2006.04131</idno>
		<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Predicting multicellular function through multi-layer tissue networks</title>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btx252</idno>
		<ptr target="http://dx.doi.org/10.1093/bioinformatics/btx252" />
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<idno type="ISSN">1460-2059</idno>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="190" to="198" />
			<date type="published" when="2017-07">Jul 2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
