<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data-Free Quantization Through Weight Equalization and Bias Correction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-11-25">25 Nov 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Markus</forename><surname>Nagel</surname></persName>
							<email>markusn@qti.qualcomm.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Equal Contribution † Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mart</forename><surname>Van Baalen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Equal Contribution † Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tijmen</forename><surname>Blankevoort</surname></persName>
							<email>tijmen@qti.qualcomm.com</email>
						</author>
						<author>
							<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
							<email>mwelling@qti.qualcomm.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Qualcomm AI Research † Qualcomm Technologies</orgName>
								<address>
									<country>Netherlands B.V</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Data-Free Quantization Through Weight Equalization and Bias Correction</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-11-25">25 Nov 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1906.04721v3[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a data-free quantization method for deep neural networks that does not require fine-tuning or hyperparameter selection. It achieves near-original model performance on common computer vision architectures and tasks. 8-bit fixed-point quantization is essential for efficient inference on modern deep learning hardware. However, quantizing models to run in 8-bit is a non-trivial task, frequently leading to either significant performance reduction or engineering time spent on training a network to be amenable to quantization. Our approach relies on equalizing the weight ranges in the network by making use of a scale-equivariance property of activation functions. In addition the method corrects biases in the error that are introduced during quantization. This improves quantization accuracy performance, and can be applied to many common computer vision architectures with a straight forward API call. For common architectures, such as the MobileNet family, we achieve state-of-the-art quantized model performance. We further show that the method also extends to other computer vision architectures and tasks such as semantic segmentation and object detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, deep learning based computer vision models have moved from research labs into the cloud and onto edge devices. As a result, power consumption and latency of deep learning inference have become an important concern. For this reason fixed-point quantization is often employed to make inference more efficient. By quantizing floating point values onto a regularly spaced grid, the original floating point values can be approximated by a set of integers, a scaling factor, and an optional zero point The original model has significant drop in performance at 12-bit quantization whereas our model maintains close to FP32 performance even at 6-bit quantization.</p><p>offset <ref type="bibr" target="#b15">[16]</ref>. This allows for the use of faster and more power-efficient integer operations in matrix multiplication and convolution computations, at the expense of lower representational power. We refer the reader to <ref type="bibr" target="#b17">[18]</ref> for details on commonly used, hardware-friendly quantization methods for deep learning models.</p><p>Quantization of 32-bit full precision (FP32) models into 8-bit fixed point (INT8) introduces quantization noise on the weights and activations, which often leads to reduced model performance. This performance degradation ranges from very minor to catastrophic. To minimize the quantization noise, a wide range of different methods have been introduced in the literature (see <ref type="bibr">Section 2)</ref>. A major drawback of these quantization methods is their reliance on data and fine-tuning. As an example, consider real-world actors that manage hardware for quantized models, such as cloudbased deep learning inference providers or cellphone manufacturers. To provide a general use quantization service they would have to receive data from the customers to fine-tune the models, or rely on their customers to do the quantization. In either case, this can add a difficult step to the process. For such stakeholders it would be preferable if FP32 models could be converted directly to INT8, without needing the know-how, data or compute necessary for running traditional quantization methods. Even for model develop-ers that have the capability to quantize their own models, automation would save significant time.</p><p>In this paper, we introduce a quantization approach that does not require data, fine-tuning or hyperparameter tuning, resulting in accuracy improvement with a simple API call. Despite these restrictions we achieve near-original model performance when quantizing FP32 models to INT8. This is achieved by adapting the weight tensors of pre-trained models such that they are more amenable to quantization, and by correcting for the bias of the error that is introduced when quantizing models. We show significant improvements in quantization performance on a wide range of computer vision models previously thought to be difficult to quantize without fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Levels of quantization solutions</head><p>In literature the practical application of proposed quantization methods is rarely discussed. To distinguish between the differences in applicability of quantization methods, we introduce four levels of quantization solutions, in decreasing order of practical applicability. Our hope is that this will enable other authors to explore solutions for each level, and makes the comparison between methods more fair. The axes for comparison are whether or not a method requires data, whether or not a method requires error backpropagation on the quantized model, and whether or not a method is generally applicable for any architecture or requires significant model reworking. We use the following definitions throughout the paper:</p><p>Level 1 No data and no backpropagation required. Method works for any model. As simple as an API call that only looks at the model definition and weights.</p><p>Level 2 Requires data but no backpropagation. Works for any model. The data is used e.g. to re-calibrate batch normalization statistics <ref type="bibr" target="#b26">[27]</ref> or to compute layer-wise loss functions to improve quantization performance. However, no fine-tuning pipeline is required.</p><p>Level 3 Requires data and backpropagation. Works for any model. Models can be quantized but need fine-tuning to reach acceptable performance. Often requires hyperparameter tuning for optimal performance. These methods require a full training pipeline (e.g. <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b34">35]</ref>).</p><p>Level 4 Requires data and backpropagation. Only works for specific models. In this case, the network architecture needs non-trivial reworking, and/or the architecture needs to be trained from scratch with quantization in mind (e.g. <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b20">21]</ref>). Takes significant extra training-time and hyperparameter tuning to work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background and related work</head><p>There are several works that describe quantization and improving networks for lower bit inference and deployment <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b34">35]</ref>. These methods all rely on fine-tuning, making them level 3 methods, whereas data-free quantization improves performance similarly without that requirement. Our method is complementary to these and can be applied as a pre-processing before quantization aware fine-tuning.</p><p>In a whitepaper, Krishnamoorthi <ref type="bibr" target="#b17">[18]</ref>, introduces a level 1 'per-channel' quantization scheme, in which the weights of a convolutional weight tensor are quantized per output channel. A major drawback of this method is that it is not supported on all hardware, and that it creates unnecessary overhead in the computation due to the necessity of scale and offset values for each channel individually. We show that our method improves on per-channel quantization, while keeping a single set of scale and offset values for the whole weight tensor instead.</p><p>Other methods to improve quantization need architecture changes or training with quantization in mind from the start <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36]</ref>. These methods are even more involved than doing quantization and fine-tuning. They also incur a relatively large overhead during training because of sampling and noisy optimization, and introduce extra hyperparameters to optimize. This makes them level 4 methods.</p><p>Methods that binarize <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref> or ternarize <ref type="bibr" target="#b18">[19]</ref> networks result in models with great inference efficiency as expensive multiplications and additions are replaced by bit-shift operations. However, quantizing models to binary often leads to strong performance degradation. Generally they need to be trained from scratch, making them level 4 methods.</p><p>Other approaches use low-bit floating point operations instead of integer operations, or other custom quantization implementations <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b34">35]</ref>. We do not consider such approaches as the hardware implementation is less efficient.</p><p>In concurrent work, Meller et al. <ref type="bibr" target="#b21">[22]</ref> also exploits the scale equivariance of the ReLU function to rescale weight channels and notice the biased error introduced by weight quantization <ref type="bibr" target="#b6">[7]</ref>, leading to a method that resembles our data-free quantization approach. Stock et al. <ref type="bibr" target="#b31">[32]</ref> also use the scale equivariance property of the ReLU function, but use it for network optimization instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Motivation</head><p>While many trained FP32 models can be quantized to INT8 without much loss in performance, some models exhibit a significant drop in performance after quantization ( <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b30">31]</ref>). For example, when quantizing a trained Mo-bileNetV2 <ref type="bibr" target="#b29">[30]</ref> model, Krishnamoorthi <ref type="bibr" target="#b17">[18]</ref> reports a drop in top-1 accuracy from 70.9% to 0.1% on the ImageNet <ref type="bibr" target="#b28">[29]</ref> validation set. The author restores near original model performance by either applying per-channel quantization, finetuning or both.  In the boxplot the min and max value, the 2nd and 3rd quartile and the median are plotted for each channel. This layer exhibits strong differences between channel weight ranges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Weight tensor channel ranges</head><p>The fact that per-channel quantization yields much better performance on MobileNetV2 than per-tensor quantization suggests that, in some layers, the weight distributions differ so strongly between output channels that the same set of quantization parameters cannot be used to quantize the full weight tensor effectively. For example, in the case where one channel has weights in the range [−128, 128] and another channel has weights in the range (−0.5, 0.5), the weights in the latter channel will all be quantized to 0 when quantizing to 8-bits.</p><p>Figure <ref type="figure" target="#fig_2">2</ref> shows that large differences in output channel weight ranges do indeed occur in a (trained) MobileNetV2 model. This figure shows the weight distribution of the output channel weights of the depthwise-separable layer in the model's first inverted residual block. Due to the strong differences between channel weight ranges that this layer exhibits, it cannot be quantized with reasonable accuracy for each channel. Several layers in the network suffer from this problem, making the overall model difficult to quantize.</p><p>We conjecture that performance of trained models after quantization can be improved by adjusting the weights for each output channel such that their ranges are more similar. We provide a level 1 method to achieve this without changing the FP32 model output in section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Biased quantization error</head><p>A common assumption in literature (e.g. <ref type="bibr" target="#b1">[2]</ref>) is that quantization error is unbiased and thus cancels out in a layer's output, ensuring that the mean of a layer's output does not change as a result of quantization. However, as we will show in this section, the quantization error on the weights might introduce biased error on the corresponding outputs. This shifts the input distribution of the next layer, which may cause unpredictable effects.</p><p>The biased error in a quantized layer's output unit j can be computed empirically using N input data points as:</p><formula xml:id="formula_0">E[ y j − y j ] ≈ 1 N n ( Wx n ) j − (Wx n ) j<label>(1)</label></formula><p>where y j and y j are the original outputs and the outputs generated using the quantized weight matrix, respectively. Figure <ref type="figure" target="#fig_3">3</ref> shows the biased error per channel of a depthwise-separable convolution layer in a trained Mo-bileNetV2 model. From this plot it is clear that for many channels in the layer's output, the error introduced by weight quantization is biased, and influences the output statistics. Depthwise-separable layers are especially susceptible to this biased error effect as each output channel has only 9 corresponding weights.</p><p>Such a biased error on the outputs can be introduced in many settings, e.g. when weights or activations are clipped <ref type="bibr" target="#b22">[23]</ref>, or in non-quantization approaches, such as weight tensor factorization or channel pruning <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>In section 4.2 we introduce a method to correct for this bias. Furthermore, we show that a model's batch normalization parameters can be used to compute the expected biased error on the output, yielding a level 1 method to fix the biased error introduced by quantization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head><p>Our proposed data-free quantization method (DFQ) consists of three steps, on top of the normal quantization. The overall flow of the algorithm is shown in Figure <ref type="figure" target="#fig_4">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Cross-layer range equalization</head><p>Positive scaling equivariance We observe that for a ReLU <ref type="bibr" target="#b24">[25]</ref> activation function f (•) the following scaling equivariance property holds:</p><formula xml:id="formula_1">f (sx) = sf (x)<label>(2)</label></formula><p>for any non-negative real number s. This follows from the definition of the ReLU: This equivariance also holds for the PreLU <ref type="bibr" target="#b10">[11]</ref> activation function. More generally, the positive scaling equivariance can be relaxed to f (sx) = s f (x) for any piece-wise linear activation functions:</p><formula xml:id="formula_2">ReLU(x) = x if x &gt; 0 0 if x ≤ 0.<label>(3)</label></formula><formula xml:id="formula_3">f (x) =            a 1 x + b 1 if x ≤ c 1 a 2 x + b 2 if c 1 &lt; x ≤ c 2 . . . a n x + b n if c n−1 &lt; x<label>(4)</label></formula><p>where f (•) is parameterized as âi = a i , bi = b i /s and ĉi = c i /s. Note that contrary to equivariance defined in eq. 2 we now also change the function f (•) into f (•).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Scaling equivariance in neural networks</head><p>The positive scaling equivariance can be exploited in consecutive layers in neural networks. Given two layers, h = f (W (1) x + b (1) ) and y = f (W (2) h + b (2) ), through scaling equivariance we have that: (2) f (W (1) x + b (1) ) + b (2) )</p><formula xml:id="formula_4">y = f (W</formula><p>(5)</p><formula xml:id="formula_5">= f (W (2) S f (S −1 W (1) x + S −1 b (1) ) + b (2) ) (6) = f ( W (2) f ( W (1) x + b (1) ) + b (2) )<label>(7)</label></formula><p>where S = diag(s) is a diagonal matrix with value S ii denoting the scaling factor s i for neuron i. This allows us to reparameterize our model with 1) . In case of CNNs the scaling will be per channel and broadcast accordingly over the spatial dimensions. The rescaling procedure is illustrated in Figure <ref type="figure" target="#fig_6">5</ref>.</p><formula xml:id="formula_6">W (2) = W (2) S, W (1) = S −1 W (1) and b (1) = S −1 b (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Equalizing ranges over multiple layers</head><p>We can exploit the rescaling and reparameterization of the model to make the model more robust to quantization. Ideally the ranges of each channel i are equal to the total range of the weight tensor, meaning we use the best possible representative power per channel. We define the precision of a channel as:</p><formula xml:id="formula_7">p(1) i = r(1) i R(1)<label>(8)</label></formula><p>Project Morpheus</p><p>Cross layer rescaling -High Level Design 7 QUALCOMM Proprietary </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Finding rescaling parameters</head><p>In order to improve the post training quantization performance, we want to select ^ in a way that it makes the dynamic ranges of each channel as similar as possible. We found that the following heuristic is a good proxy in finding the optimal rescaling parameters and does not require any expensive optimization procedure.</p><formula xml:id="formula_8">hDiEg X = max ( D[! ( L X) , 0 ) hDiEg \ = max ( D[! ( L \) , 1 ) ^= hDiEg X !nhf ( hDiEg X ⋅ hDiEg \)</formula><p>Where max ( ⋅, 0 ) is the maximum of the matrix along the output axis and max ( ⋅, 1 ) along the input axis. This heuristic ensures that the maximum/minimum per channel; which also defines the quantization range; are equally spread across the layers that share the same channel. This in turn results that we have for all channels the least possible quantization error since the quantization error between layers have a multiplicative relationship.</p><p>If more than two layers share the same channels; e.g. in case of channel wise operations such as separable convolutions, the extension is straight forward:</p><formula xml:id="formula_9">^X\ = hDiEg X o hDiEg X ⋅ hDiEg \ ⋅ hDiEg p q ^\p = o hDiEg X ⋅ hDiEg \ ⋅ hDiEg p q hDiEg \</formula><p>Where hDiEg S is the range of layer n along the shared axis.</p><p>In case of a sequence of layers that do not share common channels (e.g. in a normal feed forward network like VGG) we apply the above optimization in a sequential way, i.e. first finding the scaling parameter between L X and L \ , then between L \ and L p and so on.</p><p>The method is applied to any sequence of layers that is simply connected. Meaning from the outputs from layer1 to the inputs of layer layern that are connected there is no splitting of the activations to or from other layers. Common examples of such splits are when an output is used by two other layers in where r(1)</p><p>i is the quantization range of channel i in W (1)  and R( <ref type="formula" target="#formula_0">1</ref>) is the total range of W (1) . We want to find S such that the total precision per channel is maximized:</p><formula xml:id="formula_10">max S i p(1) i p(2) i<label>(9)</label></formula><p>In the case of symmetric quantization we have r(1)</p><formula xml:id="formula_11">i = 2 • max j | W (1) ij | and R(1) = 2 • max ij | W (1)</formula><p>ij |. Solving eq. 9 (see appendix A) leads to the necessary condition:</p><formula xml:id="formula_12">arg max j 1 s j r<label>(1)</label></formula><formula xml:id="formula_13">j = arg max k s k r<label>(2) k (10)</label></formula><p>meaning the limiting channel defining the quantization range is given by arg max i r</p><formula xml:id="formula_14">(1) i r<label>(2)</label></formula><p>i . We can satisfy this condition by setting S such that:</p><formula xml:id="formula_15">s i = 1 r (2) i r (1) i r (2) i<label>(11)</label></formula><p>which results in ∀i : r</p><formula xml:id="formula_16">(1) i = r<label>(2)</label></formula><p>i . Thus the channel's ranges between both tensors are matched as closely as possible.</p><p>When equalizing multiple layers at the same time, we iterate this process for pairs of layers that are connected to each other without input or output splits in between, until convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3.">Absorbing high biases</head><p>In case s i &lt; 1 the equalization procedure increases bias b</p><p>(1) i . This could in turn increase the range of the activation quantization. In order to avoid big differences between perchannel ranges in the activations we introduce a procedure that absorbs high biases into the subsequent layer.</p><p>For a layer with ReLU function r, there is a non-negative vector c such that r(Wx + b − c) = r(Wx + b) − c. The trivial solution c = 0 holds for all x. However, depending on the distribution of x and the values of W and b, there can be some values c i &gt; 0 for which this equality holds for (almost) all x. Following the previous two layer example, these c i can be absorbed from layer 1 into layer 2 as:</p><formula xml:id="formula_17">y = W (2) h + b (2) (12)</formula><p>= W (2) (r(W (1) x + b (1) </p><formula xml:id="formula_18">) + c − c) + b (2)<label>(13)</label></formula><p>= W (2) (r(W (1) x + b( <ref type="formula" target="#formula_0">1</ref>)</p><formula xml:id="formula_19">) + c) + b (2) (14) = W (2) ĥ + b(2) (<label>15</label></formula><formula xml:id="formula_20">)</formula><p>where b(2) = W (2) c+b (2) , ĥ = h−c, and b(1) = b (1) −c.</p><p>To find c without violating our data-free assumption we assume that the pre-bias activations are distributed normally with the batch normalization shift and scale parameters β and γ as its mean and standard deviation. We set c = max(0, β − 3γ). If c &gt; 0, the equality introduced above will hold for the 99.865% of values of x (those greater than c) under the Gaussian assumption. As we will show in section 5.1.1, this approximation does not harm the full precision performance significantly but helps for activation quantization. Note that, in case data is available, the pre-bias distribution of x can be found empirically and used to set c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Quantization bias correction</head><p>As shown empirically in the motivation, quantization can introduce a biased error in the activations. In this section we show how to correct for the bias in the error on the layer's output, and how we can use the network's batch normalization parameters to compute this bias without using data.</p><p>For a fully connected layer with weight tensor W, quantized weights W, and input activations x, we have y = Wx and therefore y = y + x, where we define the quantization error = W − W, y as the layer pre-activations of the FP32 model, and y that layer with quantization error added.</p><p>If the expectation of the error for output i, E[ x] i = 0, then the mean of the output i will change. This shift in distribution may lead to detrimental behavior in the following layers. We can correct for this change by seeing that:</p><formula xml:id="formula_21">E[y] = E[y] + E[ x] − E[ x] (16) = E[ y] − E[ x].<label>(17)</label></formula><p>Thus, subtracting the expected error on the output</p><formula xml:id="formula_22">E [ x] = E [x]</formula><p>from the biased output y ensures that the mean for each output unit is preserved.</p><p>For implementation, the expected error can be subtracted from the layer's bias parameter, since the expected error vector has the same shape as the layer's output. This method easily extends to convolutional layers as described in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Computing the expected input</head><p>To compute the expected error of the output of a layer, the expected input to the layer E[x] is required. If a model does not use batch normalization, or there are no data-usage restrictions, E[ x] can be computed by comparing the activations before and after quantization. Appendix D explains this procedure in more detail.</p><p>Clipped normal distribution When the network includes batch normalization before a layer, we can use it to calculate E[x] for that layer without using data. We assume the pre-activation outputs of a layer are normally distributed, that batch normalization is applied before the activation function, and that the activation function is some form of the class of clipped linear activation functions (e.g. ReLU, ReLU6), which clips its input range to the range [a, b] where a &lt; b, and b can be ∞.</p><p>Due to the centralization and normalization applied by batch normalization, the mean and standard deviation of the pre-activations are known: these are the batch normalization scale and shift parameters (henceforth referred to as γ and β respectively).</p><p>To compute E[x] from the previous layer's batch normalization parameters, the mean and variance need to be adjusted to account for the activation function that follows the batch normalization layer. For this purpose we introduce the clipped normal distribution. A clipped-normally distributed random variable X is a normally distributed random variable with mean µ and variance σ 2 , whose values are clipped to the range [a, b] The mean and variance of the clipped normal distribution can be computed in closed form from µ, σ, a and b. We present the mean of the clipped normal distribution for the ReLU activation function, i.e. a = 0 and b = ∞ in this section, and refer the reader to Appendix C for the closed form solution for the general clipped normal distribution.</p><p>The expected value for channel c in x, E[x c ], which is the output of a layer with batch normalization parameters β c and γ c , followed by a ReLU activation function is:</p><formula xml:id="formula_23">E[x c ] = E [ReLU (x pre c )]<label>(18)</label></formula><formula xml:id="formula_24">= γ c N −β c γ c + β c 1 − Φ −β c γ c (<label>19</label></formula><formula xml:id="formula_25">)</formula><p>where x pre c is the pre-activation output for channel c, which is assumed to be normally distributed with mean β c and variance γ 2 c , Φ(•) is the normal CDF, and the notation N (x) is used to denote the normal N (x|0, 1) PDF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section we present two sets of experiments to validate the performance of data-free quantization (DFQ). We first show in section 5.1 the effect of the different aspects of DFQ and how they solve the problems observed earlier. Then we show in section 5.2 how DFQ generalizes to other models and tasks, and sets a new state-of-the-art for level 1 quantization.</p><p>To allow comparison to previously published results, we use both weights and activations are quantized using 8bit asymmetric, per-tensor quantization in all experiments. Batch normalization is folded in the adjacent layer before quantization. Weight quantization ranges are the min and max of the weight tensor. Activation quantization ranges are set without data, by using the learned batch normalization shift and scale parameter vectors β and γ as follows:</p><p>We compute the activation range for channel i as β i ± n • γ i (with n = 6), with the minimum clipped to 0 in case of ReLU activation. We observed a wide range of n can be used without significant performance difference. All experiments are done in Pytorch <ref type="bibr" target="#b25">[26]</ref>. In appendix E we show additional experiments using short-term fine-tuning, symmetric quantization and per-channel quantization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Ablation study</head><p>In this section we investigate the effect of our methods on a pre-trained MobileNetV2 [30] model 1 . We validate the performance of the model on the ImageNet <ref type="bibr" target="#b28">[29]</ref> validation set. We first investigate the effects of different parts of our approach through a set of ablation studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1.">Cross-layer equalization</head><p>In this section we investigate the effects of cross-layer equalization and high-bias folding. We compare these methods to two baselines: the original quantized model and the less hardware friendly per-channel quantization scheme.</p><p>The models considered in this section employ residual connections <ref type="bibr" target="#b11">[12]</ref>. For these networks we apply cross-layer equalization only to the layers within each residual block. MobileNetV2 uses ReLU6 activation functions, which clips activation ranges to [0, 6]. To avoid ReLU6 requiring a different cut off per channel after applying the equalization procedure, we replace ReLU6 with regular ReLU.</p><p>The results of the equalization experiments are shown in Table <ref type="table">1</ref>. Similar to <ref type="bibr" target="#b17">[18]</ref>, we observe that the model performance is close to random when quantizing the original model to INT8. Further we note that replacing ReLU6 by ReLU does not significantly degrade the model performance. Applying equalization brings us to within 2% of FP32 performance, close to the performance of per-channel quantization. We note that absorbing high biases results in a small drop in FP32 performance, but it boosts quantized performance by 1% due to more precise activation quantization. Combining both methods improves performance over per-channel quantization, indicating the more efficient per-tensor quantization could be used instead. 1 We use the Pytorch implementation of MobileNetV2 provided by https://github.com/tonylins/pytorch-mobilenet-v2.  To illustrate the effect of cross-layer equalization, we show the weight distributions per output channel of the depthwise-separable layer in the models first inverted residual block after applying the equalization in Figure <ref type="figure" target="#fig_7">6</ref>. We observe that most channels ranges are now similar and that the strong outliers from Figure <ref type="figure" target="#fig_2">2</ref> have been equalized. Note, there are still several channels which have all weight values close to zero. These channels convey little information and can be pruned from the network with hardly any loss in accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2.">Bias correction</head><p>In this section we present results on bias correction for a quantized MobileNetV2 model. We furthermore present results of bias correction in combination with a naive weightclipping baseline, and combined with the cross-layer equalization approach.</p><p>The weight-clipping baseline serves two functions: 1) as a naive baseline to the cross-layer equalization approach, and 2) to show that bias correction can be employed in any setting where biased noise is introduced. Weight clipping solves the problem of large differences in ranges between channels by clipping large ranges to smaller ranges, but it introduces a strongly biased error. <ref type="bibr">Weight</ref>   To illustrate the effect of bias correction, Figure <ref type="figure" target="#fig_3">3</ref> shows the per output channel biased error introduced by weight quantization. The per-channel biases are obtained as described in eq. 1. This figure shows that applying bias correction reduces the bias in the error on the output of a layer to very close to 0 for most output channels.</p><p>Results for the experiments described above for Mo-bileNet V2 on the ImageNet validation set are shown in Table 2. Applying bias correction improves quantized model performance, indicating that a part of the problem of quantizing this model lies in the biased error that is introduced. However, bias correction on its own does not achieve nearfloating point performance. The reason for this is most likely that the problem described in 3.1 is more severe for this model. The experiments on weight-clipping show that bias correction can mitigate performance degradation due to biased error in non-quantized models as well as quantized models. Clipping without correction in the FP32 model introduces a 4.66% loss in accuracy; bias correction reduces that loss to a mere 0.57%. Furthermore, it shows that weight clipping combined with bias correction is a fairly strong baseline for quantizing MobileNet V2. Lastly, we show that bias correction improves results when combined with the cross-layer equalization and bias folding procedures. The combination of all methods is our data-free quantization (DFQ) method. The full DFQ approach achieves nearfloating point performance with a reduction of 0.53% top 1 accuracy relative to the FP32 baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison to other methods and models</head><p>In this section we show how DFQ generalizes to other popular computer vision tasks, namely semantic segmentation and object detection, and other model architectures such as MobileNetV1 <ref type="bibr" target="#b13">[14]</ref> and Resnet18 <ref type="bibr" target="#b11">[12]</ref> more complex level 3 and 4 approaches. This set of models was chosen as they are efficient and likely to be used in mobile applications where 8-bit quantization is frequently used for power efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Other tasks</head><p>Semantic segmentation To demonstrate the generalization of our method to semantic segmentation we apply DFQ for DeeplabV3+ with a MobileNetV2 backend <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b29">30]</ref>, performance is evaluated on the Pascal VOC segmentation challenge <ref type="bibr" target="#b5">[6]</ref>. For our experiments we use the publicly available Pytorch implementation<ref type="foot" target="#foot_0">2</ref> . We show the results of this experiment in Table <ref type="table">3</ref>. As observed earlier for classification we notice a significant drop in performance when quantizing the original model which makes it almost unusable in practice. Applying DFQ recovers almost all performance degradation and achieves less than 1% drop in mIOU compared to the full precision model. DFQ also outperforms the less hardware friendly per-channel quantization. To the best of our knowledge we are the first to publish quantization results on DeeplabV3+ as well as for semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Object detection</head><p>To demonstrate the applicability of our method to object detection we apply DFQ for MobileNetV2 SSDLite <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b19">20]</ref>, evaluated on the Pascal VOC object detection challenge <ref type="bibr" target="#b5">[6]</ref>. In our experiments we use the publicly available Pytorch implementation of SSD <ref type="foot" target="#foot_1">3</ref> .  The results are listed in Table <ref type="table" target="#tab_3">4</ref>. Similar to semantic segmentation we observe a significant drop in performance when quantizing the SSDLite model. Applying DFQ recovers almost all performance drop and achieves less than 1% drop in mAP compared to the full precision model, again per-channel quantization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>∼D ∼BP ∼AC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MobileNetV2</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">Comparison to other approaches</head><p>In this section we compare DFQ to other approaches in literature. We compare our results to two other level 1 approaches, direct per-layer quantization as well as perchannel quantization <ref type="bibr" target="#b17">[18]</ref>. In addition we also compare to multiple higher level approaches, namely quantization aware training <ref type="bibr" target="#b15">[16]</ref> as well as stochastic rounding and dynamic ranges <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, which are both level 3 approaches. We also compare to two level 4 approaches based on relaxed quantization <ref type="bibr" target="#b20">[21]</ref>, which involve training a model from scratch and to quantization friendly separable convolutions <ref type="bibr" target="#b30">[31]</ref> that require a rework of the original MobileNet architecture. The results are summarized in Table <ref type="table" target="#tab_5">5</ref>.</p><p>For both MobileNetV1 and MobileNetV2 per-layer quantization results in an unusable model whereas DFQ stays close to full precision performance. DFQ also outperforms per-channel quantization as well as most level 3 and 4 approaches which require significant fine-tuning, training or even architecture changes.</p><p>On Resnet18 we maintain full precision performance for 8-bit fixed point quantization using DFQ. Some higher level approaches <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21]</ref> report slightly higher results than our baseline model, likely due to a better training procedure than used in the standard Pytorch Resnet18 model. Since 8-bit quantization is lossless we also compare 6-bit results. DFQ clearly outperforms traditional per-layer quantization but stays slightly below per-channel quantization and higher level approaches such as QT and RQ <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>Overall DFQ sets a new state-of-the-art for 8-bit fixed point quantization on several models and computer vision tasks. It is especially strong for mobile friendly architec-tures such as MobileNetV1 and MobileNetV2 which were previously hard to quantize. Even though DFQ is an easy to use level 1 approach, we generally show competitive performance when comparing to more complex level 2-4 approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we introduced DFQ, a data-free quantization method that significantly helps quantized model performance without the need for data, fine-tuning or hyper-parameter optimization. The method can be applied to many common computer vision architectures with a straight-forward API call. This is crucial for many practical applications where engineers want to deploy deep learning models trained in FP32 to INT8 hardware without much effort. Results are presented for common computer vision tasks like image classification, semantic segmentation and object detection. We show that our method compares favorably to per-channel quantization <ref type="bibr" target="#b17">[18]</ref>, meaning that instead the more efficient per-tensor quantization can be employed in practice. DFQ achieves near original model accuracy for almost every model we tested, and even competes with more complicated training based methods.</p><p>Further we introduced a set of quantization levels to facilitate the discussion on the applicability of quantization methods. There is a difference in how easy a method is to use for generating a quantized model, which is a significant part of the impact potential of a quantization method in real world applications. We hope that the quantization levels and methods introduced in this paper will contribute to both future research and practical deployment of quantized deep learning models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Optimal range equalization of two layers</head><p>Consider two fully-connected layers with weight matrices W (1) and W (2) , that we scale as in 4.1. We investigate the problem of optimizing the quantization ranges by rescaling the weight matrices by S = diag(s), where s &gt; 0, such that W (1) = S −1 W (1) and W (2) = W (2) S the weight matrices after rescaling. We investigate the case of symmetric quantization, which also gives good results in practice for asymmetric quantization. We denote</p><formula xml:id="formula_26">r(1) = 2 • max j |S −1 W (1) ij | = S −1 r (1)<label>(20)</label></formula><formula xml:id="formula_27">r(2) = 2 • max i |W (2) ij S| = r (2) S<label>(21)</label></formula><formula xml:id="formula_28">R(k) = max i (r (k) i )<label>(22)</label></formula><p>where r(k) are the per-channel weight ranges that are scaled by S, R(k) the range for the scaled weight matrix W (k) and r (k) are the original unscaled ranges. Using this in our optimization goal of eq. 9 leads to max</p><formula xml:id="formula_29">S i p<label>(1) i p(2)</label></formula><formula xml:id="formula_30">i = max S i r(1) i r(2) i R(1) R(2)<label>(23)</label></formula><p>= max</p><formula xml:id="formula_31">S i 1 si r<label>(1)</label></formula><formula xml:id="formula_32">i • s i r (2) i max j ( 1 sj r<label>(1)</label></formula><formula xml:id="formula_33">j ) • max k (s k r<label>(2) k ) (24)</label></formula><formula xml:id="formula_34">= i r (1) i r<label>(2)</label></formula><p>i max S 1 max j ( 1 sj r</p><p>(1)</p><formula xml:id="formula_35">j ) • max k (s k r<label>(2) k )</label></formula><p>.</p><p>We observe that the specific scaling s i of each channel cancels out as long as they do not increase R, the range of the full weight matrix. We can reformulate the above to</p><formula xml:id="formula_37">min S max j 1 s j r (1) j • max k (s k r (2) k )<label>(26)</label></formula><p>which is minimal iff</p><formula xml:id="formula_38">arg max j 1 s j r<label>(1)</label></formula><formula xml:id="formula_39">j = arg max k s k r (2) k<label>(27)</label></formula><p>By contradiction, if j = k there is a small positive such that s k = s k − which will decrease max k s k r</p><p>k by r</p><p>(2) k without affecting max j</p><formula xml:id="formula_41">1 sj r<label>(1)</label></formula><p>j . Therefore such a solution would not be optimal for eq. 26.</p><p>The condition from eq. 27 implies there is a limiting channel i = arg max i r</p><p>(1) i r</p><p>(2) i which defines the quantization range of both weight matrices W (1) and W (2) . However, our optimization goal is not affected by the choice of the other s j given the resulting r(1) j and r(2) j are smaller than or equal to r(1) i and r(2) i , respectively. To break the ties of solutions we decide to set ∀i : r</p><formula xml:id="formula_42">(1) i = r (2)</formula><p>i . Thus the channel's ranges between both tensors are matched as closely as possible and the introduced quantization error is spread equally among both weight tensors. This results in our final rescaling factor</p><formula xml:id="formula_43">s i = 1 r (2) i r (1) i r (2) i<label>(28)</label></formula><p>which satisfies our necessary condition from eq. 27 and ensures that ∀i : r</p><formula xml:id="formula_44">1) i = r<label>(</label></formula><p>i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Bias correction for convolutional layers</head><p>Similarly to fully connected layers we can compute from W and W, it becomes a constant and we have that</p><formula xml:id="formula_46">E [ * x] = * E[x]. Expanding this yields: [ * E[x]] coij = cimn E[x ci,i−m,j−n ] cocimn<label>(29)</label></formula><formula xml:id="formula_47">= ci E[x ci ] mn cocimn<label>(30)</label></formula><p>where we assume that the expected value of each input channel is the same for all spatial dimensions in the input channel. Since the value of [ * E[x]] coij does not depend on the spatial dimensions i and j, the expected error is the same for the full output channel and can be folded into the layer's bias parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Clipped normal distribution</head><p>Given a normally distributed random variable X with mean µ and variance σ 2 , and a clipped-linear function f (•) that clips its argument to the range [a, b], s.t. a &lt; b, the mean and variance of f (X) can be determined using the standard rules of computing the mean and variance of a function:</p><formula xml:id="formula_48">µ c ab = ∞ −∞ f (x)p(x)dx<label>(31)</label></formula><formula xml:id="formula_49">σ c ab 2 = ∞ −∞ (f (x) − µ c ab ) 2 p(x)dx<label>(32)</label></formula><p>where we define p</p><formula xml:id="formula_50">(x) = N (x | µ, σ), µ c ab = E[f (X)] and σ c ab 2 = V ar[f (X)].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Mean of Clipped Normal Distribution</head><p>Using the fact that f</p><formula xml:id="formula_51">(x) is constant if x ∈ [a, b] we have that: µ c ab = ∞ −∞ f (x)p(x)dx (33) = a a −∞ p(x)dx + b a xp(x)dx + b ∞ b p(x)dx<label>(34)</label></formula><p>The first and last term can be computed as aΦ(α) and b(1 − Φ(β)) respectively, where we define α = a−µ σ , β = b−µ σ , and Φ(x) = CDF (x | 0, 1), the normal CDF with zero mean and unit variance.</p><p>The integral over the linear part of f (•) can be computed as:</p><formula xml:id="formula_52">b a xp(x)dx = C b a xe − 1 2σ 2 (x−µ) 2 dx (35) = −Cσ 2 e − 1 2σ 2 (x−µ) 2 b a + µ(Φ(β) − Φ(α)) (36) = σ (φ(α) − φ(β)) + µ(Φ(β) − Φ(α))<label>(37)</label></formula><p>where we define φ(•) = N (• | 0, 1), i.e. the standard normal pdf and C = 1 σ √ 2π is the normalization constant for a normal distribution with variance σ 2 , thus</p><formula xml:id="formula_53">µ c ab =σ (φ(α) − φ(β)) + µ(Φ(β) − Φ(α)) + aΦ(α) + b(1 − Φ(β)).<label>(38)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Variance of Clipped Normal Distribution</head><p>We again exploit the fact that f</p><formula xml:id="formula_54">(x) is constant if x ∈ [a, b]: σ c ab 2 = ∞ −∞ (f (x) − µ c ab ) 2 p(x)dx (39) = a −∞ (a − µ c ab ) 2 p(x)dx+ + b a (x − µ c ab ) 2 p(x)dx+ + ∞ b (b − µ c ab ) 2 p(x)dx<label>(40)</label></formula><p>The first and last term can be solved as (a − µ c ab ) 2 Φ(α) and (b − µ c ab ) 2 (1 − Φ(β)) respectively. The second term can be decomposed as follows: </p><formula xml:id="formula_55">a (x − µ c ab ) 2 p(x)dx = b a (x 2 − 2xµ c ab + µ c ab 2 )p(x)dx (41) = b a x 2 p(x)dx + Z(µ c ab 2 − 2µ c ab µ t ab )<label>(42)</label></formula><p>where we use the result from the previous subsection and define Z = Φ(β) − Φ(α), and where Evaluating the first term yields:</p><formula xml:id="formula_56">µ t ab = 1 Z b a xN (x | µ, σ 2 ) = µ+σ(φ(α)−φ(β))/Z</formula><formula xml:id="formula_57">b a x 2 p(x)dx = Z(µ 2 + σ 2 ) + σ(aφ(α) − bφ(β)) + σµ(φ(α) − φ(β))<label>(43)</label></formula><p>This results in:</p><formula xml:id="formula_58">V ar[f (X)] = Z(µ 2 + σ 2 + µ c ab 2 − 2µ c ab µ) + σ(aφ(α) − bφ(β)) + σ(µ − 2µ c ab )(φ(α) − φ(β)) + (a − µ c ab ) 2 Φ(α) + (b − µ c ab ) 2 (1 − Φ(β))<label>(44)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Empirical quantization bias correction</head><p>If a network does not use batch normalization, or does not use batch normalization in all layers, a representative dataset can be used to compute the difference between preactivation means before and after quantization. We then subtract this difference from the quantized model's preactivations. This procedure can be run with unlabeled data. The procedure should be run after BatchNorm folding and cross-layer range equalization. Clipping should be applied in the quantized network, but not in the floating point network. Since the activation function and the quantization operation are fused, this procedure is run on a network with quantized weights only. However, after this procedure is applied activations can be quantized as well. We bias correct a layer only after all the layers feeding into it have been bias-corrected. The procedure is as follows:  • Subtract E[ ] from the layer's bias parameter.</p><p>In Table <ref type="table">6</ref> we compare this empirical bias correction procedure with the analytic bias correction introduced in section 4.2. We observe that both approaches leads to similar results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Additional experiments</head><p>Combination with fine-tuning The focus of our method is data-free quantization (level 1). However, our method can also be used as a pre-processing before quantization aware fine-tuning. To demonstrate this we used DFQ together with short-term quantization aware fine-tuning <ref type="bibr" target="#b17">[18]</ref>. After just 1 epoch of quantization aware fine-tuning MobileNet V2, accuracy increases from 71.19% to 71.42%, almost recovering the FP32 performance (71.72%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Symmetric vs asymmetric quantization</head><p>In our experimental section all our experiments were performed with asymmetric quantization, since this is commonly used in literature. Here we also compare to symmetric quantization. Symmetric quantization does not use an offset, which eliminates several cross terms in the calculations done on hardware compared to asymmetric quantization. This makes symmetric quantization more efficient on some hardware at the expense of losing some expressive power.</p><p>In Table <ref type="table" target="#tab_7">7</ref> we compare symmetric and asymmetric quantization in combination with DFQ. For all three models the advantage of asymmetric quantization is almost negligible. We noticed that cross-layer equalization is effective at removing outliers, resulting in weight distributions are often close to symmetric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DFQ combined with per-channel quantization</head><p>In our experiments we focused on per-tensor quantization since the more recent per-channel quantization is not efficiently supported on all hardware. For hardware that does support it, we analyze the effect of DFQ in combination with perchannel quantization.</p><p>In Table <ref type="table" target="#tab_8">8</ref>  equalization, bias absorption and bias correction, incremental improve over per-channel quantization and reduce the total quantization error from 1.07% to only 0.39%.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Fixed point inference for MobileNetV2 on ImageNet.The original model has significant drop in performance at 12-bit quantization whereas our model maintains close to FP32 performance even at 6-bit quantization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure2. Per (output) channel weight ranges of the first depthwiseseparable layer in MobileNetV2. In the boxplot the min and max value, the 2nd and 3rd quartile and the median are plotted for each channel. This layer exhibits strong differences between channel weight ranges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Per-channel biased output error introduced by weight quantization of the second depthwise-separable layer in Mo-bileNetV2, before (blue) and after (orange) bias correction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Flow diagram of the proposed DFQ algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustrating of the rescaling for a single channel. If scaling factor ! R scales # R in layer 1; we can instead factor it out and multiply $ R in layer 2.</figDesc><graphic url="image-1.png" coords="4,69.02,72.00,198.42,122.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Illustration of the rescaling for a single channel. If scaling factor si scales ci in layer 1; we can instead factor it out and multiply di in layer 2.</figDesc><graphic url="image-2.png" coords="4,312.71,70.80,234.92,94.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Per (output) channel weight ranges of the first depthwiseseparable layer in MobileNetV2 after equalization. In the boxplot the min and max value, the 2nd and 3rd quartile and the median are plotted for each channel. Most channels in this layer are now within similar ranges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>1 . 2 .•</head><label>12</label><figDesc>Run N examples through the FP32 model and collect for each layer the per-channel pre-activation means E[y]. For each layer L in the quantized model: Collect the per-channel pre-activation means E[ y] of layer L for the same N examples as in step 1. • Compute the per-channel biased quantization error E[ ] = E[ y] − E[y].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>clipping is applied by first folding the batch normalization parameters into a layer's weights, and then clipping all values to</figDesc><table><row><cell>Model</cell><cell>FP32</cell><cell>INT8</cell></row><row><cell>Original Model</cell><cell>71.72%</cell><cell>0.12%</cell></row><row><cell>Bias Corr</cell><cell cols="2">71.72% 52.02%</cell></row><row><cell>Clip @ 15</cell><cell>67.06%</cell><cell>2.55%</cell></row><row><cell>+ Bias Corr</cell><cell cols="2">71.15% 70.43%</cell></row><row><cell cols="3">Rescaling + Bias Absorption 71.57% 70.92%</cell></row><row><cell>+ Bias Corr</cell><cell cols="2">71.57% 71.19%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Top1 ImageNet validation results for MobileNetV2, evaluated at full precision and 8-bit integer quantized. Bold results show the best result for each column in each cell. a certain range, in this case [−15, 15]. We tried multiple symmetric ranges, all provided similar results. For residual connections we calculate E[x] and Var[x] based on the sum and variance of all input expectations, taking the input to be zero mean and unit variance.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>. Afterwards we compare DFQ to methods in the literature, including MobileNetV2 SSD-lite on Pascal VOC object detection challange. Mean average precision (mAP) evaluated at full precision and 8-bit integer quantized. Per-channel quantization is our own implementation of<ref type="bibr" target="#b15">[16]</ref> applied post-training.</figDesc><table><row><cell>Model</cell><cell>FP32 INT8</cell></row><row><cell>Original model</cell><cell>72.94 41.40</cell></row><row><cell>DFQ (ours)</cell><cell>72.45 72.33</cell></row><row><cell cols="2">Per-channel quantization 72.94 71.44</cell></row><row><cell cols="2">Table 3. DeeplabV3+ (MobileNetV2 backend) on Pascal VOC</cell></row><row><cell cols="2">segmentation challenge. Mean intersection over union (mIOU)</cell></row><row><cell cols="2">evaluated at full precision and 8-bit integer quantized. Per-channel</cell></row><row><cell cols="2">quantization is our own implementation of [16] applied post-</cell></row><row><cell>training.</cell><cell></cell></row><row><cell>Model</cell><cell>FP32 INT8</cell></row><row><cell>Original model</cell><cell>68.47 10.63</cell></row><row><cell>DFQ (ours)</cell><cell>68.56 67.91</cell></row><row><cell cols="2">Per-channel quantization 68.47 67.52</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Top1 ImageNet validation results for different models and quantization approaches. The top half compares level 1 approaches (∼D: data free, ∼BP: backpropagation-free, ∼AC: Architecture change free) whereas in the second half we also compare to higher level approaches in literature. Results with * indicates our own implementation since results are not provided, ˆresults provided by<ref type="bibr" target="#b17">[18]</ref> and † results from table 2 in<ref type="bibr" target="#b20">[21]</ref>.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>is the mean of the truncated normal distribution.</figDesc><table><row><cell>Model</cell><cell cols="2">CLE+BA Clip@15</cell></row><row><cell>No BiasCorr</cell><cell>70.92%</cell><cell>2.55%</cell></row><row><cell>Analytic BiasCorr</cell><cell>71.19%</cell><cell>70.43%</cell></row><row><cell>Empirical BiasCorr</cell><cell>71.15%</cell><cell>69.85%</cell></row><row><cell cols="3">Table 6. Top1 ImageNet validation results for MobileNetV2 with</cell></row><row><cell cols="3">weights and activations quantized to INT8. Comparing analytic</cell></row><row><cell cols="3">and empirical bias correction combined with cross-layer equaliza-</cell></row><row><cell cols="2">tion (CLE), bias absorption (BA) and clipping.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>Top1 ImageNet validation results for MobileNetV2 after applying DFQ. Weights and activations quantized using symmetric and asymmetric 8-bit integer quantization.</figDesc><table><row><cell>Model</cell><cell cols="2">Symmetric Asymmetric</cell></row><row><cell>MobileNet V1</cell><cell>70.32%</cell><cell>70.51%</cell></row><row><cell>MobileNet V2</cell><cell>71.15%</cell><cell>71.19%</cell></row><row><cell>Resnet18</cell><cell>69.50%</cell><cell>69.62%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 .</head><label>8</label><figDesc>we show the results of the different components of DFQ in combination with per-channel quantization. We notice that each individual component, cross-layer Top1 ImageNet validation results for MobileNetV2 using 8-bit per-channel quantization. Activations are quantized per tensor. Showing the effect of cross-layer equalization (CLE) and bias absorption (BA) in combination with bias correction.</figDesc><table><row><cell>Model</cell><cell cols="2">No BiasCorr BiasCorr</cell></row><row><cell>Original model</cell><cell>70.65%</cell><cell>70.80%</cell></row><row><cell>CLE</cell><cell>70.93%</cell><cell>71.30%</cell></row><row><cell>CLE+BA</cell><cell>71.03%</cell><cell>71.33%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">https://github.com/jfzhang95/ pytorch-deeplab-xception</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">https://github.com/qfgaohao/pytorch-ssd</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Christos Louizos, Harris Teague, Jakub Tomczak, Mihir Jain and Pim de Haan for their helpful discussions and valuable feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Variational network quantization</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Achterhold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">Mathias</forename><surname>Koehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anke</forename><surname>Schmeink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Genewein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the efficient representation and execution of deep acoustic models</title>
		<author>
			<persName><forename type="first">Raziel</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Annual Conference of the International Speech Communication Association (Interspeech)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
				<imprint>
			<date type="published" when="2018-09">September 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">PACT: parameterized clipping activation for quantized neural networks</title>
		<author>
			<persName><forename type="first">Jungwook</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swagath</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I-Jen</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijayalakshmi</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kailash</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><surname>Gopalakrishnan</surname></persName>
		</author>
		<idno>arxiv:805.06085</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Binaryconnect: Training deep neural networks with binary weights during propagations</title>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Pierre</forename><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
				<meeting>the 28th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3123" to="3131" />
		</imprint>
	</monogr>
	<note>NIPS&apos;15</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mark Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Ali Eslami</surname></persName>
		</author>
		<author>
			<persName><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Almog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Grobman</surname></persName>
		</author>
		<idno>arxiv:1906.03193</idno>
		<title level="m">Fighting quantization bias with bias</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Shiftcnn: Generalized low-precision architecture for inference of convolutional neural networks</title>
		<author>
			<persName><forename type="first">Denis</forename><forename type="middle">A</forename><surname>Gudovskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Rigazio</surname></persName>
		</author>
		<idno>arxiv:1706.02393</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep learning with limited numerical precision</title>
		<author>
			<persName><forename type="first">Suyog</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kailash</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pritish</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning, ICML 2015</title>
				<meeting>the 32nd International Conference on Machine Learning, ICML 2015<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07-11">6-11 July 2015. 2015</date>
			<biblScope unit="page" from="1737" to="1746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ristretto: A framework for empirical study of resource-efficient inference in convolutional neural networks</title>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Gysel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><forename type="middle">J</forename><surname>Pimentel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Motamedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soheil</forename><surname>Ghiasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learning Syst</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5784" to="5789" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago</title>
				<meeting><address><addrLine>Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">December 7-13, 2015. 2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
				<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">June 27-30, 2016. 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Channel pruning for accelerating very deep neural networks</title>
		<author>
			<persName><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision, ICCV 2017</title>
				<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">October 22-29, 2017. 2017</date>
			<biblScope unit="page" from="1398" to="1406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Quantized neural networks: Training neural networks with low precision weights and activations</title>
		<author>
			<persName><forename type="first">Itay</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Quantization and training of neural networks for efficient integer-arithmetic-only inference</title>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Skirmantas</forename><surname>Kligys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2018-06">June 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Flexpoint: An adaptive numerical format for efficient training of deep neural networks</title>
		<author>
			<persName><forename type="first">Urs</forename><surname>Köster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcel</forename><surname>Nassar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjun</forename><forename type="middle">K</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Constable</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oguz</forename><surname>Elibol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stewart</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Hornof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Khosrowshahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carey</forename><surname>Kloss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruby</forename><forename type="middle">J</forename><surname>Pai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09">2017, 4-9 December 2017. 2017</date>
			<biblScope unit="page" from="1740" to="1750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Quantizing deep convolutional networks for efficient inference: A whitepaper</title>
		<author>
			<persName><forename type="first">Raghuraman</forename><surname>Krishnamoorthi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08342</idno>
		<imprint>
			<date type="published" when="2018-06">Jun 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Fengfu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<idno>arxiv:1605.04711</idno>
		<title level="m">Ternary weight networks</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SSD: single shot multibox detector</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 -14th European Conference</title>
				<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">October 11-14, 2016. 2016</date>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efstratios Gavves, and Max Welling. Relaxed quantization for discretized neural networks</title>
		<author>
			<persName><forename type="first">Christos</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Reisser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tijmen</forename><surname>Blankevoort</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Same, same but different: Recovering neural network quantization error through weight factorization</title>
		<author>
			<persName><forename type="first">Eldad</forename><surname>Meller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Almog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Grobman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
				<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-15">9-15 June 2019. 2019</date>
			<biblScope unit="page" from="4486" to="4495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">WRPN: training and inference using wide reducedprecision networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Asit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">J</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eriko</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debbie</forename><surname>Nurvitadhi</surname></persName>
		</author>
		<author>
			<persName><surname>Marr</surname></persName>
		</author>
		<idno>arxiv 1704.03079</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Daisuke</forename><surname>Miyashita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Murmann</surname></persName>
		</author>
		<idno>arxiv:1603.01025</idno>
		<title level="m">Convolutional neural networks using logarithmic data representation</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning (ICML-10)</title>
				<meeting>the 27th International Conference on Machine Learning (ICML-10)<address><addrLine>Haifa, Israel</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">June 21-24, 2010. 2010</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Jorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno>arxiv:1809.03368</idno>
		<title level="m">Probabilistic binary neural networks</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Xnor-net: Imagenet classification using binary convolutional neural networks</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 -14th European Conference</title>
				<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">October 11-14, 2016. 2016</date>
			<biblScope unit="page" from="525" to="542" />
		</imprint>
	</monogr>
	<note>Proceedings, Part IV</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2018-06">June 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A quantization-friendly separable convolution for mobilenets</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaojie</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mickey</forename><surname>Aleksic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1st Workshop on Energy Efficient Machine Learning and Cognitive Computing for Embedded Applications (EMC2)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Equi-normalization of neural networks</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rmi</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herv</forename><surname>Jgou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Soft weight-sharing for neural network compression</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Ullrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Meeds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Accelerating very deep convolutional networks for classification and detection</title>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhua</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1943" to="1955" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Incremental network quantization: Towards lossless cnns with low-precision weights</title>
		<author>
			<persName><forename type="first">Aojun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anbang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiwen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
		<idno>arxiv:1702.03044, abs/1702.03044</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Shuchang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zekun</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuheng</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06160</idno>
		<title level="m">Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
