<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ItemSage: Learning Product Embeddings for Shopping Recommendations at Pinterest</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-05-24">24 May 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Paul</forename><surname>Baltescu</surname></persName>
							<email>pauldb@pinterest.com</email>
						</author>
						<author>
							<persName><forename type="first">Haoyu</forename><surname>Chen</surname></persName>
							<email>hchen@pinterest.com</email>
						</author>
						<author>
							<persName><forename type="first">Nikil</forename><surname>Pancha</surname></persName>
							<email>npancha@pinterest.com</email>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><surname>Zhai</surname></persName>
							<email>andrew@pinterest.com</email>
						</author>
						<author>
							<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Charles</forename><surname>Rosenberg</surname></persName>
							<email>crosenberg@pinterest.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<settlement>San Francisco</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<settlement>San Francisco</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<settlement>San Francisco</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<address>
									<settlement>San Francisco</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<address>
									<settlement>San Francisco</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<address>
									<settlement>San Francisco</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ItemSage: Learning Product Embeddings for Shopping Recommendations at Pinterest</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-05-24">24 May 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2205.11728v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Representation Learning</term>
					<term>Multi-Task Learning</term>
					<term>Multi-Modal Learning</term>
					<term>Recommender Systems</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learned embeddings for products are an important building block for web-scale e-commerce recommendation systems. At Pinterest, we build a single set of product embeddings called ItemSage to provide relevant recommendations in all shopping use cases including user, image and search based recommendations. This approach has led to significant improvements in engagement and conversion metrics, while reducing both infrastructure and maintenance cost. While most prior work focuses on building product embeddings from features coming from a single modality, we introduce a transformer-based architecture capable of aggregating information from both text and image modalities and show that it significantly outperforms single modality baselines. We also utilize multi-task learning to make ItemSage optimized for several engagement types, leading to a candidate generation system that is efficient for all of the engagement objectives of the end-to-end recommendation system. Extensive offline experiments are conducted to illustrate the effectiveness of our approach and results from online A/B experiments show substantial gains in key business metrics (up to +7% gross merchandise value/user and +11% click volume).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Information systems ? Recommender systems; Multimedia and multimodal retrieval; ? Computing methodologies ? Multi-task learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Pinterest's mission is to bring everyone the inspiration to create a life they love. Users browse Pinterest to get inspired for their next home decoration project or to stay up to date with the latest fashion and beauty trends. Common feedback we hear from our users is that once they discover a product that matches their taste, they want to be able to purchase it as seamlessly as possible. In order to build a delightful shopping experience, we need our recommendation systems to evolve beyond image-based recommendations by leveraging the multi-faceted information available for products in our shopping catalog. Unlike pins -the main type of content on Pinterest, products consist of several images displaying the product from different angles or in different contexts and have high quality textual metadata provided by merchants including title, description, colors, and patterns in which the product is available for sale (see Figure <ref type="figure" target="#fig_0">1</ref> for an example). Our shopping recommendation systems also need to optimize for new types of outcomes like purchases and add-to-cart actions in addition to typical engagement metrics like clicks or saves.</p><p>This paper introduces Pinterest's learned embedding representation for products named ItemSage. Embeddings are a powerful tool for building recommendation systems at scale for e-commerce <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b34">35]</ref> and social media <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref> applications. From our experience, one of the key reasons to focus on building embedding representations lies in their versatility: we have successfully experimented with using ItemSage embeddings <ref type="bibr" target="#b0">(1)</ref> for generating candidates to upstream ranking systems via approximate nearest neighbor search, <ref type="bibr" target="#b1">(2)</ref> as features in the ranking models responsible for determining the final ordering of the products shown to users and (3) as signals in classification models aimed at inferring missing information from the shopping catalog (e.g. the category or the gender affinity for a specific product).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Home Search Closeup</head><p>With ItemSage, we made a few conscious design decisions that contrast it from other approaches in several key aspects:</p><p>Multi-modal features. Earlier approaches typically focus on building embeddings from content in a single modality, e.g. text <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b37">38]</ref> or images <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b33">34]</ref>. Product information spans both modalities. Since Pinterest is a visually dominant platform, it is important to capture the nuanced information available in a product's images to make sure shopping recommendations feel natural with the rest of the product (e.g. users tend to prefer fashion products shown in lifestyle photos over images of the products on a white background). At the same time, product images may contain other products (e.g. a sofa might be shown in a living room with a coffee table and a rug) so textual matches are crucial for providing highly relevant recommendations. We introduce a transformer-based architecture capable of combining features from these different modalities which is different from earlier work <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b32">33]</ref> that extends to multi-modal features.</p><p>Multi-modal vertical recommendations. Pinterest has 3 main surfaces (Figure <ref type="figure" target="#fig_1">2</ref>) that provide personalized recommendations: (1) in the Home surface, users are provided with recommendations based on their past activity, (2) in the Closeup surface, we provide similar recommendations to a pin the user is currently viewing, while (3) in the Search surface, we provide recommendations in response to a query string that the user has typed. Note that in each surface, the query comes from a different modality: <ref type="bibr" target="#b0">(1)</ref> in Home, the query is essentially a sequence of pins, (2) for Closeup, the query is a single pin, (3) while in Search, the query is a text string.</p><p>In contrast with other works that typically target a single vertical application (e.g. product search <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b34">35]</ref>), ItemSage can provide relevant candidates via approximate neighbor search <ref type="bibr" target="#b18">[19]</ref> for all these surfaces and, therefore, in response to queries formulated in each of these modalities. We achieve this by training ItemSage embeddings to be compatible with the learned representations for pins <ref type="bibr" target="#b32">[33]</ref> and search queries. Recommendations based on user activities are a more general case of pin-based recommendations where the activity history is first partitioned into clusters and then a few representative pins are sampled from different clusters to generate pin to product recommendations <ref type="bibr" target="#b21">[22]</ref>.</p><p>Multi-task Learning. To strike the right balance between inspiration and conversion optimization, Pinterest shopping recommendation systems optimize for several objectives including purchases, add-to-cart actions, saves and clicks. Traditionally, recommendation systems tackle multi-objective optimization in the ranking phase by building models capable of computing a score for every engagement type under consideration conditioned on the query context and the ranked candidate <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b35">36]</ref>. In our work, we learn embeddings that additionally optimize the candidate generation phase for all engagement types under consideration, leading to an overall more efficient recommendation funnel. We show that in cases where the labels are sparse, multi-task learning leads to improved results over models specialized only on sparse objectives. For objectives where labeled data is plentiful, we show that we can optimize our embeddings for new objectives with little or no performance penalty on existing objectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>The main concepts that underpin our work building product embeddings at Pinterest are multi-modal learning and multi-task learning. ItemSage aggregates multi-modal information from images and text. The embeddings are learned in a multi-task regime that supports cross-modal candidate retrieval and joint optimization for several engagement types. In this section, we briefly review these two concepts and related work. Another goal of our work is to create embeddings that are compatible with the learned representations of other entities in the Pinterest ecosystem, namely pins and search queries. We briefly review our approach for generating these "target" embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Multi-Modal Representation Learning</head><p>Multi-modal representation learning aims to aggregate information from different modalities into a common subspace <ref type="bibr" target="#b7">[8]</ref>. It has been extensively studied in areas like video classification <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b28">29]</ref>, speech recognition <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20]</ref>, and visual question answering <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b26">27]</ref> where information are often available in audio, text, and visual formats. The fusion of different modalities often follows a projection and concatenation pattern. For example, <ref type="bibr" target="#b16">[17]</ref> first projects the image, audio, and text features into the same space using autoencoders and then concatenates the hidden features to produce the final embedding with a further projection. Inspired by the success of Transformer models like BERT <ref type="bibr" target="#b4">[5]</ref>, more works adopt Transformer models for modality fusion <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b36">37]</ref>. Among these, the single-stream Transformer model <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15]</ref>, which also uses the same projection and concatenation idea, is the most suitable for our use case given our modalities.</p><p>We should note that although multi-modal representation learning has been studied in various areas, few works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b32">33]</ref> have successfully applied it to large-scale recommendation systems. To our knowledge, this work is the first one that uses the Transformer architecture to aggregate image and text modalities to learn product representations for production-scale recommendation systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-Task Representation Learning</head><p>Multi-task learning is designed to improve the model performance on individual tasks by sharing model parameters across related tasks <ref type="bibr" target="#b22">[23]</ref>. Typical multi-task learning models are deployed in the ranking phase of the recommendation systems <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b35">36]</ref>, and a model will have several outputs, one for each task. Similar model architectures are used in representation learning <ref type="bibr" target="#b15">[16]</ref> where multiple task-specific embeddings are learned by a model. However, from the production point of view, it is most convenient and economic to use a single version of embeddings for all tasks. Therefore, we take the same approach as <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b33">34]</ref> and utilize multi-task learning to optimize for a single embedding. While these works are optimized for multiple classification tasks, our work is trained on retrieval tasks with several engagement types for multi-modal vertical recommendation systems. This kind of multi-modal multi-task learning helps us solve the special challenge of making our learned product embeddings compatible with embeddings of both query images and search queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Image and Search Query Representations at Pinterest</head><p>PinSage <ref type="bibr" target="#b32">[33]</ref> is a highly-scalable implementation of the GraphSage GCN algorithm <ref type="bibr" target="#b8">[9]</ref> that is deployed in production at Pinterest to produce the image embeddings of billions of pins. It aggregates the visual and textual features along with the graph information of pins to produce a rich and compact representation for various use cases including retrieval, ranking, and classification. SearchSage 1 is our search query embedding model trained by fine-tuning DistilBERT <ref type="bibr" target="#b23">[24]</ref>. It is trained on search query and engaged pin pairs from search logs. The loss aims to optimize the cosine similarity between the embedding of the global [CLS] token which can be seen as an aggregation of the input query and the output of an MLP that summarizes the pin into an embedding based on several text features and its PinSage embedding. Because other features in addition to PinSage contribute to the candidate pin representation and because of the MLP transformation, PinSage and SearchSage embeddings are not directly compatible with one another. We will refer to this later when we discuss baselines for ItemSage embeddings.</p><p>When training ItemSage, the PinSage model is used to provide the embeddings of both the feature images of the product and the query images, while the SearchSage model embeds the search query string. Since the PinSage and SearchSage models both have multiple applications in production, they are frozen at ItemSage training 1 https://medium.com/pinterest-engineering/searchsage-learning-search-queryrepresentations-at-pinterest-654f2bb887fc time due to considerations of development velocity and ease of adoption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>In this section, we introduce our approach to building product embeddings at Pinterest. We first formally define the notion of compatibility across learned representations for different entities.</p><p>Then, we introduce our model starting with the features used as input by the model and the process for collecting the training labels for the different tasks. We provide a detailed description of the model architecture, loss function, the multi-task learning regime and the inference and serving setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Embedding Compatibility</head><p>One of the requirements for ItemSage is to create product embeddings that are compatible with PinSage embeddings for images and SearchSage embeddings for search queries. In this case, compatibility means that the distance between a query (image or text string) embedding and a product embedding should be an informative score indicating how relevant the product is as a result for the query. We use cosine similarity as a measure of the embedding distance due to its simplicity and efficiency. The compatibility requirement originates from our desire to support candidate generation via approximate nearest neighbor (ANN) search techniques like Hierarchical Navigable Small Worlds (HNSW) <ref type="bibr" target="#b18">[19]</ref>. We cannot afford to apply expensive transformations to achieve compatibility as they would significantly increase retrieval latency. On the other hand, our experience indicates that for ranking and classification applications, compatibility plays a less important role as most deep learning models operating on pretrained embeddings can learn MLP transformations that are sufficient to map embeddings into a shared latent space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Features and Labels</head><p>A product consists of a list of images depicting the product from different angles or in different contexts and a list of text features. We truncate the list of images to at most 20 (99.7% of products in our catalog have less than or equal to 20 images). Each image is represented by its pretrained PinSage embedding <ref type="bibr" target="#b32">[33]</ref> and for products with fewer than 20 images, each empty slot is represented by a zero embedding. We use 12 text features as input to our model: title, description, merchant domain, product links, google product category<ref type="foot" target="#foot_0">2</ref> , product types<ref type="foot" target="#foot_1">3</ref> , brand, colors, materials, patterns, size, and size type. In cases where a product may have several values for a particular feature (e.g. links, colors, etc.) these strings are concatenated into one string. Standard word-level tokenization and lowercasing are applied to all text features. Each processed string is represented as a bag of word unigrams, bigrams and character trigrams <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>The tokens are mapped to numerical IDs using a vocabulary V of the most frequent 200,000 word unigrams, 1,000,000 word bigrams and 64,000 character trigrams and out-of-vocabulary tokens are discarded.</p><p>We construct our training dataset by collecting labels from the Closeup and Search engagement logs. Each positive example is a pair of query and engaged product where the query represents either an image for examples mined from Closeup logs or a text string for search logs. The dataset is deduplicated such that only one instance of a query and engaged product pair is kept. We select 4 engagement types to train our models: clicks and saves are the main actions that users can take on any Pinterest content, while add-to-cart actions and checkouts are actions that express shopping intent. The labels for all tasks are collected from the same date range. The number of positive labels is summarized in Table <ref type="table" target="#tab_0">1</ref>. In addition to positive labels, our loss uses random negative labels which we sample randomly from the shopping catalog. The negative labels are joined with the features offline and streamed into the model trainer through a separate data loader. The trainer alternates between consuming a batch of positive labels and a batch of negative labels which are then concatenated into a single training batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Architecture</head><p>We use a transformer encoder as the basic building block for learning product embeddings. It takes as input a sequence of 32 embeddings representing the image and text features of each product. The image embeddings are generated by the pretrained PinSage model <ref type="bibr" target="#b32">[33]</ref>, while the text embeddings are learned jointly with the encoder. In order to deal with the large vocabulary size, we apply the hash embedding trick from <ref type="bibr" target="#b25">[26]</ref> to learn a compact embedding table. The hash embedder consists of an embedding table ? of size 100, 000 ? 256 and an importance weight table ? of size |V | ? 2. We use two hashing functions ? 1 , ? 2 to map each token ID ? = 1, . . . , |V | into two slots in the embedding table ? 1 (?), ? 2 (?). The embedding of token with ID ? is then the weighted interpolation of the two embeddings: ? ?1 ? ? 1 (?) + ? ?2 ? ? 2 (?) . The final embedding of a feature string is the summation of all its token embeddings.</p><p>As shown in Figure <ref type="figure" target="#fig_3">3</ref>, we apply a linear transformation with output size 512 for each group of feature embeddings. This allows us to relax the requirement that all input features must have the same dimension. Similar to <ref type="bibr" target="#b4">[5]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Loss</head><p>We frame the problem of learning product embeddings as an extreme classification problem where given a query entity, our goal is to predict the product from the catalog that the user will engage with next <ref type="bibr" target="#b3">[4]</ref> ?=1 be the set of engaged products in the batch. Let C denote the catalog containing all products. Given the pretrained embeddings ? ? ? ? R ? for the queries ? ? , our goal is to learn embeddings ? ? ? ? R ? for the engaged products ? ? such that ? ? ? is more similar to ? ? ? than to all of the embeddings of other products from the catalog. This can be achieved by minimizing the softmax loss</p><formula xml:id="formula_0">? ? = - 1 |B| | B | ?? ?=1 log ? ?? ? ? ,? ? ? ? ? ? C ? ?? ? ? ,? ? ? ,<label>(1)</label></formula><p>where ??, ?? denotes the dot product function. In our case ?? ? ? , ? ? ? ? is the same as the cosine similarity between the two embeddings since they are ? 2 -normalized.</p><p>The main challenge with computing the softmax loss ? ? is the expensive nature of the normalization step ? ? C ? ?? ? ? ,? ? ? which involves a summation over the entire catalog. To make the loss computation tractable, a common technique is to approximate the normalization term by treating all the other positive examples from the same training batch as negatives and ignoring all the remaining products in the catalog. This approach is very efficient as no additional product embeddings need to be generated to compute the loss. However, naively replacing the whole catalog C with B introduces a sampling bias to the full softmax. We address this issue by applying the logQ correction <ref type="bibr" target="#b31">[32]</ref> that updates the logits ?? ? ? , ? ? ? to be ?? ? ? , ? ? ?log ? ? (?|? ? ), where ? ? (?|? ? ) is the probability of ? being included as a positive example in the training batch. The loss becomes:</p><formula xml:id="formula_1">? ? ??? = - 1 |B| | B | ?? ?=1 log ? ?? ? ? ,? ? ? ?-log ? ? (? ? |? ? ) ? ?B ? ?? ? ? ,? ? ?-log ? ? (? |? ? ) ,<label>(2)</label></formula><p>We estimate the probabilities ? ? (?|? ? ) in streaming fashion using a count-min sketch that tracks the frequencies with which entities appear in the training data. The count-min sketch <ref type="bibr" target="#b2">[3]</ref> is a probabilistic data structure useful for tracking the frequency of events in streams of data that uses sub-linear memory.</p><p>One problem we have experienced with using in-batch positives as negatives is that unengaged products in the catalog will never appear as negative labels. This treatment unfairly penalizes popular products as they are ever more likely to be selected as negatives. To counter this effect, we adopt a mixed negative sampling approach inspired by <ref type="bibr" target="#b30">[31]</ref>. For each training batch, we further select a set of random negatives N (where |N | = |B|) based on which we compute a second loss term:</p><formula xml:id="formula_2">? ? ??? = - 1 |N | | N | ?? ?=1 log ? ?? ? ? ,? ? ? ?-log ? ? (? ? ) ? ?N ? ?? ? ? ,? ? ?-log ? ? (?) ,<label>(3)</label></formula><p>where ? ? (?) represents the probability of random sampling product ?. The loss term ? ? ??? helps reduce the negative contribution that popular products receive when used as negative labels. The main distinction between our approach and <ref type="bibr" target="#b30">[31]</ref> is that we optimize for ? ? ??? + ? ? ??? , while <ref type="bibr" target="#b30">[31]</ref> uses both B and N to calculate the normalization term and minimizes</p><formula xml:id="formula_3">? ? ????? = - 1 |B| | B | ?? ?=1 log ? ?? ? ? ,? ? ? ?-log ? ? (? ? |? ? ) ? ? = ?? ? ?B ? ?? ? ? ,? ? ?-log ? ? (? |? ? ) + ?? ? ?N ? ?? ? ? ,? ? ?-log ? ? (?) .<label>(4)</label></formula><p>Section 4.1.4 shows that we obtain better results separating the loss terms of the two negative sampling approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Multi-task Learning</head><p>We implement multi-task learning by combining positive labels from all the different tasks into the same training batch (Figure <ref type="figure" target="#fig_5">4</ref>). This technique is effective even when the query entities come from different modalities, the only difference being that the query embedding needs to be inferred with a different pretrained model. trade-offs between different tasks. When introducing new tasks, we find that setting ? ? = |B|/? can be a good starting point to achieve significant improvements towards the new task without hurting the performance of other tasks. We believe the lack of negative impact on existing tasks can be attributed to the correlation between tasks. For example, to purchase a product users are likely to click on it first, thus adding the click task will not hurt the performance of the purchase task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Inference and Serving</head><p>Figure <ref type="figure" target="#fig_6">5</ref> illustrates how ItemSage embeddings are deployed to power Pinterest shopping recommendations. The embeddings are inferred in a batch workflow, which ensures that the model inference latency does not impact the end-to-end latency of the vertical recommendation systems. The inference workflow runs daily to update the embeddings based on the latest features and to extend the coverage to newly ingested products. Each new set of embeddings is used to create an HNSW index <ref type="bibr" target="#b18">[19]</ref> for candidate generation and is also pushed to the online feature store for ranking applications. The HNSW index and the feature set are reused by all of the different vertical systems for Home, Closeup and Search recommendations. The Home and Closeup surfaces use precomputed PinSage embeddings fetched from the feature store as queries, while in Search, the query embeddings are inferred on the fly to support the long tail of previously unseen queries. The main thing to note is the simplicity of this design enabled by multi-task learning. By creating a single set of embeddings for all 3 vertical applications, we can use a single  inference workflow and a single HNSW index to serve recommendations, thereby reducing the infrastructure and maintenance costs by three times.</p><p>After ItemSage embeddings are published to the offline feature store, they can be used as features in classification models designed to infer missing information about the products in our catalog. One example with which we have seen early success is inferring whether a product (e.g. in the fashion vertical) is intended to be sold to male users, female users or whether it is unisex. Training a simple MLP on top of ItemSage embeddings yielded a 3.6% improvement in top-1 accuracy over our previous baseline. 4   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we provide an empirical analysis of ItemSage embeddings, focused on evaluating the design choices outlined in this paper. We first conduct an extensive evaluation of the embeddings on offline benchmarks, followed by sharing results obtained via A/B experiments on live traffic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Offline Evaluation</head><p>4.1.1 Evaluation Protocol. Our goal is to build embeddings that are effective throughout the recommendation stack starting from candidate generation. Therefore we use recall as the main metric to evaluate the quality of embeddings and potential trade-offs.</p><p>We set up eight offline benchmarks for model evaluation, including four engagement types (clicks, saves, add-to-cart actions and checkouts) for two surfaces with different query modalities (Closeup and Search). Each benchmark P consists of a set of 80,000 pairs of query and engaged products (? ? , ? ? ) that are sampled from image based recommendations or search results not included in the training data. We also randomly sampled a distractor set C of one million products from the shopping catalog C to measure the engagement-weighted recall at ?, which we define as the weighted proportion of (? ? , ? ? ) pairs for which the engaged product ? ? is ranked within top ? among C ? {? ? }, 4 We do not provide a detailed presentation on using ItemSage for classification applications in this work as our approach does not control for several important factors including dataset, features and model architecture compared to our baseline. Nonetheless, it is encouraging to see that our product embeddings can deliver impact to other applications with little effort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>??????@?</head><formula xml:id="formula_4">= 1 ? ? ? | P | ?? ?=1 ? ? 1 ?? ? ? , ? ? ? ? ?? ? ? , ? ? ? ?|? ? C ? ? ,<label>(5)</label></formula><p>where ? ? represents the engagement count associated with (? ? , ? ? ). In the following experiments, we fix the value of ? to 10 since the results for other values of ? are similar.</p><p>4.1.2 Model Architecture. In this section we compare the model architecture described above with several baselines and deeper versions of the ItemSage model with additional transformer encoder blocks. The first baseline simply applies sum-pooling and ? 2 normalization to the PinSage embeddings of the images associated with each product (denoted as Sum). While this baseline is fairly simplistic, note that PinSage embeddings have been independently trained to effectively generate image (pin) candidates for non-shopping use cases at Pinterest. Additionally, other works <ref type="bibr" target="#b20">[21]</ref> have reported simple pooling as being a baseline difficult to outperform for e-commerce search.</p><p>The Sum baseline cannot generate candidates for search results since the PinSage image model and the fine-tuned DistilBERT models produce embeddings that are not compatible with one another. We introduce a second baseline for search, Sum-MLP, which applies the pretrained MLP for image candidates from SearchSage (introduced in Section 2.3) on every product image, followed by a sum-pooling and ? 2 normalization to obtain product level embeddings.</p><p>The Sum and Sum-MLP baselines require little effort to generate. Consequently, they were adopted in production to support shopping recommendations while ItemSage embeddings were under development. We will refer to these baselines again in Section 4.2 when discussing results in A/B experiments.</p><p>The final baseline (denoted as MLP-Concat-MLP) is more competitive. It first maps each input feature into a latent space by applying a 3-layer MLP module with 256 hidden units. We learn separate MLPs for image and text features. The latent representations are concatenated into a single vector and passed through a second 3-layer MLP.</p><p>The results are presented in Table <ref type="table" target="#tab_2">2</ref>. We observe that ItemSage strongly outperforms the Sum and Sum-MLP baselines. The transformer architecture yields improvements over the MLP-Concat-MLP model baseline on all tasks; the most notable improvements can be seen in the search tasks for clicks and saves. We attribute these gains to the self-attention module in ItemSage. However, using deeper architectures does not significantly improve the model performance: the results using 2 or 3 layers are worse than the 1 layer baseline, while the model with 4 layers has mixed results. In all cases, the relative change in metrics is small and considering the increased cost of deeper architectures, we chose to deploy the variant with a single transformer layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Feature Ablation.</head><p>In this section, we analyze the benefit of learning the ItemSage embeddings from multi-modal features. We compare our final model with two different models using the same transformer architecture, but limited to using features corresponding to a single modality: image or text (Table <ref type="table">3</ref>, Row "Feature"). The image-only model takes as input just the PinSage embeddings of We observe that the model trained on features from both modalities has significantly better performance than both baselines on all 8 tasks. Also, the image only model has significantly stronger performance over the text-only model, which could be attributed to the additional information summarized into the image embeddings: (1) PinSage is a graph neural network (GNN) aggregating information from the Pinterest pin-board graph in addition to the image itself and (2) the learnings presented in this paper regarding multi-modal learning have also been applied within PinSage to textual metadata available with each image. As one might expect, the models trained on a single feature modality have stronger performance when the query comes from the same modality, i.e., the image-only model shows better performance on Closeup tasks, while the text-only model performs better on Search tasks. Inspired by PinSage and related work on GNNs [9, 10], we conducted an experiment augmenting ItemSage with information obtained from the Pinterest pin-board graph <ref type="bibr" target="#b5">[6]</ref>. Products can naturally be embedded into this graph by creating edges between a product and its own images. For each product, we performed random walks starting from its corresponding node (reusing the same configuration as PinSage <ref type="bibr" target="#b32">[33]</ref>) and kept the most frequently occurring 50 image neighbors that are different from the product's own images. The images are mapped to their corresponding PinSage embeddings which are appended to the sequence of features provided as input to the transformer. This extension to ItemSage showed neutral results (Table <ref type="table">3</ref>, Row "Feature") and increased training cost. We attribute this result to the fact that the PinSage embeddings of the product's own images are already aggregating the graph information, making the explicit extension redundant.</p><p>4.1.4 Loss Ablation. In Section 3.4, we introduce our approach for sampling negative labels which consists of two sources: (1) other positive labels from the same batch as the current example and (2) randomly sampled negative labels from the entire catalog. In this section, we compare how our model performs if the negative labels are selected from only one of these sources. The results are presented in the Row "Negative Sampling" of Table <ref type="table">3</ref>. We observe a steep drop in recall if only one source of negatives is used. Moreover, if we only select random negatives, the model converges to a degenerate solution (and thus we need to apply early stopping) where a few products become very popular and appear in the top 10 results of more than 10%-15% of the queries in the evaluation set depending on the task.</p><p>We also compare our mixed negative sampling approach with the approach from <ref type="bibr" target="#b30">[31]</ref>, and observe that our approach which introduces separate loss terms for in batch positives and random negatives provides an improvement of at least 3.5% on every task. 4.1.5 Task Ablation. In this section, we evaluate the effectiveness of the multi-task learning regime used to train ItemSage.</p><p>We first evaluate the recall of ItemSage embeddings against two baseline models trained on vertical specific tasks: "Closeup" and "Search" (Table <ref type="table">3</ref>, Row "Surface"). Each baseline uses the same model architecture and represents the performance we would expect if we deployed separate embeddings per vertical. We find it encouraging that the model optimized for both verticals performs better on 6 out of 8 tasks. This suggests that applying multi-task learning leads to improved performance even across product verticals and, in the cases where the performance degrades, the degradation is not substantial compared to a vertical specific model. This is a remarkable result considering that the PinSage and SearchSage embeddings are not compatible with one another. We also find it interesting that the largest improvements are seen on shopping specific objectives (add-to-cart actions and checkouts), these actions being 30 times more sparse than clicks and saves, suggesting that the cross vertical setup helps the model extract more information about which products are more likely to be purchased by users.</p><p>The next experiment focuses on the impact of mixing regular engagement tasks (clicks and saves) together with shopping engagement tasks (add-to-cart actions and checkouts). The results are summarized in the Row "Engagement Type" of Table <ref type="table">3</ref>. As expected, regardless of the task, we see an improvement in recall whenever we optimize the model on that specific task. Furthermore, we observe substantial gains in add-to-cart actions and checkouts compared to a model specialized at modeling just shopping engagement and minimal losses on clicks and saves tasks compared to the corresponding specialized model. We believe the substantial gains (3.7%-8.7%) of the joint model on add-to-cart actions and checkouts can be explained by the significant difference in training data volume between shopping and regular engagement, the additional click and save labels help the embeddings converge towards a more robust representation.</p><p>Finally, we show that by applying multi-task learning, ItemSage embeddings can optimize for multiple engagement types leading to improved performance for objectives with sparse labels and no penalty for objectives with sufficient training labels. By optimizing our product embeddings this way, we can ensure that the shopping recommendation stack is efficient with respect to all objectives starting from the candidate generation layer.</p><p>The effectiveness of our approach is demonstrated by thorough offline ablation studies and online A/B experiments. There are several promising areas for future work, such as replacing the bag of words model used for text features with a pretrained Transformer model or using neighborhood sampling to extract additional multi-modal features from the Pinterest entity graph from shopping specific entities (e.g. merchants) or edges (e.g. co-purchases).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Products on Pinterest consist of several images and rich textual metadata.</figDesc><graphic url="image-1.png" coords="1,317.96,237.62,240.24,130.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Screenshots of ItemSage being used for product recommendations on Home, Closeup and Search surfaces.</figDesc><graphic url="image-2.png" coords="2,49.39,81.81,80.83,147.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>, we use a global token [CLS] to aggregate the information from the input sequence. The transformed embeddings are concatenated together with the global token and then passed through a one-layer transformer block consisting of a self-attention module with 8 heads and a feed forward module</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: ItemSage model architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>. Formally, let {? ? , ? ? } | B | ?=1 be a training batch of query, engaged product pairs sampled from the engagement logs and B = {? ? } | B |</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The construction of a training batch. The white squares on the diagonal indicates a mask applied to prevent using an example's positive label also as a negative. Squares of different shades denote different queries and products.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: ItemSage inference and serving.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Training data volume.</figDesc><table><row><cell>Surface</cell><cell>Engagement</cell><cell>No. Examples</cell></row><row><cell>Closeup</cell><cell>Clicks + Saves</cell><cell>93.3M</cell></row><row><cell cols="2">Closeup Checkouts + Add-to-Cart</cell><cell>3.7M</cell></row><row><cell>Search</cell><cell>Clicks + Saves</cell><cell>99.4M</cell></row><row><cell cols="2">Search Checkouts + Add-to-Cart</cell><cell>3.5M</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of different model architectures with baselines.</figDesc><table><row><cell></cell><cell>Number of</cell><cell></cell><cell></cell><cell>Closeup</cell><cell></cell><cell></cell><cell></cell><cell>Search</cell></row><row><cell></cell><cell cols="9">Parameters Clicks Saves Add-to-Cart Checkouts Clicks Saves Add-to-Cart Checkouts</cell></row><row><cell>Sum</cell><cell>-</cell><cell cols="2">0.663 0.647</cell><cell>0.669</cell><cell>0.699</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Sum-MLP</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">0.577 0.533</cell><cell>0.561</cell><cell>0.629</cell></row><row><cell>MLP-Concat-MLP</cell><cell>30.8M</cell><cell cols="2">0.805 0.794</cell><cell>0.896</cell><cell>0.916</cell><cell cols="2">0.723 0.736</cell><cell>0.834</cell><cell>0.861</cell></row><row><cell>ItemSage</cell><cell>33.1M</cell><cell cols="2">0.816 0.812</cell><cell>0.897</cell><cell>0.916</cell><cell cols="2">0.749 0.762</cell><cell>0.842</cell><cell>0.869</cell></row><row><cell>2-Layer Transformer</cell><cell>36.3M</cell><cell cols="2">0.815 0.809</cell><cell>0.895</cell><cell>0.913</cell><cell cols="2">0.745 0.759</cell><cell>0.837</cell><cell>0.867</cell></row><row><cell>3-Layer Transformer</cell><cell>39.4M</cell><cell cols="2">0.815 0.810</cell><cell>0.896</cell><cell>0.915</cell><cell cols="2">0.747 0.758</cell><cell>0.841</cell><cell>0.869</cell></row><row><cell>4-Layer Transformer</cell><cell>42.6M</cell><cell cols="2">0.816 0.813</cell><cell>0.897</cell><cell>0.915</cell><cell cols="2">0.750 0.764</cell><cell>0.840</cell><cell>0.869</cell></row><row><cell cols="5">the product's images. The text-only model is trained based on the</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">text attributes from the shopping catalog (title, description, etc.).</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>The google product category represents the node from a standard taxonomy that merchants may tag their products with. The taxonomy can be found at https://www. google.com/basepages/producttype/taxonomy.en-US.txt</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>The product types are html breadcrumbs scraped from the merchant product page, e.g. Home Decor &gt; New Arrivals.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank <rs type="person">Pak-Ming Cheung</rs>, <rs type="person">Van Lam</rs>, <rs type="person">Jiajing Xu</rs>, <rs type="person">Cosmin Negruseri</rs>, <rs type="person">Abhishek Tayal</rs>, <rs type="person">Somnath Banerjee</rs>, <rs type="person">Vijai Mohan</rs> and <rs type="person">Kurchi Subhra Hazra</rs> who contributed or supported us throughout this project.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this work, we presented our end-to-end approach of learning ItemSage, the product embeddings for shopping recommendations at Pinterest. In contrast to other work focused on representation learning for e-commerce applications, our embeddings are able to extract information from both text and image features. Visual features are particularly effective for platforms with a strong visual component like Pinterest, while modeling text features leads to improved relevance, especially in search results.</p><p>Furthermore, we describe a procedure to make our embeddings compatible with the embeddings of other entities in the Pinterest ecosystem at the same time. Our approach enables us to deploy a single embedding version to power applications with different inputs: sequences of pins (images) for user activity based recommendations in the Home surface, single images for image based recommendations in the Closeup surface and text queries to power search results. This leads to a 3X reduction in infrastructure costs as we do not need to infer separate embeddings per vertical. From our experience, embedding version upgrades are a slow process taking consumers up to a year to completely adopt a new version before an old one can be fully deprecated. A unified embedding for all applications means less maintenance throughout this period and a more efficient, consolidated process for upgrades and deprecation.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Billion-scale pretraining with vision transformers for multi-task visual representations</title>
		<author>
			<persName><forename type="first">Josh</forename><surname>Beal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao-Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Kislyuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="564" to="573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Uniter: Learning universal image-text representations</title>
		<author>
			<persName><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An improved data stream summary: the count-min sketch and its applications</title>
		<author>
			<persName><forename type="first">Graham</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shan</forename><surname>Muthukrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Algorithms</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="58" to="75" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep neural networks for youtube recommendations</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emre</forename><surname>Sargin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM conference on recommender systems</title>
		<meeting>the 10th ACM conference on recommender systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pixie: A system for recommending 3+ billion items to 200+ million users in real-time</title>
		<author>
			<persName><forename type="first">Chantat</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Jindal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><forename type="middle">Zitao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sugnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Ulrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 world wide web conference</title>
		<meeting>the 2018 world wide web conference</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1775" to="1784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Detext: A deep text ranking framework with BERT</title>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiji</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananth</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zimeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bee-Chung</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 29th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2509" to="2516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep multimodal representation learning: A survey</title>
		<author>
			<persName><forename type="first">Wenzhong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiping</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="63373" to="63394" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Inductive representation learning on large graphs. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Heterogeneous graph transformer</title>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
		<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="2704" to="2710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Audio-visual deep learning for noise robust speech recognition</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE international conference on acoustics, speech and signal processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="7596" to="7599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Embeddingbased retrieval in Facebook search</title>
		<author>
			<persName><forename type="first">Jui-Ting</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuying</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Pronin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janani</forename><surname>Padmanabhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Ottaviano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2553" to="2561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Information &amp; Knowledge Management</title>
		<meeting>the 22nd ACM international conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2333" to="2338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Modeling multimodal clues in a hybrid deep learning framework for video classification</title>
		<author>
			<persName><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zechao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="3137" to="3147" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Liunian</forename><surname>Harold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Da Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03557</idno>
		<title level="m">Visualbert: A simple and performant baseline for vision and language</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Representation learning using multi-task deep neural networks for semantic classification and information retrieval</title>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye-Yi</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multimodal video classification with stacked contractive autoencoders</title>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqing</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiguang</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page" from="761" to="766" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youna</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiwei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Yin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.02978</idno>
		<title level="m">Graph-based multilingual product retrieval in e-commerce search</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><forename type="middle">A</forename><surname>Malkov</surname></persName>
		</author>
		<author>
			<persName><surname>Yashunin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="824" to="836" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juhan</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semantic product search</title>
		<author>
			<persName><forename type="first">Priyanka</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiwei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijai</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vihan</forename><surname>Lakshman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weitian</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Shingavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Choon</forename><surname>Hui Teo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2876" to="2885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pinnersage: Multi-modal user embedding framework for recommendations at Pinterest</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chantat</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yitong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2311" to="2320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">An overview of multi-task learning in deep neural networks</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05098</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01108</idno>
		<title level="m">DistilBERT, a distilled version of BERT: Smaller, faster, cheaper and lighter</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multimodal graph networks for compositional generalization in visual question answering</title>
		<author>
			<persName><forename type="first">Raeid</forename><surname>Saqur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Dan</forename><surname>Svenstrup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><forename type="middle">Meinertz</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ole</forename><surname>Winther</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.03933</idno>
		<title level="m">Hash embeddings for efficient word representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07490</idno>
		<title level="m">Lxmert: Learning cross-modality encoder representations from transformers</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Progressive layered extraction (PLE): A novel multi-task learning (MTL) model for personalized recommendations</title>
		<author>
			<persName><forename type="first">Hongyan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junning</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xudong</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourteenth ACM Conference on Recommender Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="269" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multimodal deep representation learning for video classification</title>
		<author>
			<persName><forename type="first">Haiman</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yudong</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><surname>Pouyanfar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu-Ching</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mei-Ling</forename><surname>Shyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World Wide Web</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1325" to="1341" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Alim</forename><surname>Virani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Baxter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Shiebler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Gautier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivam</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Binnani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linlin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.09293</idno>
		<title level="m">Lessons learned addressing dataset bias in model-based candidate generation at Twitter</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mixed negative sampling for learning two-tower neural networks in recommendations</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">Zhiyuan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">Xiaoming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taibai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion Proceedings of the Web Conference</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="441" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sampling-bias-corrected neural modeling for large corpus item recommendations</title>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">Zhiyuan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Heldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditee</forename><surname>Kumthekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM Conference on Recommender Systems</title>
		<meeting>the 13th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="269" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning a unified embedding for visual search at Pinterest</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao-Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Rosenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2412" to="2420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Towards personalized and semantic retrieval: An end-to-end solution for e-commerce search via embedding learning</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiling</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunjiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weipeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Yun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2407" to="2416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Recommending what video to watch next: A multitask ranking system</title>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniruddh</forename><surname>Nath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditee</forename><surname>Kumthekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maheswaran</forename><surname>Sathiamoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM Conference on Recommender Systems</title>
		<meeting>the 13th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="43" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unified vision-language pre-training for image captioning and VQA</title>
		<author>
			<persName><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamid</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13041" to="13049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anant</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun-Long</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balaji</forename><surname>Krishnapuram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roelof</forename><surname>Van Zwol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PinText</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2020">2020</date>
			<publisher>Attentive bag of annotations embedding</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
