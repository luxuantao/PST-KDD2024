<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Evaluating probabilities under high-dimensional latent variable models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Iain</forename><surname>Murray</surname></persName>
							<email>murray@cs.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Toronto Toronto</orgName>
								<address>
									<postCode>M5S 3G4</postCode>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Toronto Toronto</orgName>
								<address>
									<postCode>M5S 3G4</postCode>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Evaluating probabilities under high-dimensional latent variable models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E9DC91F37FD78E18483278BEC2FEBFBD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a simple new Monte Carlo algorithm for evaluating probabilities of observations in complex latent variable models, such as Deep Belief Networks. While the method is based on Markov chains, estimates based on short runs are formally unbiased. In expectation, the log probability of a test set will be underestimated, and this could form the basis of a probabilistic bound. The method is much cheaper than gold-standard annealing-based methods and only slightly more expensive than the cheapest Monte Carlo methods. We give examples of the new method substantially improving simple variational bounds at modest extra cost.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Latent variable models capture underlying structure in data by explaining observations as part of a more complex, partially observed system. A large number of probabilistic latent variable models have been developed, most of which express a joint distribution P (v, h) over observed quantities v and their unobserved counterparts h. Although it is by no means the only way to evaluate a model, a natural question to ask is "what probability P (v) is assigned to a test observation?".</p><p>In some models the latent variables associated with a test input can be easily summed out: P (v) = h P (v, h). As an example, standard mixture models have a single discrete mixture component indicator for each data point; the joint probability P (v, h) can be explicitly evaluated for each setting of the latent variable.</p><p>More complex graphical models explain data through the combination of many latent variables. This provides richer representations, but provides greater computational challenges. In particular, marginalizing out many latent variables can require complex integrals or exponentially large sums. One popular latent variable model, the Restricted Boltzmann Machine (RBM), is unusual in that the posterior over hiddens P (h|v) is fully-factored, which allows efficient evaluation of P (v) up to a constant. Almost all other latent variable models have posterior dependencies amongst latent variables, even if they are independent a priori.</p><p>Our current work is motivated by recent work on evaluating RBMs and their generalization to Deep Belief Networks (DBNs) <ref type="bibr" target="#b0">[1]</ref>. For both types of models, a single constant was accurately approximated so that P (v, h) could be evaluated point-wise. For RBMs, the remaining sum over hidden variables was performed analytically. For DBNs, test probabilities were lower-bounded through a variational technique. Perhaps surprisingly, the bound was unable to reveal any significant improvement over RBMs in an experiment on MNIST digits. It was unclear whether this was due to looseness of the bound, or to there being no difference in performance.</p><p>A more accurate method for summing over latent variables would enable better and broader evaluation of DBNs. In section 2 we consider existing Monte Carlo methods. Some of them are certainly more accurate, but prohibitively expensive for evaluating large test sets. We then develop a new cheap Monte Carlo procedure for evaluating latent variable models in section 3. Like the variational method used previously, our method is unlikely to spuriously over-state test-set performance. Our presentation is for general latent variable models, however for a running example, we use DBNs (see section 4 and <ref type="bibr" target="#b1">[2]</ref>). The benefits of our new approach are demonstrated in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Probability of observations as a normalizing constant</head><p>The probability of a data vector, P (v), is the normalizing constant relating the posterior over hidden variables to the joint distribution in Bayes rule, P (h|v) = P (h, v)/P (v). A large literature on computing normalizing constants exists in physics, statistics and computer science. In principle, there are many methods that could be applied to evaluating the probability assigned to data by a latent variable model. We review a subset of these methods, with notation and intuitions that will help motivate and explain our new algorithm.</p><p>In what follows, all auxiliary distributions Q and transition operators T are conditioned on the current test case v, this is not shown in the notation to reduce clutter. Further, all of these methods assume that we can evaluate P (h, v). Graphical models with undirected connections will require the separate estimation of a single constant as in <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Importance sampling</head><p>Importance sampling can in principle find the normalizing constant of any distribution. The algorithm involves averaging a simple ratio under samples from some convenient tractable distribution over the hidden variables, Q(h). Provided Q(h) = 0 whenever P (h, v) = 0, we obtain:</p><formula xml:id="formula_0">P (v) = h P (h, v) Q(h) Q(h) ≈ 1 S S s=1 P h (s) , v Q h (s) , h (s) ∼ Q h (s) .<label>(1)</label></formula><p>Importance sampling relies on the sampling distribution Q(h) being similar to the target distribution P (h|v). Specifically, the variance of the estimator is an α-divergence between the distributions <ref type="bibr" target="#b2">[3]</ref>. Finding a tractable Q(h) with small divergence is difficult in high-dimensional problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Harmonic mean method</head><p>Using Q(h) = P (h|v) in (1) gives an "estimator" that requires knowing P (v). As an alternative, the harmonic mean method, also called the reciprocal method, gives an unbiased estimate of 1/P (v):</p><formula xml:id="formula_1">1 P (v) = h P (h) P (v) = h P (h|v) P (v|h) ≈ 1 S S s=1 1 P v|h (s) , h (s) ∼ P h (s) |v).<label>(2)</label></formula><p>In practice correlated samples from MCMC are used; then the estimator is asymptotically unbiased.</p><p>It was clear from the original paper and its discussion that the harmonic mean estimator can behave very poorly <ref type="bibr" target="#b3">[4]</ref>. Samples in the tails of the posterior have large weights, which makes it easy to construct distributions where the estimator has infinite variance. A finite set of samples will rarely include any extremely large weights, so the estimator's empirical variance can be misleadingly low. In many problems, the estimate of 1/P (v) will be an underestimate with high probability. That is, the method will overestimate P (v) and often give no indication that it has done so.</p><p>Sometimes the estimator will have manageable variance. Also, more expensive versions of the estimator exist with lower variance. However, it is still prone to overestimate test probabilities: If 1/ PHME (v) is the Harmonic Mean Estimator in (2), Jensen's inequality gives</p><formula xml:id="formula_2">P (v) = 1 E 1/ PHME (v) ≤ E PHME (v) .</formula><p>Similarly log P (v) will be overestimated in expectation.</p><p>Hence the average of a large number of test log probabilities is highly likely to be an overestimate.</p><p>Despite these problems the estimator has received significant attention in statistics, and has been used for evaluating latent variable models in recent machine learning literature <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. This is understandable: all of the existing, more accurate methods are harder to implement and take considerably longer to run. In this paper we propose a method that is nearly as easy to use as the harmonic mean method, but with better properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Importance sampling based on Markov chains</head><p>Paradoxically, introducing auxiliary variables and making a distribution much higher-dimensional than it was before, can help find an approximating Q distribution that closely matches the target distribution. As an example we give a partial review of Annealed Importance Sampling (AIS) <ref type="bibr" target="#b6">[7]</ref>, a special case of a larger family of Sequential Monte Carlo (SMC) methods (see, e.g., <ref type="bibr" target="#b7">[8]</ref>). Some of this theory will be needed in the new method we present in section 3.</p><p>Annealing algorithms start with a sample from some tractable distribution P 1 . Steps are taken with a series of operators T 2 , T 3 , . . . , T S , whose stationary distributions, P s , are "cooled" towards the distribution of interest. The probability over the resulting sequence H = {h (1) , h (2) , . . . h (S) } is:</p><formula xml:id="formula_3">Q AIS (H) = P 1 h (1) S s=2 T s h (s) ← h (s-1) .<label>(3)</label></formula><p>To compute importance weights, we need to define a "target" distribution on the same state-space:</p><formula xml:id="formula_4">P AIS (H) = P h (S) |v S s=2 T s h (s-1) ← h (s) .<label>(4)</label></formula><p>Because h (S) has marginal P (h|v) = P (h, v)/P (v), P AIS (H) has our target, P (v), as its normalizing constant. The T operators are the reverse operators, of those used to define Q AIS .</p><p>For any transition operator T that leaves a distribution P (h|v) stationary, there is a unique corresponding "reverse operator" T , which is defined for any point h in the support of P :</p><formula xml:id="formula_5">T (h ← h ) = T (h ← h) P (h|v) h T (h ← h) P (h|v) = T (h ← h) P (h|v) P (h |v) .<label>(5)</label></formula><p>The sum in the denominator is known because T leaves the posterior stationary. Operators that are their own reverse operator are said to satisfy "detailed balance" and are also known as "reversible". Many transition operators used in practice, such as Metropolis-Hastings, are reversible. Non-reversible operators are usually composed from a sequence of reversible operations, such as the component updates in a Gibbs sampler. The reverse of these (so-called) non-reversible operators is constructed from the same reversible base operations, but applied in reverse order.</p><p>The definitions above allow us to write:</p><formula xml:id="formula_6">Q AIS (H) = P AIS (H) Q AIS (H) P AIS (H) = P AIS (H) P 1 h (1) P h (S) |v • S s=2 T s h (s) ← h (s-1)</formula><p>T s h (s-1) ← h (s)</p><formula xml:id="formula_7">= P AIS (H) P (v) P 1 h (1) P h (S) , v • S s=2 P * s (h (s) ) P * s (h (s-1) ) ≡ P AIS (H) P (v) w(H) .<label>(6)</label></formula><p>We can usually evaluate the P * s , which are unnormalized versions of the stationary distributions of the Markov chain operators. Therefore the AIS importance weight w(H) = 1/ [• • • ] is tractable as long as we can evaluate P (h, v). The AIS importance weight provides an unbiased estimate:</p><formula xml:id="formula_8">E QAIS(H) w(H) = P (v) H P AIS (H) = P (v).<label>(7)</label></formula><p>As with standard importance sampling, the variance of the estimator depends on a divergence between P AIS and Q AIS . This can be made small, at large computational expense, by using hundreds or thousands of steps S, allowing the neighboring intermediate distributions P s (h) to be close.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Chib-style estimators</head><p>Bayes rule implies that for any special hidden state h * , P (v</p><formula xml:id="formula_9">) = P (h * , v)/P (h * |v).<label>(8)</label></formula><p>This trivial identity suggests a family of estimators introduced by Chib <ref type="bibr" target="#b8">[9]</ref>. First, we choose a particular hidden state h * , usually one with high posterior probability, and then estimate P (h * |v).</p><p>We would like to obtain an estimator that is based on a sequence of states H = {h (1) , h (2) , . . . , h (S) } generated by a Markov chain that explores the posterior distribution P (h|v). The most naive estimate of P (h * |v) is the fraction of states in H that are equal to the special state s I(h</p><formula xml:id="formula_10">(s) = h * )/S.</formula><p>Obviously this estimator is impractical as it equals zero with high probability when applied to highdimensional problems. A "Rao-Blackwellized" version of this estimator, p(H), replaces the indicator function with the probability of transitioning from h (s) to the special state under a Markov chain transition operator that leaves the posterior stationary. This can be derived directly from the operator's stationary condition:</p><formula xml:id="formula_11">P (h * |v) = h T (h * ← h)P (h|v) ≈ p(H) ≡ 1 S S s=1 T (h * ← h (s) ), {h (s) } ∼ P(H),<label>(9)</label></formula><p>where P(H) is the joint distribution arising from S steps of a Markov chain. If the chain has stationary distribution P (h|v) and could be initialized at equilibrium so that</p><formula xml:id="formula_12">P(H) = P h (1) v S s=2 T h (s) ← h (s-1) ,<label>(10)</label></formula><p>then p(H) would be an unbiased estimate of P (h * |v). For ergodic chains the stationary distribution is achieved asymptotically and the estimator is consistent regardless of how it is initialized.</p><p>If T is a Gibbs sampling transition operator, the only way of moving from h to h * is to draw each element of h * in turn. If updates are made in index order from 1 to M , the move has probability:</p><formula xml:id="formula_13">T (h * ← h) = M j=1 P h * j h * 1:(j-1) , h (j+1):M .<label>(11)</label></formula><p>Equations <ref type="bibr" target="#b8">(9,</ref><ref type="bibr" target="#b10">11)</ref> have been used in schemes for monitoring the convergence of Gibbs samplers <ref type="bibr" target="#b9">[10]</ref>.</p><p>It is worth emphasizing that we have only outlined the simplest possible scheme inspired by Chib's general approach. For some Markov chains, there are technical problems with the above construction, which require an extension explained in the appendix. Moreover the approach above is not what Chib recommended. In fact, <ref type="bibr" target="#b10">[11]</ref> explicitly favors a more elaborate procedure involving sampling from a sequence of distributions. This opens up the possibility of many sophisticated developments, e.g. <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. However, our focus in this work is on obtaining more useful results from simple cheap methods. There are also well-known problems with the Chib approach <ref type="bibr" target="#b13">[14]</ref>, to which we will return.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A new estimator for evaluating latent-variable models</head><p>We start with the simplest Chib-inspired estimator based on equations <ref type="bibr" target="#b7">(8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11)</ref>. Like many Markov chain Monte Carlo algorithms, (9) provides only (asymptotic) unbiasedness. For our purposes this is not sufficient. Jensen's inequality tells us</p><formula xml:id="formula_14">P (v) = P (h * , v) P (h * |v) = P (h * , v) E[p(H)] ≤ E P (h * , v) p(H) .<label>(12)</label></formula><p>That is, we will overestimate the probability of a visible vector in expectation. Jensen's inequality also says that we will overestimate log P (v) in expectation.</p><p>Ideally we would like an accurate estimate of log P (v). However, if we must suffer some bias, then a lower bound that does not overstate performance will usually be preferred. An underestimate of P (v) would result from overestimating P (h * |v). The probability of the special state h * will often be overestimated in practice if we initialize our Markov chain at h * . There are, however, simple counter-examples where this does not happen. Instead we describe a construction based on a sequence of Markov steps starting at h * that does have the desired effect. We draw a state sequence from the following carefully designed distribution, using the algorithm in figure <ref type="figure" target="#fig_0">1</ref>:</p><formula xml:id="formula_15">Q(H) = 1 S S s=1 T h (s) ← h * S s =s+1 T h (s ) ← h (s -1) s-1 s =1 T h (s ) ← h (s +1) .<label>(13)</label></formula><p>If the initial state were drawn from P (h|v) instead of T h (s) ← h * , then the algorithm would give a sample from an equilibrium sequence with distribution P(H) defined in <ref type="bibr" target="#b9">(10)</ref>. This can be checked by repeated substitution of ( <ref type="formula" target="#formula_5">5</ref>). This allows us to express Q in terms of P, as we did for AIS:</p><formula xml:id="formula_16">Q(H) = 1 S S s=1 T h (s) ← h * P h (s) |v P(H) = 1 P (h * |v) 1 S S s=1 T h * ← h (s) P(H).<label>(14)</label></formula><p>Inputs: v, observed test vector h * , a (preferably high posterior probability) hidden state S, number of Markov chain steps T , Markov chain operator that leaves P (h|v) stationary 1. Draw s ∼ Uniform({1, . . . S})</p><p>2. Draw h (s) ∼ T h (s) ← h * 3. for s = (s + 1) : S 4. Draw h (s ) ∼ T h (s ) ← h (s -1) 5. for s = (s -1) : -1 : 1</p><formula xml:id="formula_17">6. Draw h (s ) ∼ T h (s ) ← h (s +1) 7. P (v) ≈ P (v, h * ) 1 S S s =1 T (h * ← h (s ) ) h * h (1)</formula><p>h (2)   h The quantity in square brackets is the estimator for P (h * |v) given in <ref type="bibr" target="#b8">(9)</ref>. The expectation of the reciprocal of this quantity under draws from Q(H) is exactly the quantity needed to compute P (v):</p><formula xml:id="formula_19">E Q(H) 1 1 S S s=1 T h * ← h (s) = 1 P (h * |v) H P(H) = 1 P (h * |v) . (<label>15</label></formula><formula xml:id="formula_20">)</formula><p>Although we are using the simple estimator from <ref type="bibr" target="#b8">(9)</ref>, by drawing H from a carefully constructed Markov chain procedure, the estimator is now unbiased in P (v). This is not an asymptotic result. As long as no division by zero has occurred in the above equations, the estimator is unbiased in P (v) for finite runs of the Markov chain. Jensen's implies that log P (v) is underestimated in expectation.</p><p>Neal noted that Chibs method will return incorrect answers in cases where the Markov chain does not mix well amongst modes <ref type="bibr" target="#b13">[14]</ref>. Our new proposed method will suffer from the same problem. Even if no transition probabilities are exactly zero, unbiasedness does not exclude being on a particular side of the correct answer with very high probability. Poor mixing may cause P (h * |v) to be overestimated with high probability, which would result in an underestimate of P (v), i.e., an overly conservative estimate of test performance.</p><p>The variance of the estimator is generally unknown, as it depends on the (generally unavailable) auto-covariance structure of the Markov chain. We can note one positive property: for the ideal Markov chain operator that mixes in one step, the estimator has zero variance and gives the correct answer immediately. Although this extreme will not actually occur, it does indicate that on easy problems, good answers can be returned more quickly than by AIS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Deep Belief Networks</head><p>In this section we provide a brief overview of Deep Belief Networks (DBNs), recently introduced by <ref type="bibr" target="#b1">[2]</ref>. DBNs are probabilistic generative models, that can contain many layers of hidden variables. Each layer captures strong high-order correlations between the activities of hidden features in the layer below. The top two layers of the DBN model form a Restricted Boltzmann Machine (RBM) which is an undirected graphical model, but the lower layers form a directed generative model. The original paper introduced a greedy, layer-by-layer unsupervised learning algorithm that consists of learning a stack of RBMs one layer at a time.</p><p>Consider a DBN model with two layers of hidden features. The model's joint distribution is:</p><formula xml:id="formula_21">P (v, h 1 , h 2 ) = P (v|h 1 ) P (h 2 , h 1 ),<label>(16)</label></formula><p>where P (v|h 1 ) represents a sigmoid belief network, and P (h 1 , h 2 ) is the joint distribution defined by the second layer RBM. By explicitly summing out h 2 , we can easily evaluate an unnormalized probability P * (v,h 1 ) = ZP (v, h 1 ). Using an approximating factorial posterior distribution Q(h|v), obtained as a byproduct of the greedy learning procedure, and an AIS estimate of the model's partition function Z, <ref type="bibr" target="#b0">[1]</ref> proposed obtaining an estimate of a variational lower bound:</p><formula xml:id="formula_22">log P (v) ≥ h 1 Q(h 1 |v) log P * (v, h 1 ) -log Z + H(Q(h 1 |v)).<label>(17)</label></formula><p>The entropy term H(•) can be computed analytically, since Q is factorial, and the expectation term was estimated by a simple Monte Carlo approximation:</p><formula xml:id="formula_23">h 1 Q(h 1 |v) log P * (v, h 1 ) ≈ 1 S s=1..S log P * (v, h 1(s) ), where h 1(s) ∼ Q(h 1 |v).<label>(18)</label></formula><p>Instead of the variational approach, we could also adopt AIS to estimate h 1 P * (v, h 1 ). This would be computationally very expensive, since we would need to run AIS for each test case.</p><p>In the next section we show that variational lower bounds can be quite loose. Running AIS on the entire test set, containing many thousands of test cases, is computationally too demanding. Our proposed estimator requires the same single AIS estimate of Z as the variational method, so that we can evaluate P (v, h 1 ). It then provides better estimates of log P (v) by approximately summing over h 1 for each test case in a reasonable amount of computer time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>We present experimental results on two datasets: the MNIST digits and a dataset of image patches, extracted from images of natural scenes taken from the collection of Van Hateren (http://hlab.phys.rug.nl/imlib/). The MNIST dataset contains 60,000 training and 10,000 test images of ten handwritten digits (0 to 9), with 28×28 pixels. The image dataset consisted of 130,000 training and 20,000 test 20×20 patches. The raw image intensities were preprocessed and whitened as described in <ref type="bibr" target="#b14">[15]</ref>. Gibbs sampling was used as a Markov chain transition operator throughout. All log probabilities quoted use natural logarithms, giving values in nats.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">MNIST digits</head><p>In our first experiment we used a deep belief network (DBN) taken from <ref type="bibr" target="#b0">[1]</ref>. The network had two hidden layers with 500 and 2000 hidden units, and was greedily trained by learning a stack of two RBMs one layer at a time. Each RBM was trained using the Contrastive Divergence (CD) learning rule. The estimate of the lower bound on the average test log probability, using <ref type="bibr" target="#b16">(17)</ref>, was -86.22. AIS used a hand-tuned temperature schedule designed to equalize the variance of the intermediate log weights <ref type="bibr" target="#b6">[7]</ref>. We needed 10,000 intermediate distributions to get stable results, which took about 3.6 days on a Pentium Xeon 3.00GHz machine, whereas for our proposed estimator we only used S = 40, which took about 50 minutes. For a more direct comparison we tried giving AIS 50 minutes, which allows 100 temperatures. This run gave an estimate of -89.59, which is lower than the lower bound and tells us nothing. Giving AIS ten times more time, 1000 temperatures, gave -86.05. This is higher than the lower bound, but still worse than our estimator at S = 40, or even S = 5.</p><p>Finally, using our proposed estimator, the average test log probability on the entire MNIST test data was -84.55. The difference of about 2 nats shows that the variational bound in <ref type="bibr" target="#b0">[1]</ref> was rather tight, although a very small improvement of the DBN over the RBM is now revealed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Image Patches</head><p>In our second experiment we trained a two-layer DBN model on the image patches of natural scenes. The first layer RBM had 2000 hidden units and 400 Gaussian visible units. The second layer represented a semi-restricted Boltzmann machine (SRBM) with 500 hidden and 2000 visible units. The SRBM contained visible-to-visible connections, and was trained using Contrastive Divergence together with mean-field. Details of training can be found in <ref type="bibr" target="#b14">[15]</ref>. The overall DBN model can be viewed as a directed hierarchy of Markov random fields with hidden-to-hidden connections.</p><p>To estimate the model's partition function, we used AIS with 15,000 intermediate distributions and 100 annealing runs. The estimated lower bound on the average test log probability (see Eq. 17), using a factorial approximate posterior distribution Q(h 1 |v), which we also get as a byproduct of the greedy learning algorithm, was -583.73. The estimate of the true test log probability, using our proposed estimator, was -563.39. In contrast to the model trained on MNIST, the difference of over 20 nats shows that, for model comparison purposes, the variational lower bound is quite loose.</p><p>For comparison, we also trained square ICA and a mixture of factor analyzers (MFA) using code from <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. Square ICA achieves a test log probability of -551.14, and MFA with 50 mixture components and a 30-dimensional latent space achieves -502.30, clearly outperforming DBNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>Our new Monte Carlo procedure is formally unbiased in estimating P (v). In practice it is likely to underestimate the (log-)probability of a test set. Although the algorithm involves Markov chains, importance sampling underlies the estimator. Therefore the methods discussed in <ref type="bibr" target="#b17">[18]</ref> could be used to bound the probability of accidentally over-estimating a test set probability.</p><p>In principle our procedure is a general technique for estimating normalizing constants. It would not always be appropriate however, as it would suffer the problems outlined in <ref type="bibr" target="#b13">[14]</ref>. As an example our method will not succeed in estimating the global normalizing constant of an RBM.</p><p>For our method to work well, a state drawn from T (h (s) ← h * ) should look like it could be part of an equilibrium sequence H ∼ P(H). The details of the algorithm arose by developing existing Monte Carlo estimators, but the starting state h (s) could be drawn from any arbitrary distribution:</p><formula xml:id="formula_24">Q var (H) = 1 S S s=1</formula><p>q(h (s) ) P (h (s) |v) P(H) = P (v) 1 S</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S s=1</head><p>q(h (s) ) P (h (s) , v) P(H).</p><p>As before the reciprocal of the quantity in square brackets would give an estimate of P (v). If an approximation q(h) is available that captures more mass than T (h ← h * ), this generalized estimator could perform better. We are hopeful that our method will be a natural next step in a variety of situations where improvements are sought over a deterministic approximation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Algorithm for the proposed method. The graphical model shows Q(H|s = 3) for S = 4. At each generated state T (h * ← h (s ) ) is evaluated (step 7), roughly doubling the cost of sampling. The reverse operator, eT , was defined in section 2.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: AIS, our proposed estimator and a variational method were used to sum over the hidden states for each of 50 randomly sampled test cases to estimate their average log probability. The three methods shared the same AIS estimate of a single global normalization constant Z.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>To estimate how loose the variational bound is, we randomly sampled 50 test cases, 5 of each class, and ran AIS for each test case to estimate the true test log probability. Computationally, this is equivalent to estimating 50 additional partition functions. Figure2, left panel, shows the results. The estimate of the variational bound was -87.05 per test case, whereas the estimate of the true test log probability using AIS was -85.20. Our proposed estimator, averaged over 10 runs, provided an answer of -85.22. The special state h</figDesc><table /><note><p>* for each test example v was obtained by first sampling from the approximating distribution Q(h|v), and then performing deterministic hill-climbing in log p(v, h) to get to a local mode.</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by NSERC and CFI. Iain Murray was supported by the government of Canada. We thank Geoffrey Hinton and Radford Neal for useful discussions, Simon Osindero for providing preprocessed image patches of natural scenes, and the reviewers for useful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On the quantitative analysis of Deep Belief Networks</title>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="872" to="879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Divergence measures and message passing</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Minka</surname></persName>
		</author>
		<idno>TR-2005-173</idno>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Microsoft Research</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Approximate Bayesian inference with the weighted likelihood bootstrap</title>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Newton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><forename type="middle">E</forename><surname>Raftery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="48" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Integrating topics and syntax</title>
		<author>
			<persName><forename type="first">L</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS*17)</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Topic modeling: beyond bag-of-words</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName><surname>Wallach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="977" to="984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Annealed importance sampling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="125" to="139" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sequential Monte Carlo samplers</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Del Moral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnaud</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jasra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society B</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Marginal likelihood from the Gibbs output</title>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Chib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">432</biblScope>
			<biblScope unit="page" from="1313" to="1321" />
			<date type="published" when="1995-12">December 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Facilitating the Gibbs sampler: the Gibbs stopper and the griddy-Gibbs sampler</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">A</forename><surname>Tanner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">419</biblScope>
			<biblScope unit="page" from="861" to="868" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Marginal likelihood from the Metropolis-Hastings output</title>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Chib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Jeliazkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">453</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bridge estimation of the probability density at a point</title>
		<author>
			<persName><forename type="first">Antonietta</forename><surname>Mira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Nicholls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistica Sinica</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="603" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient Bayes factor estimation from the reversible jump output</title>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Bartolucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Scaccia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonietta</forename><surname>Mira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="52" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Erroneous results in &quot;Marginal likelihood from the Gibbs output</title>
		<author>
			<persName><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><surname>Neal</surname></persName>
		</author>
		<ptr target="http://www.cs.toronto.edu/∼radford/chib-letter.html" />
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Modeling image patches with a directed hierarchy of Markov random fields</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS*20)</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast and robust fixed-point algorithms for independent component analysis</title>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="626" to="634" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The EM algorithm for mixtures of factor analyzers</title>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>CRG-TR-96-1</idno>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Studies in lower bounding probability of evidence using the Markov inequality</title>
		<author>
			<persName><forename type="first">Bozhena</forename><surname>Vibhav Gogate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rina</forename><surname>Bidyuk</surname></persName>
		</author>
		<author>
			<persName><surname>Dechter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">23rd Conference on Uncertainty in Artificial Intelligence (UAI)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Real-valued latents and Metropolis-Hastings There are technical difficulties with the original Chib-style approach applied to Metropolis-Hastings and continuous latent variables</title>
	</analytic>
	<monogr>
		<title level="m">The continuous version of equation (9), P (h * |v) = T (h * ← h)P (h|v) dh ≈ 1</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">* ← h (s) ), h (s) ∼ P(H), (20) doesn&apos;t work if T is the Metropolis-Hastings operator. The Dirac-delta function at h = h * contains a significant part of the integral</title>
		<author>
			<persName><surname>S S S=1</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>which is ignored by samples from P (h|v) with probability one</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Following [11], the fix is to instead integrate over the generalized detailed balance relationship (5)</title>
	</analytic>
	<monogr>
		<title level="m">We do the same: P (h * |v) = dh T (h * ← h)P (h|v) dh T (h ← h * )</title>
		<imprint/>
	</monogr>
	<note>Chib and Jeliazkov implicitly took out the h * = h point from all of their integrals</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m">T contains a delta function. For Metropolis-Hastings: T (h ← h * ) = q(h; h * ) min 1, a(h; h * ) , where a(h; h * ) is an easy-to-compute acceptance ratio. Sampling from q(h; h * ) and averaging min</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The algorithm in figure 1 still applies if the T &apos;s are interpreted as probability density functions. If, due to a rejection, h * is drawn in step 2. then the sum in step 7. will contain an infinite term giving a trivial underestimate P (v) = 0. (Steps 3-6 need not be performed in this case.) On repeated runs, the average estimate is still unbiased, or an underestimate for chains that can</title>
		<author>
			<persName><surname>Quantity</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>t mix. Alternatively, the variational approach (19) could be applied together with Metropolis-Hastings sampling</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
