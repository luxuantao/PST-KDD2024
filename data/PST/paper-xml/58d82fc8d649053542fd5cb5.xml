<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dual Attention Networks for Multimodal Reasoning and Matching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hyeonseob</forename><surname>Nam</surname></persName>
							<email>hyeonseob.nam@navercorp.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Search Solutions Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
							<email>jungwoo.ha@navercorp.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Search Solutions Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Naver</forename><surname>Corp</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Search Solutions Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Dual Attention Networks for Multimodal Reasoning and Matching</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>We propose Dual Attention Networks (DANs) which jointly leverage visual and textual attention mechanisms to capture fine-grained interplay between vision and language. DANs attend to specific regions in images and words in text through multiple steps and gather essential information from both modalities. Based on this framework, we introduce two types of DANs for multimodal reasoning and matching, respectively. The reasoning model allows visual and textual attentions to steer each other during collaborative inference, which is useful for tasks such as Visual Question Answering (VQA). In addition, the matching model exploits the two attention mechanisms to estimate the similarity between images and sentences by focusing on their shared semantics. Our extensive experiments validate the effectiveness of DANs in combining vision and language,</head><p>achieving the state-of-the-art performance on public benchmarks for VQA and image-text matching.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Vision and language are two central parts of human intelligence to understand the real world. They are also fundamental components in achieving artificial intelligence, and a tremendous amount of research has been done for decades in each area. Recently, dramatic advances in deep learning have broken the boundaries between vision and language, drawing growing interest in their intersection, such as visual question answering (VQA) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b34">35]</ref>, image captioning <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b1">2]</ref>, image-text matching <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30]</ref>, visual grounding <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b8">9]</ref>, etc.</p><p>One of the recent advances in neural networks is the attention mechanism <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b32">33]</ref>. It aims to focus on certain aspects of data sequentially and aggregate essential information over time to infer the results, and has been successfully applied to both areas of vision and language. In computer vision, attention based methods adaptively select a sequence of image regions to extract necessary features <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b32">33]</ref>. Similarly, attention models for natural language processing highlight specific words or sentences to distill information from input text <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b14">15]</ref>. These approaches have improved the performance of wide applications in conjunction with deep architectures including convolutional neural networks (CNNs) and recurrent neural networks (RNNs).</p><p>Despite the effectiveness of attention in handling both visual and textual data, it has been hardly attempted to establish a connection between visual and textual attention models which can be highly beneficial in various scenarios. For example, the VQA problem in Figure <ref type="figure" target="#fig_0">1a</ref> with the question What color is the umbrella? can be efficiently solved by simultaneously focusing on the region of umbrella and the word color. In the example of image-text matching in Figure <ref type="figure" target="#fig_0">1b</ref>, the similarity between the image and sentence can be effectively measured by at-tending to the specific regions and words sharing common semantics such as girl and pool.</p><p>In this paper, we propose Dual Attention Networks (DANs) which jointly learn visual and textual attention models to explore the fine-grained interaction between vision and language. We investigate two variants of DANs illustrated in Figure <ref type="figure" target="#fig_0">1</ref>, referred to as reasoning-DAN (r-DAN) and matching-DAN (m-DAN), respectively. The r-DAN collaboratively performs visual and textual attentions using a joint memory which assembles the previous attention results and guides the next attentions. It is suited to the tasks requiring multimodal reasoning such as VQA. On the other hand, the m-DAN separates visual and textual attention models with distinct memories but jointly trains them to capture the shared semantics between images and sentences. This approach eventually finds a joint embedding space which facilitates efficient cross-modal matching and retrieval. Both proposed algorithms closely connect visual and textual attention mechanisms into a unified framework, achieving outstanding performance in VQA and image-text matching problems.</p><p>To summarize, the main contributions of our work are as follows:</p><p>• We propose an integrated framework of visual and textual attentions, where critical regions and words are jointly located through multiple steps.</p><p>• Two variants of the proposed framework are implemented for multimodal reasoning and matching, and applied to VQA and image-text matching.</p><p>• Detailed visualization of the attention results validates that our models effectively focus on vital portions of visual and textual data for the given task.</p><p>• Our framework demonstrates the state-of-the-art performance on the VQA dataset <ref type="bibr" target="#b2">[3]</ref> and the Flickr30K image-text matching dataset <ref type="bibr" target="#b35">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Attention Mechanisms</head><p>Attention mechanisms allow models to focus on necessary parts of visual or textual inputs at each step of a task. Visual attention models selectively pay attention to small regions in an image to extract core features as well as reduce the amount of information to process. A number of methods have recently adopted visual attention to benefit image classification <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b27">28]</ref>, image generation <ref type="bibr" target="#b5">[6]</ref>, image captioning <ref type="bibr" target="#b32">[33]</ref>, visual question answering <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32]</ref>, etc. On the other hand, textual attention mechanisms generally aim to find semantic or syntactic input-output alignments under an encoder-decoder framework, which is especially effective in handling long-term dependency. This approach has been successfully applied to various tasks including machine translation <ref type="bibr" target="#b3">[4]</ref>, text generation <ref type="bibr" target="#b15">[16]</ref>, sentence summarization <ref type="bibr" target="#b24">[25]</ref>, and question answering <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b31">32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Visual Question Answering (VQA)</head><p>VQA is a task of answering a question in natural language regarding a given image, which requires multimodal reasoning over visual and textual data. It has received a surge of interest since Antol et al. <ref type="bibr" target="#b2">[3]</ref> presented a largescale dataset with free-form and open-ended questions. A simple baseline by Zhou et al. <ref type="bibr" target="#b36">[37]</ref> predicts the answer from a concatenation of CNN image features and bag-of-word question features. Several methods adaptively construct a deep architecture depending on the given question. For example, Noh et al. <ref type="bibr" target="#b22">[23]</ref> impose a dynamic parameter layer on a CNN which is learned by the question, while Andreas et al. <ref type="bibr" target="#b0">[1]</ref> utilize a compositional structure of the question to assemble a collection of neural modules.</p><p>One limitation of the above approaches is that they resort to a global image representation which contains noisy or unnecessary information. To address this problem, Yang et al. <ref type="bibr" target="#b34">[35]</ref> propose stacked attention networks which perform multi-step visual attention, and Shih et al. <ref type="bibr" target="#b25">[26]</ref> use object proposals to identify regions relevant to the given question. Recently, dynamic memory networks <ref type="bibr" target="#b31">[32]</ref> integrate an attention mechanism with a memory module, and multimodal compact bilinear pooling <ref type="bibr" target="#b4">[5]</ref> is exploited to expressively combine multimodal features and predict attention over the image. These methods commonly employ visual attention to find critical regions, but textual attention has been rarely incorporated into VQA. Although HieCoAtt <ref type="bibr" target="#b17">[18]</ref> applies both visual and textual attentions, it independently performs each step of co-attention without reasoning over previous co-attention outputs. On the contrary, our method moves and refines both attentions via multiple reasoning steps based on the memory of previous attentions, which facilitates close interplay between visual and textual data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Image-Text Matching</head><p>The core issue with image-text matching is measuring the semantic similarity between visual and textual inputs. It is commonly addressed by learning a joint space where image and sentence feature vectors are directly comparable. Hodosh et al. <ref type="bibr" target="#b7">[8]</ref> apply canonical correlation analysis (CCA) to find embeddings that maximize the correlation between images and sentences, which is further improved by incorporating deep neural networks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b33">34]</ref>. A recent approach by Wang et al. <ref type="bibr" target="#b29">[30]</ref> includes structure-preserving constraints within a bidirectional loss function to make the joint space more discriminative. In contrast, Ma et al. <ref type="bibr" target="#b18">[19]</ref> construct a CNN to combine an image and sentence fragments into a joint representation, from which the matching score is directly inferred. Image captioning frameworks are also exploited to estimate the similarity based on the inverse probability of sentences given a query image <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>To the best of our knowledge, no study has attempted to learn multimodal attention models for image-text matching. Even though Karpathy et al. <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b9">10]</ref> propose to find the alignments between image regions and sentence fragments, they explicitly compute all pairwise distances between them and estimate the average or best alignment score, which leads to inefficiency. On the other hand, our method automatically attends to the shared concepts between images and sentences while embedding them into a joint space, where cross-modal similarity is directly obtained by a single inner product operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dual Attention Networks (DANs)</head><p>We present two structures of DANs to consolidate visual and textual attention mechanisms: r-DAN for multimodal reasoning and m-DAN for multimodal matching. They share a common framework but differ in their ways of associating visual and textual attentions. We first describe the common framework including input representation (Section 3.1) and attention mechanisms (Section 3.2). Then we illustrate the details of r-DAN (Section 3.3) and m-DAN (Section 3.4) applied to VQA and image-text matching, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Input Representation</head><p>Image representation The image features are extracted from 19-layer VGGNet <ref type="bibr" target="#b26">[27]</ref> or 152-layer ResNet <ref type="bibr" target="#b6">[7]</ref>. We first rescale images to 448×448 and feed them into the CNNs. In order to obtain feature vectors for different regions, we take the last pooling layer of VGGNet (pool5) or the layer beneath the last pooling layer of ResNet (res5c). Finally the input image is represented by</p><formula xml:id="formula_0">{v 1 , • • • , v N },</formula><p>where N is the number of image regions and v n is a 512 (VGGNet) or 2048 (ResNet) dimensional feature vector corresponding to the n-th region.</p><p>Text representation We employ bidirectional LSTMs to generate text features as depicted in Figure <ref type="figure" target="#fig_2">2</ref>. Given one-hot encoding of T input words {w 1 , • • • , w T }, we first embed the words into a vector space by x t = Mw t , where M is an embedding matrix. Then we feed the vectors into the bidirectional LSTMs:</p><formula xml:id="formula_1">h (f ) t = LSTM (f ) (x t , h (f ) t−1 ),<label>(1)</label></formula><formula xml:id="formula_2">h (b) t = LSTM (b) (x t , h (b) t+1 ),<label>(2)</label></formula><p>where h  </p><formula xml:id="formula_3">u t = h (f ) t + h (b)</formula><p>t , we construct a set of feature vectors {u 1 , • • • , u T } where u t encodes the semantics of the t-th word in the context of the entire sentence. Note that the models discussed here including the word embedding matrix and the LSTMs are trained end-to-end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Attention Mechanisms</head><p>Our method performs visual and textual attentions simultaneously through multiple steps and gathers necessary information from both modalities. In this section, we explain the underlying attention mechanisms employed at each step, which serve as building blocks to compose the entire DANs. For simplicity, we shall omit the bias term b in the following equations.</p><p>Visual Attention. Visual attention aims to generate a context vector by attending to certain parts of the input image. At step k, the visual context vector v (k) is given by</p><formula xml:id="formula_4">v (k) = V Att({v n } N n=1 , m (k−1) v ),<label>(3)</label></formula><p>where m</p><formula xml:id="formula_5">(k−1) v</formula><p>is a memory vector encoding the information that has been attended until step k − 1. Specifically, we employ the soft attention mechanism where the context vector is obtained from a weighted average of input feature vectors. The attention weights {α</p><formula xml:id="formula_6">(k) v,n } N</formula><p>n=1 are computed by a 2-layer feed-forward neural network (FNN) and the softmax function:</p><formula xml:id="formula_7">h (k) v,n = tanh W (k) v v n ⊙ tanh W (k) v,m m (k−1) v ,<label>(4)</label></formula><formula xml:id="formula_8">α (k) v,n = softmax W (k) v,h h (k) v,n ,<label>(5)</label></formula><formula xml:id="formula_9">v (k) = tanh P (k) N n=1 α (k) v,n v n ,<label>(6)</label></formula><p>where</p><formula xml:id="formula_10">W (k) v , W<label>(k)</label></formula><p>v,m , and</p><formula xml:id="formula_11">W (k) v,h are the network parame- ters, h (k)</formula><p>v,n is a hidden state, and ⊙ is element-wise multiplication. In Equation <ref type="formula" target="#formula_9">6</ref>, we introduce an additional layer with the weight matrix P (k) in order to embed visual context vectors into a compatible space with textual context vectors, as we use pretrained image features v n .</p><p>Textual Attention. Textual attention computes a textual context vector u (k) by focusing on specific words in the input sentence every step:</p><formula xml:id="formula_12">u (k) = T Att({u t } T t=1 , m (k−1) u ),<label>(7)</label></formula><p>where m</p><formula xml:id="formula_13">(k−1) u</formula><p>is a memory vector. The textual attention mechanism is almost identical to the visual attention mechanism. In other words, the attention weights {α </p><formula xml:id="formula_14">h (k) u,t = tanh W (k) u u t ⊙ tanh W (k) u,m m (k−1) u ,<label>(8)</label></formula><formula xml:id="formula_15">α (k) u,t = softmax W (k) u,h h (k) u,t ,<label>(9)</label></formula><formula xml:id="formula_16">u (k) = t α (k) u,t u t . (<label>10</label></formula><formula xml:id="formula_17">)</formula><p>where</p><formula xml:id="formula_18">W (k) u , W<label>(k)</label></formula><p>u,m , and W</p><p>u,h are the network parameters, h (k) u,t is a hidden state. Unlike the visual attention, it does not need an additional layer after the last weighted averaging because the text features u t are already trained end-to-end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">r-DAN for Visual Question Answering</head><p>VQA is a representative problem which requires joint reasoning over multimodal data. For this purpose, the r-DAN maintains a joint memory vector m (k) which accumulates the visual and textual information that has been attended until step k. It is recursively updated by</p><formula xml:id="formula_20">m (k) = m (k−1) + v (k) ⊙ u (k) ,<label>(11)</label></formula><p>where v (k) and u (k) are the visual and textual context vectors obtained from Equation 6 and 10, respectively. This joint representation concurrently guides the visual and textual attentions, i.e.</p><formula xml:id="formula_21">m (k) = m (k) v = m (k)</formula><p>u , which allows the two attention mechanisms to closely cooperate with each other. The initial memory vector m (0) is defined based on global context vectors v (0) and u (0) as</p><formula xml:id="formula_22">m (0) = v (0) ⊙ u (0) ,<label>(12)</label></formula><p>where v (0) = tanh</p><formula xml:id="formula_23">P (0) 1 N n v n ,<label>(13)</label></formula><formula xml:id="formula_24">u (0) = 1 T t u t .<label>(14)</label></formula><p>By repeating the dual attention (Equation 3 and 7) and memory update (Equation <ref type="formula" target="#formula_20">11</ref>) for K steps, we effectively The final answer is predicted by multi-way classification to the top C frequent answers. We employ a single-layer softmax classifier with cross-entropy loss where the input is the final memory m (K) :</p><formula xml:id="formula_25">p ans = softmax W ans m (K) ,<label>(15)</label></formula><p>where p ans represents the probability over the candidate answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">m-DAN for Image-Text Matching</head><p>Image-text matching tasks usually involve comparison between numerous images and sentences, where effective and efficient computation of cross-modal similarities is crucial. To achieve this, we aim to learn a joint embedding space which satisfies the following properties. First, the embedding space encodes the shared concepts that frequently co-occur in image and sentence domains. Moreover, images and sentences are autonomously embedded into the joint space without being paired, so that arbitrary image and sentence vectors in the space are directly comparable.</p><p>Our m-DAN jointly learns visual and textual attention models to capture the shared concepts between the two modalities, but separates them at inference time to provide generally comparable representations in the embedding space. Contrary to the r-DAN which uses a joint memory, the m-DAN maintains separate memory vectors for visual and textual attentions as follows: which are initialized to v (0) and u (0) defined in Equation <ref type="formula" target="#formula_23">13</ref>and 14, respectively. At each step, we compute the similarity s (k) between visual and textual context vectors by their inner product:</p><formula xml:id="formula_26">m (k) v = m (k−1) v + v (k) ,<label>(16)</label></formula><formula xml:id="formula_27">m (k) u = m (k−1) u + u (k) ,<label>(17)</label></formula><formula xml:id="formula_28">s (k) = v (k) • u (k) .<label>(18)</label></formula><p>After performing K steps of the dual attention and memory update, the final similarity S between the given image and sentence becomes</p><formula xml:id="formula_29">S = K k=0 s (k) .<label>(19)</label></formula><p>The overall architecture of this model when K = 2 is depicted in Figure <ref type="figure" target="#fig_5">4</ref>. This network is trained with bidirectional max-margin ranking loss, which is widely adopted for multimodal similarity learning <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b29">30]</ref>. For each correct pair of an image and a sentence (v, u), we additionally sample a negative image v − and a negative sentence u − to construct two negative pairs (v − , u) and (v, u − ). Then, the loss function becomes:</p><formula xml:id="formula_30">L = (v,u) max 0, m − S(v, u) + S(v − , u) + max 0, m − S(v, u) + S(v, u − ) , (<label>20</label></formula><formula xml:id="formula_31">)</formula><p>where m is a margin constraint. By minimizing this function, the network is trained to focus on the common semantics that only appears in correct image-sentence pairs through visual and textual attention mechanisms. At inference time, an arbitrary image or sentence is embedded into the joint space by concatenating its context vectors:</p><formula xml:id="formula_32">z v = [v (0) ; • • • ; v (K) ],<label>(21)</label></formula><formula xml:id="formula_33">z u = [u (0) ; • • • ; u (K) ],<label>(22)</label></formula><p>where z v and z u are the representations for image v and sentence u, respectively. Note that these vectors are obtained via separate pipelines of visual and textual attentions, i.e. learned shared concepts are revealed from an image or sentence itself, not from an image-sentence pair. The similarity between two vectors in the joint space is simply computed by their inner product, e.g. S(v, u) = z v • z u , which is equivalent to the output of the network in Equation <ref type="formula" target="#formula_29">19</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments 4.1. Experimental Setup</head><p>We fix all the hyper-parameters applied to both r-DAN and m-DAN. The number of attention steps K is set to 2 which empirically shows the best performance. The dimension of every hidden layer-including word embedding, LSTMs, and attention models-is set to 512. We train our networks by stochastic gradient descent with a learning rate 0.1, momentum 0.9, weight decay 0.0005, dropout ratio 0.5, and gradient clipping at 0.1. The network is trained for 60 epochs, where the learning rate is dropped to 0.01 after 30 epochs. A minibatch for r-DAN and m-DAN consists of 128 pairs of image, question and 128 quadruplets of positive image, positive sentence, negative image, negative sentence , respectively. The number of possible answers C for VQA is set to 2000, and the margin m for the loss function in Equation 20 is set to 100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation on Visual Question Answering</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Dataset and Evaluation Metric</head><p>We evaluate the r-DAN on the Visual Question Answering (VQA) dataset <ref type="bibr" target="#b2">[3]</ref>, which contains approximately 200K real images from MSCOCO dataset <ref type="bibr" target="#b16">[17]</ref>. Each image is associated with three questions, and each question is labeled with ten answers by human annotators. The dataset is typically divided into four splits: train (80K images), val (40K images), test-dev (20K images), and test-std (20K images). We train our model using train and val, validate with test-dev, and evaluate on test-std. There are two forms of tasks, openended and multiple-choice, which require to answer each question without and with a set of candidate answers, respectively. For both tasks, we follow the evaluation metric used in <ref type="bibr" target="#b2">[3]</ref> as</p><formula xml:id="formula_34">Acc(â) = min #humans that labeled â 3 , 1 ,<label>(23)</label></formula><p>where â is a predicted answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Results and Analysis</head><p>The performance of r-DAN compared with state-of-the-art VQA systems is presented in Table <ref type="table" target="#tab_0">1</ref>, where our method  achieves the best performance in both open-ended and multiple-choice tasks. For fair evaluation, we compare single-model accuracies obtained without data augmentation, even though <ref type="bibr" target="#b4">[5]</ref> reported better performance using model ensembles and additional training data. Figure <ref type="figure" target="#fig_6">5</ref> describes the qualitative results from our approach with visualization of the attention weights. Our method produces correct answers to challenging problems which require fine-grained reasoning, as well as successfully attends to the specific regions and words which facilitate answering the questions. Specifically, the first and fourth examples in Figure <ref type="figure" target="#fig_6">5</ref> illustrate that the r-DAN moves its visual attention to the proper regions indicated by the attended words, while the second and third examples show that it moves its textual attention to divide a complex task into sequential subtasksfinding target objects and extracting certain attributes. (+) A woman in a brown vest is working on the computer.</p><p>(+) A woman in a brown vest is working on the computer.</p><p>(+) A man in a white shirt stands high up on scaffolding.</p><p>(+) A man in a white shirt stands high up on scaffolding.</p><p>(+) A man in a white shirt stands high up on scaffolding. (+) A woman in a red vest working at a computer.</p><p>(+) A woman in a red vest working at a computer.</p><p>(+) A woman in a red vest working at a computer.</p><p>(+) Man works on top of scaffolding.</p><p>(+) Man works on top of scaffolding.</p><p>(+) Man works on top of scaffolding.</p><p>(+) Two boys playing together at a playground.</p><p>(+) Two boys playing together at a playground.</p><p>(+) Two boys playing together at a playground.</p><p>(-) A man wearing a red t shirt sweeps the sidewalk in front of a brick building.</p><p>(-) A man wearing a red t shirt sweeps the sidewalk in front of a brick building.</p><p>(-) A man wearing a red t shirt sweeps the sidewalk in front of a brick building. (-) The two kids are playing at the playground.</p><p>(-) The two kids are playing at the playground.</p><p>(-) The two kids are playing at the playground.</p><p>(+) Boy in red shirt and black shorts sweeps driveway.</p><p>(+) Boy in red shirt and black shorts sweeps driveway.</p><p>(+) Boy in red shirt and black shorts sweeps driveway.  We employ the Flickr30K dataset <ref type="bibr" target="#b35">[36]</ref> to evaluate the m-DAN for multimodal matching. It consists of 31,783 real images with five descriptive sentences for each, and we follow the public splits by <ref type="bibr" target="#b19">[20]</ref>: 29,783 training, 1,000 valida-tion and 1,000 test images. We report the performance of m-DAN in bidirectional image and sentence retrieval using the same metrics as previous work <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30]</ref>. Recall@K (K=1, 5, 10) represents the percentage of the queries where at least one ground-truth is retrieved among the top K results and MR measures the median rank of the top-ranked ground-truth.</p><p>A woman in a cap at a coffee shop.</p><p>A woman in a cap at a coffee shop.</p><p>A woman in a cap at a coffee shop.</p><p>A boy is hanging out of the window of a yellow taxi.</p><p>A boy is hanging out of the window of a yellow taxi.</p><p>A boy is hanging out of the window of a yellow taxi.</p><p>A woman in a striped outfit on a bike.</p><p>A woman in a striped outfit on a bike.</p><p>A woman in a striped outfit on a bike.</p><p>A group of people standing on a sidewalk under some trees.</p><p>A group of people standing on a sidewalk under some trees.</p><p>A group of people standing on a sidewalk under some trees. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Results and Analysis</head><p>Table <ref type="table" target="#tab_1">2</ref> presents the quantitative results on the Flickr30K dataset, where the proposed method outperforms other recent approaches in all measures. The qualitative results from image-to-text and text-to-image retrieval are also illustrated in Figure <ref type="figure" target="#fig_7">6</ref> and Figure <ref type="figure" target="#fig_9">7</ref>, respectively, with visualization of attention outputs. At each step of attention, the m-DAN effectively discovers the essential semantics appearing in both modalities. It tends to capture the main subjects (e.g. woman, boy, people, etc.) at the first step, and figure out relevant objects, backgrounds or actions (e.g. computer, scaffolding, sweeps, etc.) at the second step. Note that this property solely comes from the training stage where visual and textual attention models are jointly learned, while images and sentences are processed independently at inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose Dual Attention Networks (DANs) to bridge visual and textual attention mechanisms. We present two architectures of DANs for multimodal reasoning and matching. The first model infers the answers collaboratively from images and sentences, while the other one embeds them into a common space by capturing their shared semantics. These models demonstrate the state-of-the-art performance in VQA and image-text matching, showing their effectiveness in extracting essential information via the dual attention mechanism. The proposed framework can be potentially generalized to various tasks at the intersection of vision and language, such as image captioning, visual grounding, video question answering, etc.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of Dual Attention Networks (DANs) for multimodal reasoning and matching. The brightness of image regions and darkness of words indicate their attention weights predicted by DANs.</figDesc><graphic url="image-2.png" coords="1,308.86,339.03,252.80,100.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(f ) t and h (b) t represent the hidden states at time t from the forward and backward LSTMs, respectively. By adding the two hidden states at each time step, i.e.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Bidirectional LSTMs for text encoding.</figDesc><graphic url="image-3.png" coords="3,308.86,72.00,236.25,149.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>} T t=1 are obtained from a 2-layer FNN and the context vector u (k) is calculated by weighted averaging:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: r-DAN in case of K = 2.</figDesc><graphic url="image-4.png" coords="4,308.86,73.00,248.06,165.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: m-DAN in case of K = 2.</figDesc><graphic url="image-5.png" coords="5,50.11,73.00,248.05,165.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Qualitative results on the VQA dataset with attention visualization. For each example, the query image, question, and the answer by DAN are presented from top to bottom; the original image (question), the first and second attention maps are shown from left to right. The brightness of images and darkness of words represent their attention weights.</figDesc><graphic url="image-12.png" coords="6,50.11,440.19,82.17,70.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Qualitative results from image-to-text retrieval with attention visualization. For each example, the query image and the top two retrieved sentences are shown from top to bottom; the original image (sentence), the first and second attention maps are shown from left to right. (+) and (-) indicate ground-truth and non ground-truth sentences, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>4. 3 .</head><label>3</label><figDesc>Evaluation on Image-Text Matching 4.3.1 Dataset and Evaluation Metric</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Qualitative results from text-to-image retrieval with attention visualization. For each example, the query sentence and the top two retrieved images are shown from top to bottom; the original sentence (image), the first and second attention maps are shown from left to right. Green and red boxes indicate ground-truth and non ground-truth images, respectively.</figDesc><graphic url="image-48.png" coords="8,50.11,383.25,82.17,70.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results on the VQA dataset compared with state-of-the-art methods.</figDesc><table><row><cell></cell><cell></cell><cell>Test-dev</cell><cell></cell><cell></cell><cell cols="3">Test-standard</cell><cell></cell></row><row><cell></cell><cell cols="2">Open-Ended</cell><cell>MC</cell><cell></cell><cell cols="2">Open-Ended</cell><cell></cell><cell>MC</cell></row><row><cell>Method</cell><cell cols="2">Y/N Num Other All</cell><cell cols="5">All Y/N Num Other All</cell><cell>All</cell></row><row><cell>iBOWIMG [37]</cell><cell>76.5 35.0</cell><cell cols="4">42.6 55.7 61.7 76.8 35.0</cell><cell cols="3">42.6 55.9 62.0</cell></row><row><cell>DPPnet [23]</cell><cell>80.7 37.2</cell><cell cols="4">41.7 57.2 62.5 80.3 36.9</cell><cell cols="3">42.2 57.4 62.7</cell></row><row><cell>VQA team [3]</cell><cell>80.5 36.8</cell><cell cols="4">43.1 57.8 62.7 80.6 36.5</cell><cell cols="3">43.7 58.2 63.1</cell></row><row><cell>SAN [35]</cell><cell>79.3 36.6</cell><cell>46.1 58.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>58.9</cell><cell>-</cell></row><row><cell>NMN [1]</cell><cell>81.2 38.0</cell><cell>44.0 58.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>58.7</cell><cell>-</cell></row><row><cell>ACK [31]</cell><cell>81.0 38.4</cell><cell>45.2 59.2</cell><cell>-</cell><cell cols="2">81.1 37.1</cell><cell cols="2">45.8 59.4</cell><cell>-</cell></row><row><cell>DMN+ [32]</cell><cell>80.5 36.8</cell><cell>48.3 60.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>60.4</cell><cell>-</cell></row><row><cell>MRN (ResNet) [12]</cell><cell>82.3 38.8</cell><cell cols="4">49.3 61.7 66.2 82.4 38.2</cell><cell cols="3">49.4 61.8 66.3</cell></row><row><cell cols="2">HieCoAtt (ResNet) [18] 79.7 38.7</cell><cell cols="2">51.7 61.8 65.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">62.1 66.1</cell></row><row><cell>RAU (ResNet) [22]</cell><cell>81.9 39.0</cell><cell cols="4">53.0 63.3 67.7 81.7 38.2</cell><cell cols="3">52.8 63.2 67.3</cell></row><row><cell>MCB (ResNet) [5]</cell><cell>82.2 37.7</cell><cell cols="2">54.8 64.2 68.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DAN (VGG)</cell><cell>82.1 38.2</cell><cell cols="2">50.2 62.0 67.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DAN (ResNet)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Bidirectional retrieval results on the Flickr30K dataset compared with state-of-the-art methods.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Image-to-Text</cell><cell></cell><cell></cell><cell cols="2">Text-to-Image</cell><cell></cell></row><row><cell>Method</cell><cell cols="8">R@1 R@5 R@10 MR R@1 R@5 R@10 MR</cell></row><row><cell>DCCA [34]</cell><cell>27.9</cell><cell>56.9</cell><cell>68.2</cell><cell>4</cell><cell>26.8</cell><cell>52.9</cell><cell>66.9</cell><cell>4</cell></row><row><cell>mCNN [19]</cell><cell>33.6</cell><cell>64.1</cell><cell>74.9</cell><cell>3</cell><cell>26.2</cell><cell>56.3</cell><cell>69.6</cell><cell>4</cell></row><row><cell>m-RNN-VGG [20]</cell><cell>35.4</cell><cell>63.8</cell><cell>73.7</cell><cell>3</cell><cell>22.8</cell><cell>50.7</cell><cell>63.1</cell><cell>5</cell></row><row><cell cols="2">GMM+HGLMM FV [14] 35.0</cell><cell>62.0</cell><cell>73.8</cell><cell>3</cell><cell>25.0</cell><cell>52.7</cell><cell>66.0</cell><cell>5</cell></row><row><cell>HGLMM FV [24]</cell><cell>36.5</cell><cell>62.2</cell><cell>73.3</cell><cell>-</cell><cell>24.7</cell><cell>53.4</cell><cell>66.8</cell><cell>-</cell></row><row><cell>SPE [30]</cell><cell>40.3</cell><cell>68.9</cell><cell>79.9</cell><cell>-</cell><cell>29.7</cell><cell>60.1</cell><cell>72.1</cell><cell>-</cell></row><row><cell>DAN (VGG)</cell><cell>41.4</cell><cell>73.5</cell><cell>82.5</cell><cell>2</cell><cell>31.8</cell><cell>61.7</cell><cell>72.5</cell><cell>3</cell></row><row><cell>DAN (ResNet)</cell><cell>55.0</cell><cell>81.8</cell><cell>89.0</cell><cell>1</cell><cell>39.4</cell><cell>69.2</cell><cell>79.1</cell><cell>2</cell></row><row><cell>(+) A woman in a</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>brown vest is</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>working on the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>computer.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural module networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep compositional captioning: Describing novel object categories without paired training data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2006">2015. 1, 2, 5, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01847</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">DRAW: A recurrent neural network for image generation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Framing image description as a ranking task: Data, models and evaluation metrics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>JAIR</publisher>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="853" to="899" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Darrell. Natural language object retrieval</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2001">June 2016. 1</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep fragment embeddings for bidirectional image sentence mapping</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">F F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-H</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-O</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-T</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01455</idno>
		<title level="m">Multimodal residual learning for visual qa</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Associating neural word embeddings with deep image representations using fisher vectors</title>
		<author>
			<persName><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ask me anything: Dynamic memory networks for natural language processing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A hierarchical neural autoencoder for paragraphs and documents</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00061</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multimodal convolutional neural networks for matching image and sentence</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep captioning with multimodal recurrent neural networks (m-rnn)</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Training recurrent answering units with joint loss minimization for vqa</title>
		<author>
			<persName><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03647</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Image question answering using convolutional neural network with dynamic parameter prediction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Hongsuck</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Where to look: Focus regions for visual question answering</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep networks with internal selective attention through feedback connections</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Stollenga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning deep structurepreserving image-text embeddings</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2007">2016. 1, 2, 5, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">van den Hengel. Ask me anything: Free-form visual question answering based on knowledge from external sources</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dynamic memory networks for visual and textual question answering</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep correlation for matching images and text</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.02167</idno>
		<title level="m">Simple baseline for visual question answering</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
