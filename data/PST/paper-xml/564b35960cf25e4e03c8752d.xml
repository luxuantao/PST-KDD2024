<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
							<email>zhangdingwen2006yyy@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Junwei</forename><surname>Han</surname></persName>
							<email>junweihan2010@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Jungong</forename><surname>Han</surname></persName>
							<email>jungong.han@northumbria.ac.uk</email>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Ling</forename><surname>Shao</surname></persName>
							<email>ling.shao@ieee.org</email>
						</author>
						<author>
							<persName><forename type="first">Junwei</forename><forename type="middle">Han</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">J</forename><surname>Han</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Cosaliency Detection Based on Intrasaliency Prior Transfer and Deep Intersaliency Mining</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Digital Technologies</orgName>
								<orgName type="institution">Northumbria University</orgName>
								<address>
									<postCode>NE1 8ST</postCode>
									<settlement>Newcastle upon Tyne</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science and Digital Technologies</orgName>
								<orgName type="institution">Northumbria University</orgName>
								<address>
									<postCode>NE1 8ST</postCode>
									<settlement>Newcastle upon Tyne</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">College of Electronic and Information Engineering</orgName>
								<orgName type="institution">Nanjing University of Information Science and Technology</orgName>
								<address>
									<postCode>210044</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8A67F647BF3DA3FF6934E8D6C29FC161</idno>
					<idno type="DOI">10.1109/TNNLS.2015.2495161</idno>
					<note type="submission">received November 10, 2014; revised October 10, 2015; accepted October 22, 2015.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Cosaliency detection</term>
					<term>deep learning</term>
					<term>prior transfer</term>
					<term>stacked denoising autoencoder (SDAE)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As an interesting and emerging topic, cosaliency detection aims at simultaneously extracting common salient objects in multiple related images. It differs from the conventional saliency detection paradigm in which saliency detection for each image is determined one by one independently without taking advantage of the homogeneity in the data pool of multiple related images. In this paper, we propose a novel cosaliency detection approach using deep learning models. Two new concepts, called intrasaliency prior transfer and deep intersaliency mining, are introduced and explored in the proposed work. For the intrasaliency prior transfer, we build a stacked denoising autoencoder (SDAE) to learn the saliency prior knowledge from auxiliary annotated data sets and then transfer the learned knowledge to estimate the intrasaliency for each image in cosaliency data sets. For the deep intersaliency mining, we formulate it by using the deep reconstruction residual obtained in the highest hidden layer of a self-trained SDAE. The obtained deep intersaliency can extract more intrinsic and general hidden patterns to discover the homogeneity of cosalient objects in terms of some higher level concepts. Finally, the cosaliency maps are generated by weighted integration of the proposed intrasaliency prior, deep intersaliency, and traditional shallow intersaliency. Comprehensive experiments over diverse publicly available benchmark data sets demonstrate consistent performance gains of the proposed method over the state-of-the-art cosaliency detection methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fig. <ref type="figure">1</ref>. Illustration of the difference between conventional saliency detection and cosaliency detection. First row: input images. Second row: saliency detection results obtained by region-based contrast <ref type="bibr" target="#b2">[3]</ref>. Third row: cosaliency detection results obtained by the proposed approach. people love taking photographs, and there is a rich collection of related pictures sharing the common foreground regions of the same object or event <ref type="bibr" target="#b3">[4]</ref>. When detecting these cosalient foregrounds, the direct use of conventional saliency detection methods that process each of these images individually may lead to unsatisfactory performance (see the second row of Fig. <ref type="figure">1</ref>). This, thus, triggers a new and interesting research area named cosaliency detection with the goal of discovering the consistent salient patterns in multiple related images and, finally, extracting the common salient foreground regions in the image group (see the third row of Fig. <ref type="figure">1</ref>). Different from cosegmentation <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref> that considers not only common salient foreground regions but also similar nonsalient background areas in images, cosaliency detection focuses on exploring the most important information, i.e., the common foreground regions, among the image group with a reduced computational demand by implying priorities based on human visual attention. Cosaliency detection can serve as a more promising preprocessing step for many high-level visual information understanding tasks, such as video foreground extraction <ref type="bibr" target="#b6">[7]</ref>, image retrieval <ref type="bibr" target="#b7">[8]</ref>, object detection <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b51">[52]</ref>, and image matching <ref type="bibr" target="#b9">[10]</ref>.</p><p>As shown in <ref type="bibr" target="#b10">[11]</ref>, cosalient image regions usually have two properties: 1) they should be prominent or noticeable regions with respect to the background in each image and 2) high homogeneity should be observed for such regions across multiple related images. To explore the first property, some earlier cosaliency models proposed in <ref type="bibr" target="#b11">[12]</ref> and <ref type="bibr" target="#b12">[13]</ref> directly combine several existing saliency detection methods for predicting the salient regions within each single image. For obtaining better performance, Fu et al. <ref type="bibr" target="#b13">[14]</ref> and Liu et al. <ref type="bibr" target="#b14">[15]</ref> proposed novel algorithms for intrasaliency prediction by modifying the existing unsupervised saliency detection models. To explore the second property, most previous approaches discover the homogeneity of cosalient regions within each image pair <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b18">[19]</ref>. To extend beyond pairwise relations, Fu et al. <ref type="bibr" target="#b13">[14]</ref> employed CIE Lab color and Gabor filter to represent each pixel, and extracted contrast cue, spatial cue, and correspondence cue from the image group for generating the cluster level cosaliency maps. Liu et al. <ref type="bibr" target="#b14">[15]</ref> proposed to derive the global similarity measures of image regions over the image set based on the quantized color features.</p><p>As can be seen, corresponding to the above two properties, the key problems in cosaliency detection lie in two aspects: 1) predicting the saliency of image regions within each single image, i.e., the intrasaliency, robustly and 2) developing an optimal mechanism to explore the homogeneity of cosalient objects, i.e., the intersaliency, among multiple related images. For the first problem, most existing approaches only directly apply or manually modify the previous unsupervised saliency detection algorithms for a single image to cosaliency detection. However, they cannot yield promising results as unsupervised saliency detection algorithms tend to lack robustness and be influenced by the complex backgrounds. In addition, the recent progress of saliency detection in a single image has acquired more prior knowledge on saliency. Knowledge transfer from single saliency detection will be certainly beneficial to the intrasaliency in cosaliency detection. For the second problem, the existing approaches mainly focus on exploring the homogeneity based on the low-level features, such as color, texture, or corner descriptors. In this paper, we call it shallow intersaliency, because they only formulate the homogeneity of the low-level visual stimulus, while the homogeneity in deeper insights into higher level concepts could not be captured. In addition, low-level features are easily influenced by the variation in luminance, shape, or viewpoint, leading to unsatisfactory performance of cosaliency detection.</p><p>In order to tackle these problems and further improve the performance of cosaliency detection, we adopt deep learning models in this paper for better solving the problems in both the generation of the robust intrasaliency prior and the discovery of the intersaliency patterns. Instead of using humans as a transfer machine, where researchers learn the knowledge of how to formulate saliency from the conventional unsupervised saliency detection approaches and, then, manually modify these approaches for predicting intrasaliency, inspired by the studies in <ref type="bibr" target="#b19">[20]</ref> and <ref type="bibr" target="#b20">[21]</ref>, we propose an alternative framework to design a real transfer machine that can learn the saliency prior knowledge from the auxiliary annotated data sets automatically and, then, transfer the learned knowledge to predict the intrasaliency for each image in cosaliency data sets. As we know, saliency is an abstract concept that relates to the contrast between the certain image regions and the image backgrounds, as well as the content within the image regions. This relationship holds true regardless of the object category. Thus, according to <ref type="bibr" target="#b21">[22]</ref>, this kind of an abstract concept is more likely to be suitable for transfer learning. In addition, the training data in cosaliency data sets appears to be limited (about 17 images per group). When the labeled training data are scarce, transfer learning of the relevant knowledge from the auxiliary data sets would yield a significant performance improvement <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>. In order to capture saliency prior from the data in the source domain and transfer it to predict the intrasaliency for the data in the target domain, we design a novel framework by adopting the stacked denoising autoencoder (SDAE). As SDAE has been demonstrated to be a powerful deep model that can learn more abstract representations based on its hierarchical architecture and take advantage of the out-of-distribution data for knowledge transfer <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b24">[25]</ref>, the proposed transfer learning framework would be an effective way to predict the intrasaliency.</p><p>Deep learning has shown outstanding performance on mining deep and hidden patterns for building powerful representations in many challenging tasks, such as visual classification and object localization <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>. In this paper, we attempt to leverage deep learning for the discovery of higher level homogeneity among cosalient regions. Specifically, we present the concept of deep intersaliency, which is formulated using the deep reconstruction residual obtained in the highest hidden layer of a self-trained SDAE. As the SDAE is trained on the image regions with higher intrasaliency priors among the multiple related images, it can extract more intrinsic and general hidden patterns to discover the homogeneity of cosalient objects in terms of some higher level concepts. Consequently, the obtained deep intersaliency could alleviate the influence of variance in luminance, shape, and view point, and should become a novel and useful cue when generating the final cosaliency map.</p><p>The flowchart of the proposed approach is shown in Fig. <ref type="figure" target="#fig_0">2</ref>. First, the input images are decomposed hierarchically into finelevel superpixels and coarse-level segments. Then, the saliency prior in this paper is formulated based on the contrast prior and the object prior. We train the contrast model and the objectness model in the auxiliary data sets, and transfer them to generate the contrast prior map and the object prior map for each image in the cosaliency data sets, respectively. The intrasaliency prior is obtained by combining the contrast prior and the object prior. Afterward, we simultaneously explore the homogeneity among the multiple related images based on low-level feature matching and high-level pattern mining to establish the shallow intersaliency and the deep intersaliency, respectively. Finally, the cosaliency maps are generated by weighted integration of the proposed intrasaliency prior, shallow intersaliency, and deep intersaliency.</p><p>We notice that some early works <ref type="bibr" target="#b41">[42]</ref> have applied deep models to solve problems in saliency detection. However, most of those algorithms are proposed for the task of eye fixation prediction rather than the task in this paper, i.e., cosaliency detection. More specifically, the deep model proposed in <ref type="bibr" target="#b41">[42]</ref> is used for extracting low-and mid-level features and computing local contrast. However, the deep learning model proposed in this paper is used for the intrasaliency prior transfer and the deep intersaliency pattern mining.</p><p>In summary, the major contributions of this paper are threefold.</p><p>1) In this paper, we make the earliest effort to cast the intrasaliency prediction in cosaliency detection as a problem of prior knowledge transfer, which could take advantage of the auxiliary fully annotated data sets and generate robust intrasaliency. 2) Besides exploring the shallow intersaliency, we also propose to mine the deep intersaliency for discovering higher level homogeneity of the cosalient objects in the image group. The generated deep intersaliency map is demonstrated in our experiments to be another critical factor in cosaliency detection. 3) SDAEs are used in this paper for better solving the problems both in the generation of the robust intrasaliency prior and in mining deep intersaliency patterns, which is the earliest effort to introduce deep learning to cosaliency detection. The rest of this paper is organized as follows. Section II reviews the related works. Section III describes the proposed approach in detail. Section IV presents the experimental results with a quantitative evaluation in comparison with a number of the state-of-the-art approaches. Finally, the conclusions are drawn in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>Most early approaches for cosaliency detection explore the joint information provided by the image pair to find cosalient regions <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b18">[19]</ref>. However, these methods only seek to detect the cosaliency of two images at a time, not accounting for the discovery of the global coherent information that may exist when there are more than two images. This results in a direct limitation for cosalient pattern exploration when extending beyond pairwise relations. To tackle this problem, some recent works <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b48">[49]</ref> have been proposed to simulate the attention mechanisms for cosaliency detection in a group of images. Based on their assumption and formulation, these methods can be subdivided into three categories.</p><p>The first category is based on the assumption that the salient areas detected by the single image-based saliency detection approaches always contain parts of the foreground object, and the cosalient regions can be decided by selecting the areas frequently occurring among the multiple related images from the detected salient areas. The most representative work for this class was proposed in <ref type="bibr" target="#b12">[13]</ref>, where the cosaliency was formulated by a simple hard constraint of the distinctness (i.e., saliency in an individual image) and the repeatedness (i.e., the consistence measured in an image group) as Cosaliency = Distinctness × Repeatedness. This algorithm gives better performance than the conventional single imagebased saliency detection methods in the task of cosaliency detection. However, it still appears to be ineffective due to its idealized assumption.</p><p>To mitigate this limitation, the second category of cosaliency detection approaches <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> relieves the hard constraint to the soft constraint, which usually considers the intrasaliency, intersaliency, and other useful factors as independent information cues, and generates the final cosaliency map through the weighted integration of these cues. Specifically, Li et al. <ref type="bibr" target="#b10">[11]</ref> proposed to generate an intraimage saliency map and an interimage saliency map based on multiscale segmentation and pairwise similarity ranking, respectively. Then, the cosaliency map was modeled as a linear combination of the two saliency maps. Fu et al. <ref type="bibr" target="#b13">[14]</ref> extracted contrast cue, spatial cue, and corresponding cue through clustering and weighted integration of these information cues based on the probability formulation. Liu et al. <ref type="bibr" target="#b14">[15]</ref> proposed a hierarchical segmentation-based cosaliency model, where the regional contrasts, global similarity, and object prior are calculated based on segmentations of multiple levels. The final cosaliency map was generated by effectively fusing the intrasaliency map and the object prior map.</p><p>Cao et al. <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b27">[28]</ref> proposed another category of algorithms for cosaliency detection, which focus on finding ways to integrate the existing saliency and cosaliency cues more reasonably. Rather than engaging to discover homogeneous information from the collection of multiple related images for representing cosalient objects, these methods mainly exploit the relationship of the obtained maps of multiple existing saliency and cosaliency approaches to obtain the self-adaptive weights for generating the final cosaliency map. Based on the most recent achievements in saliency detection and cosaliency detection, these methods produce a relatively satisfactory performance. However, the large time costs for preparing the existing saliency and cosaliency maps before the fusion process become their major limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED APPROACH</head><p>In this section, we first introduce the basic idea of the SDAE algorithm. Then, the overall procedure of the proposed algorithm is briefly introduced. Afterward, two major components of the proposed framework, i.e., the robust intraimage saliency prior transfer and the intersaliency pattern mining, are described in detail. The generation of the final cosaliency map is introduced in Section III-E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Stacked Denoising Autoencoder</head><p>SDAE is one kind of state-of-the-art deep learning models, which seeks to exploit the unknown structure in the input distribution at multiple layers to make the learned higher level representations more abstract and informative <ref type="bibr" target="#b21">[22]</ref>. Compared with the convolutional neural network, SDAE can lean informative patterns from the input data in an unsupervised manner, which is what we need in Section III-D for mining deep intersaliency patterns. In addition, compared with the other unsupervised deep learning models, e.g., the deep Boltzmann machine (DBM), SDAE has fourfold advantages in this paper. First, SDAE is a better way to extract stable and deterministic numerical feature vectors, since it can directly learn the parametric mappings from input data to their representations <ref type="bibr" target="#b25">[26]</ref>. However, although DBM can learn latent random variables to describe a posterior distribution over the observed data, the learnt posterior distribution is not yet the simple usable feature vectors in some cases <ref type="bibr" target="#b25">[26]</ref>. Second, SDAE is demonstrated in <ref type="bibr" target="#b21">[22]</ref> and <ref type="bibr" target="#b24">[25]</ref> to have the capability to handle domain adaption. Thus, it is more suitable to use SDAE to transfer the prior knowledge for cosaliency detection, as described in Section III-C. Third, SDAE is a reconstruction-based model, and the generated reconstruction residual is what we need to formulate the deep intersaliency in Section III-D. However, we cannot obtain such a term from DBM. Finally, SDAE is simpler to train and explain, provides an efficient inference, and yields the results comparable or better than the RBM-based models in series of experiments <ref type="bibr" target="#b24">[25]</ref>. All the above-mentioned advantages motivate us to use SDAE instead of other deep models in this paper.</p><p>As a basic building block in SDAE, an AE consists of an encoding process and a decoding process. With the aim to transform the input vector into output reconstructions with the least possible amount of distortion, it would learn useful representations and latent patterns of the given data. Specifically, the encoding process uses an encoding function f (x i , θ f ) to map from the input vector x i to a hidden representation vector y i , where θ f indicates the encoding parameters including an encoding projection matrix W (1) and an encoding bias b (1) . Normally, the sigmoid function sigm(η) = 1/(1 + exp(-η)) is used in the encoding function (1) x i + b (1) ).</p><formula xml:id="formula_0">y i = f (x i , θ f ) = sigm(W</formula><p>(1)</p><p>Then, with the decoding parameters θ g = {W (2) , b (2) }, a decoding function g(y i , θ g ) is utilized to map the hidden representation y i back to a reconstruction representation z i through 2) y i + b (2) ).</p><formula xml:id="formula_1">z i = g(y i , θ g ) = sigm(W<label>(</label></formula><p>(</p><formula xml:id="formula_2">)<label>2</label></formula><p>After encoding and decoding, the obtained reconstruction representation z i can be taken as a prediction of input x i , which is based on the patterns encoded in the network. To learn appropriate parameters in θ f and θ g , the training process of such a network is to minimize the cost function with two important terms. The first one is the reconstruction error constraint, which is a basic constraint to reflect the difference between the original input data and the reconstruction output of the network. The second one is called sparsity constraint, which penalizes the deviation of the expected activation of the hidden units (in representation vector) from a fixed (low) level. With these two constraint terms, the cost function is written as</p><formula xml:id="formula_3">L(X, Z, ρ, ρ j ) = 1 m m i=1 1 2 ||x i -z i || 2 2 + λ n j =1 KL(ρ|| ρ j ) (3) KL(ρ|| ρ j ) = ρlog ρ ρ j + (1 -ρ)log 1 -ρ 1 -ρ j (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>where m denotes the number of all the training and reconstructed data, respectively. λ is the weight of the sparsity constraint term, n is the dimension of the hidden representation vector, ρ is the target average activation of the hidden units, and ρ j = m i=1 [y j ] i /m is the average activation of the j th hidden unit y j over the m training data. KL(•) indicates the Kullback-Leibler divergence for providing the sparsity constraint. Like in sparse coding, a nonredundant overcomplete feature set is learned when ρ is small.</p><p>For further improving the effectiveness of AEs, Vincent et al. <ref type="bibr" target="#b28">[29]</ref> propose to build DAEs by reconstructing the input data into a corrupted and partially destroyed version. In DAE [see Fig. <ref type="figure" target="#fig_1">3(a)</ref>], the stochastic mapping function xi = q D(x i |x i ) is first added to the original input data by randomly forcing 30% of them to be zero, while the objective function is still to minimize the reconstruction loss between a clean input x i and its reconstruction output z i . Thus, it forces the learning of far more clever mapping than the identity <ref type="bibr" target="#b28">[29]</ref>. Usually, training a DAE is straightforward using the gradient descent optimization algorithm to update the parameters</p><formula xml:id="formula_5">W (l) = {W (l) i j } and b (l) = {b (l)</formula><p>i } in iterations, where l = {1, 2} indicates the representation layer and the reconstruction layer. Specifically, all these parameters are randomly initialized, and then they are updated with the updating rules</p><formula xml:id="formula_6">W (l) i j = -ε ∂ L(X, Z, ρ, ρ j ) ∂ W (l) i j (5) b (l) i = -ε ∂ L(X, Z, ρ, ρ j ) ∂b (l) i (6)</formula><p>where ε is the learning rate. The partial derivatives in ( <ref type="formula">5</ref>) and ( <ref type="formula">6</ref>) are calculated by the backpropagation algorithm <ref type="bibr" target="#b29">[30]</ref>.</p><p>Based on the observation that the layerwise stacking of feature extraction often yields better representations <ref type="bibr" target="#b25">[26]</ref>, SDAE is built by stacking additional DAE layers to form the deep architecture <ref type="bibr" target="#b28">[29]</ref> [see Fig. <ref type="figure" target="#fig_1">3(b)</ref>]. Just as other deep neural networks, training SDAE could be done in two phases: 1) layerwise self-learning and 2) fine-tuning. Given a set of training data, the layerwise self-learning allows the usage of DAE as independent blocks for training the whole deep network. The key concept in this phase is to train one layer DAE at a time. As shown in Fig. <ref type="figure" target="#fig_1">3(b)</ref>, the bottom layer DAE is first trained with the original input data to obtain its encoding parameters. Then, the obtained hidden representations are used as the input data for training the higher layer DAE. As the labels of the input data are not needed in this process, the layerwise self-learning becomes to a task-free process focusing on learning hierarchical generative representations in an unsupervised manner. After the layerwise self-learning, a logistic regression layer can be added on the top of DAEs, as shown in Fig. <ref type="figure" target="#fig_1">3</ref>(b), enabling the established deep architecture to capture more discriminative information under the supervision of the specific task.</p><p>Suppose, we have a training set {x 1 , x 2 , . . . , x m } with its label set { 1 , 2 , . . . , m }. For each input data x i∈ <ref type="bibr">[1,m]</ref> , its higher (second) layer representation, as shown in Fig. <ref type="figure" target="#fig_1">3(b)</ref>, is denoted by H V,d (x i ), where V and d indicate the parameters in the bottom two-layer neural network. These parameters include the weight matrix V (1) and offset vector d (1) between the input layer and the bottom representation layer, and V (2) and d (2) between the bottom representation layer and the higher representation layer. In the logistic regression layer, the hypothesis function is</p><formula xml:id="formula_7">h (H V,d (x i )) = 1 1 + exp(-T H V,d (x i )) (7)</formula><p>where is the parameter learned in logistic regression by minimizing the cost function</p><formula xml:id="formula_8">J = - 1 m m i=1 i logh (H V,d (x i )) + (1 -i )log(1 -h (H V,d (x i ))) . (<label>8</label></formula><formula xml:id="formula_9">)</formula><p>In this training phase, the parameters V and d are initialized by the layerwise self-learning, while is initialized by random values. Then, all these parameters are optimized under the supervised information in the top logistic regression layer, which is implemented by using the gradient descent algorithm with backpropagation to minimize the cost function in <ref type="bibr" target="#b7">(8)</ref>. The notations in this Algorithm are defined in Section III.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Overall Algorithm</head><p>By using the SDAE model introduced above, we can transfer contrast prior knowledge (in Section III-C) and explore deep intersaliency (in Section III-D) in the proposed cosaliency detection framework. The overall algorithm flow of the proposed algorithm is shown in Algorithm 1.</p><p>In contrast prior transfer, the core problem is how to learn and transfer the prior knowledge of image contrast, which is a relationship between superpixels in the image foreground and background. To solve this problem, we use the generated sample pairs in an auxiliary data set, i.e., the accurate-segmented saliency detection (ASD) data set, as the input data to train SDAEs through greedy layerwise pretraining and supervised fine-tuning, as shown in Algorithm 2. By inputting the sample pairs from the cosaliency data sets into the trained SDAEs, we can obtain the outputted boundary specific contrast prior values for each superpixel that will be fused to generate the final contrast prior, as described in Section III-C.</p><p>In deep intersaliency pattern mining, the problem is how to capture the homogeneity of the cosalient objects in terms of some higher level concepts. To solve this problem, we use the selected superpixels with higher intrasaliency as the input data to train an SDAE via greedy layerwise unsupervised learning, which is shown in Algorithm 3. Then, the obtained deep model is used to output the deep reconstruction residuals for each input superpixel to formulate the deep intersaliency, as described in Section III-D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Intrasaliency Prior Transfer</head><p>Contrast and objectness are two critical concepts for visual attention modeling <ref type="bibr" target="#b30">[31]</ref>. More importantly, these two concepts are the most general knowledge about how much certain regions are visually different from the background and likely to be parts of the salient objects. Regardless of the specific object category, these concepts would have less constraint on the choice of the auxiliary data set and be easy to transfer from the auxiliary data to the target data <ref type="bibr" target="#b19">[20]</ref>. Inspired by this insight, we propose to transfer the saliency priors from the auxiliary annotated data sets for better solving the problems in generating a robust intrasaliency map. 1) Contrast Prior Transfer: Image contrast is one of the most widely used information for saliency detection in a single image <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, because the contrast operator simulates the human receptive fields <ref type="bibr" target="#b13">[14]</ref>. As a result, image regions that are distinct from the background would capture more human visual attention and become the salient regions in the image. By following the basic rule of photographic composition, we assume most image boundaries belong to the background area and formulate saliency based on the contrast between each image region and the image boundaries. As suggested by <ref type="bibr" target="#b33">[34]</ref>, image boundaries are separated into four sides, i.e., the top boundary, left boundary, bottom boundary and right boundary, and the final contrast prior would be obtained by combining the four side-specific contrast priors.</p><p>In this paper, we choose the ASD data set <ref type="bibr" target="#b34">[35]</ref> as the auxiliary data set for learning and transferring the contrast model. Since the ASD data set is one of the largest benchmark data set for saliency detection containing 1000 images and the ground truth is manually labeled, we can use it to learn the contrast model to formulate the mechanism of human visual attention and, then, transfer the learned model to calculate the contrast prior for each image in the cosaliency data sets. Specifically, for each image, we first apply the simple linear iterative clustering algorithm <ref type="bibr" target="#b35">[36]</ref> to decompose it into K fin fine-level superpixels {Sup p }, p ∈ [1, K fin ]. Then, we extract low-level visual features of 53 dimensions for each pixel as suggested in <ref type="bibr" target="#b32">[33]</ref>, including a 5-D color feature (three RGB color values as well as the hue and the saturation components), 12-D steerable pyramid filter responses, and 36-D Gabor filter responses. For each Sup p , we use the mean features of the pixels within this superpixel as its feature vector x p .</p><p>In the learning process, we train four individual contrast models to formulate the image contrast specific to the top boundary, left boundary, bottom boundary, and right boundary, respectively. Because superpixels in different image boundaries are often dissimilar, we use them separately for better performance <ref type="bibr" target="#b33">[34]</ref>. For each image boundary, we first collect the center-boundary (CB) sample pairs (where center indicates a superpixel not in the image boundary) to generate the pairwise inputs as well as their labels, which are determined by the ground truth mask within the center superpixels. Then, a four-layer SDAE is trained based on the generated inputs and labels to formulate the side-specific contrast. Taking the top image boundary as an example [see Fig. <ref type="figure" target="#fig_4">4(a)</ref>], the superpixels within the top boundary (in purple) and a center superpixel (in yellow) are collected to form a CB sample pair. Afterward, all the superpixels in the CB sample pair are represented by the extracted low-level features. In order to establish the relationship between the center superpixel and boundary superpixels, all the image superpixel features in one CB sample pair should be concatenated into a single feature vector for representing the CB pair. Since the number of boundary superpixels is far more than that of the center superpixel, we average the feature vectors of boundary superpixels into one vector to address the imbalanced data dimension problem, and then concatenate it with the feature vector of the center superpixel. Therefore, the dimension of input vectors of SDAE should be twice of that of each superpixel representation. For training SDAE, we first use layerwise self-learning to determine the parameters among the input layer and two hidden layers, which helps to reduce the risk of falling into a poor local optimum of the whole network. Then, the supervised fine-tuning is applied with the label layer and the cost function in <ref type="bibr" target="#b7">(8)</ref> to optimize the parameters (V, d, and ) of the deep network. Thus, it could learn more complex mapping relations between the CB pair inputs and the corresponding saliency of the center superpixels.</p><p>After the learning process, the obtained SDAE models can capture the mutual patterns among CB pairs and infer their contrast hierarchically. Since the abstract concepts learned by SDAE could share a statistical strength across different but related types of examples coming from other domains than the task domain <ref type="bibr" target="#b24">[25]</ref>, it is convenient to transfer the trained SDAE models to calculate the contrast prior for the images in the cosaliency data sets without additional steps for domain adaption. Specifically, for each image in the cosaliency data set [see Fig.  where N(Sup p ) denotes the neighborhood of Sup p and D(x p , x τ ) indicates the Euclidean distance between the two feature vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Object Prior Transfer:</head><p>The object prior in this paper is a generic measurement over various classes, which is different from the category specific detectors, such as faces or cars. It indicates how likely it is for an image window to contain an object of any class rather than background, such as sky and lawn. In contrast to object detectors extensively trained from a large number of category specific training samples, our approach is relatively less expensive and easy to obtain, but it is effective to salient object detection.</p><p>According to <ref type="bibr" target="#b14">[15]</ref>, the object prior is more suitably evaluated on the coarse segmentation. Thus, we apply a graphbased segmentation algorithm <ref type="bibr" target="#b36">[37]</ref> to decompose an image into K coa coarse-level segments {Seg q }, q ∈ [1, K coa ]. Inspired by the studies in <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b37">[38]</ref>, and <ref type="bibr" target="#b38">[39]</ref>, the objectness <ref type="bibr" target="#b39">[40]</ref> is used in this paper, which is trained on PASCAL VOC07 data set to distinguish windows containing an object with a welldefined boundary from amorphous background windows based on several low-level image cues. It is then transferred to the cosaliency data sets to evaluate whether an image window contains an object or not. For each image, we can obtain a set of image windows W k with their corresponding objectness probabilities ψ k , where k ∈ [1, 1000] as suggested in <ref type="bibr" target="#b39">[40]</ref>.</p><p>Afterward, all of these windows are integrated to form the objectness map OB by the pixelwise mean of their objectness probabilities</p><formula xml:id="formula_10">OB pix = 1 | pix | W k ∈ pix ψ k (<label>11</label></formula><formula xml:id="formula_11">)</formula><p>where the subscript pix denotes a pixel in the objectness map and pix indicates the collection of the windows that contain the certain pixel.</p><p>Inspired by the work in <ref type="bibr" target="#b14">[15]</ref>, we also use the appearance characteristics of real world backgrounds in images to improve the object prior map, which assumes that the background regions are usually large and homogeneous, and have a higher ratio of connectivity with image boundaries than salient objects. Consequently, the proposed object prior for each segment can be formulated by</p><formula xml:id="formula_12">OP q = exp -γ | Seg q ∩ Bou| per q + pix∈Seg q OB pix |pix ∈ Seg q | (<label>12</label></formula><formula xml:id="formula_13">)</formula><p>where Bou denotes the image boundary, per q indicates the perimeter of Seg q , and | • | refers to the number of elements. γ is a decay factor set to be 2 as suggested in <ref type="bibr" target="#b14">[15]</ref>. Finally, the intrasaliency prior S in is obtained by the pixelwise mean of the contrast prior and object prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Intersaliency Pattern Mining</head><p>Mining the intersaliency patterns from the data pool of the multiple related images is another important component in our proposed cosaliency detection framework. Based on the intrasaliency prior, both the shallow and deep intersaliency patterns are explored in this paper to extract the common patterns of the cosalient objects among the image group (see Fig. <ref type="figure" target="#fig_5">5</ref>).</p><p>The shallow intersaliency is explored based on the observation that the cosalient regions should be the visual similar regions sharing consistent color or texture and having  As can be seen, the deep intersaliency alleviates the influence of variations in luminance, shape, and viewpoint to highlight the cosalient objects more uniformly.</p><p>higher intrasaliency prior. In other words, the cosalient regions normally have a higher global saliency and visual similarity. Specifically, for each Sup p in the image group with M images, its K sim most similar regions in each of the other images are searched based on the Euclidean distance of their features to form the collection {Sup t }, t ∈ [1, (M -1)K sim ]. Thus, we can calculate the global saliency for each superpixel by</p><formula xml:id="formula_14">S gl p = (M-1)K sim t =1</formula><p>S in t <ref type="bibr" target="#b12">(13)</ref> where S in t indicates the intrasaliency prior of Sup t and S gl p denotes the global saliency of Sup p . In order to further encourage the salient regions frequently appearing in multiple images and suppress the uncommon regions that only occur in a small number of images, we calculate the global similarity for each superpixel as follows:</p><formula xml:id="formula_15">p = (M-1)K sim t =1 D(x p , x t ) (<label>14</label></formula><formula xml:id="formula_16">)</formula><p>where a small p indicates a large similarity among the image group and vice versa. By considering these two terms, the proposed shallow intersaliency is defined as</p><formula xml:id="formula_17">S sh p = S gl p • exp(-p ). (<label>15</label></formula><formula xml:id="formula_18">)</formula><p>Note that it is difficult to uniformly highlight the cosalient objects by mining of the shallow intersaliency based on the low-level features due to the influence of variations in luminance, shape, and viewpoint (see the second row of Fig. <ref type="figure" target="#fig_6">6</ref>). To this end, we also propose to mine the deep intersaliency among the image groups. Unlike the shallow intersaliency, the deep intersaliency can capture the homogeneity of the cosalient objects in terms of some higher level concepts. This is important in cosaliency detection, whereas it is unexplored in the previous works. In this paper, the deep intersaliency is formulated by using the deep reconstruction residual obtained in a three-layer self-trained SDAE. Specifically, we first use an adaptive threshold, i.e., twice of the mean intrasaliency prior value, in each image to select superpixels with a higher priority. Then, all of these superpixels obtained in the image group are collected to form a data pool, which is then used by an SDAE model for the deep intersaliency pattern mining. With the help of the unsupervised self-training, the SDAE can abstract the generative and representative patterns layer to layer, and encode them into its weight matrices {V (1) , V (2) }. When using these learned patterns to represent input superpixels, the ones homogeneous with the cosalient regions are well represented with small reconstruction residuals and vice versa. Since the DAE trained in higher layer can capture more intrinsic and latent patterns of the cosalient regions <ref type="bibr" target="#b25">[26]</ref>, we propose to utilize the deep reconstruction residuals to formulate the deep intersaliency as</p><formula xml:id="formula_19">S dp p = exp(-p ) (M-1)K sim t =1 1 DR t (<label>16</label></formula><formula xml:id="formula_20">) DR t = 1 2 x (2) t -z (2) t 2 2<label>(17)</label></formula><p>where DR t indicates the deep reconstruction residual of Sup t , and x (2)   t and z (2)   t indicate the input vector and reconstruction vector of the higher (second) layer DAE in the SDAE model, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Cosaliency Map Generation</head><p>Until now, three critical information cues, i.e., the intrasaliency prior, the shallow intersaliency, and the deep intersaliency, have been introduced for cosaliency detection. Since each of these information cues only partially reflects one aspect of characteristics of the cosalient regions, we utilize a weighted linear combination in this paper to calculate cosaliency for each superpixel by</p><formula xml:id="formula_21">S co p = β αS dp p + (1 -α)S sh p + (1 -β)S in p (<label>18</label></formula><formula xml:id="formula_22">)</formula><p>where S co p denotes the cosaliency value of Sup p , α and β are two free parameters with values between 0 and 1. The final cosaliency map is generated by extracting the mean cosaliency values within the coarse segments of each image. As can be seen, α and β are two important parameters for the fusion process. α reflects the significance of mining intrinsic and deep structures for exploring the common patterns among multiple images, while β indicates the importance of exploring the common patterns among multiple images for the task of cosaliency detection. The final cosaliency is positively correlated with all the three information cues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENT</head><p>In this section, we evaluate the proposed approach both on image pair cosaliency detection and multiple images cosaliency detection. Qualitative and quantitative analyses of the experimental results are presented, which include the comparisons with some state-of-the-art methods on a variety of benchmark data sets.</p><p>A. Experimental Settings 1) Data Sets: Basically, we evaluate the proposed algorithm on two public benchmark data sets: the Image Pair data set <ref type="bibr" target="#b11">[12]</ref> and iCoseg data set <ref type="bibr" target="#b3">[4]</ref>. The Image Pair data set <ref type="bibr" target="#b11">[12]</ref> contains 105 image pairs (i.e., 210 images) with manually labeled ground truth data. It is the earliest benchmark data set built for evaluating the performance of cosaliency detection, in which each image pair contains one or more similar objects with different backgrounds. The iCoseg data set <ref type="bibr" target="#b3">[4]</ref> may be the largest publicly available data set so far that can be used for cosaliency detection. It consists of 38 image groups of totally 643 images along with pixel ground truth hand annotations. Since most images in the iCoseg data set contain complex background and multiple cosalient objects, and it is difficult to discover the useful information among multiple images, the iCoseg data set is considered as a more challenging data set for cosaliency detection.</p><p>2) Evaluation Metrics: To evaluate the performance of the proposed method, we adopted four widely used criteria that include the receiver operating characteristic (ROC) curve, area under the ROC curve (AUC), the precision recall (PR) curve, and the average precision (AP). Like in <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, and <ref type="bibr" target="#b33">[34]</ref>, ROC and AUC are generated by thresholding pixels in a saliency map into binary cosalient object masks with a series of fixed integers from 0 to 255. The resulting false positive rate versus true positive rate at each threshold value forms the ROC curve. Similarly, PR and AP are generated using the precision rate and the true positive rate (or the recall rate). Specifically, the precision PRE, true positive rate TPR, and false positive rate FPR values are, respectively, defined as</p><formula xml:id="formula_23">PRE = |SF ∩ GF| |SF| TPR = |SF ∩ GF| |GF| FPR = |SF ∩ GB| |GB| (<label>19</label></formula><formula xml:id="formula_24">)</formula><p>where SF, GF, and GB denote the set of segmented foreground pixels after a binary segmentation using a certain threshold, the set of ground truth foreground pixels, and the set of groundtruth background pixels, respectively.</p><p>3) Implementation Details and Parameter Analysis: It is known that there are many hyperparameters involved in such deep neural networks, affecting the performance of the model. More specifically, we used a publicly available library in http://cn.mathworks.com/MATLABcentral/fileexchange/38310 -deep-learning-toolbox, where the SDAE models are first initialized randomly and then trained with several hyperparameters, e.g., the target mean activation ρ, the  weight of the sparsity penalty, the learning rate for the backpropagation optimization, and the number of units at each hidden layer. Before training, we follow <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b42">[43]</ref>, and <ref type="bibr" target="#b43">[44]</ref> to build a three-layer network for unsupervised layerwise learning and add another label layer for supervised fine-tuning (when necessary). Then, according to <ref type="bibr" target="#b25">[26]</ref>, we set the target mean activation ρ and the number of units empirically, as shown in Table <ref type="table" target="#tab_1">I</ref>. For the other hyperparameters, we use a coordinate ascentlike method <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b44">[45]</ref> to optimize them for each layer. In addition, we show the relationship between the number of training samples and the performance of the contrast prior transfer in Fig. <ref type="figure" target="#fig_7">7</ref>. As can be seen, the performance of such transfer process reasonably relies on the number of training samples and using all the images in the ASD data sets is able to generate the best transfer performance.</p><p>Besides the hyperparameters in the SDAE models, the parameter K sim in the intersaliency pattern mining is empirically set to 3. In the experiments, we observe that the cosaliency detection results are reasonably sensitive to the parameters in <ref type="bibr" target="#b17">(18)</ref>. Thus, we set α and β to be 0.6 and 0.7, respectively, for the best performance. The detailed experiment and discussion of these two parameters can be found in the next paragraph. For a hierarchical image segmentation, we generate fine-level superpixels and coarse-level segments by setting the number of superpixels in each image to be 200 and the pixels within each segment to be larger than 200, respectively. A unified set of parameters was utilized in all experiments.</p><p>In order to discuss the main parameters in <ref type="bibr" target="#b17">(18)</ref> and investigate the contributions of the three information cues, i.e., the intrasaliency prior, the shallow intersaliency, and the deep intersaliency, on the overall performance based on the AUC curve, AP curve, AUC score, and AP score, we conduct an experiment on the iCoseg data set. The reason is that it contains more images that can be used for more comprehensive analysis. Specifically, we first set β = 1 to investigate the contributions of the shallow intersaliency and the deep intersaliency by varying α from 0 to 1. As shown in the top two histograms in Fig. <ref type="figure" target="#fig_8">8</ref>, the performance of the deep intersaliency (α = 1) is better than the shallow intersaliency (α = 0). In addition, it also shows that the best performance for the intersaliency pattern mining can be achieved when α is ∼0.6. This implies that the deep patterns are more important in mining of the intersaliency. Afterward, we fix α to be 0.6 and vary β (0-1) to investigate the contributions of the intrasaliency prior and the intersaliency mined among the related images. From the bottom two histograms in Fig. <ref type="figure" target="#fig_7">7</ref>, we can observe that the obtained intersaliency (β = 1) achieves better performance than the intrasaliency (β = 0) does, especially when looking at the AP score. In addition, it also can be found that the best fusion performance is reached when β = 0.7, indicating that mining intersaliency patterns plays a more important role in cosaliency detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation on the Image Pair Data Set</head><p>In this experiment, we first compared our cosaliency detection algorithm with a number of state-of-the-art cosalient detection algorithms, i.e., IPCS <ref type="bibr" target="#b11">[12]</ref>, CBCS <ref type="bibr" target="#b13">[14]</ref>, CSHS <ref type="bibr" target="#b14">[15]</ref>, and PCS <ref type="bibr" target="#b16">[17]</ref>. Fig. <ref type="figure" target="#fig_9">9</ref> shows some comparison results of six pairs of images from the Image Pair data set, where the common objects exhibit distinct diversities in a color or shape property. The subjective evaluations by comparing with the ground truth reveal that the proposed method can yield cosaliency maps more correctly and robustly in these image pairs.</p><p>To provide quantitative comparison, we plotted the ROC and PRC for each approach and calculated the corresponding AUC and AP scores. As shown in Fig. <ref type="figure">10</ref>, compared with the stateof-the-art cosaliency detection algorithms (i.e., IPCS, CBCS, CSHS, and PCS), the proposed approach can consistently achieve the highest true positive rates on the whole ROC curve and the highest precisions on the whole PR curve. To demonstrate the effectiveness of the proposed saliency prior transfer method, we compared the proposed saliency prior transfer method with the intrasaliency detection method CBCS-S <ref type="bibr" target="#b13">[14]</ref>, the two state-of-the-art unsupervised single image saliency detection algorithms HS <ref type="bibr" target="#b31">[32]</ref> and LR <ref type="bibr" target="#b32">[33]</ref>, and another outstanding supervised single image saliency detection method DRFI <ref type="bibr" target="#b45">[46]</ref>. The experimental results shown in Fig. <ref type="figure">10</ref> demonstrate that transferring a contrast prior and an object prior from the auxiliary data sets is a promising way to formulate intraimage saliency, which outperforms both the intraimage saliency detection methods proposed in the stateof-the-art cosaliency detection and the recent single image saliency detection algorithms. The AUC and AP scores for each method are listed in Table <ref type="table" target="#tab_2">II</ref>, from which we can observe that the proposed approach achieves the best performance with respect to both the AUC score and the AP score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation on the iCoseg Data Set</head><p>We further evaluate the proposed algorithm on the iCoseg data set in which each image group may contain much more (17 on average) related images. Since IPCS <ref type="bibr" target="#b11">[12]</ref> and PCS <ref type="bibr" target="#b16">[17]</ref> are not valid on more than two images, we only compared the proposed approach with the two state-of-the-art cosaliency detection methods, i.e., CBCS <ref type="bibr" target="#b13">[14]</ref> and CSHS <ref type="bibr" target="#b14">[15]</ref>, in this data set. Some experimental results are shown in Fig. <ref type="figure">11</ref>, which contains five image groups, i.e., the Cheetah group, the Elephants group, the Gymnastics group, the Stonehenge Fig. <ref type="figure">10</ref>. ROC curves and PR curves for the proposed approach and other state-of-the-art algorithms (including the cosaliency methods and the single image methods) on the Image Pair data set. Solid lines: methods for cosaliency detection. Dashed lines: approaches used for intraimage saliency detection. OURS-intra corresponds to the performance of the proposed intraimage saliency prior. OURS-CP and OURS-OP are the curves of the proposed contrast prior and object prior, respectively. OURS-DP and OURS-SH are the curves of the proposed deep intersaliency and shallow intersaliency, respectively. CBCS-S is the intraimage saliency detection approach proposed in <ref type="bibr" target="#b13">[14]</ref>. group, and the Panda group. As can be seen, the proposed approach can obtain robust performance in the sense that it suppresses the cluttered and complex background regions (see the top two groups in Fig. <ref type="figure">11</ref>), and meanwhile, uniformly highlights the cosalient objects with different viewpoints and shapes (see the bottom three groups in Fig. <ref type="figure">11</ref>). Similar to what we did in the Image Pair data set, we also compared the proposed saliency prior transfer method with the intrasaliency detection method CBCS-S <ref type="bibr" target="#b13">[14]</ref>, and another three state-of-the-art single image saliency detection algorithms HS <ref type="bibr" target="#b31">[32]</ref>, LR <ref type="bibr" target="#b32">[33]</ref>, and DRFT <ref type="bibr" target="#b45">[46]</ref> in the iCoseg data set. The ROC curves and PR curves of these approaches were drawn in Fig. <ref type="figure" target="#fig_0">12</ref>, and the corresponding AUC scores and AP scores were listed in Table <ref type="table" target="#tab_2">III</ref>. From Fig. <ref type="figure" target="#fig_0">12</ref> and Table <ref type="table" target="#tab_2">III</ref>, it shows that the proposed saliency prior transfer method still obtains satisfactory performance, which is better than those two unsupervised single saliency models LR and HS, but worse than the supervised single saliency method DRFI. Due to our analysis, the reason for the promising performance of DRFI mainly lies in some additional considered factors, e.g., discriminative regional description and learning-based multilevel saliency fusion. This finding suggests a potential utility in transferring more useful knowledge for cosaliency detection in our future work. More importantly, like in the Image Pair data set, the cosaliency detection results of the proposed method could also outperform all other state-of-the-art algorithms and achieve the highest true positive rates on the whole ROC curve as well as the highest precisions on the whole PR curve consistently.</p><p>To perform further verification, we compared the AUC and AP scores between the proposed approach and the other stateof-the-art cosaliency detection methods for each image group in the iCoseg data set in Fig. <ref type="figure" target="#fig_1">13</ref>. As can be seen, the proposed approach is superior to the other state-of-the-art algorithms in 25 image groups among the overall 38 image groups. For some image groups, e.g., Stonehenge2, Elephants, and Woman Soccer Players2, the proposed approach improves the performance of the existing cosaliency detection algorithms to a large extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Computational Cost and Runtime</head><p>Given an image group with M images, the time complexity of the proposed algorithm for generating cosaliency maps for these images is O(Mτ logτ ) + O(M 2 ), where τ indicates the number of pixels in each image. For intuitional comparison, Table <ref type="table" target="#tab_3">IV</ref> lists the average execution time for each image by using different approaches. The experiment was run on a PC with Intel i3-2130 3.4-GHz CPU and 8-GB RAM. The code was implemented in MATLAB without optimization. For IPCS <ref type="bibr" target="#b11">[12]</ref> and CBCS <ref type="bibr" target="#b13">[14]</ref>, we run the source codes provided by the authors on the same environment. Since the authors of CSHS <ref type="bibr" target="#b14">[15]</ref> did not release their source code, we directly reported its runtime listed in their paper, which was run on the PC with a similar configuration to ours. As can be seen, the proposed algorithm achieves the best performance with the moderate computational complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we have proposed a novel cosaliency detection framework, which is one of the earliest efforts to investigate the feasibility of using deep learning in cosaliency detection. For better solving the problems in generating a robust intrasaliency map, this paper made the earliest effort to transfer useful knowledge from the auxiliary annotated data sets. Rather than just exploring the shallow intersaliency, we also proposed to mine the deep intersaliency by discovering the intrinsic and coherent structures of the cosalient objects. Comprehensive experiments on two publicly available benchmarks have demonstrated the effectiveness of the proposed work.</p><p>For the further work, we tend to extend the proposed work in the following directions. First, we will improve the proposed work by using more principled integration framework to fuse the obtained information cues. Second, we will embed the cosaliency detection process into weakly supervised learning framework <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref> for helping the object selecting with weakly labeled images. Third, the proposed method can also be extended and applied to a wide range of video processing tasks, such as video foreground extraction, video categorization, and video memorability computation <ref type="bibr" target="#b52">[53]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Flowchart of the proposed cosaliency detection approach.</figDesc><graphic coords="3,56.03,58.85,499.34,142.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Illustration of the architecture of DAE and SDAE. (a) DAE acted as one unit for building the SDAE. (b) SDAE built by two DAE layers and a logistic regression layer.</figDesc><graphic coords="4,317.51,58.13,239.78,129.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 2 : 1 : 5 :Algorithm 3 : 1 : 2 : 3 :</head><label>2153123</label><figDesc>Train SDAE Models for Transferring Contrast Prior Input: Superpixels and their features in an auxiliary dataset; Output: The learnt boundary-specific contrast SDAE models; For Boundary = [top, left, bottom, right] Collect the boundary-specific CB sample pairs and 2: their labels; Use the boundary-specific CB sample pairs as input 3: data to layer-wise train the boundary-specific SDAE in an unsupervised manner; Use the labels of the input data to fin-turning the 4: boundary-specific SDAE model by using back-propagation. End for Train the SDAE Model to Formulate Deep Intersaliency Input: The features and intra-saliency prior values of superpixels in each image of an image group; Output: The learnt SDAE model; Use the adaptive threshold in each image to select superpixels with higher intra-saliency prior; Collect all the selected superpixels in the image group to form the training data; Train the SDAE model in a completely unsupervised layer-wise manner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>4(b)], we first sample each center image superpixel [the yellow superpixel in the top-left image of Fig. 4(b)]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Illustration of learning and transferring the contrast prior. (a) Learning process of the top-side-specific SDAE model. (b) Transferring contrast model for generating contrast prior. Yellow nodes: feature representation of the yellow center superpixel. Violet, green, blue, and black nodes: feature representations of the top, left, bottom, and right image boundaries, respectively.</figDesc><graphic coords="7,53.51,58.49,501.02,180.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Illustration of the mining of intersaliency based on shallow and deep cosalient cues.</figDesc><graphic coords="8,55.43,58.85,237.14,149.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Some examples of the shallow intersaliency and the deep intersaliency.As can be seen, the deep intersaliency alleviates the influence of variations in luminance, shape, and viewpoint to highlight the cosalient objects more uniformly.</figDesc><graphic coords="8,54.47,241.25,239.66,101.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Illustration of relationship between the number of training images and the performance of the transferred contrast priors.</figDesc><graphic coords="9,317.51,153.65,239.90,78.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Illustration of the mining of intersaliency based on the shallow and deep cosalient cues.</figDesc><graphic coords="9,317.51,279.53,239.90,153.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Qualitative comparison of cosaliency maps on the Image Pair data set.</figDesc><graphic coords="10,54.95,58.25,501.79,191.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 .Fig. 13 .</head><label>1213</label><figDesc>Fig.12. ROC curves and PR curves for the proposed approach and other state-of-the-art algorithms (including the cosaliency methods and the single image methods) on the iCoseg data set. Solid lines: methods for cosaliency detection. Dashed lines: approaches used for intraimage saliency detection. OURS-intra corresponds to the performance of the proposed intraimage saliency prior. OURS-CP and OURS-OP are the curves of the proposed contrast prior and object prior, respectively. OURS-DP and OURS-SH are the curves of the proposed deep intersaliency and shallow intersaliency, respectively. CBCS-S is the intraimage saliency detection approach proposed in<ref type="bibr" target="#b13">[14]</ref>.</figDesc><graphic coords="12,102.47,58.49,411.74,144.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="11,63.23,58.73,490.58,172.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="11,64.43,389.93,482.97,288.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm 1 :</head><label>1</label><figDesc>Overall Procedure of Our Algorithm Input: A group of images; Output: Co-saliency maps of these images; 1: Generate fine-level superpixels {Sup p } for each image and extract the feature vectors {x p }; 2: Train boundary-specific contrast SDAE models via Algorithm 2 and use the learnt SDAE models to calculate the final contrast prior C P p via Eq. 9 and</figDesc><table><row><cell>Eq. 10;</cell><cell></cell></row><row><cell cols="2">3: Generate course-level segments {Seg q } for each image;</cell></row><row><cell cols="2">4: Use the objectness model learnt in [40] to calculate</cell></row><row><cell cols="2">the object prior for each segment via Eq. 11 and Eq. 12;</cell></row><row><cell cols="2">5: Use the pixel-wise mean of the contrast prior and</cell></row><row><cell cols="2">object prior to generate the intra-saliency prior S in ;</cell></row><row><cell cols="2">6: Calculate the shallow inter-saliency S sh p by using</cell></row><row><cell>Eq. 13, Eq. 14, and Eq. 15;</cell><cell></cell></row><row><cell cols="2">7: Train a SDAE model via Algorithm 3 and use it to calculate the deep inter-saliency S dp p using Eq. 16 and</cell></row><row><cell>Eq. 17; 8: Use the obtained S in , S sh p , and S</cell><cell>dp p to generate the</cell></row><row><cell>final co-saliency maps via Eq. 18.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I HYPERPARAMETERS</head><label>I</label><figDesc>IN THE SDAE MODELS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II COMPARISON</head><label>II</label><figDesc>OF AUC AND AP SCORES BETWEEN THE PROPOSED APPROACH AND THE OTHER</figDesc><table /><note><p>STATE-OF-THE-ART METHODS ON THE IMAGE PAIR DATA SET Fig. 11. Qualitative comparisons of cosaliency maps on the iCoseg data set.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV AVERAGE</head><label>IV</label><figDesc>RUNTIME (s) PER IMAGE</figDesc><table /></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Science Foundation of China under Grant 61473231 and Grant 61522207 and in part by the Doctorate Foundation through Northwestern Polytechnical University.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised extraction of visual attention objects in color images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Ngan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="145" />
			<date type="published" when="2006-01">Jan. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Image visual attention computation and application via the learning of object attributes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Vis. Appl</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1671" to="1683" />
			<date type="published" when="2013-10">Oct. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="569" to="582" />
			<date type="published" when="2015-03">Mar. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">iCoseg: Interactive co-segmentation with intelligent scribble guidance</title>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kowdle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="page" from="3169" to="3176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Distributed cosegmentation via submodular optimization on anisotropic diffusion</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-11">Nov. 2011</date>
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scale invariant cosegmentation for image groups</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>Providence, RI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06">Jun. 2011</date>
			<biblScope unit="page" from="1881" to="1888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Object-based multiple foreground video co-segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06">Jun. 2013</date>
			<biblScope unit="page" from="3166" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Object retrieval using visual query context</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hanjalic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1295" to="1307" />
			<date type="published" when="2011-12">Dec. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Robust object co-detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>Portland, OR, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06">Jun. 2013</date>
			<biblScope unit="page" from="3206" to="3213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image matching via saliency region correspondences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06">Jun. 2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Co-salient object detection from multiple images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Ngan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1896" to="1909" />
			<date type="published" when="2013-12">Dec. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A co-saliency model of image pairs</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Ngan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3365" to="3375" />
			<date type="published" when="2011-12">Dec. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">From co-saliency to co-segmentation: An efficient and fully unsupervised energy minimization model</title>
		<author>
			<persName><forename type="first">K.-Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-H</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>Providence, RI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06">Jun. 2011</date>
			<biblScope unit="page" from="2129" to="2136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cluster-based co-saliency detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3766" to="3778" />
			<date type="published" when="2013-10">Oct. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Co-saliency detection based on hierarchical segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">Le</forename><surname>Meur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="88" to="92" />
			<date type="published" when="2014-01">Jan. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cosaliency: Where people look when comparing images</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annu. ACM Symp. User Interface Softw</title>
		<meeting>Annu. ACM Symp. User Interface Softw<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-10">Oct. 2010</date>
			<biblScope unit="page" from="219" to="228" />
		</imprint>
	</monogr>
	<note type="report_type">Technol.</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Preattentive co-saliency detection</title>
		<author>
			<persName><forename type="first">H.-T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process</title>
		<meeting>IEEE Int. Conf. Image ess<address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-09">Sep. 2010</date>
			<biblScope unit="page" from="1117" to="1120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image co-saliency detection by propagating superpixel affinities</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-M</forename><surname>Pun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust., Speech Signal ess<address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-05">May 2013</date>
			<biblScope unit="page" from="2114" to="2118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Saliency map fusion based on rank-one constraint</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Multimedia Expo</title>
		<meeting>IEEE Int. Conf. Multimedia Expo<address><addrLine>San Jose, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-07">Jul. 2013</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Transfer learning by ranking for weakly supervised object annotation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Brit. Mach. Vis. Conf</title>
		<meeting>Brit. Mach. Vis. Conf<address><addrLine>Guildford, U.K.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-09">Sep. 2012</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Weakly supervised localization and learning with generic knowledge</title>
		<author>
			<persName><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="275" to="293" />
			<date type="published" when="2012-12">Dec. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep learning of representations for unsupervised and transfer learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Unsupervised Transf. Learn. Challenge Workshop</title>
		<meeting>Unsupervised Transf. Learn. Challenge Workshop<address><addrLine>Bellevue, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-07">Jul. 2011</date>
			<biblScope unit="page" from="17" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06">Jun. 2014</date>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Transfer learning for visual categorization: A survey</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1019" to="1034" />
			<date type="published" when="2015-05">May 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep learners benefit more from out-of-distribution examples</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf</title>
		<meeting>Int. Conf<address><addrLine>Fort Lauderdale, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-04">Apr. 2011</date>
			<biblScope unit="page" from="164" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013-08">Aug. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis<address><addrLine>Zürich, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-09">Sep. 2014</date>
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Self-adaptively weighted co-saliency detection via rank constraint</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4175" to="4186" />
			<date type="published" when="2014-09">Sep. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010-01">Jan. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning Representations by Back-Propagating Errors</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An objectoriented visual saliency detection framework based on sparse coding representations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2009" to="2021" />
			<date type="published" when="2013-12">Dec. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hierarchical saliency detection</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>Portland, OR, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06">Jun. 2013</date>
			<biblScope unit="page" from="1155" to="1162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A unified approach to salient object detection via low rank matrix recovery</title>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>Providence, RI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="853" to="860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Saliency detection via graph-based manifold ranking</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>Portland, OR, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06">Jun. 2013</date>
			<biblScope unit="page" from="3166" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Süsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>Miami, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-06">Jun. 2009</date>
			<biblScope unit="page" from="1597" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">SLIC superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Süsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
			<date type="published" when="2012-11">Nov. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Efficient graph-based image segmentation</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="181" />
			<date type="published" when="2004-09">Sep. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of interactions between humans and objects</title>
		<author>
			<persName><forename type="first">A</forename><surname>Prest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="601" to="614" />
			<date type="published" when="2012-03">Mar. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Figure-ground segmentation by transferring window masks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kuettel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>Providence, RI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="558" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Measuring the objectness of image windows</title>
		<author>
			<persName><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2189" to="2202" />
			<date type="published" when="2012-11">Nov. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Stacked autoencoders for unsupervised feature learning and multiple organ detection in a pilot study using 4D patient data</title>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Orton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Doran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Leach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1930" to="1943" />
			<date type="published" when="2013-08">Aug. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Saliency detection within a deep convolutional architecture</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Workshops AAAI Conf</title>
		<meeting>Workshops AAAI Conf<address><addrLine>Québec City, QC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-07">Jul. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders for face pose normalization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Inf. Process</title>
		<meeting>Neural Inf. ess<address><addrLine>Stateline, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-12">Dec. 2013</date>
			<biblScope unit="page" from="241" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">On latent fingerprint minutiae extraction using stacked denoising sparse autoencoders</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sankaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vatsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Joint Conf</title>
		<meeting>IEEE Int. Joint Conf<address><addrLine>Clearwater, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10">Oct. 2014</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Practical recommendations for gradient-based training of deep architectures</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade (Lecture Notes in Computer Science)</title>
		<editor>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Montavon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Orr</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Salient object detection: A discriminative regional feature integration approach</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>Portland, OR, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06">Jun. 2013</date>
			<biblScope unit="page" from="2083" to="2090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Two-stage learning to predict human eye fixations via stacked denoising autoencoders</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint/>
	</monogr>
	<note>to be published</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Background prior-based salient object detection via deep reconstruction residual</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1309" to="1321" />
			<date type="published" when="2015-08">Aug. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Co-saliency detection via looking deep and wide</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06">Jun. 2015</date>
			<biblScope unit="page" from="2994" to="3002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Object detection in optical remote sensing images based on weakly supervised learning and high-level feature learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3325" to="3337" />
			<date type="published" when="2015-06">Jun. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Weakly supervised learning for target detection in remote sensing images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="701" to="705" />
			<date type="published" when="2015-04">Apr. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Efficient, simultaneous detection of multi-class geospatial targets based on visual saliency modeling and discriminative learning of sparse coding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="37" to="48" />
			<date type="published" when="2014-03">Mar. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning computational models of video memorability from fMRI brain imaging</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1692" to="1703" />
			<date type="published" when="2015-08">Aug. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
