<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hardware-Aware Neural Architecture Search: Survey and Taxonomy</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hadjer</forename><surname>Benmeziane</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Polytechnique Hauts-de-France</orgName>
								<orgName type="institution" key="instit2">LAMIH/CNRS</orgName>
								<address>
									<settlement>Valenciennes</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kaoutar</forename><surname>El Maghraoui</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">IBM T. J. Watson Research Center</orgName>
								<address>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hamza</forename><surname>Ouarnoughi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Université Polytechnique Hauts-de-France</orgName>
								<orgName type="institution" key="instit2">LAMIH/CNRS</orgName>
								<orgName type="institution" key="instit3">INSA</orgName>
								<address>
									<settlement>Valenciennes</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Smail</forename><surname>Niar</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Université Polytechnique Hauts-de-France</orgName>
								<orgName type="institution" key="instit2">LAMIH/CNRS</orgName>
								<orgName type="institution" key="instit3">INSA</orgName>
								<address>
									<settlement>Valenciennes</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Martin</forename><surname>Wistuba</surname></persName>
							<email>martin.wistuba@ibm.com</email>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">IBM Research AI</orgName>
								<orgName type="institution" key="instit2">IBM Technology Campus</orgName>
								<address>
									<settlement>Dublin</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Naigang</forename><surname>Wang</surname></persName>
							<email>nwang@us.ibm.com</email>
							<affiliation key="aff2">
								<orgName type="institution">IBM T. J. Watson Research Center</orgName>
								<address>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hardware-Aware Neural Architecture Search: Survey and Taxonomy</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There is no doubt that making AI mainstream by bringing powerful, yet power hungry Deep Neural Networks (DNNs) to resource-constrained devices would require an efficient co-design of algorithms, hardware and software. The increased popularity of DNN applications deployed on a wide variety of platforms, from tiny microcontrollers to datac enters, have resulted in multiple questions and challenges related to constraints introduced by the hardware. In this survey on Hardware-Aware Neural Architecture Search (HW-NAS), we present some of the existing answers proposed in the literature for the following questions: "Is it possible to build an efficient Deep Learning (DL) model that meets the latency and energy constraints of tiny edge devices?", "How can we find the best trade-off between the accuracy of a DL model and its ability to be deployed in a variety of platforms?". The survey provides a new taxonomy of HW-NAS and assesses the hardware cost estimation strategies. We also highlight the challenges and limitations of existing approaches and potential future directions. We hope that this survey will help to fuel the research towards efficient deep learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The past years have witnessed a remarkable development in neural network models and algorithms targeting multiple tasks from computer vision and natural language processing to games. Given the difficulty to manually design a Deep Learning (DL) model, many researchers turn to Neural Architecture Search (NAS). Current state-of-the-art model in image classification and object detection, EfficientNet-L2 <ref type="bibr" target="#b22">[Pham et al., 2020]</ref>, was obtained by automatically searching for the hyperparameters and operators of the architecture.</p><p>A typical NAS process is composed of three fundamental parts. First, the search space which represents a considerable number of different architectures and different value ranges for each hyperparameter. Second, the search algorithm that explores the search space looking for the best architecture that * Contact Author maximizes the objective function. Finally, as we need a way to measure an architecture's performance to compute the objective function, a third component, the evaluator, is used to obtain the model's accuracy.</p><p>While considerable efforts have focused on the accuracy, the resulting models tend to be exponentially complex and require increasing memory and compute. Therefore, in realworld applications such as autonomous driving cars, hardware constraints come out as a critical limiting factor to exploit DL models at their full potential. Besides, an architecture's manual design is significantly more difficult if we consider the hardware variety and limitations. This is why, since 2017, we have seen a new wave of NAS algorithms, hardware-aware neural architecture search (HW-NAS), that incorporate the hardware constraints into their objective function and optimize their search space according to a target hardware platform.</p><p>The goal of this survey is to provide a comprehensive summary of HW-NAS by highlighting the following attributes: -The survey proposes a special taxonomy for HW-NAS according to the final hardware target in section 2.</p><p>-It explains how the multi-objective problem is formulated and how the hardware constraints such as the latency, energy and memory footprint are incorporated into the objective function (see section 3).</p><p>-The survey also describes, in section 4, the Hardware Search Space, a new component in HW-NAS, as opposed to the conventional Architecture Search Space used in NAS.</p><p>-One crucial component is how to profile the hardware metrics and collect them. We discuss these in section 5. -Finally, in section 6, the paper focuses on helping quickly ramp up new ideas in HW-NAS by exposing the limitations and the future directions towards building efficient deep learning models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">HW-NAS Taxonomy</head><p>HW-NAS has multiple objectives and multiple views of the problem emerging from the variety of hardware platforms available. As shown in figure <ref type="figure" target="#fig_0">1</ref>, we can classify these goals into three categories, described in the following subsections.  Most of the existing HW-NAS fall under this category. The goal is to find the best architecture in terms of accuracy and hardware efficiency for one fixed target hardware. Let's suppose that our hardware platform is an edge NVIDIA GPU, the HW-NAS will explore the space of architectures to look for the one that optimize accuracy, latency, energy consumption, etc. Within this category, two approaches are adopted:</p><p>-Hardware-aware search strategy where the search is cast as a multi-objective optimization problem <ref type="bibr" target="#b26">[Tan et al., 2018;</ref><ref type="bibr" target="#b1">Cai et al., 2018;</ref><ref type="bibr" target="#b30">Wu et al., 2019]</ref>. While searching for the best architecture, the search algorithm calls the accuracy evaluator component to get the accuracy of the generated architecture and a special evaluator that measures the hardware cost metric (e.g., latency, memory usage, energy consumption). Both model accuracy and hardware cost guide the search and enable the NAS to find the most efficient architecture.</p><p>-Hardware-aware Search Space uses a restricted pool of architectures. Before the search, we either measure the operators' performance on the target platform or define a set of rules that will refine the search space; eliminate all the architectures' operators that do not perform well on the target hardware. The prior empirical study helps define the rules. HURRICANE <ref type="bibr" target="#b35">[Zhang et al., 2020]</ref> uses different operator choices for three types of mobile processors: Hexagon DSP, ARM CPU and Myriad Vision Processing Unit (VPU). Accumulated domain knowledge from prior experimentation on a given hardware platform help narrow down the search space. For instance, they do not use depthwise convolutions for CPU, squeeze and excitation mechanisms for VPU and they do not lower the kernel sizes for a DSP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Single Target, Multiple Configurations.</head><p>This category aims not only to get the most optimal architecture based on the accuracy but also to get an optimal architecture with latency guaranteed to meet the target hardware specification. For example, the authors of FNAS <ref type="bibr" target="#b11">[Jiang et al., 2019b]</ref> define a new hardware search space containing the different FPGA specifications (e.g., tiling configurations). They also use a performance abstraction model to measure the searched neural architectures' latency without training. This allows them to quickly prune architectures that do not meet the target hardware specifications. In <ref type="bibr" target="#b31">[Yang et al., 2020]</ref>, the authors use the same ASICs approach and define a hardware search space containing various ASIC templates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multiple Targets.</head><p>In this third category, the goal is to find the best architecture when given a set of hardware platforms to optimize for. In other words, we try to find a single model that performs relatively well across different hardware platforms. This approach is the most favourable choice, especially in mobile development, as it provides more portability. This problem was tackled by <ref type="bibr" target="#b4">[Chu et al., 2020;</ref><ref type="bibr" target="#b13">Jiang et al., 2020b]</ref> by defining a multi-hardware search space. The search space contains the intersection of all the architectures that can be deployed in the different targets. Note that, targeting multiple hardware specifications is more challenging as the best model for a GPU, can be very different to the best model for a CPU (i.e., for GPUs wider models are more appropriate while for CPUs deeper models are).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">HW-NAS Problem Formulation</head><p>The Hardware-aware NAS is cast as a multi-objective optimization problem in which we consider the accuracy and hardware constraints, such as the latency, memory footprint and energy consumption, as different objectives. In this section, we classify the formulations into two classes: single and multi-objective optimization.</p><p>We denote the space of all feasible architectures as A, also called architecture search space. The optimization method is looking for the architecture α that maximizes the performance metric denoted by f for a given dataset δ.</p><p>1. Single-Objective Optimization: A) Two-Stage optimization: In this formulation (equation 1), the search is solely based on the accuracy f . Once an architecture is found, different compression techniques are applied to specialize it. <ref type="bibr" target="#b9">[Han et al., 2019]</ref> applies a reinforcement learning agent to find the best quantization bitwidth and pruning level after selecting the most accurate model. <ref type="bibr" target="#b2">[Cai et al., 2020]</ref> searches over a pre-trained and selected architecture to find the most efficient one in terms of latency and energy consumption in the second stage.</p><formula xml:id="formula_0">max α∈A f (α, δ)<label>(1)</label></formula><p>B) Constrained optimization: As illustrated in equation 2, we consider in this formulation the hardware metrics g i as constraints to our optimization problem. The threshold T i and the trade-off between different constraints can be adapted to practical requirements. As most of the optimization methods used by NAS (i.e. reinforcement learning and evolutionary algorithms) were designed for unconstrained optimization problems, this formulation is hard to be adopted directly. Therefore, many researchers turned to penalty methods to transform the equation into a single objective function that contains the hardware constraints, and the accuracy measurement <ref type="bibr" target="#b26">[Tan et al., 2018;</ref><ref type="bibr" target="#b30">Wu et al., 2019;</ref><ref type="bibr" target="#b1">Cai et al., 2018]</ref>. For example, MNASNet <ref type="bibr" target="#b26">[Tan et al., 2018]</ref> defines g as the latency of the model and uses a learnable parameter to control the effect of the hardware constraints on the global objective function.</p><formula xml:id="formula_1">max α∈A f (α, δ) subject to g i (α) ≤ T i ∀i ∈ I (2)</formula><p>2. Multi-objective Optimization: A) Scalarization Methods: In this approach, we use a parameterized aggregation function h to transform the multi-objective optimization problem into a singleobjective optimization problem. The function h can be a weighted sum, a weighted exponential sum, a weighted min-max or a weighted product. However, in most situations, finding all pareto optimal solutions in solving this problem with a fixed setting of the weights is not possible. Therefore, the problem is solved for multiple values of the vector w, which requires multiple optimization runs. <ref type="bibr" target="#b10">[Hsu et al., 2018]</ref> proposes to use the weighted sum as the objective function. The proposed formulation of this function is described by equation 3. ACC refers to the accuracy metric, E refers to the energy consumed by the architecture α and w is a learned parameter to adjust the effect of the energy on the reward function.</p><formula xml:id="formula_2">max α∈A w • ACC(α, δ) − (1 − w) • E(α)<label>(3)</label></formula><p>Since the accuracy is not differentiable, some search strategies, including gradient-based once <ref type="bibr" target="#b1">[Cai et al., 2018]</ref>, use the cross-entropy to which they add the hardware constraint. B) NSGA-II: An alternative approach is to use the elitist evolutionary algorithm NSGA-II <ref type="bibr" target="#b5">[Deb et al., 2002]</ref>.</p><p>Hardware-aware NAS works <ref type="bibr" target="#b17">[Lu et al., 2019b;</ref><ref type="bibr" target="#b3">Chu et al., 2019]</ref> have been using NSGA-II algorithm to ensure the exploration of diverse architectures in the search space. Moreover, NSGANet <ref type="bibr" target="#b17">[Lu et al., 2019b]</ref> uses Bayesian Optimization to profit from search history. MoreMNAS <ref type="bibr" target="#b3">[Chu et al., 2019]</ref> uses a hybrid search strategy combining NSGA-II with reinforcement learning to regulate random mutations.</p><formula xml:id="formula_3">max α∈A f 1 (α, δ), f 2 (α, δ), ..., f n (α, δ)<label>(4)</label></formula><p>4 Hardware Search Space</p><p>When the HW-NAS targets a single hardware platform with multiple configurations, the NAS process includes a new component that we call the Hardware Search Space (HSS), as shown in figure <ref type="figure" target="#fig_0">1</ref>: "Single Target, Multiple Platform Configurations". This section describes this search space's characteristics and proposes a taxonomy based on the existing works. We refer the readers to <ref type="bibr" target="#b29">[Wistuba et al., 2019]</ref> to read more about the architecture search space. First of all, we need to discuss the different hardware platforms used for DL applications. We classify them according to these three categories. Server Processors. This type of platforms is found in cloud data centers, on-premise data centers, edge servers, or supercomputers. They provide abundant computational resources and can vary from CPUs, GPUs, FPGAs and ASICs. When available, machine learning researchers focus on accuracy. Many HW-NAS works target server processors to speed up the training process. Mobile Devices. With billions of users worldwide, ML researchers' focus has shifted to enable fast and efficient DL on mobile devices. As these devices are heavily constrained with respect to their memory and computational capabilities, ML researchers' objective becomes to assess the trade-off between accuracy and efficiency. Many HW-NAS algorithms target smartphones including FBNet <ref type="bibr" target="#b30">[Wu et al., 2019]</ref> and ProxylessNAS <ref type="bibr" target="#b1">[Cai et al., 2018]</ref>. Figure <ref type="figure" target="#fig_1">2</ref> shows the types of hardware targeted by HW-NAS from 2017. We can see an increasing variety of platforms with the rise of domain-specific processors and FPGAs for inference. 'no specific target' means that the hardware metrics are either the FLOPs or the number of parameters.</p><p>The HSS generates different hardware specifications and optimizations by applying various algorithmic transformations to fit the hardware design. Although the co-exploration is effective, it increases the search space-time complexity significantly as the search strategy looks for different pairs of (architecture, platform configuration).</p><p>Hardware Search Space (HSS) can be further categorized as follows. Parameter-based. A set of different parameter configurations formalizes the search space. If we take FPGAs as an example, their design space may include IP instance categories, IP reuse strategies, quantization schemes, parallel factors, data transfer behaviours, tiling parameters, and buffer sizes. FNAS <ref type="bibr" target="#b11">[Jiang et al., 2019b]</ref> and <ref type="bibr">FNASs [Yu et al., 2019]</ref> are two HW-NAS that builds on this representation. The authors in <ref type="bibr" target="#b11">[Jiang et al., 2019a;</ref><ref type="bibr" target="#b16">Lu et al., 2019a</ref>] used a multi-FPGA hardware search space. The search consists of dividing the architecture into pipeline stages that can be assigned to an FPGA according to its memory and DSP slices, in addition to applying an optimizer that adjusts the tiling parameters. Another example is <ref type="bibr" target="#b20">[Mohamed et al., 2020]</ref>, where the adopted approach takes the global structure of an FPGA and adds all possible parameters to its hardware search space including the input buffer depth, memory interface width, filter size and ratio of the convolution engine.</p><p>Template-based. In this category, the search space is defined as a set of pre-configured templates. For example, NA-SAIC <ref type="bibr" target="#b31">[Yang et al., 2020]</ref> integrates NAS with Application-Specific Integrated Circuits (ASIC). Their search space includes templates of several existing successful designs. The goal is to find the best model with the different possible parallelizations among all templates. In addition to the tiling parameters and bandwidth allocation, the authors in <ref type="bibr" target="#b12">[Jiang et al., 2020a]</ref> define a set of FPGA platforms, and the search finds a coupling of the architecture and FPGA platform that fits a set of pre-defined constraints (e.g., max latency 5ms)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Hardware Metrics Collection</head><p>In this section, we will review the methods to collect hardware performance. First of all, let's discuss the different metrics used to judge if a deep learning model is efficient or not. FLOPs. HW-NAS methods presented in 2016 and 2017 use <ref type="bibr" target="#b25">[Smithson et al., 2016;</ref><ref type="bibr" target="#b8">Gordon et al., 2017]</ref> as an analytical function to minimize the number of parameters and the number of FLOPs. Such strategies presume that the number of operations is positively correlated with the execution time. However, recent work has shown that two models may have the same number of FLOPs, but different latencies <ref type="bibr" target="#b0">[Bouzidi et al., 2021;</ref><ref type="bibr" target="#b35">Zhang et al., 2020;</ref><ref type="bibr" target="#b27">Wang et al., 2020a]</ref>. For example, NASNet-A and MobileNetV1 have about the same number of FLOPs, but NASNet-A may have slower latency due to its hardware-unfriendly structure. Therefore, using FLOPs as a hardware cost metric is not effective and can return suboptimal architectures. Latency. Low-latency architectures at inference time are crucial on edge scenarios where the edge devices impose a constraint on the latency. Therefore, many works consider latency in their objective function and attempt a trade-off between inference time and accuracy. Energy Consumption. Energy is often profiled by NVIDIA's given hardware platform profilers, such as nvprof. We can formalize energy either as peak or average power usage, and the two metrics are used by various HW-NAS works, including <ref type="bibr" target="#b10">[Hsu et al., 2018;</ref><ref type="bibr" target="#b31">Yang et al., 2018;</ref><ref type="bibr" target="#b7">Gong et al., 2019]</ref>. Area. Another metric that is of concern to chip manufacturers is the chip's area. The aim is to get the smallest processor to run the best model. <ref type="bibr" target="#b31">[Yang et al., 2020]</ref> uses <ref type="bibr">MAESTRO [Kwon et al., 2019]</ref> to profile the area and power consumption. They posit that the area of the circuit is also a good indicator of static power consumption. These two values are correlated. Memory Footprint. This metric corresponds to the exact amount of memory a DL model uses. Rather than calculating the number of parameters an architecture has, one can profile the memory footprint and use it as an objective term. To reduce the memory footprint in edges devices, model compression techniques <ref type="bibr" target="#b5">[Deng et al., 2020]</ref> are usually applied. We can classify the types of methods that collect the hardware metrics into four categories:</p><p>1. Real-time Measurements in which the explored models are executed on the target hardware while searching. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Lookup Table Models</head><p>where a lookup table is created beforehand and filled with each operator hardware metrics on the targeted hardware. Once the search starts, the system calculates the overall cost from the lookup table. 3. Analytical Estimation which consists of computing a rough estimate using the processing time, the stall time, and the starting time. 4. Prediction Model where we build an ML model to predict the cost using architecture and dataset features. Real-world measurements provide high accuracy in measuring the hardware efficiency of an architecture. MnasNet <ref type="bibr" target="#b26">[Tan et al., 2018]</ref> uses this method in the exploration. It achieves 75.2% top-1 accuracy with 78ms latency on a Pixel phone platform, which is 1.8x faster than MobileNetV2 with 0.5% higher accuracy. However, this method considerably slows down the search algorithm by averaging hundreds of runs to get precise measurements. Additionally, this strategy is not scalable and requires that all the hardware platforms are available. That's why many works tend to use a prediction model <ref type="bibr" target="#b1">[Cai et al., 2018;</ref><ref type="bibr" target="#b35">Zhang et al., 2020;</ref><ref type="bibr" target="#b11">Jiang et al., 2019a;</ref><ref type="bibr" target="#b16">Lu et al., 2019a;</ref><ref type="bibr" target="#b34">Zhang et al., 2019;</ref><ref type="bibr" target="#b10">Hao et al., 2019]</ref> or a pre-collected lookup table <ref type="bibr" target="#b30">[Wu et al., 2019;</ref><ref type="bibr" target="#b20">Mohamed et al., 2020;</ref><ref type="bibr" target="#b12">Jiang et al., 2020a]</ref> or computing an analytical estimation <ref type="bibr" target="#b34">[Zhang et al., 2019;</ref><ref type="bibr" target="#b19">Marchisio et al., 2020]</ref>. Although these techniques are efficient, they require hardware knowledge to build the models.</p><p>To fairly compare each method's accuracy, we computed, in figure <ref type="figure">3</ref>, each architecture's latency in NAS-Bench-101 <ref type="bibr" target="#b32">[Ying et al., 2019]</ref> and compare it to the real-time measurements, according to three collecting methods: lookup table, prediction model and analytical estimation (Figure <ref type="figure">3</ref>). For the lookup table, we calculated each operator's latency in the cell of the benchmark. When a cell is generated, we sum the constructing operators' latency, and we get the whole cell's latency. For the prediction model, we used two models: a simple MLP and XGBoost, both trained on a portion of the benchmark's real-time measurements. We choose these two methods because they are both used by popular HW-NAS in <ref type="bibr" target="#b1">[Cai et al., 2018]</ref> and <ref type="bibr" target="#b30">[Wu et al., 2019]</ref> respectively. Lastly, for the analytical estimation, we computed the number of MAC for the cell and multiplied that by the latency of one multiply-add tensor instruction. We ran this experiment on a Tesla K80 GPU, the prediction model MLP was trained for 50 epochs with early stopping. NAS-Bench-101 defines more than 400k cells. The search algorithm used to calculate the search time is an evolutionary algorithm based on the benchmark's validation accuracy and the different methods' latency. As expected, the analytical estimation does not produce good results compared to the prediction models or the lookup table method. We expected the analytical estimation to be the fastest, but looping through the model graph to extract the number of MAC is time consuming. Even with a simple XGBoost, the prediction models give the best results and accelerate the search more than five times compared to the real-time measurements. However, it is worth mentioning that prediction methods require large dataset and consume a considerable preparation time in order to collect the data and train the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Challenges and Limitations</head><p>In this section, we set out the critical obstacles that keep from exploiting the HW-NAS's full potential. The majority of problems often apply to general NAS approaches as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Benchmarking and Reproducibility</head><p>To compare different search algorithms, we need to unify the search spaces. To that end, many benchmarks were released <ref type="bibr" target="#b32">[Ying et al., 2019;</ref><ref type="bibr" target="#b24">Siems et al., 2020]</ref>. These benchmarks present a dataset of architectures with their respective accuracy and latency metrics. However, two limitations are relevant in the context of HW-NAS. (1) most of them define Con-vNets architectures with a cell-based approach, i.e., the architectures aren't flexible enough for different hardware platforms, (2) most of the performance are measured on CPU or GPU which isn't suited for HW-NAS. A more recent paper introduced the first hardware-aware NAS benchmark, HW-NAS-Bench <ref type="bibr" target="#b15">[Li et al., 2021]</ref>. This work extends the number of hardware metrics and records the latency, the energy consumed on six hardware devices, including commercial edge devices, FPGA, and ASIC. Its search space is a combination of FBNet <ref type="bibr" target="#b30">[Wu et al., 2019]</ref> search space and NAS- <ref type="bibr">Bench-201 [Dong and Yang, 2020]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Transfer Learning</head><p>Transfer Learning is one of the fundamental concepts in DL. It consists on training a model on a proxy dataset, then finetuning its weights on the target dataset. Using this technique, better precision with the final model is obtained. However, not all architectures are easily transferable from one dataset to another. Cell-based search space <ref type="bibr" target="#b36">[Zoph et al., 2018]</ref> enhance transferability by adding more repeating cells to the model and updating a small number of hyperparameters. However, stacking the same blocks seems to be not efficient when incorporating hardware constraints. As MNASNet <ref type="bibr" target="#b26">[Tan et al., 2018]</ref> argued, restricting cell diversity is critical for achieving high accuracy and low latency on mobile settings.</p><p>Many NAS works <ref type="bibr" target="#b16">[Liu et al., 2018;</ref><ref type="bibr" target="#b21">Nayman et al., 2019</ref>] have included dedicated evaluations of the transferability of their final model. XNAS <ref type="bibr" target="#b21">[Nayman et al., 2019]</ref> transferred their final cell structure on six popular classification benchmarks surpassing other conventional NAS methods while taking into account the hardware constraints.</p><p>NAT <ref type="bibr" target="#b18">[Lu et al., 2020]</ref> leverages the NAS process to find transferable weights directly and get rid of the fine-tuning stage. NAT's key idea is that they start from a supernetwork and adaptively modify it to obtain a task-specific supernetwork. This latter can then be used directly to search for architectures within one task without the additional training cost. They demonstrated the efficacy of NAT on 11 benchmarks, including ImageNet, on mobile settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Transferability of HW-NAS Across Multiple Platforms</head><p>HW-NAS suffers from conditional optimality due to the variety of existing devices. Ideally, we should design different architectures for different platforms. However, in real situations, given the prohibitive cost of the search and the cost of training on multiple architectures, we often resort to designing one architecture and deploying it anywhere. Transferring a model from one platform to another or producing hardware transferable models via the NAS process is an exciting challenge for HW-NAS. In the following section, we discuss two popular approaches. Each approach has its pros and cons, as discussed below.</p><p>Transfer the entire NAS process. It consists of reexecuting the single-target HW-NAS to suit a new target. This includes finding a collection method that is scalable and flexible to multiple hardware platforms. When using real-world measurements, <ref type="bibr" target="#b9">[Han et al., 2019]</ref> ran the NAS search for three hardware platforms. However, using real-world measurements considerably slows down the search algorithm and requires the targeted hardware's availability during the search time. Using other collection methods such as the lookup table or the prediction model, we'll need to collect data from the new platforms by rerunning the entire set of operators. For example, Once-for-all <ref type="bibr" target="#b2">[Cai et al., 2020</ref>] created a lookup table with the reported inference latency on each tested hardware platforms. According to the used measurement method, transferring the NAS process to target another platform is increasingly difficult and not scalable.</p><p>Transfer the final model. This approach consists of finding the best model for one hardware platform and then specialize it for another one. It has been proposed by <ref type="bibr" target="#b2">[Cai et al., 2020;</ref><ref type="bibr" target="#b1">Cai et al., 2018;</ref><ref type="bibr" target="#b21">Nayman et al., 2019]</ref>. The specialization is usually done by compressing the model using quantization, enabling the model to fit in tiny devices. However, specialization presents the following challenges:</p><p>• An operator may be efficient for one platform and less efficient in another: In <ref type="bibr" target="#b4">[Chu et al., 2020]</ref>, the authors argued that separable convolutions give great results when ran on GPUs but perform badly on CPUs.</p><p>• Limits of the compression methods: We consider here the quantization and pruning. We know that theoretically, the compression ratio has a threshold that cannot be surpassed for these two methods. For example, quantizing a model implies encoding its activations and weights into the minimum possible bit length. Theoretically, this length is 2 (one bit).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Outlook and Future Directions</head><p>HW-NAS suffers from the same limitations as NAS. The main one is the cost of the search algorithm. The current popular way to speed up the process is to use differentiable NAS and create a supernetwork that can represent the whole search space. This supernetwork is trained once to get the weights of all the sub-networks and thus avoids training each sampled architecture. This technique reduces the search time from several days to hours. However, a major disadvantage of this network is the restriction it makes on the targeted task and domain. More research is needed to explore efficient ways and tricks to make HW-NAS more practical and more amenable to a diverse set of tasks and domains. This would especially be useful in commercial settings.</p><p>There is no doubt that the future of mobile and handheld devices is AI. AI-focused mobile chips from top manufacturing companies like Apple, Samsung, Huwaei and others are making their ways into the mainstream. These devices use SoCs that take advantage of multiple platforms (e.g., GPUs, CPUs and NPU in the same chip). However, this heterogeneity of platforms needs to be well understood to speed up the inference time of deep neural networks <ref type="bibr" target="#b28">[Wang et al., 2020b]</ref>. This motivates further the research community to simultaneously explore both the architecture search space and the hardware design space to identify the best neural architecture and hardware pairs that maximize both test accuracy and hardware efficiency. Such co-exploration will be key to allow designing architectures that can be deployed efficiently on a variety of platforms: data center, edge, mobile, and embedded. The HW-NAS methods should co-explore compression techniques and models search spaces to find the best trade-off between model size, inference time and accuracy.</p><p>HW-NAS should also look at exploring neural architecture search with emerging computing paradigms such as inmemory-computing <ref type="bibr" target="#b14">[Jiang et al., 2021]</ref>. These new non-von-Neumann paradigms present novel solutions to AI computing based on emerging nano-devices called Phase-Change Memory (PCM) <ref type="bibr" target="#b23">[Sebastian et al., 2019]</ref>. The optimization space spans multiple design points that range from device types, to circuit topologies, to device non-idealities and variations, to neural architectures.</p><p>While DNNs have clear commercial use cases, the next AI breakthrough may require an entirely different combination of algorithm, hardware and software. HW-NAS offers a paradigm that opens up the design space and pushes forward the Pareto frontier between hardware efficiency and model accuracy for efficient and improved hardware/software codesign, hence pushing AI to its next frontiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>Efficient deep learning techniques are increasingly attracting both academia and industry researchers. This paper explores one of the trending approaches to build a model that respects the trade-off between accuracy and hardware constraints. We point out several potential improvements and new directions in this area, such as accounting for new hardware platforms (e.g., heterogeneous systems-on-chip and in-memory computing) and extending it to new application domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Taxonomy of HW-NAS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Statistics on the targeted platforms. No specific target categorizes the HW-NAS that incorporate model size or number of parameters in their objective function. Multiple categorizes the HW-NAS that targets various platforms at ones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Goals of Hardware-Aware Neural Architecture Search Single Target Multiple Targets Best Architecture Best Platform Configuration Multiple Platform Configurations Fixed Platform Configurations</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Select a Set of Hardware</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Targets</cell></row><row><cell>Hardware-aware Search</cell><cell>Hardware-aware Search</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Strategy</cell><cell>Space</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Multi-Hardware</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Architecture Search Space</cell></row><row><cell>Architecture Search</cell><cell>Architecture Search</cell><cell>Architecture</cell><cell>Hardware Search</cell><cell>Intersection of all</cell></row><row><cell>Space</cell><cell>Space</cell><cell>Search Space</cell><cell>Space</cell><cell cols="2">architectures that can be</cell></row><row><cell>Conv, Pooling, ...</cell><cell>Conv, Pooling, ...</cell><cell>Conv, Pooling, ...</cell><cell>Tiling parameters</cell><cell>deployed in all targets.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Number of PE</cell><cell></cell></row><row><cell>Search Strategy</cell><cell>Hw Cost (Energy,</cell><cell></cell><cell>Search</cell><cell></cell></row><row><cell></cell><cell>latency ....)</cell><cell cols="2">Strategy</cell><cell></cell></row><row><cell>Hw Cost (Energy, latency ....)</cell><cell>Search Strategy</cell><cell cols="2">latency ....) Hw Cost (Energy,</cell><cell>Search Strategy</cell><cell>Special Metric</cell></row><row><cell cols="2">search strategy selects an architecture</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">request performance measure</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">remove inefficient architectures</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence Survey Track</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Smaïl Niar, and Abdessamad Ait El Cadi. Performance prediction for convolutional neural networks on edge gpus</title>
		<author>
			<persName><surname>Bouzidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CF &apos;21: Computing Frontiers Conference</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="54" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName><surname>Cai</surname></persName>
		</author>
		<idno>CoRR, abs/1812.00332</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Once-for-all: Train one network and specialize it for efficient deployment</title>
		<author>
			<persName><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Multi-objective reinforced evolution in mobile neural architecture search</title>
		<author>
			<persName><surname>Chu</surname></persName>
		</author>
		<idno>CoRR, abs/1901.01074</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Discovering multi-hardware mobile models via architecture search</title>
		<author>
			<persName><surname>Chu</surname></persName>
		</author>
		<idno>CoRR, abs/2008.08178</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Model compression and hardware acceleration for neural networks: A comprehensive survey</title>
		<author>
			<persName><forename type="first">Deb</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on evolutionary computation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="485" to="532" />
			<date type="published" when="2002">2002. 2002. 2020. 2020</date>
		</imprint>
	</monogr>
	<note>Proc. IEEE</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Nasbench-201: Extending the scope of reproducible neural architecture search</title>
		<author>
			<persName><forename type="first">Dong</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><forename type="middle">;</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mixed precision neural architecture search for energy efficient deep learning</title>
		<author>
			<persName><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCAD</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Morphnet: Fast &amp; simple resource-constrained structure learning of deep networks</title>
		<author>
			<persName><forename type="first">Gordon</forename></persName>
		</author>
		<idno>CoRR, abs/1711.06798</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Design automation for efficient deep learning computing</title>
		<author>
			<persName><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10616</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fpga/dnn co-design: An efficient design methodology for iot intelligence on the edge</title>
		<author>
			<persName><surname>Hao</surname></persName>
		</author>
		<idno>CoRR, abs/1806.10332</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Design Automation Conference</title>
				<meeting>the 56th Annual Design Automation Conference</meeting>
		<imprint>
			<date type="published" when="2018">2019. 2019. 2018. 2018</date>
		</imprint>
	</monogr>
	<note>MONAS: multi-objective neural architecture search using reinforcement learning</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Accuracy vs. efficiency: Achieving both through fpga-implementation aware neural architecture search</title>
		<author>
			<persName><surname>Jiang</surname></persName>
		</author>
		<idno>CoRR, abs/1907.04650</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Design Automation Conference</title>
				<editor>
			<persName><forename type="first">Xinyi</forename><surname>Jiang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Edwin</forename><surname>Zhang</surname></persName>
		</editor>
		<editor>
			<persName><surname>Hsing-Mean</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lei</forename><surname>Sha</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Qingfeng</forename><surname>Yang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yiyu</forename><surname>Zhuge</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jingtong</forename><surname>Shi</surname></persName>
		</editor>
		<editor>
			<persName><surname>Hu</surname></persName>
		</editor>
		<meeting>the 56th Annual Design Automation Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019a. 2019. 2019b. 2019</date>
		</imprint>
	</monogr>
	<note>Hardware/software co-exploration of neural architectures</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Standing on the shoulders of giants: Hardware and neural architecture cosearch with hot start</title>
		<author>
			<persName><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020a. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hardware-aware transformable architecture search with efficient search space</title>
		<author>
			<persName><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo, ICME</title>
				<imprint>
			<date type="published" when="2020">2020b. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Understanding reuse, performance, and hardware cost of dnn dataflow: A data-centric approach</title>
		<author>
			<persName><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hyoukjun Kwon, Prasanth Chatarasi, Michael Pellauer, Angshuman Parashar, Vivek Sarkar, and Tushar Krishna</title>
				<imprint>
			<date type="published" when="2019">2021. 2021. 2019. 2019</date>
		</imprint>
	</monogr>
	<note>Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">{HW}-{nas}-bench: Hardware-aware neural architecture search benchmark</title>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS&apos;20)</title>
				<imprint>
			<date type="published" when="2020">2021. 2021. 2020. 2020</date>
		</imprint>
	</monogr>
	<note>International Conference on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On neural architecture search for resource-constrained hardware platforms</title>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
		<idno>CoRR, abs/1911.00105</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018. 2019a. 2019</date>
		</imprint>
	</monogr>
	<note>Progressive neural architecture search</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Nsga-net: Neural architecture search using multi-objective genetic algorithm</title>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Banzhaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Lu</surname></persName>
		</author>
		<idno>CoRR, abs/2005.05859</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Genetic and Evolutionary Computation Conference</title>
				<meeting>the Genetic and Evolutionary Computation Conference</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2020. 2020</date>
		</imprint>
	</monogr>
	<note>Neural architecture transfer</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Nascaps: A framework for neural architecture search to optimize the accuracy and hardware efficiency of convolutional capsule networks</title>
		<author>
			<persName><surname>Marchisio</surname></persName>
		</author>
		<editor>IC-CAD</editor>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Best of both worlds: Automl codesign of a cnn and its hardware accelerator</title>
		<author>
			<persName><forename type="first">Mohamed</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th ACM/EDAC/IEEE Design Automation Conference</title>
				<meeting>the 57th ACM/EDAC/IEEE Design Automation Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">XNAS: neural architecture search with expert advice</title>
		<author>
			<persName><surname>Nayman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing System</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1975" to="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><surname>Pham</surname></persName>
		</author>
		<editor>Pham, Qizhe Xie, Zihang Dai, and Quoc V. Le. Meta</editor>
		<imprint>
			<date type="published" when="2003">2020. 2003.10580, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Anastasios Petropoulos, Christophe Piveteau, Theodore Antonakopoulos, Bipin Rajendran, Manuel Le Gallo, and Evangelos Eleftheriou. Computational memory-based inference and training of deep neural networks</title>
		<author>
			<persName><forename type="first">Sebastian</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 Symposium on VLSI Circuits</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Margret Keuper, and Frank Hutter. {NAS}-bench-301 and the case for surrogate benchmarks for neural architecture search</title>
		<author>
			<persName><surname>Siems</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><surname>Smithson</surname></persName>
		</author>
		<idno>CoRR, abs/1611.02120</idno>
		<title level="m">Neural networks designing neural networks: Multi-objective hyper-parameter optimization</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Mnasnet: Platformaware neural architecture search for mobile</title>
		<author>
			<persName><surname>Tan</surname></persName>
		</author>
		<idno>CoRR, abs/1807.11626</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">HAT: Hardware-aware transformers for efficient natural language processing</title>
		<author>
			<persName><forename type="first">Collins ; Fika</forename><surname>Tx Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">X</forename><surname>Ventures</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huston</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<ptr target="https://venturebeat.com/2020/01/11/why-tinyml-is-a-giant-opportunity/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020-01-20">2020. 2020. 2020-01-20. 2020a. 2020</date>
		</imprint>
	</monogr>
	<note>Why tinyml is a giant opportunity</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<title level="m">Siqi Wang, Anuj Pathania, and Tulika Mitra. Neural network inference on mobile socs. IEEE Des. Test</title>
				<imprint>
			<date type="published" when="2020">2020b. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A survey on neural architecture search</title>
		<author>
			<persName><surname>Wistuba</surname></persName>
		</author>
		<idno>CoRR, abs/1905.01392</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search</title>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<publisher>CVPR. Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Co-exploration of neural architectures and heterogeneous ASIC accelerator designs targeting multiple tasks</title>
		<author>
			<persName><forename type="first">Yang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 15th European Conference Computer Vision</title>
				<editor>
			<persName><forename type="first">Hyoukjun</forename><surname>Kwon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Liangzhen</forename><surname>Lai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vikas</forename><surname>Chandra</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Weiwen</forename><surname>Jiang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yiyu</forename><surname>Shi</surname></persName>
		</editor>
		<meeting><address><addrLine>Zheyu Yan, Meng Li</addrLine></address></meeting>
		<imprint>
			<publisher>Lei Yang</publisher>
			<date type="published" when="2018">2018. 2018. 2020. 2020</date>
		</imprint>
	</monogr>
	<note>Netadapt: Platform-aware neural network adaptation for mobile applications. In 57th Design Automation Conference, DAC. IEEE</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Nas-bench-101: Towards reproducible neural architecture search</title>
		<author>
			<persName><forename type="first">Ying</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML, Proceedings of Machine Learning Research</title>
				<editor>
			<persName><forename type="first">Kamalika</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<meeting>the 36th International Conference on Machine Learning, ICML, Machine Learning Research</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Software-defined design space exploration for an efficient AI accelerator architecture</title>
		<author>
			<persName><forename type="first">Yu</forename></persName>
		</author>
		<idno>CoRR, abs/1903.07676</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">When neural architecture search meets hardware implementation: from hardware awareness to co-design</title>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Annual Symposium on VLSI (ISVLSI)</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fast hardware-aware neural architecture search</title>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<publisher>CVPR. IEEE</publisher>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName><surname>Zoph</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
