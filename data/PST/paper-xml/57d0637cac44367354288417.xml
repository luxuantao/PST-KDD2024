<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">People counting based on head detection combining Adaboost and CNN in crowded surveillance environment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-05-28">May 28, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chenqiang</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Chongqing Key Laboratory of Signal and Information Processing</orgName>
								<orgName type="institution">Chongqing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Chongqing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Chongqing Key Laboratory of Signal and Information Processing</orgName>
								<orgName type="institution">Chongqing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Chongqing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Chongqing Key Laboratory of Signal and Information Processing</orgName>
								<orgName type="institution">Chongqing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Chongqing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Chongqing Key Laboratory of Signal and Information Processing</orgName>
								<orgName type="institution">Chongqing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Chongqing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pei</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Chongqing Key Laboratory of Signal and Information Processing</orgName>
								<orgName type="institution">Chongqing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Chongqing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Chongqing Key Laboratory of Signal and Information Processing</orgName>
								<orgName type="institution">Chongqing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Chongqing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Chongqing Key Laboratory of Signal and Information Processing</orgName>
								<orgName type="institution">Chongqing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Chongqing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Chongqing Key Laboratory of Signal and Information Processing</orgName>
								<orgName type="institution">Chongqing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Chongqing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yajun</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Chongqing Key Laboratory of Signal and Information Processing</orgName>
								<orgName type="institution">Chongqing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Chongqing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Chongqing Key Laboratory of Signal and Information Processing</orgName>
								<orgName type="institution">Chongqing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Chongqing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Chongqing Key Laboratory of Signal and Information Processing</orgName>
								<orgName type="institution">Chongqing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Chongqing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Chongqing Key Laboratory of Signal and Information Processing</orgName>
								<orgName type="institution">Chongqing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Chongqing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Chongqing Key Laboratory of Signal and Information Processing</orgName>
								<orgName type="institution">Chongqing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Chongqing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Chongqing Key Laboratory of Signal and Information Processing</orgName>
								<orgName type="institution">Chongqing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Chongqing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Chongqing Key Laboratory of Signal and Information Processing</orgName>
								<orgName type="institution">Chongqing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Chongqing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Chongqing Key Laboratory of Signal and Information Processing</orgName>
								<orgName type="institution">Chongqing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Chongqing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Chongqing Key Laboratory of Signal and Information Processing</orgName>
								<orgName type="institution">Chongqing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Chongqing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Chongqing Key Laboratory of Signal and Information Processing</orgName>
								<orgName type="institution">Chongqing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Chongqing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Chongqing Key Laboratory of Signal and Information Processing</orgName>
								<orgName type="institution">Chongqing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Chongqing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Chongqing Key Laboratory of Signal and Information Processing</orgName>
								<orgName type="institution">Chongqing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Chongqing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">People counting based on head detection combining Adaboost and CNN in crowded surveillance environment</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-05-28">May 28, 2016</date>
						</imprint>
					</monogr>
					<idno type="MD5">DEB3511B5A98686360273A2F7E681953</idno>
					<idno type="DOI">10.1016/j.neucom.2016.01.097</idno>
					<note type="submission">Received date: 29 September 2015 Revised date: 24 November 2015 Accepted date: 25 January 2016 Preprint submitted to Neurocomputing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T10:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>People counting</term>
					<term>Head detection</term>
					<term>Adaboost</term>
					<term>CNN</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>People counting is one of the key techniques in video surveillance. This task usually encounters many challenges in crowded environment, such as heavy occlusion, low resolution, imaging viewpoint variability, etc. Motivated by the success of R-CNN [1] on object detection, in this paper we propose a head detection based people counting method combining the Adaboost algorithm and the CNN. Unlike the R-CNN which uses the general object proposals as the inputs of CNN, our method uses the cascade Adaboost algorithm to obtain the head region proposals for CNN, which can greatly reduce the following classification time. Resorting to the strong ability of feature learning of the CNN, it is used as a feature extractor in this paper, instead of as a classifier as its commonlyused strategy. The final classification is done by a linear SVM classifier trained on the features extracted using the CNN feature extractor. Finally, the prior knowledge can be applied to post-process the detection results to increase the precision of head detection and the people count is obtained by counting the head detection results. A real classroom surveillance dataset is used to evaluate the proposed method and experimental results show that this method has good performance and outperforms the baseline methods, including deformable part model and cascade Adaboost methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Counting people is one of the most important tasks in intelligent video surveillance systems and it has a wide range of applications and business value in many places, such as banks, railway stations, shopping malls, schools, etc. However, counting people in a crowded surveillance environment is a challenge task due to low resolution, occlusion, illumination change, imaging viewpoint variability, background clutter, etc.</p><p>In the past decades, various methods of people counting have been proposed.</p><p>Generally speaking, these methods can be divided into three groups:</p><p>Counting by trajectory clustering: This kind of methods count people by adopting tracking techniques. Antonini et al. <ref type="bibr" target="#b1">[2]</ref> applied a multilayer clustering technique to trajectories which are obtained from a tracking system. This method can reduce the bias between the number of tracks and the real number of persons. Topkaya et al. <ref type="bibr" target="#b2">[3]</ref> improved the clustering method using a generic person detector. This method fused different types of features, including color, spatial and temporal features into clustering. For some applications, these approaches can get comparative good performance. However, they are usually time-consuming and computation resource-consuming since they tightly depend on tracking techniques. Besides, the performance would degrade a lot when applying these methods to motion imaging platforms since this situation usually makes the track algorithms unstable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Counting by regression: This type of methods adopt regression-based</head><p>techniques to learn a mapping between low-level features and the people count in a scene. Lempitsky et al. <ref type="bibr" target="#b3">[4]</ref> introduced an object counting method through pixel-level object density map regression whose integral over any image region gives the count of objects within that region and learning to infer such density can be formulated as a minimization of a regularized risk quadratic cost function.</p><p>Chan et al. <ref type="bibr" target="#b4">[5]</ref> extracted a set of holistic low-level features from each segmented region, and learned a function that maps features into the number of people with the Bayesian regression. Morerio et al. <ref type="bibr" target="#b5">[6]</ref> proposed a global group-based approach to estimate the count of people relying on accurate camera calibration. Idrees et al. <ref type="bibr" target="#b6">[7]</ref> estimated the number of individuals in extremely dense crowds based on multi-source and multi-scale features and regress using SVR from images. Zhang et al. <ref type="bibr" target="#b7">[8]</ref> alternatively trained a CNN regression model for crowd counting with two related learning objectives, namely the crowd density and the crowd count. The obvious advantage of these methods is that they can circumvent explicit object segmentation and detection. However, their performance is heavily influenced by the occlusion and perspective effect. Besides, these methods just provide the information of the people count, and fail to locate the individuals. Actually, the location information of individuals is usually very important for video surveillance systems. For example, we can use this information to automatically obtain the spatial distribution of people in a scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Counting by detection:</head><p>The basic idea of this type of methods is to design a detector to detect each individual to obtain people count. The adopted detection methods usually include body detection <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, shoulders detection <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b14">14]</ref> and head detection <ref type="bibr" target="#b15">[15]</ref>. Comparatively speaking, head detection methods usually outperform the other two types of methods in crowded applications with heavy occlusion since only the heads of many persons may be visible in this situation. In order to effectively detect heads, many methods have been presented, such as skeleton graph <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b17">17]</ref>, the head template matching <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b19">19]</ref> and learning-based methods <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b21">21]</ref>. These methods can be applied to some specific scenes and would fail for some real surveillance applications.</p><p>In this paper, we proposed an effective head detection based people counting method which can work well in real surveillance scenes with low resolution head imaging, body occlusion and unconstrained imaging viewpoints. A typical application example is to count people in classrooms as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. From Fig. <ref type="figure" target="#fig_0">1</ref>, we can observe that in addition to the imaging viewpoint diversity, the images are blurred and the heads are of low resolution. Due to the heavy perspective effect, the scales of heads change greatly from 66×66 to 18×18, which makes it hard to correctly detect heads. Specifically, it is also hard even for humans to carry out the counting task when the people are far from the camera. These challenges usually make many state-of-the-art methods fail to complete the counting task, even for the deformable part model (DPM) <ref type="bibr" target="#b22">[22]</ref> as will be discussed in experiments in Section 3.</p><p>Recently, the R-CNN achieved a great success on the object detection task <ref type="bibr" target="#b0">[1]</ref>. Actually, this success benefits greatly from the good object classification capability of CNN in different situations. Motivated by this, we adopt the CNN to handle above mentioned challenges. Since the CNN can just take on the classification task, we have to provide potential object candidates to CNN for the object detection task. A direct strategy is to adopt an off-the-shelf region proposal method, such as selective search <ref type="bibr" target="#b23">[23]</ref> used in R-CNN <ref type="bibr" target="#b0">[1]</ref>, to get the potential object regions. However, these region proposal methods are usually time-consuming. This would not satisfy the requirement of the above kind of applications, since the counting task usually needs to be simultaneously carried out for many classrooms or scenes.</p><p>In this paper we adopt the Adaboost method <ref type="bibr" target="#b24">[24]</ref> to fast obtain head proposals. Unlike the the region proposal method used in R-CNN which provides general object proposals, our achieved proposals are just ones which are more similar to heads and thus our proposals are much fewer than those obtained by using region proposal methods. This will further greatly reduce the classification time of CNN. These advantages make our final system can serve simultaneously more than 50 classrooms with robust performance on a single PC machine with a common configuration.</p><p>The rest of the paper is organized as follows. In Section 2, we first briefly describe the overview of our framework and then introduce each module of the proposed method in detail. In Section 3, we introduce our classroom dataset and then evaluate our method on this dataset. Conclusions and future work will be given in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The proposed method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Overview of our framework</head><p>As shown in Fig. <ref type="figure" target="#fig_1">2</ref> In the online detection stage, firstly we apply the cascade head detector to the test image to get head proposals, and then we use the trained SVM classifier to further recognize the head. Finally, we post-process the recognition results and count the final results as the people count.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Head proposal detection</head><p>In order to utilize the great advantage of the CNN on classification tasks to address the object detection problem, we have to provide CNN the object candidates firstly. To do this, adopting off-the-shelf region proposal methods would lead to two problems for real surveillance applications. First, the off-theshelf region proposal methods are usually time-consuming, which would be not satisfy the time requirement. Second, the common strategy of these methods is to provide top N candidate object regions, where N is usually not smaller than 1000 <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b0">1]</ref>. For most situations, the provided candidate regions are much more than real objects. Meanwhile, most of candidate regions are not related to interesting objects since these methods just detect general objects, not a specific object. Thus, this strategy will increase much classification time of CNN.</p><p>In this paper, we adopt the cascade Adaboost method to fast obtain the candidate head regions. We choose HOG as the feature representation. We also test Harr-like <ref type="bibr" target="#b27">[27]</ref> and LBP <ref type="bibr" target="#b28">[28]</ref> features, and find that the HOG feature has the best performance for our task.</p><p>The cascade of HOG was first proposed in <ref type="bibr" target="#b29">[29]</ref> to significantly speed up human detection. The basic idea is to combine the HOG descriptors and the cascading classifiers algorithm to aggregate their advantages. Unlike blocks of uniform size in original HOG descriptors, this method introduces blocks that vary in size, location, and aspect ratio. From a large set of possible blocks, This method uses Adaboost algorithm to select the appropriate set of blocks to be included in the cascade. This method can achieve comparable performance to the original Dalal and Triggs algorithm <ref type="bibr" target="#b25">[25]</ref>, but operated at speeds up to 70 times faster.</p><p>In this paper, in order to ensure that the head region proposals most probably contain all true heads, we keep a high recall rate by adjusting related parameters of the cascade detector. We will discuss this in Section 3.2 in detail.</p><p>In the following, we refer to the head detector as cascade detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Head classification based on CNN</head><p>The candidate head regions from our cascade detector consist of two parts: real head regions and non-head regions which constitute the dataset2 as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. To some extent, the second part is similar to real heads and it is hard for the cascade detector to determine their labels. Actually, the second part can be considered as hard negative examples as in <ref type="bibr" target="#b30">[30]</ref>. Thus, the following task is how to reliably discriminate real head regions from the hard negative examples.</p><p>Considering the success of CNN on classification tasks, we adopt the CNN to cope with this task. Instead of directly using the CNN as a classifier, we just use the CNN as a feature extractor as in <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b33">33]</ref>, and then the classification is done by training a linear SVM classifier based on the CNN features as in <ref type="bibr" target="#b34">[34]</ref>.</p><p>The basic idea of adopting this strategy is that the CNN model can learn a very useful feature representation and the SVM classifier has good classification performance when giving features.</p><p>Currently, the common strategy of extracting CNN feature is to use an offthe-shelf CNN trained on a large scale dataset, such as ImageNet <ref type="bibr" target="#b35">[35]</ref>, to extract the feature as in <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b33">33]</ref>. This is actually a typical transfer learning strategy and it can gain good performance when the test images are similar to the original dataset of training the CNN. However, this transfer learning strategy will fail for our problem since the ImageNet dataset which usually used to train CNNs is different from our surveillance dataset. Thus, we have to train a new CNN model using the surveillance dataset. To do this, we need a good CNN architecture.</p><p>Unfortunately, the commonly used CNN architecture is not suitable for our case due to two aspects. On one hand, it has usually a very deep structure. This would cause over-fitting concerns for our limited surveillance dataset and meanwhile this deep structure is time-consuming for feature extraction. On the other hand, unlike the images in the ImageNet dataset, our samples are of low resolution, with a minimum size of 16×16.  We use the standard stochastic gradient descent (SGD) algorithm <ref type="bibr" target="#b36">[36]</ref> to learn the parameters of the proposed CNN which includes two stages: a feed forward pass stage and a back propagation pass stage. The learning algorithm of CNN is shown in Algorithm 1.</p><p>Algorithm 1 Learning process of our convolutional neural network h θ (x).</p><p>Input: training set: {(x (1) , y (1) ), . . . , (x (m) , y (m) )} of m training examples from dataset2.</p><p>Output: learned CNN model h θ (x) with parameter θ.</p><p>1. Initialize network parameters θ with random values.</p><p>2. for i:=1 to training epochs for j:=1 to number of batches (a) Feed-forward stage:</p><p>i. Select out k random samples {(x (1) , y (1) ), . . . , (x (k) , y (k) )} from training set.</p><p>ii. Input k samples into h θ (x), calculate response values in layer h l θ with parameters θ (l) .</p><p>(b) Back-forward stage, update network parameters θ with backpropagation:</p><p>i. Calculate the loss function for the softmax layer:</p><formula xml:id="formula_0">J(θ; x, y) = - k i=1 y (i) log h θ (x (i) ) + (1 -y (i) ) log(1 -h θ (x (i)</formula><p>)) . ii. Update network parameters θ: Suppose the error on the l-th layer is: δ (l) = (θ (l) ) T δ (l+1) • f (z (l) ), and then the derivative is:</p><p>∇ θ (l) J(θ; x, y) = δ (l+1) (a (l) ) T , where a (l) is the activation function. Then θ (l) can be updated with:</p><formula xml:id="formula_1">θ (l) = θ (l) -α 1 k ∇ θ (l) J(θ; x, y) + λθ (l)</formula><p>. end for j end for i</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Return h θ (x);</head><p>After obtaining the trained CNN, we apply it to all samples in dataset2 and use the output of the second fully-connected layer as the feature representation of each sample. Then, we train a linear SVM classifier based on CNN features.</p><p>Once we obtain the CNN feature extractor and the SVM classifier, they are always used together. In the following, we denote them as the CNN-SVM classifier for convenience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Online detection for people counting</head><p>In the online detection stage, as described in Section 2.1, firstly we obtain the head proposals by applying the cascade detector to the test image, and then most of false head proposals are rejected by the trained CNN-SVM classifier.</p><p>After that, we use a post-processing module to further increase the precision of the head detection and finally count the final results as the people count.</p><p>In practice, the post-processing module is very important in real applications.</p><p>Taking the classroom surveillance system as an example, because the camera is usually fixed for each each classroom, we can gain much prior knowledge to further reject the false positive results. These prior knowledge includes regions of interest (ROI), size information of heads in different image spaces and so on. However, for fairness, the results of our method are achieved without any post-processing in following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Real surveillance dataset</head><p>Our classroom dataset is collected from our real classroom surveillance system, where some typical scenes are shown in Fig. <ref type="figure" target="#fig_0">1</ref>. Due to the heavy perspective effect, there is a big scale variability. Specifically, the heads in last rows are so small and obscure that it is hard even for humans to discriminate them. Besides, different variabilities of the imaging viewpoints, illumination, head poses, etc. further make it difficult to correctly count the people.</p><p>The training set and the test set are sampled during different times, where the time interval is more than four months. In order to train the cascade de-   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">How many stages should be used for the cascade detector</head><p>The number of the stages of the cascade detector is an important parameter for cascade Adaboost algorithm. In this paper we hope the trained cascade detector has a high recall rate and meanwhile keeps an acceptable accuracy. This is in order to most possibly input all real head objects into the CNN-SVM classifier, while not remarkably increase its computation time. The detection results with different stages are listed in Table <ref type="table" target="#tab_1">1</ref>, where the detection rate and false positive rate are set as 0.995 and 0.5, respectively at each training stage, as the same configurations as in <ref type="bibr" target="#b29">[29]</ref>. From the Table <ref type="table" target="#tab_1">1</ref>, we can observe that the detector with 15 stages has the highest recall rate, with a precision of 0.33 which is comparatively acceptable. Thus, in this paper, we use the cascade detector with 15 stages to detect head candidate regions.    Layer 4, which is the second fully-connected layer of our CNN. Thus, we choose the output of Layer 4 as the feature representation in this paper. Our test set contains 120 images from different classrooms, where we sample 5 images for each classroom at different times. The final people count, as well as the ground truth, of all test images is plotted in Fig. <ref type="figure" target="#fig_10">7</ref>. It can be seen that the people count of our method totally approaches to the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Results on our real surveillance dataset</head><p>Specifically, our method has good performance when the people in a classroom is sparse. When the people becomes dense, the performance begins to degrade because of heave occlusion, as shown in Fig. <ref type="figure">6 (d)</ref>.</p><p>Actually, for our method, the combination of cascade Adaboost algorithm and the CNN-SVM classifier can still be considered as a head detector. The people counting accuracy of our method greatly depends on the head detection performance. In order to evaluate head detection performance of our method, we also compute the related evaluation criteria, including recall and precision as well as the true positive and false positive numbers. We choose the DPM and cascade Adaboost as the baseline methods since they usually are considered as successful object detection methods. Here the cascade Adaboost is of 18 stages, different from the configuration in our method. This is because this configuration can make the cascade Adaboost method has a reasonable precision. The results of different people densities are listed in Table <ref type="table" target="#tab_4">3</ref>. It can be observed that the recall rates of three methods gradually decrease with the density increasing mainly due to heavy occlusion and head pose variability. In contrast, the precisions increase for three methods with the density increasing since the proportion of true positive detections increases. Comparing our method to the cascade Adaboost method, we can find that the recalls of both methods are similar, where ours is only a little higher than the cascade Adaboost method on average. However, the precision of our method is much higher. On average, the precision of the cascade Adaboost method is 0.47, while ours is 0.80. The performance of the DPM method is the worst among three methods. This may be because the advantage of DPM method does not exhibits on the condition of low resolution objects in our task.</p><p>We test our method on a PC with a 2.6GHz CPU (E3400), 4GB RAM and the method is implemented using mixture coding of Matlab and C++, where the Adaboost algorithm is implemented by C++, while the CNN-SVM is by </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>In this paper we propose a head detection based people counting method combining the Adaboost algorithm and the CNN in crowded surveillance environment. Instead of adopting general object region proposal method, Our method uses the cascade Adaboost algorithm to obtain the head region proposals. This strategy can greatly reduce the following classification time. Instead of using the CNN as a classifier, we use it as a feature extractor resorting to its strong ability of feature learning. Then we train a linear SVM to take on the final classification task. Finally, we post-process the detection results to further reject false detection and the people count is obtained by counting the head detection results. Experimental results show that this method has good performance and outperforms the baseline methods of DPM, cascade Adaboost methods.</p><p>In the future, we will extend current method to handle the surveillance scenes with several cameras. Especially, we can use the head pose classification techniques <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b40">40]</ref> and the state-of-the-art classifier techniques <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b42">42]</ref> to improve the head detection performance. Besides, we will integrate tracking techniques into our method to handle the people-flow counting task. For example, after we get the head location, the tracking techniques can be adopted to capture the motion information of peoples. And thus, we can count the people-flow along different direction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples of classroom scenes.</figDesc><graphic coords="5,144.31,134.76,103.11,84.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The framework of our people counting method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>, the proposed people counting framework includes three modules: two off-line training stages and one online detection stage. In the first off-line training stage, we adopt the cascade Adaboost algorithm to learn a fast cascade head detector with the histograms of oriented gradients (HOG) feature [25]. In the second off-line training stage, we train a CNN model using a new dataset constructed from the detection results of applying the cascade head detector to original training dataset. This CNN model is used as a feature extractor in the following step. Applying the CNN feature extractor to the dataset in the second stage, we learn a SVM classifier for head classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Our CNN architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Fig.4 (a) and (b), respectively. In this paper, we denote the training dataset for training the cascade detector by dataset1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a) positive samples for cascade detector (b) negative samples for cascade detector (c) positive samples for CNN classifier (d) negative samples for CNN classifier</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Parts of samples: (a) and (b) are from the dataset1, while (c) and (d) from dataset2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5</head><label>5</label><figDesc>Fig.5 shows several detection results of our cascade detector. It can be seen that almost all heads regions are detected correctly. Although there are still many false positive detection regions, most of them can be rejected by the following CNN-SVM classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>3. 3 .Figure 5 :</head><label>35</label><figDesc>Figure 5: Several detection results of our cascade head detector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 6 Figure 6 :</head><label>66</label><figDesc>Fig.6shows the results of applying the CNN-SVM classifier to detection results in Fig.5. Comparing two group of results, we can observe that our CNN-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The comparison of our people count and the ground truth on the test set with 120 test images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Matlab. On the off-training stage, the training time of Adaboost algorithm is 9.63 minutes for the used dataset1 with 15292 positive samples(24×24) and 24891 negative sample images (704×576), and the training time of CNN-SVM classifier is 108.18 minutes on the used dataset2 with 6985 positive and 5627 negative samples (28×28). On the online-testing stage, we test the 120 images (704×576) from different classrooms, and the number of people in these images varies from 13 to 82. The test time of each image is 1.50s on average.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The comparisons of cascade head detectors with different stages.</figDesc><table><row><cell>Stages</cell><cell>10</cell><cell>11</cell><cell>12</cell><cell>13</cell><cell>14</cell><cell>15</cell><cell>16</cell><cell>17</cell><cell>18</cell><cell>19</cell></row><row><cell cols="11">Recall 0.36 0.46 0.66 0.67 0.82 0.86 0.80 0.80 0.72 0.70</cell></row><row><cell cols="2">Precision 0.08</cell><cell cols="9">0.1 0.16 0.19 0.25 0.33 0.36 0.43 0.47 0.53</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The performance comparisons of different CNN feature maps as feature representa-</figDesc><table><row><cell>tions.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Layer</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell cols="6">Recall 0.53 0.13 0.20 0.74 0.73</cell></row><row><cell cols="3">Precision 0.49 0.46</cell><cell cols="2">0.1 0.81</cell><cell>0.8</cell></row><row><cell cols="6">outputs of different network layer shown in Fig.3. The results are listed in</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 ,</head><label>2</label><figDesc>from which we can observe that the best performance is achieved on</figDesc><table><row><cell>245</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Performance comparisons of different methods, where the cascade Adaboost method is denoted by "Ada" and the Ground Truth is denoted by "GT".</figDesc><table><row><cell></cell><cell>GT</cell><cell></cell><cell cols="2">True positive</cell><cell cols="3">False positive</cell><cell>Recall</cell><cell>Precision</cell></row><row><cell></cell><cell></cell><cell cols="8">DPM Ada Ours DPM Ada Ours DPM Ada Ours DPM Ada Ours</cell></row><row><cell></cell><cell>13</cell><cell>3</cell><cell>10</cell><cell>11</cell><cell>5</cell><cell>79</cell><cell>11</cell><cell cols="2">0.23 0.77 0.85 0.37 0.13 0.50</cell></row><row><cell></cell><cell>21</cell><cell>3</cell><cell>20</cell><cell>18</cell><cell>2</cell><cell>87</cell><cell>10</cell><cell cols="2">0.14 0.95 0.86 0.60 0.23 0.64</cell></row><row><cell></cell><cell>21</cell><cell>1</cell><cell>15</cell><cell>15</cell><cell>3</cell><cell>75</cell><cell>11</cell><cell cols="2">0.05 0.71 0.71 0.25 0.20 0.57</cell></row><row><cell>Low</cell><cell>21</cell><cell>0</cell><cell>15</cell><cell>17</cell><cell>0</cell><cell>27</cell><cell>3</cell><cell cols="2">0.00 0.71 0.81 0.00 0.56 0.85</cell></row><row><cell>(&lt; 30)</cell><cell>22</cell><cell>2</cell><cell>19</cell><cell>19</cell><cell>4</cell><cell>34</cell><cell>7</cell><cell cols="2">0.09 0.86 0.86 0.33 0.56 0.73</cell></row><row><cell></cell><cell>22</cell><cell>2</cell><cell>13</cell><cell>19</cell><cell>5</cell><cell>67</cell><cell>13</cell><cell cols="2">0.09 0.59 0.86 0.29 0.19 0.59</cell></row><row><cell></cell><cell>23</cell><cell>3</cell><cell>16</cell><cell>18</cell><cell>2</cell><cell>67</cell><cell>7</cell><cell cols="2">0.13 0.69 0.78 0.60 0.24 0.72</cell></row><row><cell></cell><cell>26</cell><cell>2</cell><cell>20</cell><cell>19</cell><cell>1</cell><cell>29</cell><cell>2</cell><cell cols="2">0.08 0.77 0.73 0.67 0.69 0.90</cell></row><row><cell></cell><cell>35</cell><cell>2</cell><cell>26</cell><cell>27</cell><cell>1</cell><cell>51</cell><cell>2</cell><cell cols="2">0.06 0.74 0.77 0.67 0.51 0.93</cell></row><row><cell></cell><cell>40</cell><cell>4</cell><cell>32</cell><cell>30</cell><cell>3</cell><cell>56</cell><cell>2</cell><cell cols="2">0.10 0.80 0.75 0.57 0.57 0.94</cell></row><row><cell></cell><cell>44</cell><cell>3</cell><cell>29</cell><cell>32</cell><cell>3</cell><cell>47</cell><cell>4</cell><cell cols="2">0.07 0.66 0.73 0.50 0.62 0.89</cell></row><row><cell></cell><cell>47</cell><cell>3</cell><cell>36</cell><cell>34</cell><cell>7</cell><cell>75</cell><cell>10</cell><cell cols="2">0.06 0.77 0.72 0.30 0.48 0.77</cell></row><row><cell>Middle</cell><cell>47</cell><cell>4</cell><cell>39</cell><cell>35</cell><cell>1</cell><cell>63</cell><cell>5</cell><cell cols="2">0.08 0.83 0.74 0.80 0.62 0.88</cell></row><row><cell>(&lt; 50)</cell><cell>48</cell><cell>5</cell><cell>37</cell><cell>36</cell><cell>7</cell><cell>60</cell><cell>6</cell><cell cols="2">0.10 0.77 0.75 0.42 0.62 0.86</cell></row><row><cell></cell><cell>48</cell><cell>4</cell><cell>31</cell><cell>32</cell><cell>2</cell><cell>60</cell><cell>1</cell><cell cols="2">0.08 0.66 0.67 0.67 0.52 0.97</cell></row><row><cell></cell><cell>49</cell><cell>2</cell><cell>34</cell><cell>37</cell><cell>1</cell><cell>58</cell><cell>14</cell><cell cols="2">0.04 0.69 0.75 0.67 0.59 0.73</cell></row><row><cell></cell><cell>50</cell><cell>2</cell><cell>37</cell><cell>37</cell><cell>1</cell><cell>61</cell><cell>3</cell><cell cols="2">0.04 0.74 0.74 0.67 0.61 0.93</cell></row><row><cell></cell><cell>57</cell><cell>4</cell><cell>40</cell><cell>41</cell><cell>3</cell><cell>85</cell><cell>3</cell><cell cols="2">0.07 0.70 0.72 0.57 0.47 0.93</cell></row><row><cell></cell><cell>57</cell><cell>4</cell><cell>37</cell><cell>34</cell><cell>3</cell><cell>60</cell><cell>5</cell><cell cols="2">0.07 0.65 0.60 0.57 0.62 0.87</cell></row><row><cell></cell><cell>57</cell><cell>4</cell><cell>31</cell><cell>36</cell><cell>2</cell><cell>79</cell><cell>10</cell><cell cols="2">0.07 0.54 0.63 0.67 0.39 0.78</cell></row><row><cell>High</cell><cell>57</cell><cell>6</cell><cell>38</cell><cell>41</cell><cell>0</cell><cell>75</cell><cell>4</cell><cell cols="2">0.11 0.67 0.62 1.00 0.51 0.84</cell></row><row><cell>(&gt; 50)</cell><cell>58</cell><cell>5</cell><cell>41</cell><cell>40</cell><cell>5</cell><cell>80</cell><cell>4</cell><cell cols="2">0.09 0.71 0.61 0.50 0.51 0.84</cell></row><row><cell></cell><cell>60</cell><cell>4</cell><cell>41</cell><cell>41</cell><cell>0</cell><cell>79</cell><cell>9</cell><cell cols="2">0.07 0.68 0.59 1.00 0.52 0.83</cell></row><row><cell></cell><cell>82</cell><cell>1</cell><cell>47</cell><cell>57</cell><cell>2</cell><cell cols="2">113 11</cell><cell cols="2">0.01 0.57 0.57 0.33 0.42 0.82</cell></row><row><cell cols="7">average 41.87 3.04 29.33 30.25 2.62 65</cell><cell>10</cell><cell cols="2">0.08 0.72 0.73 0.54 0.47 0.80</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment This work is supported by the National Natural Science Foundation of China (No. 61571071, 61102131, 61275099), the Natural Science Foundation of Chongqing Science and Technology Commission (No. cstc2014jcyjA40048), Wenfeng innovation and start-up project of Chongqing University of Posts and Telecommunications (No. WF201404), Chongqing key science and technology project (No. cstc2014jcsf70001).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Counting pedestrians in video sequences using trajectory clustering, Circuits and Systems for Video Technology</title>
		<author>
			<persName><forename type="first">G</forename><surname>Antonini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Thiran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1008" to="1020" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Counting people by clustering person detector outputs</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Topkaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Video and Signal Based Surveillance (AVSS), 2014 11th IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="313" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to count objects in images</title>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1324" to="1332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Counting people with low-level features and bayesian regression, Image Processing</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2160" to="2177" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Advanced video and signal-based surveillance (AVSS)</title>
		<author>
			<persName><forename type="first">P</forename><surname>Morerio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Marcenaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Regazzoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Ninth International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="476" to="480" />
		</imprint>
	</monogr>
	<note>People count estimation in small crowds</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-source multi-scale counting in extremely dense crowd images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Saleemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Seibert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2547" to="2554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cross-scene crowd counting via deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Joint deep learning for pedestrian detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2056" to="2063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A method for counting people in crowded scenes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Conte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Foggia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Percannella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tufano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Video and Signal Based Surveillance (AVSS), 2010 Seventh IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="225" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robust head-shoulder detection by pca-based multilevel hog-lbp detector for people counting</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m">20th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2069" to="2072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A new edge feature for head-shoulder detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Processing (ICIP), 2013 20th IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2822" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Estimating the number of people in crowded scenes by mid based foreground segmentation and head-shoulder detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR 2008. 19th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
	<note>Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rapid and robust human detection and tracking based on omega-shape features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="2545" to="2548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Head detection in stereo data for people counting and segmentation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Van Oosterhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bakkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Kröse</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>VISAPP</publisher>
			<biblScope unit="page" from="620" to="625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast people counting using head detection from skeleton graph</title>
		<author>
			<persName><forename type="first">D</forename><surname>Merad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-E</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Video and Signal Based Surveillance (AVSS), 2010 Seventh IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Pedestrian head detection and tracking using skeleton graph for people counting in crowded environments</title>
		<author>
			<persName><forename type="first">K.-E</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Merad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fertil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>MVA</publisher>
			<biblScope unit="page" from="516" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Head detection for video surveillance based on categorical hair and skin colour models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gunes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Piccardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Processing</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="1137" to="1140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An accurate algorithm for head detection based on xyz and hsv hair and skin color models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gunes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Piccardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008. 2008. 2008</date>
			<biblScope unit="page" from="1644" to="1647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Counting people in the crowd using a generic head detector, in: Advanced Video and Signal-Based Surveillance (AVSS)</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">B</forename><surname>Subburaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Descamps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Carincotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Ninth International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="470" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Brain-inspired classroom occupancy monitoring on a low-power mobile platform</title>
		<author>
			<persName><forename type="first">F</forename><surname>Conti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pullini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Benini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops (CVPRW), 2014 IEEE Conference</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="624" to="629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A discriminatively trained, multiscale, deformable part model</title>
		<author>
			<persName><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>CVPR 2008. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Empirical analysis of detection cascades of boosted classifiers for rapid object detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lienhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kuranov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pisarevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
	<note>CVPR 2005</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bing: Binarized normed gradients for objectness estimation at 300fps</title>
		<author>
			<persName><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3286" to="3293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Joint haar-like features for face detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tenth IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1619" to="1626" />
		</imprint>
	</monogr>
	<note>Computer Vision, 2005. ICCV 2005</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Boosting local binary pattern (lbp)-based face recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in biometric person authentication</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="179" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fast human detection using a cascade of histograms of oriented gradients</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-C</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1491" to="1498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models, Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.1531</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning and transferring midlevel image representations using convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1717" to="1724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cnn features off-theshelf: an astounding baseline for recognition</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops (CVPRW), 2014 IEEE Conference</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="512" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COMPSTAT&apos;2010</title>
		<meeting>COMPSTAT&apos;2010</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A multi-task learning framework for head pose estimation under target motion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Lanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">No matter where you are: Flexible graph-guided multi-task learning for multi-view head pose classification under target motion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Lanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="1177" to="1184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Active transfer learning for multi-view head-pose classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Lanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition (ICPR), 2012 21st International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1168" to="1171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">On the relationship between head pose, social attention and personality prediction for unstructured and dynamic group interactions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Staiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Lanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM on International conference on multimodal interaction</title>
		<meeting>the 15th ACM on International conference on multimodal interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Complex event detection using semantic saliency and nearly-isotonic svm</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML-15</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML-15</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1348" to="1357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Compound rankk projections for bilinear analysis, Neural Networks and Learning Systems</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2015.2441735</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
