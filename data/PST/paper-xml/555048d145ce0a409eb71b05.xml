<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Deep Representations for Graph Clustering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Fei</forename><surname>Tian</surname></persName>
							<email>tianfei@mail.ustc.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Bin</forename><surname>Gao</surname></persName>
							<email>bingao@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Qing</forename><surname>Cui</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
							<email>cheneh@ustc.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Yan</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Deep Representations for Graph Clustering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently deep learning has been successfully adopted in many applications such as speech recognition and image classification. In this work, we explore the possibility of employing deep learning in graph clustering. We propose a simple method, which first learns a nonlinear embedding of the original graph by stacked autoencoder, and then runs k-means algorithm on the embedding to obtain clustering result. We show that this simple method has solid theoretical foundation, due to the similarity between autoencoder and spectral clustering in terms of what they actually optimize. Then, we demonstrate that the proposed method is more efficient and flexible than spectral clustering. First, the computational complexity of autoencoder is much lower than spectral clustering: the former can be linear to the number of nodes in a sparse graph while the latter is super quadratic due to eigenvalue decomposition. Second, when additional sparsity constraint is imposed, we can simply employ the sparse autoencoder developed in the literature of deep learning; however, it is nonstraightforward to implement a sparse spectral method. The experimental results on various graph datasets show that the proposed method significantly outperforms conventional spectral clustering, which clearly indicates the effectiveness of deep learning in graph clustering.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning has been a hot topic in the communities of machine learning and artificial intelligence. Many algorithms, theories, and large-scale training systems towards deep learning have been developed and successfully adopted in real tasks, such as speech recognition <ref type="bibr" target="#b8">(Dahl et al. 2012)</ref>, image classification <ref type="bibr" target="#b13">(Krizhevsky, Sutskever, and Hinton 2012)</ref>, and natural language processing <ref type="bibr" target="#b7">(Collobert et al. 2011)</ref>. However, to our knowledge, the adoption of deep learning in clustering has not been adequately investigated yet. The goal of this work is to conduct some preliminary investigations along this direction.</p><p>Clustering aims to group similar patterns among massive data points. Graph clustering is a key branch of clustering, which tries to find disjoint partitions of graph nodes such that the connections between nodes within the same partition are much denser than those across different partitions. On one hand, many real-world problems can be cast as graph clustering such as image segmentation <ref type="bibr" target="#b19">(Shi and Malik 2000)</ref>, community detection <ref type="bibr" target="#b20">(Smyth and White 2005)</ref>, and VLSI design <ref type="bibr" target="#b5">(Chan, Schlag, and Zien 1994)</ref>; on the other hand, it is easy to transform a clustering problem in the vector space to a clustering problem on the similarity graph built from the vector representations of the data points. Therefore, we choose to put our focus on graph clustering in this work, and in particular, we investigate the use of stacked sparse autoencoder to perform graph clustering.</p><p>Our proposal is motivated by the similarity between autoencoder and spectral clustering, a state-of-the-art graph clustering method, in terms of what they actually optimize. Among many existing graph clustering algorithms <ref type="bibr" target="#b12">(Karypis and Kumar 1998)</ref> <ref type="bibr" target="#b19">(Shi and Malik 2000)</ref> (Van Dongen 2000) <ref type="bibr" target="#b9">(Dhillon, Guan, and Kulis 2007)</ref> <ref type="bibr" target="#b18">(Satuluri and Parthasarathy 2009)</ref>, spectral clustering has attracted people's great attention in the past decades due to its solid theoretical foundation and global optimal solution. Given an n-node graph, spectral clustering method runs an Eigenvalue Decomposition (EVD) on the normalized graph Laplacian matrix; then the eigenvectors corresponding to the k smallest non-zero eigenvalues are extracted as the representation of the graph nodes, where k is the predefined number of clusters;<ref type="foot" target="#foot_0">1</ref> after that, a k-means method is run on the graph representations to get the clusters results.</p><p>Note that these k eigenvectors are also the eigenvectors of the normalized graph similarity matrix, whereas corresponding to its k largest eigenvalues. Therefore these eigenvectors can be regarded as an encoding of the normalized graph similarity matrix, and according to the Eckart-Young-Mirsky theorem, this encoding can lead to the optimal rank-k reconstruction of the original normalized graph similarity matrix. People familiar with autoencoder may immediately realize that spectral clustering is very similar to autoencoder: autoencoder also attempts to find a low-dimensional encoding of the input data that can keep the most information of the original set of data through reconstruction. Actually, our theoretical study shows that the objective functions of spectral clustering and autoencoder are actually very similar when they are used to solve graph clustering problems (i.e., the reconstruction error of the normalized similarity matrix under the Frobenuis norm by the encoding), and both of them can achieve the desired solution when the optimization processes are well done. <ref type="foot" target="#foot_1">2</ref>While autoencoder is similar to spectral clustering in theory, it is much more efficient and flexible in practice. First, as we know, spectral clustering is computationally expensive because it involves an eigenvalue decomposition (EVD). The complexity of a straightforward implementation of EVD is cubic to the number of nodes in the graph, while even the fastest implementation to our knowledge requires a super-quadratic computational complexity. In contrast, autoencoder is highly efficient due to its back-propagation framework, whose computational complexity can be linear to the number of nodes in the graph when the nodes are sparsely connected. Second, When dealing with very largescale data, we usually hope to get some sparse representations so as to improve the efficiency of the data processing. However, the encoding produced by spectral clustering cannot guarantee the sparsity property because the eigenvectors of the graph Laplacian are probably very dense. In addition, it is non-straightforward to incorporate sparsity constraints into spectral clustering without destroying its nature of being a spectral method. In contrast, it is very easy to fulfill the sparsity requirement by using the autoencoder. Actually, one can simply use the sparse autoencoder developed in the literature of deep learning for this purpose, which basically introduces a L 1 regularization term to the original objective function of autoencoder. Furthermore, one can stack multiple layers of sparse autoencoders, to achieve additional benefit from deep structures.</p><p>Based on the above discussions, we propose a method called GraphEncoder for graph clustering. First, we feed the normalized graph similarity matrix into a deep neural network (DNN) which takes sparse autoencoder as the building block. Then through a greedy layer-wise pretraining process, we seek the best non-linear graph representations that can approximate the input matrix through reconstruction and achieve the desired sparsity properties. After stacking several layers of sparse autoencoders, we run kmeans on the sparse encoding output by the final layer to obtain the clustering results. To verify the effectiveness of the proposed method, we conducted extensive experiments on various real-world graph datasets. The experimental results show that the proposed algorithm can significantly out-perform the baseline algorithms like spectral clustering. The results also indicate that the stacked deep structure can boost the clustering results, i.e., the results become more and more accurate when going from the shallow layers to the deep layers.</p><p>To the best of our knowledge, this is the first work that investigates how to use deep learning for graph clustering. It enriches our understanding on the power of deep learning, by opening a door to use unsupervised pre-training techniques like stacked sparse autoencoder to deal with the clustering problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Related Work</head><p>Given an undirected weighted graph G = (V, E), where V = {v 1 , v 2 , ..., v n } is the node set and E = {e i j } is the edge set, graph clustering aims to find a disjoint partition {V i } k i=1 of V , where k is cluster number. As has been mentioned before, our proposed model is highly related to spectral clustering and deep learning, so we will briefly review these two methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Spectral Clustering</head><p>We use S = {s i j } to denote the similarity matrix of graph G, and thus s i j (i, j = 1, 2, ? ? ? , n) is the similarity score between node i and j. Let d i = ? j s i j be the degree of node i, based on which we measure the capacity of a subset A of V , i.e., vol(A) = ? i?A d i . For any disjoint subsets A, B ? V , we define link(A, B) = 1 2 ? i?A, j?B s i j . One of the commonly-used clustering objective that spectral clustering aims to minimize is the Normalized Cut (NCut):</p><formula xml:id="formula_0">NCut(V 1 , ...,V k ) = k ? i=1 link(V i , Vi ) vol(V i ) .<label>(1)</label></formula><p>Here V i ? Vi = / 0 and V i ? Vi = V . To achieve this goal, spectral clustering converts the above objective function to the following discrete optimization problem: </p><formula xml:id="formula_1">H i j = 1/ vol(V j ), i f v i ? V j 0, otherwise , (i = 1, 2, ? ? ? , n; j = 1, 2, ? ? ? , k); D = diag(d 1 , d 2 , ? ? ? , d n ); L = D -S.</formula><p>(</p><formula xml:id="formula_2">)<label>2</label></formula><p>Here L is the so-called graph Laplacian matrix. It can be seen that the discrete optimization problem in ( <ref type="formula" target="#formula_2">2</ref>) is NP-Hard <ref type="bibr" target="#b25">(Wagner and Wagner 1993)</ref>. Therefore, spectral clustering turns to relax the condition in (2) by allowing H i j to take any real values. According to some matrix calculus, the solution yields H to consist the eigenvectors corresponding to the k smallest non-zero eigenvalues of the normalized Laplacian matrix D -1 L. The final clustering results are then obtained through running the k-means algorithm on the graph embedding matrix H.</p><p>There are works towards efficient implementation of spectral clustering such as <ref type="bibr" target="#b6">(Chen et al. 2011)</ref>, in which the authors' aim is to construct a sparse similarity graph as the input to the parallelized spectral clustering method, whereas what we aim to achieve is to replace the expensive EVD in spectral clustering, leading to the difference with former works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep Learning</head><p>Recently deep learning has won great success in many applications such as image classification, speech recognition, and natural language processing <ref type="bibr" target="#b1">(Bengio 2009</ref>) <ref type="bibr" target="#b8">(Dahl et al. 2012</ref>) <ref type="bibr" target="#b7">(Collobert et al. 2011)</ref>  <ref type="bibr" target="#b13">(Krizhevsky, Sutskever, and Hinton 2012)</ref>. One of the strategies that make training deep architectures possible and effective is the greedy layerwised unsupervised pretraining <ref type="bibr" target="#b10">(Hinton and Salakhutdinov 2006</ref>) <ref type="bibr" target="#b0">(Bengio et al. 2007</ref>). This strategy aims to learn useful representations one layer at a time, and then to set the output features to be the input of the next layer. Each layer in this process involves some kind of non-linearity, such as nonlinear activation function (e.g., sigmoid or tanh) or some regularization on features (e.g., sparsity constraints <ref type="bibr" target="#b15">(Poultney et al. 2006))</ref>. By stacking these non-linear single layers together, deep learning are believed to yield better representations (Part 4 in <ref type="bibr" target="#b0">(Bengio, Courville, and Vincent 2013)</ref>).</p><p>In the greedy layerwised pretraining process, autoencoder <ref type="bibr" target="#b4">(Bourlard and Kamp 1988)</ref>  <ref type="bibr" target="#b11">(Hinton and Zemel 1994)</ref> is commonly used as a basic unit to generate new representations in each layer, and it is also the main building block in our model. In the autoencoder framework, a feature extraction function f (?; ? 1 ) is firstly implemented on original feature vector</p><formula xml:id="formula_3">x i , i = 1, 2, ? ? ? , n (n is the number of train- ing samples), yielding a new representation f (x i ; ? 1 ). Func- tion f (?; ? 1 ) is named as encoder. After that, f (x i ; ? 1</formula><p>) is transformed back into the input space by another function g(?; ? 2 ), which is called as decoder. The aim of autoencoder is to minimize the reconstruction loss between the original data and the reconstructed data from the new representations, i.e.,</p><formula xml:id="formula_4">Loss(? 1 , ? 2 ) = N ? i=1 l (x i , g ( f (x i ; ? 1 ); ? 2 )) .</formula><p>(3)</p><p>Here l(?) is the sample-wise loss function. Usually encoder and decoder are composed of a linear transformation, followed by an activation function. That is, f</p><formula xml:id="formula_5">(x; ? 1 ) = f 0 (W x + b), g(x; ? 2 ) = g 0 (Mx + d)</formula><p>, where f 0 and g 0 are activation functions like the element-wise sigmoid function. In this sense, ? 1 = {W, b} and ? 2 = {M, d} are the parameters to be learned in the training process. Furthermore, there are works on sparse autoencoder, which aims to penalize the large hidden layer outputs <ref type="bibr" target="#b14">(Olshausen and Field 1997</ref>) <ref type="bibr">(Boureau, Cun, and others 2007)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model Description</head><p>In this section, we introduce our proposed deep learning method for graph clustering, including the motivation, the basic building block, and the implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motivation</head><p>As mentioned in the introduction, our proposal is motivated by the similarity between autoencoder and spectral clustering. To show this, we will first explain spectral clustering in the viewpoint of matrix reconstruction, and then demonstrate that autoencoder is a better choice than spectral clustering in the scenario of large-scale graph clustering. We start with some notations. Let Q = D -1 L, where L is the graph Laplacian matrix and D is the diagonal matrix with the node degrees in the corresponding diagonal elements. According to the properties of the Laplacian matrix, Q is symmetric and its rank is assumed to be nr, where r is the number of connected components in the graph G. We write the eigenvalue decomposition of</p><formula xml:id="formula_6">Q as Q = Y ?Y T , where Y ? R n?n stacks the N eigenvectors of Q in columns and Y T Y = I, ? = diag(? 1 , ? 2 , ? ? ? , ? n ), ? 1 &gt; ? 2 &gt; ? ? ? &gt; ? n</formula><p>are the n eigenvalues of Q. Let ? k denote the diagonal matrix with the k smallest non-zero eigenvalues in ? in its diagonal elements, and Y k ? R n?k be the matrix containing the k columns from Y corresponding to the k non-zero eigenvalues in ? k . Thus Y k is the embedding matrix used in spectral clustering when minimizing the normalized cut in order to cluster the graph nodes into k groups.</p><p>Note that the non-zero requirement on the eigenvalues does not impact the clustering result by much when k is relatively large. The reason is as follows. The zero eigenvalue of Q corresponds to the eigenvector with all its elements equal to 1. If we put this eigenvector into Y k , it will have no effect on the clustering result except that it will squeeze out the least informational eigenvector. For simplicity, in the following discussions we will directly talk about the k smallest eigenvalues in ? without requiring the eigenvalues to be non-zero. Further note that</p><formula xml:id="formula_7">Q = D -1 L = D -1 (D -S) = I -D -1 S.</formula><p>Therefore, the k smallest eigenvalues of Q are exactly the k largest eigenvalues of the normalized graph similarity matrix D -1 S, and accordingly Y k contains the k eigenvectors of D -1 S corresponding to its k largest eigenvalues. The Eckart-Young-Mirsky Theorem <ref type="bibr" target="#b10">(Eckart and Young 1936)</ref> explains the reconstruction nature of spectral clustering, which is related to the low-rank approximation of a matrix.</p><p>Theorem 1 (Eckart-Young-Mirsky). For a rank-r matrix P ? R m?n , with singular value decomposition (SVD) P = U?V T , U T U = I, V T V = I; if k &lt; r, we have:</p><formula xml:id="formula_8">arg min P ? R m?n rank( P) = k ||P -P|| F = U ?V T (4)</formula><p>where ? is the same matrix as ? except that it contains only the k largest singular values and the other singular values are replaced with 0.</p><p>The above theorem shows that the matrix reconstruction by the truncated largest k singular vectors in SVD is the best rank-k approximation of the original matrix. Furthermore, if a matrix Z is symmetric, i.e., Z = PP T , there exists orthogonal decomposition Z = U? 2 U T =U?U T , where U T U = I and ? is diagonal matrix with the eigenvalues of Z in the diagonal elements. This is the well-known fact that the SVD of a matrix P is highly related to the EVD of the symmetric matrix PP T . Specifically, for symmetric matrix Z, the matrix reconstruction by the truncated largest k eigenvectors in EVD is also the best rank-k approximation under the Frobe-nuis norm. According to the above discussions, we obtain the following corollary.</p><formula xml:id="formula_9">Corollary 2 Y k ? k Y T</formula><p>k is the best reconstruction of the symmetric matrix D -1 S in terms of the Frobenuis norm among all rank-k matrices, where Y k is the embedding matrix in spectral clustering.</p><p>On one hand, Corollary 2 tells us that spectral clustering can be regarded as a process of matrix reconstruction for D -1 S. From the new embedding matrix Y k obtained through eigenvalue decomposition, it can build the best rank-k matrix approximation towards the normalized graph similarity matrix D -1 S in terms of Frobenuis norm. On the other hand, imagine that if we use the normalized graph similarity matrix D -1 S as the input feature matrix for an autoencoder, we are actually also seeking for the best matrix reconstruction for D -1 S in terms of Frobenuis norm by solving the autoencoder. That is, both autoencoder and spectral clustering minimize the reconstruction error for the original normalized similarity matrix D -1 S. We therefore say that autoencoder and spectral clustering are similar in terms of what they optimize.</p><p>While autoencoder is similar to spectral clustering in theory, the former is much more flexible in incorporating additional constraints. In many real large-scale graph clustering problems, it is desirable to seek some sparse representations as the graph embedding. On one aspect, this can greatly improve the efficiency of the system in terms of both storage and data processing. On another aspect, it usually also improves the clustering accuracy, since it can remove some noisy information that hurts the clustering results. However, the embedding produced by spectral clustering cannot guarantee the sparsity property since the eigenvectors of Q are probably very dense. Furthermore, it is non-straightforward to introduce sparsity constraint to spectral clustering; if we do it in a stiff way (e.g., directly add a sparsity constraint to the objective function of spectral clustering), it is very likely that we cannot get a spectral method any more. In sharp contrast, we will have a much easier life if we use autoencoder. That is, we can simply adopt the sparse autoencoder developed in the literature of deep learning. It basically introduces a sparsity regularization term to the original objective function of autoencoder and can still benefit from the efficient back-propagation algorithm for the optimization. Furthermore, one can stack multiple layers of sparse autoencoders, to achieve additional benefit from deep structures. The above discussions motivate us to adopt an autoencoderbased method for graph clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">GraphEncoder</head><p>In this subsection we introduce the autoencoder-based graph clustering model called GraphEncoder. In general, the key component of GraphEncoder is a deep neural network with sparse autoencoder as its building block. As stated in the previous sections, given an n-node graph G with its similarity matrix S, we can treat S as the training set contain- </p><formula xml:id="formula_10">ing n instances s 1 , s 2 , ? ? ? , s n , s i ? R n , i = 1, 2, ? ? ? , n. Note that s i = {s i j }, j = 1, 2, ? ? ? , n.</formula><formula xml:id="formula_11">matrix D = diag{d 1 , d 2 , ? ? ? , d n }</formula><p>, where d i is the degree of node i; DNN layers number ?, with number of nodes n ( j) in layer j; n (1) = n; X ( j) ? R n?n ( j) is the input to layer j. X (1) = D -1 S. For j = 1 to ? 1. Build a three layer sparse autoencoder with input data X ( j) . 2. Train the sparse autoencoder by optimizing (6) with backpropagation. Obtain the hidden layer activations h ( j) . 3. Set X ( j+1) = h ( j) .</p><formula xml:id="formula_12">End Run k-means on X ? ? R n?n (?) . Output Final clustering result.</formula><p>output features in the deepest layer of DNN as the graph embedding. At last, k-means clustering is implemented on the graph embedding of G to produce the final clustering result.</p><p>Specifically, we consider the autoencoder in each layer of the deep neural network. Let x i be the i-th input vector of this layer, and f 0 and g 0 be the activations of the hidden layer and the output layer respectively. We have h i = f 0 (W x i + b) and y i = g 0 (Mh i + d), where ? = {? 1 , ? 2 } = {W, b, M, d} are the parameters to be learned, f 0 and g 0 are the non-linear operators such as the sigmoid function (sigmoid(z) = 1/(1 + exp(-z))) or tanh function (tanh(z) = (e ze -z )/(e z + e -z )). Then the optimization goal is to minimize the reconstruction error between the original data x i and the reconstructed data y i from the new representation h i ,</p><formula xml:id="formula_13">arg min ? ?? n ? i=1 ||y i -x i || 2 .</formula><p>(5)</p><p>We also impose the sparsity constraints to the activation in the hidden layer. That is, we add a regularization term to the reconstruction error in (5),</p><formula xml:id="formula_14">Loss(? ) = n ? i=1 ||y i -x i || 2 + ? KL(?|| ?), (<label>6</label></formula><formula xml:id="formula_15">)</formula><p>where ? controls the weight of the sparsity penalty, ? is set to be a small constant such as 0.01, ? = 1 n ? n j=1 h j is the average of the hidden layer activations, and KL(?|| ?) is defined as:</p><formula xml:id="formula_16">KL(?|| ?) = | ?| ? j=1 ?log ? ? j + (1 -?)log 1 -? 1 -? j . (<label>7</label></formula><formula xml:id="formula_17">)</formula><p>Note that ( <ref type="formula" target="#formula_14">6</ref>) can be solved by standard back-propagation algorithms. After the training of the current layer is completed, we use the hidden layer activations as the inputs to train the next layer. This greedy layer-wise training process forms the model of the Stacked Sparse Autoencoder <ref type="bibr" target="#b23">(Vincent et al. 2010)</ref>. When all the layers are trained in this manner, we use the output of the final layer as the new graph representation and run k-means on it to get the clustering results. The whole procedure is summarized as the algorithm in Table <ref type="table" target="#tab_0">1</ref>.</p><p>The proposed model GraphEncoder has the following advantages:</p><p>? Usually in autoencoder, the dimension of the hidden layer is lower than that of the input layer. This captures the intuition that not all edges in the original graph are necessary in clustering. For a certain node, it is possible that only its relationships with part of the other nodes (e.g., the nodes with the highest degrees) determine which cluster it should belong to. The optimization of (5) captures this useful low dimensional information. ? The sparsity penalty in ( <ref type="formula" target="#formula_14">6</ref>) not only strengthens the requirements of deleting edges as stated above, but also makes the computation more efficient given the sparsity target ? is a small value and the activations approach zero. ? The stacked structures provide a smooth way of eliminating edges. Graph representations are expected to become clearer and clearer for clustering as the training goes from shallow layers to deep layers. We will use an example in the experimental session to demonstrate this effect of the deep structure. ? GraphEncoder is much more efficient than spectral clustering. Spectral clustering relies on EVD. To our knowledge, the computational complexity of the fastest EVD solver is O(n 2.367 ) where n is the number of nodes in the graph, when the graph has some special sparse and dense structure like Toeplitz matrix <ref type="bibr" target="#b15">(Pan and Chen 1999)</ref>. In contrast, it is not difficult to see that the complexity of GraphEncoder is O(ncd), where d is the maximum number of hidden layer nodes in DNN, and c is the average degree of the graph. Usually c can be regarded as a fixed value: for example, in the top-k similarity graph, c = k; and in a social network graph, c, the average friend number of people, is also bounded. Parameter d is related to the predefined number of clusters (more clusters lead to larger d), but not related to n. Therefore we can regard cd as some constant that does not increase with n, and the overall complexity of GraphEncoder is linear to the number of nodes. In addition, whereas EVD is hard to parallel, the stochastic gradient descent (SGD) <ref type="bibr" target="#b2">(Bottou 1998</ref>) training of DNN is comparatively easy to parallel, as have been well explored in the literature of DNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Evaluation</head><p>We report the experimental results in this section. We first introduce the datasets and the benchmark algorithms used in the experiments, and then give the experiment settings. After that, we show the clustering performance of our proposed clustering algorithm compared with benchmark algorithms.</p><p>In addition, we give an example to show the power of deep structures in clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>To test the performance of our DNN-based model, we evaluated its performance on several real world datasets.</p><p>1. Wine. This is a dataset from UCI Machine Learning Repository <ref type="bibr" target="#b0">(Asuncion and Newman 2007)</ref>, consisting of 178 instances with 13 attributes. Every instance corresponds to a certain wine with its chemical analysis information as the attributes. All instances are labeled with 3 wine categories. We built a cosine similarity graph using these instances and used the labels as the groundtruth.</p><p>2. 20-Newsgroup. This dataset is a collection of about 20,000 newsgroup documents. The documents are partitioned into 20 different groups according to their topics. We represented every document as a vector of tf-idf scores of each word and built the cosine similarity graph based on the tf-idf scores. To demonstrate the robustness of our algorithms with different targeting cluster numbers, we constructed three graphs built from 3, 6, and 9 different newsgroups respectively. The newsgroup names in each graph are listed as the following, where the abbreviation NG used in graph names is short for Newsgroup. For each chosen group, we randomly selected 200 documents from it, and thus the three graphs contain 600, 1,200, and 1,800 nodes respectively. The document labels are used as the groundtruth.</p><p>3. DIP. This is an unweighted protein-protein interaction (PPI) network from the Database of Interacting Proteins <ref type="bibr" target="#b17">(Salwinski et al. 2004</ref>). The average degree of nodes is about 6.</p><p>4. BioGrid. The last dataset is another PPI network obtained from the BioGrid Database <ref type="bibr" target="#b21">(Stark et al. 2011)</ref>. We removed the nodes without any connections to other nodes from the original graph and got a final graph of 5,964 nodes. The average degree is approximately 65, which is much higher than that of DIP. We used the protein complex data in CYC2008 <ref type="bibr" target="#b16">(Pu et al. 2009)</ref> as the groundtruth for the two PPI networks.</p><p>The detailed information of all these graphs are summarized in Table <ref type="table" target="#tab_2">2</ref>. Since all these datasets have groundtruth, we evaluated the performance of a clustering algorithm by the Normalized Mutual Information (NMI) of its clustering results. The range of NMI values is [0, 1]. The higher the NMI is, the better the corresponding clustering results are. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Benchmark Algorithms</head><p>We used the following graph clustering methods as the benchmark algorithms.</p><p>1. Spectral Clustering. We used two versions of spectral clustering: unnormalized spectral clustering (Von Luxburg 2007), which aims to minimize ratio cut, and normalized spectral clustering <ref type="bibr" target="#b19">(Shi and Malik 2000)</ref>, which aims to minimize normalized cut. The better NMI value output by the two versions was recorded as the benchmark results.</p><p>2. k-means. Our algorithm runs k-means on the new graph embedding in the deep layers. To show the power of the deep structures, we also run the k-means algorithm directly on the original graph (i.e., k-means on the n ? n normalized similarity matrix) as a benchmark method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiment Settings</head><p>We implemented GraphEncoder based on Sparse Autoencoder (SAE), which is the autoencoder model that penalizes both the reconstruction error and the sparsity error in the hidden layer. In our experiments, we tuned two parameters of SAE: sparsity penalty, which tradeoffs the weight of the two errors in the optimization, and sparsity target, which is the target value that the hidden layer activations aim to reach. The neural networks for Wine and 3-NG have 3 layers and the neural networks for all the other graphs have 5 layers. The number of nodes in each layer are listed in Table <ref type="table" target="#tab_3">3</ref>. All the neural networks use element-wise sigmoid function as activations of each layer. For Wine and the three 20-Newsgroup graphs, since their underlying cluster numbers by the groundtruth labels are small, we do not vary the target cluster numbers in the experiments. We just set the cluster numbers to their real cluster numbers, i.e. Wine with 3, the three 20-Newsgroup graphs with 3, 6, and 9 respectively.</p><p>For the two PPI networks, we checked the algorithm performance with various predefined cluster numbers. That is, we varied the target cluster number from 5 to 400 (with average interval between two consecutive clustering numbers to be 50), and plotted the NMI values varying with the target cluster number for each algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experimental Results</head><p>Clustering Performance. The clustering results on Wine and 20-Newsgroup datasets are summarized in Table <ref type="table" target="#tab_4">4</ref>. It can be observed that the best performances of each dataset, listed in bolded digits, are all obtained by GraphEncoder built on sparse autoencoder. The experimental results on the two PPI networks are shown in Figure <ref type="figure">1</ref>  axis to be predefined clusters number, and vertical axis to be the corresponding NMI value.</p><p>We can see that for all the 6 graphs: (i) GraphEncoder based on sparse autoencoder beats spectral clustering. This is exactly the empirical justification of our claim that sparsity on graph embedding can help to improve clustering results. (ii) GraphEncoder based on sparse autoencoder beats k-means directly running on original normalized similarity graph, showing that deep structures can help to get even better graph representations.</p><p>Power of Deep Structures. To show the power of the stacked deep structures, we list the NMI values of k-means clustering on each layer's embedding by GraphEncoder in Table <ref type="table" target="#tab_5">5</ref>, in which we aim to cluster DIP and BioGrid datasets into 50 groups. Note that DNN for both datasets has five layers, with the shallowest layer to be layer 1 and deepest layer to be layer 5. The NMI value on layer 1 is exactly the result of directly running k-means on the input normalized similarity graph. From Table <ref type="table" target="#tab_5">5</ref> we can observe that the NMI values become larger when the layer goes from shallower to deeper, showing that the deep structures play an important role in generating good graph representations for clustering. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we have proposed a novel graph clustering method based on deep neural network, which takes the sparse autoencoder as its building block. We introduced a layer-wise pretraining scheme to map the input graph similarity matrix to the output graph embedding. Experimental results on several real world datasets have shown that the proposed method outperforms several state-of-the-art baselines including spectral clustering.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>. H T DH = I.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 1: Clustering Results on DIP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Then we feed the normalized training set D -1 S into the deep neural network and use the Clustering with GraphEncoder Input n-node graph G, with similarity matrix S ? R n?n and degree</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>? 3-NG: corp.graphics, rec.sport.baseball, talk.politics.guns. ? 6-NG: alt.atheism, comp.sys.mac.hardware, rec.motorcycles, rec.sport.hockey, soc.religion.christian, talk.religion.misc. ? 9-NG: talk.politics.mideast, talk.politics.misc, comp.os.mswindows.misc, comp.sys.ibm.pc.hardware, sci.electronics, sci.crypt, sci.med, sci.space, misc.forsale</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Datasets Information.</figDesc><table><row><cell>Dataset</cell><cell cols="2">#Nodes Weighted or Not</cell><cell>AvgDegree</cell></row><row><cell>Wine</cell><cell>178</cell><cell>Weighted</cell><cell>Fully Connected</cell></row><row><cell>3-NG</cell><cell>600</cell><cell>Weighted</cell><cell>Fully Connected</cell></row><row><cell>6-NG</cell><cell>1,200</cell><cell>Weighted</cell><cell>Fully Connected</cell></row><row><cell>9-NG</cell><cell>1,800</cell><cell>Weighted</cell><cell>Fully Connected</cell></row><row><cell>DIP</cell><cell>4,741</cell><cell>Unweighted</cell><cell>6.4</cell></row><row><cell>BioGrid</cell><cell>5,964</cell><cell>Unweighted</cell><cell>64.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Neural Network Structures</figDesc><table><row><cell>Dataset</cell><cell>#nodes in each layer</cell></row><row><cell>Wine</cell><cell>178-128-64</cell></row><row><cell>3-NG</cell><cell>600-512-256</cell></row><row><cell>6-NG</cell><cell>1,200-1,024-512-256-128</cell></row><row><cell>9-NG</cell><cell>1,800-1,024-512-256-128</cell></row><row><cell>DIP</cell><cell>4,741-2,048-1,024-512-256</cell></row><row><cell>BioGrid</cell><cell>5,964-2,048-1,024-256-128</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Clustering Results on Wine and 20-Newsgroup.</figDesc><table><row><cell cols="5">(Measured by NMI) Wine 3-NG 6-NG 9-NG</cell></row><row><cell>Spectral Clustering</cell><cell>0.71</cell><cell>0.60</cell><cell>0.55</cell><cell>0.39</cell></row><row><cell>k-means</cell><cell>0.70</cell><cell>0.72</cell><cell>0.26</cell><cell>0.22</cell></row><row><cell>SAE</cell><cell>0.84</cell><cell>0.81</cell><cell>0.60</cell><cell>0.41</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Layer-wise NMI Values on Two PPI Networks with #cluster= 50.</figDesc><table><row><cell>Layer</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell>DIP</cell><cell cols="5">0.40 0.42 0.45 0.50 0.51</cell></row><row><cell cols="6">BioGrid 0.55 0.55 0.56 0.58 0.59</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Some researchers also suggest selecting the eigenvectors corresponding to the ? k smallest non-zero eigenvalues.Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Note that spectral clustering can obtain the global optimum; however, autoencoder usually leads to a local optimum due to the back-propagation algorithm it employs.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks. Advances in neural information processing systems 19:153</title>
		<author>
			<persName><forename type="first">A</forename><surname>Asuncion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2007">2007. 2007. 2013</date>
		</imprint>
	</monogr>
	<note>Representation learning: A review and new perspectives</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning deep architectures for ai</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and trends in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="127" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Online learning and stochastic approximations</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Online learning in neural networks</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sparse feature learning for deep belief networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>-L.; Cun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1185" to="1192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Auto-association by multilayer perceptrons and singular value decomposition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bourlard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological cybernetics</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">4-5</biblScope>
			<biblScope unit="page" from="291" to="294" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spectral kway ratio-cut partitioning and clustering. Computer-Aided Design of Integrated Circuits and Systems</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Schlag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1088" to="1096" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Parallel spectral clustering in distributed systems. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="568" to="586" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. Audio, Speech, and Language Processing</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="42" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Weighted graph cuts without eigenvectors a multilevel approach. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1944" to="1957" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The approximation of one matrix by another of lower rank</title>
		<author>
			<persName><forename type="first">C</forename><surname>Eckart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="1936">1936. 2006</date>
		</imprint>
	</monogr>
	<note>Science</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Autoencoders, minimum description length, and helmholtz free energy. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="3" to="3" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A fast and high quality multilevel scheme for partitioning irregular graphs</title>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on scientific Computing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="359" to="392" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sparse coding with an overcomplete basis set: A strategy employed by v1?</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Field</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="3311" to="3325" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient learning of sparse representations with an energybased model</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Acm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Poultney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Cun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirty-first annual ACM symposium on Theory of computing</title>
		<meeting>the thirty-first annual ACM symposium on Theory of computing</meeting>
		<imprint>
			<date type="published" when="1999">1999. 2006</date>
			<biblScope unit="page" from="1137" to="1144" />
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Up-to-date catalogues of yeast protein complexes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Wodak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="825" to="831" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The database of interacting proteins: 2004 update</title>
		<author>
			<persName><forename type="first">L</forename><surname>Salwinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">K</forename><surname>Pettit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">U</forename><surname>Bowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eisenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="449" to="D451" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scalable graph clustering using stochastic flows: applications to community discovery</title>
		<author>
			<persName><forename type="first">V</forename><surname>Satuluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parthasarathy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGKDD conference</title>
		<meeting>the 15th ACM SIGKDD conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="737" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A spectral clustering approach to finding communities in graphs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th SIAM International Conference on Data Mining</title>
		<meeting>the 5th SIAM International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="76" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<author>
			<persName><forename type="first">C</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-J</forename><surname>Breitkreutz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chatr-Aryamontri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Boucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Oughtred</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Livstone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Van Auken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The biogrid interaction database: 2011 update</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="698" to="D704" />
		</imprint>
	</monogr>
	<note>suppl</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Graph clustering by flow simulation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Van Dongen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9999</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName><forename type="first">Von</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Between min cut and graph bisection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Symposium on Mathematical Foundations of Computer Science, MFCS &apos;93</title>
		<meeting>the 18th International Symposium on Mathematical Foundations of Computer Science, MFCS &apos;93<address><addrLine>London, UK, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="744" to="750" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
