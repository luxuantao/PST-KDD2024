<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Detecting Adversarial Samples from Artifacts</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Reuben</forename><surname>Feinman</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ryan</forename><forename type="middle">R</forename><surname>Curtin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Saurabh</forename><surname>Shintre</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><forename type="middle">B</forename><surname>Gardner</surname></persName>
						</author>
						<title level="a" type="main">Detecting Adversarial Samples from Artifacts</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural networks (DNNs) are powerful nonlinear architectures that are known to be robust to random perturbations of the input. However, these models are vulnerable to adversarial perturbations-small input changes crafted explicitly to fool the model. In this paper, we ask whether a DNN can distinguish adversarial samples from their normal and noisy counterparts. We investigate model confidence on adversarial samples by looking at Bayesian uncertainty estimates, available in dropout neural networks, and by performing density estimation in the subspace of deep features learned by the model. The result is a method for implicit adversarial detection that is oblivious to the attack algorithm. We evaluate this method on a variety of standard datasets including MNIST and CIFAR-10 and show that it generalizes well across different architectures and attacks. Our findings report that 85-93% ROC-AUC can be achieved on a number of standard classification tasks with a negative class that consists of both normal and noisy samples.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep neural networks (DNNs) are machine learning techniques that impose a hierarchical architecture consisting of multiple layers of nonlinear processing units. In practice, DNNs achieve state-of-the-art performance for a variety of generative and discriminative learning tasks from domains including image processing, speech recognition, drug discovery and genomics <ref type="bibr" target="#b10">(LeCun et al., 2015)</ref>.</p><p>Although DNNs are known to be robust to noisy inputs <ref type="bibr" target="#b2">(Fawzi et al., 2016)</ref>, they have been shown to be vulnerable to specially-crafted adversarial samples <ref type="bibr" target="#b18">(Szegedy et al., 2014;</ref><ref type="bibr" target="#b6">Goodfellow et al., 2015)</ref>. These samples are constructed by taking a normal sample and perturbing it, ei-1 Center for Advanced Machine Learning at Symantec, Mountain View, CA, USA 2 Symantec Research Labs, Mountain View, CA, USA. Correspondence to: Reuben Feinman &lt;reuben.feinman@nyu.edu&gt;, Ryan R. Curtin &lt;ryan@ratml.org&gt;, Saurabh Shintre &lt;saurabh shintre@symantec.com&gt;.  <ref type="bibr" target="#b8">(Kurakin et al., 2017)</ref> and fool the model into misclassifying 100% of the time.</p><p>ther at once or iteratively, in a direction that maximizes the chance of misclassification. Figure <ref type="figure" target="#fig_0">1</ref> shows some examples of adversarial MNIST images alongside noisy images of equivalent perturbation size. Adversarial attacks which require only small perturbations to the original inputs can induce high-efficacy DNNs to misclassify at a high rate. Some adversarial samples can also induce a DNN to output a specific target class <ref type="bibr" target="#b14">(Papernot et al., 2016b)</ref>. The vulnerability of DNNs to such adversarial attacks highlights important security and performance implications for these models <ref type="bibr" target="#b14">(Papernot et al., 2016b)</ref>. Consequently, significant effort is ongoing to understand and explain adversarial samples and to design defenses against them <ref type="bibr" target="#b18">(Szegedy et al., 2014;</ref><ref type="bibr" target="#b6">Goodfellow et al., 2015;</ref><ref type="bibr" target="#b15">Papernot et al., 2016c;</ref><ref type="bibr" target="#b19">Tanay &amp; Griffin, 2016;</ref><ref type="bibr" target="#b12">Metzen et al., 2017)</ref>.</p><p>Using the intuition that adversarial samples lie off the true data manifold, we devise two novel features that can be used to detect adversarial samples:</p><p>• Density estimates, calculated with the training set in the feature space of the last hidden layer. These are meant to detect points that lie far from the data manifold. • Bayesian uncertainty estimates, available in dropout neural networks. These are meant to detect when points lie in low-confidence regions of the input space, and can detect adversarial samples in situations where density estimates cannot.</p><p>When both of these features are used as inputs to a simple logistic regression model, we observe effective detection of adversarial samples, achieving an ROC-AUC of 92.6%</p><p>The source code repository for this paper is located at http://github.com/rfeinman/detecting-adversarial-samples arXiv:1703.00410v3 [stat.ML] 15 Nov 2017 on the MNIST dataset with both noisy and normal samples as the negative class. In Section 2 we provide the relevant background information for our approach, and in Section 3 we briefly review a few state-of-the-art adversarial attacks. Then, we introduce the intuition for our approach in Section 4, with a discussion of manifolds and Bayesian uncertainty. This leads us to our results and conclusions in Sections 5 and 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>While neural networks are known to be robust to random noise <ref type="bibr" target="#b2">(Fawzi et al., 2016)</ref>, they have been shown to be vulnerable to adversarially-crafted perturbations <ref type="bibr" target="#b18">(Szegedy et al., 2014;</ref><ref type="bibr" target="#b6">Goodfellow et al., 2015;</ref><ref type="bibr" target="#b14">Papernot et al., 2016b)</ref>. Specifically, an adversary can use information about the model to craft small perturbations that fool the network into misclassifying their inputs. In the context of object classification, these perturbations are often imperceptible to the human eye, yet they can force the model to misclassify with high model confidence.</p><p>A number of works have attempted to explain the vulnerability of DNNs to adversarial samples. <ref type="bibr" target="#b18">Szegedy et al. (2014)</ref> offered a simple preliminary explanation for the phenomenon, arguing that low-probability adversarial "pockets" are densely distributed in input space. As a result, they argued, every point in image space is close to a vast number of adversarial points and can be easily manipulated to achieve a desired model outcome. <ref type="bibr" target="#b6">Goodfellow et al. (2015)</ref> argued that it is a result of the linear nature of deep classifiers. Although this explanation has been the most well-accepted in the field, it was recently weakened by counterexamples <ref type="bibr" target="#b19">(Tanay &amp; Griffin, 2016)</ref>. <ref type="bibr" target="#b19">Tanay &amp; Griffin (2016)</ref> introduced the 'boundary tilting' perspective, suggesting instead that adversarial samples lie in regions where the classification boundary is close to the manifold of training data.</p><p>Research in adversarial attack defense generally falls within two categories: first, methods for improving the robustness of classifiers to current attacks, and second, methods for detecting adversarial samples in the wild. <ref type="bibr" target="#b6">Goodfellow et al. (2015)</ref> proposed augmenting the training loss function with an additional adversarial term to improve the robustness of these models to a specific adversarial attack. Defensive distillation <ref type="bibr" target="#b15">(Papernot et al., 2016c</ref>) is another recently-introduced technique which involves training a DNN with the softmax outputs of another neural network that was trained on the training data, and can be seen as a way of preventing the network from fitting too tightly to the data. Defensive distillation is effective against the attack of <ref type="bibr" target="#b14">Papernot et al. (2016b)</ref>. However, <ref type="bibr" target="#b1">Carlini &amp; Wagner (2016)</ref> showed that defensive distillation is easily broken with a modified attack.</p><p>On the detection of adversarial samples, <ref type="bibr" target="#b12">Metzen et al. (2017)</ref> proposed augmenting a DNN with an additional "detector" subnetwork, trained on normal and adversarial samples. Although the authors show compelling performance results on a number of state-of-the-art adversarial attacks, one major drawback is that the detector subnetwork must be trained on generated adversarial samples. This implicitly trains the detector on a subset of all possible adversarial attacks; we do not know how comprehensive this subset is, and future attack modifications may be able to surmount the system. The robustness of this technique to random noise is not currently known.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Adversarial Attacks</head><p>The typical goal of an adversary is to craft a sample that looks similar to a normal sample, and yet that gets misclassified by the target model. In the realm of image classification, this amounts to finding a small perturbation that, when added to a normal image, causes the target model to misclassify the sample, but remains correctly classified by the human eye. For a given input image x, the goal is to find a minimal perturbation η such that the adversarial input x = x + η is misclassified. A significant number of adversarial attacks satisfying this goal have been introduced in recent years. This allows us a wide range of attacks to choose from in our investigation. Here, we introduce some of the most well-known and most recent attacks.</p><p>Fast Gradient Sign Method (FGSM): <ref type="bibr" target="#b6">Goodfellow et al. (2015)</ref> introduced the Fast Gradient Sign Method for crafting adversarial perturbations using the derivative of the model's loss function with respect to the input feature vector. Given a base input, the approach is to perturb each feature in the direction of the gradient by magnitude , where is a parameter that determines perturbation size. For a model with loss J(Θ, x, y), where Θ represents the model parameters, x is the model input, and y is the label of x, the adversarial sample is generated as</p><formula xml:id="formula_0">x * = x + sign(∇ x J(Θ, x, y)).</formula><p>With small , it is possible to fool DNNs trained for the MNIST and CIFAR-10 classification tasks with high success rate <ref type="bibr" target="#b6">(Goodfellow et al., 2015)</ref>.</p><p>Basic Iterative Method (BIM): <ref type="bibr" target="#b8">Kurakin et al. (2017)</ref> proposed an iterative version of FGSM called the Basic Iterative Method. This is a straightforward extension; instead of merely applying adversarial noise η once with one parameter , apply it many times iteratively with small . This gives a recursive formula:</p><formula xml:id="formula_1">x * 0 = x, x * i = clip x, (x * i−1 + sign(∇ x * i−1 J(Θ, x * i−1 , y))).</formula><p>Here, clip x, (•) represents a clipping of the values of the adversarial sample such that they are within anneighborhood of the original sample x. This approach is convenient because it allows extra control over the attack. For instance, one can control how far past the classification boundary a sample is pushed: one can terminate the loop on the iteration when x * i is first misclassified, or add additional noise beyond that point.</p><p>The basic iterative method was shown to be typically more effective than the FGSM attack on ImageNet images <ref type="bibr" target="#b8">(Kurakin et al., 2017)</ref>.</p><p>Jacobian-based Saliency Map Attack (JSMA): <ref type="bibr" target="#b14">Papernot et al. (2016b)</ref> proposed a simple iterative method for targeted misclassification. By exploiting the forward derivative of a DNN, one can find an adversarial perturbation that will force the model to misclassify into a specific target class. For an input x and a neural network F , the output for class j is denoted F j (x). To achieve a target class t, F t (X) must be increased while the probabilities F j (X) of all other classes j = t decrease, until t = arg max j F j (X). This is accomplished by exploiting the adversarial saliency map, which is defined as</p><formula xml:id="formula_2">S(X, t)[i] = 0, if ∂Ft(X) ∂Xi &lt; 0 or j =t ∂Fj (X) ∂Xi &gt; 0 ( ∂Ft(X) ∂Xi )| j =t ∂Fj (X) ∂Xi |, otherwise</formula><p>for an input feature i. Starting with a normal sample x, we locate the pair of features {i, j} that maximize S(X, t)[i] + S(X, t)[j], and perturb each feature by a constant offset . This process is repeated iteratively until the target misclassification is achieved. This method can effectively produce MNIST samples that are correctly classified by human subjects but misclassified into a specific target class by a DNN with high success rate.</p><p>Carlini &amp; Wagner (C&amp;W): Carlini &amp; Wagner (2016) recently introduced a technique that is able to overcome defensive distillation. In fact, their technique encompasses a range of attacks, all cast through the same optimization framework. This results in three powerful attacks, each for a different distance metric: an L 2 attack, an L 0 attack, and an L ∞ attack. For the L 0 attack, which we will consider in this paper, the perturbation δ is defined in terms of an auxiliary variable ω as</p><formula xml:id="formula_3">δ * i = 1 2 (tanh(ω i + 1)) − x i .</formula><p>Then, to find δ * (an 'unrestricted perturbation'), we optimize over ω:</p><formula xml:id="formula_4">min ω 1 2 (tanh(ω) + 1) − x 2 2 + cf 1 2 tanh(ω) + 1</formula><p>where f (•) is an objective function based on the hinge loss:</p><formula xml:id="formula_5">f (x) = max(max{Z(x) i : i = t} − Z(x) t , −κ).</formula><p>Here, Z(x) i is the pre-softmax output for class i, t is the target class, and κ is a parameter that controls the confidence with which the misclassification occurs.</p><p>Finally, to produce the adversarial sample x * = x + δ, we convert the unrestricted perturbation δ * to a restricted perturbation δ, in order to reduce the number of changed pixels. By calculating the gradient ∇f (x + δ * ), we may identify those pixels δ * i with little importance (small gradient values) and take δ i = 0; otherwise, for larger gradient values we take δ i = δ * i . This allows an effective attack with few modified pixels, thus helping keep the norm of δ low.</p><p>These three attacks were shown to be particularly effective in comparison to other attacks against networks trained with defensive distillation, achieving adversarial sample generation success rates of 100% where other techniques were not able to top 1%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Artifacts of Adversarial Samples</head><p>Each of these adversarial sample generation algorithms are able to change the predicted label of a point without changing the underlying true label: humans will still correctly classify an adversarial sample, but models will not. This can be understood from the perspective of the manifold of training data. Many high-dimensional datasets, such as images, are believed to lie on a low-dimensional manifold <ref type="bibr" target="#b11">(Lee &amp; Verleysen, 2007)</ref>. <ref type="bibr" target="#b5">Gardner et al. (2015)</ref> recently showed that by carefully traversing the data manifold, one can change the underlying true label of an image. The intuition is that adversarial perturbations-which do not constitute meaningful changes to the input-must push samples off of the data manifold. <ref type="bibr" target="#b19">Tanay &amp; Griffin (2016)</ref> base their investigation of adversarial samples on the assumption that adversarial samples lie near class boundaries that are close to the edge of a data submanifold. Similarly, <ref type="bibr" target="#b6">Goodfellow et al. (2015)</ref> demonstrate that DNNs perform correctly only near the small manifold of training data. Therefore, we base our work here on the assumption that adversarial samples do not lie on the data manifold.</p><p>If we accept that adversarial samples are points that would not arise naturally, then we can assume that a technique to generate adversarial samples will, from a source point x with class c x , typically generate an adversarial sample x * that does not lie on the manifold and is classified incorrectly as c x * . If x * lies off of the data manifold, we may split into three possible situations:  2. x * is near the submanifold c x * but not on it, and x * is far from the classification boundary separating classes c x and c x * .</p><p>3. x * is near the submanifold c x * but not on it, and x * is near the classification boundary separating classes c x and c x * .</p><p>Figures 2a through 2c show simplified example illustrations for each of these three situations in a two-dimensional binary classification setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Density Estimation</head><p>If we have an estimate of what the submanifold corresponding data with class c x * is, then we can determine whether x * falls near this submanifold after observing the prediction c x * . Following the intuition of <ref type="bibr" target="#b5">Gardner et al. (2015)</ref> and hypotheses of <ref type="bibr" target="#b0">Bengio et al. (2013)</ref>, the deeper layers of a DNN provide more linear and 'unwrapped' manifolds to work with than input space; therefore, we may use this idea to model the submanifolds of each class by performing kernel density estimation in the feature space of the last hidden layer.</p><p>The standard technique of kernel density estimation can, given the point x and the set X t of training points with label t, provide a density estimate f (x) that can be used as a measure of how far x is from the submanifold for t. Specifically,</p><formula xml:id="formula_6">f (x) = 1 |X t | xi∈Xt k(x i , x)<label>(1)</label></formula><p>where k(•, •) is the kernel function, often chosen as a Gaussian with bandwidth σ:</p><formula xml:id="formula_7">k σ (x, y) ∼ exp(− x − y 2 /σ 2 ).<label>(2)</label></formula><p>The bandwidth may typically be chosen as a value that maximizes the log-likelihood of the training data <ref type="bibr" target="#b7">(Jones et al., 1996)</ref>. A value too small will give rise to a 'spiky' density estimate with too many gaps (see Figure <ref type="figure" target="#fig_3">3</ref>), but a value too large will give rise to an overly-smooth density estimate (see Figure <ref type="figure">4</ref>). This also implies that the estimate is improved as the training set size |X t | increases, since we are able to use smaller bandwidths without the estimate becoming too 'spiky.'</p><p>For the manifold estimate, we operate in the space of the last hidden layer. This layer provides a space of reasonable dimensionality in which we expect the manifold of our data to be simplified. If φ(x) is the last hidden layer activation vector for point x, then our density estimate for a point x with predicted class t is defined as</p><formula xml:id="formula_8">K(x, X t ) = xi∈Xt k σ (φ(x), φ(x i )) (3)</formula><p>where X t is the set of training points of class t, and σ is the tuned bandwidth.</p><p>To validate our intuition about the utility of this density estimate, we perform a toy experiment using the BIM attack with a convnet trained on MNIST data. In Figure <ref type="figure">5</ref>, we  plot the density estimate of the source class and the final predicted class for each iteration of BIM. One can see that the adversarial sample moves away from a high density estimate region for the correct class, and towards a high density estimate region for the incorrect class. This matches our intuition: we expect the adversarial sample to leave the correct class manifold and move towards (but not onto) the incorrect class manifold.</p><p>While a density estimation approach can easily detect an adversarial point that is far from the c x * submanifold, this strategy may not work well when x * is very near the c x * submanifold. Therefore, we must investigate alternative approaches for those cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Bayesian Neural Network Uncertainty</head><p>Beyond distance-based metrics, another powerful tool to identify low-confidence regions of the input space is the uncertainty output of Bayesian models, e.g., the Gaussian process <ref type="bibr" target="#b16">(Rasmussen &amp; Williams, 2005)</ref>. Gaussian processes assume a Gaussian prior over the set of all functions, F, that can be used to map the input space to the output space. As observations (x, y) are made, only those functions f ∈ F are retained for which f (x) = y. For a new test point x * , the prediction for each function f , y * = f (x * ), is computed and the expected value over y * is the used as the final prediction. Simultaneously, the variance of the output values y * is also used as an indicator of the model's uncertainty. Figure <ref type="figure" target="#fig_5">6</ref> illustrates how in simple cases, Bayesian uncertainty can provide additional information about model confidence not conveyed by distance metrics like a density estimate.</p><p>Recently, <ref type="bibr" target="#b4">Gal &amp; Ghahramani (2015)</ref> proved that DNNs trained with dropout are equivalent to an approximation of the deep Gaussian process. As result, we can extract Bayesian uncertainty estimates from a wide range of DNN architectures without modification. Dropout, first introduced as a method to reduce overfitting when training DNNs <ref type="bibr" target="#b17">(Srivastava et al., 2014)</ref>, works by dropping hidden nodes from the network randomly with some probability p during the training phase. During the testing phase, all nodes are kept, but the weights are scaled by p. <ref type="bibr" target="#b4">Gal &amp; Ghahramani (2015)</ref> showed that the dropout training objective converges to a minimization of the Kullback-Leibler divergence between an aproximate distribution and the posterior of a deep Gaussian process marginalized over its covariance function parameters. After iterating to convergence, uncertainty estimates can be extracted from dropout DNNs in the following manner.</p><p>We sample T times from our distribution of network configurations, typically i.i.d. Bernoulli(o l ) for each layer l, and obtain parameters {W 1 , • • • , W T }. Here W t = {W t 1 , ..., W t L } are the L weight matrices sampled at iteration t. Thereafter, we can evaluate a Monte Carlo estimate of the output, i.e. the first moment, as:</p><formula xml:id="formula_9">E q(y * |x * ) [y * ] ≈ 1 T T i=1 ŷ * (x * , W t ).</formula><p>(4)</p><p>Similarly, we can evaluate the second moment with Monte Carlo estimation, leading us to an estimate of model variance</p><formula xml:id="formula_10">V q(y * |x * ) [y * ] ≈ τ −1 I D + 1 T T i=1 ŷ * (x * , W t ) T ŷ * (x * , W t ) −E q(y * |x * ) [y * ] T E q(y * |x * ) [y * ] (5)</formula><p>where τ is our model precision.</p><p>Relying on the intuition that Bayesian uncertainty can be useful to identify adversarial samples, we make use of dropout variance values in this paper, setting T = 50. As dropout on its own is known to be a powerful regularization technique <ref type="bibr" target="#b17">(Srivastava et al., 2014)</ref>, we use neural network models without weight decay, leaving τ −1 = 0. Thus, for a test sample x * and stochastic predictions {ŷ * 1 , ..., ŷ * T }, our uncertainty estimate U (x * ) can be computed as</p><formula xml:id="formula_11">U (x * ) = 1 T T i=1 ŷ * i T ŷ * i − 1 T T i=1 ŷ * i T 1 T T i=1 ŷ * i .<label>(6)</label></formula><p>Because we use DNNs with one output node per class, we look at the mean of the uncertainty vector as a scalar representation of model uncertainty.</p><p>To demonstrate the efficacy of our uncertainty estimates in detecting adversarial samples, we trained the LeNet convnet <ref type="bibr" target="#b9">(LeCun et al., 1989)</ref> with a dropout rate of 0.5 applied after the last pooling layer and after the inner-product layer for MNIST classification. Figures <ref type="figure" target="#fig_6">7a and 7b</ref> compare the distribution of Bayesian uncertainty for adversarial samples to those of normal samples and of noisy samples with equivalent perturbation size; both the BIM and JSMA cases are shown. Clearly, uncertainty distributions for adversarial samples are statistically distinct from normal and noisy samples, verifying our intuition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In order to evaluate the proficiency of our density and uncertainty features for adversarial detection, we test these features on MNIST, CIFAR10, and SVHN. All pixels are scaled to floats in the range of [0, 1]. Our models achieve near state-of-the-art accuracy on the normal holdout sets for each dataset and are described in Section 5.1. In order to properly evaluate our method, we only perturb those test samples which were correctly classified by our models in their original states. An adversary would have no reason to perturb samples that are already misclassified.</p><p>We implement each of the four attacks (FGSM, BIM, JSMA, and C&amp;W) described in Section 3 in TensorFlow, using the cleverhans library for FGSM and JSMA <ref type="bibr" target="#b13">(Papernot et al., 2016a)</ref>. For the BIM attack, we implement two versions: BIM-A, which stops iterating as soon as miclassification is achieved ('at the decision boundary'), and BIM-B, which runs for a fixed number of iterations that is well beyond the average misclassification point ('beyond the decision boundary'). For each attack type, we also craft an equal number of noisy test samples as a benchmark.</p><p>For FGSM and BIM, these are crafted by adding Guassian noise to each pixel with a scale set so that the mean L 2 -norm of the perturbation matches that of the adversarial samples. For JSMA and C&amp;W, which flip pixels to their min or max values, these are crafted by observing the number of pixels that were altered in the adversarial case and flipping an equal number of pixels randomly. Details about model accuracies on the adversarial sets and average perturbation sizes are provided in Table <ref type="table" target="#tab_0">1</ref>. Some examples of normal, noisy and adversarial samples are displayed in Figure <ref type="figure" target="#fig_7">8</ref>.  Sample MNIST CIFAR-10 Type  </p><formula xml:id="formula_12">u(x * ) u(x) &gt; 1 d(x * ) d(x) &lt; 1 u(x * ) u(x n ) &gt; 1 d(x * ) d(x n ) &lt; 1 u(x * ) u(x) &gt; 1 d(x * ) d(x) &lt; 1 u(x * ) u(x n ) &gt; 1 d(x * ) d(x n ) &lt;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Network setup</head><p>Here, we briefly describe the models used for each dataset and their accuracies on normal and noisy test samples.</p><p>• MNIST: We use the <ref type="bibr">LeNet (LeCun et al., 1989)</ref> convnet architecture with a dropout rate of 0.5 after last pooling layer and after the inner-product layer. This model reports 98.7% accuracy on normal samples and 97.2% accuracy on noisy samples.</p><p>• SVHN: We use the LeNet architecture with an extra intermediate inner-product layer to assist with higher dimensionality. We used a dropout rate of 0.5 after the last pooling layer and after each inner-product layer. This model reports 92.2% accuracy on normal samples and 79.2% accuracy on noisy samples.</p><p>• CIFAR-10: We use a deep 12-layer convnet with a dropout rate of 0.5 applied after the last pooling layer and after each of the 2 inner-product layers. This model reports 82.6% accuracy on normal samples and 79.2% accuracy on noisy samples.</p><p>Training was done using the Adadelta optimizer with crossentropy loss and a batch size of 256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Feature Values</head><p>When we generate adversarial samples, the uncertainty typically grows larger than the original sample, and the density estimate typically grows smaller. This makes sense: the adversarial sample is likely to be in a region of higher uncertainty, lower density estimates, or both. In addition, the change is far more pronounced than if we simply perturb the sample with random noise.</p><p>In order to demonstrate this phenomenon, we generate adversarial samples and randomly perturbed (noisy) samples from the test data points for MNIST and CIFAR-10. For each attack, we calculate the percentage of points with higher uncertainty values than the corresponding original unperturbed samples, and the percentage of points with lower density estimates than the corresponding original unperturbed samples. The results are shown in Table <ref type="table" target="#tab_2">2</ref>.</p><p>We can see clearly that uncertainty is generally increased when adversarial samples are generated, and density estimates are generally decreased. These results suggest that our two features are reliable indicators of adversarial samples. Therefore, we next move on to the task of detecting adversarial samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Adversarial Sample Classifiers</head><p>To evaluate our adversarial detection method, we aggregate all adversarial samples of different types into a unified set, and do the same with the noisy and normal samples. For each dataset and attack, we have built three binary classifiers:</p><p>• uncertainty: this classifier simply thresholds on the uncertainty value of a sample.</p><p>• density: this classifier simply thresholds on the negative log kernel density of a sample.</p><p>• combined: this is a simple logistic regression classifier with two features as input: the uncertainty and the density estimate.</p><p>These detection models are used to distinguish adversarial samples-the positive class-from normal and noisy sam-    Because these are all threshold-based classifiers, we may generate an ROC for each method. Figure <ref type="figure" target="#fig_9">9</ref> shows ROCs for each classifier with a couple of datasets. We see that the performance of the combined classifier is better than either the uncertainty or density classifiers, demonstrating that each feature is able to detect different qualities of adversarial features. Further, the ROCs demonstrate that the uncertainty and density estimates are effective indicators that can be used to detect if a sample is adversarial.  In Table <ref type="table" target="#tab_4">3</ref>, the ROC-AUC measures are shown, for each of the three classifiers, on each dataset, for each attack. The performance is quite good, suggesting that the combined classifier is able to effectively detect adversarial samples from a wide range of attacks on a wide range of datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We have shown that adversarial samples crafted to fool DNNs can be effectively detected with two new features: kernel density estimates in the subspace of the last hidden layer, and Bayesian neural network uncertainty estimates. These two features handle complementary situations, and can be combined as an effective defense mechanism against adversarial samples. Our results report that we can, in some cases, obtain an ROC-AUC for an adversarial sample detector of up to 90% or more when both normal and noisy samples constitute the negative class. The performance is good on a wide variety of attacks and a range of image datasets.</p><p>In our work here, we have only considered convolutional neural networks. However, we believe that this approach can be extended to other neural network architectures as well. <ref type="bibr" target="#b3">Gal (2015)</ref> showed that the idea of dropout as a Bayesian approximation could be applied to RNNs as well, allowing for robust uncertainty estimation. In future work, we aim to apply our features to RNNs and other network architectures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Examples of normal (top), noisy (middle) and adversarial (bottom) MNIST samples for a convnet. Adversarial samples were crafted via the Basic Iterative Method (Kurakin et al., 2017) and fool the model into misclassifying 100% of the time.</figDesc><graphic url="image-1.png" coords="1,327.24,160.33,194.41,97.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>1. x * is far away from the submanifold of c x * .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. (a): The adversarial sample x * is generated by moving off the '-' submanifold and across the decision boundary (black dashed line), but x * still lies far from the '+' submanifold. (b): the '+' submanifold has a 'pocket', as in Szegedy et al. (2014). x * lies in the pocket, presenting significant difficulty for detection. (c): the adversarial sample x * is near both the decision boundary and both submanifolds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. 'spiky' density estimate from a too-small bandwidth on 1-D points sampled from a bimodal distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Figure 4. Overly smooth density estimate from a too-large bandwidth on 1-D points sampled from a bimodal distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. A simple 1-dimensional Gaussian process regression. The dashed line indicates the mean prediction and the shaded area indicates the 95% interval. While two test points (red x's) are equidistant from their nearest training points (black dots), their uncertainty estimates differ significantly.</figDesc><graphic url="image-2.png" coords="5,307.44,67.06,218.70,145.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Model uncertainty distributions per sample type for MNIST. Distributions are based on a histogram with 100 bins.</figDesc><graphic url="image-4.png" coords="6,57.87,213.23,238.14,132.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Some example images from MNIST, CIFAR-10 and SVHN. The original image is shown in the left-most column. For each attack, the left image is the adversarial sample and the right image is a corresponding noisy sample of equal perturbation size.</figDesc><graphic url="image-5.png" coords="6,310.23,487.07,228.42,177.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. ROCS for the different classifier types. The blue line indicates uncertainty, green line density, and red line combined. The negative class consists of both normal and noisy samples.</figDesc><graphic url="image-6.png" coords="8,55.44,147.05,160.38,111.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. ROC results per adversarial attack for combined classifier on MNIST.</figDesc><graphic url="image-9.png" coords="8,63.09,545.71,218.70,152.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10</head><label>10</label><figDesc>Figure10shows the ROCs for each individual attack; the combined classifier is able to most easily handle the JSMA, BIM-A and C&amp;W attacks.</figDesc><graphic url="image-7.png" coords="8,218.25,147.05,160.38,111.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Adversarial attack details. For each algorithm, the average L2-norm of the perturbation is shown, as well as the model accuracy on the adversarial set.</figDesc><table><row><cell>Dataset</cell><cell cols="2">FGSM</cell><cell cols="2">BIM-A</cell><cell cols="2">BIM-B</cell><cell></cell><cell>JSMA</cell><cell></cell><cell>C&amp;W</cell></row><row><cell></cell><cell>L 2</cell><cell>Acc.</cell><cell>L 2</cell><cell>Acc.</cell><cell>L 2</cell><cell>Acc.</cell><cell>L 2</cell><cell>Acc.</cell><cell>L 2</cell><cell>Acc.</cell></row><row><cell>MNIST</cell><cell cols="10">6.22 5.87% 2.62 0.00% 5.37 0.00% 5.00 2.70% 4.71 0.79%</cell></row><row><cell cols="11">CIFAR-10 2.74 7.03% 0.48 0.57% 2.14 0.57% 3.45 0.20% 2.70 0.89%</cell></row><row><cell>SVHN</cell><cell cols="10">7.08 3.29% 0.83 0.00% 6.56 0.00% 2.96 0.32% 2.37 0.87%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>The uncertainty of an adversarial sample is typically larger than that of its noisy and normal counterparts, and the density estimate is typically smaller. x * indicates an adversarial sample, x is a regular sample and x n a noisy sample.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Dataset FGSM BIM-A BIM-B JSMA C&amp;W Overall MNIST 90.57% 97.23% 82.06% 98.13% 97.94% 92.59% CIFAR-10 72.23% 81.05% 95.41% 91.52% 92.17% 85.54% SVHN 89.04% 82.12% 99.91% 91.34% 92.82% 90.20%</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>ROC-AUC measures for each dataset and each attack for the logistic regression classifier (combined).</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Nikolaos Vasiloglou and Nicolas Papernot for useful discussions that helped to shape the direction of this paper. We also thank Symantec Corporation for providing us with the resources used to conduct this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Better mixing via deep representations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mesnil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rifai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on International Conference on Machine Learning (ICML &apos;13)</title>
				<meeting>the 30th International Conference on International Conference on Machine Learning (ICML &apos;13)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="552" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.04644</idno>
		<title level="m">Towards evaluating the robustness of neural networks</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Robustness of classifiers: from adversarial to random noise</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1632" to="1640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29 (NIPS 2016)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1019" to="1027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dropout as a Bayesian approximation: representing model uncertainty in deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning (ICML &apos;16)</title>
				<meeting>The 33rd International Conference on Machine Learning (ICML &apos;16)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Upchurch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Hopcroft</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06421</idno>
		<title level="m">Deep manifold traversal: changing labels with convolutional features</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples. International Conference on Learning Representations</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A brief survey of bandwidth selection for density estimation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Marron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Sheather</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">433</biblScope>
			<biblScope unit="page" from="401" to="407" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep learning. Nature</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Nonlinear Dimensionality Reduction</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Verleysen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Springer</publisher>
			<pubPlace>Science+Business Media, LLC, New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Genewein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bischoff</surname></persName>
		</author>
		<title level="m">On detecting adversarial perturbations. International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sheatsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Feinman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.00768v3</idno>
		<title level="m">cleverhans v1.0.0: an adversarial machine learning library</title>
				<imprint>
			<date type="published" when="2016">2016a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The limitations of deep learning in adversarial settings</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">B</forename><surname>Celik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st IEEE European Symposium on Security and Privacy</title>
				<meeting>the 1st IEEE European Symposium on Security and Privacy</meeting>
		<imprint>
			<date type="published" when="2016">2016b</date>
			<biblScope unit="page" from="372" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distillation as a defense to adversarial perturbations against deep neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Security and Privacy</title>
				<imprint>
			<date type="published" when="2016">2016c</date>
			<biblScope unit="page" from="582" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning)</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>The MIT Press</publisher>
			<biblScope unit="volume">026218253</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Tanay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Griffin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.07690</idno>
		<title level="m">A boundary tilting perspective on the phenomenon of adversarial samples</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
