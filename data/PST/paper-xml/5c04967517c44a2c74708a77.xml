<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Compact Generalized Non-local Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kaiyu</forename><surname>Yue</surname></persName>
							<email>yuekaiyu@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Central South University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ming</forename><surname>Sun</surname></persName>
							<email>sunming05@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Central South University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuchen</forename><surname>Yuan</surname></persName>
							<email>yuanyuchen02@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Central South University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Feng</forename><surname>Zhou</surname></persName>
							<email>zhoufeng09@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Central South University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Errui</forename><surname>Ding</surname></persName>
							<email>dingerrui@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Central South University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fuxin</forename><surname>Xu</surname></persName>
							<email>fxxu@csu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Central South University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Baidu</forename><surname>Vis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Central South University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Baidu</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Central South University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Compact Generalized Non-local Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">18F380E35E9BA5C0FB2AF207921E004E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The non-local module <ref type="bibr" target="#b26">[27]</ref> is designed for capturing long-range spatio-temporal dependencies in images and videos. Although having shown excellent performance, it lacks the mechanism to model the interactions between positions across channels, which are of vital importance in recognizing fine-grained objects and actions. To address this limitation, we generalize the non-local module and take the correlations between the positions of any two channels into account. This extension utilizes the compact representation for multiple kernel functions with Taylor expansion that makes the generalized non-local module in a fast and low-complexity computation flow. Moreover, we implement our generalized non-local method within channel groups to ease the optimization. Experimental results illustrate the clear-cut improvements and practical applicability of the generalized non-local module on both fine-grained object recognition and video classification. Code is available at: https://github.com/KaiyuYue/cgnl-network.pytorch.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Figure <ref type="figure">1</ref>: Comparison between non-local (NL) and compact generalized non-local (CGNL) networks on recognizing an action video of kicking the ball. Given the reference patch (green rectangle) in the first frame, we visualize for each method the highly related responses in the other frames by thresholding the feature space. CGNL network out-performs the original NL network in capturing the ball that is not only in long-range distance from the reference patch but also corresponds to different channels in the feature map.</p><p>Capturing spatio-temporal dependencies between spatial pixels or temporal frames plays a key role in the tasks of fine-grained object and action classification. Modeling such interactions among images and videos is the major topic of various feature extraction techniques, including SIFT, LBP, Dense Trajectory <ref type="bibr" target="#b25">[26]</ref>, etc. In the past few years, deep neural network automates the feature designing pipeline by stacking multiple end-to-end convolutional or recurrent modules, where each of them processes correlation within spatial or temporal local regions. In general, capturing the long-range dependencies among images or videos still requires multiple stacking of these modules, which greatly hinders the learning and inference efficiency. A recent work <ref type="bibr" target="#b15">[16]</ref> also suggests that stacking more layers cannot always increase the effective receptive fields to capture enough local relations.</p><p>Inspired by the classical non-local means for image filtering, the recently proposed non-local neural network <ref type="bibr" target="#b26">[27]</ref> addresses this challenge by directly modeling the correlation between any two positions in the feature maps in a single module. Without bells and whistles, the non-local method can greatly improve the performances of existing networks on many video classification benchmarks. Despite its great performances, the original non-local network only considers the global spatio-temporal correlation by merging channels, and it might miss the subtle but important cross-channel clues for discriminating fine-grained objects or actions. For instance, the body, the ball and their interaction are all necessary for describing the action of kicking the ball in Fig. <ref type="figure">1</ref>, while the original non-local operation learns to focus on the body part relations but neglect the body-ball interactions that usually correspond to different channels of the input features.</p><p>To improve the effectiveness in fine-grained object and action recognition tasks, this work extends the non-local module by learning explicit correlations among all of the elements across the channels. First, this extension scale-ups the representation power of the non-local operation to attend the interaction between subtle object parts (e.g., the body and ball in Fig. <ref type="figure">1</ref>). Second, we propose its compact representation for various kernel functions to address the high computation burden issue. We show that as a self-contained module, the compact generalized non-local (CGNL) module provides steady improvements in classification tasks. Third, we also investigate the grouped CGNL blocks, which model the correlations across channels within each group.</p><p>We evaluate the proposed CGNL method on the task of fine-grained classification and action recognition. Extensive experimental results show that: 1) The CGNL network are easy to optimize as the original non-local network; 2) Compared with the non-local module, CGNL module enjoys capturing richer features and dense clues for prediction, as shown in Figure <ref type="figure">1</ref>, which leads to results substantially better than those of the original non-local module. Moreover, in the appendix of extensional experiments, the CGNL network can also promise a higher accuracy than the baseline on the large-scale ImageNet dataset <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Channel Correlations: The mechanism of sharing the same conv kernel among channels of a layer in a ConvNet <ref type="bibr" target="#b11">[12]</ref> can be seen as a basic way to capture correlations among channels, which aggregates the channels of feature maps by the operation of sum pooling. The SENet <ref type="bibr" target="#b9">[10]</ref> may be the first work that explicitly models the interdependencies between the channels of its spatial features. It aims to select the useful feature maps and suppress the others, and only considers the global information of each channel. Inspired by <ref type="bibr" target="#b26">[27]</ref>, we present the generalized non-local (GNL) module, which generalizes the non-local (NL) module to learn the correlations between any two positions across the channels. Compared to the SENet, we model the interdependencies among channels in an explicit and dense manner.</p><p>Compact Representation: After further investigation, we find that the non-local module contains a second-order feature space (Sect.3.1), which is used widely in previous computer vision tasks, e.g., SIFT <ref type="bibr" target="#b14">[15]</ref>, Fisher encoding <ref type="bibr" target="#b16">[17]</ref>, Bilinear model <ref type="bibr" target="#b13">[14]</ref> [5] and segmentation task <ref type="bibr" target="#b1">[2]</ref>. However, such second-order feature space involves high dimensions and heavy computational burdens. In the area of kernel learning <ref type="bibr" target="#b20">[21]</ref>, there are many prior works such as compact bilinear pooling (CBP) <ref type="bibr" target="#b4">[5]</ref> that uses the Tensor Sketching <ref type="bibr" target="#b17">[18]</ref> to address this problem. But this type of method is not perfect yet. Because the it cannot produce a light computation to the various size of sketching vectors. Fortunately, in mathematics, the whole non-local operation can be viewed as a trilinear formation. It can be fast computed with the associative law of matrix production. To the other types of pairwise function, such as Embedded Gaussian or RBF <ref type="bibr" target="#b18">[19]</ref>, we propose a tight approximation for them by using the Taylor expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>In this section, we introduce a general formulation of the proposed general non-local operation. We then show that the original non-local and the bilinear pooling are special cases of this formulation. After that, we illustrate that the general non-local operation can be seen as a modality in the trilinear matrix production and show how to implement our generalized non-local (GNL) module in a compact representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Review of Non-local Operation</head><p>We begin by briefly reviewing the original non-local operation <ref type="bibr" target="#b26">[27]</ref> in matrix form. Suppose that an image or video is given to the network and let X ∈ R N ×C denote (see notation <ref type="foot" target="#foot_1">1</ref> ) the input feature map of the non-local module, where C is the number of channels. For the sake of notation clarity, we collapse all the spatial (width W and height H) and temporal (video length T ) positions in one dimension, i.e., N = HW or N = HW T . To capture long-range dependencies across the whole feature map, the original non-local operation computes the response Y ∈ R N ×C as the weighted sum of the features at all positions,</p><formula xml:id="formula_0">Y = f θ(X), φ(X) g(X),<label>(1)</label></formula><p>where θ(•), φ(•), g(•) are learnable transformations on the input. In <ref type="bibr" target="#b26">[27]</ref>, the authors suggest using 1 × 1 or 1 × 1 × 1 convolution for simplicity, i.e., the transformations can be written as</p><formula xml:id="formula_1">θ(X) = XW θ ∈ R N ×C , φ(X) = XW φ ∈ R N ×C , g(X) = XW g ∈ R N ×C ,<label>(2)</label></formula><p>parameterized by the weight matrices W θ , W φ , W g ∈ R C×C respectively. The pairwise function f (•, •) : R N ×C × R N ×C → R N ×N computes the affinity between all positions (space or space-time).</p><p>There are multiple choices for f , among which dot-product is perhaps the simplest one, i.e.,</p><formula xml:id="formula_2">f θ(X), φ(X) = θ(X)φ(X) .<label>(3)</label></formula><p>Plugging Eq. 2 and Eq. 3 into Eq. 1 yields a trilinear interpretation of the non-local operation,</p><formula xml:id="formula_3">Y = XW θ W φ X XW g ,<label>(4)</label></formula><p>where the pairwise matrix XW θ W φ X ∈ R N ×N encodes the similarity between any locations of the input feature. The effect of non-local operation can be related to the self-attention module <ref type="bibr" target="#b0">[1]</ref> based on the fact that each position (row) in the result Y is a linear combination of all the positions (rows) of XW g weighted by the corresponding row of the pairwise matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Review of Bilinear Pooling</head><p>Analogous to the conventional kernel trick <ref type="bibr" target="#b20">[21]</ref>, the idea of bilinear pooling <ref type="bibr" target="#b13">[14]</ref> has recently been adopted in ConvNets for enhancing the feature representation in various tasks, such as fine-grained classification, person re-id, action recognition. At a glance, bilinear pooling models pairwise feature interactions using explicit outer product at the final classification layer:</p><formula xml:id="formula_4">Z = X X ∈ R C×C ,<label>(5)</label></formula><p>where X ∈ R N ×C is the input feature map generated by the last convolutional layer. Each element of the final descriptor</p><formula xml:id="formula_5">z c1c2 = n x nc1 x nc2 sum-pools at each location n = 1, • • • , N the bilinear product x nc1 x nc2 of the corresponding channel pair c 1 , c 2 = 1, • • • , C.</formula><p>Despite the distinct design motivation, it is interesting to see that bilinear pooling (Eq. 5) can be viewed as a special case of the second-order term (Eq. 3) in the non-local operation if we consider,</p><formula xml:id="formula_6">θ(X) = X ∈ R C×N , φ(X) = X ∈ R C×N . (<label>6</label></formula><formula xml:id="formula_7">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Generalized Non-local Operation</head><p>The original non-local operation aims to directly capture long-range dependencies between any two positions in one layer. However, such dependencies are encoded in a joint location-wise matrix f (θ(X), φ(X)) by aggregating all channel information together. On the other hand, channel-wise correlation has been recently explored in both discriminative <ref type="bibr" target="#b13">[14]</ref> and generative <ref type="bibr" target="#b23">[24]</ref> models through the covariance analysis across channels. Inspired by these works, we generalize the original non-local operation to model long-range dependencies between any positions of any channels.</p><p>We first reshape the output of the transformations (Eq. 2) on X by merging channel into position:</p><formula xml:id="formula_8">θ(X) = vec(XW θ ) ∈ R N C , φ(X) = vec(XW φ ) ∈ R N C , g(X) = vec(XW g ) ∈ R N C . (7)</formula><p>By lifting the row space of the underlying transformations, our generalized non-local (GNL) operation pursues the same goal of Eq. 1 that computes the response Y ∈ R N ×C as:</p><formula xml:id="formula_9">vec(Y) = f vec(XW θ ), vec(XW φ ) vec(XW g ).<label>(8)</label></formula><p>Compared to the original non-local operation (Eq. 4), GNL utilizes a more general pairwise function</p><formula xml:id="formula_10">f (•, •) : R N C × R N C → R N C×N C</formula><p>that can differentiate between pairs of same location but at different channels. This richer similarity greatly augments the non-local operation in discriminating fine-grained object parts or action snippets that usually correspond to channels of the input feature.</p><p>Compared to the bilinear pooling (Eq. 5) that can only be used after the last convolutional layer, GNL maintains the input size and can thus be flexibly plugged between any network blocks. In addition, bilinear pooling neglects the spatial correlation which, however, is preserved in GNL.</p><p>Recently, the idea of dividing channels into groups has been established as a very effective technique in increasing the capacity of ConvNets. Well-known examples include Xception <ref type="bibr" target="#b2">[3]</ref>, MobileNet <ref type="bibr" target="#b8">[9]</ref>, ShuffleNet <ref type="bibr" target="#b30">[31]</ref>, ResNeXt <ref type="bibr" target="#b28">[29]</ref> and Group Normalization <ref type="bibr" target="#b27">[28]</ref>. Given its simplicity and independence, we also realize the channel grouping idea in GNL by grouping all C channels into G groups, each of which contains C = C/G channels of the input feature. We then perform GNL operation independently for each group to compute Y and concatenate the results along the channel dimension to restore the full response Y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Compact Representation</head><p>A straightforward implementation of GNL (Eq. 8) is prohibitive as the quadratic increase with respect to the channel number C in the presence of the N C × N C pairwise matrix. Although the channel grouping technique can reduce the channel number from C to C/G, the overall computational complexity is still much higher than the original non-local operation. To mitigate this problem, this section proposes a compact representation that leads to an affordable approximation for GNL.</p><p>Let us denote θ = vec(XW θ ), φ = vec(XW φ ) and g = vec(XW g ), each of which is a N C-D vector column. Without loss of generality, we assume f is a general kernel function (e.g., RBF, bilinear, etc.) that computes a N C × N C matrix composed by the elements,</p><formula xml:id="formula_11">f (θ, φ) ij ≈ P p=0 α 2 p (θ i φ j ) p ,<label>(9)</label></formula><p>which can be approximated by Taylor series up to certain order P . The coefficient α p can be computed in closed form once the kernel function is known. Taking RBF kernel for example,</p><formula xml:id="formula_12">[f (θ, φ)] ij = exp(-γ θ i -φ j 2 ) ≈ P p=0 β (2γ) p p! (θ i φ j ) p ,<label>(10)</label></formula><p>where</p><formula xml:id="formula_13">α 2 p = β (2γ) p p! and β = exp -γ( θ 2 + φ 2</formula><p>) is a constant and β = exp(-2γ) if the input vectors θ and φ are 2-normalized. By introducing two matrices,</p><formula xml:id="formula_14">Θ = [α 0 θ 0 , • • • , α P θ P ] ∈ R N C×(P +1) , Φ = [α 0 φ 0 , • • • , α P φ P ] ∈ R N C×(P +1)<label>(11)</label></formula><p>our compact generalized non-local (CGNL) operation approximates Eq. 8 via a trilinear equation,</p><formula xml:id="formula_15">vec(Y) ≈ ΘΦ g. (<label>12</label></formula><formula xml:id="formula_16">)</formula><p>At first glance, the above approximation still involves the computation of a large pairwise matrix ΘΦ ∈ R N C×N C . Fortunately, the order of Taylor series is usually relatively small P N C. According to the associative law, we could alternatively compute the vector z = Φ g ∈ R P +1 first and then calculate Θz in a much smaller complexity of O(N C(P + 1)). In another view, the process that this bilinear form Φ g is squeezed into scalars can be treated as a related concept of the SE module <ref type="bibr" target="#b9">[10]</ref>.</p><p>Complexity analysis: Table <ref type="table" target="#tab_0">1</ref> compares the computational complexity of CGNL network with the GNL ones. We cannot afford for directly computing GNL operation because of its huge complexity of O(2(N C) 2 ) in both time and space. Instead, our compact method dramatically eases the heavy calculation to O(N C(P + 1)).   where</p><formula xml:id="formula_17">N = H × W or N = T × H × W .</formula><p>X is first fed into three 1 × 1 × 1 convolutional layers that are described by the weights W θ , W φ , W g respectively in Eq. 7. To improve the capacity of neural networks, the channel grouping idea <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b27">28]</ref> is then applied to divide the transformed feature along the channel dimension into G groups. As shown in Fig. <ref type="figure" target="#fig_0">2</ref>, we approximate for each group the GNL operation (Eq. 8) using the Taylor series according to Eq. 12. To achieve generality and compatibility with existing neural network blocks, the CGNL block is implemented by wrapping Eq. 8 in an identity mapping of the input as in residual learning <ref type="bibr" target="#b7">[8]</ref>:</p><formula xml:id="formula_18">Z = concat(BN (Y W z )) + X,<label>(13)</label></formula><p>where W z ∈ R C×C denotes a 1 × 1 or 1 × 1 × 1 convolution layer followed by a Batch Normalization <ref type="bibr" target="#b10">[11]</ref> in each group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluate the CGNL network on multiple tasks, including fine-grained classification and action recognition. For fine-grained classification, we experiment on the Birds-200-2011 (CUB) dataset <ref type="bibr" target="#b24">[25]</ref>, which contains 11788 images of 200 bird categories. For action recognition, we experiment on two challenging datasets, Mini-Kinetics <ref type="bibr" target="#b29">[30]</ref> and UCF101 <ref type="bibr" target="#b21">[22]</ref>. The Mini-Kinetics dataset contains 200 action categories. Due to some video links are unavaliable to download, we use 78265 videos for training and 4986 videos for validation. The UCF101 dataset contains 101 actions, which are separated into 25 groups with 4-7 videos of each action in a group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>Given the steady performance and efficiency, the ResNet <ref type="bibr" target="#b7">[8]</ref> series (ResNet-50 and ResNet-101) are adopted as our baselines. For video tasks, we keep the same architecture configuration with <ref type="bibr" target="#b26">[27]</ref>, where the temporal dimension is trivially addressed by max pooling. Following <ref type="bibr" target="#b26">[27]</ref> the convolutional layers in the baselines are implemented as 1 × k × k kernels, and we insert our CGNL blocks into the network to turn them into compact generalized non-local (CGNL) networks. We investigate the configurations of adding 1 and 5 blocks. <ref type="bibr" target="#b26">[27]</ref> suggests that adding 1 block on the res4 is slightly better than the others. So our experiments of adding 1 block all target the res4 of ResNet. The experiments of adding 5 blocks, on the other hand, are configured by inserting 2 blocks on the res3, and 3 blocks on the res4, to every other residual block in ResNet-50 and ResNet-101.</p><p>Training: We use the models pretrained on ImageNet <ref type="bibr" target="#b19">[20]</ref> to initialize the weights. The frames of a video are extracted in a dense manner. Following <ref type="bibr" target="#b26">[27]</ref>, we generate 32-frames input clips for models, first randomly crop out 64 consecutive frames from the full-length video and then drop every other frame. The way to choose these 32-frames input clips can be viewed as a temporal augmentation. The crop size for each clip is distributed evenly between 0.08 and 1.25 of the original image and its aspect ratio is chosen randomly between 3/4 and 4/3. Finally we resize it to 224. We use a weight decay of 0.0001 and momentum of 0.9 in default. The strategy of gradual warmup is used in the first ten epochs. The dropout <ref type="bibr" target="#b22">[23]</ref> with ratio 0.5 is inserted between average pooling layer and last fully-connected layer. To keep same with <ref type="bibr" target="#b26">[27]</ref>, we use zero to initialize the weight and bias of the BatchNorm (BN) layer in both CGNL and NL blocks <ref type="bibr" target="#b5">[6]</ref>. To train the networks on CUB dataset, we follow the same training strategy above but the final crop size of 448.</p><p>Inference: The models are tested immediately after training is finished. In <ref type="bibr" target="#b26">[27]</ref>, spatially fullyconvolutional inference<ref type="foot" target="#foot_2">2</ref> is used for NL networks. For these video clips, the shorter side is resized to 256 pixels and use 3 crops to cover the entire spatial size along the longer side. The final prediction is the averaged softmax scores of all clips. For fine-grined classification, we do 1 center-crop testing in size of 448.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Experiments</head><p>Kernel Functions: We use three popular kernel functions, namely dot production, embedded Gaussian and Gaussian RBF, in our ablation studies. For dot production, Eq. 12 will be held for direct computation. For embedded Gaussian, the α 2 p will be 1 p! in Eq. 9. And for Gaussian RBF, the corresponding formula is defined as Eq. 10. We expend the Taylor series with third order and the hyperparameter γ for RBF is set by 1e-4 <ref type="bibr" target="#b3">[4]</ref>. Table <ref type="table" target="#tab_1">2a</ref> suggests that dot production is the best kernel functions for CGNL networks. Such experimental observations are consistent with <ref type="bibr" target="#b26">[27]</ref>. The other kernel functions we used, Embedded Gaussion and Gaussian RBF, has a little improvements for performance. Therefore, we choose the dot production as our main experimental configuration for other tasks.</p><p>Grouping: The grouping strategy is another important technique. On Mini-Kinetics, Table <ref type="table" target="#tab_1">2d</ref> shows that grouping can bring higher accuracy. The improvements brought in by adding groups are larger than those by reducing the channel reduction ratio. The best top1 accuracy is achieved by splitting into 8 groups for CGNL networks. On the other hand, however, it is worthwhile to see if more groups can always improve the results, and Table <ref type="table" target="#tab_1">2c</ref> gives the answer that more groups will hamper the performance improvements. This is actually expected, as the affinity in CGNL block considers the points across channels. When we split the channels into a few groups, it can facilitate the restricted optimization and ease the training. However, if too many groups are adopted, it hinder the affinity to capture the rich correlations between elements across the channels.   Comparison of CGNL Block to Simple Residual Block: There is a confusion about the efficiency caused by the possibility that the scalars from Φ g in Eq. 12 could be wiped out by the BN layer.</p><p>Because according to Algorithm 1 in <ref type="bibr" target="#b10">[11]</ref>, the output of input Θ weighted by the scalars s = Φ g can be approximated to</p><formula xml:id="formula_19">O = sΘ-E(sΘ) √ V ar(sΘ) * γ + β = sΘ-sE(Θ) √ s 2 V ar(Θ) * γ + β = Θ-E(Θ) √ V ar(Θ) * γ + β. At</formula><p>first glance, the scalars s is totally erased by BN in this mathmatical process. However, the de facto operation of a convolutional module has a process order to aggregate the features. Before passing into the BN layer, the scalars s has already saturated in the input features Θ and then been transformed into a different feature space by a learnable parameter W z . In other words, it is W z that "protects" s from being erased by BN via the convolutional operation. To eliminate this confusion, we further compare adding 1 CGNL block (with the kernel of dot production) in  <ref type="table" target="#tab_2">3</ref>. The top1 accuracy 84.11% of adding a simple residual block is slightly better than 84.05% of the baseline, but still worse than 85.14% of adding a linear kerenlized CGNL module. We think that the marginal improvement (84.06% → 84.11%) is due to the more parameters from the added simple residual block.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Main Results</head><p>Table <ref type="table" target="#tab_3">4a</ref> shows that although adding 5 NL and CGNL blocks in the baseline networks can both improve the accuracy, the improvement of CGNL network is larger. The same applies to Table <ref type="table" target="#tab_1">2b</ref> and Table <ref type="table" target="#tab_3">4b</ref>. In experiments on UCF101 and CUB dataset, the similar results are also observed that adding 5 CGNL blocks provides the optimal results both for R-50 and R-101.</p><p>Table <ref type="table" target="#tab_3">4a</ref> shows the main results on Mini-Kinetics dataset. Compared to the baseline R-50 whose top1 is 75.54%, adding 1 NL block brings improvement by about 1.0%. Similar results can be found in the experiments based on R-101, where adding 1 CGNL provides about more than 2% improvement, which is larger than that of adding 1NL block. Table <ref type="table" target="#tab_1">2b</ref> shows the main results on the UCF101 dataset, where adding 1CGNL block achieves higher accuracy than adding 1NL block. And Table <ref type="table" target="#tab_3">4b</ref> shows the main results on the CUB dataset. To understand the effects brought by CGNL network, we show the visualization analysis as shown in <ref type="bibr">Fig 5 and Fig 6.</ref> Additionly, to investigate the capacity and the generalization ability of our CGNL network. We test them on the task of object detection and instance segmentation. We add 1 NL and 1 CGNL block in the R-50 backbone for Mask-RCNN <ref type="bibr" target="#b6">[7]</ref>. Table <ref type="table" target="#tab_3">4c</ref> shows the main results on COCO2017 dataset <ref type="bibr" target="#b12">[13]</ref> by adopting our 1 CGNL block in the backbone of Mask-RCNN <ref type="bibr" target="#b6">[7]</ref>. It shows that the performance of adding 1 CGNL block is still better than that of adding 1 NL block.</p><p>We observe that adding CGNL block can always obtain better results than adding the NL block with the same blocks number. These experiments suggest that considering the correlations between any two positions across the channels can significantly improve the performance than that of original non-local methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have introduced a simple approximated formulation of compact generalized non-local operation, and have validated it on the task of fine-grained classification and action recognition from RGB images. Our formulation allows for explicit modeling of rich interdependencies between any positions across channels in the feature space. To ease the heavy computation of generalized non-local operation, we propose a compact representation with the simple matrix production by using Taylor expansion for multiple kernel functions. It is easy to implement and requires little additional parameters, making it an attractive alternative to the original non-local block, which only considers the correlations between two positions along the specific channel. Our model produces competitive or state-of-the-art results on various benchmarked datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix: Experiments on ImageNet</head><p>As a general method, the CGNL block is compatible with complementary techniques developed for the image task of fine-grained classification, temporal feature needed task of action recognition and the basic task of object detection. In this appendix, we further report the results of our spatial CGNL network on the large-scale ImageNet <ref type="bibr" target="#b19">[20]</ref> dataset, which has 1.2 million training images and 50000 images for validation in 1000 object categories. The training strategy and configurations of our CGNL networks is kept same as those in Sec 4, only except the crop size here used for input is 224. For a better demonstration of the generality of our CGNL network, we investigate both adding 1 dot production CGNL block and 1 Gaussian RBF CGNL block (identified by CGNLx) in Table <ref type="table" target="#tab_4">5</ref>. We compare these models with two strong baselines, R-50 and R-152. In Table <ref type="table" target="#tab_4">5</ref>, all the best top1 and top5 accuracies are reported under the single center crop testing. The CGNL networks beat the basemodels by larger than 1 point no matter whichever the dot production or Gaussian RBF plays as the kernel function in the CGNL module.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>ӨFigure 2 :</head><label>2</label><figDesc>Figure 2: Grouped compact generalized non-local (CGNL) module. The feature maps are shown with the shape of their tensors, e.g., [C, N ],where N = T HW or N = HW . The feature maps will be divided along channels into multiple groups after three conv layers whose kernel size and stride both equals 1 (k = 1, s = 1). The channels dimension is grouped into C = C/G, where G is a group number. The compact representations for generalized non-local module are build within each group. P indicates the order of Taylor expansion for kernel functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2</head><label>2</label><figDesc>Fig.2illustrates the workflow of how CGNL module processes a feature map X of the size N × C, where N = H × W or N = T × H × W . X is first fed into three 1 × 1 × 1 convolutional layers that are described by the weights W θ , W φ , W g respectively in Eq. 7. To improve the capacity of neural networks, the channel grouping idea<ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b27">28]</ref> is then applied to divide the transformed feature along the channel dimension into G groups. As shown in Fig.2, we approximate for each group the GNL operation (Eq. 8) using the Taylor series according to Eq. 12. To achieve generality and compatibility with existing neural network blocks, the CGNL block is implemented by wrapping Eq. 8 in an identity mapping of the input as in residual learning<ref type="bibr" target="#b7">[8]</ref>:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The workflow of our CGNL block. The corresponding formula is shown below in a blue tinted box.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The workflow of the simple residual block for comparison. The corresponding formula is shown below in a blue tinted box.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Fig 3 and adding 1 simple residual block in Fig 4 on CUB dataset in Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: Result analysis of the NL block and our CGNL block on CUB. Column 1: the input images with a small reference patch (green rectangle), which is used to find the highly related patches (white rectangle). Column 2: the highly related clues for prediction in each feature map found by the NL network. The dimension of self-attention space in NL block is N × N , where N = HW . So its visualization only has one column. Columns 3 to 7: the most related patches computed by our compact generalized non-local module. We first pick a reference position in the space of g, then we use the corresponding vectors in Θ and Φ to compute the attention maps with a threshold (here we use 0.7). Last column: the ground truth of body parts. The highly related areas of CGNL network can easily cover all of the standard parts that provide the prediction clues.</figDesc><graphic coords="7,108.00,466.71,395.96,157.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Visualization with feature heatmaps. We select a reference patch (green rectangle) in one frame, then visualize the high related ares by heatmaps. The CGNL network enjoys capturing dense relationships in feature space than NL networks.</figDesc><graphic coords="8,108.00,71.99,395.97,236.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Complexity comparison of GNL and CGNL operations, where N and C indicate the number of positions and channels respectively.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablations. Top1 and top5 accuracy (%) on various datasets. Results of channel grouped CGNL networks on CUB. A few groups can boost the performance. But more groups tend to prevent the CGNL block from capturing the correlations between positions across channels.</figDesc><table><row><cell cols="3">(a) Results of adding 1 CGNL block on CUB. The kernel of dot production achieves the best result. The accuracies of others are at the edge of baselines.</cell><cell>(c) model</cell><cell>groups</cell><cell>top1</cell><cell>top5</cell><cell>model</cell><cell>groups</cell><cell>top1</cell><cell>top5</cell></row><row><cell>model</cell><cell>top1</cell><cell>top5</cell><cell>R-101</cell><cell>-</cell><cell>85.05</cell><cell>96.70</cell><cell>R-101</cell><cell>-</cell><cell>85.05</cell><cell>96.70</cell></row><row><cell>R-50. Dot Production Gaussian RBF</cell><cell>84.05 85.14 84.10</cell><cell>96.00 96.88 95.78</cell><cell>+ 1 CGNL block</cell><cell>1 4 8 16</cell><cell>86.17 86.24 86.35 86.13</cell><cell>97.82 97.05 97.86 96.75</cell><cell>+ 5 CGNL block</cell><cell>1 4 8 16</cell><cell>86.01 86.19 86.24 86.43</cell><cell>95.97 96.07 97.23 98.89</cell></row><row><cell>Embedded Gaussian</cell><cell>84.01</cell><cell>96.08</cell><cell></cell><cell>32</cell><cell>86.04</cell><cell>96.69</cell><cell></cell><cell>32</cell><cell>86.10</cell><cell>97.13</cell></row><row><cell cols="3">(b) Results of comparison on UCF-</cell><cell cols="8">(d) Results of grouped CGNL networks on Mini-Kinetics. More groups</cell></row><row><cell cols="3">101. Note that CGNL network is not</cell><cell cols="7">help the CGNL networks improve top1 accuracy obveriously.</cell><cell></cell></row><row><cell>grouped in channel.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>model</cell><cell>gorups</cell><cell>top1</cell><cell>top5</cell><cell>model</cell><cell>gorups</cell><cell>top1</cell><cell>top5</cell></row><row><cell>model</cell><cell>top1</cell><cell>top5</cell><cell>R-50</cell><cell>-</cell><cell>75.54</cell><cell>92.16</cell><cell>R-101</cell><cell>-</cell><cell>77.44</cell><cell>93.18</cell></row><row><cell>R-50. + 1 NL block + 1 CGNL block</cell><cell>81.62 82.88 83.38</cell><cell>94.62 95.74 95.42</cell><cell>+ 1 CGNL block</cell><cell>1 4 8</cell><cell>77.16 77.56 77.76</cell><cell>93.56 93.00 93.18</cell><cell>+ 1 CGNL block</cell><cell>1 4 8</cell><cell>78.79 79.06 79.54</cell><cell>93.64 93.54 93.84</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results comparison of the CGNL block to the simple residual block on CUB dataset.</figDesc><table><row><cell>model</cell><cell>top1</cell><cell>top5</cell></row><row><cell>R-50</cell><cell>84.05</cell><cell>96.00</cell></row><row><cell>+ 1 Residual Block</cell><cell>84.11</cell><cell>96.23</cell></row><row><cell>+ 1 CGNL block</cell><cell>85.14</cell><cell>96.88</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Main results. Top1 and top5 accuracy (%) on various datasets.</figDesc><table><row><cell cols="3">(a) Main validation results on</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Mini-Kinetics. The CGNL net-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">works is build within 8 groups.</cell><cell>model</cell><cell>top1</cell><cell>top5</cell><cell>model</cell><cell></cell><cell>top1</cell><cell></cell><cell>top5</cell></row><row><cell></cell><cell></cell><cell></cell><cell>R-50</cell><cell>84.05</cell><cell>96.00</cell><cell>R-101</cell><cell></cell><cell cols="2">85.05</cell><cell>96.70</cell></row><row><cell>model</cell><cell>top1</cell><cell>top5</cell><cell>+ 1 NL block</cell><cell>84.79</cell><cell>96.76</cell><cell cols="2">+ 1 NL block</cell><cell cols="2">85.49</cell><cell>97.04</cell></row><row><cell>R-50</cell><cell>75.54</cell><cell>92.16</cell><cell>+ 1 CGNL block</cell><cell>85.14</cell><cell>96.88</cell><cell cols="2">+ 1 CGNL block</cell><cell cols="2">86.35</cell><cell>97.86</cell></row><row><cell>+ 1 NL block</cell><cell>76.53</cell><cell>92.90</cell><cell>+ 5 NL block</cell><cell>85.10</cell><cell>96.18</cell><cell cols="2">+ 5 NL block</cell><cell cols="2">86.10</cell><cell>96.35</cell></row><row><cell>+ 1 CGNL block</cell><cell>77.76</cell><cell>93.18</cell><cell>+ 5 CGNL block</cell><cell>85.68</cell><cell>96.69</cell><cell cols="2">+ 5 CGNL block</cell><cell cols="2">86.24</cell><cell>97.23</cell></row><row><cell>+ 5 NL block</cell><cell>77.53</cell><cell>94.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>+ 5 CGNL block</cell><cell>78.79</cell><cell>94.37</cell><cell cols="8">(c) Results on COCO. 1 NL or 1 CGNL block is added in Mask R-CNN.</cell></row><row><cell>R-101 + 1 NL block</cell><cell>77.44 78.02</cell><cell>93.18 93.86</cell><cell>model</cell><cell>AP box</cell><cell>AP box 50</cell><cell>AP box 75</cell><cell>AP mask</cell><cell>AP mask 50</cell><cell cols="2">AP mask 75</cell></row><row><cell>+ 1 CGNL block</cell><cell>79.54</cell><cell>93.84</cell><cell>Baseline</cell><cell>34.47</cell><cell>54.87</cell><cell>36.58</cell><cell>30.44</cell><cell>51.55</cell><cell cols="2">31.95</cell></row><row><cell>+ 5 NL block</cell><cell>79.21</cell><cell>93.21</cell><cell>+ 1 NL block</cell><cell>35.02</cell><cell>55.79</cell><cell>37.54</cell><cell>30.23</cell><cell>52.40</cell><cell cols="2">32.77</cell></row><row><cell>+ 5 CGNL block</cell><cell>79.88</cell><cell>93.37</cell><cell>+ 1 CGNL block</cell><cell>35.70</cell><cell>56.07</cell><cell>38.69</cell><cell>31.22</cell><cell>52.44</cell><cell cols="2">32.67</cell></row></table><note><p>(b) Results on CUB. The CGNL networks are set by 8 channel groups.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Results on ImageNet. Best top1 and top5 accuracy (%).</figDesc><table><row><cell>model</cell><cell>top1</cell><cell>top5</cell></row><row><cell>R-50</cell><cell>76.15</cell><cell>92.87</cell></row><row><cell>+ 1 CGNL block</cell><cell>77.69</cell><cell>93.64</cell></row><row><cell>+ 1 CGNLx block</cell><cell>77.32</cell><cell>93.46</cell></row><row><cell>R-152</cell><cell>78.31</cell><cell>94.06</cell></row><row><cell>+ 1 CGNL block</cell><cell>79.53</cell><cell>94.59</cell></row><row><cell>+ 1 CGNLx block</cell><cell>79.37</cell><cell>94.47</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>Bold capital letters denote a matrix X, bold lower-case letters a column vector x. xi represents the i th column of the matrix X. xij denotes the scalar in the i th row and j th column of the matrix X. All non-bold letters represent scalars. 1m ∈ R m is a vector of ones. In ∈ R n×n is an identity matrix. vec(X) denotes the vectorization of matrix X. X • Y and X ⊗ Y are the Hadamard and Kronecker products of matrices.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>https://github.com/facebookresearch/video-nonlocal-net</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P J U L J A N G L K</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic segmentation with second-order pooling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="430" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Kernel pooling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Compact bilinear pooling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="317" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Accurate, large minibatch sgd: training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
	<note>Computer Vision (ICCV)</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01507</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<title level="m">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bilinear cnn models for fine-grained visual recognition</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1449" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Understanding the effective receptive field in deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4898" to="4906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="143" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast and scalable polynomial kernels via explicit feature maps</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pagh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="239" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Networks for approximation and learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Girosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="1481" to="1497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning with kernels: support vector machines, regularization, optimization, and beyond</title>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">What does it take to generate natural textures</title>
		<author>
			<persName><forename type="first">I</forename><surname>Ustyuzhaninov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD birds-200-2011 dataset</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Action recognition by dense trajectories</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3169" to="3176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07971</idno>
		<title level="m">Non-local neural networks</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08494</idno>
		<title level="m">Group normalization</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Rethinking spatiotemporal feature learning for video understanding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.04851</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01083</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
