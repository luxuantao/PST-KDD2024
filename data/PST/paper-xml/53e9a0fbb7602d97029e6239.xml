<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Near-duplicate Detection and Sub-image Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yan</forename><surname>Ke</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Intel Research Pittsburgh</orgName>
								<address>
									<addrLine>Carnegie Mellon University 417 S. Craig Street Suite 300 Pittsburgh</addrLine>
									<postCode>15213, 15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA, PA</region>
									<country>U.S.A. U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
							<email>rahuls@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Intel Research Pittsburgh</orgName>
								<address>
									<addrLine>Carnegie Mellon University 417 S. Craig Street Suite 300 Pittsburgh</addrLine>
									<postCode>15213, 15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA, PA</region>
									<country>U.S.A. U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Larry</forename><surname>Huston</surname></persName>
							<email>larry.huston@intel.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Intel Research Pittsburgh</orgName>
								<address>
									<addrLine>Carnegie Mellon University 417 S. Craig Street Suite 300 Pittsburgh</addrLine>
									<postCode>15213, 15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA, PA</region>
									<country>U.S.A. U.S.A</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Near-duplicate Detection and Sub-image Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9AC6B84814CC51E11BB9BA6BCD79DA47</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.2.8 [Database Management]: Database Applications-Image databases; I.4.10 [Computing Methodologies]: Image Processing and Computer Vision-Image Representation Algorithms</term>
					<term>Performance</term>
					<term>Experimentation Sub-image retrieval</term>
					<term>Near-duplicate image detection</term>
					<term>Interest points</term>
					<term>Local image descriptors</term>
					<term>Locality-sensitive hashing (LSH)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a system for near-duplicate detection and sub-image retrieval. Such a system is useful for finding copyright violations and detecting forged images. We define near-duplicates as images altered with common transformations such as changing contrast, saturation, scaling, cropping, framing, etc. Our system builds a parts-based representation of images using distinctive local descriptors which give high quality matches even under severe transformations. To cope with the large number of features extracted from the images, we employ locality-sensitive hashing to index the local descriptors. This allows us to make approximate similarity queries that only examine a small fraction of the database. Although locality-sensitive hashing has excellent theoretical performance properties, a standard implementation would still be unacceptably slow for this application. We show that, by optimizing layout and access to the index data on disk, we can efficiently query indices containing millions of keypoints. Our system achieves nearperfect accuracy (100% precision at 99.85% recall) on the tests presented in Meng et al. [16], and consistently strong results on our own, significantly more challenging experiments. Query times are interactive even for collections of thousands of images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Near-duplicate image detection and sub-image retrieval is an important problem with several applications. Our system is motivated by two practical scenarios: finding (potentially modified) copyrighted images <ref type="bibr" target="#b1">[1]</ref> and detecting forged images <ref type="bibr" target="#b6">[6]</ref>.</p><p>As more images are published on the Web, and as image manipulation software becomes more powerful and user-friendly, pirating images is becoming increasingly easy. Although digital watermarking techniques exist, these schemes are very difficult to design and there is an inherent trade-off between the robustness of the watermark and the amount of degradation induced in the image. To circumvent digital watermarking, the pirated images are often altered slightly -for instance, by cropping and rescaling. The problem of matching a slightly altered photograph to its original is termed near-duplicate image detection. A photo publishing agency could use a system like ours to automatically identify potential copyright violations and dispense with digital watermarks altogether.</p><p>A more insidious form of image manipulation, one that is becoming increasingly popular in propaganda, is the creation of fake photographs by cutting and pasting pieces extracted from different original sources. For instance, one could crop political figures from two different photographs and create a fake composite image showing them shaking hands, even though the individuals may never have met in reality <ref type="bibr" target="#b6">[6]</ref>. Figure <ref type="figure">1</ref> shows an example where a girl's head from one painting was grafted into a scene from another painting. The problem of matching a small portion of one image to its original is termed sub-image retrieval. If the original images were stored in our system, we could detect query images that were composites and accurately identify the exact sources used in their creation.</p><p>We believe that practical systems that address the applications discussed above should satisfy the following requirements:</p><p>1. High recall. All images in the database that contain subimages that are present in the query image should be found, even if the sub-images only occupy a small portion of the original. 2. High precision. If the database images and the query image do not have sub-images in common, then they should not be matched. This is important because incorrectly-flagged images will waste the user's time. 3. Efficiency. The time needed to query an image should be small, enabling the system to scale to very large databases. Near-duplicate image detection and sub-image retrieval have both been studied extensively in recent years <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b19">19]</ref>. However, previous work has typically cast the task into a traditional content-based image retrieval (CBIR) context, which tends to suffer from the following two problems. First, many techniques calcu-Query image 210x174 Retrieved images 500x800 510x650</p><p>Figure <ref type="figure">1</ref>: Example of sub-image retrieval applied to forgery detection. Given the top (forged) image as a query, our system correctly retrieves the bottom two (source) images, without any false matches, from a database containing 6100 images of similar paintings. Note that the query image contains only a very small portion from each source image, and that these portions have been cropped, resized and rotated.</p><p>late and store global statistics for each image, which is efficient but insufficiently accurate: recall can suffer when a significant transformation perturbs global statistics; and precision can be poor because global statistics, such as histograms (that are robust to geometric transforms), tend to generate many false positives. Second, those systems that compute local statistics of an image (e.g., by partitioning an image into smaller pieces) can suffer from low precision <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b19">19]</ref>. This is because they typically concatenate all of an image's local statistics into a single feature vector describing the image. They must then relax the matching threshold in order to match small parts of the feature vectors, which makes the vectors less distinctive. We argue that, instead of using a single feature vector to describe an entire image, one should identify and independently index a large number of local features, each of which is highly distinctive, while being robust to typical image transforms. Such an approach would selectively identify local features that match extremely well, rather than seeking loose partial matches between complicated global image features. Unlike existing techniques, such a scheme would be highly resistant to occlusions and cropping, both of which can destroy a significant fraction of the features. The main drawback of the proposed approach is that a single image could generate thousands of local features, and a single query would require the system to search for each of these features in a database containing millions or billions of features. Since features would not generate exact matches, each of the individual searches would become a similarity query in a very highdimensional feature space. Consequently, such approaches have previously been dismissed as computationally impractical. However, this paper shows that these ideas can become the foundation for near-duplicate and sub-image retrieval system that is both extremely accurate and that scales well to large image collections.</p><p>The research that is most similar to ours is <ref type="bibr" target="#b1">[1]</ref>, where local features are extracted from images and matched using an approximate similarity search. Our system differs significantly in the following respects. First, we use scale-and rotation-invariant interest point detectors, more distinctive local descriptors, and perform geometric verification on the matched features. Second, instead of an ad hoc approximate similarity search, we employ locality-sensitive hashing <ref type="bibr" target="#b7">[7]</ref>, an algorithm with provable performance bounds. These contribute to the dramatic improvements in accuracy shown in Section 5.1. Third, we build offline indices that are optimized for disk access and search for all of the query local descriptors in a single pass. This enables us to query large image collections in interactive time.</p><p>The remainder of this paper is organized as follows. Section 2 reviews the relevant research for each of our components. Section 3 describes our system and gives implementation details. Section 4 details our evaluation metrics, experimental methodology, and the different datasets. Section 5 presents results of our system's retrieval accuracy and explores the impact of individual optimizations on execution time. Finally, we discuss some limitations of our work in Section 6 and conclude in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BACKGROUND</head><p>We first introduce the three building blocks of our system: distinctive interest points, locality-sensitive hashing, and efficient layout of data on disk. Using distinctive interest points allows us to achieve high recall and precision, while using locality-sensitive and efficient data layout gives our system interactive query times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Distinctive Interest Points</head><p>Interest points <ref type="bibr">[8,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b21">21]</ref> are commonly employed in a number of real-world applications such as object recognition <ref type="bibr" target="#b4">[4]</ref> and image retrieval <ref type="bibr" target="#b17">[17]</ref> because they can be computed efficiently, are resistant to partial occlusion, and are relatively insensitive to changes in viewpoint. There are three considerations to using interest points in these applications. First, the interest points should be localized in position and scale. Typically, interest points are placed at local peaks in a scale-space search, and filtered to preserve only those that are likely to remain stable over transformations. Second, the neighborhood surrounding each interest point should be modeled by a local descriptor. Ideally, this description should be distinctive (reliably differentiating one interest point from others), concise, and invariant over expected geometric and photometric transformations. Finally, the matching between local descriptors must be accurate and computationally efficient (discussed further in Section 2.2).</p><p>For interest point detection, we use Lowe's Difference of Gaussian <ref type="bibr" target="#b13">[13]</ref> (DoG) detector because it has been shown to be robust and efficient. The DoG detector consists of three major stages:</p><p>(1) scale-space peak selection; (2) interest point localization; (3) orientation assignment. In the first stage, potential interest points are identified by scanning the image over location and scale. This is implemented efficiently by constructing a Gaussian pyramid and searching for local peaks, termed keypoints, in a series of differenceof-Gaussian images. In the second stage, candidate keypoints are localized to sub-pixel and sub-scale accuracy, and eliminated if found to be unstable. The third stage identifies the dominant orientations for each keypoint based on its local image patch. The assigned orientation(s), scale and location for each keypoint enables us to construct a canonical view for the keypoint that is invariant to similarity transforms. For a 640x480 pixel image, we typically find hundreds to thousands of keypoints in the image, depending on the complexity of the image. Figure <ref type="figure" target="#fig_1">2</ref> shows the keypoints found in  For interest point representation, we use PCA-SIFT <ref type="bibr" target="#b11">[11]</ref>, a local descriptor that has been shown to be both more distinctive and compact than the original SIFT <ref type="bibr" target="#b13">[13]</ref> descriptor. Given the location, size, and orientation of a keypoint, PCA-SIFT extracts a 41 × 41 pixel patch at the given scale and rotates it to a canonical orientation. The extracted patch covers an area in the original image proportional to the size of the keypoint. PCA-SIFT then generates its compact feature vector by computing the local gradient image of the patch, normalizing it, and projects it onto a precomputed eigenspace. As described in <ref type="bibr" target="#b11">[11]</ref>, this eigenspace is generated once (off-line) from a large number of keypoints extracted from images of natural scenes, and is not specific to our image collection. The top 36 components of the projected vector are used as the local descriptor.</p><p>The use of local descriptors has several characteristics that are ideal for solving the near-duplicate image detection problem. First, the interest points are scale and rotation invariant. This allows us to detect and match the same set of interest points even after images have been arbitrarily rotated or scaled. Our approach is also robust to deformations such as Gaussian blurring, median filtering, and the addition or removal of noise, which can degrade or destroy the high frequency content of the original image. This is because a subset of interest points in the original image will continue to match those interest points that encode lower frequency content in the transformed image (corresponding to larger image areas). Second, the descriptors are robust to image deformations such as affine warp, changes in brightness and contrast, etc. Furthermore, PCA-SIFT ignores color and operates on gray-scale images, making the algorithm immune to transforms that manipulate the color content of the image, such as saturation and colorization. Finally, because we use local descriptors, our system can find matches even if there is significant occlusion or cropping in the images. The system requires as few as five interest points (out of hundreds) to match between two images in terms of descriptor similarity and geometric constraints. Despite the small number of interest points needed to match, we maintain a low false positive rate because the local de-scriptors are highly distinctive and the geometric constraints further discard many false positives. In practice, the smallest sub-image we can reliably match between two images is approximately 100 × 100 pixels. This technique is also well suited to approximate similarity search algorithms, where one achieves a much faster query time at the cost of missed matches; note that although recall may suffer at the keypoint level, the overall recall of the system can continue to be very high because so few keypoint matches are needed.</p><p>Because of the large number of keypoints present in each image, it is cost prohibitive to do a linear search through the database for each query. Therefore, we employ an approximate similarity search that is well suited for high dimensional data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Locality Sensitive Hashing</head><p>Locality-sensitive hashing (LSH), proposed by Indyk &amp; Motwani <ref type="bibr" target="#b10">[10]</ref>, is an approximate similarity search technique that works efficiently even for high-dimensional data. Traditional data structures for similarity search suffer from the curse of dimensionality, in that they scale poorly for data with dimensions greater than 20, where they perform no better than an exhaustive linear search through the entire database. It has been shown that LSH out-performs tree-based structures such as the Sphere/Rectangle-tree (SR-tree) by at least an order of magnitude. Given that our data consists of many, high-dimensional (36-dimensional) feature vectors, LSH becomes an attractive indexing scheme.</p><p>Locality-sensitive hashing solves the following similarity search problem, termed (r, ε)-NN, in sub-linear time. If, for a point q (query) in d-dimensional space, there exists an indexed point p such that d(p, q) ≤ r, then LSH will, with high probability, return an indexed point p such that d(p , q) ≤ (1 + ε)r. If no indexed point lies within (1 + ε)r of q, then LSH will return nothing with high probability. This is accomplished using a set of special hash functions that satisfy the intuitive notion that the probability of a hash collision for two points be related to the similarity (distance) between the points. By using multiple such hash functions in parallel, LSH reduces the rate of false negatives.</p><p>A popular algorithm for LSH, introduced by Gionis et al. <ref type="bibr" target="#b7">[7]</ref> conceptually transforms each point p into a binary vector by concatenating the unary representation of each (discretized) coordinate of p. The resulting bit string is a point in a high-dimensional Hamming space, where L1 distances between points in the original space are preserved. Hash functions that simply select a subset of the bits that satisfy the desired locality-sensitive properties. The algorithm builds a set of l such hash functions, each of which selects k bits from the bit string (each function uses a different, randomlyselected set of k bits). These k bits are hashed once more to index into the buckets in our hash table, and a 32-bit checksum hash value is also generated. The two parameters, k and l enable the designer to select an appropriate trade-off between accuracy and running time. Our implementation of this algorithm is described in Section 3. In our experiments, we use k=450 and l=20, based on performance on a separate validation dataset. As seen in Section 5.2, our choice of (k, l) favors execution speed over similarity point recall; given that each image contains hundreds of keypoints, we are willing to risk missing a significant fraction of them in exchange for speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Efficient Disk Access</head><p>Locality-sensitive hashing was originally designed to work efficiently in memory, where random access is fast. For large datasets, one must store the database on disk, and a naive implementation of LSH fails miserably. This is because random access on disk is expensive, on the order of 10ms per seek. Multiple queries into a hash table, by definition, requires random seeks on disk. Our initial experiments revealed that querying our database for the keypoints from just one image took several minutes, indicating that the standard LSH implementation could never be practical for our problem.</p><p>The key difference between our system and other systems that use LSH for other applications is that all of our queries occur in batches of hundreds or thousands (corresponding to all of the keypoints in the query image). We extract the keypoints from the query image, and search on the entire set of keypoints to determine if any of them match the keypoints in the database. An earlier disk-based implementation of LSH by Gionis et al. <ref type="bibr" target="#b7">[7]</ref> was designed for efficient single point queries rather than the batch queries required by our system. Since disk seek times are the bottleneck, our approach relies on organizing the batch queries so as to minimize the motion of the disk heads. We do this by precomputing all of the hash bins that we need to access, sorting them, and accessing them in sequential order. Reducing the disk head motion in this manner translates to a dramatic improvement in effective seek time -cutting it to approximately 1ms per seek. Gionis et al. also suggested inlining the data in the hash table instead of storing only the pointers as one would for an in-memory implementation. Their goal was to halve the number of seeks because one would not need to follow a pointer to the actual data. However, for our application, inlined data led to a massive increase in required disk space (20x for our dataset) and actually slowed our search. Since our searches do not require random seeks, we achieve better performance by employing a small hash table with an auxiliary keypoint database (and scanning both in-order) rather than a large hash table with inlined data.</p><p>All of these components are required to make our system practical. The use of robust interest point detectors and distinctive local descriptors enables us to query images with high recall and precision, as shown in section 5. By using locality-sensitive hashing and optimizing the data layout on disk, we achieve interactive response times for queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">IMPLEMENTATION</head><p>This section describes the implementation details of our system. Our algorithm consists of two stages. First, in the index construction phase, we process the image collection and index all of the extracted keypoints. Then, in the database query phase, the user can issue queries to find near-duplicates or to perform sub-image retrieval. These are summarized in Figure <ref type="figure" target="#fig_3">4</ref> and detailed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Index Construction</head><p>Given the collection of images to be indexed, we first use the SIFT DoG detector to locate all of the interest points. Then, we use PCA-SIFT to build local descriptors using a small image patch centered around each interest point. The source code for these two steps was downloaded from the web and the default parameters for each algorithm were employed.</p><p>We create three disk-based data structures, which are carefully laid sequentially on disk. <ref type="foot" target="#foot_0">1</ref> The data structures store a list of file names (FT), a list of keypoints from all the images (KT), and the locality-sensitive hash table of pointers to the keypoints (HTs). We construct the data structures, illustrated in Figure <ref type="figure" target="#fig_2">3</ref>, as follows.</p><p>First, we create the file name table (FT) using a list of fixed-sized records on disk. Each record is 256 bytes in length, where the first byte denotes the length of the file name and the rest are used to store the string. Implicitly, the id of each file is its index location in the name table. Similarly, we create the keypoint table (KT) using fixed-sized records. Each record stores one keypoint and consists of a file id (where the keypoint came from), its x and y location, orientation, scale, and its local descriptor. In total, each record is 92 bytes in length. Assuming that there are a thousand keypoints per image, it takes approximately 90MB to store the keypoints from one thousand images. Wherever possible, our implementation optimizes disk read access. For instance, given a list of keypoints that need to be read from disk, we first sort the list by keypoint id, thus ordering the disk reads to be efficient, and thereby reducing the average seek time.</p><p>Finally, we create the locality-sensitive hash tables (HTs). Recall that the LSH algorithm builds l independent hash tables, each with its own hash function. Below, we describe the layout of just one of these hash tables. All of the independent hash tables are concatenated and stored sequentially on disk. The hash tables are of fixed size, so the number of keypoints that we need to store must be determined before we create the hash tables. Each hash table consists of B buckets, where each bucket can store up to m keypoints. With a utilization value of α, we need B = n/(αm) buckets to store n keypoints. A higher α will lead to better space utilization, with an increased risk that some keypoints will not be indexed due to full buckets. A smaller bucket size m will lead to faster search times, but also a higher risk of dropped keypoints. For our experiments, we set m = 20 and α = 0.5. Two items are stored per keypoint: the keypoint id and a checksum hash value that enables the system to avoid verifying every keypoint in the same bucket. Therefore, the system utilizes approximately 16MB of storage, per independent hash table, per million keypoints. One of our major system performance optimizations is that each hash table is created separately and entirely in memory before being written to disk. It would be impractical to create the hash table otherwise. This is feasible because we're only storing pointers to keypoints, and thus we can easily accommodate databases of 50 million keypoints (from 50 thousand images) in 1GB of main memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Database query</head><p>Once the index is created on disk, we can issue queries on new images using a parallel set of operations. First, we locate interest points in the query image and build local descriptors, as described above. Then, we calculate the bucket id's of each keypoint using the locality hash functions without accessing the disk. If we were to read data from the hash buckets (from HTs) as we hashed each keypoint, then this would be equivalent to doing random seeks on disk and would be unfeasibly slow. Instead, we sort the bucket id's and read the buckets in order, which corresponds to a linear seek on disk. We read all of the keypoints within a bucket and confirm that the checksum hash values match. All of the candidate keypoints are stored in a list sorted by keypoint id.</p><p>Finally, we read the keypoint data (location, orientation, size, and descriptor) from the keypoint table (KT) in order to generate a list of candidate matches for the query keypoints. Because LSH only returns approximate matches with respect to the L1 (Manhattan) norm, we need to check both for false positives and for points outside the threshold distance under the L2 (Euclidean) norm. We discard false matches by checking that the distance between the local descriptors of the query keypoint and the candidate keypoints is within the threshold distance under L2.</p><p>At this point, we look up the file id (in FT) corresponding to matched keypoints and separate them according to file id. The greater the number of matches found per file, the more likely it is that the image is a near-duplicate. However, it is still likely that there are significant false positives at the keypoint match phase. In other words, although some keypoints are within the threshold distance, they belong to patches of images that are not near-duplicates. We do affine geometric verification using RANSAC <ref type="bibr" target="#b5">[5]</ref> to eliminate such outliers. The affine transformation between two images can be derived using three pairs of matched keypoints. RANSAC verifies whether the majority of the other matched keypoints support this transform and discards any outliers. The remaining pairs of matched keypoints correspond to the target image under an affine warp from the query image. The affine transformation includes rotation, scale, and shearing along the axes. The remaining set of images are discarded if fewer than θ matches are found, where θ is an adjustable parameter that controls the recall-precision of the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EVALUATION METHODOLOGY</head><p>We present a number of experiments to show the effectiveness of our system. For our initial experiments, we use the methodology from <ref type="bibr" target="#b16">[16]</ref> so that our results can be compared. A direct comparison should ideally test both algorithms on the same dataset. Unfortunately, neither their source code nor their image database was available. Therefore, we have created a very similar dataset by using identical image transformations for our experiments, and compare our algorithm against Meng et al.'s published results <ref type="bibr" target="#b16">[16]</ref>.</p><p>The experiments employ a small set of query images (probes) and a much larger set of test images (gallery). The gallery is composed of transformed versions of the probe images, augmented by a large number of similar-looking random images that serve as distractors. We use the transformations described in <ref type="bibr" target="#b16">[16]</ref> and build a gallery with the same fraction of distractors.</p><p>We also created a significantly more challenging gallery by applying difficult transformations to the probe images to gain additional insight into the performance of our algorithms. All of our datasets are detailed below, and they can be downloaded from http://www.cs.cmu.edu/˜yke/retrieval/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Index Construction</head><p>1. For each image in the gallery: 2.</p><p>Find keypoints using DoG detector 3.</p><p>Build PCA-SIFT local descriptors for each keypoint 4. Build and store file name table (FT) 5. Build and store keypoint table (KT) 6. For each of the l hash tables (HTs): 7.</p><p>For each keypoint: 8.</p><p>Hash keypoint and store id in table (in memory) 9.</p><p>Store hash table (HT) on disk</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Database Query</head><p>1. Find keypoints in query image using DoG detector 2. For each keypoint: 3.</p><p>Build its PCA-SIFT local descriptor 4.</p><p>Compute the l LSH hashes for the descriptor 5. Sort hashes by bucket id, scan hash tables (HTs) 6. Sort returned keypoint ids and scan KT linearly 7. For each returned image: 8.</p><p>Determine best affine transform using RANSAC 9.</p><p>Discard if a valid transform was not found 10. Print matched file names by reading FT </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation Metrics</head><p>In order to evaluate our system's performance, we measure the recall and precision of our algorithm. Intuitively, we want to maximize the number of correct positives and minimize the number of false positives. A correct positive is defined as a match between a probe image and one of its transformed versions in the gallery. Recall and precision are defined as: recall = number of correct-positives total number of positives and precision = number of correct-positives total number of matches (correct or false) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental setup</head><p>For our first experiment, our image database consists of 6261 images of fine art downloaded from an online art gallery <ref type="bibr" target="#b2">[2]</ref>. We resize each image so that the size of its larger dimension is 512 pixels. We randomly select 150 images to be the probes, and use the rest are added to the gallery as distractors. Each probe is modified according to the following 40 transformations, and these 6000 nearduplicates are added to the gallery to create a dataset with 12,111 images.</p><p>Using the Difference of Gaussian interest point detector, we identify 13.6 million keypoints in the images, and apply PCA-SIFT to build local descriptors. There are 1100 keypoints per image on average, although some images generate as few as 67 keypoints and others as many as 3000, depending on the complexity of the image content. We use locality-sensitive hashing to index all of the keypoints with parameters k=450 and l=20. We define two PCA-SIFT descriptors as matching if their L2 distance is within 3000. The minimum match threshold θ is 5 for our experiments. These parameter values were empirically selected using tests on a small validation set. All of our experiments use a 3GHz Intel R Pentium R 4 machine with 1GB of memory running Linux 2.4. The algorithms are implemented in C++.</p><p>As discussed above, our image transforms are identical to those described in <ref type="bibr" target="#b16">[16]</ref>, and are implemented using ImageMagick <ref type="bibr" target="#b9">[9]</ref>. These 40 transforms are described below. The number in brackets next to each operation denotes the number of near-duplicate images generated by that particular operation. 2  1. Colorizing <ref type="bibr" target="#b3">[3]</ref>: Tint the (a) red, (b) green, or (c) blue channels of the image by 10%. 2. Changing contrast <ref type="bibr" target="#b2">[2]</ref>: (a) Increase or (b) decrease image contrast using default parameters. 3. Cropping <ref type="bibr" target="#b4">[4]</ref>: Crop the image by (a) 5%, (b) 10%, (c) 20%, or (d) 30%, preserving the center region. Resize cropped image to original size. 4. Despeckling <ref type="bibr" target="#b1">[1]</ref>: ImageMagick's despeckle operation. 5. Downsampling <ref type="bibr" target="#b7">[7]</ref>: Downsample (without Gaussian blurring) the image so that its size is reduced by (a) 10%, (b) 20%, (c) 30%, (d) 40%, (e) 50%, (f) 70%, or (g) 90%. 6. Changing format <ref type="bibr" target="#b1">[1]</ref>: Convert JPEG source image to GIF format. This compresses the color space to a palette of 256 colors. 7. Framing <ref type="bibr" target="#b4">[4]</ref>: Add an outer frame to the image, where the size of the frame is 10% of the framed image. Four images are produced, each with a frame of a random color. 8. Rotating <ref type="bibr" target="#b4">[4]</ref>: Rotate image by (a) 90 • , (b) 180 • , or (c) 270 • . 9. Scaling <ref type="bibr" target="#b6">[6]</ref>: Scale the size of image up by (a) 2, (b) 4, (c) 8 times; or down by (d) 4, (e) 4, (f) 8 times. The scaled image is resized to the original size. 10. Changing saturation <ref type="bibr" target="#b5">[5]</ref>: Change image saturation amplitude by (a) 70%, (b) 80%, (c) 90%, (d) 110%, or (e) 120%. 11. Changing intensity. <ref type="bibr" target="#b4">[4]</ref>: Change image intensity amplitude by (a) 80%, (b) 90%, (c) 110%, or (d) 120%. Note that because we use rotation-, scale-, and illumination-invariant grayscale local descriptors, we are inherently robust to all of the above transforms (confirmed by results in Section 5). To examine where our system could fail, we constructed a second set of more difficult experiments with larger data sets. We randomly selected 15,582 photos from the MM270K <ref type="bibr" target="#b15">[15]</ref> database and randomly chose 314 of them as query images. We applied the following more difficult transformations to both the art and MM270K databases:</p><p>12. Cropping <ref type="bibr" target="#b3">[3]</ref>: Crop the image by (a) 50%, (b) 70%, or (c) 90%, preserving the center region. Resize cropped image to original size. 13. Shearing <ref type="bibr" target="#b3">[3]</ref>: Apply an affine warp along the x axis by (a) 5 • , (b) 10 • , or (c) 15 • . 14. Changing intensity <ref type="bibr" target="#b2">[2]</ref>: Change the brightness of the image by (a) 50% or (b) 150%. 15. Changing contrast <ref type="bibr" target="#b2">[2]</ref>: (a) Increase contrast by 3x, or (b) decrease contrast 3x. Figure <ref type="figure">5</ref> shows a small subset of these transformations. Our last series of experiments verifies that our system can apply sub-image retrieval to detect forged images. For the first test, we manually generated three fake images by carefully tracing and pasting images of people from six source photographs. Using the fake images as probes, our system correctly identifies all of the source images with no false positives. The second test used automaticallygenerated composite images as queries. Each composite was generated by drawing a pair of random images from a gallery of 1000 images, and the center 10% region from one image was selected and pasted on to the other. We created a probe set of 1000 images, 2 Our tests do not include the flipping transform; those are easily matched using mirrored versions of the probe images. and judged the system's response to be correct only if both of the source images for that query were correctly retrieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RESULTS</head><p>We now present our image retrieval accuracy results and the effect of our design choices on running time performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Retrieval results</head><p>Table <ref type="table" target="#tab_0">1</ref> shows the results on the first experiment using the set of transformations from <ref type="bibr" target="#b16">[16]</ref> on 12,111 images. We see that we perform extremely well on both the recall and precision. There are 150 query images, each with 40 near-duplicates in the database. Therefore, there are 6000 possible total correct matches. For θ = 5, our system only fails to find 9/6000 near-duplicates and generates zero false positives. By comparison, a baseline strategy that randomly selected 40 images from the gallery to match each probe would have a recall of 0.3% and a precision of 0.01%. Meng et al.'s system achieves a recall of 90% with a precision of 67% (as shown by the ROC plots published in <ref type="bibr" target="#b16">[16]</ref> on their dataset).</p><p>Table <ref type="table" target="#tab_1">2</ref> shows the results for the second set of experiments. The accuracy remains very high, despite the more challenging transformations used to generate the near-duplicates. For the art database, we used 150 query images and for the MM270K database, we used 314 query images. Each query image has 10 near-duplicates in the database. Although the recall is high for both databases, the precision is significantly lower for the MM270K database. The drop in precision is due to the fact that this database contains several images that are slightly-different views of the same scene, taken from the same location, at the same time. A manual inspection of the false positives shows them to be panned or zoomed versions of others in this database, or photos of the same scene taken with a different color filter. If these photos of the same scene are not penalized as false positives, our system's precision remains above 95%.</p><p>Finally, Table <ref type="table" target="#tab_2">3</ref> shows the results of querying the 1000 composite "forged" images to detect their sources. Again, our system does extremely well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Running time results</head><p>While the previous section focused on the accuracy of our nearduplicate detector, we now turn to the system design and show that   First, we compare the execution time of locality-sensitive hashing versus an exhaustive linear search through the keypoints, for the selected LSH parameters. Table <ref type="table" target="#tab_3">4</ref> shows the performance of searching for matches of 1010 keys in the database. We give the running time and the number of keys checked by each. We see that LSH, due to the approximate nature of its search, misses a very large fraction (71%) of keys that were correctly matched by the exhaustive search. On the other hand, our execution speed is faster by two orders of magnitude. Despite the large number of keys missed by LSH, our system still performs well because we extract hundreds of interest points per image, while requiring as few as θ=5 matching keys to identify near-duplicates.</p><p>Recall that our implementation of LSH assumes L1 (Manhattan) distance in the analysis of near neighbors, while PCA-SIFT requires L2 (Euclidean) distance to be calculated during the actual matching of keypoints. Our algorithm makes the implicit assumption that using L1 in the LSH stage does not force us to examine and discard too many points during the PCA-SIFT matching stage. Our next experiment quantifies the inefficiency induced by this assumption. Table <ref type="table" target="#tab_4">5</ref> shows that most of the keys (94%) that are checked but not matched under L2 are due to hash table collisions. The number of keys matched under L1 but not L2 only account for 2% of the total keys checked. Therefore, the L1 distance assumption is acceptable for our data. While the use of LSH gives us tremendous theoretical gains in performance, careful system design is required to realize LSH's benefits for our application. When creating the hash tables, we build the independent hash tables in memory, and then stream them sequentially to disk. Table <ref type="table" target="#tab_5">6</ref> compares the running time of building the hash table in memory and directly on disk. We see that eliminating the random seeks to disk reduces the running time by a factor of 7.</p><p>For queries, we must also linearize the disk accesses to remove as many random seeks as possible. By sorting the hash bucket id's and keypoint id's before reading them from disk, we get a dramatic improvement in running time. Table <ref type="table" target="#tab_6">7</ref> shows that this optimization results in a 20x speed-up. It is only by combining all of these optimizations that we are able to create a practical near-duplicate and forgery detector that scales to large image databases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">LIMITATIONS</head><p>While our experiments show excellent results in retrieving nearduplicates and sub-images, there are limitations to our technique. Our system can match similar images of the same scene even if they were not near-duplicates nor forged images, as in our MM270K database. This scenario could occur if the copyrighted image database contains pictures of famous landmarks, where there are likely to be many pictures of the same landmarks on the web. Our system may find similar keypoints on the landmarks and incorrectly match them. Others have exploited this property as a feature to detect images of the same scene taken from different camera locations as in <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b20">20]</ref>. To informally test the performance of our system under such conditions, we gathered 59 pictures from the Web of Big Ben, Eiffel Tower, and Half Dome, and queried them against 18 professional pictures of the same landmarks. We were pleased to see only one match (i.e., false positive) among the 59 queries. The reason why we do not accidentally identify similar scenes as forged images more frequently is due to well-known limitations in applying current interest point detectors to recognize real-world objects. They include the instability of the DoG interest point detector on three dimensional objects and appearance changes due to self occlusion or shadowing. For example, during the course of the day, Big Ben's appearance changes significantly due to self-shadows. Similarly, Half Dome's appearance varies with seasonal vegetation. The sensitivity of keypoints to these appearance variations is seen as a limitation in the object recognition domain. However, for our application, this inability to generalize is a benefit. Since our goal is to detect perturbations of an image, we can set our system's appearance matching and geometric verification thresholds to much tighter bounds. This allows us to detect simple manipulations to copyright images while rejecting different views of the same scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSION</head><p>The primary contributions of our paper are the synthesis of recent advances in robust interest point detection (DoG detector), local descriptor representation (PCA-SIFT), and efficient similarity search of high-dimensional data (LSH). By using a robust interest point detector and distinctive local descriptors, we accurately solve the near-duplicate and sub-image retrieval problem. Because we use a parts-based approach, our system is highly resistant to cropping, scaling, and other common transforms that traditional approaches based on global features can not cope with. A potential drawback in using a parts-based approach is that the system needs to query hundreds to thousands of features at a time, which could be slow. We make our system theoretically efficient through localitysensitive hashing. Further, we make our system practical through careful data placement and batched disk accesses to minimize random seeks. We show experimentally that our system has near perfect accuracy (99.85% recall and 100% precision) on a standard test set. For future work, we plan to further optimize the data structures to gain additional query performance and further improve accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Original image (b) Rotated, scaled, and sheared</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The keypoints located in this pair of images are shown as white circles, with lines denoting dominant orientations and radius denoting scale. Note that the keypoints are found at the same locations in each image, enabling us to accurately match the transformed image to the original. Note that the size and orientation of the keypoints reflects how the image was scaled, rotated and sheared. For illustration purposes, keypoints with a very small scale are not shown.</figDesc><graphic coords="3,59.18,53.80,108.00,108.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Format of the disk-based data structures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Summary of our algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Original 5 (Figure 5 :</head><label>55</label><figDesc>Figure 5: Examples of automatically-generated near-duplicates. Only 7 out of 50 transforms are shown; all were correctly identified.</figDesc><graphic coords="7,95.99,161.11,88.41,72.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 : Recall-Precision for standard transformations.</head><label>1</label><figDesc></figDesc><table><row><cell>recall</cell><cell>precision</cell></row><row><cell cols="2">Baseline -select 40 random images</cell></row><row><cell>0.3%</cell><cell>0.01%</cell></row><row><cell cols="2">Weighted Sampling Threshold method from [16]</cell></row><row><cell>90%</cell><cell>67%</cell></row><row><cell>100%</cell><cell>6%</cell></row><row><cell cols="2">Our method on art database of 12,000 images</cell></row><row><cell>99.85%</cell><cell>100%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 : Recall-Precision for difficult transformations.</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell>recall</cell><cell>precision</cell></row><row><cell cols="2">Art database of 7,611 images</cell></row><row><cell></cell><cell cols="2">98.40% 99.86%</cell></row><row><cell cols="3">MM270K database of 18,722 images</cell></row><row><cell>Original</cell><cell cols="2">96.78% 88.78%</cell></row><row><cell cols="3">Same scene removed 96.78% 96.12%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 : Recall-Precision for composite images.</head><label>3</label><figDesc></figDesc><table><row><cell>recall</cell><cell>precision</cell></row><row><cell cols="2">98.85% 99.65%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 : Efficiency of LSH versus linear search.</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell>linear search</cell><cell>LSH</cell></row><row><cell>Running time in sec. (σ)</cell><cell>80.3 (0.06)</cell><cell>0.97 (0.04)</cell></row><row><cell>Pairs of keys checked</cell><cell>268 million</cell><cell>2656</cell></row><row><cell>Pairs of keys matched</cell><cell>5464</cell><cell>1611</cell></row><row><cell cols="3">each of our optimizations were necessary in order to make our ap-</cell></row><row><cell cols="3">proach practical. All of the following experiments were done us-</cell></row><row><cell cols="3">ing a small test set of 200 random images combined with 10 near-</cell></row><row><cell cols="3">duplicates of the query image from the difficult test set, for a total</cell></row><row><cell cols="3">of with 265,000 extracted keypoints. There are 1010 keys in the</cell></row><row><cell>query image.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 : Inefficiency due to L1 assumption.</head><label>5</label><figDesc></figDesc><table><row><cell>No. of keys</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 : Importance of building hash table in memory.</head><label>6</label><figDesc></figDesc><table><row><cell>Running time in sec. (σ)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 : Importance of linearizing disk accesses in queries.</head><label>7</label><figDesc></figDesc><table><row><cell></cell><cell>Running time in sec. (σ)</cell></row><row><cell>Unoptimized disk reads</cell><cell>65 (1.8)</cell></row><row><cell>Sorted disk reads</cell><cell>3.4 (0.1)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In practice, it is difficult for a user program to control the data layout on disk; we start with a defragmented disk.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Yan Ke is supported by the Intel Research Scholar program. We thank M. Satyanarayanan and Derek Hoiem for valuable feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust content-based image searches for copyright protection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Berrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Amsaleg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM Workshop on Multimedia Databases</title>
		<meeting>ACM Workshop on Multimedia Databases</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cgfa -A</forename><surname>Virtual</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Art</forename><surname>Museum</surname></persName>
		</author>
		<ptr target="http://cgfa.sunsite.dk/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">RIME: A replicated image detector for the world-wide web</title>
		<author>
			<persName><forename type="first">E</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wiederhold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Object class recognition by unsupervised scale-invariant learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2003-06">June 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="1981-06">June 1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Detection of copy-move forgery in digital images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fridrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Soukal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lukas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Digital Forensic Research Workshop</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Similarity search in high dimensions via hashing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Very Large Databases</title>
		<meeting>International Conference on Very Large Databases</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A combined corner and edge detector</title>
		<author>
			<persName><forename type="first">C</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stephens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Alvey Vision Conference</title>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<ptr target="http://www.imagemagick.org/" />
		<title level="m">ImageMagick</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Approximate nearest neighbortowards removing the curse of dimensionality</title>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Symposium on Theory of Computing</title>
		<meeting>Symposium on Theory of Computing</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">PCA-SIFT: A more distinctive representation for local image descriptors</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A software system for automatic albuming of consumer pictures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Loui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM International Conference on Multimedia</title>
		<meeting>ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Content based sub-image retrieval via hierarchical tree matching</title>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nascimento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM Workshop on Multimedia Databases</title>
		<meeting>ACM Workshop on Multimedia Databases</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m">Media Graphics International. 270,000 Multimedia Graphics Pack</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Enhancing DPF for near-replica image recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Indexing based on scale invariant interest points</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision</title>
		<meeting>International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2001-07">July 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automated location matching in movies</title>
		<author>
			<persName><forename type="first">F</forename><surname>Schaffalitzky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-scale sub-image search</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huijsmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM International Conference on Multimedia</title>
		<meeting>ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Video Google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision</title>
		<meeting>International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2003-10">Oct. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Affine/photometric invariants for planar intensity patterns</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ungureanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
