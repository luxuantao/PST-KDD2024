<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ExtEnD: Extractive Entity Disambiguation</title>
				<funder ref="#_sPG2xHg">
					<orgName type="full">MIUR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Edoardo</forename><surname>Barba</surname></persName>
							<email>edoardo.barba@uniroma1.it</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Sapienza NLP Group</orgName>
								<orgName type="institution">Sapienza University of Rome</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luigi</forename><surname>Procopio</surname></persName>
							<email>luigi.procopio@uniroma1.it</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Sapienza NLP Group</orgName>
								<orgName type="institution">Sapienza University of Rome</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
							<email>roberto.navigli@uniroma1.it</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Sapienza NLP Group</orgName>
								<orgName type="institution">Sapienza University of Rome</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ExtEnD: Extractive Entity Disambiguation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Local models for Entity Disambiguation (ED) have today become extremely powerful, in most part thanks to the advent of large pretrained language models. However, despite their significant performance achievements, most of these approaches frame ED through classification formulations that have intrinsic limitations, both computationally and from a modeling perspective. In contrast with this trend, here we propose EXTEND, a novel local formulation for ED where we frame this task as a text extraction problem, and present two Transformer-based architectures that implement it. Based on experiments in and out of domain, and training over two different data regimes, we find our approach surpasses all its competitors in terms of both data efficiency and raw performance. EXTEND outperforms its alternatives by as few as 6 F 1 points on the more constrained of the two data regimes and, when moving to the other higher-resourced regime, sets a new state of the art on 4 out of 6 benchmarks under consideration, with average improvements of 0.7 F 1 points overall and 1.1 F 1 points out of domain. In addition, to gain better insights from our results, we also perform a fine-grained evaluation of our performances on different classes of label frequency, along with an ablation study of our architectural choices and an error analysis. We release our code and models for research purposes at https:// github.com/SapienzaNLP/extend.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Being able to associate entity mentions in a given text with the correct entity they refer to is a crucial task in Natural Language Processing (NLP). Formally referred to as Entity Disambiguation (ED), this task entails, given a mention m occurring in a text c m , identifying the correct entity e * out of a set of candidates e 1 , . . . , e n , coming from a reference knowledge base (KB). First introduced by Bunescu * Equal contribution.</p><p>and <ref type="bibr" target="#b8">Pa?ca (2006)</ref>, ED aims to identify the actors involved in human language and, as such, has shown potential in downstream applications like Question Answering <ref type="bibr" target="#b40">(Yin et al., 2016)</ref>, Information Extraction <ref type="bibr" target="#b21">(Ji and Grishman, 2011;</ref><ref type="bibr" target="#b17">Guo et al., 2013)</ref>, Text Generation <ref type="bibr" target="#b30">(Puduppully et al., 2019)</ref> and Semantic Parsing <ref type="bibr" target="#b3">(Bevilacqua et al., 2021;</ref><ref type="bibr" target="#b29">Procopio et al., 2021)</ref>.</p><p>Since the advent of Deep Learning within the NLP community, this task has mostly been framed as a multi-label classification problem <ref type="bibr" target="#b32">(Shahbazi et al., 2019;</ref><ref type="bibr" target="#b7">Broscheit, 2019)</ref>, especially leveraging the bi-encoder paradigm <ref type="bibr" target="#b20">(Humeau et al., 2020;</ref><ref type="bibr" target="#b36">Wu et al., 2020)</ref>. However, although simple and yet powerful enough to push scores past 90% inKB Micro F 1 on standard benchmarks, this formulation suffers from a number of downsides. First, the actual disambiguation is only modeled through a dot product between independent mention and entity vectors, which may not capture complex mention-entity interactions. Second, from a computational perspective, entities are represented through high-dimensional vectors that are cached in a pre-computed index. Thus, classifying against a large KB has a significant memory cost that, in fact, scales linearly with respect to the number of entities. Besides this, adding a new entity also requires modifying the index itself. To address these issues, De <ref type="bibr">Cao et al. (2021b)</ref> have recently proposed an auto-regressive formulation where, given mentions in their context, models are trained to generate, token-by-token, the correct entity identifiers. <ref type="foot" target="#foot_0">1</ref>While this approach has addressed the aforementioned issues effectively, it requires an autoregressive decoding process, which has speed implications, and, what is more, does not let the model see its possible output choices, something that has shown significant potential in other semantic tasks <ref type="bibr">(Barba et al., 2021a)</ref>. In this work, we focus on these shortcomings and, inspired by this latter research trend, propose Extractive Entity Disambiguation (EXTEND), the first entity disambiguator that frames ED as a text extraction task. Given as input a context c m in which a mention m occurs, along with a text representation for each of the possible candidates e 1 , . . . , e n , a model has to extract the span associated with the text representation of the entity that best suits m. We implement this formulation through 2 architectures: i) a Transformer system <ref type="bibr" target="#b34">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b11">Devlin et al., 2019)</ref> that features an almost identical modeling power to that of previous works, and ii) a variant that relaxes the computational requirements of our approach when using common Transformer-based architectures. Evaluating our two systems over standard benchmarks, we find our formulation to be particularly suited to ED. In particular, when restricting training resources to the AIDA-CoNLL dataset <ref type="bibr" target="#b19">(Hoffart et al., 2011)</ref> only, EXTEND appears to be significantly more data-efficient than its alternatives, surpassing them by more than 6 inKB Micro F 1 points on average across in-domain and out-of-domain datasets. Furthermore, when pre-training on external ED data as in De Cao et al. (2021b), our system sets a new state of the art on 4 out of 6 benchmarks under consideration, with average improvements of 0.7 overall and 1.1 when moving out of domain. Finally, we also perform a thorough investigation of our system performances, providing insights and pinpointing the reasons behind our improvements via a fine-grained evaluation on different label-frequency classes.</p><p>Our contributions are therefore as follows:</p><p>? We propose a new framing of ED as a text extraction task;</p><p>? We put forward two architectures that implement our formulation, whose average score across different benchmarks surpasses all previous works in both data regimes we consider;</p><p>? We perform a thorough analysis of our systems' performances, evaluating their behavior over different label-frequency classes.</p><p>We release our code and models for research purposes at https://github.com/ SapienzaNLP/extend.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Entity Disambiguation (ED) is the task of identifying, given a mention in context, the most suitable entity among a set of candidates stored in a knowledge base (KB). Generally the last step in an Entity Linking system <ref type="bibr" target="#b7">(Broscheit, 2019)</ref>, coming immediately after mention detection and candidate generation, this task has been the object of a vast and diverse literature, with approaches typically clustered into two groups, depending on how they model co-occurring mentions in the same document. On the one hand, global models strive to enforce a global coherence across the disambiguations within the same document, leveraging different techniques and heuristics to approximate this objective<ref type="foot" target="#foot_1">2</ref>  <ref type="bibr" target="#b19">(Hoffart et al., 2011;</ref><ref type="bibr" target="#b27">Moro et al., 2014;</ref><ref type="bibr" target="#b37">Yamada et al., 2016;</ref><ref type="bibr" target="#b15">Ganea and Hofmann, 2017;</ref><ref type="bibr" target="#b22">Le and Titov, 2018;</ref><ref type="bibr" target="#b39">Yang et al., 2018)</ref>.</p><p>On the other hand, local models disambiguate each mention independently of the others, conditioning the entity choice only on the mention and its context. Thanks to the advent of large pretrained language models, this group has recently witnessed a significant improvement in performances, which are nowadays on par with, or even above, those achieved by state-of-the-art global systems <ref type="bibr" target="#b32">(Shahbazi et al., 2019)</ref>. These approaches usually frame ED as a multi-label classification problem <ref type="bibr" target="#b7">(Broscheit, 2019)</ref> and a diverse set of formulations have been proposed. Among these, the bi-encoder paradigm <ref type="bibr" target="#b6">(Bromley et al., 1994;</ref><ref type="bibr" target="#b20">Humeau et al., 2020)</ref> has been particularly successful <ref type="bibr" target="#b16">(Gillick et al., 2019;</ref><ref type="bibr" target="#b33">Tedeschi et al., 2021;</ref><ref type="bibr" target="#b5">Botha et al., 2020)</ref>: here, two encoders are trained to learn vector representations in a shared space for mentions in context and entities, respectively. Classification of a given mention is then performed by retrieving the entity whose representation is closest according to some metric (e.g. cosine similarity).</p><p>Although remarkably powerful, these formulations present a number of disadvantages, such as their large memory footprint (each entity in the KB needs to be represented by a high-dimensional vector) and the fact that the actual disambiguation process is only expressed via a dot product of independently computed vectors, potentially neglecting mention-entity interactions. While a number of works <ref type="bibr" target="#b26">(Logeswaran et al., 2019;</ref><ref type="bibr" target="#b36">Wu et al., 2020)</ref> attempt to address the latter issue via multi-stage approaches where a cross-encoder is stacked after an initial bi-encoder<ref type="foot" target="#foot_2">3</ref> or other retrieval functions, an interesting alternative direction that tackles both problems was recently presented by De Cao et al. (2021b): the authors frame ED as a generation problem and, leveraging an auto-regressive formulation, train a sequence-to-sequence model to generate the correct entity identifier for a given mention and its context.</p><p>Nevertheless, while this approach can model more complex interactions, some of these can only occur indirectly inside the backtracking of their beam search. Furthermore, the disambiguation involves an auto-regressive decoding that, although mitigated by later efforts <ref type="bibr">(De Cao et al., 2021a)</ref>, has intrinsic speed limitations. In contrast, here we propose an extractive formulation, where a model receives as input the mention, its context and the text representation of each candidate, and has to extract the span corresponding to the representation of the entity that best matches the (mention, context) pair under consideration. Note that this differs from the aforementioned cross-encoder formulations <ref type="bibr" target="#b26">(Logeswaran et al., 2019;</ref><ref type="bibr" target="#b36">Wu et al., 2020)</ref> where, instead, each entity was encoded together with the (mention, context) pair, but independently from all the other entities. With our schema, complex mention-entity and entity-entity interactions can be explicitly modeled by the neural system, as all the information is provided in input.</p><p>Glancing over other related tasks in the area of semantics, arguably closest to our work is ESC <ref type="bibr">(Barba et al., 2021a)</ref>, where the authors propose a new framing of Word Sense Disambiguation (WSD) as an extractive sense comprehension task. Yet, differently from their work, we propose here a new framing for ED, i.e. focus on entity descriptions rather than word sense definitions, present a baseline system that implements it and devise an additional architecture that deals with the computational challenges that arise from such implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>We now introduce EXTEND, our proposed approach for ED. We first present the formulation we adopt (Section 3.1) and, then, describe the two architectures that implement it (Section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Formulation</head><p>Inspired by recent trends in other semantic tasks <ref type="bibr">(Barba et al., 2021a)</ref>, we formulate Entity Disambiguation as a text extraction problem: given a query x q and a context x c , a model has to learn to extract the text span of x c that best answers x q . Formally, let m be a mention occurring in a context c m and denote by Cnd(m) = {cnd 1 , . . . , cnd n } the set of n text representations associated with each candidate of m. Then, we formulate ED as follows: we treat the tuple (m, c m ) and the concatenation of cnd 1 , . . . , cnd n as the query x q and the context x c , respectively, and train a model to extract the text span from x c associated with the correct cnd * ? Cnd(m); the overall process is illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. This formulation helps to better model the input provided, with the possible candidates of m included in the contextualization process, while also disposing of large output vocabularies as in De <ref type="bibr">Cao et al. (2021b)</ref> and, yet, not resorting to auto-regressive decoding strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Architectures</head><p>To implement our formulation, we consider two Transformer-based architectures. For both of these, the input is composed of the concatenation of the query x q and the context x c , subword-tokenized and separated by a [SEP] special symbol. Since x q is a tuple in our formulation, whereas Transformer models only support text sequences as input, we convert x q into a string xq by taking only c m and surrounding the text span where m occurs with the special tokens &lt;t&gt; and &lt;/t&gt;. Additionally, to better separate entity candidate representations and ease their full span identification, we add a trailing special symbol &lt;/ec&gt; to each of them; henceforth, we denote this resulting modified context by xc .</p><p>As our first architecture, we use two independent classification heads on top of BART <ref type="bibr" target="#b24">(Lewis et al., 2020)</ref> computing, respectively, for each word w in xc , whether w is the start or end of the correct entity representation cnd * . We train the model with a cross-entropy criterion over the start and end of cnd * . At inference time, we select the entity candidate representation cnd ? ? Cnd(m) whose joint probability over the 2 heads is highest. However, framing ED as we propose here implies that the length of the input to the model scales linearly with the number of output choices m. Taking into account that the attention mechanism of Transformer architectures has quadratic complexity and that several pre-trained models actually support inputs only up to a fixed maximum length,<ref type="foot" target="#foot_3">4</ref> this might pose significant computational limitations depending on the dataset and knowledge base under consideration. To cope with these technical challenges, we consider a second system, similar to the previous one but for two main differences. First, we change the underlying Transformer model, replacing BART with a pre-trained Longformer model <ref type="bibr" target="#b2">(Beltagy et al., 2020)</ref>, a Transformer architecture with an attention mechanism that is linear with respect to the input length and that can handle longer sequences. This linear complexity is achieved by essentially applying a sliding attention window over each token but for a few pre-selected ones (e.g.</p><p>[CLS]), which instead feature a symmetric global attention: they attend upon and are attended by all the other tokens in the input sequence. This global mechanism is intended to be task-specific and enables the model to learn representations potentially close to those standard fully-attentive Transformers would learn, while still maintaining the overall attention complexity linear with respect to the input size. Therefore, as our second modification, we adapt this global pattern to our setting, activating it on the [CLS] special token and on the first token of each cnd i ? Cnd(m); this allows to better mimic the original quadratic mechanism where different entity candidate representations can also attend upon each other. Furthermore, differently from <ref type="bibr" target="#b2">Beltagy et al. (2020)</ref>, we disable the global attention mechanism on the tokens in the query xq . In Section 5, we report and discuss the impact of these modifications. We illustrate the proposed architecture in Figure <ref type="figure" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Entity Disambiguation Evaluation</head><p>We now assess the effectiveness of EXTEND on Entity Disambiguation. We first introduce the experimental setup we consider (Section 4.1). Then, we present the results achieved by EXTEND both in terms of raw performances (Section 4.2) and via a breakdown of its behavior on different classes of label frequency (Section 4.3). For ease of readability, we focus here only on the Longformer-based architecture, which we consider as our main model. We defer the comparison with the BART-based system to Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Data To evaluate EXTEND on Entity Disambiguation, we reproduce the same setting used by De <ref type="bibr">Cao et al. (2021b)</ref>. Specifically, we adopt their same candidate sets, which were originally proposed by <ref type="bibr" target="#b22">Le and Titov (2018)</ref>,<ref type="foot" target="#foot_4">5</ref> use Wikipedia titles (e.g. Metropolis (comics)) as the text representation for entities and perform training, along with in-domain evaluation, on the AIDA-CoNLL dataset <ref type="bibr">(Hoffart et al., 2011, AIDA)</ref>; similarly, we use their cleaned version of MSNBC, AQUAINT, ACE2004, WNED-CWEB (CWEB) and WNED-WIKI (WIKI) <ref type="bibr" target="#b18">(Guo and Barbosa, 2018;</ref><ref type="bibr" target="#b13">Evgeniy et al., 2013)</ref> for out-of-domain evaluation. iv) ACE2004: a manually annotated subset of the ACE co-reference data set <ref type="bibr" target="#b12">(Doddington et al., 2004)</ref> Evaluation Following common practice in ED literature, results over the evaluation datasets are expressed in terms of inKB Micro F 1 . Furthermore, to better highlight the performance on the out-ofdomain datasets, we report both the average score over those and AIDA (Avg) and over those alone (Avg OOD ), that is, when the result on AIDA is excluded from the average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison Systems</head><p>In order to contextualize EXTEND performances within the current landscape of Entity Disambiguation, we evaluate our approach against recent state-of-the-art systems in the literature. Specifically, we consider:</p><p>? Global Models: <ref type="bibr" target="#b15">Ganea and Hofmann (2017)</ref>; <ref type="bibr" target="#b18">Guo and Barbosa (2018)</ref>; <ref type="bibr" target="#b39">Yang et al. (2018</ref><ref type="bibr" target="#b38">Yang et al. ( , 2019))</ref>; <ref type="bibr" target="#b23">Le and Titov (2019)</ref> EXTEND Setup As previously mentioned, we use the Longformer model <ref type="bibr" target="#b2">(Beltagy et al., 2020)</ref> as our reference architecture and retrieve the pre-trained weights, for both its base and large variants, from the HuggingFace Transformers library <ref type="bibr" target="#b35">(Wolf et al., 2020)</ref>; we refer to these variants as EXTEND Base (139M parameters) and EXTEND Large (435M parameters). Following GENRE standard practice, we use the last encoder output for the representation of each token and a simple linear layer on top of it to compute the start and end tokens probability distributions. We use a 64token attention window and fine-tune the whole architecture using the Rectified Adam <ref type="bibr" target="#b25">(Liu et al., 2020)</ref> optimizer with 10 -5 learning rate for at most 100,000 steps. We use 8 steps of gradient accumulation and batches made of a maximum of 1024 tokens. We evaluate the model on the validation dataset every 2000 steps, enforcing a patience of 15 evaluation rounds. We train every model for a single run on a GeForce RTX 3090 graphic card with 24 gigabytes of VRAM. Due to computational constraints, we do not perform any hyperparameter tuning, except for the attention window where we try <ref type="bibr">[32,</ref><ref type="bibr">64,</ref><ref type="bibr">128]</ref>, and select the other hyperparameters following previous literature. We implement our work in PyTorch <ref type="bibr" target="#b28">(Paszke et al., 2019)</ref>, using classy<ref type="foot" target="#foot_8">9</ref> as the underlying framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>We report in Table <ref type="table" target="#tab_0">1</ref> (top) the inKB Micro F 1 score EXTEND and its comparison systems attain on the evaluation datasets in the Wikipedia+AIDA setting.</p><p>Arguably the most interesting finding we report is the improvement EXTEND achieves over its comparison systems. EXTEND Large + BLINK, that is, EXTEND Large pre-trained on BLINK 10 and then fine-tuned on AIDA, sets a new state of the art on 4 out of 6 datasets, with the only exceptions being in-domain AIDA and CWEB, where we fall short compared to the global model of <ref type="bibr" target="#b39">Yang et al. (2018)</ref>. On the Avg score, EXTEND pushes performances up by 0.7 points, and this improvement becomes even more marked when considering Avg OOD (+1.1). These results suggest that our approach is indeed well-suited for ED and, furthermore, is particularly effective when scaling out of domain.</p><p>Additionally, we also evaluate EXTEND on the AIDA-only training setting and compare against De Cao et al. (2021b) and <ref type="bibr" target="#b33">Tedeschi et al. (2021)</ref>, the only systems available in this setting. As shown in Table <ref type="table" target="#tab_0">1</ref> (bottom), EXTEND behaves better, with both EXTEND Base and EXTEND Large achieving higher Avg scores. In particular, EXTEND Base , which features only 149M parameters, fares better (by almost 5 points) than De <ref type="bibr">Cao et al. (2021b)</ref>, whose model parameters amount to 406M (2.7?). Moreover, the Avg OOD results, which are also higher, further confirm our previous hypothesis as regards the benefits of our approach in out-ofdomain scalability. Paired together, these results highlight the higher data efficiency that our formulation achieves, in comparison to its alternatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Fine-grained Results</head><p>Inspired by standard practices in the evaluation of Word Sense Disambiguation systems <ref type="bibr" target="#b4">(Blevins and Zettlemoyer, 2020;</ref><ref type="bibr">Barba et al., 2021a)</ref>, we perform a fine-grained analysis where we break down the performances of our model into different classes of label frequency. To this end, we partition both the AIDA test set and the concatenation of all the out-of-domain datasets in three different subsets: i) MFC, containing all the instances in the test set where the target mention is associated with its most frequent candidate in the training corpus (i.e. the AIDA training split).; ii) LFC, containing all the instances in the test set annotated with a least frequent candidate of the target mention that appeared at least once in the training corpus; iii) Unseen, containing all the instances in the test set whose mention was never seen in the training corpus.</p><p>We then evaluate all the systems of the AIDA setting, except for De <ref type="bibr">Cao et al. (2021b)</ref>  the original model is unavailable, on these six test sets. To put the results in perspective, we introduce a simple baseline (PEM-MFC) that consists in always predicting the most frequent candidate for each mention, taking mention-candidate frequencies from <ref type="bibr" target="#b22">Le and Titov (2018)</ref>.</p><p>As we can see from Table <ref type="table" target="#tab_1">2</ref>, PEM-MFC is a rather strong baseline, confirming the skewness of the distribution with which each mention is annotated with one of its possible candidates towards the most frequent ones. Indeed, the gap between the performances of all the models on the MFC split and the LFC split is rather large, with a difference of almost 50 points in the outof-domain setting. While future works should investigate the performances on these splits more in depth, here we can see that EXTEND Base and especially EXTEND Large outperform their competitors in the LFC and Unseen splits, in both the in-domain and out-of-domain settings. This highlights the strong generalization capabilities of our proposed approach, which is able to better handle rare or unseen instances at the cost of only 1 point in F 1 score on the MFC of the in-domain setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Model Ablation</head><p>While the above-mentioned experiments showed our approach to be rather effective, we only focused on the Longformer-based architecture, to which we resorted owing to the computational challenges we mentioned in Section 3.2. We now investigate this model choice, evaluating first how the BARTbased system fares. Then, we ablate the attention pattern we propose for the Longformer and, finally, discuss the trade-off between our two proposed architectures.</p><p>BART Strictly speaking, the results we reported in the previous Section are not exactly conclusive as to whether or not our formulation is beneficial. Indeed, while it is true that we use a new formulation, we also rely upon a Transformer model that none of our comparison systems considered. Therefore, to better pinpoint the origin of the improvements, we train our BART-based architecture in the AIDA setting; we refer to this model as BART. Note that the underlying Transformer is identical to that of De <ref type="bibr">Cao et al. (2021b)</ref>, except for the final classification heads.<ref type="foot" target="#foot_10">11</ref> As shown in Table <ref type="table" target="#tab_2">3</ref>, BART with our extractive formulation attains significantly better performances. This finding suggests that the overall improvement does indeed originate from our extractive formulation. Furthermore, as the two systems are entirely identical except for the framing adopted, this finding further underlines the data efficiency of our approach.</p><p>Longformer Ablations We now compare our chosen global attention strategy with two standard alternatives. First, we consider the schema originally proposed by <ref type="bibr" target="#b2">Beltagy et al. (2020)</ref> for questionanswering tasks, where all the tokens in the input query (i.e. the text containing the mention) have a global attention (Longformer query ). Then, we compare against an EXTEND variant where the only token with global attention enabled is the start of sequence token (i.e. [CLS]). Table <ref type="table" target="#tab_2">3</ref> shows how the three systems behave, reporting both their indomain and out-of-domain scores, along with the average percentage of tokens in the input sequence with global attention enabled (GA%). From these results, we can see that i) our approach fares the best and that ii) Longformer CLS achieves performances almost in the same ballpark, making it a viable option for more computationally limited scenarios.</p><p>BART and Longformer Finally, we compare our two architectures. As we can see from Table <ref type="table" target="#tab_2">3</ref>, BART performs better in the in-domain dataset, whereas the Longformer outperforms it in the outof-domain setting. Nevertheless, neither of these differences is very significant and, thus, this result confirms our initial hypothesis that using our second architecture is a valid approximation of the standard quadratic attention strategy for the extractive Entity Disambiguation task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Error Analysis</head><p>To further investigate the generalization capabilities of EXTEND, we performed a black-box testing (Ribeiro et al., 2020) of our system leveraging the available test sets. Apart from the problem of label frequencies (e.g. unseen entities), we discovered two additional main classes of errors, namely i) insufficient context, and ii) titles alone might not be enough.</p><p>Insufficient Context Since the average number of candidates for each mention is roughly 50, the probability of having multiple valid candidates given the input context is far from negligible. For instance, let us consider the following example: "In the last game Ronaldo scored two goals despite coming from a bad injury.". In this sentence, the mention Ronaldo can refer both to Cristiano Ronaldo, the Portuguese player, and to Ronaldo de Lima, the Brazilian player. While this particular problem holds for several instances in the test sets, the performance drop is, in fact, mitigated by the labels skewness towards the most frequent candidates. Indeed, the model appears always to predict the most frequent candidate for this kind of instance, therefore being right in the majority of cases.</p><p>Titles might not be enough For both comparability and performance purposes, the text representation we use for a given entity in this work is simply its Wikipedia title. While article titles in Wikipedia are rather informative, in several circumstances they do not contain enough information to make them sufficiently distinguishable from other candidates. For example, several pages describing "Persons" are entitled just with their respective names and surnames. This kind of identifier is especially ineffective if the mentions taken into consideration were not present in the training dataset, or were rare or unseen during the underlying Transformer pre-training. To this end, we strongly believe that future research might benefit from focusing on enriching entities' identifiers by adding a small description of the articles (summary) or at least some keyword representing the domain the entity belongs to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this work we presented EXTEND, a novel local formulation for ED that frames this task as a text extraction problem: given as input a string containing a marked mention in context and the text representation of each entity in its candidate set, a model has to extract the span corresponding to the text representation of the correct entity. Together with this formulation, we also presented two Transformer models that implement it and, by evaluating them across several experiments, we found our approach to be particularly suited to ED. First, it is extremely data efficient, surpassing its alternatives by more than 6 F 1 points when considering an AIDA-only training setting. Second, pre-training on BLINK data enables the model to set a new state of the art on 4 out of 6 benchmarks under consideration and yield average improvements of 0.7 F 1 points overall and 1.1 F 1 points when focusing only on out-of-domain evaluation datasets.</p><p>As future work, we plan to relax the requirements towards the candidate set and explore adapting this local formulation to a global one, so as to enforce coherence across predictions. For instance, we believe integrating the feedback loop strategy we proposed in <ref type="bibr">Barba et al. (2021b)</ref> would be an interesting direction to pursue.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of EXTEND on the example sentence After a long fight Superman saved Metropolis. The model takes as input a sentence with the target mention to disambiguate, Metropolis, explicitly marked (for better visualization, we resort here to highlighting with a different color rather than surrounding it with special tokens) along with the text representation of each candidate. As in our experiments, the knowledge base here is Wikipedia and the candidate text representations are Wikipedia page titles. Then, the model performs the disambiguation by indicating the start and end token of the span containing the predicted entity representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Longformer-based architecture for EXTEND. The input context and the candidate textual representations are fed to the model in the same sequence separated by a [SEP] special token. The mention is surrounded by two special tokens &lt;t&gt; and &lt;/t&gt; and, for the sake of readability, we omit the trailing special tokens &lt;/ec&gt;. We highlight in red the tokens with global attention. Best seen in colors.</figDesc><graphic url="image-2.png" coords="5,93.54,70.84,408.20,265.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>; Fang et al. (2019); ? Local Models: Shahbazi et al. (2019) and Tedeschi et al. (2021); ? The auto-regressive approach proposed by De Cao et al. (2021b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>. It contains 257 annotated instances. Results (inKB Micro F 1 ) on the in-domain and out-of-domain settings when training on the AIDA training split only (bottom) and when using additional resources coming from Wikipedia (top). We mark in bold the best scores and underline the second best.</figDesc><table><row><cell>v) CWEB: a dataset automatically extracted</cell></row><row><cell>from the ClueWeb corpus 6 by Guo and Bar-</cell></row><row><cell>bosa (2018) containing English Websites, con-</cell></row><row><cell>sisting of 11,154 annotated instances.</cell></row><row><cell>vi) WIKI: an automatically extracted corpus</cell></row><row><cell>comprised of Wikipedia pages released by</cell></row><row><cell>Evgeniy et al. (2013), with 6821 annotated</cell></row><row><cell>instances.</cell></row><row><cell>vii) BLINK: a dataset made up of 9 million (doc-</cell></row><row><cell>ument, entity, mention) triples automatically</cell></row><row><cell>extracted from Wikipedia.</cell></row><row><cell>For each of these resources, 7 we use the prepro-</cell></row><row><cell>cessed datasets, along with the mention candidate</cell></row><row><cell>sets, made available by De Cao et al. (2021b) in</cell></row><row><cell>the authors' official repository. 8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>for whichof De Cao et al. (2021b)  and our pre-training performed a significantly smaller number of updates. The scores reported here are therefore likely to be higher. Results (inKB Micro F 1 ) when training on the AIDA training split only, on the MFC, LFC and UNS (Unseen) partitions for both in-domain and out-ofdomain settings. We mark in Bold the best scores.</figDesc><table><row><cell></cell><cell>In-domain</cell><cell>Out-of-domain</cell></row><row><cell>Model</cell><cell cols="2">MFC LFC UNS MFC LFC UNS</cell></row><row><cell>PEM-MFC</cell><cell cols="2">79.2 12.6 74.0 82.2 37.1 66.1</cell></row><row><cell cols="3">Tedeschi et al. (2021) 95.8 60.9 89.0 91.1 43.0 61.7</cell></row><row><cell>EXTEND Base</cell><cell cols="2">94.2 53.2 87.1 94.0 43.9 75.0</cell></row><row><cell>EXTEND Large</cell><cell cols="2">94.8 62.4 89.1 94.3 48.1 77.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results (inKB Micro F 1 ) of the ablation study for the in-domain and out-of-domain settings along with the percentage of global tokens (GA%). We mark in Bold the best scores.</figDesc><table><row><cell>Model</cell><cell cols="3">In-domain Out-of-domain GA%</cell></row><row><cell>De Cao et al. (2021b)</cell><cell>88.6</cell><cell>78.2</cell><cell>100.0</cell></row><row><cell>EXTEND</cell><cell>90.0</cell><cell>84.9</cell><cell>21.1</cell></row><row><cell>Longformer query</cell><cell>89.2</cell><cell>84.1</cell><cell>43.3</cell></row><row><cell>Longformer CLS</cell><cell>88.8</cell><cell>84.3</cell><cell>0.8</cell></row><row><cell>BART</cell><cell>90.4</cell><cell>84.5</cell><cell>100.0</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>i.e. a textual description of the entity; in De Cao et al. (2021b), they use the titles of Wikipedia articles, since their reference KB is Wikipedia.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Approximation is necessary as the exact computation of coherence objectives is NP-hard<ref type="bibr" target="#b22">(Le and Titov, 2018)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>This bi-encoder, rather than performing the actual classification, is tasked to generate a filtered set of candidates.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>For instance, the implementation of BART available in HuggingFace Transformers<ref type="bibr" target="#b35">(Wolf et al., 2020)</ref> supports inputs only up to 1024 subwords.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>These candidate sets were generated relying upon count statistics from Wikipedia, a large Web corpus and the YAGO dictionary.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>https://lemurproject.org/clueweb12</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>Which are all freely available for research purposes.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>https://github.com/facebookresearch/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8"><p>https://github.com/sunglasses-ai/ classy</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9"><p>We note that, due to computational and hardware constraints, we were unable to match the training configuration</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_10"><p>The model of De Cao et al. (2021b)  has a single head on the whole output vocabulary, whereas we have two (start and end).</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The authors gratefully acknowledge the support of the <rs type="grantName">ERC Consolidator Grant</rs> MOUSSE No. <rs type="grantNumber">726487</rs>. This work was partially supported by the <rs type="funder">MIUR</rs> under the grant "<rs type="projectName">Dipartimenti di eccellenza 2018-2022</rs>" of the <rs type="institution">Department of Computer Science of Sapienza University</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_sPG2xHg">
					<idno type="grant-number">726487</idno>
					<orgName type="grant-name">ERC Consolidator Grant</orgName>
					<orgName type="project" subtype="full">Dipartimenti di eccellenza 2018-2022</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">ESC: Redesigning WSD with extractive sense comprehension</title>
		<author>
			<persName><forename type="first">Edoardo</forename><surname>Barba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Pasini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.371</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4661" to="4672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">2021b. ConSeC: Word sense disambiguation as continuous sense comprehension</title>
		<author>
			<persName><forename type="first">Edoardo</forename><surname>Barba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luigi</forename><surname>Procopio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="1492" to="1503" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">One SPRING to rule them both: Symmetric AMR semantic parsing and generation without a complex pipeline</title>
		<author>
			<persName><forename type="first">Michele</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rexhina</forename><surname>Blloshmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Moving down the long tail of word sense disambiguation with gloss informed bi-encoders</title>
		<author>
			<persName><forename type="first">Terra</forename><surname>Blevins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.95</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1006" to="1017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Entity Linking in 100 Languages</title>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">A</forename><surname>Botha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zifei</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gillick</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.630</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7833" to="7845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Signature verification using a &quot;siamese&quot; time delay neural network</title>
		<author>
			<persName><forename type="first">Jane</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>S?ckinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roopak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Morgan-Kaufmann</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Investigating entity knowledge in BERT with simple neural end-to-end entity linking</title>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Broscheit</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K19-1063</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>the 23rd Conference on Computational Natural Language Learning (CoNLL)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="677" to="685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Using encyclopedic knowledge for named entity disambiguation</title>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marius</forename><surname>Pa?ca</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="9" to="16" />
			<pubPlace>Trento, Italy</pubPlace>
		</imprint>
	</monogr>
	<note>In 11th Conference of the European Chapter</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">2021a. Highly parallel autoregressive entity linking with discriminative correction</title>
		<author>
			<persName><forename type="first">Nicola</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilker</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="7662" to="7669" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">2021b. Autoregressive entity retrieval</title>
		<author>
			<persName><forename type="first">Nicola</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<publisher>OpenReview</publisher>
			<date type="published" when="2021">May 3-7, 2021</date>
		</imprint>
	</monogr>
	<note>net</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The automatic content extraction (ACE) program -tasks, data, and evaluation</title>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">R</forename><surname>Doddington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Przybocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lance</forename><forename type="middle">A</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><forename type="middle">M</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><forename type="middle">M</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Conference on Language Resources and Evaluation</title>
		<meeting>the Fourth International Conference on Language Resources and Evaluation<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association</publisher>
			<date type="published" when="2004-05-26">2004. 2004. May 26-28, 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">FACC1: Freebase annotation of clueweb corpora, version 1 (release date 2013-06-26</title>
		<author>
			<persName><forename type="first">Gabrilovich</forename><surname>Evgeniy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ringgaard</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subramanya</forename><surname>Amarnag</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>format version 1, correction level 0</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Joint entity linking with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanbing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3308558.3313517</idno>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-05-13">2019. 2019. May 13-17, 2019</date>
			<biblScope unit="page" from="438" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep joint entity disambiguation with local neural attention</title>
		<author>
			<persName><forename type="first">Octavian-Eugen</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1277</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2619" to="2629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning dense representations for entity retrieval</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sayali</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Lansing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Presta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Ie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Garcia-Olano</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K19-1049</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>the 23rd Conference on Computational Natural Language Learning (CoNLL)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="528" to="537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">To link or not to link? a study on end-toend tweet entity linking</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emre</forename><surname>Kiciman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1020" to="1030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Robust named entity disambiguation with random walks</title>
		<author>
			<persName><forename type="first">Zhaochen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denilson</forename><surname>Barbosa</surname></persName>
		</author>
		<idno type="DOI">10.3233/SW-170273</idno>
	</analytic>
	<monogr>
		<title level="j">Semantic Web</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="459" to="479" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Robust disambiguation of named entities in text</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><forename type="middle">Amir</forename><surname>Yosef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilaria</forename><surname>Bordino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hagen</forename><surname>F?rstenau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Spaniol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilyana</forename><surname>Taneva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Edinburgh, Scotland</addrLine></address></meeting>
		<imprint>
			<publisher>UK. Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="782" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Poly-encoders: Architectures and pre-training strategies for fast and accurate multi-sentence scoring</title>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Humeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR 2020</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26">2020. April 26-30, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Knowledge base population: Successful approaches and challenges</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1148" to="1158" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improving entity linking by modeling latent relations between mentions</title>
		<author>
			<persName><forename type="first">Phong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1148</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1595" to="1604" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Boosting entity linking performance by leveraging unlabeled documents</title>
		<author>
			<persName><forename type="first">Phong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1187</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1935" to="1945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On the variance of the adaptive learning rate and beyond</title>
		<author>
			<persName><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26">2020. April 26-30, 2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Zero-shot entity linking by reading entity descriptions</title>
		<author>
			<persName><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1335</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3449" to="3460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Entity linking meets word sense disambiguation: a unified approach</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Moro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Raganato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00179</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="231" to="244" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alch?-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">SGL: Speaking the graph languages of semantic parsing via multilingual translation</title>
		<author>
			<persName><forename type="first">Luigi</forename><surname>Procopio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rocco</forename><surname>Tripodi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.30</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="325" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Data-to-text generation with entity modeling</title>
		<author>
			<persName><forename type="first">Ratish</forename><surname>Puduppully</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1195</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2023" to="2035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Beyond accuracy: Behavioral testing of NLP models with CheckList</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongshuang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.442</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4902" to="4912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Entity-aware elmo: Learning contextual entity representation for entity disambiguation</title>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Shahbazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoli</forename><forename type="middle">Z</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Ghaeini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rasha</forename><surname>Obeidat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasad</forename><surname>Tadepalli</surname></persName>
		</author>
		<idno>CoRR, abs/1908.05762</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Named Entity Recognition for Entity Linking: What works and what&apos;s next</title>
		<author>
			<persName><forename type="first">Simone</forename><surname>Tedeschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Conia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Cecconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<meeting><address><addrLine>Punta Cana</addrLine></address></meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2584" to="2596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">? Ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scalable zeroshot entity linking with dense entity retrieval</title>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Josifoski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.519</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6397" to="6407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Joint learning of the embedding of words and entities for named entity disambiguation</title>
		<author>
			<persName><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshiyasu</forename><surname>Takefuji</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K16-1025</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="250" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning dynamic context augmentation for global entity linking</title>
		<author>
			<persName><forename type="first">Xiyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhigang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1026</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="271" to="281" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Collective entity disambiguation with structured gradient tree boosting</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazi</forename><surname>Shefaet Rahman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1071</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="777" to="786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Simple question answering by attentive convolutional neural network</title>
		<author>
			<persName><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1746" to="1756" />
		</imprint>
	</monogr>
	<note>The COL-ING 2016 Organizing Committee</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
