<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automatic learning of cost functions for graph edit distance</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Michel</forename><surname>Neuhaus</surname></persName>
							<email>mneuhaus@iam.unibe.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Applied Mathematics</orgName>
								<orgName type="institution">University of Bern</orgName>
								<address>
									<addrLine>Neubru ¨ckstrasse 10</addrLine>
									<postCode>CH-3012</postCode>
									<settlement>Bern</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Horst</forename><surname>Bunke</surname></persName>
							<email>bunke@iam.unibe.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Applied Mathematics</orgName>
								<orgName type="institution">University of Bern</orgName>
								<address>
									<addrLine>Neubru ¨ckstrasse 10</addrLine>
									<postCode>CH-3012</postCode>
									<settlement>Bern</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Automatic learning of cost functions for graph edit distance</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">80760959800107CA651B314A46FE7A73</idno>
					<idno type="DOI">10.1016/j.ins.2006.02.013</idno>
					<note type="submission">Received 10 February 2006; accepted 20 February 2006</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graph matching</term>
					<term>Graph edit distance</term>
					<term>Edit cost function</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph matching and graph edit distance have become important tools in structural pattern recognition. The graph edit distance concept allows us to measure the structural similarity of attributed graphs in an error-tolerant way. The key idea is to model graph variations by structural distortion operations. As one of its main constraints, however, the edit distance requires the adequate definition of edit cost functions, which eventually determine which graphs are considered similar. In the past, these cost functions were usually defined in a manual fashion, which is highly prone to errors. The present paper proposes a method to automatically learn cost functions from a labeled sample set of graphs. To this end, we formulate the graph edit process in a stochastic context and perform a maximum likelihood parameter estimation of the distribution of edit operations. The underlying distortion model is learned using an Expectation Maximization algorithm. From this model we finally derive the desired cost functions. In a series of experiments we demonstrate the learning effect of the proposed method and provide a performance comparison to other models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, graphs have increasingly been used for structural pattern representation <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>. By converting patterns into graphs, we turn the pattern classification problem into the problem of evaluating the structural similarity of graphs, which is commonly referred to as graph matching. A large variety of methods have been proposed for graph matching, ranging from exact matching techniques, such as maximum common subgraph based methods, to error-tolerant approaches based on continuous optimization theory and spectral decomposition methods <ref type="bibr" target="#b6">[7]</ref>. One of the most intuitive ways to approach the graph matching problem is via the definition of a graph similarity measure <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b23">24]</ref>. Among several possible alternatives, graph edit distance <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b15">16]</ref> has been recognized a general and flexible method to measure the dissimilarity (or similarity) of attributed graphs by taking structural errors into account. However, the structural dissimilarity of graphs is only correctly reflected by graph edit distance if the underlying edit costs are defined appropriately. For the problem of string matching, Ristad and Yianilos <ref type="bibr" target="#b19">[20]</ref> propose a model to obtain edit costs from an estimation of the frequency of edit operations. Related to this approach, we introduce in this paper a probabilistic model of the distribution of graph edit operations that allows us to derive edit costs that are optimal with respect to certain criteria. Unlike other graph matching methods based on machine learning techniques <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14]</ref>, we propose a training of the graph matching system according to the edit operation model of structural similarity.</p><p>This paper is structured as follows. In Section 2 we briefly introduce graph edit distance. Section 3 describes the algorithm for learning edit costs. An experimental evaluation of the proposed model is presented in Section 4, and conclusions are drawn in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Graph edit distance</head><p>The adequate definition of object similarity is a key problem in pattern recognition. In the case of graph matching, objects are represented by graph structures consisting of nodes, edges connecting pairs of nodes, and attribute labels attached to nodes and edges. While in general the similarity of graphs can be measured in various ways <ref type="bibr" target="#b6">[7]</ref>, some graph matching methods seem to be too restricted to cope with noisy real-world data. Exact graph isomorphism based methods, for instance, provide for a very precise theoretical foundation of graph matching, yet they often fail to successfully model structural variations. Other methods are limited to special classes of graphs that occur rather infrequently in graph data based on real-world problems.</p><p>A common concept to measure the similarity of graphs is based on graph edit distance <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b15">16]</ref>. Its main advantage is that graph edit distance can be applied to all kinds of graphs, while being able to model structural variation in a very intuitive and illustrative way. Edit distance has originally been developed for string matching <ref type="bibr" target="#b22">[23]</ref>, and a considerable amount of variants and extensions to edit distance have been proposed in recent years for strings and graphs. The basic idea is to model structural variation by edit operations, which reflect modifications in structure, such as the removal of a single node or the modification of an attribute attached to an edge. A standard set of edit operations consists of a node insertion, node deletion, node substitution, edge insertion, edge deletion, and edge substitution operation. A node deletion operation, for example, is used to model the removal of a node from a graph, and an edge substitution operation is used to model the modification of an edge attribute. A key property is that any graph can be transformed into any other graph by iteratively applying edit operations. Computing the edit distance of two graphs g and g 0 is equivalent to finding a sequence of edit operations transforming graph g into graph g 0 . Such a sequence of edit operations is also termed an edit path from g to g 0 . A trivial edit path from any graph g to any other graph g 0 is given by sequentially deleting all nodes and edges from g and then inserting all nodes and edges into g 0 , although it is usually not an adequate way to model the differences between the two graphs under consideration. The problem of measuring the similarity of two graphs hence turns into to the problem of finding the best model of the structural differences of two graphs.</p><p>To be able to quantify whether an edit operation modifies a graph's structure heavily or only slightly, edit cost functions are introduced. Edit cost functions assign a cost value to each edit operation reflecting the strength of the modification applied to the graph. To obtain a cost function on edit paths, individual edit operation costs of the edit path are accumulated. An edit path from g to g 0 with minimal costs can then be defined as the best model for the structural differences of g and g 0 . Not only can an optimal edit path be used for a visual description of the optimal stepwise transformation from g to g 0 , but it also provides us with a minimal transformation cost assigned to the two graphs, called graph edit distance. More formally, given two graphs g and g 0 , let E(g, g 0 ) denote the set of all edit paths from g to g 0 , and c denote a function assigning non-negative costs to edit operations. The graph edit distance of g and g 0 is then defined by dðg; g 0 Þ ¼ min</p><formula xml:id="formula_0">ðe 1 ;...;e k Þ2Eðg;g 0 Þ X k i¼1 cðe i Þ.<label>ð1Þ</label></formula><p>While leaving the general edit distance framework unchanged, edit cost functions can be used to tailor edit distance to specific applications and datasets. Node insertion, deletion, and substitution costs, for example, determine whether it is cheaper to delete a node u and subsequently insert a node u 0 instead of substituting node u with u 0 (which means that one prefers deletion and insertion over substitution in an optimal edit path). Edit operation costs are usually limited to non-negative values. If the cost functions additionally satisfy the conditions of positive definiteness and symmetry as well as the triangle inequality at the level of single edit operations, the resulting edit distance function d : G • G ! R + [ {0} is known to be a metric <ref type="bibr" target="#b3">[4]</ref>. The term distance is therefore legitimate for edit distance.</p><p>The edit distance of graphs is usually computed by means of a tree search procedure that basically evaluates all possible node-to-node correspondences <ref type="bibr" target="#b15">[16]</ref>. In some cases, the running time and memory requirements can be reduced by applying heuristics to the tree search. Yet, the overall computational complexity is rather highthe time and space complexity of a graph edit distance computation is exponential in the number of nodes involved. Thus, graph edit distance is in general only feasible for small graphs.</p><p>In edit distance based graph matching, it is often sufficient to deal with distances of graphs only, without recourse to the actual graph objects. For a graph dataset provided with a full distance matrix, a visual representation of the graph distribution, for instance, can be obtained by applying multidimensional scaling <ref type="bibr" target="#b7">[8]</ref>. The result of multidimensional scaling is a set of vectors in two or three dimensions reflecting the original graph edit distances. A commonly used approach in pattern classification is based on nearest-neighbor classification. That is, an unknown object is assigned the class or identity of its closest known element, or nearest neighbor. Although more complex algorithms are available, in the experiments presented in this paper we employ a simple nearest-neighbor classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Learning edit costs from samples</head><p>A major difficulty of edit distance, independent of its computational complexity, is the adequate definition of edit costs. The objective is to define edit costs in such a way that the resulting intra-class distances are small while inter-class distances are large, which is equivalent to graphs from the same class being similar and graphs from different classes being dissimilar, based on their edit distance.</p><p>The approach presented in this paper is based on a probabilistic modelling of the distribution of edit operations. Motivated by a cost learning method for string matching introduced in <ref type="bibr" target="#b19">[20]</ref>, we adopt a stochastic view on the process of editing a graph into another one. Instead of applying edit operations to graphs, we assume that a sequence of random edit operations is observed. Under a few weak constraints, one can show that every random sequence of edit operations is equivalent to a pair of graphs. Given a probability distribution on edit operations, the probability of a sequence of edit operations can be derived by summing up individual probabilities under the condition of stochastic independence. Hence, as the probability of each edit path is well defined, we define the probability of two graphs g and g 0 by</p><formula xml:id="formula_1">pðg; g 0 Þ ¼ Z ðe 1 ;...;e k Þ2Eðg;g 0 Þ dP ðe 1 ; . . . ; e k jUÞ;<label>ð2Þ</label></formula><p>where U denotes the parameters of the edit operation distribution. In this model, the probability of two graphs g and g 0 is governed by the probability of all underlying edit transformations from g to g 0 . In cases where the whole set of edit paths is not available for some reason, one can compute an approximate probability instead</p><formula xml:id="formula_2">pðg; g 0 Þ ¼ max ðe 1 ;...;e k Þ2Eðg;g 0 Þ P ðe 1 ; . . . ; e k jUÞ.<label>ð3Þ</label></formula><p>If we assume that the structural similarity of two graphs can be expressed by their probability, we obtain a dissimilarity measure on graphs by defining</p><formula xml:id="formula_3">dðg; g 0 Þ ¼ À log pðg; g 0 Þ.<label>ð4Þ</label></formula><p>Hence we arrive at a graph distance measure based on edit distance defined with respect to an underlying edit operation distribution. The issue of learning edit costs can thus be understood as learning the probability distribution of edit operations. As our general objective is to assign low distances to graphs from the same class and high distances to graphs from different classes, we modify the edit operation distribution such that graphs from the same class are assigned high probabilities while graphs from different classes have low probabilities.</p><p>Our general procedure therefore is as follows. We first introduce a model for the distribution of edit operations, train the model to obtain high intra-class probabilities, and finally derive edit costs from the model. These steps are described in greater detail in the remainder of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Distribution model</head><p>In the following we assume that node labels as well as edge labels are vectors of a fixed dimension. Note that this restriction is reasonable and rather weak from the application-oriented point of view, as most real-world graphs belong to this category or can easily be converted into it. The distribution model we propose is based on mixtures of Gaussians. Identifying edit operations by their labels, we use a Gaussian mixture density to model every edit operation type individually. Our model hence consists of three weighted node mixture densities and three weighted edge mixture densities, one for insertion, one for deletion, and one for substitution operations. The probability of an edge insertion, for instance, is given by the probability of the corresponding edge label in the edge insertion mixture density. Similarly, the probability of a node substitution is given by the probability of the corresponding pair of node labels in the node substitution mixture density. That is, the Gaussian mixtures can be considered an approximation of the unknown edit operation distribution in the space of node and edge labels.</p><p>More formally, if G( AE jl, R) denotes a multivariate Gaussian density with mean l and covariance matrix R, the probability of an edit path e = (e 1 , . . . , e k ) is given by</p><formula xml:id="formula_4">pðe 1 ; . . . ; e k Þ ¼ Y k j¼1 b tj X m t j i¼1 a i tj Gðe j jl i tj ; R i tj Þ;<label>ð5Þ</label></formula><p>where t j denotes the type of edit operation e j . Every edit operation type is additionally provided with a model weight b t j , a number of mixture components m tj , and for each component i 2 f1; . . . ; m tj g with a mixture weight a i tj , a mean vector l i tj , and a covariance matrix R i tj . If node labels are d-dimensional vectors, node insertion and deletion mixture components consist of d-variate Gaussian densities, and node substitution mixture components consist of 2d-variate Gaussian densities. In the proposed model we use weighted mixtures of Gaussians as they can be parametrized in a straightforward way and are suitable for an approximation of unknown distributions. The general considerations, however, are not limited to this kind of probability distribution models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Cost learning algorithm</head><p>The training of the distribution model of edit operations, and hence the learning of the edit cost functions, is intended to improve the compactness of graph classes, that is, to increase intra-class probabilities in a controlled manner. To this end we introduce a maximum likelihood criterion with respect to sample graphs. We assume that a labeled sample set of training graphs is given, that is, the class or identity of every graph in the training set is known. We proceed by extracting intra-class graph pairs from the training corpus, hence obtaining pairs of graphs required to be similar. During learning the underlying density model is adapted such that the probability of intra-class training pairs is increased, which is equivalent to intra-class distances according to Eq. ( <ref type="formula" target="#formula_3">4</ref>) being decreased.</p><p>To train the edit operation model, we apply the Expectation Maximization (EM) algorithm <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19]</ref>. EM can be used to estimate maximum likelihood parameters coping with missing or hidden data. An initial parametrized distribution is improved in a dual-step process by locally maximizing the conditional expectation of the hidden data given an observation. A convenient property of the EM algorithm is that the likelihood of the observed data does not decrease in consecutive EM steps. In our context, the likelihood of the training samples is maximized by modifying the hidden parameters of the underlying distribution. For the specific edit operation model introduced above, we can conclude that the training algorithm will converge to a stationary point on the likelihood surface <ref type="bibr" target="#b26">[27]</ref>.</p><p>The EM algorithm we propose is related to general EM mixture density learning. During learning, the parameters of the edit operation model given in Eq. ( <ref type="formula" target="#formula_4">5</ref> </p><formula xml:id="formula_5">R i t j ¼ P ðe 1 ;...;e k Þ pðe 1 ; . . . ; e k Þ P k l¼1 w il Á ðe l À l i tj Þðe l À l i tj Þ T P ðe 1 ;...;e k Þ pðe 1 ; . . . ; e k Þ P k l¼1 w il ;<label>ð8Þ</label></formula><p>where v tj ðe 1 ; . . . ; e k Þ denotes the number of edit operations of type t j occurring in edit path (e 1 , . . . , e k ), and w il is the posterior probability of mixture component i given edit operation e l . Note that the summation is performed over all edit paths between training graph pairs. A predefined initial set of distribution parameters is required by the training algorithm. The learning of edit costs does not lead to globally optimal parameters from the initial set of parameters in general, but only to a local optimum on the likelihood surface. Instead of choosing random values for the number of mixture components, the mixture weights, and the Gaussian parameters, we employ an iterative technique for mixture density estimation <ref type="bibr" target="#b21">[22]</ref>. Every mixture density is initialized with a single component derived from the training samples. Whenever the mixture density appears to converge during training, a number of new candidate components are added by evaluating a Taylor approximation of the likelihood function. The candidate that performs best in the evaluation is finally chosen. This procedure is terminated if the overall likelihood of the samples cannot be improved any further. Theoretical considerations encourage the use of such an iterative parameter initialization strategy <ref type="bibr" target="#b21">[22]</ref>. After convergence, all mixture components contributing only little to the overall distribution are removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental results</head><p>In the following, we present an experimental evaluation of the proposed cost learning method on synthetically generated letter graphs and graphs extracted from a standard database of fingerprint images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Recognition of letter line drawings</head><p>In the first experiment we consider the set of all capital characters that consist of straight line segments only <ref type="figure">(A,</ref><ref type="figure">E,</ref><ref type="figure">F,</ref><ref type="figure">H,</ref><ref type="figure">I,</ref><ref type="figure">K,</ref><ref type="figure">L,</ref><ref type="figure">M,</ref><ref type="figure">N,</ref><ref type="figure">T,</ref><ref type="figure">V,</ref><ref type="figure">W,</ref><ref type="figure">X,</ref><ref type="figure">Y,</ref><ref type="figure">Z</ref>). To obtain a letter graph database, we manually create a prototype of an ideal letter drawing -one for each of the 15 classes considered. The class prototypes then undergo a distortion process leading to a random removal, insertion, or displacement of one or several line segments. An arbitrary number of sample graphs can be generated by repeatedly distorting the ideal prototype. An illustration of an ideal prototype and two distorted instances of a letter A line drawing are provided in Fig. <ref type="figure" target="#fig_1">1a</ref> and<ref type="figure">b-c</ref>, respectively. Such line drawings are then converted into attributed graphs by representing ending points by nodes (with a label giving the position of the node) and lines by edges (without label) <ref type="bibr" target="#b17">[18]</ref>. We begin our evaluation by visualizing the learning of edit costs. For a database of three letter classes and 10 graphs per class, the edit distance between all graph pairs is computed before learning and after termination of the learning algorithm. For a visual representation of the distribution of the graphs, we derive a Euclidean embedding from the full distance matrix by means of a multidimensional scaling technique <ref type="bibr" target="#b7">[8]</ref>. In Fig. <ref type="figure" target="#fig_2">2</ref>, the graph distribution before learning and after learning is illustrated. It can clearly be observed that before learning the three classes severely overlap (Fig. <ref type="figure" target="#fig_2">2a</ref>). After learning, however, well-separated clusters of graphs are obtained (Fig. <ref type="figure" target="#fig_2">2b</ref>).</p><p>In the following we use a cluster validation index to measure the quality of a clustering in quantitative terms, instead of resorting to a visual interpretation. A cluster validation measure is a function indicating how well the classes are clustered in the graph space. The measure we use is based on the C-index <ref type="bibr" target="#b12">[13]</ref> and is adapted so as to produce high values for compact and well-separated clusters. In the context of the learning of edit costs, the edit distance, and hence eventually the underlying edit costs, determine what clusters result from the graph matching process. In Fig. <ref type="figure" target="#fig_3">3a</ref>, the edit cost learning process is illustrated in terms of the cluster validation index. Note that an improvement in the clustering structure and an apparent convergence of the validation index after only 10 iterations are clearly visible.</p><p>For the letter graph dataset, another edit cost model that is specifically suitable for the letter graph representation has been heuristically developed. This application-specific model assigns costs that are proportional to the Euclidean distance of two labels in case of substitutions, while insertions and deletions have fixed costs. The model hence takes into account that the node parameters represent Euclidean coordinates. To evaluate  which model best copes with strong distortions, we carry out another experiment on the same dataset. By applying an additional distortion operator in the form of a shearing transformation to the graphs, the original dataset is converted into a more difficult dataset with stronger distortions. We proceed by generating six sample sets of graphs from the original dataset with various degrees of distortion. (For an example of distorted characters see an ideal letter prototype in Fig. <ref type="figure" target="#fig_1">1a</ref> and two distorted letter instances in Fig. <ref type="figure" target="#fig_1">1d-e.</ref>) Comparing the proposed model to the application-specific heuristic model, we find that the probabilistic estimation of the edit operation distribution is especially effective on heavily distorted data. The accuracy of a leave-one-out nearest-neighbor classifier on the original letter dataset and six datasets of various degrees of distortion is illustrated in Fig. <ref type="figure" target="#fig_4">4a</ref>. The handcrafted application-specific model performs well compared to the probabilistic model in case of small distortions. With an increasing degree of distortion, however, it rapidly deteriorates, and the stochastic model proposed in this paper becomes superior.</p><p>We conclude from the letter graph experiments that the proposed learning algorithm is able to estimate the edit operation distribution such that an improved set of edit costs can be derived from the underlying mixture model. The learning of the edit costs results in compact clusters and an improvement in classification accuracy, particularly in case of heavy distortions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Fingerprint classification</head><p>Fingerprint classification is the task of grouping fingerprints into classes with similar characteristics <ref type="bibr" target="#b14">[15]</ref>. The fingerprint classification system we describe in this paper is based on a structural representation of singular regions extracted from fingerprints <ref type="bibr" target="#b16">[17]</ref>. A fingerprint image, the corresponding characteristic regions, and the extracted graph are shown in Fig. <ref type="figure" target="#fig_5">5a-c</ref>.  We demonstrate the usefulness of the cost learning algorithm on 500 graphs from the NIST-4 database of fingerprints <ref type="bibr" target="#b24">[25]</ref>. We again compute a cluster validation index reflecting the quality of the graph clustering. The learning of edit costs in terms of the clustering quality is illustrated in Fig. <ref type="figure" target="#fig_3">3b</ref>. It turns out that the validation index improves only slightly, the improvement being virtually invisible in the illustration. Hence, the training process is unable to strongly adapt the edit costs to the fingerprint graph sample.</p><p>If we compare the classification accuracy, however, we observe that the probabilistic model proposed in this paper outperforms the application-specific model. In Fig. <ref type="figure" target="#fig_4">4b</ref>, the classification performance obtained on an independent test set of 500 fingerprints by means of a nearest-neighbor classifier on the training set is illustrated. As expected from the results of the training process shown in Fig. <ref type="figure" target="#fig_3">3b</ref>, the classification performance of the probabilistic model in Fig. <ref type="figure" target="#fig_4">4b</ref> is almost independent of the training iteration. The initial estimation of the edit operation distribution is therefore not significantly adapted by the training process. Yet, the proposed system in its initial state already provides us with better cost functions than the manually designed system. Using the proposed model, we can improve the classification rate from 66.6% to 77.6%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper we propose a method to derive graph edit costs from a probabilistic model. Edit costs are used to compute distances between graphs by performing a structural matching. We introduce a probabilistic model for graph edit operations and show how to estimate the edit operation distribution from a labeled set of graphs. The edit costs are adapted so as to decrease the distance between graphs from the same class, leading to compact graph clusters. In an experimental evaluation, we show that our method can be used to learn edit costs that result in enhanced clusterings and improved recognition performance. The learning process is demonstrated on synthetically generated graphs representing letter drawings and on real-world fingerprint graphs. The proposed method is found to outperform application-specific models of edit operation costs. In the future, we intend to apply the learning algorithm to other graph datasets. We also plan to further investigate the convergence behavior of the system on difficult graph representations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>0020-0255/$ -see front matter Ó 2006 Elsevier Inc. All rights reserved. doi:10.1016/j.ins.2006.02.013</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) Ideal letter prototype, (b-c) distorted instances of ideal prototype, and (d-e) sheared instances of ideal prototype.</figDesc><graphic coords="5,90.71,590.01,354.96,78.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Distribution of three graph classes (a) before learning and (b) after learning.</figDesc><graphic coords="6,102.75,309.57,343.17,176.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Clustering quality of the probabilistic model on (a) letter graphs and (b) fingerprint graphs.</figDesc><graphic coords="6,75.20,524.99,396.18,143.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Classification accuracy of the probabilistic model compared to an application-specific model on (a) letter graphs and (b) fingerprint graphs.</figDesc><graphic coords="7,70.87,67.32,397.44,146.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. (a) Fingerprint image, (b) detected characteristic regions, and (c) extracted attributed graph.</figDesc><graphic coords="7,93.54,540.18,354.60,128.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>) are adapted according to b tj ¼ P ðe 1 ;...;e k Þ pðe 1 ; . . . ; e k Þv t j ðe 1 ; . . . ; e k Þ P ðe 1 ;...;e k Þ pðe 1 ; . . . ; e k Þ Á k ;...;e k Þ pðe 1 ; . . . ; e k Þ P k l¼1 w il P ðe 1 ;...;e k Þ pðe 1 ; . . . ; e k Þ Á v tj ðe 1 ; . . . ; e k Þ</figDesc><table><row><cell></cell><cell></cell><cell>;</cell><cell>ð6Þ</cell></row><row><cell>a i tj ¼</cell><cell cols="2">P ðe 1 ;</cell><cell>ð7Þ</cell></row><row><cell>l i tj ¼</cell><cell>P P ðe 1 ;...;e k Þ pðe 1 ; . . . ; e k Þ ðe 1 ;...;e k Þ pðe 1 ; . . . ; e k Þ P k l¼1 w il Á e l P k l¼1 w il</cell><cell>;</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>M. Neuhaus, H. Bunke / Information Sciences 177 (2007) 239-247</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by the Swiss National Science Foundation NCCR program ''Interactive Multimodal Information Management (IM)2'' in the Individual Project ''Multimedia Information Access and Content Protection''.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Special Section on Graph algorithms and computer vision</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1040" to="1151" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Special Issue on graph based representations</title>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1033" to="1122" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Special Issue on Graph matching in pattern recognition and computer vision</title>
	</analytic>
	<monogr>
		<title level="j">International Journal of Pattern Recognition and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="261" to="517" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Inexact graph matching for structural pattern recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Allermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="245" to="253" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A graph distance metric based on the maximal common subgraph</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shearer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="255" to="259" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Structural matching in computer vision using probabilistic relaxation</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Petrou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="749" to="764" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Thirty years of graph matching in pattern recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Conte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Foggia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sansone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Pattern Recognition and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="265" to="298" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">T</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Multidimensional</forename><surname>Scaling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Chapman and Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Graph matching with a dual-step EM algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D J</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1236" to="1253" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A graph distance metric combining maximum common subgraph and minimum common supergraph</title>
		<author>
			<persName><forename type="first">M.-L</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Valiente</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6-7</biblScope>
			<biblScope unit="page" from="753" to="758" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Symbolic graph matching with the EM algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Finch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1777" to="1790" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Quadratic assignment as a general data analysis strategy</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Journal of Mathematical and Statistical Psychology</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="190" to="241" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Structural graph matching using the EM algorithm and singular value decomposition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1120" to="1136" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Maltoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Prabhakar</surname></persName>
		</author>
		<title level="m">Handbook of Fingerprint Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A new algorithm for error-tolerant subgraph isomorphism detection</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Messmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="493" to="504" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A graph matching based approach to fingerprint classification using directional variance</title>
		<author>
			<persName><forename type="first">M</forename><surname>Neuhaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Audio-and Video-Based Biometric Person Authentication</title>
		<meeting>the 5th International Conference on Audio-and Video-Based Biometric Person Authentication</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3546</biblScope>
			<biblScope unit="page" from="191" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Self-organizing maps for learning the edit costs in graph matching</title>
		<author>
			<persName><forename type="first">M</forename><surname>Neuhaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics (Part B)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="503" to="514" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mixture densities, maximum likelihood and the EM algorithm</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Redner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">F</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Society for Industrial and Applied Mathematics (SIAM) Review</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="195" to="239" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning string edit distance</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ristad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yianilos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="522" to="532" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A distance measure between attributed relational graphs for pattern recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sanfeliu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics (Part B)</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="353" to="363" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A greedy EM algorithm for Gaussian mixture learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Vlassis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Likas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Processing Letters</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="87" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The string-to-string correction problem</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Association for Computer Machinery</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="168" to="173" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Graph distances using graph union</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">D</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shoubridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kraetzl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="701" to="704" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIST Special Database</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="1992-03">March 1992</date>
		</imprint>
	</monogr>
	<note type="report_type">Fingerprint Database</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Structural matching by discrete relaxation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="634" to="648" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On the convergence properties of the EM algorithm</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="95" to="103" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
