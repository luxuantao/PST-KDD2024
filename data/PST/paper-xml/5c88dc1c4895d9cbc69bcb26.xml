<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint Feature Selection and Classification for Multilabel Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jun</forename><surname>Huang</surname></persName>
							<email>huangjun13b@mails.ucas.ac.cn</email>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Guorong</forename><surname>Li</surname></persName>
							<email>liguorong@ucas.ac.cn</email>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Qingming</forename><surname>Huang</surname></persName>
							<email>qmhuang@ucas.ac.cn</email>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Xindong</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>101480</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Anhui University of Technology</orgName>
								<address>
									<postCode>243032</postCode>
									<settlement>Maanshan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>101480</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>101480</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">School of Computing and Informatics</orgName>
								<orgName type="institution">University of Louisiana at Lafayette</orgName>
								<address>
									<postCode>70503</postCode>
									<settlement>Louisiana</settlement>
									<region>LA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Joint Feature Selection and Classification for Multilabel Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B9983620A9AAECDD499FC72F70904CE2</idno>
					<idno type="DOI">10.1109/TCYB.2017.2663838</idno>
					<note type="submission">received August 26, 2016; revised November 29, 2016; accepted January 23, 2017.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Feature selection</term>
					<term>label correlation</term>
					<term>label-specific features</term>
					<term>multilabel classification</term>
					<term>shared features</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multilabel learning deals with examples having multiple class labels simultaneously. It has been applied to a variety of applications, such as text categorization and image annotation. A large number of algorithms have been proposed for multilabel learning, most of which concentrate on multilabel classification problems and only a few of them are feature selection algorithms. Current multilabel classification models are mainly built on a single data representation composed of all the features which are shared by all the class labels. Since each class label might be decided by some specific features of its own, and the problems of classification and feature selection are often addressed independently, in this paper, we propose a novel method which can perform joint feature selection and classification for multilabel learning, named JFSC. Different from many existing methods, JFSC learns both shared features and labelspecific features by considering pairwise label correlations, and builds the multilabel classifier on the learned low-dimensional data representations simultaneously. A comparative study with state-of-the-art approaches manifests a competitive performance of our proposed method both in classification and feature selection for multilabel learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>I N MULTILABEL learning, each example might be asso- ciated with multiple class labels simultaneously, which may be correlated with each other. Recent works have witnessed the fast development of multilabel learning in various research areas, such as text categorization <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, image annotation <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b4">[5]</ref>, and video annotation <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. However, with the ever-growing digital data in text, image, video, etc., the big volume and high dimensionality of data will hinder the performance of multilabel learning algorithms. Over the past years, a variety of algorithms have been proposed for multilabel learning. Most of these algorithms are designed for multilabel classification, while feature selection for multilabel learning does not attract sufficient attention. Besides, there are two limitations with existing efforts.</p><p>First, existing multilabel algorithms mainly utilize an identical data representation in the discrimination of all the class labels. In multilabel learning, each example is represented by a single instance and associated with several labels, each of which might be determined by some specific features of its own. Taking image annotation as an example, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>, each moment feature corresponds to a block region in the image. Label "sky" might be only related to the green blocks, label "ship" might be only related to the yellow blocks, and "sea water" might be only related to the blue blocks.</p><p>Second, current feature selection algorithms mainly learn a subset of features which are shared by all the class labels. As aforementioned, each class label might be decided by some specific features of its own. In addition, feature selection and multilabel classification are often addressed separately.</p><p>In order to solve the aforementioned problems, we propose to perform joint feature selection and classification (JFSC) for multilabel learning in an unified framework. We try to learn label-specific and shared features with structured sparsity regularization, and build a multilabel classifier on the learned low-dimensional data representation. In addition, we expect that the instances could become more separable in the learned low-dimensional space. Motivated by the core idea of linear discriminant analysis <ref type="bibr" target="#b7">[8]</ref>, we propose a Fisher discriminant-based regularization term to minimize the inner-class distance and maximize the intraclass distance for each label.</p><p>We summarize the contributions of this paper as follows. 1) Joint feature selection with sparsity and multilabel classification are combined into a single framework, which can select the most discriminative features for each label and learn an effective classification model. Thus, the 2168-2267 c 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. proposed method JFSC can perform multilabel classification and feature selection simultaneously. 2) Different from existing feature selection methods, we propose to learn label-specific features and shared features by exploiting pairwise label correlation. Different from most existing multilabel classification methods, the proposed method uses label-specific data representation to discriminate each corresponding class label. 3) We propose a Fisher discriminant-based regularization term to minimize the inner-class distance and maximize the intraclass distance for each label. 4) The experiment on 16 multilabel benchmark data sets shows a competitive performance of our proposed method against the state-of-the-art multilabel learning algorithms both in classification and feature selection. The rest of this paper is organized as follows. Section II reviews previous works on multilabel classification and feature selection. Section III presents details of the proposed method JFSC. Experimental results and analyses are shown in Section V. Finally, we conclude this paper in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Multilabel Classification</head><p>In multilabel learning <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b12">[13]</ref>, the examples have multiple class labels simultaneously and each example is represented by one single instance. The task of multilabel learning is how to learn an effective learning function which can predict a set of possible class labels for an unseen example. Formally, let X = R m denote the m-dimensional input space and Y = {y 1 , y 2 , . . . , y l } denote the label space with l class labels. Then, the task of multilabel learning is to learn a multilabel classification function f : X → 2 Y which assigns each instance x ∈ X with a set of possible class labels f (x) ⊆ Y.</p><p>In the past decades, many well-established approaches have been proposed to solve multilabel classification problems in various domains. According to the popular taxonomy presented in <ref type="bibr" target="#b8">[9]</ref>, multilabel learning approaches can be divided into two categories: 1) problem transformation approaches and 2) algorithm adaption approaches.</p><p>Problem transformation approaches transform a multilabel classification problem into either one or more single-label classification problems that are solved with a single-label classification algorithm. The binary relevance (BR) approach <ref type="bibr" target="#b2">[3]</ref> decomposes a multilabel learning problem into l independent binary (one-versus-rest) classification problems. BR is simple and effective, but it is criticized for ignoring label correlation.</p><p>The label powerset (LP) approach <ref type="bibr" target="#b8">[9]</ref> considers each unique set of labels that exists in a multilabel training data as a new label, then a multilabel classification problem is transformed into a multiclass one. LP is effective and simple and is also able to model label correlations in the training data. It will generate a larger number of new labels, and it is possible to have limited training examples for these new labels. Also, it cannot predict unseen label sets. Some approaches have been proposed to overcome these problems, such as Random k labelsets (RAkEL) <ref type="bibr" target="#b13">[14]</ref> and multi-label classification using ensembles of pruned sets (EPS) <ref type="bibr" target="#b14">[15]</ref>.</p><p>Algorithm adaption approaches modify traditional singlelabel classification algorithms for multilabel classification directly. Almost all single-label classification algorithms have been revisited in order to be adapted to multilabel data, such as <ref type="bibr" target="#b1">[2]</ref> and <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b17">[18]</ref>. multi-label k-nearest neighbor (ML-kNN) <ref type="bibr" target="#b16">[17]</ref> is derived from traditional kNN algorithm. For each new test example, its k nearest neighbors in the training data are first identified. Then, the maximum a posteriori rule is utilized to make prediction by reasoning with the labeling information embodied in the neighbors. RankSVM <ref type="bibr" target="#b17">[18]</ref> adapts a maximum margin strategy to deal with multilabel data, where a set of linear classifiers are optimized to minimize the empirical ranking loss with quadratic programming and enabled to handle nonlinear cases with kernel tricks.</p><p>On the other hand, in multilabel learning, labels may be dependent on each other, not necessarily mutually exclusive. Previous works have theoretically or practically proved the effectiveness of mining such dependency relationship between labels. Over the past decades, a multitude of approaches have been proposed for multilabel classification by mining the dependency among labels. According to the way of how the dependency relationship is modeled, existing approaches can be generally grouped into three categories, i.e., first-order, second-order, and high-order approaches <ref type="bibr" target="#b9">[10]</ref>.</p><p>First-order approaches tackle multilabel learning without exploiting label correlation among labels, such as BR <ref type="bibr" target="#b2">[3]</ref>, multi-label learning with Label specific features (LIFT) <ref type="bibr" target="#b18">[19]</ref>, and algorithm adaption approaches ML-kNN <ref type="bibr" target="#b16">[17]</ref> and sparse weighted instance-based multilabel (SWIM) <ref type="bibr" target="#b19">[20]</ref>. First-order algorithms are simple and efficient, but could be less effective due to the ignorance of label correlations.</p><p>Second-order approaches tackle multilabel learning by mining pairwise relationship between label pairs. The most popular way to model pairwise relationship is to exploit the interaction between any pair of labels, such as calibrated label ranking <ref type="bibr" target="#b20">[21]</ref>, learning label-specific features for multilabel classification (LLSF) <ref type="bibr" target="#b21">[22]</ref>, and multi-label teaching-to-learn and learning-to-teach <ref type="bibr" target="#b22">[23]</ref>. Another way is to incorporate the criterion of ranking loss into the objective function to be optimized when learning the classification models, e.g., RankSVM <ref type="bibr" target="#b17">[18]</ref>, back propagation for multi-label learning <ref type="bibr" target="#b1">[2]</ref>, and relative labeling-importance aware multi-label learning <ref type="bibr" target="#b23">[24]</ref>. These algorithms exploit the dependency relationship between two labels. However, one label might be dependent on multiple class labels.</p><p>High-order approaches tackle the multilabel learning problem by mining relationships among all the class labels or a subset of class labels, such as the transformation approaches LP <ref type="bibr" target="#b8">[9]</ref>, RAkEL <ref type="bibr" target="#b13">[14]</ref>, EPS <ref type="bibr" target="#b14">[15]</ref>, and classifier chains (CCs) <ref type="bibr" target="#b24">[25]</ref>. CC transforms a multilabel classification problem into a chain of l binary classification subproblems, where the ith classifier f i is trained by using the results of labels y 1 , y 2 , . . . , y i-1 as additional features. To predict subsequent labels in a given chain order, CC resorts to using outputs of the preceding classifiers f 1 , f 2 , . . . , f i-1 . The performance of CC is seriously constrained by the chain order and error propagation (i.e., the incorrect predictions of preceding labels will propagate to subsequent labels), which can be alleviated by ensemble learning. In addition, it might be inappropriate that each label is dependent on all the preceding labels in a given chain order. Extended works on CC mainly search for suitable chain orders or dependent structures among class labels and reduce the complexity of inference (see <ref type="bibr" target="#b25">[26]</ref>- <ref type="bibr" target="#b31">[32]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multilabel Feature Selection</head><p>1) Feature Selection: Feature selection plays an important role in data mining and machine learning, as it can speed up the learning process and even boost the performance of classification. Traditionally, feature selection approaches can be grouped into three categories, i.e., filter, wrapper and embedded approaches <ref type="bibr" target="#b32">[33]</ref>. Filter approaches select features by ranking them with correlation coefficients, such as Fisher score <ref type="bibr" target="#b7">[8]</ref>, f-statistic <ref type="bibr" target="#b33">[34]</ref>, ReliefF <ref type="bibr" target="#b34">[35]</ref>, and feature selection through message passing <ref type="bibr" target="#b35">[36]</ref>. Wrapper approaches iteratively apply a heuristic search strategy (e.g., genetic algorithm) to determine one or more small subsets of features and evaluate their corresponding performance of classification using an off-the-shelf classifier. While embedded approaches directly incorporate feature selection as a part of the classifier training process, such as decision trees <ref type="bibr" target="#b7">[8]</ref>, neural networks <ref type="bibr" target="#b7">[8]</ref>, lasso <ref type="bibr" target="#b36">[37]</ref>, and robust feature selection (RFS) <ref type="bibr" target="#b37">[38]</ref>.</p><p>For multilabel learning, based on the taxonomy of multilabel classification algorithms, feature selection approaches can be grouped into two categories: 1) transformation-based approaches and 2) direct (adapted) approaches <ref type="bibr" target="#b38">[39]</ref>. Problem transformation approaches transform a multilabel data instance into either one or more single-label data instances. Then, the traditional single-label feature selection approaches can be employed directly, e.g., the aforementioned filter, wrapper and embedded approaches. However, the correlation among labels in multilabel learning is usually ignored.</p><p>Direct feature selection approaches revise traditional singlelabel feature selection algorithms to process the multilabel data directly, including the filter, wrapper and embedded approaches, such as <ref type="bibr" target="#b39">[40]</ref>- <ref type="bibr" target="#b49">[50]</ref>. MReliefF and MF-statistic <ref type="bibr" target="#b39">[40]</ref> are adapted from the traditional filter algorithms ReliefF and F-statistic. soft-constrained Laplacian score <ref type="bibr" target="#b40">[41]</ref> is adapted from the constrained Laplacian score for semisupervised multilabel feature selection. graph-margin based multi-label feature selection algorithm <ref type="bibr" target="#b44">[45]</ref> describes the multilabel data with a graph, and then measures the features with the large margin theory based on it. MLNB <ref type="bibr" target="#b47">[48]</ref> is a wrapper feature selection approach for multilabel learning. It uses a genetic algorithm as the search component, and proposed a multilabel Naive Bayes classifier to select the best features. LLSF <ref type="bibr" target="#b21">[22]</ref>, subfeature uncovering with sparsity (SFUS) <ref type="bibr" target="#b48">[49]</ref>, multilabel informed feature selection (MIFS) <ref type="bibr" target="#b50">[51]</ref>, multiview multilabel <ref type="bibr" target="#b51">[52]</ref>, and convex semi-supervised multi-label feature selection <ref type="bibr" target="#b52">[53]</ref> are embedded feature selection approaches for multilabel classification. These approaches learn linear classifiers (e.g., linear regression) with sparse regularization to conduct feature selection. Also, there are some feature extraction approaches that have been proposed for multilabel learning, such as multi-label dimensionality reduction via dependence maximization <ref type="bibr" target="#b53">[54]</ref>, the complementary decision reduct algorithm <ref type="bibr" target="#b54">[55]</ref>, and maximizes feature variance and maximizes feature-label dependence <ref type="bibr" target="#b55">[56]</ref>.</p><p>2) Label-Specific Features: Previous multilabel feature selection approaches mainly select a subset of common features shared by all the class labels. In multilabel learning, one example is associated with multiple class labels simultaneously, and each class label might be determined by some specific features of its own. Traditional feature selection approaches can be used to select a subset of label-specific features for each class label based on the BR transformation framework, such as lasso and F-score. However, the label correlation information among class labels is ignored.</p><p>LIFT <ref type="bibr" target="#b18">[19]</ref> utilizes label-specific features to represent instances to predict the corresponding class label. It can be viewed as a feature mapping method, which lacks of interpretability, and does not exploit label correlation. multi-label learning approach with label-specific feature reduction based on fuzzy rough set <ref type="bibr" target="#b56">[57]</ref> tries to select a subset of the labelspecific features generated by LIFT with the fuzzy rough set approach. LLSF <ref type="bibr" target="#b21">[22]</ref> learns label-specific features for multilabel classification by exploiting the second-order label correlation. Meanwhile, some works have been proposed to learn common and task-specific features for multitask learning, such as DirtyLasso <ref type="bibr" target="#b57">[58]</ref>, GFLasso <ref type="bibr" target="#b58">[59]</ref>, and FelMuG <ref type="bibr" target="#b59">[60]</ref>. DirtyLasso <ref type="bibr" target="#b57">[58]</ref> employs the 1,∞ -norm and 1 -norm to extract essential features shared by all the tasks and task-specific features, respectively. DirtyLasso does not exploit the correlation among different tasks. GFLasso <ref type="bibr" target="#b58">[59]</ref> encourages highly correlated tasks to share a common set of relevant features by calculating the distance between coefficient vectors of tasks, and the 1 -norm is employed to extract task-specific features. FelMuG <ref type="bibr" target="#b59">[60]</ref> is a task sensitive feature exploration and learning approach for multitask graph classification. It aims to learn common features, task auxiliary features and task specific features. The discriminability of label (task)-specific features of these approaches are mainly modeled by the least square loss.</p><p>By surveying previous works on multilabel learning, we find that existing multilabel classification approaches mainly use an identical data representation composed of all the features of a data set to discriminate all the class labels. However, each class label might be determined by some specific features of its own. Existing multilabel feature selection approaches mainly learn a low-dimensional data representation, composed of a subset of common features shared by all the class labels. On the other hand, feature selection and multilabel classification are often addressed separately. Embedded approaches can model feature selection and classification simultaneously, but many of them were proposed for single-label classification, e.g., lasso <ref type="bibr" target="#b36">[37]</ref>, RFS <ref type="bibr" target="#b37">[38]</ref>, and archive-based steady state micro genetic programming <ref type="bibr" target="#b60">[61]</ref>.</p><p>In this paper, we propose an unified framework for multilabel classification and feature selection. We seek to learn label-specific features and shared features for the discrimination of each class label by exploiting pairwise label correlations, and build a multilabel classifier on the learned low-dimensional data representations simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED APPROACH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Preliminary</head><p>For an arbitrary matrix A, a i and a j are the ith row and the jth column of A, respectively, and a ij is the (i, j)th entry. Given a multilabel data D = {(x i , y i )} n i=1 with n examples. We denote the training data as a matrix X = [x 1 , x 2 , . . . , x n ] T ∈ R n×m , where m is the dimension of the data set. Let Y = [y 1 , y 2 , . . . , y n ] T ∈ {0, 1} n×l be the label matrix, l stands for the number of class labels. y i = [y i1 , y i2 , . . . , y il ] is a ground truth label vector of x i . If x i belongs to label y j , then y ij = 1; otherwise y ij = 0.</p><p>Our goal is to learn a coefficient matrix W = [w 1 , w 2 , . . . , w l ] ∈ R m×l , which can be used for multilabel classification. Moreover, the weight of each element of W can guide feature selection simultaneously. To guarantee that W can achieve excellent performance on classification and feature selection, we expect it to possess the following properties.</p><p>1) The coefficient matrix W can map the data matrix X to the label matrix Y well. 2) We assume that each class label is only determined by a subset of specific features from the original feature set of a given data set. These label-specific features are determined by the nonzero entries of each w i , and have strong discriminability to the corresponding class label. 3) For each class label, we expect a large interclass distance and a small inner-class distance in the low-dimensional space with label-specific features indicated by each w i , 1 ≤ i ≤ l. 4) In multilabel learning, labels may be correlated with each other. We expect that any two strongly correlated class labels can share more features with each other than two uncorrelated or weakly correlated ones. To achieve these goals, we can generalize our problem as the following optimization formulation:</p><formula xml:id="formula_0">min W L(W) + αS(W) + βR 1 (W) + γ R 2 (W) (1)</formula><p>where L(•) is a loss function, S(•) is employed to model the sharing of label-specific features, R 1 (•) is the regularization term to model large interclass distances and small inner-class distances in the low-dimensional space, and R 2 (•) is the sparsity regularization term to learn label-specific features. α, β, and γ are the tradeoff parameters with non-negative values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Discriminability and Sparsity of Label-Specific Features</head><p>The loss function L(W) can be implemented with various ways. Here we employ the robust loss function x i w k + b k -y ik 2 which is less sensitive to outliers than the squared loss</p><formula xml:id="formula_1">x i w k + b k -y ik 2 2</formula><p>. Thus, the L(W) can be formulated as</p><formula xml:id="formula_2">L(W, b) = 1 2 l k=1 n i=1 x i w k + b k -y ik 2 (2) where b = [b 1 , b 2 , . . . , b l ] T ∈ R l×1</formula><p>is the bias. For simplicity, the bias b can be absorbed into W when the constant value 1 is added as an additional dimension for each data</p><formula xml:id="formula_3">x i (1 ≤ i ≤ n).</formula><p>Thus L(W, b) can be rewritten as L(W)</p><formula xml:id="formula_4">L(W) = 1 2 l k=1 n i=1 x i w k -y ik 2 = 1 2 Tr (XW -Y) T D(XW -Y)<label>(3)</label></formula><p>where D is a diagonal matrix with element d ii defined as</p><formula xml:id="formula_5">d ii = 1 2 x i W -y i 2 . (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>To induce sparse label-specific features, we add an 1 -norm on the coefficient matrix W</p><formula xml:id="formula_7">R 2 = W 1 .</formula><p>(5)</p><p>Thus, the nonzero entry w ij indicates that the ith features is discriminative to label y j . The larger the value of |w ji |, the stronger the discriminability of the jth feature to y i . We say these features are label-specific features to y j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Fisher Discriminant-Based Regularization</head><p>For each class label, we expect a large interclass distance between positive and negative examples, and a small innerclass distance for the positive and negative examples in the low-dimensional space with label-specific features. To reach this goal, motivated by the core idea of linear discriminant analysis <ref type="bibr" target="#b7">[8]</ref>, we propose a new Fisher discriminant-based regularization term R 1 (W).</p><p>First, we define the sets of positive and negative examples in the original feature space for each label. Examples associating with the kth label are considered as positive examples, while those without this class label are considered as negative ones</p><formula xml:id="formula_8">P k = x i (x i , y i ) ∈ D, y ik = 1 (<label>6</label></formula><formula xml:id="formula_9">)</formula><formula xml:id="formula_10">N k = x i (x i , y i ) ∈ D, y ik = 0 (<label>7</label></formula><formula xml:id="formula_11">)</formula><p>where P k and N k are the sets of positive and negative examples of the kth label, respectively. Then, the mean vectors of positive set P k and negative set N k in the low-dimensional space with label-specific features can be defined as m + k and m - k , respectively</p><formula xml:id="formula_12">m + k = 1 |P k | x i ∈P k x i w k = x+ k w k (8) m - k = 1 |N k | x i ∈N k x i w k = x- k w k (9)</formula><p>where </p><formula xml:id="formula_13">x i w k = [x i1 w 1k ,</formula><formula xml:id="formula_14">x+ kj = (1/|P k |) x i ∈P k x ij . x- k = [x - k1 , x- k2 , . . . , x- km ], and x- kj = (1/|N k |) x i ∈N k x ij .</formula><p>Thus, the interclass distance between positive and negative examples can be calculated by</p><formula xml:id="formula_15">S k 2 = m + k -m - k 2 2 = w k T diag(s k )w k (<label>10</label></formula><formula xml:id="formula_16">)</formula><p>where</p><formula xml:id="formula_17">s k = (x + k -x- k ) (x + k -x- k ).</formula><p>Third, the within-class distance of positive examples can be calculated by</p><formula xml:id="formula_18">S + k 2 = x i ∈P k x i w k -m + k 2 2 = w k T diag s + k w k (11)</formula><p>where</p><formula xml:id="formula_19">s + k = x i ∈P k s + ki and s + ki = (x i -x+ k ) (x i -x+ k ).</formula><p>Similarly, the within-class distance of negative examples can be calculated by</p><formula xml:id="formula_20">S - k 2 = x i ∈N k x i w k -m - k 2 2 = w k T diag s - k w k (12)</formula><p>where</p><formula xml:id="formula_21">s - k = x i ∈N k s - ki and s - ki = (x i -x- k ) (x i -x- k ).</formula><p>Finally, we can define the Fisher discriminant-based regularization term as</p><formula xml:id="formula_22">R 1 (W) = 1 2 l k=1 S + k 2 + r k S - k 2 -λS k 2 (<label>13</label></formula><formula xml:id="formula_23">)</formula><p>where </p><formula xml:id="formula_24">r k = |P k |/|N k | is</formula><formula xml:id="formula_25">R 1 (W) = 1 2 l k=1 w k T S k w k (<label>14</label></formula><formula xml:id="formula_26">)</formula><p>where</p><formula xml:id="formula_27">S k = diag(s + k + r k s - k -|P k |s k ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Sharing of Label-Specific Features</head><p>Previous works indicate that exploiting label correlation can improve the performance of multilabel classifiers. Motivated by the works on multitask learning <ref type="bibr" target="#b57">[58]</ref>- <ref type="bibr" target="#b59">[60]</ref>, which learns the sharable features between tasks or modalities by considering their relationships, we assume that any two strongly correlated class labels can share more features with each other than two uncorrelated or weakly correlated ones. In other words, if label y i and label y j are strongly correlated, features discriminative to y i may also be discriminative to y j with a higher probability. Then the corresponding coefficients w i and w j will be very similar, and thus the inner product between them will be large, otherwise, the inner product will be small.</p><p>Similar to <ref type="bibr" target="#b21">[22]</ref>, we exploit the pairwise label correlation to model the property of sharing of label-specific features between label pairs by using the regularization in the following equation:</p><formula xml:id="formula_28">S(W) = 1 2 l i=1 l j=1 r ij w i T w j = Tr WRW T (15)</formula><p>where R ∈ R l×l with each element r ij = 1c ij , and c ij indicates the correlation between y i and y j . Similar to previous works, in this paper, correlation is calculated by cosine similarity between label pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. OPTIMIZATION VIA ACCELERATED PROXIMAL GRADIENT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Optimization</head><p>Based on the definitions of the terms regarding loss function and regularization, we can rewrite the objective function in (1) as follows:</p><formula xml:id="formula_29">F(W) = 1 2 Tr (XW -Y) T D(XW -Y) + α 2 Tr WRW T + β 2 l k=1 w k T S k w k + γ W 1 . (<label>16</label></formula><formula xml:id="formula_30">)</formula><p>The problem ( <ref type="formula" target="#formula_29">16</ref>) is convex but nonsmooth due to the 1 -norm. We seek to solve it by the accelerated proximal gradient method. A general accelerated proximal gradient method can be written as the following convex optimization problem:</p><formula xml:id="formula_31">min W∈H {F(W) = f (W) + g(W)} (<label>17</label></formula><formula xml:id="formula_32">)</formula><p>where H is a real Hilbert space, f (W) is convex and smooth, and g(W) is convex and typically nonsmooth. f (W) is further Lipschitz continuous, i.e., ∇f (W 1 ) -∇f (W 2 ) ≤ L f W , where W = W 1 -W 2 , and L f is the Lipschitz constant. For <ref type="bibr" target="#b15">(16)</ref>, f (W) and g(W) can be defined as</p><formula xml:id="formula_33">f (W) = 1 2 Tr (XW -Y) T D(XW -Y) + α 2 Tr WRW T + β 2 l k=1 w k T S k w k (18) g(W) = γ W 1 . (<label>19</label></formula><formula xml:id="formula_34">)</formula><p>Thus, we can obtain the derivative of f (W) with respect to W as</p><formula xml:id="formula_35">∇f (W) = X T DXW -X T DY + αWR + A W (20)</formula><p>where represents the Hadamard product.</p><formula xml:id="formula_36">A = [a 1 , a 2 , . . . , a l ] ∈ R m×l , each element a k = β(s + k + r k s - k -|P k |s k ). Then, we have ∇f (W 1 ) -∇f (W 2 ) 2 F = X T DX W + α WR + A W 2 F ≤ 3 X T DX W 2 F + 3 α WR 2 F + 3 A W 2 F ≤ 3 X T DX 2 2 W 2 F + 3 αR 2 2 W 2 F + 3max a 2 ij i,j W 2 F = 3 X T DX 2 2 + αR 2 2 + max a 2 ij i,j W 2 F .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: JFSC for Multilabel Learning</head><p>Input: Training data matrix X ∈ R n×m , label matrix Y ∈ R n×l , and weighting parameters α, β, γ , η;  </p><formula xml:id="formula_37">Output: Model coefficient matrix W * ∈ R m×l . 1 Initialization: b 0 , b 1 ← 1; t ← 1; W 0 , W 1 ← (X T X + ηI) -1 X T Y; 2 compute</formula><formula xml:id="formula_38">W (t) ← W t + b t-1 -1 b t (W t -W t-1 ); 8 G (t) ← W (t) -1 L f ∇f (W (t) ); 9 W t ← prox (γ /L f ) (G (t) ); b t ← 1+ √ 4b 2 t +1 2 ; t ← t + 1; until stop criterion reached; W * ← W t ; Here A W 2 F = i,j a 2 ij w 2 ij ≤ i,j max{a 2 ij } i,j w 2 ij = max{a 2 ij } i,j i,j w 2 ij = max{a 2 ij } i,j W 2 F . Thus, A W 2 F ≤ max{a 2 ij } i,j W 2 F .</formula><p>Therefore, the Lipschitz constant can be calculated by</p><formula xml:id="formula_39">L f = 3 X T DX 2 2 + αR 2 2 + max a 2 ij i,j . (<label>21</label></formula><formula xml:id="formula_40">)</formula><p>In the accelerated proximal gradient method, the accelerated proximal gradient iterates as follows:</p><formula xml:id="formula_41">W (t) = W t + b t-1 -1 b t (W t -W t-1 ) (<label>22</label></formula><formula xml:id="formula_42">)</formula><formula xml:id="formula_43">W t = prox W (t) - 1 L f ∇f W (t) . (<label>23</label></formula><formula xml:id="formula_44">)</formula><p>In <ref type="bibr" target="#b61">[62]</ref>, the work has shown that setting</p><formula xml:id="formula_45">W (t) = W t +(b t-1 - 1/b t )(W t -W t-1 ) for a sequence b t satisfying b 2 t -b t ≤ b 2 t-1</formula><p>can improve the convergence rate to O(1/t<ref type="foot" target="#foot_1">2</ref> ), where W t is the result of W at the tth iteration. is the step size, and set to be γ /L f in this paper. The proximal operator associated with the 1 -norm is the soft-thresholding operator</p><formula xml:id="formula_46">prox (w) = (|w| -) + sign(w). (<label>24</label></formula><formula xml:id="formula_47">)</formula><p>Consequently, all the optimization steps of our proposed method can be summarized in Algorithm 1. After learning W, we can obtain the prediction for a test data X t by sign(P t -τ ) with the given threshold τ , where P t = X t W and τ is set to be 0.5 in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Complexity Analysis</head><p>For simplicity, we mainly analyze the complexity of the optimization parts listed in Algorithm 1. Note that X ∈ R n×m , Y ∈ R n×l , R ∈ R l×l , A ∈ R m×l , and D ∈ R n×n , where n is the number of instances, m is the dimensionality, and l is the number of labels. For the initialization of W 1 , the calculation consists of some operations of matrix multiplications and inversion. This leads to a complexity of O(nm 2 + m 3 + nml + m 2 l). To calculate matrix A, we should calculate the mean vectors, inner-class and intraclass distances for each label, and it needs O(nml). The correlation among labels is calculated by cosine similarity, and it needs O(nl 2 ). The most time-consuming components are steps 5, 6, and 8 in the iterations. It leads to a complexity of O(nml) for calculating matrix D in step 5. Calculating L f in step 6 refers to matrix multiplications and singular value decomposition. Although D is an n × n matrix, it is diagonal and only needs a memory of O(n). Thus, calculating X T DX only needs O(nm + nm 2 ), and the complexity in step 6 is O(nm 2 + nm + ml + m 3 + l 3 ). In step 8, it needs to calculate the gradient of f (W), which leads to a complexity of O(nm 2 + m 2 l + ml 2 + nml).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Configuration</head><p>We compare our proposed method JFSC with the following state-of-the-art multilabel classification and feature selection methods. The search range and configuration for the parameters of each comparing algorithm are suggested by their original published paper.</p><p>1) JFSC (Proposed Method): Parameters α, β, and γ are searched in {4 -5 , 4 -4 , . . . , 4 5 }, and η is searched in {0.1, 1, 10}. The threshold τ = 0.5, and the Lipschitz constant L f is calculated according to <ref type="bibr" target="#b20">(21)</ref>. 2) BR <ref type="bibr" target="#b2">[3]</ref>: It decomposes the multilabel classification problem into l independent binary (one-versus-rest) classification subproblems. 3) Ensemble CCs (ECC) <ref type="bibr" target="#b24">[25]</ref>: It is an ensemble version of CC, where the ensemble size m is set to be 10. The chain order for each CC is generated randomly. 4) MLkNN<ref type="foot" target="#foot_0">1</ref>  <ref type="bibr" target="#b16">[17]</ref>: A lazy learning approach to multilabel learning. The parameter k is searched in {3, 5, . . . , 21}. 5) LLSF 2 <ref type="bibr" target="#b21">[22]</ref>: Parameters α and β are searched in {2 -10 , 2 -9 , . . . , 2 10 }, ρ is searched in {0.1, 1, 10}, and τ = 0.5. 6) Lasso <ref type="bibr" target="#b36">[37]</ref>: Least squared loss with 1 -norm regularization. It learns sparse label-specific features without  <ref type="bibr" target="#b37">[38]</ref>: Efficient and RFS via joint 2,1 -norms minimization. The regularization parameter γ is tuned in {10 -5 , 2 -4 , . . . , 10 1 }. 8) MIFS <ref type="bibr" target="#b50">[51]</ref>: The regularization parameters α, β and γ are tuned in {10 -4 , 10 -3 , 10 -2 , 0.1, 0.2, 0.4, 0.6, 0.8, 1, 10}. 9) SFUS 4 <ref type="bibr" target="#b48">[49]</ref>: SFUS incorporates joint sparse feature selection with multilabel learning to uncover shared feature subspace. The regularization parameters α and β are tuned in {10 -3 , 2 -2 , . . . , 10 3 }. 10) F-Score <ref type="bibr" target="#b7">[8]</ref>: Fisher score for feature selection. 3 source code: http://www.escience.cn/system/file?fileId=67410. 4 source code: http://www.escience.cn/system/file?fileId=67613.</p><p>LIBSVM <ref type="bibr" target="#b62">[63]</ref> is utilized as the base binary learner for each binary classifier of BR and ECC, where the kernel function is set as linear kernel, and the parameter C is tuned in {10 -4 , 10 -3 , . . . , 10 4 }. The experiments are conducted on 16 multilabel benchmark data sets, the details of which are summarized in Table <ref type="table">I</ref>. "Card" indicates the average number of labels per example of a data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Metrics</head><p>Given a testing data set D t = {(x i , y i )} n t i=1 , where y i ∈ {0, 1} l is the ground truth labels of the ith test example, and ŷi = h(x i ) is its predicted labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Example-Based Evaluation Metrics:</head><p>We use two types of evaluation metrics, i.e., example-based and label-based <ref type="bibr" target="#b9">[10]</ref>. They can evaluate the performance of multilabel learning algorithms from various aspects. y ij = ŷij .</p><p>2) Accuracy evaluates Jaccard similarity between the predicted labels and the ground truth labels</p><formula xml:id="formula_48">Accuracy = 1 n t n t i=1 |y i ∧ ŷi | |y i ∨ ŷi |<label>.</label></formula><p>3) Exact-Match evaluates how many times the prediction and the ground truth are exactly matched</p><formula xml:id="formula_49">Exact-Match = 1 n t n t i=1 y i = ŷi .</formula><p>4) F 1 is the harmonic mean of recall and precision. p i and r i are the precision and recall for the ith example</p><formula xml:id="formula_50">F 1 = 1 n t n t i=1 2p i r i p i + r i .</formula><p>2) Label-Based Evaluation Metrics: The label-based evaluation metrics are defined as</p><formula xml:id="formula_51">Macro B(h) = 1 l l q=1</formula><p>B TP q , FP q , TN q , FN q Micro</p><formula xml:id="formula_52">B(h) = B ⎛ ⎝ l q=1 TP q , l q=1 FP q , l q=1 TN q , l q=1 FN q ⎞ ⎠</formula><p>where TP q , FP q , TN q , and FN q represent the number of true positive, false positive, true negative, and false negative test examples, respectively, with respect to label y q , and B(TP q , FP q , TN q , FN q ) indicates some specific binary classification metrics (e.g., F 1 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Application to Multilabel Classification</head><p>In this section, we compare JFSC with BR <ref type="bibr" target="#b2">[3]</ref>, ECC <ref type="bibr" target="#b24">[25]</ref>, MLkNN <ref type="bibr" target="#b16">[17]</ref>, LLSF <ref type="bibr" target="#b21">[22]</ref>, Lasso <ref type="bibr" target="#b36">[37]</ref>, and RFS <ref type="bibr" target="#b37">[38]</ref>. JFSC, LLSF, Lasso, and RFS can be utilized for multilabel classification and feature selection. For multilabel classification, the embedded linear classifiers (i.e., linear regression) of these approaches are employed in classification.</p><p>For each comparing algorithm, fivefold cross-validation is performed on the training data of each data set. Tables II and III report the average results (mean±std) of each comparing algorithm over 16 data sets in terms of each evaluation metric. To analyze the relative performance among the comparing algorithms systematically, Friedman test <ref type="bibr" target="#b63">[64]</ref> is employed to conduct performance analysis. Table <ref type="table" target="#tab_5">IV</ref> summarizes the Friedman statistics F F and the corresponding critical value in terms of each evaluation metric. As shown in Table <ref type="table" target="#tab_5">IV</ref>, at significance level α = 0.05, the null hypothesis that all the comparing algorithms perform equivalently is clearly rejected in terms of each evaluation metric. Consequently, we can proceed with a post-hoc test <ref type="bibr" target="#b63">[64]</ref> to analyze the relative performance among the comparing algorithms.</p><p>The Nemenyi test <ref type="bibr" target="#b63">[64]</ref> is employed to test whether our proposed method achieves a competitive performance against the comparing algorithms, where JFSC is considered as the control algorithm. The performance between two classifiers will be significantly different if their average ranks differ by at least one critical difference CD = q α √ (k(k + 1)/6N). For Nemenyi test, q α = 2.948 at significance level α = 0.05, and thus CD = 2.2516 (k = 7, N = 16). Fig. <ref type="figure" target="#fig_4">2</ref> shows the CD diagrams on each evaluation metric. In each subfigure, any comparing algorithm whose average rank is within one CD to  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Application to Multilabel Feature Selection</head><p>In this section, we employ BR with LIBSVM <ref type="bibr" target="#b62">[63]</ref> as the multilabel classifier to evaluate the performance of JFSC and other feature selection approaches on multilabel feature selection.</p><p>We compare JFSC with F-Score <ref type="bibr" target="#b7">[8]</ref>, Lasso <ref type="bibr" target="#b36">[37]</ref>, LLSF <ref type="bibr" target="#b21">[22]</ref>, RFS <ref type="bibr" target="#b37">[38]</ref>, MIFS <ref type="bibr" target="#b50">[51]</ref>, and SFUS <ref type="bibr" target="#b48">[49]</ref>. Parameters for each comparing algorithm are searched by fivefold cross-validation on the training data according the performance of themselves on multilabel classification (i.e., the linear classifiers). Features are selected according to the absolute weight of the coefficient (or score) matrix of these approaches. Given a percentage of features to be selected, for each label, a lowdimensional data representation, composed of the features with top weights, is taken as the input data for the corresponding binary classifier of BR. LIBSVM with linear kernel is initialized as the one-versus-rest binary classifier for each label, and the parameter C is tuned in {10 -4 , 10 -3 , . . . , 10 4 }.</p><p>The experiments are conducted on six data sets, i.e., cal500, genbase, medical, language log, rcv1subset1, and bibtex. For each data set, we randomly split it into training (80%) and testing (20%) parts, and the number of selected features in varied in top {2%, 4%, . . . , 40%} of the total number of features. The average result of five times repetitions of each comparing algorithm on different data sets are reported in Fig. <ref type="figure">3</ref>. "AllFea" means that the original data with no feature selection is used as a baseline, and its performance is equal to BRs. As the number of selected features is varied from top 2% to 40% with a step of 2%, there are 21 points totally. Table V summarizes the overall pairwise comparison results of JFSC with other compared feature selection approaches over the six data sets in terms of each evaluation metric. Each cell is composed of three numbers: from left to right, how many times that JFSC achieves a better/tied/worse performance than the compared approach at different percentages of selected features with a given evaluation metric. For example, in the third row and the second column, "113 0 13" means that JFSC wins AllFea 113 times, ties 0 time, and loses 13 times over the six data sets (21 × 6 = 126 points totally) in terms of Accuracy. The sign test <ref type="bibr" target="#b63">[64]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Parameter Sensitivity Analysis</head><p>To conduct parameter sensitivity analysis of JFSC, we first find a group of best configurations for the parameters by crossvalidation on the training data of the rcv1(subset1) data set, and then fix the value of one parameter and vary the values of the other two parameters.</p><p>We randomly split the data set into training (80%) and testing (20%) parts five times, the average results of JFSC over the five repetitions with different values of α, β, and γ are depicted from Fig. <ref type="figure" target="#fig_5">4</ref>(a)-(o). We can observe that the highest performance is achieved at some intermediate values of α, β, and γ . The experiments show that the performance of JFSC is sensitive to the values of the regularization parameters. However, large candidate sets for α, β, and γ can be employed in practice to obtain satisfactory performance. With a fixed setting of α, β, and γ , we evaluate the influence of η to the performance of JFSC over five data sets. The experimental results are shown from Fig. <ref type="figure" target="#fig_5">4</ref>(p)-(t). The performance of JFSC is improved slightly and then declines with the increase of η, and the best results are obtained at η = 1 in most cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Discussion</head><p>From the experimental results shown in Fig. <ref type="figure" target="#fig_5">4</ref>, the performance of JFSC is sensitive to the parameter configuration. An appropriate configuration of parameters could be obtained by cross-validation on the training data. For large-scale data sets, it will cost significant amount of time.</p><p>According the time complexity analysis (see Section IV-B), we find that the complexities of the initialization of W 1 (step 1) and the calculation of the Lipschitz constant L f (step 6) are cubic with respect to the number of features m and the number of labels l. This would indeed make our proposed approach less scalable to multilabel data sets with large numbers of features. Actually, if the initialization of W 1 is solved by the gradient descent algorithm, then its complexity will decrease to quadratic. Moreover, an appropriate L ≥ L f can be searched by a backtracking stepsize rule <ref type="bibr" target="#b61">[62]</ref> and used instead of L f . Then the complexity of step 6 will also decrease to quadratic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we proposed an unified framework which can perform JFSC for multilabel learning. We proposed to learn label-specific features and shared features for the discrimination of each class label by exploiting pairwise label correlations, and then build a multilabel classifier on the lowdimensional data representations composed of these learned features. The experiments verified the usefulness of exploiting label correlation and learning label-specific data representation for multilabel learning. A comparative study with state-of-theart approaches manifested a competitive performance of our proposed method both in classification and feature selection for multilabel learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Image annotation: sky, ship, and sea water.</figDesc><graphic coords="2,118.99,53.40,110.64,94.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>used to account for potential class imbalance between positive and negative examples. λ is the tradeoff parameter between inner-class distances and intraclass distances, and is set to be |P k | to allow S k 2 to be the same order of magnitude with S + k 2 and r k S - k 2 in this paper. After some mathematical operations, (13) can be rewritten as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>repeat 5 compute</head><label>5</label><figDesc>the matrix A in Eq. (20); 3 compute R by cosine similarity on Y; 4 the diagonal matrix D according to Eq. (4); 6 compute L f according to Eq. (21);</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>7</head><label>7</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Comparison of JFSC (control algorithm) against other comparing algorithms with the Nemenyi test. Groups of classifiers that are not significantly different from JFSC (at α = 0.05) are connected. (a) Hamming loss. (b) Accuracy. (c) Exact-match. (d) F 1 . (e) Macro F 1 . (f) Micro F 1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Parameter sensitivity analysis of JFSC. (a) Accuracy of JFSC with fixed γ . (b) Exact-match of JFSC with fixed γ . (c) F 1 of JFSC with fixed γ . (d) Macro F 1 of JFSC with fixed γ . (e) Micro F 1 of JFSC with fixed γ . (f) Accuracy of JFSC with fixed β. (g) Exact-match of JFSC with fixed β. (h) F 1 of JFSC with fixed β. (i) Macro F 1 of JFSC with fixed β. (j) Micro F 1 of JFSC with fixed β. (k) Accuracy of JFSC with fixed α. (l) Exact-match of JFSC with fixed α. (m) F 1 of JFSC with fixed α. (n) Macro F 1 of JFSC with fixed α. (o) Micro F 1 of JFSC with fixed α. (p) Accuracy of JFSC with fixed α, β and γ . (q) Exact-match of JFSC with fixed α, β and γ . (r) F 1 of JFSC with fixed α, β and γ . (s) Macro F 1 of JFSC with fixed α, β and γ . (t) Micro F 1 of JFSC with fixed α, β and γ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>x i2 w 2k , . . . , x im w mk ] denotes an elementwise product. x+ k and xk are the mean vectors of positive and negative sets in the original feature space. x+</figDesc><table /><note><p>k = [x + k1 , x+ k2 , . . . , x+ km ], and</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II EXPERIMENTAL</head><label>II</label><figDesc>RESULTS OF EACH COMPARING ALGORITHM ON REGULAR-SCALE DATA SETS IN TERMS OF EACH EVALUATION METRIC. ↑ (↓) INDICATES THE LARGER (SMALLER) THE VALUE, THE BETTER THE PERFORMANCE. BEST RESULTS ARE HIGHLIGHTED IN BOLD considering label correlations. The regularizer parameter is searched in {2 -10 , 2 -9 , . . . , 2 10 }. 7) RFS 3</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III EXPERIMENTAL</head><label>III</label><figDesc>RESULTS OF EACH COMPARING ALGORITHM ON RELATIVE LARGE-SCALE DATA SETS IN TERMS OF EACH EVALUATION METRIC. ↑ (↓) INDICATES THE LARGER (SMALLER) THE VALUE, THE BETTER THE PERFORMANCE. BEST RESULTS ARE HIGHLIGHTED IN BOLD 1) Hamming loss evaluates how many times an examplelabel pair is misclassified. x is an indication function, it returns 1 if x holds and 0, otherwise. The smaller the value of Hamming loss, the better performance of the classifier</figDesc><table><row><cell>Hamming loss =</cell><cell>1 n t</cell><cell>n t i=1</cell><cell>1 l</cell><cell>l j=1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>. 3. Results of feature selection: BR with LIBSVM is employed as the multilabel classifier. cal500: (a) Accuracy (b) F 1 , (c) Exact-match, (d) Macro F 1 , and (e) Micro F 1 . medical: (f) Accuracy, (g) F 1 , (h) Exact-match, (i) Macro F 1 , and (j) Micro F 1 . language log: (k) Accuracy, (l) F 1 , (m) Exact-match, (n) Macro F 1 , and (o) Micro F 1 . genbase: (p) Accuracy, (q) F 1 , (r) Exact-match, (s) Macro F 1 , and (t) Micro F 1 . rcv1(subset1): (u) Accuracy, (v) F 1 , (w) Exact-match, (x) Macro F 1 , and (y) Micro F 1 . bibtex: (z) Accuracy, (aa) F 1 , (ab) Exact-match, (ac) Macro F 1 , and (ad) Micro F 1 .</figDesc><table><row><cell>TABLE IV</cell></row><row><cell>SUMMARY OF THE FRIEDMAN STATISTICS F F (k = 7, N = 16) AND THE</cell></row><row><cell>CRITICAL VALUE IN TERMS OF EACH EVALUATION METRIC (k: #</cell></row><row><cell>COMPARING ALGORITHMS; N: # DATA SETS)</cell></row><row><cell>that of JFSC is connected. Otherwise, any algorithm not con-</cell></row><row><cell>nected with JFSC is considered to have significantly different</cell></row><row><cell>performance between them.</cell></row><row><cell>Based on these experimental results, the following observa-</cell></row><row><cell>tions can be made.</cell></row><row><cell>1) All the first-order approaches (i.e., BR, Lasso, RFS, and</cell></row><row><cell>MLkNN) achieve better performance on Hamming loss</cell></row><row><cell>[see Fig. 2(a)] than second-order approach (i.e., JFSC</cell></row><row><cell>and LLSF) and high-order approaches (i.e., ECC), as</cell></row><row><cell>first-order approaches try to optimize Hamming loss.</cell></row><row><cell>JFSC achieves better performance than LLSF and ECC</cell></row><row><cell>in terms of Hamming loss.</cell></row><row><cell>2) All the second-order approaches (i.e., JFSC and LLSF)</cell></row><row><cell>and high-order approach (i.e., ECC) achieve bet-</cell></row><row><cell>ter performance on Exact-Match [see Fig. 2(c)] than</cell></row><row><cell>the first-order approaches (i.e., BR, Lasso, RFS, and</cell></row><row><cell>MLkNN). As previous works suggest that optimizing</cell></row><row><cell>Exact-Match need to model label correlations. JFSC</cell></row><row><cell>achieves better performance than LLSF in terms of</cell></row><row><cell>Exact-Match.</cell></row><row><cell>3) JFSC significantly outperforms Lasso, BR, MLkNN,</cell></row><row><cell>and RFS in terms of other four evaluation metrics.</cell></row><row><cell>Lasso can be regarded as a plain version of JFSC by</cell></row><row><cell>learning label-specific features without considering label</cell></row><row><cell>correlation and innerclass and intraclass distances for</cell></row><row><cell>each label. The super performance of JFSC against</cell></row><row><cell>these approaches indicates the effectiveness of learning</cell></row><row><cell>label-specific features and exploiting label correlation.</cell></row><row><cell>4) Furthermore, JFSC performs worse than ECC in</cell></row><row><cell>terms of Exact-Match, and obtains statistically supe-</cell></row><row><cell>rior performance against ECC in terms of the other five</cell></row><row><cell>evaluation metrics.</cell></row></table><note><p>Fig5) JFSC achieves statistically superior performance against LLSF in terms of each evaluation metric. The better performance of JFSC against LLSF indicates the effectiveness of utilizing a robust loss function and modeling innerclass and intraclass distances.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V RESULTS</head><label>V</label><figDesc>OF PAIRWISE COMPARISON APPLIED TO JFSC WITH COMPARING ALGORITHMS</figDesc><table><row><cell>To summarize, JFSC achieves a competitive performance</cell></row><row><cell>against other well-established multilabel classification</cell></row><row><cell>approaches.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>is employed to test whether JFSC achieves a competitive performance against the other comparing algorithms. If the number of wins is at least N/2 + 1.96 √ N/2, the algorithm is significantly better with significance level α &lt; 0.05, where</figDesc><table><row><cell>N = 126. According to the experimental results, we can make</cell></row><row><cell>the following observations.</cell></row><row><cell>1) These feature selection methods generally perform better</cell></row><row><cell>than AllFea which does not conduct feature selec-</cell></row><row><cell>tion. This observation indicates that feature selection</cell></row><row><cell>contributes to improvement of multilabel classification</cell></row><row><cell>performance.</cell></row><row><cell>2) JFSC and LLSF generally obtain better performance</cell></row><row><cell>than other feature selection methods. The advantage</cell></row><row><cell>indicates the effectiveness of learning label-specific fea-</cell></row><row><cell>tures for each class label in multilabel learning.</cell></row><row><cell>3) JFSC consistently outperforms the other feature selec-</cell></row><row><cell>tion methods in terms of each evaluation metric.</cell></row><row><cell>The advantage indicates the effectiveness of our</cell></row><row><cell>proposed method in feature selection for multilabel</cell></row><row><cell>learning.</cell></row><row><cell>To summarize, JFSC achieves a competitive performance</cell></row><row><cell>against other feature selection approaches.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>source code: http://cse.seu.edu.cn/PersonalPage/zhangml/index.htm.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>source code: http://www.escience.cn/people/huangjun/index.html.</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Natural Science Foundation of China under Grant 61332016, Grant 61620106009, Grant U1636214, and Grant 61650202, in part by the National Basic Research Program of China (973 Program) under Grant 2015CB351800, in part by the Key Research Program of Frontier Sciences, Chinese Academy of Sciences under Grant QYZDJ-SSW-SYS013, in part by the Program for Changjiang Scholars and Innovative Research Team in University of the Ministry of Education, China under Grant IRT13059, and in part by the U.S. National Science Foundation under Grant 1652107. This paper was recommended by Associate Editor S. Ventura.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-label text classification with a mixture model trained by EM</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Workshop Text Learn</title>
		<meeting>AAAI Workshop Text Learn<address><addrLine>Orlando, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multilabel neural networks with applications to functional genomics and text categorization</title>
		<author>
			<persName><forename type="first">M.-L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1338" to="1351" />
			<date type="published" when="2006-10">Oct. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning multi-label scene classification</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Boutell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-P</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1757" to="1771" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weakly supervised multilabel clustering and its applications in computer vision</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3220" to="3232" />
			<date type="published" when="2016-12">Dec. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image annotation by multiple-instance learning with discriminative feature mapping and selection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="669" to="680" />
			<date type="published" when="2014-05">May 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Correlated label propagation with application to multi-label learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.</title>
		<imprint>
			<biblScope unit="page" from="1719" to="1726" />
			<date type="published" when="2006">2006</date>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Correlative multi-label video annotation</title>
		<author>
			<persName><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Multimedia</title>
		<meeting>ACM Multimedia<address><addrLine>Augsburg, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="17" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
		<title level="m">Pattern Classification</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mining multi-label data</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Katakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Vlahavas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining and Knowledge Discovery Handbook</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="667" to="685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A review on multi-label learning algorithms</title>
		<author>
			<persName><forename type="first">M.-L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1819" to="1837" />
			<date type="published" when="2014-08">Aug. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A tutorial on multilabel learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Gibaja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ventura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surveys</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Charte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Rivera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Del</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesus</forename></persName>
		</author>
		<title level="m">Multilabel Classification: Problem Analysis, Metrics and Techniques</title>
		<meeting><address><addrLine>Cham, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-label learning: A review of the state of the art and ongoing research</title>
		<author>
			<persName><forename type="first">E</forename><surname>Gibaja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ventura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rev. Data Min. Knowl. Disc</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="411" to="444" />
			<date type="published" when="2014">2014</date>
			<publisher>Wiley Interdisc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Random k-labelsets: An ensemble method for multilabel classification</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Vlahavas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Mach. Learn</title>
		<meeting>Eur. Conf. Mach. Learn<address><addrLine>Warsaw, Poland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="406" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-label classification using ensembles of pruned sets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Read</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Data Min</title>
		<meeting>IEEE Int. Conf. Data Min<address><addrLine>Pisa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="995" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Knowledge discovery in multi-label phenotype data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Clare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Data Min. Knowl. Disc., Freiburg im Breisgau</title>
		<meeting>Data Min. Knowl. Disc., Freiburg im Breisgau<address><addrLine>Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="42" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ML-KNN: A lazy learning approach to multi-label learning</title>
		<author>
			<persName><forename type="first">M.-L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2038" to="2048" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A kernel method for multi-labelled classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Elisseeff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jason</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Inf</title>
		<meeting>Neural Inf<address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="681" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Lift: Multi-label learning with label-specific features</title>
		<author>
			<persName><forename type="first">M.-L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="120" />
			<date type="published" when="2015-01">Jan. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning instance correlation functions for multilabel classification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="499" to="510" />
			<date type="published" when="2017-02">Feb. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multilabel classification via calibrated label ranking</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fürnkranz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hüllermeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Mencía</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Brinker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="133" to="153" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning label specific features for multi-label classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q.-M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-D</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Data Min</title>
		<meeting>IEEE Int. Conf. Data Min<address><addrLine>Atlantic City, NJ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="181" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Teaching-to-learn and learningto-teach for multi-label propagation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf</title>
		<meeting>AAAI Conf<address><addrLine>Phoenix, AZ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1610" to="1616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Leveraging implicit relative labeling-importance information for effective multi-label learning</title>
		<author>
			<persName><forename type="first">Y.-K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Data Min</title>
		<meeting>IEEE Int. Conf. Data Min<address><addrLine>Atlantic City, NJ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="251" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Classifier chains for multi-label classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Read</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Mach. Learn</title>
		<meeting>Eur. Conf. Mach. Learn<address><addrLine>Bled, Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="254" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bayes-optimal hierarchical multilabel classification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2907" to="2918" />
			<date type="published" when="2015-11">Nov. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bayes optimal multilabel classification via probabilistic classifier chains</title>
		<author>
			<persName><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hüllermeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Dembczyński</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn<address><addrLine>Haifa, Israel</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="279" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bayesian chain classifiers for multidimensional classification</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Zaragoza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Sucar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bielza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Larrañaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf</title>
		<meeting>Int. Joint Conf<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2192" to="2197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Beam search algorithms for multilabel learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vembu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="89" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient Monte Carlo methods for multi-dimensional learning with classifier chains</title>
		<author>
			<persName><forename type="first">J</forename><surname>Read</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luengo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1535" to="1546" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Context-aware MIML instance annotation: Exploiting label correlations with classifier chains</title>
		<author>
			<persName><forename type="first">F</forename><surname>Briggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Z</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="79" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning labelspecific features and class-dependent labels for multi-label classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q.-M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-D</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3309" to="3323" />
			<date type="published" when="2016-12">Dec. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An introduction to variable and feature selection</title>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elisseeff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1157" to="1182" />
			<date type="published" when="2003-03">Mar. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Feature Selection for Knowledge Discovery and Data Mining</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Motoda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Springer</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Estimating attributes: Analysis and extensions of relief</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kononenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Mach. Learn</title>
		<meeting>Eur. Conf. Mach. Learn<address><addrLine>Catania, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="171" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Feature selection through message passing</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint/>
	</monogr>
	<note>to be published</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Stat. Soc. B</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Efficient and robust feature selection via joint 2,1 -norms minimization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Inf</title>
		<meeting>Neural Inf<address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1813" to="1821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Categorizing feature selection methods for multi-label classification</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Plastino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zadrozny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H C</forename><surname>Merschmann</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10462-016-9516-4</idno>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Rev</title>
		<imprint>
			<date type="published" when="2016-09">Sep. 2016</date>
		</imprint>
	</monogr>
	<note>to be published</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multi-label reliefF and Fstatistic feature selections for image annotation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit</title>
		<imprint>
			<biblScope unit="page" from="2352" to="2359" />
			<date type="published" when="2012">2012</date>
			<pubPlace>Providence, RI, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Soft-constrained Laplacian score for semi-supervised multi-label feature selection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Alalga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Benabdeslem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Taleb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="75" to="98" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">gMLC: A multi-label feature selection framework for graph classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="281" to="305" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Low-rank approximation for multi-label feature selection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-W</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Mach. Learn. Comput</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="42" to="46" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Ensemble constrained Laplacian score for efficient and robust semi-supervised feature selection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Benabdeslem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Elghazel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hindawi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1161" to="1185" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Graph-margin based multi-label feature selection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Mach. Learn</title>
		<meeting>Eur. Conf. Mach. Learn<address><addrLine>Riva del Garda, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="540" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multi-view ensemble learning: An optimal feature set partitioning for high-dimensional data classification</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Minz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="59" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Effective and efficient multi-label feature selection approaches via modifying Hilbert-Schmidt independence criterion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Neural Inf. Process</title>
		<meeting>Int. Conf. Neural Inf. ess<address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="385" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Feature selection for multi-label naive Bayes classification</title>
		<author>
			<persName><forename type="first">M.-L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Peña</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Robles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">179</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="3218" to="3229" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Web image annotation via subspace-sparsity collaborated feature selection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1021" to="1030" />
			<date type="published" when="2012-08">Aug. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multi-graph-view subgraph mining for graph classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="54" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multi-label informed feature selection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf</title>
		<meeting>Int. Joint Conf<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1627" to="1633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Block-row sparse multiview multilabel learning for image classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="450" to="461" />
			<date type="published" when="2016-02">Feb. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A convex formulation for semi-supervised multi-label feature selection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf</title>
		<meeting>AAAI Conf<address><addrLine>Quebec City, QC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1171" to="1177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Multilabel dimensionality reduction via dependence maximization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Knowl. Disc. Data</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A novel attribute reduction approach for multi-label data based on rough set theory</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Info. Sci</title>
		<imprint>
			<biblScope unit="page" from="827" to="847" />
			<date type="published" when="2016-11">Nov. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A multi-label feature extraction algorithm via maximizing feature variance and feature-label dependence simultaneously</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Based Syst</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="172" to="184" />
			<date type="published" when="2016-04">Apr. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Multi-label learning with label-specific feature reduction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Based Syst</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="52" to="61" />
			<date type="published" when="2016-07">Jul. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A dirty model for multi-task learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jalali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sanghavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Ravikumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Inf</title>
		<meeting>Neural Inf<address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="964" to="972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A multivariate regression approach to association analysis of a quantitative trait network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="204" to="212" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Task sensitive feature exploration and learning for multitask graph classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint/>
	</monogr>
	<note>to be published</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A multiobjective genetic programming-based ensemble for simultaneous feature selection and classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="499" to="510" />
			<date type="published" when="2016-02">Feb. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A fast iterative shrinkage-thresholding algorithm for linear inverse problems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Teboulle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Imag. Sci</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="183" to="202" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Statistical comparisons of classifiers over multiple data sets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Demšar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2006-01">Jan. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
