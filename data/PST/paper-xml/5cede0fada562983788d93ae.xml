<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TENSORFLOW EAGER: A MULTI-STAGE, PYTHON-EMBEDDED DSL FOR MACHINE LEARNING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-02-27">27 Feb 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Akshay</forename><surname>Agrawal</surname></persName>
							<email>&lt;akshayka@cs.stanford.edu&gt;</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Brain</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Akshay</forename><forename type="middle">Naresh</forename><surname>Modi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Brain</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Brain</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Allen</forename><surname>Lavoie</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Brain</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Brain</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Asim</forename><surname>Shankar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Brain</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Igor</forename><surname>Ganichev</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Brain</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Brain</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mingsheng</forename><surname>Hong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Brain</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rajat</forename><surname>Monga</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Brain</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shanqing</forename><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Brain</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TENSORFLOW EAGER: A MULTI-STAGE, PYTHON-EMBEDDED DSL FOR MACHINE LEARNING</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-02-27">27 Feb 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1903.01855v1[cs.PL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>TensorFlow Eager is a multi-stage, Python-embedded domain-specific language for hardware-accelerated machine learning, suitable for both interactive research and production. TensorFlow, which TensorFlow Eager extends, requires users to represent computations as dataflow graphs; this permits compiler optimizations and simplifies deployment but hinders rapid prototyping and run-time dynamism. TensorFlow Eager eliminates these usability costs without sacrificing the benefits furnished by graphs: It provides an imperative front-end to TensorFlow that executes operations immediately and a JIT tracer that translates Python functions composed of TensorFlow operations into executable dataflow graphs. TensorFlow Eager thus offers a multi-stage programming model that makes it easy to interpolate between imperative and staged execution in a single package.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Many contemporary libraries for machine learning share a similar structure: they provide suites of primitive operations and functions to automatically differentiate compositions thereof (see, e.g., <ref type="bibr" target="#b2">Bergstra et al., 2010;</ref><ref type="bibr" target="#b32">Tokui et al., 2015;</ref><ref type="bibr" target="#b19">Maclaurin et al., 2015;</ref><ref type="bibr" target="#b4">Chen et al., 2015;</ref><ref type="bibr" target="#b0">Abadi et al., 2016;</ref><ref type="bibr" target="#b23">Paszke et al., 2017;</ref><ref type="bibr">The Gluon Team, 2017;</ref><ref type="bibr" target="#b21">Neubig et al., 2017;</ref><ref type="bibr" target="#b10">Innes, 2018;</ref><ref type="bibr">Frostig et al., 2018)</ref>. These software packages in fact more closely resemble domain-specific languages (DSLs) than libraries <ref type="bibr" target="#b12">(Innes et al., 2018)</ref>. Indeed, models written using automatic differentiation software are often referred to as differentiable programs.</p><p>DSLs for differentiable programming are usually embedded in a host language (for a reference on embedded DSLs, see <ref type="bibr" target="#b9">Hudak, 1996)</ref>, and they can be roughly classified as either imperative or declarative, in the programming languages sense. Programming in an imperative DSL for differentiable programming is like programming in an imperative programming language such as Python: the construction and execution of primitive operations are inextricably tied, with each operation returning concrete numerical data. While imperative DSLs provide a natural programming paradigm, when embedded in an interpreted language like Pythonwhich is the case for popular DSLs like Chainer <ref type="bibr" target="#b32">(Tokui et al., 2015)</ref> and PyTorch <ref type="bibr" target="#b23">(Paszke et al., 2017)</ref>-performance is bottlenecked on the interpreter and serialization of models is difficult. To address these problems, declarative DSLs separate the specification of models from their execution. These "define-before-run" libraries require users to stage their models as dataflow graphs, permitting compiler optimizations and the exploitation of parallelism, and simplifying deployment, distribution, and code generation (see, e.g., <ref type="bibr" target="#b2">Bergstra et al., 2010;</ref><ref type="bibr" target="#b0">Abadi et al., 2016)</ref>. But, because declarative DSLs prevent users from using arbitrary host-language constructs, they have steep learning curves and are not suitable for expressing models with data-dependent structures.</p><p>An ideal DSL would offer the flexibility and accessibility of imperative execution along with the many benefits of declarative programming, without either of their costs. It is with this motivation in mind that we present TensorFlow Eager, a Python-embedded DSL for differentiable programming that lets developers interpolate between imperative and staged computations in a single package. TensorFlow Eager offers a multi-stage programming model that lets users rapidly prototype programs and selectively stage parts that they wish to accelerate or serialize. It is implemented as an opt-in extension to TensorFlow, and it can be enabled by calling a single TensorFlow library function at program start-up.</p><p>To empower machine learning practitioners and researchers to be productive from the start, TensorFlow Eager executes imperatively by default. To reap the benefits of dataflow graphs, TensorFlow Eager provides a Python decorator that traces its Python function in a graph-building context, staging primitive operations to construct a dataflow graph with named inputs and outputs and returning an executable graph function. While invoking a graph function is syntactically equivalent to calling the Python function from which it was generated, the execution of graph functions bypasses Python: they are executed using a C++ dataflow runtime or are compiled to generate optimized code for CPUs, GPUs, and ASICs. Graph functions and imperative code share a lexical environment, making it simple to invoke graph functions from imperative code, create graph functions that close over imperatively constructed data, and embed imperative code in graph functions via unstaging annotations.</p><p>Our contributions are two-fold:</p><p>• Our implementation is elegant. TensorFlow Eager can be viewed as a multi-stage front-end to TensorFlow.</p><p>Imperative and staged TensorFlow Eager code share a single set of primitive operations, kernels, and uservisible APIs. Not only does this sharing result in an easy-to-maintain implementation, it also lets us present a single, coherent API surface to our users that is agnostic to execution mode and lets users enjoy the rich ecosystem of tools developed for TensorFlow.</p><p>• While we are not the first in the differentiable programming community to recognize the value in bridging imperative and declarative programming, we are among the first to present this line of work in the context of multi-stage programming. This contextualization is a contribution insofar as it clarifies discourse and connects two otherwise separate communities. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In TensorFlow Eager, users must manually stage computations, which might require refactoring code (see §4.1). An ideal framework for differentiable programming would automatically stage computations, without programmer intervention. One way to accomplish this is to embed the framework in a compiled procedural language and implement graph extraction and automatic differentiation as compiler rewrites;</p><p>this is what, e.g., DLVM, Swift for TensorFlow, and Zygote do <ref type="bibr" target="#b33">(Wei et al., 2017;</ref><ref type="bibr">Lattner &amp; the Swift for TensorFlow Team, 2018;</ref><ref type="bibr" target="#b11">Innes, 2019)</ref>. Python's flexibility makes it difficult for DSLs embedded in it to use such an approach. Some projects, like AutoGraph <ref type="bibr" target="#b20">(Moldovan et al., 2019)</ref> do operate on Python abstract syntax trees to rewrite imperative code to code that constructs dataflow graphs, but such techniques are out of the scope of this paper.</p><p>An alternative to staging computations as graphs for performance is to implement fused kernels. For example, NVIDIA provides fused CuDNN kernels for popular recurrent neural network operations that are dramatically faster than nonfused implementations <ref type="bibr" target="#b5">(Chetlur et al., 2014)</ref>. This approach, while useful, is difficult to scale, as it requires substantial programmer intervention.</p><p>TensorFlow Eager is not the first Python library to offer a multi-stage programming model. JAX <ref type="bibr">(Frostig et al., 2018)</ref>, a tracing-JIT compiler that generates code for heterogeneous devices via XLA <ref type="bibr" target="#b31">(The XLA team, 2017)</ref>, provides a similar programming paradigm; MXNet and Gluon also let users interpolate between imperative and staged computations, but at a level of abstraction that is higher than ours <ref type="bibr" target="#b4">(Chen et al., 2015;</ref><ref type="bibr">The Gluon Team, 2017)</ref>; and PyTorch is implementing a staging tracer that is similar to ours (PyTorch team, 2018). Outside of differentiable programming, Terra is a Lua-embedded DSL that supports code generation, and the paper in which it was introduced presents a thorough treatment of multi-stage programming that is more formal than ours <ref type="bibr" target="#b7">(DeVito et al., 2013)</ref>; as another example, OptiML is a Scala-embedded DSL for machine learning with support for staging and code generation but without support for automatic differentiation <ref type="bibr" target="#b28">(Sujeeth et al., 2011)</ref>. Outside of DSLs, there are several projects that provide just-in-time (JIT) compilation for Python, of which Numba <ref type="bibr" target="#b16">(Lam et al., 2015)</ref> and PyPy <ref type="bibr" target="#b3">(Bolz et al., 2009)</ref> are two examples.</p><p>Multi-stage programming is a well-studied topic in programming languages; a good reference is <ref type="bibr" target="#b29">(Taha, 2004)</ref>, and a modern design from which we drew inspiration is Scala's lightweight modular staging <ref type="bibr" target="#b24">(Rompf &amp; Odersky, 2010)</ref>. Multi-stage programming is related to staging transformations in compilers and partial evaluation in programming languages, for which <ref type="bibr" target="#b14">(Jørring &amp; Scherlis, 1986)</ref> and <ref type="bibr" target="#b13">(Jones et al., 1993)</ref> are classic references, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DESIGN PRINCIPLES</head><p>Our design strives to satisfy two goals: TensorFlow Eager should be immediately recognizable to Python programmers-for example, users should feel at home exploring APIs and prototyping models in IPython notebooksand it should also provide a smooth path to testing ideas at scale and deploying models for inference on heterogeneous devices. The first two of the following three principles are in service of the former goal, while the third is in service of the latter.</p><p>Privilege imperative execution. Because Python is an imperative language, TensorFlow Eager operates in an imperative fashion by default; staged execution is opt-in and often unnecessary (see §4.1 and §6 for details).</p><p>Seamlessly embed into Python. Whereas writing Tensor-Flow code is an exercise in metaprogramming, imperative execution lets programmers enjoy the full extent of the host language: programmers write Pythonic code, complete with familiar language constructs like native control flow (e.g., Python if statements and while loops), recursion, arbitrary data structures, and even pdb breakpoints. And, because we implement automatic differentiation via tracing ( §4.2), the programmer can differentiate through all these constructs. Host-language integration is more than just syntactic sugar-it greatly simplifies the implementation of data-dependent models like segmental recurrent neural networks and recursive neural networks <ref type="bibr" target="#b15">(Kong et al., 2015;</ref><ref type="bibr" target="#b26">Socher et al., 2011)</ref>.</p><p>Stage imperative code as dataflow graphs. To leverage the benefits of dataflow graphs, TensorFlow Eager provides a mechanism to trace Python functions and stage their operations as graph functions. The staging workflow is detailed in §4.1, and the mechanism is described in §4.6. TensorFlow graphs come with their own set of design principles, which are presented in <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref>. The following terminology will be used in the sequel: a tensor is a multi-dimensional, typed array, an operation is a primitive, possibly stateful function that takes tensors as inputs and produces tensors as outputs, a kernel is a devicespecific implementation of an operation, and a model is a composition of primitive operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXECUTION MODEL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Multi-stage programming</head><p>TensorFlow Eager provides two ways of executing operations: imperatively or as part of a static dataflow graph. Both execution models have access to the same set of operations and kernels, but they differ in how they dispatch kernels.</p><p>Imperative execution. By default, TensorFlow Eager executes operations immediately-library functions such as tf.matmul construct operations and then immediately execute their kernels. Under this regime, TensorFlow Eager resembles a NumPy-like library for hardware-accelerated numerical computation and machine learning. Calling .numpy() on a tensor fetches a NumPy array storing the tensor's data, and tensors can be supplied to external libraries like matplotlib that expect NumPy arrays (for a reference on NumPy, see <ref type="bibr" target="#b22">Oliphant, 2015)</ref>. As an example, import tensorflow as tf tf.enable_eager_execution()</p><formula xml:id="formula_0">def select(vector): A = tf.constant([[1.0, 0.0]]) return tf.matmul(A, vector) x = tf.constant([[2.0], [-2.0]]) print(select(x)) prints tf.Tensor( [[ 2.]], shape=(1, 1), dtype=float32).</formula><p>Staged execution. While imperative execution simplifies prototyping, the overhead of going back and forth into the Python interpreter limits its performance; representing computations as dataflow graphs before executing them not only removes this bottleneck but also allows for inter-op parallelism and optimizations like constant-folding and buffer reuse. Thus, TensorFlow Eager provides a mechanism to stage computations as dataflow graphs. In particular, we provide a decorator, function, that traces the execution of a Python function, recording all TensorFlow operations and the tensors flowing between them in a dataflow graph. function can be thought of as an opt-in, JIT compiler that generates an optimized polymorphic function for a Python function, creating concrete functions backed by dataflow graphs via a straightforward binding-time analysis at runtime. The analogy to compilers is imperfect because the traces generated by function only record TensorFlow operations and not arbitrary Python code, but it nonetheless provides an approximate mental model. One advantage of this tracing mechanism is that the underlying dataflow graph format does not need to support all the dynamism present in the Python code being traced; as long as the set of operations in the trace does not depend on Python state we can generate a correct trace.</p><p>Invoking a callable returned by function will execute a dataflow graph instead of the corresponding Python function. In fact, graph functions are themselves executed by an operation that takes tensors as inputs and a function name as an attribute, and these operations are automatically constructed and executed for the user. For example, if the select function defined in the previous section were decorated with @function, then select(x) would execute an operation that would in turn execute the appropriate graph function. The dataflow graph runtime, which is written in C++, automatically partitions subgraphs across devices and parallelizes operations when possible. Readers interested in the runtime should consult <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref>.</p><p>The function decorator supports code generation via XLA <ref type="bibr" target="#b31">(The XLA team, 2017)</ref>. TensorFlow Eager relies upon XLA to execute code on Tensor Processing Units (TPUs) <ref type="bibr" target="#b25">(Sato et al., 2017)</ref> (see §4.4). In addition to performance and hardware acceleration, dataflow graphs simplify distribution ( §4.5) and deployment. Details about the mechanism of function are provided in §4.6.</p><p>A multi-stage workflow. Many users will find the performance of imperative execution sufficient. Purely imperative TensorFlow Eager can match the performance of graph execution when training models with sufficiently expensive kernels, like ResNet-50 <ref type="bibr" target="#b8">(He et al., 2016)</ref> (see §6). But when imperative performance disappoints, we recommend the following multi-stage workflow, modeled after <ref type="bibr" target="#b29">(Taha, 2004)</ref>.</p><p>1. Implementation. Develop, debug, and test a singlestage imperative program.</p><p>2. Analysis. Using any profiling tool the user is familiar with, identify performance-critical blocks of operations, and express these blocks as staging-friendly Python functions or callable objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>Staging. Decorate the functions identified in the previous step with @function.</p><p>With respect to the analysis step, the key fact to keep in mind is that function is not a compiler for arbitrary Python code. Rather, it is a JIT tracer that executes Because function generates graphs by tracing and not by source code analysis, it fully unrolls Python for and while loops, potentially creating large graphs. If that is a problem, the programmer might need to replace their loops with the equivalent TensorFlow control flow constructs. Similarly, the branches of if statements that are taken during tracing are baked into the emitted graphs. Conditionals that depend on the value of tensors will need to be written using tf.cond, and while loops that depend on tensor values will need to be rewritten in terms of tf.while loops.</p><p>Python functions that depend on the values of tensors in complicated ways (e.g., via data structures that depend on the values of tensors) might prove to be prohibitively difficult to stage correctly. In such cases, users might need to refactor their functions into staging-friendly and stagingunfriendly helper functions (see the discussion on escaping staged computations in §4.7 for other options).</p><p>Note that staging trades off imperative execution (and therefore interactivity) and Python integration (and therefore run-time dynamism) for performance. It is up to the programmer to decide when this trade-off is acceptable and to use staging annotations judiciously. This trade-off can be diminished by using tools like AutoGraph that operate on abstract syntax trees and rewrite Python control flow to dataflow control flow <ref type="bibr" target="#b20">(Moldovan et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Automatic differentiation</head><p>We implement a variant of tracing-based reverse-mode automatic differentiation <ref type="bibr" target="#b1">(Baydin et al., 2018)</ref>, with a few changes to better support partially staged computation. Our implementation is similar to the implementations of Chainer <ref type="bibr" target="#b32">(Tokui et al., 2015)</ref>, Autograd <ref type="bibr" target="#b19">(Maclaurin et al., 2015)</ref>, and PyTorch <ref type="bibr" target="#b23">(Paszke et al., 2017)</ref>, but our API allows for more fine-grained control over which computations are traced.</p><p>The main user-visible concept in the gradient API is a tape.</p><p>If a tape watches a value, operations taking this value as an input will be recorded. It is possible to differentiate any scalar that is computed while a tape is active with respect to any watched value. Tapes are composable data structures: multiple tapes can be active simultaneously, and higherorder gradients can computed by having one tape recording while another tape computes a gradient. Listing 1 gives an example of nesting tapes to compute a second derivative.</p><p>x = tf.constant(3.0) with tf.GradientTape() as t1: with tf.GradientTape() as t2: t1.watch(x) t2.watch(x) y = x * x dy_dx = t2.gradient(y, x) # 6.0 d2y_dx2 = t1.gradient(dy_dx, x) # 2.0 Listing 1. Tapes can be nested to compute higher-order derivatives.</p><p>Exposing the tape directly (as opposed to high-level Autograd-like gradient functions) lets users control which parts of the computation are traced for automatic differentiation, which can help limit the run-time overhead incurred in the tracing process.</p><p>The tape is tightly integrated with the logic responsible for staging code. The first time a graph function is called when a tape is both active and watching one of its inputs, we build a "forward" version of this function that returns any intermediate values needed for the backward step, in addition to its named outputs. As such, there is no meaningful change in the amount of computation or memory needed in the backward pass by staging or unstaging a particular function, leading to more predictable performance. Moreover, this ensures that if a computation was staged in the forward pass, its corresponding backward pass will also be staged.</p><p>Note that gradient computation is itself expressed as a function which executes primitive operations, so it is possible to stage it or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">State</head><p>Like TensorFlow, TensorFlow Eager keeps program state in variables, restoring a variable's value by assigning to it from a restore operation and periodically saving it to disk by sending its value to a save operation. Variables are useful when implementing models because accessing a variable's value automatically watches it on all active tapes, as shown in Listing 2.</p><p>x = tf.Variable(3.0) with tf.GradientTape() as t1:</p><p>with tf.GradientTape() as t2: y = x * x dy_dx = t2.gradient(y, x) # 6.0 d2y_dx2 = t1.gradient(dy_dx, x) # 2.0 Listing 2. Gradient tapes automatically watch variables; compare this code to Listing 1.</p><p>In TensorFlow Eager, variables correspond to Python objects. Each variable object has its own unique storage that is deleted when Python deletes the object. This is true even for traced computations, where staged read, write, save, and restore operations may interact with variables. Staged computations reference variables by unique identifiers, which are no longer usable if the Python variable objects they reference do not exist. This correspondence ensures that TensorFlow Eager state conforms to programmer expectations, stored like any other Python state and accessible through Python identifiers.</p><p>One challenge when moving from purely staged computation to keeping state in Python objects is matching state between executions of the same program. TensorFlow uses unique names for each variable in a program, which relies on the user creating variables in a consistent order. For example creating two copies of the same model requires special consideration when restoring the second model. Ten-sorFlow Eager uses a graph-based matching system, where a directed graph with named edges between objects is serialized along with the program state. On restore, a greedy matching determines a correspondence between serialized Python state and the objects being restored. This matching is local in that it depends only on the objects being saved and restored, not on other parts of the program. Listing 3 and Figure <ref type="figure">1</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Devices</head><p>TensorFlow Eager makes it simple to use a variety of devices, such as CPUs, GPUs, and TPUs. During program startup, the runtime detects the devices that are available to the machine, and makes it possible to both execute operations on them and store data on them. Imperative and staged computations use the same underlying Device abstraction, which makes it possible to both execute operations on devices and store data on them. A user-visible API endpoint list devices is exposed which lists all devices that the runtime is aware of.</p><p>All tensors exposed to the user are handles to data stored on a particular device. The runtime is also aware of how to copy data between various types of devices, and exposes this functionality through API endpoints on tensor instances. When executing an operation, the runtime expects to have a specific device to run the operation on. TensorFlow Eager exposes a context manager, device, so that the user can control which device operations execute on. The user is not required to use this API, as the runtime is able to select a device based on the availability of kernels. When an operation has inputs on devices different from the device where the operation is executing, the runtime transparently copies the inputs to the correct device. This frees the user from having to explicitly copy tensors between various devices. Because graph functions are executed via a primitive operation, it is also possible to use the device context manager to run graph functions on various devices. If operations inside the graph function are explicitly placed on another device, they override the outer device context.</p><p>Graph functions can serve as a unit of compilation for accelerators; we use this to efficiently execute code on TPUs. When a staged computation is placed on a TPU, TensorFlow Eager automatically invokes XLA to compile the graph and produce a TPU-compatible executable. TensorFlow Eager does make it possible to execute code imperatively on TPUs, but the overhead of compiling operations for TPU and dispatching the generated code is significant. When amortized over a large graph function, this overhead becomes negligible (see §6 for a quantitative example). Note that this programming model is similar to <ref type="bibr">JAX (Frostig et al., 2018)</ref>, which provides a Python decorator that JIT-compiles functions via tracing and XLA. Finally, compiling staged computations through XLA provides us more opportunities for optimization, including layout optimization, instruction scheduling for concurrency, and operation fusion. Techniques like tensor re-materialization can make it possible to fit a staged model into TPU memory when it would be impossible to do so on an operation-by-operation basis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Distribution</head><p>The current system supports distributed execution with a single central server running the main (typically Python) program and several worker servers running on remote hosts. Each worker server adds its locally available devices (for example, CPUs, GPUs, or TPUs) to the pool of devices available to the main program. The main program can then execute operations or whole graph functions on remote devices through the worker servers.</p><p>The remote devices are identified by application-level names.</p><p>The names contain the job name, task inside the job, as well as the specific device available for the task. For example, "/job:training/task:2/device:GPU:0". When a server is brought up to be a part of a cluster, it is given the mapping from the application-level names to specific server instances identified by DNS names or IP addresses.</p><p>To run an operation on a remote device, the user uses the same syntax as for local devices (see 4.4) but uses a remote device name instead of the local device name. Tensors produced as the result of running an operation on a remote device stay on the remote device. Users can then either perform more operations on these tensors or copy them to the central server (e.g. to use their value in an if statement).</p><p>Some computations running on remote devices can directly communicate and synchronize between each other. In such cases, developers need to start these computations concur-rently, e.g. using Python threads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Staging computations</head><p>The particular type of staging that TensorFlow Eager supports is similar to lightweight modular staging <ref type="bibr" target="#b24">(Rompf &amp; Odersky, 2010)</ref>, which in turn is a form of partial evaluation <ref type="bibr" target="#b13">(Jones et al., 1993)</ref>. As stated in §4.1, we expose a uservisible API endpoint named function that takes a Python function and returns an object which, when called, executes a dataflow graph created by running the user-provided Python function in a graph-building context. In this section, we discuss the implementation of function in detail.</p><p>Polymorphism. All Python functions are polymorphic in their inputs. In contrast, graph functions are not polymorphic: they have a fixed number of inputs, which are statically typed. We bridge this semantic gap between Python functions and graph functions by implementing a trace cache, similar to the one described in JAX <ref type="bibr">(Frostig et al., 2018)</ref>.</p><p>The object F = function(f) maintains a cache mapping from inferred input signatures to concrete graph functions. In particular, each time F is invoked, its inputs are processed and their signature is inferred: tensors are represented as abstract types (numerical type and shape tuples), while non-tensor values are encoded by object identity. This input signature, coupled with a small amount of metadata about the surrounding program state such as the requested device, becomes a key into a cache of graph functions. A cache miss triggers a trace of f on the given inputs, while a cache hit results in the reuse of a previously created graph function. In this sense, function provides ad hoc polymorphism <ref type="bibr" target="#b27">(Strachey, 2000)</ref> or function overloading.</p><p>Not only is specializing functions on input types required for correctness, it also lets us generate optimized graphsthis kind of optimization is well-known, and, indeed, one of the primary motivations for partial evaluation <ref type="bibr" target="#b13">(Jones et al., 1993;</ref><ref type="bibr" target="#b29">Taha, 2004;</ref><ref type="bibr" target="#b24">Rompf &amp; Odersky, 2010)</ref>.</p><p>Like JAX, function specializes on the run-time values of non-tensor arguments to let them parameterize the computation (function specializes automatically, whereas JAX makes this process a manual one). For example, it is common to write Python functions that take a boolean is training argument that determines whether or not dropout is applied. Our implementation of binding-time analysis ensures that graph functions are specialized on the value of the boolean argument (see listing 6 for an example).</p><p>@tf.contrib.eager.function def lossy_matmul(W, x, training=True): outputs = tf.matmul(W, x) if training: outputs = tf.nn.dropout(outputs, 0.2) return outputs W = tf.random_normal((3, 5))</p><p>x = tf.random_normal((5, 1)) # Executes a graph with dropout. lossy_outputs = lossy_matmul(W, x, training=True) # Executes a graph without dropout. exact_outputs = lossy_matmul(W, x, training=False)</p><p>Listing 6. This code transparently makes two graph functions.</p><p>The user also has the option of specifying an input signature to eliminate input polymorphism. In this case, we guarantee that we only generate a single graph function using only the shape and numeric type information specified in the signature. This can be useful for serialization and errorchecking, and for creating a single function that can handle arbitrary batch sizes or sequence lengths.</p><p>Lexical closure. function is capable of tracing Python functions that lexically close over tensors or variablesthese closed-over objects are treated as "captured" inputs that are silently passed to the graph function at call-time, without programmer intervention. Variables are captured by reference and not by value, which means that graph functions are free to mutate them. Listing 7 provides an example. Composition. Because graph function execution is implemented as an operation, graph functions compose naturally: the graph of a function may include a function-call operation that executes another function. For example, consider the following code block:  The call to outer will generate two graph functions, one for inner, and one for outer that contains a call to inner's graph function. Figure <ref type="figure" target="#fig_4">2</ref> shows what their corresponding graphs look like.</p><p>State creation. When building machine learning models, it is common to write Python functions that create and initialize variables the first time they are called. To support this idiom, function imposes some requirements on the decorated function f. State, such as TensorFlow variables, must only be created the first time f is called; how that is accomplished is left to the implementation of f. If any variables are created in the first execution of f, then function will trace f a second time to record the behavior that will be used from then on. No variables may be created during that second trace, or any subsequent one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Escaping staged computations</head><p>Embedding imperative code in graphs. As discussed in §4.1, staging computations requires the programmer to refactor the to-be-staged code into Python functions that, when traced, construct dataflow graphs. This process may at times seem prohibitively difficult, as it can require replacing complicated Python control flow with TensorFlow control flow or even implementing custom operations along with custom C++ kernels -indeed, this observation was one of the motivations for building TensorFlow Eager to begin with.</p><p>For concreteness, say that we have a Python function that we wish to stage, and say that the function is almost entirely staging-friendly with the exception of a call to a datadependent recursive Python function that performs some operations on tensors. In this case, we have three options: we can refactor the function into three functions, staging the code before and after the recursive call and leaving the recursive call unstaged; we can give up on staging the function if refactoring proves too onerous; or we can stage the entire function while wrapping the recursive call in a py func, an operation that takes a Python function as an attribute and executes it imperatively, even in the context of staged code.</p><p>py func executes its Python function under a gradient tape (see §4.2) and as such it is differentiable; it also has both CPU and GPU kernels. When executing in imperative mode, wrapping a Python function in a py func has essentially no effect. But, in staged computations, i.e. in dataflow graphs, the py func operation is a way to embed imperative, Pythonic code into a dataflow graph. Equivalently, py func can be viewed as a way to quickly implement custom operations using Python instead of C++.</p><p>The benefit of py func is that it makes it easier to decorate large Python functions with @function. Disadvantages include a potential performance hit, as py func returns control to a single-threaded Python interpreter, and the fact that graphs with py funcs are not in general serializable.</p><p>Escaping traces. We provide a Python context manager, tf.init scope, that pauses the trace and jumps into the imperative context. We use this scope to implement function's state-creation contract; most users, on the other hand, will have no use for it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">IMPLEMENTATION</head><p>We have implemented the design presented in §4, and all of our code is open source<ref type="foot" target="#foot_0">1</ref> . Because TensorFlow Eager was built as an extension to TensorFlow, the implementation is not large: staging is implemented in approximately 2000 lines of Python, automatic differentiation is split across 900 lines of Python and 600 lines of C, and the imperative runtime-i.e., the code responsible for constructing and executing operations-is implemented in approximately 4000 lines of C++. TensorFlow Eager also provides a lightweight C API that exposes our runtime, and several of our colleagues are using this API directly in their own projects.</p><p>TensorFlow Eager inherits the benefits of TensorFlow's implementation. In particular, TensorFlow Eager is crossplatform, running on the Linux, Mac OS X, Windows, Android, and iOS operating systems, and various x86, ARM, and NVIDIA GPU architectures; it executes staged computations using a dataflow executor that can run over ten thousand subgraphs in parallel and that runs kernels in parallel when possible, across multiple CPU cores or GPU streams; it provides high-level Python APIs for training models and C++ APIs for inference (see <ref type="bibr">Abadi et al., 2016, §5)</ref>. TensorFlow Eager also provides access to the over 900 primitive operations that TensorFlow offers.</p><p>TensorFlow Eager and TensorFlow differ slightly but sig-nificantly in their implementations of staged execution. In TensorFlow, the dataflow graph defines the union of all the computations that the author of the graph might be interested in; the actual computation to execute is defined when the programmer requests the runtime to fetch the concrete values of some set of tensors resident in the graph. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EVALUATION</head><p>TensorFlow Eager considerably simplifies rapid prototyping. This at times trades off execution speed for development ease. In this section, we present examples<ref type="foot" target="#foot_1">2</ref> showing how we can use function to recover the speed of TensorFlow.</p><p>Experimental setup. The benchmarks were run within a docker container on a machine with an Intel(R) Xeon(R) W-2135 CPU with 12 cores at 3.7GHz, 64GB of memory, and a GTX 1080 GPU with 8GB of memory. The TPU benchmark was run on a publicly available Cloud TPU. Each benchmark run was 10 iterations, and an average of 3 runs was reported. For staged computations, build and optimization times were not included as these are one-time costs that are usually amortized over a number of runs.</p><p>ResNet-50. In Figure <ref type="figure" target="#fig_5">3</ref>  increases, since the ratio of the time spent in kernels over the time spent in Python increases. Additionally, training a ResNet doesn't benefit significantly from inter-op parallelization, so the staged computation is effectively as serial as the unstaged computation. These performance characteristics should hold true for other sufficiently large models, i.e., imperative performance will often be similar to staged performance. The code used to generate these benchmarks all rely on the same Model class; converting the code to use function is simply a matter of decorating two functions.</p><p>ResNet-50 on TPU. It is possible to run single operations on a TPU using TensorFlow Eager. An important caveat is that these benchmarks do not exploit the hardware optimally. They are presented as illustrative of how staging lets us target accelerators like TPUs with practically no code changes. We don't present an accompanying TensorFlow benchmark for this reason.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L2HMC.</head><p>In Figure <ref type="figure">4</ref> we show performance of an L2HMC <ref type="bibr" target="#b18">(Levy et al., 2018)</ref>  Eager, TensorFlow Eager with function, and TensorFlow on synthetic data running on the CPU. The benchmark samples from a 2-dimensional distribution, with 10 steps for the leapfrog integrator. This example highlights the tradeoff between debuggability and performance: by bypassing Python overheads and via buffer reuse and other static optimization, staging increasing examples per second by at least an order of magnitude. And while the trade-off exists, it is not particularly onerous here -simply decorating a single function recovers the full performance of TensorFlow. This benchmark stages computation aggressively, essentially running the entire update as a graph function. Depending on the desired visibility into the model's execution during development, it is possible to stage less aggressively.</p><p>Note. These examples were chosen as they lie at opposite ends of the tradeoff between execution speed and development speed. We expect most real-world models to fall somewhere between these two, and to be able to recover performance by staging as required. TensorFlow Eager is an evolving technology, and closing the gap between imperative and staged performance is being worked on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We presented TensorFlow Eager, an extension to Tensor-Flow that makes what was once a declarative DSL for differentiable programming into a multi-stage, imperative-first one. TensorFlow Eager's imperative-by-default behavior makes it suitable for beginners and researchers alike, and the option to stage computations as graph functions lets users trade off the interactivity and Python integration furnished by imperative execution for the benefits provided by static graphs, performance and ease of serialization among them.</p><p>Within Alphabet, dozens have adopted TensorFlow Eager. For example, some researchers use it to implement dynamic language models and reinforcement learning methods, and several internal workshops on TensorFlow Eager have been attended widely. Multiple groups are restructuring their machine learning frameworks to make TensorFlow Eager the default way of using them (examples include libraries for probabilistic machine learning and reinforcement learning), and at least one large research group has engineers dedicated to supporting TensorFlow Eager. Externally, some university courses have included TensorFlow Eager as part of their curriculum, and 48 percent of respondents to a survey distributed at the 2018 TensorFlow Developer Summit agreed with the statement, "[TensorFlow Eager] is important to me as an iterative development and debugging tool."</p><p>TensorFlow Eager is an evolving technology. While it is well-suited for research and pedagogy alike, we are still working to provide an out-of-the-box solution for imperatively-driven distributed training. And while multistage programming is powerful -wrapping large Python functions in function often "does the right thing" -staging computations with dynamic control flow can require nontrivial programmer intervention. We hope to decrease this friction via Autograph <ref type="bibr" target="#b20">(Moldovan et al., 2019)</ref>.</p><p>Finally, TensorFlow Eager has informed the evolution of TensorFlow itself: the upcoming TensorFlow 2.0 uses our implementation to provide an imperative-first, multi-stage programming model similar to the one outlined in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We'd like to thank everyone on the TensorFlow team for their feedback and help with the design and implementation of this system. Alex Wiltschko, Pierre Sermanet, Xin Pan, Yaroslav Bulatov, Manjunath Kudlur, and Yuan Yu contributed significantly to motivating and early prototyping of TF Eager. Early users like Sergio Guadarrama, Daniel Abolafia, David Berthelot, Chen Li, Debidatta Dwibedi, among others, were key in helping us shape the requirements of this system. A lot of it wouldn't look like it does now without important feedback from DeepMind, especially from Aedan Pope and Tom Hennigan. Franc ¸ois Chollet was very helpful in integrating TF Eager with Keras.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>a = tf.constant(1.0) # stored on CPU b = a.gpu() # stored on GPU Listing 4. Tensor copies between CPU and GPU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>device("/gpu:0"): c = tf.add(a, b) assert c.numpy() == 3.0 Listing 5. Executing a GPU operation with inputs on the CPU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Listing 7. function transparently captures closed-over tensors and variables, forwarding them to TensorFlow functions as inputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>@</head><label></label><figDesc>tf.contrib.eager.function def inner(a): return tf.nn.relu(a) @tf.contrib.eager.function def outer(a, b): return inner(tf.matmul(a, b)) outer(tf.eye(3), tf.diag([-1.0, 1.0, 2.0]))Listing 8. Graph functions can be nested.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. function composes; above, the graphs for Listing 8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Examples per second when training ResNet-50 on a GPU (top). Percent improvement over TensorFlow Eager (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>different output every time it is invoked, the dataflow graph generated by function(add noise) will return the same value every time it is called, since a particular random offset generated by NumPy will be inserted into the graph as a constant. Note that if state is represented in terms of operations (e.g., if we replace the call to np.random.randn with tf.random normal), we can preserve semantics under this tracing model. As a corollary, if a Python function f has Python side-effects (e.g., every call to it increments a global Python counter), then executing it multiple times will not necessarily be semantically equivalent to repeatedly executing the callable returned by function(f). Python functions must also be resilient to being executed multiple times, as the callable returned by function might trace its Python function multiple times (see the discussion on polymorphism in §4.6).</figDesc><table><row><cell>Python func-</cell></row><row><cell>tions in a graph-building context and only records operations</cell></row><row><cell>and tensors. In a graph-building context, operations return</cell></row><row><cell>symbolic representations of values to be computed instead</cell></row><row><cell>of concrete values, and non-TensorFlow Python code ex-</cell></row><row><cell>ecutes normally. Python functions that are amenable to</cell></row><row><cell>staging are those that, when called in a graph-building con-</cell></row><row><cell>text, generate a graph that encapsulates the computation</cell></row><row><cell>of interest. This means that if a Python function executes</cell></row><row><cell>non-TensorFlow code, then there might be semantic discrep-</cell></row><row><cell>ancies between executing the Python function and executing</cell></row><row><cell>the traced dataflow graph. For example, whereas the Python</cell></row><row><cell>function</cell></row></table><note>def add_noise(): eye = tf.eye(5) randn = np.random.randn(5, 5) return eye + randn will return a</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>contain a short example. Python object and matched as part of a directed graph with named edges. Examples include an iterator over input data whose position in a dataset is serialized, mutable hash tables, and outside of traced code even miscellaneous Python state such as NumPy arrays can use graph-based state matching.</figDesc><table><row><cell>Staging enables serializing the program for use without a</cell><cell></cell></row><row><cell>Python interpreter, as in TensorFlow. A typical development</cell><cell></cell></row><row><cell>workflow involves using graph-based state matching while</cell><cell></cell></row><row><cell>writing and tweaking a TensorFlow Eager program, then</cell><cell></cell></row><row><cell>serializing a trace for use in a production environment that</cell><cell></cell></row><row><cell>executes the trace using TensorFlow's C++ API.</cell><cell></cell></row><row><cell cols="2">class Net(tf.keras.Model):</cell></row><row><cell>def __init__(self):</cell><cell></cell></row><row><cell cols="2">super(Net, self).__init__()</cell></row><row><cell cols="2">self.v = tf.Variable(1.)</cell></row><row><cell cols="2">self.out = tf.layers.Dense(1)</cell></row><row><cell>def call(self, x):</cell><cell></cell></row><row><cell>return self.out(</cell><cell></cell></row><row><cell cols="2">tf.nn.softplus(x * self.v))</cell></row><row><cell cols="2">Listing 3. Model-building code which implicitly constructs a graph</cell></row><row><cell cols="2">with named directed edges (from attribute names), used for state</cell></row><row><cell>matching.</cell><cell></cell></row><row><cell>v</cell><cell></cell></row><row><cell>out</cell><cell>kernel</cell></row><row><cell></cell><cell>bias</cell></row></table><note>Figure 1. Visualization of the dependency graph for Listing 3, with filled-in intermediate nodes and nodes without fill containing state.Variables are the most common type of state, but other state is similarly scoped to a</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Graph functions provide benefits outside the realm of usability as well. Because graph functions are executed via an operation, we get function composition for free. In the context of single-coordinator distributed training, in which a single subgraph is executed by N workers, graph functions can reduce memory pressure on the coordinator: the coordinator only needs to own a graph function that contains N function-call operations (instead of N copies of a subgraph).</figDesc><table><row><cell>This amounts</cell></row><row><cell>to a discrepancy between what is expressed in Python and</cell></row><row><cell>what is executed by the TensorFlow runtime. To provide</cell></row><row><cell>a more Pythonic programming model, TensorFlow Eager</cell></row><row><cell>represents each staged computation as a graph function, i.e.,</cell></row><row><cell>a graph with named inputs and outputs, representing the</cell></row><row><cell>exact computation of interest. This approach still allows for</cell></row><row><cell>graph optimizations: for example, non-stateful operations</cell></row><row><cell>that are not reachable from the outputs of a function are</cell></row><row><cell>pruned, just as in TensorFlow.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>The performance of training ResNet-50 on ImageNet<ref type="bibr" target="#b6">(Deng et al., 2009)</ref> using Ten-sorFlow Eager versus TensorFlow Eager with function is shown in Table1. Training the model in a per-operation fashion is slow, even at a batch size of 32; staging yields an order of magnitude improvement in examples per second.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 1 .</head><label>1</label><figDesc>Examples per second training ResNet-50 on a TPU.</figDesc><table><row><cell>implementation, comparing TensorFlow</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/tensorflow/tensorflow</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">The code for these example models and others is available at https://github.com/tensorflow/ tensorflow/tree/master/tensorflow/contrib/ eager/python/examples.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation</title>
				<meeting>the 12th USENIX Conference on Operating Systems Design and Implementation<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic differentiation in machine learning: a survey</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Baydin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Pearlmutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Radul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Siskind</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">153</biblScope>
			<biblScope unit="page" from="1" to="43" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Theano: A CPU and GPU math compiler in Python</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Python in Science Conference</title>
				<meeting>the 9th Python in Science Conference</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Compilation, Optimization of Object-Oriented Languages and Programming Systems</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Bolz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cuni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fijalkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rigo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th workshop on the Implementation</title>
				<meeting>the 4th workshop on the Implementation</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="18" to="25" />
		</imprint>
	</monogr>
	<note>Tracing the meta-level: PyPy&apos;s tracing JIT compiler</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Mxnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.01274</idno>
		<title level="m">A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems. arXiv e-prints, art</title>
				<imprint>
			<date type="published" when="2015-12">Dec 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Chetlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Woolley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandermersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.0759</idno>
		<title level="m">Efficient Primitives for Deep Learning. arXiv e-prints, art</title>
				<imprint>
			<date type="published" when="2014-10">Oct 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the 2009 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Compiling machine learning programs via high-level tracing</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hegarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aiken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vitek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Terra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation</title>
				<meeting>the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013. 2018</date>
			<biblScope unit="page" from="105" to="116" />
		</imprint>
	</monogr>
	<note>the 1st SysML Conference</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Building domain-specific embedded languages</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hudak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4es</biblScope>
			<date type="published" when="1996-12">December 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Flux: Elegant machine learning with Julia</title>
		<author>
			<persName><forename type="first">M</forename><surname>Innes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Open Source Software</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Don&apos;t unroll adjoint: Differentiating SSA-form programs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Innes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd SysML Conference</title>
				<meeting>the 2nd SysML Conference</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On machine learning and programming languages</title>
		<author>
			<persName><forename type="first">M</forename><surname>Innes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karpinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Besard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Churavy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Danisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Edelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malmaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Revels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 1st SysML Conference</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Partial Evaluation and Automatic Program Generation</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Gomard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sestoft</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Prentice-Hall, Inc</publisher>
			<pubPlace>Upper Saddle River, NJ, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Compilers and staging transformations</title>
		<author>
			<persName><forename type="first">U</forename><surname>Jørring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Scherlis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages</title>
				<meeting>the 13th ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="86" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06018</idno>
		<title level="m">Segmental Recurrent Neural Networks. arXiv e-prints, art</title>
				<imprint>
			<date type="published" when="2015-11">Nov 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A LLVMbased Python JIT compiler</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pitrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seibert</surname></persName>
		</author>
		<author>
			<persName><surname>Numba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on the LLVM Compiler Infrastructure in HPC</title>
				<meeting>the Second Workshop on the LLVM Compiler Infrastructure in HPC<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">and the Swift for TensorFlow Team</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lattner</surname></persName>
		</author>
		<ptr target="https://github.com/tensorflow/swift" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Swift for TensorFlow</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generalizing hamiltonian monte carlo with neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Learning Representations</title>
				<meeting>the 6th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Autograd: Effortless gradients in NumPy</title>
		<author>
			<persName><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the AutoML Workshop of the 32nd International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Autograph: Imperative-style coding with graphbased performance</title>
		<author>
			<persName><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Decker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Nado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rompf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Wiltschko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd SysML Conference</title>
				<meeting>the 2nd SysML Conference</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Clothiaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Oda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Saphra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><surname>Dynet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.03980</idno>
		<title level="m">The Dynamic Neural Network Toolkit</title>
				<imprint>
			<date type="published" when="2017-01">Jan 2017</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints, art</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Guide to NumPy</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Oliphant</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>CreateSpace Independent Publishing Platform</publisher>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Conference on Neural Information Processing Systems</title>
				<meeting>the 31st Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Lightweight modular staging: a pragmatic approach to runtime code generation and compiled DSLs</title>
		<author>
			<persName><forename type="first">T</forename><surname>Rompf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Odersky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Conference on Generative Programming and Component Engineering</title>
				<meeting>the 9th International Conference on Generative Programming and Component Engineering<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="127" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">An in-depth look at Google&apos;s first Tensor Processing Unit (TPU</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<ptr target="https://cloud.google.com/blog/products/gcp/an-in-depth-look-at-googles-first-tensor-processing-unit-tpu" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Parsing natural scenes and natural language with recursive neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning</title>
				<meeting>the 28th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Fundamental concepts in programming languages. Higher-order and symbolic computation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Strachey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="11" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">OptiML: an implicitly parallel domain-specific language for machine learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sujeeth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rompf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Atreya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Odersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Olukotun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning</title>
				<meeting>the 28th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="609" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A gentle introduction to multi-stage programming</title>
		<author>
			<persName><forename type="first">W</forename><surname>Taha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Domain-Specific Program Generation</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="30" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<ptr target="https://gluon.mxnet.io/" />
		<title level="m">The Gluon Team. Deep learning: The straight dope</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Xla</forename><surname>The</surname></persName>
		</author>
		<author>
			<persName><surname>Team</surname></persName>
		</author>
		<author>
			<persName><surname>Xla -Tensorflow</surname></persName>
		</author>
		<ptr target="https://developers.googleblog.com/2017/03/xla-tensorflow-compiled.html" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Chainer: a next-generation open source framework for deep learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tokui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hido</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of workshop on machine learning systems (LearningSys) in the 29th annual conference on Neural Information Processing Systems</title>
				<meeting>workshop on machine learning systems (LearningSys) in the 29th annual conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">DLVM: A modern compiler infrastructure for deep learning systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Adve</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.03016</idno>
		<imprint>
			<date type="published" when="2017-11">Nov 2017</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints, art</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
