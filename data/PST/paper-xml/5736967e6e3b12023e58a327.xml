<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Micro-expression Recognition Using Color Spaces</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Su-Jing</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
							<email>wangsujing@psych.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Wen-Jing</forename><forename type="middle">J</forename><surname>Yan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiaobai</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Guoying</forename><surname>Zhao</surname></persName>
							<email>gyzhao@ee.oulu.fi</email>
						</author>
						<author>
							<persName><forename type="first">Chun-Guang</forename><forename type="middle">G</forename><surname>Zhou</surname></persName>
							<email>cgzhou@jlu.edu.cn</email>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Xiaolan</forename><surname>Fu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Minghao</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Jianhua</forename><surname>Tao</surname></persName>
							<email>jhtao@nlpr.ia.ac.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Symbolic Computation and Knowledge Engineering of Ministry of Education</orgName>
								<orgName type="institution">Jilin University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Institute of Psychology</orgName>
								<orgName type="laboratory">Key Laboratory of Behavior Sciences</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100101</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Key Laboratory of Symbolic Computation and Knowledge Engineering of Ministry of Education</orgName>
								<orgName type="institution">Jilin University</orgName>
								<address>
									<postCode>130012</postCode>
									<settlement>Changchun</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">College of Teacher Education</orgName>
								<orgName type="institution">Wenzhou University</orgName>
								<address>
									<postCode>325035</postCode>
									<settlement>Wenzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Oulu</orgName>
								<address>
									<postBox>P. O. Box 4500</postBox>
									<postCode>FI-90014</postCode>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Jilin University</orgName>
								<address>
									<postCode>130012</postCode>
									<settlement>Changchun</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="laboratory">Key Laboratory of Symbolic Computation and Knowledge Engineering of Ministry of Education</orgName>
								<orgName type="institution">Jilin University</orgName>
								<address>
									<postCode>130012</postCode>
									<settlement>Changchun</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="department">State Key Laboratory of Brain and Cognitive Science</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="department">Institute of Psychology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100101</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff9">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recogni-tion (NLPR</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<addrLine>CASIA) No.95 ZhongGuanCun East Street</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff10">
								<orgName type="institution">HaiDian District</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Micro-expression Recognition Using Color Spaces</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">400BCC1E16F593DDAF299A8AFDEDBA05</idno>
					<idno type="DOI">10.1109/TIP.2015.2496314</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2015.2496314, IEEE Transactions on Image Processing IEEE TRANSACTIONS ON IMAGE PROCESSING 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Micro-expression recognition</term>
					<term>Color Spaces</term>
					<term>Tensor Analysis</term>
					<term>Local Binary Patterns</term>
					<term>Facial Action Coding System Lower Face Action Units -Orbital Actions</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Micro-expressions are brief involuntary facial expressions that reveal genuine emotions and, thus help detect lies. Because of their many promising applications, they have attracted the attention of researchers from various fields. Recent research reveals that two perceptual color spaces (CIELab and CIELuv) provide useful information for expression recognition. This paper is an extended version of our International Conference on Pattern Recognition (ICPR) paper <ref type="bibr" target="#b0">[1]</ref>, in which we propose a novel color space model, Tensor Independent Color Space (TICS), to help recognize micro-expressions. In this paper, we further show that CIELab and CIELuv are also helpful in recognizing micro-expressions, and we indicate why these three color spaces achieve better performance. A micro-expression color video clip is treated as a fourth-order tensor, i.e., a four-dimension array. The first two dimensions are the spatial information, the third is the temporal information, and the fourth is the color information. We transform the fourth dimension from RGB into TICS, in which the color components are as independent as possible. The combination of dynamic texture and independent color components achieves a higher accuracy than does that of RGB. In addition, we define a set of Regions of Interest (ROIs) based on the Facial Action Coding System (FACS) and calculated the dynamic texture histograms for each ROI. Experiments are conducted on two micro-expression databases, CASME and CASME 2, and the results show that the performances for TICS, CIELab and CIELuv are better than those for RGB or gray.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Micro-expressions are brief facial expressions that reveal the emotions that a person tries to conceal, especially in high-stakes situations <ref type="bibr" target="#b1">[2]</ref> <ref type="bibr" target="#b2">[3]</ref>. Compared with normal facial expressions, micro-expressions have three significant characteristics. They are of short duration, and low intensity and are generally fragments of prototypical facial expressions. Microexpressions are generally known for their potential use in many fields, such as clinical diagnosis <ref type="bibr" target="#b3">[4]</ref>, national security <ref type="bibr" target="#b4">[5]</ref> and interrogations <ref type="bibr" target="#b5">[6]</ref>, because they may reveal genuine emotions and thus help detect lies. The polygraph is invasive because it must be connected to the individual's body throughout the session <ref type="bibr" target="#b6">[7]</ref>. Thus, individuals are aware that they are being monitored and may develop countermeasures. In comparison, lie detection based on micro-expressions is unobtrusive, and the individuals being observed are less likely to develop countermeasures.</p><p>More than 30 years ago, psychologists began to show interest in micro-expressions. Haggard and Isaacs first discovered micro-expressions (micro-momentary expressions) and viewed them as cues for repressed emotions <ref type="bibr" target="#b7">[8]</ref> <ref type="bibr" target="#b8">[9]</ref>. In 1969, Ekman analyzed a video of an interview with a patient, who was suffering from depression and had tried to commit suicide, and observed micro-expressions. Since that time, Ekman's group has conducted many studies on micro-expressionds <ref type="bibr" target="#b9">[10]</ref>. According to Ekman, micro-expressions are the most promising approach for detecting deception <ref type="bibr" target="#b2">[3]</ref>.</p><p>Although micro-expressions have potential application in a variety of fields, humans have difficulty in detecting and recognizing them. This difficulty may be the result of their short duration, and low intensity in addition to fragmental action units <ref type="bibr" target="#b1">[2]</ref> <ref type="bibr" target="#b10">[11]</ref>. Although there is debate regarding their duration, the generally accepted limit is 0.5 seconds <ref type="bibr" target="#b10">[11]</ref> <ref type="bibr" target="#b11">[12]</ref>. Micro-expressions are usually very subtle because individuals try to control and repress them <ref type="bibr" target="#b10">[11]</ref>. In addition, microexpressions usually exhibit only parts of the action units of fully-stretched facial expressions. For example, only the upper face or lower face may show action units, not both at the same time as in macro-expressions <ref type="bibr" target="#b12">[13]</ref>. To improve human performance in recognizing micro-expressions, Ekman <ref type="bibr" target="#b13">[14]</ref> developed the Micro-Expression Training Tool (METT), which trains people to better recognize seven categories<ref type="foot" target="#foot_0">1</ref> of microexpressions. Due to the increasing power of computers and the overwhelming quantity of expressions to monitor, researchers have turned to automatic micro-expression recognition.</p><p>However, there are few papers addressing micro-expression recognition. Polikovsky et al. <ref type="bibr" target="#b14">[15]</ref> used a 3D-gradient descriptor for micro-expressions recognition. Wang et al. <ref type="bibr" target="#b15">[16]</ref> treated a micro-expression gray-scale video clip as a 3rdorder tensor and used Discriminant Tensor Subspace Analysis (DTSA) and Extreme Learning Machine (ELM) to recognize micro-expression. However, the subtle movements of microexpressions may be lost in the process of solving DTSA. Pfister et al. <ref type="bibr" target="#b16">[17]</ref> used a Temporal Interpolation Model (TIM) based on Laplacian matrix to normalize the frame numbers of micro-expression video clips. Then, the LBP-TOP <ref type="bibr" target="#b17">[18]</ref> was used to extract the motion and appearance features of micro-expressions and multiple kernel learning was used for classification.</p><p>The LBP operator has been widely used in ordinary texture analysis. It efficiently describes the local structures of images, and in recent years, has aroused increasing interests in many areas of image processing and computer vision, exhibiting its effectiveness in a number of applications. Recently, research on LBP has flourished. Tan and Triggs <ref type="bibr" target="#b18">[19]</ref> developed a generalization of the local texture descriptor named as Local Ternary Pattern (LTP) for face recognition, which is more discriminative and less sensitive to noise in uniform regions. Zhu et al. <ref type="bibr" target="#b19">[20]</ref> divided P neighbors into [P/4] groups and calculated an LBP histogram for each group including at most 4 points. The lines connecting two neighboring points to the central point are orthogonal. They named the method as the orthogonal combination of local binary patterns (OC-LBP). The objective of OC-LBP is to reduce the dimensionality of the original LBP operator while keeping its discriminative power and computational efficiency. The authors also proposed six new local descriptors based on OC-LBP enhanced with color information for image region description. The main idea is to independently calculate the original OC-LBP descriptor over different channels in a given color space, and then concatenate them to obtain the final color OC-LBP descriptor <ref type="bibr" target="#b19">[20]</ref>. Lee et al. <ref type="bibr" target="#b20">[21]</ref> presents a novel expression recognition method that exploits the effectiveness of color information and sparse representation.</p><p>Color is a fundamental aspect of human perception, and its effects on cognition and behavior have intrigued generations of researchers <ref type="bibr" target="#b21">[22]</ref>. Recent research efforts <ref type="bibr">[23][24]</ref>[25] <ref type="bibr" target="#b25">[26]</ref>[27] <ref type="bibr" target="#b27">[28]</ref> revealed that color may provide useful information for face recognition. In <ref type="bibr" target="#b22">[23]</ref>, it is also revealed that the face recognition system for various color spaces (such as RGB, PCS and YIQ) is better than for gray images. Liu <ref type="bibr" target="#b23">[24]</ref> applied PCA, ICA and LDA to obtain the uncorrelated color space (UCS), the independent color space (ICS), and the discriminating color space (DCS) for face recognition, respectively. In <ref type="bibr" target="#b24">[25]</ref>, the authors took advantage of the ICS to improve performance of the Face Recognition Grand Challenge (FRGC) <ref type="bibr" target="#b28">[29]</ref>. Yang and Liu proposed <ref type="bibr" target="#b25">[26]</ref> the Color Image Discriminant (CID) model borrowing the idea of LDA to not only obtain the discriminative color space but also to obtain the optimal spatial transformation matrix. Wang et al. <ref type="bibr" target="#b26">[27]</ref> presented a Tensor Discriminant Color Space (TDCS) model that uses a 3rd-order tensor to represent a color facial image. To make the model more robust to noise, they <ref type="bibr" target="#b27">[28]</ref> also used an elastic net to propose a Sparse Tensor Discriminant Color Space (STDCS). Lajevardi and Wu <ref type="bibr" target="#b29">[30]</ref> also treated a color facial expression image as a 3rd-order tensor and showed that the perceptual color spaces (CIELab and CIELuv) are better overall than other color spaces for facial expression recognition.</p><p>When the emotional and physiological states of humans change, the facial skin color hue subtly changes, due to variations in the levels of hemoglobin and oxygenation under the skin. Ramirez et al. <ref type="bibr" target="#b30">[31]</ref> showed that facial skin color changes can be used to infer the emotional state of a person in the valence dimension with an accuracy of 77.08%. We infer that when different micro-expressions occur, the facial skin color hues are also different. Given such a consideration, facial color information could help improve micro-expression recognition.</p><p>This paper is an extended version of our International Conference on Pattern Recognition (ICPR) paper <ref type="bibr" target="#b0">[1]</ref>, in which we propose a novel color space model, Tensor Independent Color Space (TICS), to help recognize micro-expressions. In this paper, we further show that CIELab and CIELuv are also helpful in recognizing micro-expressions. In these color spaces, LBP-TOP is used to extract the dynamic texture features of micro-expression clips from three color components. Then, the histograms of the LBP-TOP codes are concatenated as a long feature vector, which is treated as the input of SVM to recognize micro-expressions. The results in TICS are slightly better than those in CIELab and CIELuv. A key difference with CIELab and CIELuv is that TICS is modeled by learning from samples. The three color components in TICS are as independent from each other as possible. We use the mutual information to explain why TICS, CIELab and CIELuv achieve better performance than the RGB color space. In addition, we use tensors to generalize LBP-TOP to a higherdimensional space, and propose Tensor Orthogonal LBP (TO-LBP). We also show LBP-TOP is a special case of TO-LBP in 3D space.</p><p>The rest of this paper is organized as follows: in Section II, we briefly review the fundamentals of tensors and the perceptual color spaces; in Section III, we present the Tensor Independent Color Space model (TICS); in Section IV, we introduce LBP-TOP, which is used to extract the dynamic texture features of micro-expression clips from three components in TICS; in Section V, we design 16 Region of Interests (ROIs) based on Action Units; in Section VI, we describe the micro-expression recognition framework based on TICS and LBP-TOP; in Section VII, the experiments are conducted on two micro-expression databases CASME and CASME 2, the results showing the efficiency and performance of TICS; finally in Section VIII, conclusions are drawn and several issues for future work are discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND A. Tensor Fundamentals</head><p>A tensor is a multidimensional array. It is the higher-order generalization of a scalar (zero-order tensor), vector (1st-order tensor), and matrix (2nd-order tensor). In this paper, lowercase italic letters (a, b, ...) denote scalars, bold lowercase letters (a, b, ...) denote vectors, bold uppercase letters (A, B, ...) denote matrices, and calligraphic uppercase letters (A, B, ...) denote tensors. The formal definition is given below <ref type="bibr" target="#b31">[32]</ref>:</p><formula xml:id="formula_0">Definition 1. The order of a tensor A ∈ R I1×I2×...×IN is N . An element of A is denoted by A i1i2...iN or a i1i2...iN , where 1 ≤ i n ≤ I n , n = 1, 2, . . . , N .</formula><p>Definition 2. The n-mode vectors of A are the I n -dimensional vectors obtained from A by fixing every index but index i n . Definition 3. The n-mode unfolding matrix of A, denoted by (A) (n) ∈ R In×(I1...In-1In+1...×I N ) , contains the element a i1...iN at the i n th row and jth column, where</p><formula xml:id="formula_1">j = 1 + N ∑ k=1,k̸ =n (i k -1)J k , with J k = k-1 ∏ m=1,m̸ =n I m .</formula><p>(</p><formula xml:id="formula_2">)<label>1</label></formula><p>We can generalize the product of two matrices to the product of a tensor and a matrix.</p><formula xml:id="formula_3">Definition 4. The n-mode product of a tensor A ∈ R I1×I2×...×IN by a matrix U ∈ R Jn×In , denoted by A × n U, is an (I 1 × I 2 × . . . × I n-1 × J n × I n+1 × . . . × I N )-tensor</formula><p>for which the entries are given by:</p><formula xml:id="formula_4">(A× n U) i1i2...in-1jnin+1...iN def = ∑ in a i1i2...in-1inin+1...iN u jnin .</formula><p>(2) Definition 5. The scalar product of two tensors A, B ∈ R I1×I2×...×IN , denoted by ⟨A, B⟩, is defined in a straightforward way as ⟨A, B⟩</p><formula xml:id="formula_5">def = ∑ i1 ∑ i2 . . . ∑ iN a i1i2...iN b i1i2...iN . The Frobenius norm of a tensor A ∈ R I1×I2×...×I N is then defined as ∥A∥ F def = √ ⟨A, A⟩</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Perceptual Color Spaces</head><p>In this section, we introduce two perceptual color spaces: CIELab, and CIELuv, which can enhance the performance of expression recognition <ref type="bibr" target="#b29">[30]</ref>. In computer vision, the most widely used color space is RGB color space, which is the basis for other color spaces (such as YCbCr, CIELab, and CIELuv) that are usually defined by its linear or nonlinear transformations. To reduce the lighting effect, the RGB color space is usually normalized, and denoted as (R norm , G norm , B norm ).</p><p>To convert from RGB to perceptual color spaces (CIELab or CIELuv), the RGB color space is first converted to the CIE XYZ color space, which is the basis for conversion to perceptual color spaces. The component L is the same for both the CIELab and CIELuv color spaces. The component L indicates lightness and is independent of the other two components. The conversion procedure is as follows <ref type="bibr" target="#b29">[30]</ref>: </p><formula xml:id="formula_6">  X Y Z   =   0.</formula><formula xml:id="formula_7">    R norm G norm B norm  <label>(3)</label></formula><formula xml:id="formula_8">L = { 116 × ( Y Yn ) 1 3 -16, Y Yn &gt; 0.008856 903 × ( Y Yn ), Y Yn ≤ 0.008856<label>(4)</label></formula><formula xml:id="formula_9">a = 500 × ( f ( X X n ) -f ( Y Y n ) ) (5) b = 200 × ( f ( Y Y n ) -f ( Z Z n ) ) (<label>6</label></formula><formula xml:id="formula_10">)</formula><p>where X n , Y n , and Z n are reference white tristimulus values, which are defined in the CIE chromaticity diagram <ref type="bibr" target="#b32">[33]</ref> and</p><formula xml:id="formula_11">f (t) = { t 1 3</formula><p>, t &gt; 0.008856 7.787 × t + 16  116 , t ≤ 0.008856</p><p>For the u and v color components, the conversion is defined by</p><formula xml:id="formula_13">u = 13×L×(u ′ -u ′ n ) and v = 13×L×(v ′ -v ′ n ). (8)</formula><p>The equations for u ′ and v ′ are given below</p><formula xml:id="formula_14">u ′ = 4X X + 15Y + 3Z and v ′ = 9Y X + 15Y + 3Z<label>(9)</label></formula><p>The quantities u ′ n and v ′ n are the (u ′ , v ′ ) chromaticity coordinates of a specified white object and are defined by <ref type="bibr" target="#b9">(10)</ref> In Section VII, we will show that the mutual information among each component in CIELab (or CIELuv) is small, and this will explain why perceptual color spaces are better than RGB for micro-expression recognition.</p><formula xml:id="formula_15">u ′ n = 4X n X n + 15Y + 3Z n and v ′ n = 9Y n X n + 15Y n + 3Z n</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. TENSOR INDEPENDENT COLOR SPACE (TICS)</head><p>Unlike CIELab and CIELuv, Tensor Independent Color Space (TICS) is not a fixed linear or nonlinear transformation of RGB. Its transformation matrix is obtained by learning from the provided samples. A color micro-expression video clip is naturally represented by a 4th-order tensor, where mode-1 and mode-2 of a tensor are facial spatial information, mode-3 is the temporal information and mode-4 is the color space information. For instance, a color micro-expression video clip with a resolution of I 1 × I 2 is represented as a tensor X ∈ R I1×I2×I3×I4 , where I 3 is the number of frames and I 4 = 3 has 3 components corresponding to R, G and B in RGB space. However, the R, G and B components are correlated. If we can transform the three correlated components into a series of uncorrelated components T 1 , T 2 and T 3 , and extract the dynamic texture features from each uncorrelated component, we can obtain better results.</p><p>Given the assumption that M is the number of color microexpression video clips, X i is the ith color micro-expression video clip. We want to seek a color space transformation matrix U 4 ∈ R I4×L4 (usually L 4 = I 4 ) for transformation</p><formula xml:id="formula_16">Y i = X i × 4 U T 4 , i = 1, 2, . . . , M. (<label>11</label></formula><formula xml:id="formula_17">)</formula><p>such that the components of mode-4 of Y i are as independent as possible. To obtain U 4 , we use ICA<ref type="foot" target="#foot_1">2</ref> to decorrelate the RGB color space. M 4th-order tensor X i are concatenated to a 5th-order tensor F ∈ R I1×I2×I3×I4×M . The mode-4 unfolding matrix F (4) is a 3 × K matrix, where The color space transformation matrix U 4 may be derived using ICA on F <ref type="bibr" target="#b3">(4)</ref> . The ICA of F (4) factorizes the covariance matrix Σ F into the following form:</p><formula xml:id="formula_18">K = I 1 × I 2 × I 3 × M</formula><formula xml:id="formula_19">Σ F = U -1 4 ▽U -T 4 (<label>12</label></formula><formula xml:id="formula_20">)</formula><p>where ▽ ∈ R 3×3 is diagonal real positive and U 4 transforms RGB color space to a new color space whose three components are independent or the most possible independent. U 4 in Eq. ( <ref type="formula" target="#formula_19">12</ref>) may be derived using Comon's ICA algorithm by calculating mutual information and higher-order statistics <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. LBP DESCRIPTION FROM THREE ORTHOGONAL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PLANES</head><p>Local Binary Patterns (LBPs) <ref type="bibr" target="#b34">[35]</ref> are used on gray images to extract texture features. Given a pixel c in the gray image, its LBP code is computed by comparing it with its P neighbors p. The neighbors lie on a circle with center c and a radius equal to R.</p><formula xml:id="formula_21">LBP P,R = P -1 ∑ p=0 s(g p -g c )2 p (<label>13</label></formula><formula xml:id="formula_22">)</formula><p>where g c is the gray value of the given pixel c, g p is the value of its neighbor p, and s(u) is 1 if u ≥ 0 and 0 otherwise. If the coordinates of c are (x c , y c ), the coordinates of p are (x c + Rcos(2πp/P ), y c -Rsin(2πp/P )). The coordinates of the neighbors that do not fall exactly on pixels are approximated by bilinear interpolation. The LBP encoding process is illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. It is possible to use only a subset of 2 p binary patterns to describe the texture of the images. Ojala et al. named these patterns uniform patterns. An LBP is called uniform, if it contains at most two bitwise transitions from 0 to 1 or vice versa when the corresponding bit string is considered circular.</p><p>After the LBP of each pixel is coded, a histogram is calculated to represent the texture</p><formula xml:id="formula_23">H(k) = I ∑ i=1 J ∑ j=1 f (LBP P,R , k), k ∈ [0, K) (<label>14</label></formula><formula xml:id="formula_24">)</formula><p>where K is the number of LBP pattern values, and I and J are the height and width of the image, respectively. f (x, y) is 1 if x = y and 0 otherwise. The LBP is defined on a gray image, which is treated as a 2D object. To extract the dynamic texture of a 3D object (such as a gray micro-expression video clip), a dynamic LBP description from three orthogonal planes (LBP-TOP) was formed.</p><p>Fig. <ref type="figure" target="#fig_1">2</ref> shows the spatiotemporal volume of a video. It also illustrates the XY plane and the resulting XT and YT planes from a single row and column of the volume. The LBP-TOP description is formed by calculating the LBP features from the planes and concatenating the histograms. Intuitively it can be understood that XT and YT planes encode the vertical and horizontal motion patterns respectively.</p><p>The original LBP operator was based on a circular sampling pattern; however, different radii and neighborhoods can also be used. An elliptic sampling is used for the XT and YT planes:</p><formula xml:id="formula_25">LBP (x c , y c , t c ) = P plane -1 ∑ p=0 s(g p -g c )2 p (<label>15</label></formula><formula xml:id="formula_26">)</formula><p>where g c is the gray value of the center pixel (x c , y c , t c ) and g p are the gray values at the P plane sampling points. s(u) is 1 if u ≥ 0 and 0 otherwise. P plane can be different on each plane. The gray values g p are taken from the sampling point: (x c -R x sin(2πp/P xt ), y c , t c -R t cos(2πp/P xt )) on the XT plane and similarly (x c , y c -R y sin(2πp/P yt ), t c -R t cos(2πp/P yt )) on the YT plane. R d is the radius of the ellipse in the direction of axis d (x, y or t). As the XY plane encodes only the appearance, i.e., both axes have the same meaning, circular sampling is suitable. Values g p for points that do not fall on pixels are estimated using bilinear interpolation. The length of the feature histogram for LBP-TOP is 2 Pxy +2 Pxt +2 Pyt when all three planes are considered.</p><p>For 4D or higher dimensional objects, the LBP can be extended to higher-dimensional space from the tensor viewpoint. From the conceptual formal, given a pixel c in a D dimensional object, its D dimensional LBP is computed by comparing it with its P neighbors p. The neighbors lie on a D-dimensional hyper-sphere with center c and a radius equal to R. However, the conceptual formal is infeasible. In the higher-dimensional space, a large enough number of neighbors P ensures that the maximum local information of c is coded. This means that the length of the LBP code is very long, beyond the capacity of a PC. An additional problem is how to choose the P neighbors on a D-dimensional hyper-sphere such that the P neighbors are evenly distributed.</p><p>To address these problems, we introduce Tensor Orthogonal LBP (TO-LBP). In D-dimensional space, the number of D-1dimensional hyper-planes is D. These D -1-dimensional hyper-planes are orthogonal to each other. In each D -1dimensional hyper-plane, we can find a D -1-dimensional hyper-sphere, with center c and a radius equal to R. Similarly, in each D -1-dimensional space, the number of D -2-dimensional hyper-planes is D -1. These D -2-dimensional hyper-planes are orthogonal to each other. In each D -2dimensional hyper-plane, we can find a D -2-dimensional hyper-sphere, with center c and a radius equal to R. This is a recursive procedure, until the 2-dimensional plane, on which there is a circle with center c and a radius equal to R. Hence, we have D×(D-1)×. . .×3 circles, each of which represents a special direction in D-dimensional space. When D = 3, TO-TOP degenerates into LBP-TOP.</p><p>Although a color micro-expression video clip may be represented as a fourth-order tensor, its mode-4 has only 3 elements. We can therefor use LBP-TOP to extract the dynamic textures from each color component, and then concatenate them as a long feature vector to represent the micro-expression sample. Similar to LBP, LBP-TOP also need to divide the sample into several patches, then code for each patch. In following section, we will design a set of Regions of Interest (ROIs) for coding LBP-TOP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. ACTION UNITS AND REGIONS OF INTEREST</head><p>The Facial Action Coding System (FACS) <ref type="bibr" target="#b35">[36]</ref> is an objective method for quantifying facial movement based on a combination of 38 elementary components. These elementary components comprising 32 action units (AUs) and 6 action descriptors (ADs), can be seen as the phonemes of facial expressions. As words are temporal combinations of phonemes, micro-expression are spatial combinations of AUs. Each AU depicts a local facial movement. We selected a frontal neutral facial image as the template face and divided it into 16 Regions of Interest (ROIs) based on these AUs. Each ROI corresponds to one or more AUs. Fig. <ref type="figure" target="#fig_2">3</ref> shows the template face, the 16 ROIs and the corresponding AUs. Table I also lists the AU number, FACS name and the corresponding ROI according to <ref type="bibr" target="#b35">[36]</ref>.</p><p>In <ref type="bibr" target="#b35">[36]</ref>, the AUs are divided into 6 groups. Group 6 Miscellaneous Actions and Supplementary Codes is not specific to any muscular basis and has no corresponding ROI, the muscular anatomy and muscular action of the other groups are illustrated in Fig. <ref type="figure">4</ref> and Fig. <ref type="figure">5</ref>, which are taken from <ref type="bibr" target="#b35">[36]</ref>. The ROIs are drawn according to the muscular action. In Group 5, AUs were seldom found in micro-expressions, perhaps because they last longer than 500 milliseconds. Thus the ROIs do not take these AUs into account.</p><p>Many of the 16ROIs correspond to multiple AUs with different directions of movement. For example, AU16 and AU20 are contained in ROI R 14 (or R 13 ). The direction of movement of AU16 is vertical (up/down) and that of AU20 is horizontal. So the texture descriptor (LBP or LBP-TOP) on R 14 therefore has more discriminant power to discriminate between AU16 and AU20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. LBP-TOP ON TICS FOR MICRO-EXPRESSION RECOGNITION</head><p>LBP-TOP is a dynamic texture operator and can represent not only appearance information but also motion information. It has already successfully been used for expression recognition <ref type="bibr" target="#b17">[18]</ref> and micro-expression recognition <ref type="bibr" target="#b16">[17]</ref>. However,    only gray video clips were used in these studies. Recent research has shown that the use of color information may significantly improve the discriminative power of LBP <ref type="bibr" target="#b19">[20]</ref>, while other research has shown that expression recognition achieves better performance in perceptual color spaces <ref type="bibr" target="#b29">[30]</ref>.</p><formula xml:id="formula_27">AU2 AU9 AU6 AU11 AU12 AU13 AU14 AU15 AU17 AU16 AU20 AU6 AU10 AU1 AU4 AU7 R 1 R 2 R 3 R 4 R 6 R 5 R 1 5 R 8 R 7 R 1 0 R 9 R 11 R 1 2 R 1 4 R 1 3 R 1 6</formula><p>Motivated by these studies, we propose a novel idea to use LBP-TOP on Tensor Independent Color Space (TICS) for micro-expression recognition. Fig. <ref type="figure">6</ref> shows the level diagram of our method. First, we register the micro-expression video to address the large variations in the spatial appearances of faces. A face in the first frame of each video clip was normalized to a template face by registering 68 facial landmark points detected using the Active Shape Model (ASM) <ref type="bibr" target="#b36">[37]</ref>. The registration transformation is denoted as T . Others frames are transformed by the same T .</p><p>Then, the registered video was normalized. The facial region of each frame was cropped and normalized to 163 × 134 pixels. The frame numbers of each video clip were normalized to I 3 by using linear interpolation. Hence, each video was normalized to a fourth-order tensor X 163×134×I3×3 . Its 4mode includes 3 color components (R, G and B) in RGB color space.</p><p>TICS is performed to transform the 4-mode from RGB into TICS. TICS has three color components T 1 , T 2 and T 3 . Each color component video was divided in 16 ROIs, and an LBP-TOP histogram was calculated from each ROI. The 48 histograms (3 color components, 16 ROIs) were concatenated to form the final vector. Fig. <ref type="figure">7</ref> illustrates the color components in RGB color space and TICS color space. LBP-TOP is used to extract dynamic texture features from the T 1 , T 2 and T 3 components. Fig. <ref type="figure">7</ref> also shows the LBP codes on the XT plane in the color components. The LBP codes of the R, G and B color components are all 01110000, while the LBP codes of the T 1 , T 2 and T 3 color components are 11111000, 00001111 and 1111000, respectively.</p><p>Why are the LBP codes of R, G and B color components usually are the same? The explanation is given as follows. Given a pixel c in the RGB image, its value in the R color components is denoted as g r c , and the values of its neighbors in the R color components are denoted as g r p (p = 0, 1, . . . , 7). In the G color components, the values of the given pixel c and its neighbors are similarly denoted as g g c and g g p (see Fig. <ref type="figure">8</ref>). In an extreme case, we assume that R and G have a linear correlation.</p><formula xml:id="formula_28">g r p g r c = g g p g g c = k p (16)</formula><p>According to Eq. ( <ref type="formula" target="#formula_21">13</ref>), the LBP codes of the given pixel c in color components G and B are</p><formula xml:id="formula_29">LBP r = 7 ∑ p=0 s(g r p -g r c )2 p = 7 ∑ p=0 s((k p -1)g r c )2 p (<label>17</label></formula><formula xml:id="formula_30">)</formula><p>and</p><formula xml:id="formula_31">LBP g = 7 ∑ p=0 s(g g p -g g c )2 p = 7 ∑ p=0 s((k p -1)g g c )2 p . (<label>18</label></formula><formula xml:id="formula_32">)</formula><p>Because g r c ≥ 0 and g r c ≥ 0, we have</p><formula xml:id="formula_33">s((k p -1)g r c ) = s(k p -1)<label>(19)</label></formula><p>and s((k p -1)g g c ) = s(k p -1).</p><p>Hence, LBP r = LBP g , which means that the LBP codes of the given pixel c in color components G and B are the same. However, Eq. ( <ref type="formula">16</ref>) does not always hold. We assume that</p><formula xml:id="formula_35">g r p g r c = k r p and g g p g g c = k g p . (<label>21</label></formula><formula xml:id="formula_36">)</formula><p>Hence, we have</p><formula xml:id="formula_37">LBP r = 7 ∑ p=0 s(k r p -1)2 p (<label>22</label></formula><formula xml:id="formula_38">)</formula><p>and</p><formula xml:id="formula_39">LBP g = 7 ∑ p=0 s(k g p -1)2 p . (<label>23</label></formula><formula xml:id="formula_40">)</formula><p>If the conditions k r p ≥ 1 and k g p ≥ 1 or k r p &lt; 1 and k g p &lt; 1 are met at same time, we also obtain LBP r = LBP g .</p><p>What is the probability that these conditions are met? An investigation was performed using the CASME and CASME 2 databases. In CASME, the probability that the conditions are met in color components R and G is 89.01%, and in CASME 2, the probability is 88.98%.</p><p>We use similar method to investigate the probabilities that the conditions are met in each color component pairs in TICS, CIELab, CIELuv and RGB. Table <ref type="table" target="#tab_1">II</ref> shows the probabilities that the conditions are met in each color component pairs. Based on the table, we can see that the probability that the LBP codes of the three color components are the same is over 85% in RGB color space. Hence, the combination of the LBP of R, G and B color components can not significantly improve the final performance. In TICS, there is at least one color component pair for which the probability of the conditions being met at the same time is very low, which means the probability of the LBP codes in the two color components being the same is very low. Hence, the combination of the LBP of the color components in TICS may significantly improve upon the final performance. The mutual information of two random variables is a measure of their mutual dependence. Formally, the mutual IEEE TRANSACTIONS ON IMAGE PROCESSING information of two discrete random variables X and Y can be defined as:</p><formula xml:id="formula_41">I(X; Y ) = ∑ y∈Y ∑ x∈X p(x, y) log( p(x, y) p(x)p(y) ) (<label>24</label></formula><formula xml:id="formula_42">)</formula><p>where p(x, y) is the joint probability distribution function of X and Y , and p(x) and p(y) are the marginal probability distribution functions of X and Y respectively. The larger I(X; Y ) is, the more sharing information between X and Y there will be. This means that the combination of X and Y provides less additional information. A face image is randomly picked from CASME 2, and the mutual information is calculated from each pair of color components.</p><formula xml:id="formula_43">I(LBP r ; LBP g ) = I(LBP g ; LBP b ) = I(LBP b ; LBP r ) = 0.</formula><p>42, I(LBP T1 ; LBP T2 ) = I(LBP T2 ; LBP T3 ) = 0 and I(LBP T1 ; LBP T3 ) = 0.38. Hence, the LBP features in TICS are more independent than RGB space such that the performance in TICS superior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. CASME</head><p>The Chinese Academy of Sciences Micro-Expression (CASME) database <ref type="bibr" target="#b37">[38]</ref> includes 195 spontaneous facial micro-expressions recorded by two 60 fps cameras. The samples were selected from more than 1500 facial expressions. The selected micro-expressions had either a total duration of less than 500 ms or an onset duration (time from onset frame to apex frame<ref type="foot" target="#foot_3">3</ref> ) of less than 250 ms. These samples are coded with the onset, apex and offset frames, and tagged with action units (AUs) <ref type="bibr" target="#b38">[39]</ref>. In this database, micro-expressions are labeled into seven categories (happiness, surprise, disgust, fear, sadness, repression and tense). Fig. <ref type="figure" target="#fig_5">9</ref> is an example.</p><p>The CASME database is divided into two classes: Set A and Set B. The samples in Set A were recorded with a BenQ M31 consumer camera with 60fps, with the resolution set to 1280 × 720 pixels. The participants were recorded in natural light. The samples in Set B were recorded with a Point Grey GRAS-03K2C industrial camera with 60fps, with the resolution set to 640 × 480 pixels. The participants were recorded in a room with two LED lights. Industrial cameras may capture the subtler movements of micro-expressions with higher frame rate, but the color depth is no more than 16-bit, which is far lower than that of consumer cameras. Therefore, Set B is used in the following experiments.</p><p>In the experiments, we merged the seven categories into four classes. Such a classification may be more easily applied in practice. Positive (4 samples) contains happy microexpression, which indicates "good" emotions in the individual. Negative (47 samples) contains disgust, sadness and fear, which are usually considered "bad" emotions. Surprise (13 samples) usually occurs when there is a difference between expectations and reality and can be neutral/moderate, pleasant, unpleasant, positive, or negative. Tense and repression indicate the ambiguous feelings of an individual and require further inference, so they were categorized in the Other class (33 samples). We selected 97 samples <ref type="foot" target="#foot_4">4</ref>  used as the test set once and the final recognition accuracy is calculated based on all of the results. Of the 97 samples, the frame number of the shortest sample is 10 and that of the longest sample is 68. The frame numbers of all samples are normalized to 70 using linear interpolation. Hence, each sample was normalized to a fourth-order tensor with a size of 163 × 134 × 70 × 3.</p><p>In the experiments, we compared the micro-expression recognition rates in TICS, RGB, gray, and two perceptual color spaces <ref type="bibr" target="#b29">[30]</ref>: CIELuv and CIELab. For the gray color space, we extracted LBP-TOP to represent the dynamic texture features for each ROI and built histograms. Then, the histograms were concatenated into a vector as an input for the classifier. A support vector machine (SVM) classifier was selected and used the linear kernel as the kernel function. For the TICS, CIELuv, CIELab, and RGB color spaces, we used the same method to build the LBP-TOP histograms and concatenate them into a vector for each color component. The vectors were concatenated to a long vector as an input for SVM. For LBP-TOP, the radii in axes X and Y (be marked as R x and R y ) were set as 1 and the radii in axies T (marked as R t ) was assigned various values from 2 to 4. The number of neighboring points (marked as P ) in the XY, XT and YT planes were all set to 4 and 8. The uniform pattern and the basic LBP were used in LBP coding. The results are shown in Table <ref type="table" target="#tab_4">III</ref>.</p><p>From the table, we can see that the performances in the TICS color space is the best in most cases. When R t = 2, CIELab achieves the best performances in the first three cases, but the performances of TICS is better than those of RGB and gray. We can see that the performance of the uniform pattern is the same as that of the basic LBP in most cases, although its code length is far shorter. In addition, the accuracies with P = 8 are not better than the accuracies with P = 4 in many cases. Therefore, we used the uniform pattern and set P as 4 in the following experiments. Fig. <ref type="figure" target="#fig_6">10</ref> shows five confusion matrices of TICS, CIELuv, CIELab, RGB, and GRAY in Set B of CASME. No Positive sample is misrecognized as Negative, and no Negative sample is misrecognized as Positive. According to the field of psychology, Positive and Negative expressions are opposites. Positive facial expressions usually have distinct differences in their appearance from Negative facial expressions. For happiness, AU6 and AU12 are linked to upward movements of the mouth corner, while for Negative facial expressions such as disgust, fear and sadness, the mouth corners move downward (such as AU16) and/or there are movements of the eye-brows (such as AU 4) or chin (such as <ref type="bibr">AU 17)</ref>. Surprise, however, can be positive, negative, or neutral, depending on different situations, which makes it more likely to be misrecognized as other categories.</p><p>The recognition accuracy of each class of micro-expression (except for Positive in TICS) is higher in the TICS, CIELuv, and CIELab color spaces than in the RGB color space. For Others, TICS has the highest recognition accuracy 51.52%. For Negative, TICS and CIELuv achieved better performances than CIELab. For Positive, however, TICS achieved a worse performance than CIELuv and CIELab. From the figure, we can see that the recognition accuracy of Positive is lower in RGB than in gray because the number of Positive samples was too small (only 4 samples).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. CASME2</head><p>The CASME2 <ref type="bibr" target="#b39">[40]</ref> database includes 255 spontaneous facial micro-expressions recorded by two 200 fps cameras. These samples were selected from more than 2,500 facial expressions. Compared with CASME, this spontaneous microexpression database is improved in its increased sample size, fixed illumination, and higher resolution (both temporal and spatial). This database selected micro-expressions either with a total duration of less than 500 ms or an onset duration (time from the onset frame to apex frame) of less than 250 ms. These samples are coded with the onset and offset frames ans tagged with action units (AUs) and emotions. Fig. <ref type="figure" target="#fig_8">11</ref> is an example. In this database, micro-expressions are labeled into seven categories (happiness, surprise, disgust, fear, sadness, repression and tense).</p><p>We also merged the seven categories into four classes: Positive (32 samples), Negative (66 samples), Surprise (25 samples), and Others (129 samples). The LOSO crossvalidation mentioned in the previous experiment is also used in this experiment. To address the large variations in the    spatial appearances of micro-expressions, we used the same transformation method as in the previous experiments. In the samples, the frame number of the shortest sample is 24, and that of the longest sample is 146. The frame numbers of all samples are normalized to 150 by linear interpolation. The size of each frame is normalized to 163 × 134 pixels. Therefore, each sample was normalized to a fourth-order tensor with a size of 163 × 134 × 150 × 3.</p><p>To estimate the performance of micro-expression recognition in TICS, CIELuv, and CIELab color spaces, we compared them with RGB and gray color spaces. The radii in axes X and Y were assigned various values from 1 to 4. To avoid too many combinations of parameters, we made R x = R y . The radius in axis T was assigned various values from 2 to 4. The number of neighboring points in the XY, XT and YT planes was set as 4. A uniform pattern was used in LBP coding. The other settings are the same as in previous experiments. The results are listed in Table <ref type="table" target="#tab_7">IV</ref>.</p><p>From the table, the performances in the TICS, CIELuv, and CIELab color spaces are better than those of RGB and GRAY in most cases. Both TICS and CIELuv reach the highest recognition accuracy 62.30%. The amount of information in the RGB color space is three times as much as that in gray. However, the accuracy in the RGB color space is sometimes (for example, in R x = 2, R y = 2, R t = 2 cases) worse than that in gray. This is due to the large amount of redundant information in the RGB color space in general, which is an obstruction of the further improvement in accuracy in the RGB color space. As the redundancy is removed from the TICS color space, the accuracy is better in general.</p><p>Fig. <ref type="figure" target="#fig_9">12</ref> shows the five confusion matrices of TICS, CIELuv, CIELab, RGB, and GRAY in CASME 2. Compared with RGB and GRAY, the recognition accuracies of Positive, Negative and Others in TICS, CIELuv, and CIELab color spaces are improved. The recognition accuracy of Others in TICS achieves a highest value of 72.09%, the recognition accuracy of Positive in CIELab achieves a highest value of 53.13%, and the recognition accuracy of Negative in CIELuv achieves a highest value of 57.58%.</p><p>The classical descriptors based on LBP are only applied on gray images. Color information, however, may significantly improve the discriminative power of descriptors. There exist two main methods to combine color and texture cues to improve the discriminative power <ref type="bibr" target="#b40">[41]</ref> <ref type="bibr" target="#b41">[42]</ref>.</p><p>Early Fusion: Early fusion involves combining color and texture cues at the pixel level. A common way is to compute the texture descriptors on the color channels and then to concatenate them.</p><formula xml:id="formula_44">T E = [T R , T G , T B ]<label>(25)</label></formula><p>where T can be any texture descriptor. Late Fusion: Late fusion involves combining color and texture cues at the image level. The color and texture cues are processed independently. The two histograms are then concatenated into a single representation.</p><formula xml:id="formula_45">T L = [H T , H C ]<label>(26)</label></formula><p>where H T and H C are explicit texture and color histograms.</p><p>The proposed method belongs to early fusion. Following, we also use later fusion in CASME 2. In the experiments, H T is the histogram of LBP-TOP on gray video. For RGB color space, we use the RGB designated as H C . The RGB histogram <ref type="bibr" target="#b42">[43]</ref> is a combination of three 1D histograms based on the R, G the and B color components of the RGB color space. Each histogram is normalized to [0, 1]. For TICS color space, the values of TICS are normalized to [0, 255]. Three histograms are calculated from the T 1 , T 2 and T 3 color components and are then concatenated into H C . Table V lists the recognition accuracies of early fusion and late fusion, in which the recognition accuracies of early fusion are from Table <ref type="table" target="#tab_7">IV</ref>. From the table, we can see that in most cases the recognition accuracies of early fusion are higher than those of late fusion. To investigate the mutual information among the three components in these color spaces, we calculate the LBP-TOP codes for each component of the micro-expression video clips. Then, the mutual information based on these LBP-TOP codes is calculated. Each sample then has three mutual information values: components 1 and 2, components 2 and 3, components 3 and 1. We plot these values in Fig. <ref type="figure" target="#fig_11">13</ref>. From the figure, the mutual information values in TICS, CIELuv, and CIELab are smaller than those in RGB color space. This explains why the performances of TICS, CIELuv, and CIELab are better than that of RGB.</p><p>We also use the template face as the target image, and produce scatter plots of more than 5,000 randomly chosen data points in the four color spaces. Fig. <ref type="figure" target="#fig_13">14</ref> depicts these scatter plots, which show three pairs of axes plotted against each other. The data points are decorrelated if the data are axis-aligned. The RGB color space shows an almost complete correlation between all pairs of axes because of the data cluster around a line with a 45-degree slope. The TICS color space shows that the correlation between T 2 and T 3 is small. Based on this observation, it is deduced that the combination of T 2 and T 3 maybe achieve better performance.</p><p>To verify this assumption, the same experiment was conducted on all pairs of components in TICS.     (T 2 , T 3 ) is the best. Comparing the combination of (T 2 , T 3 ) with the combination of (T 1 , T 2 , T 3 ) (see Table <ref type="table" target="#tab_7">IV</ref>), we find that their results are almost the same. However, the combination of (T 2 , T 3 ) has a lower costs of storage and higher efficiency.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION</head><p>We have presented a novel color space, Tensor Independent Color Space (TICS) to recognize micro-expression. In TICS, the three color components are as independent from each other as possible. The combination of LBP codes in TICS is thus more effective than that in RGB, and we used the mutual information to explain this. For the locality of LBP, we designed a set of ROIs based on action units such a result. The ROIs can remove some noises such as the nose tip. In this paper, we also showed that the performance of microexpression recognition is better in the two perceptual color spaces. The experiments on two micro-expression databases revealed that the performances in TICS, CIELuv, and CIELab are better than those in RGB or gray, because their components are as independent from each other as possible.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example of a basic LBP operator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of a spatiotemporal volume of a video, the XY plane (original frames) and the resulting temporal planes for LBP feature extraction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Template face and 16 ROIs [1].</figDesc><graphic coords="5,391.91,72.01,89.34,81.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .Fig. 5 .Fig. 6 .</head><label>456</label><figDesc>Fig. 4. Muscular Anatomy [36]. The numbers in the figures are the AU numbers. (a) Upper Face Action Units; (b) Lower Face Action Units -Up/Down Actions; (c) Lower Face Action Units -Horizontal Actions; (d) Lower Face Action Units -Oblique Actions; (e) Lower Face Action Units -Orbital Actions.</figDesc><graphic coords="6,60.51,63.15,95.35,141.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Fig. 7. Illustration of R, G, and B color components, the various components generated by TICS and the corresponding LBP-TOP codes on the XT plane [1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. A demonstration of the frame sequence of a micro-expression in CASME. The AU number for this micro-expression is 15, which indicates Lip Corner Depressor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Five confusion matrices of TICS, CIELuv, CIELab, RGB, and GRAY in Set B of CASME.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. A demonstration of the frame sequence of a micro-expression in CASME 2. The AU number for this micro-expression is 4, which indicates Brow Lowerer. The three rectangles above the images show the right inner brow (AU4) in zoom in mode.</figDesc><graphic coords="10,330.99,591.93,86.34,64.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Five confusion matrices of TICS, CIELuv, CIELab, RGB, and GRAY in CASME 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>ut ua l in fo rm at io n be tw ee n co m po ne nt s l info rm atio n bet we en com pon ent s 1 and 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 13 .</head><label>13</label><figDesc>Fig.<ref type="bibr" target="#b12">13</ref>. Mutual information between the three color components of TICS, CIELuv, CIELab, and RGB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Scatter plots made in TICS, CIELuv, CIELab, and RGB color spaces. Note that the degree of correlation in these plots is defined by the angle of rotation of the mean axis of the point clouds, with rotations of 0 or 90 degrees indicating uncorrelated data and in-between values indicate various degrees of correlation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I AU</head><label>I</label><figDesc>NUMBER, FACS NAME AND CORRESPONDING ROI</figDesc><table><row><cell></cell><cell>Group 1: Upper Face Action Units</cell><cell></cell></row><row><cell cols="2">AU Number FACS Name</cell><cell>Region of Interest</cell></row><row><cell>AU1</cell><cell>Inner Brow Raiser</cell><cell>R 1 , R 2</cell></row><row><cell>AU2</cell><cell>Outer Brow Raiser</cell><cell>R 3 , R 4</cell></row><row><cell>AU4</cell><cell>Brow Lowerer</cell><cell>R 1 , R 2</cell></row><row><cell>AU5</cell><cell>Upper Lid Raiser</cell><cell>No ROI</cell></row><row><cell>AU7</cell><cell>Lid Tightener</cell><cell>R 5 , R 6</cell></row><row><cell>AU6</cell><cell>Cheek Raiser and Lid Compressor</cell><cell>R 7 , R 8 , R 9 , R 10</cell></row><row><cell>AU43</cell><cell>Eye Closure -Optional</cell><cell>No ROI</cell></row><row><cell>AU45</cell><cell>Blink -Optional</cell><cell>No ROI</cell></row><row><cell>AU46</cell><cell>Wink -Optional</cell><cell>No ROI</cell></row><row><cell cols="3">Group 2: Lower Face Action Units -Up/Down Actions</cell></row><row><cell cols="2">AU Number FACS Name</cell><cell>Region of Interest</cell></row><row><cell>AU9</cell><cell>Nose Wrinkler</cell><cell>R 15</cell></row><row><cell>AU10</cell><cell>Upper Lip Raiser</cell><cell>R 9 , R 10</cell></row><row><cell>AU17</cell><cell>Chin Raiser</cell><cell>R 16</cell></row><row><cell>AU15</cell><cell>Lip Corner Depressor</cell><cell>R 11 , R 12</cell></row><row><cell>AU25</cell><cell>Lips Part</cell><cell>No ROI</cell></row><row><cell>AU26</cell><cell>Jaw Drop</cell><cell>No ROI</cell></row><row><cell>AU27</cell><cell>Mouth Stretch</cell><cell>No ROI</cell></row><row><cell>AU16</cell><cell>Lower Lip Depressor</cell><cell>R 13 , R 14</cell></row><row><cell cols="3">Group 3: Lower Face Action Units -Horizontal Actions</cell></row><row><cell cols="2">AU Number FACS Name</cell><cell>Region of Interest</cell></row><row><cell>AU20</cell><cell>Lip Stretcher</cell><cell>R 13 , R 14</cell></row><row><cell>AU14</cell><cell>Dimpler</cell><cell>R 11 , R 12</cell></row><row><cell></cell><cell cols="2">Group 4: Lower Face Action Units -Oblique Actions</cell></row><row><cell cols="2">AU Number FACS Name</cell><cell>Region of Interest</cell></row><row><cell>AU11</cell><cell>Nasolabial Furrow Deepener</cell><cell>R 11 , R 12</cell></row><row><cell>AU12</cell><cell>Lip Corner Puller</cell><cell>R 11 , R 12</cell></row><row><cell>AU13</cell><cell>Sharp Lip Puller</cell><cell>R 11 , R 12</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2015.2496314, IEEE Transactions on Image Processing</figDesc><table /><note><p>1057-7149 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. WANG et al.: MICRO-EXPRESSION RECOGNITION USING COLOR SPACES</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III MICRO</head><label>III</label><figDesc>-EXPRESSION RECOGNITION ACCURACIES (%) IN GRAY, RGB AND TICS COLOR SPACES IN SET B OF CASME.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TICS</cell><cell cols="3">CIELuv CIELab</cell><cell>RGB</cell><cell>GRAY</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">P = 4, uniform pattern 57.73</cell><cell>58.76</cell><cell>61.86</cell><cell></cell><cell>52.58</cell><cell>51.55</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Rt = 2</cell><cell cols="3">P = 4, basic LBP P = 8, uniform pattern 59.79 57.73</cell><cell>58.76 59.79</cell><cell>61.86 60.82</cell><cell></cell><cell>52.58 54.64</cell><cell>51.55 52.58</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">P = 8, basic LBP</cell><cell>59.79</cell><cell>59.79</cell><cell>58.76</cell><cell></cell><cell>53.61</cell><cell>51.55</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">P = 4, uniform pattern 61.86</cell><cell>61.86</cell><cell>58.76</cell><cell></cell><cell>55.67</cell><cell>53.61</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Rt = 3</cell><cell cols="3">P = 4, basic LBP P = 8, uniform pattern 61.86 61.86</cell><cell>61.86 55.67</cell><cell>58.76 59.79</cell><cell></cell><cell>55.67 54.64</cell><cell>53.61 54.64</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">P = 8, basic LBP</cell><cell>60.82</cell><cell>59.79</cell><cell>56.70</cell><cell></cell><cell>54.64</cell><cell>54.64</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">P = 4, uniform pattern 60.82</cell><cell>60.82</cell><cell>58.76</cell><cell></cell><cell>54.64</cell><cell>54.64</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Rt = 4</cell><cell cols="3">P = 4, basic LBP P = 8, uniform pattern 57.73 60.82</cell><cell>60.82 54.64</cell><cell>58.76 59.79</cell><cell></cell><cell>54.64 57.73</cell><cell>54.64 54.64</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">P = 8, basic LBP</cell><cell>60.82</cell><cell>59.79</cell><cell>60.82</cell><cell></cell><cell>55.67</cell><cell>54.64</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Predicted</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TICS</cell><cell></cell><cell></cell><cell cols="2">CIELuv</cell><cell></cell><cell></cell><cell></cell><cell cols="2">CIELab</cell></row><row><cell></cell><cell></cell><cell cols="13">Positive Negative Surprise Others Positive Negative Surprise Others Positive Negative Surprise Others</cell></row><row><cell></cell><cell>Positive</cell><cell>25.00</cell><cell>0</cell><cell>0</cell><cell>75.00</cell><cell>50.00</cell><cell>0</cell><cell>0</cell><cell cols="2">50.00</cell><cell>50.00</cell><cell>0</cell><cell>0</cell><cell>50.00</cell></row><row><cell>Ground</cell><cell>Negative</cell><cell>0</cell><cell>80.85</cell><cell>0</cell><cell>19.15</cell><cell>0</cell><cell>80.85</cell><cell>0</cell><cell cols="2">19.15</cell><cell>0</cell><cell>78.72</cell><cell>2.13</cell><cell>19.15</cell></row><row><cell>Truth</cell><cell>Surprise</cell><cell>15.38</cell><cell>23.08</cell><cell>30.77</cell><cell>30.77</cell><cell>7.69</cell><cell>23.08</cell><cell>30.77</cell><cell cols="2">38.46</cell><cell>15.38</cell><cell>23.08</cell><cell>30.77</cell><cell>30.77</cell></row><row><cell></cell><cell>Others</cell><cell>9.09</cell><cell>24.24</cell><cell>15.15</cell><cell>51.52</cell><cell>9.09</cell><cell>33.33</cell><cell>9.09</cell><cell cols="2">48.48</cell><cell>9.09</cell><cell>33.33</cell><cell>9.09</cell><cell>48.48</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">RGB</cell><cell></cell><cell></cell><cell>GRAY</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="9">Positive Negative Surprise Others Positive Negative Surprise Others</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Positive</cell><cell>25.00</cell><cell>0</cell><cell>0</cell><cell>75.00</cell><cell>50.00</cell><cell>0</cell><cell>0</cell><cell cols="2">50.00</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ground</cell><cell>Negative</cell><cell>0</cell><cell>78.72</cell><cell>2.13</cell><cell>19.15</cell><cell>0</cell><cell>74.47</cell><cell>4.26</cell><cell cols="2">21.28</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Truth</cell><cell>Surprise</cell><cell>7.69</cell><cell>38.46</cell><cell>23.08</cell><cell>30.77</cell><cell>7.69</cell><cell>30.77</cell><cell>23.08</cell><cell cols="2">38.46</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Others</cell><cell>12.12</cell><cell>45.45</cell><cell>9.09</cell><cell>33.33</cell><cell>12.12</cell><cell>42.42</cell><cell>12.12</cell><cell cols="2">33.33</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Table VI lists the experimental results. In most cases, the combination of</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE IV MICRO</head><label>IV</label><figDesc>-EXPRESSION RECOGNITION ACCURACIES (%) IN TICS RGB AND GRAY COLOR SPACES IN CASME 2.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Parameters</cell><cell></cell><cell>TICS</cell><cell cols="2">CIELuv CIELab</cell><cell>RGB</cell><cell>GRAY</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Rx = 1, Ry = 1, Rt = 2</cell><cell>56.75</cell><cell>57.54</cell><cell>59.52</cell><cell>58.33</cell><cell>54.37</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Rx = 1, Ry = 1, Rt = 3</cell><cell>58.73</cell><cell>58.73</cell><cell>59.13</cell><cell>56.35</cell><cell>55.16</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Rx = 1, Ry = 1, Rt = 4</cell><cell>61.90</cell><cell>59.13</cell><cell>59.92</cell><cell>58.73</cell><cell>55.95</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Rx = 2, Ry = 2, Rt = 2</cell><cell>59.92</cell><cell>59.92</cell><cell>59.52</cell><cell>55.95</cell><cell>56.35</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Rx = 2, Ry = 2, Rt = 3</cell><cell>61.11</cell><cell>59.92</cell><cell>59.92</cell><cell>57.54</cell><cell>55.16</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Rx = 2, Ry = 2, Rt = 4</cell><cell>62.30</cell><cell>58.33</cell><cell>61.11</cell><cell>59.52</cell><cell>56.75</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Rx = 3, Ry = 3, Rt = 2</cell><cell>53.97</cell><cell>59.92</cell><cell>57.94</cell><cell>54.76</cell><cell>51.59</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Rx = 3, Ry = 3, Rt = 3</cell><cell>55.16</cell><cell>60.32</cell><cell>59.13</cell><cell>56.35</cell><cell>55.14</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Rx = 3, Ry = 3, Rt = 4</cell><cell>56.75</cell><cell>61.90</cell><cell>59.13</cell><cell>59.52</cell><cell>56.75</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Rx = 4, Ry = 4, Rt = 2</cell><cell>58.33</cell><cell>58.33</cell><cell>55.56</cell><cell>56.35</cell><cell>51.59</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Rx = 4, Ry = 4, Rt = 3</cell><cell>58.33</cell><cell>60.71</cell><cell>57.53</cell><cell>53.57</cell><cell>50.79</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Rx = 4, Ry = 4, Rt = 4</cell><cell>57.54</cell><cell>62.30</cell><cell>58.33</cell><cell>55.16</cell><cell>55.16</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Predicted</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>TICS</cell><cell></cell><cell></cell><cell></cell><cell cols="2">CIELuv</cell><cell></cell><cell></cell><cell></cell><cell>CIELab</cell></row><row><cell></cell><cell></cell><cell cols="12">Positive Negative Surprise Others Positive Negative Surprise Others Positive Negative Surprise Others</cell></row><row><cell></cell><cell>Positive</cell><cell>50.00</cell><cell>9.38</cell><cell>6.25</cell><cell>34.38</cell><cell>43.75</cell><cell>6.25</cell><cell>6.25</cell><cell cols="2">43.75 53.13</cell><cell>12.50</cell><cell>6.25</cell><cell>28.13</cell></row><row><cell>Ground</cell><cell>Negative</cell><cell>7.58</cell><cell>54.55</cell><cell>6.06</cell><cell>31.82</cell><cell>6.06</cell><cell>57.58</cell><cell>4.55</cell><cell cols="2">31.82 6.06</cell><cell>53.03</cell><cell>6.06</cell><cell>34.85</cell></row><row><cell>Truth</cell><cell>Surprise</cell><cell>20.00</cell><cell>16.00</cell><cell>48.00</cell><cell>16.00</cell><cell>8.00</cell><cell>12.00</cell><cell>48.00</cell><cell cols="2">32.00 12.00</cell><cell>16.00</cell><cell>48.00</cell><cell>24.00</cell></row><row><cell></cell><cell>Others</cell><cell>7.75</cell><cell>17.83</cell><cell>2.33</cell><cell>72.09</cell><cell>11.63</cell><cell>20.93</cell><cell>3.10</cell><cell cols="2">64.34 8.53</cell><cell>18.60</cell><cell>3.10</cell><cell>69.77</cell></row><row><cell></cell><cell></cell><cell></cell><cell>RGB</cell><cell></cell><cell></cell><cell></cell><cell cols="2">GRAY</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="8">Positive Negative Surprise Others Positive Negative Surprise Others</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Positive</cell><cell>40.63</cell><cell>9.38</cell><cell>3.13</cell><cell>46.88</cell><cell>40.63</cell><cell>12.50</cell><cell>6.25</cell><cell>40.63</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ground</cell><cell>Negative</cell><cell>6.06</cell><cell>54.55</cell><cell>3.03</cell><cell>36.36</cell><cell>6.06</cell><cell>50.00</cell><cell>4.55</cell><cell>39.39</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Truth</cell><cell>Surprise</cell><cell>8.00</cell><cell>24.00</cell><cell>60.00</cell><cell>8.00</cell><cell>8.00</cell><cell>16.00</cell><cell>60.00</cell><cell>16.00</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Others</cell><cell>10.85</cell><cell>18.60</cell><cell>3.88</cell><cell>66.67</cell><cell>13.18</cell><cell>20.93</cell><cell>2.33</cell><cell>63.57</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VI MICRO</head><label>VI</label><figDesc>-EXPRESSION RECOGNITION ACCURACIES (%) OF THREE DIFFERENT COMBINATIONS OF COMPONENT PAIRS IN TICS.</figDesc><table><row><cell>Components</cell><cell>T 1 , T 2</cell><cell>T 2 , T 3</cell><cell>T 1 , T 3</cell></row><row><cell>Rx = 1, Ry = 1, Rt = 2</cell><cell>53.97</cell><cell>56.75</cell><cell>50.40</cell></row><row><cell>Rx = 1, Ry = 1, Rt = 3</cell><cell>54.76</cell><cell>58.73</cell><cell>51.19</cell></row><row><cell>Rx = 1, Ry = 1, Rt = 4</cell><cell>52.38</cell><cell>61.51</cell><cell>53.17</cell></row><row><cell>Rx = 2, Ry = 2, Rt = 2</cell><cell>56.75</cell><cell>59.92</cell><cell>51.19</cell></row><row><cell>Rx = 2, Ry = 2, Rt = 3</cell><cell>57.54</cell><cell>61.51</cell><cell>52.78</cell></row><row><cell>Rx = 2, Ry = 2, Rt = 4</cell><cell>55.95</cell><cell>62.30</cell><cell>50.79</cell></row><row><cell>Rx = 3, Ry = 3, Rt = 2</cell><cell>57.14</cell><cell>53.97</cell><cell>55.16</cell></row><row><cell>Rx = 3, Ry = 3, Rt = 3</cell><cell>53.97</cell><cell>55.16</cell><cell>57.54</cell></row><row><cell>Rx = 3, Ry = 3, Rt = 4</cell><cell>55.56</cell><cell>56.75</cell><cell>56.35</cell></row><row><cell>Rx = 4, Ry = 4, Rt = 2</cell><cell>55.95</cell><cell>58.33</cell><cell>56.75</cell></row><row><cell>Rx = 4, Ry = 4, Rt = 3</cell><cell>55.95</cell><cell>58.73</cell><cell>55.56</cell></row><row><cell>Rx = 4, Ry = 4, Rt = 4</cell><cell>54.37</cell><cell>57.54</cell><cell>56.75</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Contempt was added besides the basic six emotions</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>For ICA operations, we used Hyvarinen's fixed-point algorithmhttp://www. cis.hut.fi/projects/ica/fastica/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2"><p>IEEE TRANSACTIONS ON IMAGE PROCESSING</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>The onset is the first frame that changes from the baseline (usually a neutral facial expression). The apex is the frame that reaches the highest intensity of the facial expression. The offset is the last frame of the expression (before turning back to a neutral facial expression). Occasionally, a facial expression faded very slowly, and the changes between frames are very difficult to detect by eyes. For such offset frames, the coders only coded the last clear frame as the offset frame while ignoring the nearly imperceptible changing frames.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>There is a sample with 122 frames. It was removed.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_5"><p>WANG et al.: MICRO-EXPRESSION RECOGNITION USING COLOR SPACES</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by grants from the National Natural Science Foundation of China (61379095, 61375009, 61472138, 31500875, 61332017), the Beijing Natural Science Foundation (4152055), the Open Projects Program of National Laboratory of Pattern Recognition (201306295), the open project program of AU Number FACS Name Region of Interest AU18 Lip Pucker No ROI AU22 Lip Funneler No ROI AU23 Lip Tightener No ROI AU24 Lip Presser No ROI AU28 Lips Suck No ROI Group 6: Miscellaneous Actions and Supplementary Codes AU Number FACS Name Region of Interest AU8+25 Lips Toward Each Other No ROI AU21 Neck Tightener No ROI AU31 Jaw Clencher No ROI AU38 Nostril Dilator No ROI AU39 Nostril Compressor No ROI</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Micro-expression recognition using dynamic textures on tensor independent color space</title>
		<author>
			<persName><forename type="first">S.-J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition (ICPR), 2014 22nd International Conference on</title>
		<imprint>
			<date type="published" when="2014-08">Aug 2014</date>
			<biblScope unit="page" from="4678" to="4683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Nonverbal leakage and clues to deception</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Friesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DTIC Document, Tech. Rep</title>
		<imprint>
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Lie catching and microexpressions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="118" to="133" />
		</imprint>
	</monogr>
	<note>The Philosophy of Deception</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">I see how you. feel: Training laypeople and professionals to recognize fleeting emotions</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Herbasz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K K</forename><surname>Sinuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the Annual Meeting of the International Communication Association</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Police lie detection accuracy: The effect of lie scenario</title>
		<author>
			<persName><forename type="first">M</forename><surname>Osullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hurley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tiwana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Law and Human Behavior</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="530" to="538" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Behavior and security</title>
		<author>
			<persName><forename type="first">M</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Maccario</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Govindaraju</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Greenwood Pub Group</publisher>
			<biblScope unit="page" from="86" to="106" />
			<pubPlace>Santa Barbara, California</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic facial expression recognition: Feature extraction and selection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lajevardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal, Image and Video Processing</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="159" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Methods of Research in Psychotherapy</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Haggard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Isaacs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1966">1966</date>
			<publisher>Appleton-Century-Crofts</publisher>
			<biblScope unit="page" from="154" to="165" />
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>ch. Micromomentary facial expressions as indicators of ego mechanisms in psychotherapy</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Darwin, deception, and facial expression</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of the New York Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">1000</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="205" to="221" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Airport security: Intent to deceive</title>
		<author>
			<persName><forename type="first">S</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">465</biblScope>
			<biblScope unit="issue">7297</biblScope>
			<biblScope unit="page" from="412" to="415" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">How fast are the leaked facial expressions: The duration of micro-expressions</title>
		<author>
			<persName><forename type="first">W.-J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Nonverbal Behavior</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Evidence for training the ability to read microexpressions of emotion</title>
		<author>
			<persName><forename type="first">D</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Motivation and Emotion</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="181" to="191" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reading between the lies</title>
		<author>
			<persName><forename type="first">S</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Ten</forename><surname>Brinke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">508</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Microexpression training tool (METT)</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<pubPlace>San Francisco</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of California</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Facial micro-expressions recognition using high speed camera and 3D-gradient descriptor</title>
		<author>
			<persName><forename type="first">S</forename><surname>Polikovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kameda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ohta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Crime Detection and Prevention</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Face recognition and micro-expression based on discriminant tensor subspace analysis plus extreme learning machine</title>
		<author>
			<persName><forename type="first">S.-J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Processing Letters</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recognising spontaneous facial micro-expressions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th IEEE International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1449" to="1456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dynamic texture recognition using local binary patterns with an application to facial expressions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="915" to="928" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Enhanced local texture feature sets for face recognition under difficult lighting conditions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1635" to="1650" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>Image Processing</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Image region description using orthogonal combination of local binary patterns enhanced with color information</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-E</forename><surname>Bichot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Using color texture sparsity for facial expression recognition</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Ro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Plataniotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face and Gesture Recognition (FG), 2013 10th IEEE International Conference and Workshops on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Blue or red? exploring the effect of color on cognitive task performances</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">5918</biblScope>
			<biblScope unit="page" from="1226" to="1229" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Face verification on color images using local features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, 2008. CVPRW &apos;08</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning the uncorrelated, independent, and discriminating color spaces for face recognition</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="213" to="222" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">ICA color space for pattern recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chengjun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="248" to="257" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Color image discriminant models and algorithms for face recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2088" to="2098" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Tensor discriminant color space for face recognition</title>
		<author>
			<persName><forename type="first">S.-J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-G</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2490" to="2501" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sparse tensor discriminant color space for face verification</title>
		<author>
			<persName><forename type="first">S.-J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-G</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="876" to="888" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Overview of the face recognition grand challenge</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Scruggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Worek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Computer Society Conf. Computer Vision and Pattern Recognition CVPR 2005</title>
		<meeting>IEEE Computer Society Conf. Computer Vision and Pattern Recognition CVPR 2005</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="947" to="954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Facial expression recognition in perceptual color space</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lajevardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3721" to="3733" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Color analysis of facial skin: Detection of emotional state</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Fuentes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Crites</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ordonez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops (CVPRW), 2014 IEEE Conference</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="474" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Tensor decompositions and applications</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Kolda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Bader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Siam Review</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="455" to="500" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Digital image processing using MATLAB</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Woods</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Eddins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Pearson Education India</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Independent component analysis, a new concept</title>
		<author>
			<persName><forename type="first">P</forename><surname>Comon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal processing</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="287" to="314" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Maenpaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Facial Action Coding System (The Manual on CD Rom)</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Friesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hager</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>Network Information Research Corporation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Active shape models-their training and application</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer vision and image understanding</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="38" to="59" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">CASME Database: A dataset of spontaneous micro-expressions collected from neutralized faces</title>
		<author>
			<persName><forename type="first">W.-J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th IEEE Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Friesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hager</surname></persName>
		</author>
		<title level="m">Facs investigators guide,&quot; A Human Face</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">CASME II: An improved spontaneous micro-expression database and the baseline evaluation</title>
		<author>
			<persName><forename type="first">W.-J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Evaluating the impact of color on texture recognition</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Analysis of Images and Patterns</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="154" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Classification with color and texture: jointly or separately?</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mäenpää</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1629" to="1640" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Evaluating color descriptors for object and scene recognition</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1582" to="1596" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
