<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Real-time Human Motion Analysis by Image Skeletonizadion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hironobu</forename><surname>Fujiyoshi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The Robotics Institute Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Ave Pittsburgh</addrLine>
									<postCode>152 13</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alan</forename><forename type="middle">J</forename><surname>Lipton</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The Robotics Institute Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Ave Pittsburgh</addrLine>
									<postCode>152 13</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Real-time Human Motion Analysis by Image Skeletonizadion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">994EF9AE505722842C227EA73E8034A9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this papel; a process is described for analysing the motion of a human target in a video stream. Moving targets are detected and their boundaries extracted. From these, a "star" skeleton is produced. Two motion cues are determined from this skeletonization: body posture, and cyclic motion of skeleton segments. These cues are used to determine human activities such as walking or running, and even potentially, the target S gait. Unlike other methods, this does not require an a priori human model, or a large number of 'pixels on target". Furthermore, it is computationally inexpensive, and thus ideal for real-world video applications such as outdoor video surveillance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Using video in machine understanding has recently become a significant research topic. One of the more active areas is activity understanding from video imagery <ref type="bibr" target="#b5">[6]</ref>. Understanding activities involves being able to detect and classify targets of interest and analyze what they are doing. Human motion analysis is one such research area. There have been several good human detection schemes, such as <ref type="bibr" target="#b6">[7]</ref> which use static imagery. But detecting and analyzing human motion in real time from video imagery has only recently become viable with algorithms like PJinder <ref type="bibr" target="#b8">[9]</ref> and W4 <ref type="bibr" target="#b3">[4]</ref>. These algorithms represent a good first step to the problem of recognizing and analyzing humans, but they still have some drawbacks. In general, they work by detecting features (such as hands, feet and head), tracking them, and fitting them to some apriori human model such as the cardboard model of Ju et a1 <ref type="bibr" target="#b4">[ 5 ]</ref> .</p><p>There are two main drawbacks of these systems in their present forms: they are completely human specific, and they require a great deal of image-based iinformation in order to work effectively. For general video applications, it may be necessary to derive motion analysis tools which are not constrained to human models, but are aplplicable to other types of targets, or even to classifying targets into different types.</p><p>In some real video applications, such as outdoor surveillance, it is unlikely that there will be enough "pixels on target" to adequately apply these methods. What is required is a fast, robust system which can make broad assumptions about target motion from small amounts of image data. This paper proposes the use of the "star" skeletonization procedure for analyzing the motion of targetsparticularly, human targets. The notion is that a simple form of skeletonization which only extracts the broad internal motion features of a target can be employed lo analyze its motion.</p><p>Once a skeleton is extracted, motiion cues can be determined from it. The two cues dealt with in this paper are: cyclic motion of "leg" segments, and the posture of the "torso" segment. These cues, when taken together can be used to classify the motion of an erect human as "walking" or "running". This paper is organized as follows: section 2 describes how moving targets are extracted in real-time from a video stream, section 3 describes the processing of these target images and section 4 describes human motion analysis. System analysis and conclusions are presented in sections 5and6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Real-time target extraction</head><p>The initial stage of the human motion analysis problem is the extraction of moving targets from a video stream. There are three conventional approaches to moving target detection: temporal differencing; (two-frame or three-frame) [ 11, background subtraction [4, 91 and optical flow (see <ref type="bibr" target="#b1">[2]</ref> for an excellent discussion). Temporal differencing is very adaptive to dynamic environments, but generally does a poor job of extracting all relevant feature pixels. Background subtraction provides the most complete feature data, but is extremely sensitive to dynamic scene changes due to lighting and extraneous events. Optical flow can be used to detect independently moving targets in the presence of camera motion, however most optical flow computation methods are very complex and are inapplicable to real-time algorithms without specialized hardware.</p><p>The approach presented here is similar to that taken in <ref type="bibr" target="#b3">[4]</ref> and is an attempt to make background subtraction more robust to environmental dynamism. The notion is to use an adaptive background model to accommodate changes to the background while maintaining the ability to detect independently moving targets.</p><p>Consider a stabilized video stream or a stationary video camera viewing a scene. The returned image stream is denoted I, where n is the frame number. There arc four types of image motion which are significant for the purposes of moving target detection: slow dynamic changes to the environment such as slowly changing lighting conditions; "once-off" independently moving false alarms such as tree branches breaking and falling to the ground; moving environmental clutter such as leaves blowing in the wind; and legitimate moving targets.</p><p>The first of these issues is dealt with by using a statistical model of the background to provide a mechanism to adapt to slow changes in the environment. For each pixel value pn in the nth frame, a running average jTn and a form of standard deviation g P n are maintained by temporal filtering. Due to the filtering process, these statistics change over time reflecting dynamism in the environment.</p><p>The filter is of the form</p><formula xml:id="formula_0">F ( t ) = e+ (1)</formula><p>where T is a time constant which can be configured to refine the behavior of the system. The filter is implemented:</p><p>(2)</p><formula xml:id="formula_1">F n + l = QPntl+ (1 -. )Fn gn+1 = Q/Pn+l -Pn+ll + (1 -.)(m -</formula><p>where a = r x f , and f is the frame rate. Unlike the models of both [4] and 191, this statistical model incorporates noise measurements to determine foreground pixels, rather than a simple threshold. This idea is inspired by [3].</p><p>If a pixel has a value which is more than 2u fromp,, then it is considered a foreground pixel. At this point a multiple hypothesis approach is used for determining its behavior. A new set of statistics (p', U ' ) is initialized for this pixel and the original set is remembered. If, after time t = 3r, the pixel value has not returned to its original statistical value, the new statistics are chosen as replacements for the old.</p><p>"Moving" pixels are aggregated using a connected component approach so that individual target regions can be extracted. Transient moving objects will cause short term changes to the image stream that will not be included in the background model, but will be continually tracked, whereas more permanent changes will (after 3 ~) be absorbed into the background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Target pre-processing</head><p>No motion detection algorithm is perfect. There will be spurious pixels detected, holes in moving features, "interlacing" effects from video digitization processes, and other anomalies. Foreground regions are initially filtered for size to remove spurious features, and then the remaining targets are pre-processed before motion analysis is performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Pre-processing</head><p>The first pre-processing step is to clean up anomalies in the targets. This is done by a morphological dilation followed by an erosion. This removes any small holes in the target and smoothes out any interlacing anomalies. In this implementation, the target is dilated twice followed by a single erosion. This effectively robustifies small features such as thin arm or leg segements.</p><p>After the target has been cleaned, its outline is extracted using a border following algorithm. The process is shown in figure <ref type="figure" target="#fig_0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">"Star" skeletonization</head><p>An important cue in determining the internal motion of a moving target is the change in its boundary shape over time and a good way to quantify this is to use skeletonization. There are many standard techniques for skeletonization such as thinning and distance transformation. However, these techniques are computationally expensive and moreover, are highly susceptible to noise in the target boundary. The method proposed here provides a simple, real-time, robust way of detecting extremal points on the boundary of the target to produce a "star" skeleton. The "star" skeleton consists of only the gross extremities of the target joined to its centroid in a "star" fashion. 1. The centroid of the target image boundary (zc, ye) is determined.</p><p>(</p><p>where (z,, yc) is the average boundary pixel position, Nb is the number of boundary pixels, and (xi, yi) is a pixel on the boundary of the target.</p><p>2. The distances di from the centroid (xc, ye) to each border point (xi, yi) are calculated</p><formula xml:id="formula_3">di = d(zi -~c ) ' + (Yi -~c ) ~ (4)</formula><p>These are expressed as a one dimensional discrete function d( i) = di. Note that this function is periodic with period Nb.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>The signal d ( i ) is then smoothed for noise reduction, becoming d"(i). This can be done using a linear smoothing filter or low pass filtering in the Fourier domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>Local maxima of d ( i ) are taken as extremal points, and the "star" skeleton is constructed by connecting them to the target centroid (xc , ye). Local maxima are detected by finding zero-crossings of the difference function</p><formula xml:id="formula_4">6 ( i ) = J ( i ) -J(i -1)<label>( 5 )</label></formula><p>This procedure for producing "star" skeletons is illustrated in figure <ref type="figure" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Advantages of "star" skeletonization</head><p>There are three main advantages of this type of skeletonization process. It is not iterative and is, therefore, computationally cheap. It also explicitly provides a mechanism for controlling scale sensitivity. Finally, it relies on no a priori human model.</p><p>The scale of features which can be detected is directly configurable by changing the cutoff frequency c of the lowpass filter. Figure <ref type="figure">3</ref> shows two smoothed versions of d( i) for different values of e: c = 0.01 x Nb and c = 0.025 x Nb. For the higher value of e, more detail is included in the "star" skeleton because more of the smaller boundary features are retained in (i(i). So the method can be scaled for different levels of target complexity.</p><p>An interesting application of this scalability is the ability to measure the complexity of a target by examining the number of extremal points extracted as a function of smoothing.</p><p>Other analysis techniques <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b3">4]</ref>, require apriori models of humans -such as the cardboard model in order to analyze human activities. Using the skeletonization approach, no such models are required, so the rnethod can be applied to other objects like animals and vehicles (see Figure <ref type="figure" target="#fig_5">4</ref>). It is clear that the structure and rigidity of the skeleton are important cues in analysing different types of targets. However, in this implementation, only huiman motion is considered. Also, unlike other methods which require the tracking of specific features, this method uses only the object's boundary so there is no requirement for a large number of "pixels on target".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Human motion analysis</head><p>One technique often used to analyze the motion or gait of an individual target is the cyclic motion of skeletal components [8]. However, in this implementation, the knowledge of individual joint positions cannot be determined in real-time. So a more fundamental cyclic analysis must be performed.</p><p>Another cue to the gait of the target is its posture. Using only a metric based on the "star" skeleton, it is possible to determine the posture of a moving human. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Significant features of the "star" skeleton</head><p>For the cases in which a human is moving in an upright position, it can be assumed that the lower extremal points are legs, so choosing these as points to analyze cyclic motion seems a reasonable approach. In particular, the leftmost lower extremal point (1, , 1,) is used as the cyclic point.</p><p>Note that this choice does not guarantee that the analysis is being performed on the same physical leg at all times, but the cyclic structure of the motion will still be evident from this point's motion. If { (xi, yi) 1 is the set of extremal points, (I,, l, ) is chosen according to the following condition:</p><p>(l,, l,) = (zt , y i ) : z: = min z:   One cue to determining the posture of a moving human is the inclination of the torso. This can be approximated by the angle of the upper-most extremal point of the target. This angle 4 can be determined in exactly the same manner as 8. See figure <ref type="figure" target="#fig_6">5(b)</ref>.</p><p>Figure <ref type="figure" target="#fig_7">6</ref> shows human target skeleton motion sequences for walking and running and the values of 8, for the cyclic point. These data were acquired in real-time from a video stream with frame rate 8Hz. This value is not a constant in this technique but depends on the amount of processing which is required to perform motion analysis and target preprocessing.</p><p>Note that in figure 6(c), there is an offset in the value of t ?, in the negative direction. This is because only the leftmost leg (from a visual point of view) is used in the calculation and the calculation of 8 is therefore biased towards the negative. There is also a bias introduced by the gait of the person. If slhe is running, the body tends to lean forward, and the values of t ?, tend to reflect this overall posture. Another feature which can clearly be observed is that the frequency of the cyclic motion point is clearly higher in the case of the running person, so this can be used as a good metric for classifying the speed of human motion.</p><p>Comparing the average values 7, in figures 6(e)-(f)</p><p>show that the posture of a running target can easily be distinguished from that of a walking one using the angle of the torso segment as a guide.  quantify these signals, it is useful to move into the Fourier domain. However, there is a great deal of signal noise, so a naive Fourier transform will not yield useful resultssee figure <ref type="figure" target="#fig_8">7</ref>(b). Here, the power spectrum of 6, shows a great deal of background noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Cycle detection</head><p>To emphasize the major cyclic component, an autocorre- </p><formula xml:id="formula_5">N Ri = + -C &amp;&amp;-i n = l<label>(8) l</label></formula><p>where N is number of frames. This is shown in figure <ref type="figure" target="#fig_8">7(c</ref>). This autocorrelation process introduces a new source of noise due to the bias (or DC component) of the 6 , signal. When low frequency components are autocorrelated, they remain in the signal and show up in the power spectrum as a large peak in the low frequencies with a degeneration of 6 [dB/oct] in the case of figure <ref type="figure" target="#fig_8">7(d</ref>). To alleviate this problem, a high frequency pre-emphasis filter H ( 2 ) is applied to the signal before autocorrelation. The filter used is:</p><formula xml:id="formula_6">H ( 2 ) = 1 -az-l (9)</formula><p>with a chosen empirically to be M 1 .O. This yields the figure shown in figure <ref type="figure" target="#fig_8">7(e)</ref>.</p><p>Finally, figure <ref type="figure" target="#fig_8">7</ref>(g) shows that the major cyclic component of the cyclic point can be easily extracted from the power spectrum of this processed signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Analysis</head><p>This motion analysis scheme has been tried on a database of video sequences of people walking and running. There  are approximately 20 video sequences in each category, with pixles on target ranging from M 50 to M 400. The targets are a mixture of adults and children. The end-to-end process of MTD, target pre-processing, and motion analysis was performed on an SGI 0 2 machine containing an RIO000 175Mhz processor.</p><p>Figure <ref type="figure">8</ref> shows histograms of the peaks of the power spectrum for each of the video streams. It is clear from figure <ref type="figure">8</ref>(a) that the low frequency noise would cause a serious bias if motion classification were attempted. However, figure <ref type="figure">8(b)</ref> shows how effective the pre-emphasis filter is in removing this noise. It also shows how it is possible to classify motion in terms of walking or running based on the frequency of the cyclic motion. The average walking frequency is 1.75[Hz] and for running it is 2.875[Hz]. A Threshold frequency of 2.O[Hz] correctly classifies 97.5% of the target motions. Note that these frequencies are twice the actual footstep frequency because only the visually leftmost leg is considered. Another point of interest is that the variance of running frequencies is greater than that of walking frequencies, so it could be possible to classify different ''types'' of running such as jogging or sprinting. For each </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Analyzing human motion for video applications is a complex problem. Real-world implementations will have to be computationally inexpensive and be applicable to real scenes in which targets are small and data is noisy. The notion of using a target's boundary to analyze its motion is a useful one under these conditions. Algorithms need only be applied to a small number of pixels and internal target detail, which may be sketchy, becomes less important. This paper presents the approach of "star" skeletonization by which the component parts of a target with internal motion may easily, if grossly, be extracted. Further, two analysis techniques have been investigated which can broadly classify human motion. Body inclination can be measured from the "star" skeleton to determine the posture of the human, which derives clues as to the type of motion being executed. In addition, cyclic analysis of extrema1 points provides a very clean way of broadly distinguishing human motion in terms of walking and running and potentially even different types of gait.</p><p>In the future, this analysis technique will be applied to more complex human motions such as crawling, jumping, and so on. It may even be applied to the gaits of animals.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Target pre-processing. A moving target region is morphologically dilated (twice) then eroded. Then its border is extracted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The boundary is "unwrapped" as a distance function from the centroid. This function is then smoothed and extremal points are extracted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>signal 2 Figure 3 .</head><label>23</label><figDesc>Figure 3. Effect of cut-off value e. When c is small only gross features are extracted, but larger values of c detect more extremal points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5</head><label>5</label><figDesc>Figure 5(a)  shows the definition of (l,, Zy) and 8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Skeletonization of different moving targets. It is clear the structure and rigidity of the skeleton is significant in analyzing target motion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Determination of skeleton features. (a) 8 is the angle the left cyclic point (leg) makes with the vertical, and (b) 4 is the angle the torso makes with the vertical.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figures 6 (</head><label>6</label><figDesc>Figures 6(c)-(d) display a clear cyclical nature in 8,. To</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Process for detecting cyclic motion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>leg angle0 of a walking person (d) leg angle0 of a running person torso angle4 of a walking person ( f ) torso angle 4 of a running person</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Skeleton motion sequences. Clearly, the periodic motion of 8, provides cues to the target's motion as does the mean value of 4,.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Figure 8. Histogram of cyclic motion frequency peaks. (a) The bias in On often produces a frequency peak which is significantly higher than the peak produced by cyclic motion. (b) The pre-emphasis filter effectively removes this noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Average inclination histogram of torso for classification.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Change detection and tracking using pyramid transformation techniques</title>
		<author>
			<persName><forename type="first">C</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Van Der Wal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SPIE -Intelligent Robots and Computer Ksion</title>
		<meeting>SPIE -Intelligent Robots and Computer Ksion</meeting>
		<imprint>
			<date type="published" when="1985">1985</date>
			<biblScope unit="volume">579</biblScope>
			<biblScope unit="page" from="72" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Performance of optical flow techniques</title>
		<author>
			<persName><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Beauchemin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="42" to="77" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A forest of sensors</title>
		<author>
			<persName><forename type="first">E</forename><surname>Grimson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings ofDARP -VSAMworkshopZZ</title>
		<meeting>DARP -VSAMworkshopZZ</meeting>
		<imprint>
			<date type="published" when="1997-11">November 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Haritaoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harwood</surname></persName>
		</author>
		<title level="m">w4 who? when? where? what? a real time system for detecing and tracking people</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
	<note>FGR98 (submitted)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cardboard people: A parameterized model of articulated image motion</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yacoob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Face and Gesture Analysis</title>
		<meeting>International Conference on Face and Gesture Analysis</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cooperative multisensor video surveillance</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Anandan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Burt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of DARPA Image Understanding Workshop</title>
		<meeting>DARPA Image Understanding Workshop</meeting>
		<imprint>
			<date type="published" when="1997-05">May 1997</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pedestrian detection using wavelet templates</title>
		<author>
			<persName><forename type="first">M</forename><surname>Oren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Osuna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE CVPR 97</title>
		<meeting>IEEE CVPR 97</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="193" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cyclic motion detection for motion based recognition</title>
		<author>
			<persName><forename type="first">Si</forename><forename type="middle">P</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ketter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kasparis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pfinder: Real-time tracking of the human body</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Azarbayejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="159" to="160" />
			<date type="published" when="1994">1997. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
