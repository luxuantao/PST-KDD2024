<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Closed-loop Matters: Dual Regression Networks for Single Image Super-Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-05-22">22 May 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yong</forename><surname>Guo</surname></persName>
							<email>guo.yong@mail.scut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Guangzhou Laboratory</orgName>
								<orgName type="institution" key="instit1">South China University of Technology</orgName>
								<orgName type="institution" key="instit2">Microsoft Research Asia</orgName>
								<orgName type="institution" key="instit3">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Guangzhou Laboratory</orgName>
								<orgName type="institution" key="instit1">South China University of Technology</orgName>
								<orgName type="institution" key="instit2">Microsoft Research Asia</orgName>
								<orgName type="institution" key="instit3">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Guangzhou Laboratory</orgName>
								<orgName type="institution" key="instit1">South China University of Technology</orgName>
								<orgName type="institution" key="instit2">Microsoft Research Asia</orgName>
								<orgName type="institution" key="instit3">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qi</forename><surname>Chen</surname></persName>
							<email>sechenqi@mail.scut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Guangzhou Laboratory</orgName>
								<orgName type="institution" key="instit1">South China University of Technology</orgName>
								<orgName type="institution" key="instit2">Microsoft Research Asia</orgName>
								<orgName type="institution" key="instit3">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
							<email>secaojiezhang@mail.scut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Guangzhou Laboratory</orgName>
								<orgName type="institution" key="instit1">South China University of Technology</orgName>
								<orgName type="institution" key="instit2">Microsoft Research Asia</orgName>
								<orgName type="institution" key="instit3">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zeshuai</forename><surname>Deng</surname></persName>
							<email>sedengzeshuai@mail.scut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Guangzhou Laboratory</orgName>
								<orgName type="institution" key="instit1">South China University of Technology</orgName>
								<orgName type="institution" key="instit2">Microsoft Research Asia</orgName>
								<orgName type="institution" key="instit3">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yanwu</forename><surname>Xu</surname></persName>
							<email>ywxu@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Guangzhou Laboratory</orgName>
								<orgName type="institution" key="instit1">South China University of Technology</orgName>
								<orgName type="institution" key="instit2">Microsoft Research Asia</orgName>
								<orgName type="institution" key="instit3">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
							<email>mingkuitan@scut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Guangzhou Laboratory</orgName>
								<orgName type="institution" key="instit1">South China University of Technology</orgName>
								<orgName type="institution" key="instit2">Microsoft Research Asia</orgName>
								<orgName type="institution" key="instit3">Baidu Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Closed-loop Matters: Dual Regression Networks for Single Image Super-Resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-05-22">22 May 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2003.07018v4[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images</head><p>to high-resolution (HR) images. However, there are two underlying limitations to existing SR methods. First, learning the mapping function from LR to HR images is typically an ill-posed problem, because there exist infinite HR images that can be downsampled to the same LR image. As a result, the space of the possible functions can be extremely large, which makes it hard to find a good solution. Second, the paired LR-HR data may be unavailable in real-world applications and the underlying degradation method is often unknown. For such a more general case, existing SR models often incur the adaptation problem and yield poor performance. To address the above issues, we propose a dual regression scheme by introducing an additional constraint on LR data to reduce the space of the possible functions. Specifically, besides the mapping from LR to HR images, we learn an additional dual regression mapping estimates the down-sampling kernel and reconstruct LR images, which forms a closed-loop to provide additional supervision. More critically, since the dual regression process does not depend on HR images, we can directly learn from LR images. In this sense, we can easily adapt SR models to real-world data, e.g., raw video frames from YouTube. Extensive experiments with paired training data and unpaired real-world data demonstrate our superiority over existing methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep neural networks (DNNs) have been the workhorse of many real-world applications, including image classification <ref type="bibr" target="#b20">[18,</ref><ref type="bibr" target="#b16">14,</ref><ref type="bibr" target="#b11">9,</ref><ref type="bibr" target="#b17">15,</ref><ref type="bibr" target="#b29">27,</ref><ref type="bibr" target="#b15">13]</ref>, video understanding <ref type="bibr">[46, 45,</ref> Figure <ref type="figure">1</ref>. Performance comparison of the images produced by the state-of-the-art methods for 8× SR. Our dual regression scheme is able to produce sharper images than the baseline methods. <ref type="bibr" target="#b46">44,</ref><ref type="bibr" target="#b8">6]</ref> and many other applications <ref type="bibr" target="#b9">[7,</ref><ref type="bibr" target="#b52">50,</ref><ref type="bibr" target="#b54">52,</ref><ref type="bibr" target="#b13">11,</ref><ref type="bibr" target="#b22">20]</ref>. Recently, image super-resolution (SR) has become an important task that aims at learning a nonlinear mapping to reconstruct high-resolution (HR) images from low-resolution (LR) images. Based on DNNs, many methods have been proposed to improve SR performance <ref type="bibr" target="#b53">[51,</ref><ref type="bibr" target="#b28">26,</ref><ref type="bibr" target="#b12">10,</ref><ref type="bibr" target="#b14">12,</ref><ref type="bibr" target="#b51">49]</ref>. However, these methods may suffer from two limitations.</p><p>First, learning the mapping from LR to HR images is typically an ill-posed problem since there exist infinitely many HR images that can be downscaled to obtain the same LR image <ref type="bibr" target="#b38">[36]</ref>. Thus, the space of the possible functions that map LR to HR images becomes extremely large. As a result, the learning performance can be limited since learning a good solution in such a large space is very hard. To improve the SR performance, one can design effective models by increasing the model capacity, e.g., EDSR <ref type="bibr" target="#b28">[26]</ref>, DBPN <ref type="bibr" target="#b18">[16]</ref>, and RCAN <ref type="bibr" target="#b53">[51]</ref>. However, these methods still suffer from the large space issue of possible mapping functions, resulting in the limited performance without producing sharp textures <ref type="bibr" target="#b26">[24]</ref> (See Figure <ref type="figure">1</ref>). Thus, how to reduce the possible space of the mapping functions to improve the training of SR models becomes an important problem.</p><p>Second, it is hard to obtain a promising SR model when the paired data are unavailable <ref type="bibr" target="#b45">[43,</ref><ref type="bibr" target="#b56">54]</ref>. Note that most SR methods rely on the paired training data, i.e., HR images with their Bicubic-degraded LR counterparts. However, the paired data may be unavailable and the unpaired data often dominate the real-world applications. Moreover, the realworld data do not necessarily have the same distribution to the LR images obtained by a specific degradation method (e.g., Bicubic). Thus, learning a good SR model for realworld applications can be very challenging. More critically, if we directly apply existing SR models to real-world data, they often incur a severe adaptation problem and yield poor performance <ref type="bibr" target="#b45">[43,</ref><ref type="bibr" target="#b56">54]</ref>. Therefore, how to effectively exploit the unpaired data to adapt SR models to real-world applications becomes an urgent and important problem.</p><p>In this paper, we propose a novel dual regression scheme that forms a closed-loop to enhance SR performance. To address the first limitation, we introduce an additional constraint to reduce the possible space such that the superresolved images can reconstruct the input LR images. Ideally, if the mapping from LR→HR is optimal, the superresolved images can be downsampled to obtain the same input LR image. With such a constraint, we are able to estimate the underlying downsampling kernel and hence reduce the space of possible functions to find a good mapping from LR to HR (See theoretical analysis in Remark 1). Thus, it becomes easier to obtain promising SR models (See the comparison in Figure <ref type="figure">1</ref>). To address the second limitation, since the regression of LR images does not depend on HR images, our method can directly learn from the LR images. In this way, we can easily adapt SR models to the real-world LR data, e.g., raw video frames from Youtube. Extensive experiments on the SR tasks with paired training data and unpaired real-world data demonstrate the superiority of our method over existing methods.</p><p>Our contributions are summarized as follows:</p><p>• We develop a dual regression scheme by introducing an additional constraint such that the mappings can form a closed-loop and LR images can be reconstructed to enhance the performance of SR models.</p><p>Moreover, we also theoretically analyze the generalization ability of the proposed scheme, which further confirms its superiority to existing methods.</p><p>• We study a more general super-resolution case where there is no corresponding HR data w.r.t. the real-world LR data. With the proposed dual regression scheme, deep models can be easily adapted to real-world data, e.g., raw video frames from YouTube.</p><p>• Extensive experiments on both the SR tasks with paired training data and unpaired real-world data demonstrate the effectiveness of the proposed dual regression scheme in image super-resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Primal Regression Task</head><p>Dual Regression Task </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Supervised super-resolution. Many efforts have been made to improve the performance of SR, including the interpolation-based approaches <ref type="bibr" target="#b21">[19]</ref> and reconstructionbased methods <ref type="bibr" target="#b18">[16,</ref><ref type="bibr" target="#b27">25,</ref><ref type="bibr" target="#b53">51]</ref>. Haris et al. <ref type="bibr" target="#b18">[16]</ref> propose a backprojection network (DBPN) that consists of several up-and down-sampling layers to iteratively produce LR and HR images. Zhang et al. <ref type="bibr" target="#b53">[51]</ref> propose the channel attention mechanism to build a deep model called RCAN to further improve the performance of SR. However, these methods still have a very large space of the possible mappings which makes it hard to learn a good solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unsupervised super-resolution.</head><p>There is an increasing interest in learning super-resolution models without paired data in the unsupervised setting <ref type="bibr" target="#b45">[43,</ref><ref type="bibr" target="#b56">54]</ref>. Based on Cycle-GAN <ref type="bibr" target="#b58">[56]</ref>, Yuan et al. <ref type="bibr" target="#b45">[43]</ref> propose a CinCGAN model to generate HR images without paired data. Recently, some blind SR methods <ref type="bibr" target="#b4">[2,</ref><ref type="bibr" target="#b57">55]</ref> were proposed to learn the unknown degradation methods. However, these methods often totally discard the paired synthetic data, which can be obtained very easily and used to boost the training. On the contrary, our dual regression scheme seeks to adapt SR models to new LR data by exploiting both the real-world LR data and the paired synthetic data.</p><p>Dual learning. Dual learning methods <ref type="bibr" target="#b19">[17,</ref><ref type="bibr" target="#b42">40,</ref><ref type="bibr" target="#b43">41,</ref><ref type="bibr" target="#b55">53]</ref> contain a primal model and a dual model to learn two opposite mappings simultaneously to enhance the performance of language translation. Recently, this scheme has also been used to perform image translation without paired training data, e.g., CycleGAN <ref type="bibr" target="#b58">[56]</ref> and DualGAN <ref type="bibr" target="#b44">[42]</ref>. Specifically, a cycle consistency loss is proposed to avoid the mode collapse issue of GAN methods <ref type="bibr" target="#b58">[56,</ref><ref type="bibr" target="#b6">4,</ref><ref type="bibr" target="#b7">5]</ref> and help minimize the distribution divergence. However, these methods cannot be directly applied to the standard SR problem. By contrast, we use the closed-loop to reduce the space of possible functions of SR. Moreover, we consider learning asymmetric mappings and provide a theoretical guarantee on the rationality and necessity of using a cycle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>We propose a dual regression scheme to deal with both the paired and unpaired training data for super-resolution (SR). The overall training scheme is shown in Figure <ref type="figure" target="#fig_0">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dual Regression Scheme for Paired Data</head><p>Existing methods only focus on learning the mapping from LR to HR images. However, the space of the possible mapping functions can be extremely large, making the training very difficult. To address this issue, we propose a dual regression scheme by introducing an additional constraint on LR data. Specifically, besides learning the mapping LR→ HR, we also learn an inverse/dual mapping from the super-resolved images back to the LR images.</p><p>Let x ∈ X be LR images and y ∈ Y be HR images. We simultaneously learn the primal mapping P to reconstruct HR images and the dual mapping D to reconstruct LR images. Note that the dual mapping can be regarded as the estimation of the underlying downsampling kernel. Formally, we formulate the SR problem into the dual regression scheme which involves two regression tasks.</p><p>Definition 1 (Primal Regression Task) We seek to find a function P : X →Y, such that the prediction P (x) is similar to its corresponding HR image y.</p><p>Definition 2 (Dual Regression Task) We seek to find a function D: Y→X , such that the prediction of D(y) is similar to the original input LR image x.</p><p>The primal and dual learning tasks can form a closedloop and provide informative supervision to train the models P and D. If P (x) was the correct HR image, then the down-sampled image D(P (x)) should be very close to the input LR image x. With this constraint, we can reduce the function space of possible mappings and make it easier to learn a better mapping to reconstruct HR images. To verify this, we provide a theoretical analysis in Section 4.2.</p><p>By jointly learning these two learning tasks, we propose to train the super-resolution models as follows. Given a set of N paired samples S P = {(x i , y i )} N i=1 , where x i and y i denote the i-th pair of low-and high-resolution images in the set of paired data. The training loss can be written as</p><formula xml:id="formula_0">N i=1 L P P (x i ), y i primal regression loss + λ L D D(P (x i )), x i dual regression loss ,<label>(1)</label></formula><p>where L P and L D denote the loss function (ℓ 1 -norm) for the primal and dual regression tasks, respectively. Here, λ controls the weight of the dual regression loss (See the sensitivity analysis of λ in Section 6.2).</p><p>Actually, we can also add a constraint on the HR domain, i.e., downscaling and upscaling to reconstruct the original  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dual Regression for Unpaired Data</head><p>We consider a more general SR case where there is no corresponding HR data w.r.t. the real-world LR data. More critically, the degradation methods of LR images are often unknown, making this problem very challenging. In this case, existing SR models often incur the severe adaptation problem <ref type="bibr" target="#b45">[43,</ref><ref type="bibr" target="#b56">54]</ref>. To alleviate this issue, we propose an efficient algorithm to adapt SR models to the new LR data. The training algorithm is shown in Algorithm 1.</p><p>Note that the dual regression mapping learns the underlying degradation methods and does not necessarily depend on HR images. Thus, we can use it to directly learn from the unpaired real-world LR data to perform model adaptation. To ensure the reconstruction performance of HR images, we also incorporate the information from paired synthetic data that can be obtained very easily (e.g., using the Bicubic kernel). Given M unpaired LR samples and N paired synthetic samples, the objective function can be written as:</p><formula xml:id="formula_1">M +N i=1 1 S P (x i )L P P (x i ), y i + λL D D(P (x i )), x i ,<label>(2)</label></formula><p>where 1 S P (x i ) is an indicator function that equals 1 when x i ∈ S P , and otherwise the function equals 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training Method</head><p>Training method on paired data. Given paired training data, we follow the learning scheme of supervised SR methods <ref type="bibr" target="#b18">[16,</ref><ref type="bibr" target="#b28">26]</ref> and train model by minimizing Eqn. ( <ref type="formula" target="#formula_0">1</ref>). More details are shown in Section 5 and the supplementary.</p><p>Training method on unpaired data. As shown in Algorithm 1, for each iteration, we first sample m unpaired realworld data from S U and n paired synthetic data from S P , respectively. Then, we train our model end-to-end by minimizing the objective in Eqn. <ref type="bibr" target="#b4">(2)</ref>. For convenience, we define the data ratio of unpaired data as</p><formula xml:id="formula_2">ρ = m/(m + n).<label>(3)</label></formula><p>Since paired synthetic data can be obtained very easily (e.g., performing Bicubic kernel to produce LR-HR pairs), we can adjust ρ by changing the number of paired synthetic samples n. In practice, we set ρ = 30% to obtain the best results (See the discussions in Section 6.3). With the proposed dual regression scheme, we can adapt SR models to the various unpaired data while preserving good reconstruction performance (See results in Section 5.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Differences from CycleGAN based SR Methods</head><p>There are several differences and advantages of DRN compared to CycleGAN based SR methods. First, Cycle-GAN based methods <ref type="bibr" target="#b45">[43,</ref><ref type="bibr" target="#b58">56]</ref> use a cycle consistency loss to avoid the possible mode collapse issue when solving the under-constrained image translation problem <ref type="bibr" target="#b58">[56]</ref>. Unlike these methods, we seek to improve the performance of our SR model by adding an extra constraint, which reduces the possible function space by mapping the SR images back to the corresponding LR images. Second, CycleGAN based methods totally discard the paired synthetic data, which, however, can be obtained very easily. On the contrary, our DRN simultaneously exploits both paired synthetic data and real-world unpaired data to enhance the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">More Details</head><p>In this section, we first depict the architecture of our dual regression network (DRN). Then, we conduct a theoretical analysis to justify the proposed dual regression scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Architecture Design of DRN</head><p>We build our DRN upon the design of U-Net for superresolution <ref type="bibr" target="#b24">[22,</ref><ref type="bibr" target="#b33">31]</ref> (See Figure <ref type="figure" target="#fig_2">3</ref>). Our DRN model consists of two parts: a primal network and a dual network. We present the details for each network as follows.</p><p>The primal network follows the downsamplingupsampling design of U-Net. Both the downsampling (left half of Figure <ref type="figure" target="#fig_2">3</ref>) and upsampling (right half of Figure <ref type="figure" target="#fig_2">3</ref>) modules contain log 2 (s) basic blocks, where s denotes the scale factor. This implies that the network will have 2 blocks for 4× upscaling (See Figure <ref type="figure" target="#fig_2">3</ref>) and 3 blocks for 8× upscaling. Unlike the baseline U-Net, we build each basic block using B residual channel attention block (RCAB) <ref type="bibr" target="#b53">[51]</ref> to improve the model capacity. Following <ref type="bibr" target="#b41">[39,</ref><ref type="bibr" target="#b25">23]</ref>, we add additional outputs to produce images at the corresponding scale (i.e., 1×, 2×, and 4× images) and apply the proposed loss to them to train the model. Note that we use the Bicubic kernel to upscale LR images before feeding them into the primal network. Please refer to the supplementary for more details.</p><p>We design a dual network to produce the down-sampled LR images from the super-resolved ones (See red lines in Figure <ref type="figure" target="#fig_2">3</ref>). Note that the dual task aims to learn a downsampling operation, which is much simpler than the primal task for learning the upscaling mapping. Thus, we design the dual model with only two convolution layers and a LeakyReLU activation layer <ref type="bibr" target="#b30">[28]</ref>, which has much lower computation cost than the primal model but works well in practice (See results in Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Theoretical Analysis</head><p>We theoretically analyze the generalization bound for the proposed dual regression scheme on paired data. Since the case with unpaired data is more complicated, we will investigate the theoretical analysis method in the future. Due to the page limit, all the definitions, proofs, and lemmas are put in the supplementary.</p><p>The generalization error of the dual regression scheme is to measure how accurately the algorithm predicts the unseen test data in the primal and dual tasks. Let E(P, D) = E[L P (P (x), y)+λL D (D(P (x)), x)] and Ê(P, D) is its empirical loss, we obtain a generalization bound of the proposed model using Rademacher complexity <ref type="bibr" target="#b32">[30]</ref>.</p><p>Theorem 1 Let L P (P (x), y)+λL D (D(P (x)), x) be a mapping from X ×Y to [0, C] with the upper bound C, and the function space H dual be infinite. Then, for any error δ&gt;0, with probability at least 1−δ, the generalization error E(P, D) (i.e., expected loss) satisfies for all (P, D)∈H dual :</p><formula xml:id="formula_3">E(P, D) ≤ Ê(P, D)+2 RDL Z (H dual )+3C 1 2N log 1 δ ,</formula><p>where N is the number of samples and RDL Z is the empirical Rademacher complexity of dual learning. Let B(P, D) be the generalization bound of the dual regression SR, i.e.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B(P, D)=2 RDL</head><p>Z (H dual )+3C This theorem shows the generalization bound of the dual regression scheme relies on the Rademacher complexity of a function space H dual . From Theorem 1, the dual regression SR scheme has a smaller generalization bound than traditional SR method, and thus it helps to achieve more accurate SR predictions. More discussions can be referred to Remark 1. We highlight that the derived generalization bound of the dual regression scheme, where the loss function is bounded by [0, C], is more general than <ref type="bibr" target="#b42">[40]</ref>. Moreover, this generalization bound is tight when training data is sufficient, and the primal and dual models are powerful enough.   Remark 1 Based on the definition of the Rademacher complexity, the capacity of the function space H dual ∈P×D is smaller than the capacity of function space H∈P or H∈D in traditional supervised learning, i.e., RDL Z ≤ RSL Z , where RSL Z is the Rademacher complexity defined in supervised learning. In other words, the dual regression scheme has smaller generalization bound than the primal feed-forward scheme and the proposed dual regression scheme helps the primal model to achieve more accurate SR predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We extensively evaluate the proposed method on the image super-resolution tasks under the scenarios with paired Bicubic data and unpaired real-world data. All implementations are based on PyTorch. <ref type="foot" target="#foot_0">1</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Results on Supervised Image Super-Resolution</head><p>In this section, we first show an illustrated comparison in terms of performance and model size for 4× and 8× SR in Figure <ref type="figure" target="#fig_5">4</ref>. Then, we provide a detailed comparison for 4× and 8× SR. In the experiments, we propose two models, namely a small model DRN-S and a large model DRN-L. We obtain the results of all the compared methods from their pretrained models, released code, or their original paper.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Datasets and Implementation Details</head><p>We compare different methods on five benchmark datasets, including SET5 <ref type="bibr" target="#b5">[3]</ref>, SET14 <ref type="bibr" target="#b49">[47]</ref>, BSDS100 <ref type="bibr" target="#b3">[1]</ref>, UR-BAN100 <ref type="bibr" target="#b23">[21]</ref> and MANGA109 <ref type="bibr" target="#b31">[29]</ref>. Two commonly used image quality metrics are adopted as the metrics, such as PSNR and SSIM <ref type="bibr" target="#b40">[38]</ref>. Following <ref type="bibr" target="#b39">[37]</ref>, we train our models on DIV2K <ref type="bibr" target="#b36">[34]</ref> and Flickr2K <ref type="bibr" target="#b28">[26]</ref> datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Comparison with State-of-the-art Methods</head><p>We compare our method with state-of-the-art SR methods in terms of both quantitative results and visual results. For quantitative comparison, we compare the PSNR and SSIM values of different methods for 4× and 8× super-resolution. From Table <ref type="table" target="#tab_0">1</ref>, our DRN-S with about 5M parameters yields promising performance. Our DRN-L with about 10M parameters yields comparable performance with the considered methods for 4× SR and yields the best performance for 8× SR. For quality comparison, we provide visual comparisons for our method and the considered methods (See Figure <ref type="figure" target="#fig_6">5</ref>). For both 4× and 8× SR, our model consistently produces sharper edges and shapes, while other baselines may give more blurry ones. The results demonstrate the effectiveness of the proposed dual regression scheme in gen- We also compare the number of parameters in different models for 4× and 8× SR. Due to the page limit, we only show the results for 4× SR and put the 8× SR in the supplementary. From Figure <ref type="figure" target="#fig_5">4</ref>, our DRN-S obtains promising performance with a small number of parameters. When we increase the number of channels and layers, the larger model DRN-L further improves the performance and obtains the best results. Both the empirical results and the theoretical analysis in Theorem 1 show the effectiveness of the proposed dual regression scheme for image super-resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Adaptation Results on Unpaired Data</head><p>In this experiment, we apply the proposed method to a variety of real-world unpaired data. Different from the supervised setting, we first consider a toy case where we evaluate SR models on the LR images with different degradation methods (e.g., Nearest and BD <ref type="bibr" target="#b50">[48]</ref>). During training, we can only access the LR images but not their corresponding HR images. Then, we also apply our method to LR raw video frames from YouTube.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Datasets and Implementation Details</head><p>In this experiment, we obtain the paired synthetic data by downsampling existing images. Considering the real-world SR applications, all the paired data belong to a different domain from the unpaired data (See more discussions in supplementary). Following <ref type="bibr" target="#b34">[32]</ref>, we randomly choose 3k images from ImageNet (called ImageNet3K) and obtain LR images using different degradation methods, including Nearest and BD. We adopt DIV2K (Bicubic) as the paired synthetic data 2 and ImageNet3K LR images with different degradations as the unpaired data. Note that ImageNet3K HR images are not used in our experiments. For the SR task 2 We can also use other degradation methods to obtain the paired synthetic data. We put the impact of degradation methods in supplementary. on video, we collect 3k raw video frames as the unpaired data to train the models. In this section, we use our DRN-S model to evaluate the proposed adaptation algorithm and call the resultant model DRN-Adapt. More details can be found in supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Comparison on Unpaired Synthetic Data</head><p>To evaluate the adaptation performance on unpaired data, we compare our DRN-Adapt and the baseline methods on synthetic data. We report the PSRN and SSIM values of different methods for 8× super-resolution in Table <ref type="table" target="#tab_3">2</ref>.</p><p>From Table <ref type="table" target="#tab_3">2</ref>, DRN-Adapt consistently outperforms the supervised methods on all the datasets. For CycleGAN based method, CinCGAN achieves better performance than the supervised learning methods but still cannot surpass our method due to the inherent limitations mentioned before. Note that, for Nearest LR data, we also report the recovering results of the Nearest kernel, which is the same as the degradation method. Our method also yields a large performance improvement over this baseline. These results demonstrate the effectiveness of the proposed adaptation algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Comparison on Unpaired Real-world Data</head><p>We apply our method to YouTube raw video frames, which are more challenging owing to the complicated and unknown degradation in real-world scenarios. Since there are no ground-truth HR images, we only provide the visual comparison. From Figure <ref type="figure" target="#fig_7">6</ref>, the generated frames from three supervised baselines (i.e., EDSR, DBPN, and RCAN) contain numerous mosaics. For CinCGAN, the SR results are distorted and contain a lot of noise due to the sensitivity to data differences between unpaired LR and HR images. By contrast, our DRN-Adapt produces visually promising images with sharper and clearer textures. to the page limit, we put more visual results in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Further Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Ablation Study on Dual Regression Scheme</head><p>We conduct an ablation study on the dual regression scheme and report the results for 4× SR in Table <ref type="table" target="#tab_4">3</ref>. Compared to the baselines, the models equipped with the dual regression scheme yield better performance on all the datasets. These results suggest that the dual regression scheme can improve the reconstruction of HR images by introducing an additional constraint to reduce the space of the mapping function. We also evaluate the impact of our dual regression scheme on other models, e.g., SRResNet <ref type="bibr" target="#b26">[24]</ref> based network (See more details in the supplementary).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Effect of λ on DRN</head><p>We conduct an experiment to investigate the impact of the hyper-parameter λ in Eqn. <ref type="bibr" target="#b3">(1)</ref>. From Table <ref type="table" target="#tab_5">4</ref>, when we increase λ from 0.001 to 0.1, the dual regression loss gradually becomes more important and provides powerful supervision. If we further increase λ to 1 or 10, the dual regression loss term would overwhelm the original primal regression loss and hamper the final performance. To obtain a good tradeoff between the primal and dual regression, we set λ = 0.1 in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Effect of ρ on Adaptation Algorithm</head><p>We investigate the effect of ρ on the proposed adaptation algorithm. We compare the performance when we change the data ratio of unpaired data ρ and show the corresponding training curves in Figure <ref type="figure" target="#fig_8">7</ref>. From Figure <ref type="figure" target="#fig_8">7</ref>, when we set ρ ∈ {30%, 50%, 70%}, the resultant models obtain better performance than the baseline model, i.e., with ρ=0%. In practice, we set ρ=30% to obtain the best performance. We also compare the models with and without the pretrained parameters. From Figure <ref type="figure" target="#fig_8">7</ref>, the model trained from scratch yields slightly worse result but still outperforms the baseline model without adaptation. These results demonstrate the effectiveness of the proposed adaptation algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we have proposed a novel dual regression scheme for paired and unpaired data. On the paired data, we introduce an additional constraint by reconstructing LR images to reduce the space of possible functions. Thus, we can significantly improve the performance of SR models. Furthermore, we also focus on the unpaired data and apply the dual regression scheme to real-world data., e.g., raw video frames from YouTube. Extensive experiments on both paired and unpaired data demonstrate the superiority of our method over the considered baseline methods. </p><formula xml:id="formula_4">≤|H dual |e − 2N ǫ 2 C 2 . Let |H dual |e − 2N ǫ 2 C 2 = δ, we have ǫ = C log |H dual |+log 1 δ 2N</formula><p>and conclude the theorem.</p><p>This theorem shows that a larger sample size N and smaller function space can guarantee the generalization. Next, we will give a generalization bound of a general case of an infinite function space using Rademacher complexity.</p><p>Theorem 3 Let L P (P (x), y)+λL D (D(P (x)), x) be a mapping from X ×Y to [0, C] with the upper bound C, and the function space H dual be infinite. Then, for any δ&gt;0, with probability at least 1−δ, the generalization error E(P, D) (i.e., expected loss) satisfies for all (P, D)∈H dual :</p><formula xml:id="formula_5">E(P, D) ≤ Ê(P, D)+2 RDL Z (H dual )+3C 1 2N log 1 δ , (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>where N is the number of samples and RDL Z is the empirical Rademacher complexity of dual learning. Let B(P, D) be the generalization bound of the dual regression SR, i.e. B(P, D)=2 RDL Z (H dual )+3C Proof 2 Based on Theorem 3.1 in <ref type="bibr" target="#b32">[30]</ref>, we extend a case for L P (P (x), y) + λL D (D(P (x)), x) bounded in [0, C], and we have the generalization bound in <ref type="bibr" target="#b7">(5)</ref>. According to the definition of Rademacher complexity, we have RDL Z (H dual )≤ RSL Z (H) because the capacity of the function space H dual ∈P×D is smaller than the capacity of the function space H∈P. With the same number of samples, we have B(P, D)≤B(P ).</p><p>Theorem 3 shows that with probability at least 1 − δ, the generalization error is smaller than</p><formula xml:id="formula_7">2R DL N + C 1 2N log( 1 δ ) or 2 RDL Z + 3C 1 2N log( 1 δ</formula><p>). It suggests that using the function space with larger capacity and more samples can guarantee better generalization. Moreover, the generalization bound of dual learning is more general for the case that the loss function L P (P (x), y) + λL D (D(P (x)), x) is bounded by [0, C], which is different from <ref type="bibr" target="#b42">[40]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 2 Based on the definition of Rademacher complexity, the capacity of the function space</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Model Details of Dual Regression Network</head><p>Deep neural networks (DNNs) have achieved great success in image classification <ref type="bibr" target="#b16">[14,</ref><ref type="bibr" target="#b11">9,</ref><ref type="bibr" target="#b15">13,</ref><ref type="bibr" target="#b17">15]</ref>, image generation <ref type="bibr" target="#b13">[11,</ref><ref type="bibr" target="#b6">4]</ref>, and image restoration <ref type="bibr" target="#b12">[10,</ref><ref type="bibr" target="#b14">12]</ref>. In this paper, we propose a novel Dual Regression Network (DRN), which contains a primal model and a dual model. Specifically, the primal model contains 2 basic blocks for 4× SR and 3 blocks for 8× SR. To form a</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Adaptation to Real-world Scenarios with Unpaired Data</head><p>Training data. To obtain the unpaired synthetic data, we randomly choose 3k images from ImageNet <ref type="bibr" target="#b34">[32]</ref> (called Ima-geNet3k) and obtain the LR images using different degradation methods, including Nearest and BD. More specifically, we use Matlab to obtain the Nearest data. The BD data is obtained using the Gaussian kernel with size 7 × 7 and a standard deviation of 1.6. Note that ImageNet3K HR images are not used in our experiments. Moreover, we collect 3k LR raw video frames from YouTube as the unpaired real-world data to evaluate the proposed DRN in a more general and challenging case. More critically, we use both paired data (DIV2K <ref type="bibr" target="#b36">[34]</ref>) and unpaired data to train the proposed models. Test data. For quantitative comparison on unpaired synthetic data, we obtain the LR images of five benchmark datasets using Nearest and BD degradation methods separately. Implementation details. We train a DRN-Adapt model for each kind of unpaired data, i.e., Nearest data, BD data, and video frames collected from YouTube. Thus, there are 3 DRN-adapt models in total. And We also train a CinCGAN <ref type="bibr" target="#b58">[56]</ref> model for each kind of unpaired data for comparison. Based on pretrained DRN-S, We train our DRN-Adapt models with a learning rate of 10 −4 and the data ratio of unpaired data ρ = 30% for a total of 10 5 iterations. Moreover, we apply Adam with β 1 = 0.9, β 2 = 0.99 to optimize the models, and set minibatch size as 16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. More Ablation Studies on Dual Regression</head><p>In this section, we first provide an additional ablation study of the dual regression scheme on other architectures. Then, we investigate the effect of the dual regression scheme on HR images. Last, we investigate the impact of different degradation methods to obtain paired synthetic data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Effect of Dual Regression Scheme on Other Architectures</head><p>To verify the impact of the dual regression scheme, we also conduct an ablation study of the dual network for SRResNet (see architecture in Figure <ref type="figure" target="#fig_10">A</ref>). "SRResNet + Dual" denotes the baseline SRResNet equipped with the dual regression scheme. From Table <ref type="table" target="#tab_8">B</ref>, the model with the dual regression scheme consistently outperforms the baseline counterpart, which further demonstrates the effectiveness of our method.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Effect of the Dual Regression on HR Data</head><p>As mentioned in Section 3.1, one can also add a dual regression constraint on the HR domain, i.e., downscaling and upscaling to reconstruct the original HR images. In this experiment, we investigate the impact of dual regression loss on HR data and show the results in Table <ref type="table" target="#tab_9">C</ref>. For convenience, we use "DRN-S with dual HR" to represent the model with the regression on both LR and HR images. From Table C, DRN-S yields comparable performance with "DRN-S with dual HR" while only needs half the computation cost. Thus, it is not necessary to apply the dual regression on HR images in practice. In this experiment, we investigate the impact of different degradation methods to obtain paired synthetic data. We change kernel from Bicubic to Nearest and evaluate the adaptation models on BD data. From Table F, DRN-Adapt obtain similar results when we use different degradation methods to obtain the paired synthetic data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. More Comparisons and Results</head><p>For supervised super-resolution, we put more visual results in this section shown in Figures C and D, respectively. Considering the scenario with unpaired data, we put more visual results on real-world unpaired data (See Figure <ref type="figure" target="#fig_13">E</ref>). From these results, our models are able to produce the images with sharper edges and clearer textures than state-of-the-art methods.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Dual regression training scheme, which contains a primal regression task for super-resolution and a dual regression task to project super-resolved images back to LR images. The primal and dual regression tasks form a closed-loop.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1SP 8 // Update the dual model 9 Update</head><label>89</label><figDesc>(xi)LP P (xi), yi +λLD D(P (xi)), xi D by minimizing the objective: 10 m+n i=1 λLD D(P (xi)), xi 11 end HR images. However, it greatly increases the computation cost (approximately 2× of the original SR model) and the performance improvement is very limited (See results in supplementary). In practice, we only add the dual regression loss on LR data, which significantly improves the performance while preserving the approximately the same cost to the original SR model (See discussions in Section 4.1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The architecture of DRN for 4× SR. DRN contains a primal network and a dual network (marked as red lines). The green box denotes the feature maps of the downsampling module (left half) while the yellow box refers to the feature maps of the upsampling module (right half). Following U-Net, we concatenate the corresponding shallow and deep feature maps via shortcut connections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Performance vs. model size for 4× SR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Performance vs. model size for 8× SR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Comparisons of the performance and the number of parameters among different 4× SR models on the Set5 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Visual comparison of different methods for (a) 4× and (b) 8× image super-resolution.</figDesc><graphic url="image-10.png" coords="6,57.63,418.40,236.25,100.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Visual comparison of model adaptation to real-world video frames (from YouTube) for 8× SR.</figDesc><graphic url="image-12.png" coords="7,314.77,254.20,224.43,97.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Comparisons of the performance on unpaired data with Nearest degradation (testing on Set5) for 4 × SR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Proof 1 2 C 2</head><label>122</label><figDesc>Based on Hoeffding's inequality, since L P (P (x), y) + λL D (D(P (x)), x) is bounded in [0, C], for any (P, D) ∈ H dual , then P E(P, D) − Ê(P, D) &gt; ǫ ≤ e − 2N ǫ Based on the union bound, we have P ∃(P, D) ∈ H dual : E(P, D) − Ê(P, D) &gt; ǫ ≤ (P,D)∈H dual P E(P, D) − Ê(P, D) &gt; ǫ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure A .</head><label>A</label><figDesc>Figure A. The SRResNet architecture equipped with the proposed dual regression scheme for 4× SR.</figDesc><graphic url="image-13.png" coords="14,74.86,491.05,445.51,104.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure C .</head><label>C</label><figDesc>Figure C. Visual comparison for 4× image super-resolution on benchmark datasets.</figDesc><graphic url="image-16.png" coords="16,116.94,452.06,361.33,111.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure D .</head><label>D</label><figDesc>Figure D. Visual comparison for 8× image super-resolution on benchmark datasets.</figDesc><graphic url="image-19.png" coords="17,116.94,353.40,361.34,111.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure E .</head><label>E</label><figDesc>Figure E. Visual comparison of model adaptation for 8× super-resolution on real-world video frames (from YouTube).</figDesc><graphic url="image-21.png" coords="17,301.35,561.51,232.65,100.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm 1 :</head><label>1</label><figDesc>Adaptation Algorithm on Unpaired Data.</figDesc><table /><note>Input: Unpaired real-world data: S U ; Paired synthetic data: S P ; Batch sizes for S U and S P : m and n; Indicator function: 1 S P (•). 1 Load the pretrained models P and D. 2 while not convergent do 3 Sample unlabeled data {x i } m i=1 from S U ; 4 Sample labeled data {(x i , y i )} m+n i=m+1 from S P ; 5 // Update the primal model 6 Update P by minimizing the objective:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Performance comparison with state-of-the-art algorithms for 4× and 8× image super-resolution. The bold number indicates the best result and the blue number indicates the second best result. "-" denotes the results that are not reported.</figDesc><table><row><cell>Algorithms</cell><cell>Scale #Params (M)</cell><cell>Set5 PSNR / SSIM PSNR / SSIM PSNR / SSIM PSNR / SSIM PSNR / SSIM Set14 BSDS100 Urban100 Manga109</cell></row><row><cell>Bicubic</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Adaptation performance of super-resolution models on images with different degradation methods for 8× SR.</figDesc><table><row><cell>Algorithms</cell><cell>Degradation</cell><cell cols="5">Set5 PSNR / SSIM PSNR / SSIM PSNR / SSIM PSNR / SSIM PSNR / SSIM Set14 BSDS100 Urban100 Manga109</cell></row><row><cell>Nearest</cell><cell></cell><cell>21.22 / 0.560</cell><cell>20.11 / 0.485</cell><cell>20.64 / 0.471</cell><cell>17.76 / 0.454</cell><cell>18.51 / 0.594</cell></row><row><cell>EDSR [26]</cell><cell></cell><cell>19.56 / 0.580</cell><cell>18.24 / 0.498</cell><cell>18.53 / 0.479</cell><cell>15.68 / 0.435</cell><cell>17.22 / 0.598</cell></row><row><cell>DBPN [16] RCAN [51]</cell><cell>Nearest</cell><cell>18.80 / 0.541 18.33 / 0.534</cell><cell>17.36 / 0.461 17.11 / 0.436</cell><cell>17.94 / 0.456 17.67 / 0.444</cell><cell>15.07 / 0.400 14.73 / 0.380</cell><cell>16.67 / 0.550 16.25 / 0.525</cell></row><row><cell>CinCGAN [43]</cell><cell></cell><cell>21.76 / 0.648</cell><cell>20.64 / 0.552</cell><cell>20.89 / 0.528</cell><cell>18.21 / 0.505</cell><cell>18.86 / 0.638</cell></row><row><cell>DRN-Adapt</cell><cell></cell><cell>23.00 / 0.715</cell><cell>21.52 / 0.561</cell><cell>21.98 / 0.539</cell><cell>19.07 / 0.518</cell><cell>19.83 / 0.613</cell></row><row><cell>EDSR [26]</cell><cell></cell><cell>23.54 / 0.702</cell><cell>22.13 / 0.594</cell><cell>22.71 / 0.567</cell><cell>19.70 / 0.551</cell><cell>20.64 / 0.700</cell></row><row><cell>DBPN [16]</cell><cell></cell><cell>23.05 / 0.693</cell><cell>21.65 / 0.586</cell><cell>22.50 / 0.565</cell><cell>19.28 / 0.538</cell><cell>20.16 / 0.689</cell></row><row><cell>RCAN [51]</cell><cell>BD</cell><cell>22.23 / 0.678</cell><cell>21.01 / 0.567</cell><cell>21.85 / 0.552</cell><cell>18.36 / 0.509</cell><cell>19.34 / 0.659</cell></row><row><cell>CinCGAN [43]</cell><cell></cell><cell>23.39 / 0.682</cell><cell>22.14 / 0.581</cell><cell>22.73 / 0.554</cell><cell>20.36 / 0.538</cell><cell>20.29 / 0.670</cell></row><row><cell>DRN-Adapt</cell><cell></cell><cell>24.62 / 0.719</cell><cell>23.07 / 0.612</cell><cell>23.59 / 0.583</cell><cell>20.57 / 0.591</cell><cell>21.52 / 0.714</cell></row></table><note>erating more accurate and visually promising HR images. More results are put in the supplementary.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>The impact of the proposed dual regression scheme on super-resolution performance in terms of PSNR score on the five benchmark datasets for 4 × SR.</figDesc><table><row><cell cols="6">Model Dual Set5 Set14 BSDS100 Urban100 Manga109</cell></row><row><cell>DRN-S</cell><cell>✗</cell><cell>32.53 28.76 32.68 28.93</cell><cell>27.68 27.78</cell><cell>26.54 26.84</cell><cell>31.21 31.52</cell></row><row><cell>DRN-L</cell><cell>✗</cell><cell>32.61 28.84 32.74 28.98</cell><cell>27.72 27.83</cell><cell>26.77 27.03</cell><cell>31.39 31.73</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Effect of the hyper-parameter λ in Eqn. (1) on the performance of DRN-S (testing on Set5) for 4 × SR.</figDesc><table><row><cell>λ</cell><cell>0.001 0.01</cell><cell>0.1</cell><cell>1.0</cell><cell>10</cell></row><row><cell cols="5">PSNR on Set5 32.57 32.61 32.67 32.51 32.37</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>H dual ∈P×D is smaller than the capacity of the function space H∈P or H∈D in traditional supervised learning, i.e., RDL</figDesc><table><row><cell>Z</cell><cell>≤ RSL Z , where RSL Z is</cell></row><row><cell cols="2">Rademacher complexity defined in supervised learning. In other words, dual learning has a smaller generalization bound</cell></row><row><cell cols="2">than supervised learning and the proposed dual regression model helps the primal model to achieve more accurate SR</cell></row><row><cell>predictions.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table B .</head><label>B</label><figDesc>The impact of the proposed dual regression scheme on the SRResNet model in terms of PSNR score on the five benchmark datasets for 4× SR.</figDesc><table><row><cell>Method</cell><cell>Set5</cell><cell cols="4">Set14 BSDS100 Urban100 Manga109</cell></row><row><cell>SRResNet</cell><cell cols="2">32.26 28.53</cell><cell>27.61</cell><cell>26.24</cell><cell>31.03</cell></row><row><cell cols="3">SRResNet + Dual 32.47 28.77</cell><cell>27.70</cell><cell>26.58</cell><cell>31.24</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table C .</head><label>C</label><figDesc>The impact of the dual regression loss on HR data for 4× SR. We take DRN-S as the baseline model.</figDesc><table><row><cell>Method</cell><cell>MAdds</cell><cell>Set5</cell><cell cols="3">Set14 BSDS100 Urban100 Manga109</cell></row><row><cell cols="4">DRN-S with dual HR 51.20G 32.69 28.93</cell><cell>27.79</cell><cell>26.85</cell><cell>31.54</cell></row><row><cell>DRN-S (Ours)</cell><cell cols="3">25.60G 32.68 28.93</cell><cell>27.78</cell><cell>26.84</cell><cell>31.52</cell></row><row><cell cols="6">D.3. Impact of Different Degradation Methods to Obtain Paired Synthetic Data</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table F .</head><label>F</label><figDesc>The impact of different degradation methods on DRN-Adapt for 8× SR.</figDesc><table><row><cell>Degradation Method</cell><cell>Set5</cell><cell cols="4">Set14 BSDS100 Urban100 Manga109</cell></row><row><cell>Nearest</cell><cell cols="2">24.60 23.03</cell><cell>23.60</cell><cell>20.61</cell><cell>21.46</cell></row><row><cell>Bicubic</cell><cell cols="2">24.62 23.07</cell><cell>23.59</cell><cell>20.57</cell><cell>21.52</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">The source code is available at https://github.com/guoyongcs/DRN.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partially supported by Guangdong Provincial Scientific and Technological Funds under Grants 2018B010107001, key project of NSFC 61836003, Fundamental Research Funds for the Central Universities D2191240, Program for Guangdong Introducing Innovative and Enterpreneurial Teams 2017ZT07X183, Tencent AI Lab Rhino-Bird Focused Research Program JR201902, Guangdong Special Branch Plans Young Talent with Scientific and Technological Innovation 2016TQ03X445, Guangzhou Science and Technology Planning Project 201904010197, Natural Science Foundation of Guangdong Province 2016A030313437, and Microsoft Research Asia (MSRA Collaborative Research Program).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Materials for "Closed-loop Matters: Dual Regression Networks</head><p>for Single Image Super-Resolution"</p><p>We organize our supplementary materials as follows. First, we provide the derivation of generalization error bound for the dual regression scheme in Section A. Second, we provide more details on the architecture of the proposed DRN model in Section B. Third, we provide more implementation details on the training method for the SR tasks with paired data and unpaired data in Section C. Fourth, we conduct more ablation studies on the proposed dual regression scheme in Section D. Last, we report more visual comparison results in Section E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Theoretical Analysis</head><p>In this section, we will analyze the generalization bound for the proposed method. The generalization error of the dual learning scheme is to measure how accurately the algorithm predicts for the unseen test data in the primal and dual tasks. Firstly, we will introduce the definition of the generalization error as follows:</p><p>Definition 3 Given an underlying distribution S and hypotheses P ∈ P and D ∈ D for the primal and dual tasks, where P = {P θxy (x); θ xy ∈ Θ xy } and D = {D θyx (y); θ yx ∈ Θ yx }, and Θ xy and Θ yx are parameter spaces, respectively, the generalization error (expected loss) is defined by: E(P, D) = E (x,y)∼P [L P (P (x), y) + λL D (D(P (x)), x)] , ∀P ∈ P, D ∈ D.</p><p>In practice, the goal of the dual learning is to optimize the bi-directional tasks. For any P ∈ P and D ∈ D, we define the empirical loss on the N samples as follows:</p><p>Following <ref type="bibr" target="#b32">[30]</ref>, we define Rademacher complexity for dual learning in this paper. We define the function space as H dual ∈ P × D, this Rademacher complexity can measure the complexity of the function space, that is it can capture the richness of a family of the primal and the dual models. For our application, we mildly rewrite the definition of Rademacher complexity in <ref type="bibr" target="#b32">[30]</ref> as follows:</p><p>Definition 4 (Rademacher complexity of dual learning) Given an underlying distribution S, and its empirical distribution</p><p>, where z i = (x i , y i ), then the Rademacher complexity of dual learning is defined as:</p><p>where RZ (P, D) is its empirical Rademacher complexity defined as:</p><p>.</p><p>Generalization bound. Here, we analyze the generalization bound for the proposed dual regression scheme. We first start with a simple case of finite function space. Then, we generalize it to a more general case with infinite function space.</p><p>Theorem 2 Let L P (P (x), y) + λL D (D(P (x)), x) be a mapping from X × Y to [0, C], and suppose the function space H dual is finite, then for any δ &gt; 0, with probability at least 1 − δ, the following inequality holds for all (P, D) ∈ H dual :</p><p>closed-loop, according to the architecture design of the primal model, there are 2 dual models for 4× SR and 3 dual models for 8× SR, respectively. Let B be the number of RCABs <ref type="bibr" target="#b53">[51]</ref> and F be the number of base feature channels. For 4× SR, we set B = 30 and F = 16 for DRN-S and B = 40 and F = 20 for DRN-L. For 8× SR, we set B = 30 and F = 8 for DRN-S and B = 36 and F = 10 for DRN-L. Moreover, we set the reduction ratio r = 16 in all RCABs for our DRN model and set the negative slope to 0.2 for all LeakyReLU in DRN. We show the detailed architecture of the 8× DRN model in Table <ref type="table">A</ref>. To obtain the 4× model, one can simply remove one basic block from the 8× model. <ref type="table">A</ref>, we use Conv(1,1) and Conv <ref type="bibr" target="#b5">(3,</ref><ref type="bibr" target="#b5">3)</ref> to represent the convolution layer with the kernel size of 1 × 1 and 3 × 3, respectively. We use Conv s2 to represent the convolution layer with the stride of 2. Following the settings of EDSR <ref type="bibr" target="#b28">[26]</ref>, we build the Upsampler with one convolution layer and one pixel-shuffle <ref type="bibr" target="#b35">[33]</ref> layer to upscale the feature maps. Moreover, we use h and w to represent the height and width of the input LR images. Thus, the shape of output images should be 8h × 8w for the 8× model. Training data. Following <ref type="bibr" target="#b39">[37]</ref>, we train our model on DIV2K <ref type="bibr" target="#b36">[34]</ref> and Flickr2K <ref type="bibr" target="#b28">[26]</ref> datasets, which contain 800 and 2650 training images separately. We use the RGB input patches of size 48 × 48 from LR images and the corresponding HR patches as the paired training data, and augment the training data following the method in <ref type="bibr" target="#b28">[26,</ref><ref type="bibr" target="#b53">51]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in Table</head><p>Test data. For quantitative comparison on paired data, we evaluate different SR models using five benchmark datasets, including SET5 <ref type="bibr" target="#b5">[3]</ref>, SET14 <ref type="bibr" target="#b49">[47]</ref>, BSDS100 <ref type="bibr" target="#b3">[1]</ref>, URBAN100 <ref type="bibr" target="#b23">[21]</ref> and MANGA109 <ref type="bibr" target="#b31">[29]</ref>. Implementation details. For training, we apply Adam with β 1 = 0.9, β 2 = 0.99 and set minibatch size as 32. The learning rate is initialized to 10 −4 and decreased to 10 −7 with a cosine annealing out of 10 6 iterations in total.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<idno>35] 2.0 32.02 / 0.893 28.50 / 0.778 27.53 / 0.733 26.05 / 0.781 29.49 / 0.899</idno>
	</analytic>
	<monogr>
		<title level="j">SRDenseNet</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<idno>35] 2.3 25.99 / 0.704 24.23 / 0.581 24.45 / 0.530 21.67 / 0.562 23.09 / 0.712</idno>
	</analytic>
	<monogr>
		<title level="j">SRDenseNet</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Visual comparison for 4× super-resolution. (b) Visual comparison for 8× super-resolution</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Blind super-resolution kernel estimation using an internal-gan</title>
		<author>
			<persName><forename type="first">Sefi</forename><surname>Bell-Kligler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Assaf</forename><surname>Shocher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="284" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Low-complexity single-image super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aline</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Line</forename><surname>Alberi-Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adversarial learning with local coordinate coding</title>
		<author>
			<persName><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-marginal wasserstein gan</title>
		<author>
			<persName><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Langyuan</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1774" to="1784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Relation attention for temporal action localization</title>
		<author>
			<persName><forename type="first">Peihao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangyao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Intelligent home 3d: Automatic 3d-house design from linguistic descriptions only</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Second-order attention network for single image super-resolution</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianrui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongbing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Qing Du, Anton Van Den Hengel, Qinfeng Shi, and Mingkui Tan. Multi-way backpropagation for training compact deep neural networks</title>
		<author>
			<persName><forename type="first">Yong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Dual reconstruction nets for image super-resolution with gradient sensitive loss</title>
		<author>
			<persName><forename type="first">Yong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanwu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.07099</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Auto-embedding generative adversarial networks for high resolution image synthesis</title>
		<author>
			<persName><forename type="first">Yong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinfeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Hierarchical neural architecture search for single image super-resolution</title>
		<author>
			<persName><forename type="first">Yong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongsheng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04619</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Anton Van Den Hengel, and Qinfeng Shi. The shallow end: Empowering shallower deep-convolutional networks through auxiliary outputs</title>
		<author>
			<persName><forename type="first">Yong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01773</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Double forward propagation for memorized batch normalization</title>
		<author>
			<persName><forename type="first">Yong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Nat: Neural architecture transformer for accurate and compact architectures</title>
		<author>
			<persName><forename type="first">Yong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep back-projection networks for super-resolution</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norimichi</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2007">2018. 1, 2, 4, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dual learning for machine translation</title>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tieyan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="820" to="828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cubic splines for image interpolation and digital filtering</title>
		<author>
			<persName><forename type="first">Hsieh</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><surname>Andrews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="508" to="517" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-level visual-semantic alignments with relationwise dual attention network for image and text matching</title>
		<author>
			<persName><forename type="first">Zhibin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongsheng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Single image super-resolution from transformed self-exemplars</title>
		<author>
			<persName><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Super-resolution 1h magnetic resonance spectroscopic imaging utilizing deep learning</title>
		<author>
			<persName><forename type="first">Zohaib</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilbert</forename><surname>Hangel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislav</forename><surname>Motyka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Bogner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in oncology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate super-resolution</title>
		<author>
			<persName><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Photorealistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2008">2017. 1, 6, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Feedback network for image superresolution</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinglei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaomin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gwanggil</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3867" to="3876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyoung</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
				<imprint>
			<date type="published" when="2006">2017. 1, 4, 6</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bohan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuangwei</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.01050</idno>
		<title level="m">Discrimination-aware network pruning for deep model compression</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName><forename type="first">Awni</forename><forename type="middle">Y</forename><surname>Andrew L Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Yusuke</forename><surname>Matsui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kota</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuji</forename><surname>Aramaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azuma</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toru</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
		<title level="m">Sketch-based manga retrieval using manga109 dataset. Multimedia Tools and Applications</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning</title>
		<author>
			<persName><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>MIT Press</publisher>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Andrew P Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehan</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Methods and results</title>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyoung</forename><surname>Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Image super-resolution using dense skip connections</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiejie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinquan</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4809" to="4817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep image prior</title>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9446" to="9454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Esrgan: Enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshops</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamid</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eero</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep networks for image super-resolution with sparse prior</title>
		<author>
			<persName><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="370" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dual supervised learning</title>
		<author>
			<persName><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Model-level dual learning</title>
		<author>
			<persName><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unsupervised dual learning for image-to-image translation</title>
		<author>
			<persName><forename type="first">Zili</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><forename type="middle">Tan</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unsupervised image superresolution using cycle-in-cycle generative adversarial networks</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongbing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
				<imprint>
			<date type="published" when="2007">2018. 2, 3, 4, 7</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Breaking winner-takes-all: Iterative-winners-out networks for weakly supervised temporal action localization</title>
		<author>
			<persName><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peihao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for temporal action localization</title>
		<author>
			<persName><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2001">Oct 2019. 1</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Dense regression network for video grounding</title>
		<author>
			<persName><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peihao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2001">June 2020. 1</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparse-representations</title>
		<author>
			<persName><forename type="first">Roman</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matan</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Curves and Surfaces</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning a single convolutional super-resolution network for multiple degradations</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Ranksrgan: Generative adversarial networks with ranker for image super-resolution</title>
		<author>
			<persName><forename type="first">Wenlong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3096" to="3105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">From whole slide imaging to microscopy: Deep microscopy adaptation network for histopathology cancer image classification</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanbo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
		</author>
		<editor>MICCAI. Springer</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2006">2018. 1, 2, 4, 6</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Collaborative unsupervised domain adaptation for medical image diagnosis</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuaicheng</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Imaging meets NeurIPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deep mutual learning</title>
		<author>
			<persName><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Unsupervised degradation learning for single image super-resolution</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongwei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghua</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04240</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Kernel modeling superresolution on real low-resolution images</title>
		<author>
			<persName><forename type="first">Ruofan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabine</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
