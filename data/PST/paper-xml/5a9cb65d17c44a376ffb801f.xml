<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Learning for Sentiment Analysis: A Survey</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>LinkedIn</roleName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
							<email>lzhang32@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Shuai</forename><surname>Wang</surname></persName>
							<email>shuaiwanghk@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
							<email>liub@uic.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois at Chicago</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Learning for Sentiment Analysis: A Survey</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning has emerged as a powerful machine learning technique that learns multiple layers of representations or features of the data and produces state-of-the-art prediction results. Along with the success of deep learning in many other application domains, deep learning is also popularly used in sentiment analysis in recent years. This paper first gives an overview of deep learning and then provides a comprehensive survey of its current applications in sentiment analysis.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>Sentiment analysis or opinion mining is the computational study of people's opinions, sentiments, emotions, appraisals, and attitudes towards entities such as products, services, organizations, individuals, issues, events, topics, and their attributes. <ref type="bibr" target="#b0">1</ref> The inception and rapid growth of the field coincide with those of the social media on the Web, for example, reviews, forum discussions, blogs, micro-blogs, Twitter, and social networks, because for the first time in human history, we have a huge volume of opinionated data recorded in digital forms. Since early 2000, sentiment analysis has grown to be one of the most active research areas in natural language processing (NLP). It is also widely studied in data mining, Web mining, text mining, and information retrieval. In fact, it has spread from computer science to management sciences and social sciences such as marketing, finance, political science, communications, health science, and even history, due to its importance to business and society as a whole. This proliferation is due to the fact that opinions are central to almost all human activities and are key influencers of our behaviours. Our beliefs and perceptions of reality, and the choices we make, are, to a considerable degree, conditioned upon how others see and evaluate the world. For this reason, whenever we need to make a decision we often seek out the opinions of others. This is not only true for individuals but also true for organizations.</p><p>Nowadays, if one wants to buy a consumer product, one is no longer limited to asking one's friends and family for opinions because there are many user reviews and discussions about the product in public forums on the Web. For an organization, it may no longer be necessary to conduct surveys, opinion polls, and focus groups in order to gather public opinions because there is an abundance of such information publicly available. In recent years, we have witnessed that opinionated postings in social media have helped reshape businesses, and sway public sentiments and emotions, which have profoundly impacted on our social and political systems. Such postings have also mobilized masses for political changes such as those happened in some Arab countries in 2011. It has thus become a necessity to collect and study opinions <ref type="bibr" target="#b0">1</ref> .</p><p>However, finding and monitoring opinion sites on the Web and distilling the information contained in them remains a formidable task because of the proliferation of diverse sites. Each site typically contains a huge volume of opinion text that is not always easily deciphered in long blogs and forum postings. The average human reader will have difficulty identifying relevant sites and extracting and summarizing the opinions in them. Automated sentiment analysis systems are thus needed. Because of this, there are many start-ups focusing on providing sentiment analysis services. Many big corporations have also built their own in-house capabilities. These practical applications and industrial interests have provided strong motivations for research in sentiment analysis.</p><p>Existing research has produced numerous techniques for various tasks of sentiment analysis, which include both supervised and unsupervised methods. In the supervised setting, early papers used all types of supervised machine learning methods (such as Support Vector Machines (SVM), Maximum Entropy, Naïve Bayes, etc.) and feature combinations. Unsupervised methods include various methods that exploit sentiment lexicons, grammatical analysis, and syntactic patterns. Several survey books and papers have been published, which cover those early methods and applications extensively. <ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3</ref> Since about a decade ago, deep learning has emerged as a powerful machine learning technique <ref type="bibr" target="#b3">4</ref> and produced state-of-the-art results in many application domains, ranging from computer vision and speech recognition to NLP. Applying deep learning to sentiment analysis has also become very popular recently. This paper first gives an overview of deep learning and then provides a comprehensive survey of the sentiment analysis research based on deep learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NEURAL NETWORKS</head><p>Deep learning is the application of artificial neural networks (neural networks for short) to learning tasks using networks of multiple layers. It can exploit much more learning (representation) power of neural networks, which once were deemed to be practical only with one or two layers and a small amount of data.</p><p>Inspired by the structure of the biological brain, neural networks consist of a large number of information processing units (called neurons) organized in layers, which work in unison. It can learn to perform tasks (e.g., classification) by adjusting the connection weights between neurons, resembling the learning process of a biological brain. Based on network topologies, neural networks can generally be categorized into feedforward neural networks and recurrent/recursive neural networks, which can also be mixed and matched. We will describe recurrent/recursive neural networks later. A simple example of a feedforward neural network is given in Figure <ref type="figure" target="#fig_0">1</ref>, which consists of three layers 𝐿 ! , 𝐿 ! and 𝐿 ! . 𝐿 ! is the input layer, which corresponds to the input vector (𝑥 ! , 𝑥 ! , 𝑥 ! ) and intercept term +1. 𝐿 ! is the output layer, which corresponds to the output vector (𝑠 ! ). 𝐿 ! is the hidden layer, whose output is not visible as a network output. A circle in 𝐿 ! represents an element in the input vector, while a circle in 𝐿 ! or 𝐿 ! represents a neuron, the basic computation element of a neural network. We also call it an activation function. A line between two neurons represents a connection for the flow of information. Each connection is associated with a weight, a value controlling the signal between two neurons. The learning of a neural network is achieved by adjusting the weights between neurons with the information flowing through them. Neurons read output from neurons in the previous layer, process the information, and then generate output to neurons in the next layer. As in Figure <ref type="figure" target="#fig_0">1</ref>, the neutral network alters weights based on training examples (𝑥 <ref type="bibr">(!)</ref> , 𝑦 <ref type="bibr">(!)</ref> ). After the training process, it will obtain a complex form of hypotheses ℎ !,! (𝑥) that fits the data.</p><p>Diving into the hidden layer, we can see that each neuron in 𝐿 ! takes input 𝑥 ! , 𝑥 ! , 𝑥 ! and intercept +1 from 𝐿 ! , and outputs a value 𝑓(𝑊 ! 𝑥) = 𝑓( 𝑊 ! 𝑥 ! + 𝑏)</p><formula xml:id="formula_0">! !!!</formula><p>by the activation function 𝑓. 𝑊 ! are weights of the connections; 𝑏 is the intercept or bias; 𝑓 is normally non-linear. The common choices of 𝑓 are sigmoid function, hyperbolic tangent function (tanh), or rectified linear function (ReLU). Their equations are as follows.</p><formula xml:id="formula_1">𝑓 𝑊 ! 𝑥 = 𝑠𝑖𝑔𝑚𝑜𝑖𝑑 𝑊 ! 𝑥 = ! !!!"# !! ! ! (1) 𝑓 𝑊 ! 𝑥 = tanh 𝑊 ! 𝑥 = ! ! ! ! !! !! ! ! ! ! ! ! !! !! ! ! (2) 𝑓 𝑊 ! 𝑥 = 𝑅𝑒𝐿𝑈 𝑊 ! 𝑥 = max 0, 𝑊 ! 𝑥<label>(3)</label></formula><p>The sigmoid function takes a real-valued number and squashes it to a value in the range between 0 and 1. The function has been in frequent use historically due to its nice interpretation as the firing rate of a neuron: 0 for not firing or 1 for firing. But the non-linearity of the sigmoid has recently fallen out of favour because its activations can easily saturate at either tail of 0 or 1, where gradients are almost zero and the information flow would be cut. What is more is that its output is not zerocentered, which could introduce undesirable zig-zagging dynamics in the gradient updates for the connection weights in training. Thus, the tanh function is often more preferred in practice as its output range is zero-centered, [-1, 1] instead of [0, 1]. The ReLU function has also become popular lately. Its activation is simply thresholded at zero when the input is less than 0. Compared with the sigmoid function and the tanh function, ReLU is easy to compute, fast to converge in training and yields equal or better performance in neural networks. <ref type="bibr" target="#b4">5</ref> In 𝐿 ! , we can use the softmax function as the output neuron, which is a generalization of the logistic function that squashes a K-dimensional vector 𝑋 of arbitrary real values to a K-dimensional vector 𝜎(𝑋) of real values in the range (0, 1) that add up to 1. The function definition is as follows.</p><formula xml:id="formula_2">𝜎 𝑋 ! = ! ! ! ! ! ! ! !!! 𝑓𝑜𝑟 𝑗 = 1, … , 𝑘<label>(4)</label></formula><p>Generally, softmax is used in the final layer of neural networks for final classification in feedforward neural networks.</p><p>By connecting together all neurons, the neural network in Figure <ref type="figure" target="#fig_0">1</ref> has parameters (𝑊, 𝑏) = (𝑊 ! , 𝑏 ! , 𝑊 ! , 𝑏 (!) ), where 𝑊 !" (!) denotes the weight associated with the connection between neuron 𝑗 in layer 𝑙, and neuron 𝑖 in layer 𝑙 + 1. 𝑏 ! (!) is the bias associated with neuron 𝑖 in layer 𝑙 + 1.</p><p>To train a neural network, stochastic gradient descent via backpropagation <ref type="bibr" target="#b5">6</ref> is usually employed to minimize the cross-entropy loss, which is a loss function for softmax output. Gradients of the loss function with respect to weights from the last hidden layer to the output layer are first calculated, and then gradients of the expressions with respect to weights between upper network layers are calculated recursively by applying the chain rule in a backward manner. With those gradients, the weights between layers are adjusted accordingly. It is an iterative refinement process until certain stopping criteria are met. The pseudo code for training the neural network in Figure <ref type="figure" target="#fig_0">1</ref> is as follows.  <ref type="figure" target="#fig_0">1</ref>.</p><p>The above algorithm can be extended to generic feedforward neural network training with multiple hidden layers. Note that stochastic gradient descent estimates the parameters for every training example as opposed to the whole set of training examples in batch gradient descent. Therefore, the parameter updates have a high variance and cause the loss function to fluctuate to different intensities, which helps discover new and possibly better local minima.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DEEP LEARNING</head><p>The research community lost interests in neural networks in late 1990s mainly because they were regarded as only practical for "shallow" neural networks (neural networks with one or two layers) as training a "deep" neural network (neural networks with more layers) is complicated and computationally very expensive. However, in the past 10 years, deep learning made breakthrough and produced state-of-the-art results in many application domains, starting from computer vision, then speech recognition, and more recently, NLP. <ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8</ref> The renaissance of neural networks can be attributed to many factors. Most important ones include: (1) the availability of computing power due to the advances in hardware (e.g., GPUs), (2) the availability of huge amounts of training data, and (3) the power and flexibility of learning intermediate representations. <ref type="bibr" target="#b8">9</ref> In a nutshell, deep learning uses a cascade of multiple layers of nonlinear processing units for feature extraction and transformation. The lower layers close to the data input learn simple features, while higher layers learn more complex features derived from lower layer features. The architecture forms a hierarchical and powerful feature representation. Figure <ref type="figure" target="#fig_1">2</ref> shows the feature hierarchy from the left (a lower layer) to the right (a higher layer) learned by deep learning in face image classification. <ref type="bibr" target="#b9">10</ref> We can see that the learned image features grow in complexity, starting from blobs/edges, then noses/eyes/cheeks, to faces. In recent years, deep learning models have been extensively applied in the field of NLP and show great potentials. In the following several sections, we briefly describe the main deep learning architectures and related techniques that have been applied to NLP tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WORD EMBEDDING</head><p>Many deep learning models in NLP need word embedding results as input features. <ref type="bibr" target="#b6">7</ref> Word embedding is a technique for language modelling and feature learning, which transforms words in a vocabulary to vectors of continuous real numbers (e.g., 𝑤𝑜𝑟𝑑 "ℎ𝑎𝑡" → (… , 0.15, … , 0.23, … , 0.41, … ) ). The technique normally involves a mathematic embedding from a high-dimensional sparse vector space (e.g., one-hot encoding vector space, in which each word takes a dimension) to a lower-dimensional dense vector space. Each dimension of the embedding vector represents a latent feature of a word. The vectors may encode linguistic regularities and patterns.</p><p>The learning of word embeddings can be done using neural networks <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref> or matrix factorization. <ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17</ref> One commonly used word embedding system is Word2Vec i , which is essentially a computationallyefficient neural network prediction model that learns word embeddings from text. It contains Continuous Bag-of-Words model (CBOW) <ref type="bibr" target="#b12">13</ref> , and Skip-Gram model (SG) <ref type="bibr" target="#b13">14</ref> . The CBOW model predicts the target word (e.g., "wearing") from its context words ("the boy is _ a hat", where "_" denotes the target word), while the SG model does the inverse, predicting the context words given the target word. Statistically, the CBOW model smoothens over a great deal of distributional information by treating the entire context as one observation. It is effective for smaller datasets. However, the SG model treats each context-target pair as a new observation and is better for larger datasets. Another frequently used learning approach is Global Vector ii (GloVe) <ref type="bibr" target="#b16">17</ref> , which is trained on the nonzero entries of a global word-word co-occurrence matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AUTOENCODER AND DENOISING AUTOENCODER</head><p>Autoencoder Neural Network is a three-layer neural network, which sets the target values to be equal to the input values. Figure <ref type="figure" target="#fig_2">3</ref> shows an example of an autoencoder architecture. Given the input vector 𝑥 ∈ [0,1] ! , the autoencoder first maps it to a hidden representation 𝑦 ∈ [0,1] ! ! by an encoder function ℎ(•) (e.g., the sigmoid function). The latent representation 𝑦 is then mapped back by a decoder function 𝑔(•) into a reconstruction 𝑟 𝑥 = 𝑔(ℎ 𝑥 ) . The autoencoder is typically trained to minimize a form of reconstruction error 𝑙𝑜𝑠𝑠(𝑥, 𝑟 𝑥 ). The objective of the autoencoder is to learn a representation of the input, which is the activation of the hidden layer. Due to the nonlinear function ℎ(•) and 𝑔(•), the autoencoder is able to learn nonlinear representations, which give it much more expressive power than its linear counterparts, such as Principal Component Analysis (PCA) or Latent Semantic Analysis (LSA).</p><p>One often stacks autoencoders into layers. A higher level autoencoder uses the output of the lower one as its training data. The stacked autoencoders <ref type="bibr" target="#b17">18</ref> along with Restricted Boltzmann Machines (RBMs) <ref type="bibr" target="#b18">19</ref> are earliest approaches to building deep neural networks. Once a stack of autoencoders has been trained in an unsupervised fashion, their parameters describing multiple levels of representations for 𝑥 (intermediate representations) can be used to initialize a supervised deep neural network, which has been shown empirically better than random parameter initialization.</p><p>The Denoising Autoencoder (DAE) <ref type="bibr" target="#b19">20</ref> is an extension of autoencoder, in which the input vector 𝑥 is stochastically corrupted into a vector 𝑥. And the model is trained to denoise it, that is, to minimize a denoising reconstruction error 𝑙𝑜𝑠𝑠(𝑥, 𝑟 𝑥 ). The idea behind DAE is to force the hidden layer to discover more robust features and prevent it from simply learning the identity. A robust model should be able to reconstruct the input well even in the presence of noises. For example, deleting or adding a few of words from or to a document should not change the semantic of the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONVOLUTIONAL NEURAL NETWORK</head><p>Convolutional Neural Network (CNN) is a special type of feedforward neural network originally employed in the field of computer vision. Its design is inspired by the human visual cortex, a visual mechanism in animal brain. The visual cortex contains a lot of cells that are responsible for detecting light in small and overlapping sub-regions of the visual fields, which are called receptive fields. These cells act as local filters over the input space. CNN consists of multiple convolutional layers, each of which performs the function that is processed by the cells in the visual cortex.</p><p>Figure <ref type="figure" target="#fig_4">4</ref> shows a CNN for recognizing traffic signs. <ref type="bibr" target="#b20">21</ref> The input is a 32x32x1 pixel image (32 x 32 represents image width x height; 1 represents input channel). In this first stage, the filter (size 5x5x1) is used to scan the image. Each region in the input image that the filter projects on is a receptive field. The filter is actually an array of numbers (called weights or parameters). As the filter is sliding (or convolving), it is multiplying its weight values with the original pixel values of the image (element wise multiplications). The multiplications are all summed up to a single number, which is a representative of the receptive field. Every receptive field produces a number. After the filter finishes scanning over the image, we can get an array (size 28x28x1), which is called the activation map or feature map. In CNN, we need to use different filters to scan the input. In Figure <ref type="figure" target="#fig_4">4</ref>, we apply 108 kinds of filters and thus have 108 stacked feature maps in the first stage, which consists of the first convolutional layer. Following the convolutional layer, a subsampling (or pooling) layer is usually used to progressively reduce the spatial size of the representation, thus to reduce the number of features and the computational complexity of the network. For example, after subsampling in the first stage, the convolutional layer reduces its dimensions to (14x14x108). Note that while the dimensionality of each feature map is reduced, the subsampling step retains the most important information, with a commonly used subsampling operation being the max pooling. Afterwards, the output from the first stage becomes input to the second stage and the new filters are employed. The new filter size is 5x5x108, where 108 is the feature map size of the last layer. After the second stage, CNN uses a fully connected layer and then a softmax readout layer with output classes for classification.</p><p>Convolutional layers in CNN play the role of feature extractor, which extracts local features as they restrict the receptive fields of the hidden layers to be local. It means that CNN has a special spatiallylocal correlation by enforcing a local connectivity pattern between neurons of adjacent layers. Such a characteristic is useful for classification in NLP, in which we expect to find strong local clues regarding class membership, but these clues can appear in different places in the input. For example, in a document classification task, a single key phrase (or an n-gram) can help in determining the topic of the document. We would like to learn that certain sequences of words are good indicators of the topic, and do not necessarily care where they appear in the document. Convolutional and pooling layers allow the CNN to learn to find such local indicators, regardless of their positions.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RECURRENT NEURAL NETWORK</head><p>Recurrent Neural Network (RNN) <ref type="bibr" target="#b21">22</ref> is a class of neural networks whose connections between neurons form a directed cycle. Unlike feedforward neural networks, RNN can use its internal "memory" to process a sequence of inputs, which makes it popular for processing sequential information. The "memory" means that RNN performs the same task for every element of a sequence with each output being dependent on all previous computations, which is like "remembering" information about what has been processed so far.</p><p>Figure <ref type="figure">5</ref>: Recurrent neural network Figure <ref type="figure">5</ref> shows an example of a RNN. The left graph is an unfolded network with cycles, while the right graph is a folded sequence network with three time steps. The length of time steps is determined by the length of input. For example, if the word sequence to be processed is a sentence of six words, the RNN would be unfolded into a neural network with six time steps or layers. One layer corresponds to a word.</p><p>In Figure <ref type="figure">5</ref>, 𝑥 ! is the input vector at time step 𝑡. ℎ ! is the hidden state at time step 𝑡, which is calculated based on the previous hidden state and the input at the current time step.</p><formula xml:id="formula_3">ℎ ! = 𝑓 𝑤 !! ℎ !!! + 𝑤 !! 𝑥 !<label>(5)</label></formula><p>In Equation ( <ref type="formula" target="#formula_3">5</ref>), the activation function 𝑓 is usually the tanh function or the ReLU function. 𝑤 !! is the weight matrix used to condition the input 𝑥 ! . 𝑤 !! is the weight matrix used to condition the previous hidden state ℎ !!! .</p><p>𝑦 ! is the output probability distribution over the vocabulary at step t. For example, if we want to predict the next word in a sentence, it would be a vector of probabilities across the word vocabulary.</p><formula xml:id="formula_4">𝑦 ! = 𝑠𝑜𝑓𝑡𝑚𝑎𝑥 𝑤 !! ℎ !<label>(6)</label></formula><p>The hidden state ℎ ! is regarded as the memory of the network. It captures information about what happened in all previous time steps. 𝑦 ! is calculated solely based on the memory ℎ ! at time 𝑡 and the corresponding weight matrix 𝑤 !! .</p><p>Unlike a feedforward neural network, which uses different parameters at each layer, RNN shares the same parameters (𝑊 !! , 𝑊 !! , 𝑊 !! ) across all steps. This means that it performs the same task at each step, just with different inputs. This greatly reduces the total number of parameters needed to learn.</p><p>Theoretically, RNN can make use of the information in arbitrarily long sequences, but in practice, the standard RNN is limited to looking back only a few steps due to the vanishing gradient or exploding gradient problem.  For instance, to predict a missing word in a sequence, we may need to look at both the left and the right context. A bidirectional RNN 24 consists of two RNNs, which are stacked on the top of each other.</p><p>The one that processes the input in its original order and the one that processes the reversed input sequence. The output is then computed based on the hidden state of both RNNs. Deep bidirectional RNN is similar to bidirectional RNN. The only difference is that it has multiple layers per time step, which provides higher learning capacity but needs a lot of training data. Figure <ref type="figure" target="#fig_6">6</ref> shows examples of bidirectional RNN and deep bidirectional RNN (with two layers) respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LSTM NETWORK</head><p>Long Short Term Memory network (LSTM) <ref type="bibr" target="#b24">25</ref> is a special type of RNN, which is capable of learning long-term dependencies.</p><p>All RNNs have the form of a chain of repeating modules. In standard RNNs, this repeating module normally has a simple structure. However, the repeating module for LSTM is more complicated. Instead of having a single neural network layer, there are four layers interacting in a special way.</p><p>Besides, it has two states: hidden state and cell state. , where 1 means "completely keep" and 0 means "completely dump" in Equation (7).</p><formula xml:id="formula_5">𝑓 ! = 𝜎 𝑊 ! 𝑥 ! + 𝑈 ! ℎ !!!<label>(7)</label></formula><p>Then LSTM decides what new information to store in the cell state. This has two steps. First, a sigmoid function/layer, called the "input gate" as Equation ( <ref type="formula" target="#formula_6">8</ref>), decides which values LSTM will update. Next, a tanh function/layer creates a vector of new candidate values 𝐶 ! , which will be added to the cell state. LSTM combines these two to create an update to the state.</p><formula xml:id="formula_6">𝑖 ! = 𝜎 𝑊 ! 𝑥 ! + 𝑈 ! ℎ !!! (<label>8</label></formula><formula xml:id="formula_7">)</formula><formula xml:id="formula_8">𝐶 ! = tanh 𝑊 ! 𝑥 ! + 𝑈 ! ℎ !!!<label>(9)</label></formula><p>It is now time to update the old cell state 𝐶 !!! into new cell state 𝐶 ! as Equation (10). Note that forget gate 𝑓 ! can control the gradient passes through it and allow for explicit "memory" deletes and updates, which helps alleviate vanishing gradient or exploding gradient problem in standard RNN.</p><formula xml:id="formula_9">𝐶 ! = 𝑓 ! * 𝐶 !!! + 𝑖 ! * 𝐶 !<label>(10)</label></formula><p>Finally, LSTM decides the output, which is based on the cell state. LSTM first runs a sigmoid layer, which decides which parts of the cell state to output in Equation ( <ref type="formula" target="#formula_10">11</ref>), called "output gate". Then, LSTM puts the cell state through the tanh function and multiplies it by the output of the sigmoid gate, so that LSTM only outputs the parts it decides to as Equation (12).</p><formula xml:id="formula_10">𝑜 ! = 𝜎 𝑊 ! 𝑥 ! + 𝑈 ! ℎ !!!<label>(11)</label></formula><formula xml:id="formula_11">ℎ ! = 𝑜 ! * tanh 𝐶 !<label>(12)</label></formula><p>LSTM is commonly applied to sequential data but can also be used for tree-structured data. Tai et al. <ref type="bibr" target="#b25">26</ref> introduced a generalization of the standard LSTM to Tree-structured LSTM (Tree-LSTM) and showed better performances for representing sentence meaning than a sequential LSTM.</p><p>A slight variation of LSTM is the Gated Recurrent Unit (GRU). <ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28</ref> It combines the "forget" and "input" gates into a single update gate. It also merges the cell state and hidden state, and makes some other changes. The resulting model is simpler than the standard LSTM model, and has been growing in popularity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ATTENTION MECHANISM WITH RECURRENT NEURAL NETWORK</head><p>Supposedly, bidirectional RNN and LSTM should be able to deal with long-range dependencies in data. But in practice, the long-range dependencies are still problematic to handle. Thus, a technique called the Attention Mechanism was proposed.</p><p>The attention mechanism in neural networks is inspired by the visual attention mechanism found in humans. That is, the human visual attention is able to focus on a certain region of an image with "high resolution" while perceiving the surrounding image in "low resolution" and then adjusting the  Bahdanau et al. <ref type="bibr" target="#b28">29</ref> first utilized the attention mechanism for machine translation in NLP. They proposed an encoder-decoder framework where an attention mechanism is used to select reference words in the original language for words in the target language before translation. Figure <ref type="figure" target="#fig_3">8</ref> illustrates the use of the attention mechanism in their bidirectional RNN. Note that each decoder output word 𝑦 ! depends on a weighted combination of all the input states, not just the last state as in the normal case. 𝑎 !,! are weights that define in how much of each input state should be weighted for each output. For example, if 𝑎 !,! has a big value, it means that the decoder pays a lot of attention to the second state in the source sentence while producing the second word of the target sentence. The weights of 𝑎 !,! sum to 1 normally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MEMORY NETWORK</head><p>Weston et al. <ref type="bibr" target="#b29">30</ref>  Then the G component updates a piece of memory based on the current sentence representation. After all sentences are processed, a memory matrix (each row representing a sentence) is generated, which stores the semantics of the sentences. For a question, MemNN encodes it into a vector representation, then the O component uses the vector to select some related evidences from the memory and generates an output vector. Finally, the R component takes the output vector as the input and outputs a final response.</p><p>Based on MemNN, Sukhbaatar et al. <ref type="bibr" target="#b30">31</ref> proposed an End-to-End Memory Network (MemN2N), which is a neural network architecture with a recurrent attention mechanism over the long-term memory component and it can be trained in an End-to-End manner through standard backpropagation. It demonstrates that multiple computational layers (hops) in the O component can uncover more abstractive evidences than a single layer and yield improved results for question answering and language modelling. It is worth noting that each computational layer can be a content-based attention model. Thus, MemN2N refines the attention mechanism to some extent. Note also a similar idea is the Neural Turing Machines reported by Graves et al. <ref type="bibr" target="#b31">32</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RECURSIVE NEURAL NETWORK</head><p>Recursive Neural Network (RecNN) is a type of neural network that is usually used to learn a directed acyclic graph structure (e.g., tree structure) from data. A recursive neural network can be seen as a generalization of the recurrent neural network. Given the structural representation of a sentence (e.g., a parse tree), RecNN recursively generates parent representations in a bottom-up fashion, by combining tokens to produce representations for phrases, eventually the whole sentence. The sentence level representation can then be used to make a final classification (e.g., sentiment classification) for a given input sentence. An example process of vector composition in RecNN is shown in Figure <ref type="figure" target="#fig_10">9</ref> 33 . The vector of node "very interesting" is composed from the vectors of the node "very" and the node "interesting". Similarly, the node "is very interesting" is composed from the phrase node "very interesting" and the word node "is". </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SENTIMENT ANALYSIS TASKS</head><p>We are now ready to survey deep learning applications in sentiment analysis. But before doing that, we first briefly introduce the main sentiment analysis tasks in this section. For additional details, please refer to Liu's book 1 on sentiment analysis.</p><p>Researchers have mainly studied sentiment analysis at three levels of granularity: document level, sentence level, and aspect level. Document level sentiment classification classifies an opinionated document (e.g., a product review) as expressing an overall positive or negative opinion. It considers the whole document as the basic information unit and assumes that the document is known to be opinionated and contain opinions about a single entity (e.g., a particular phone). Sentence level sentiment classification classifies individual sentences in a document. However, each sentence cannot be assumed to be opinionated. Traditionally, one often first classifies a sentence as opinionated or not opinionated, which is called subjectivity classification. Then the resulting opinionated sentences are classified as expressing positive or negative opinions. Sentence level sentiment classification can also be formulated as a three-class classification problem, that is, to classify a sentence as neutral, positive or negative. Compared with document level and sentence level sentiment analysis, aspect level sentiment analysis or aspect-based sentiment analysis is more fine-grained. Its task is to extract and summarize people's opinions expressed on entities and aspects/features of entities, which are also called targets. For example, in a product review, it aims to summarize positive and negative opinions on different aspects of the product respectively, although the general sentiment on the product could be positive or negative. The whole task of aspect-based sentiment analysis consists of several subtasks such as aspect extraction, entity extraction, and aspect sentiment classification. For example, from the sentence, "the voice quality of iPhone is great, but its battery sucks", entity extraction should identify "iPhone" as the entity, and aspect extraction should identify that "voice quality" and "battery" are two aspects. Aspect sentiment classification should classify the sentiment expressed on the voice quality of the iPhone as positive and on the battery of the iPhone as negative. Note that for simplicity, in most algorithms aspect extraction and entity extraction are combined and are called aspect extraction or sentiment/opinion target extraction.</p><p>Apart from these core tasks, sentiment analysis also studies emotion analysis, sarcasm detection, multilingual sentiment analysis, etc. See Liu's book <ref type="bibr" target="#b0">1</ref> for more details. In the following sections, we survey the deep learning applications in all these sentiment analysis tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DOCUMENT LEVEL SENTIMENT CLASSIFICATION</head><p>Sentiment classification at the document level is to assign an overall sentiment orientation/polarity to an opinion document, i.e., to determine whether the document (e.g., a full online review) conveys an overall positive or negative opinion. In this setting, it is a binary classification task. It can also be formulated as a regression task, for example, to infer an overall rating score from 1 to 5 stars for the review. Some researchers also treat this as a 5-class classification task.</p><p>Sentiment classification is commonly regarded as a special case of document classification. In such a classification, document representation plays an important role, which should reflect the original information conveyed by words or sentences in a document. Traditionally, the bag-of-words model (BoW) is used to generate text representations in NLP and text mining, by which a document is regarded as a bag of its words. Based on BoW, a document is transformed to a numeric feature vector with a fixed length, each element of which can be the word occurrence (absence or presence), word frequency, or TF-IDF score. Its dimension equals to the size of the vocabulary. A document vector from BoW is normally very sparse since a single document only contains a small number of words in a vocabulary. Early neural networks adopted such feature settings.</p><p>Despite its popularity, BoW has some disadvantages. Firstly, the word order is ignored, which means that two documents can have exactly the same representation as long as they share the same words. Bag-of-N-Grams, an extension for BoW, can consider the word order in a short context (n-gram), but it also suffers from data sparsity and high dimensionality. Secondly, BoW can barely encode the semantics of words. For example, the words "smart", "clever" and "book" are of equal distance between them in BoW, but "smart" should be closer to "clever" than "book" semantically.</p><p>To tackle the shortcomings of BoW, word embedding techniques based on neural networks (introduced in the aforementioned section) were proposed to generate dense vectors (or lowdimensional vectors) for word representation, which are, to some extent, able to encode some semantic and syntactic properties of words. With word embeddings as input of words, document representation as a dense vector (or called dense document vector) can be derived using neural networks.</p><p>Notice that in addition to the above two approaches, i.e., using BoW and learning dense vectors for documents through word embeddings, one can also learn a dense document vector directly from BoW. We distinguish the different approaches used in related studies in Table <ref type="table" target="#tab_2">2</ref>.</p><p>When documents are properly represented, sentiment classification can be conducted using a variety of neural network models following the traditional supervised learning setting. In some cases, neural networks may only be used to extract text features/text representations, and these features are fed into some other non-neural classifiers (e.g., SVM) to obtain a final global optimum classifier. The properties of neural networks and SVM complement each other in such a way that their advantages are combined.</p><p>Besides sophisticated document/text representations, researchers also leveraged the characteristics of the data -product reviews, for sentiment classification. For product reviews, several researchers found it beneficial to jointly model sentiment and some additional information (e.g., user information and product information) for classification. Additionally, since a document often contains long dependency relations, the attention mechanism is also frequently used in document level sentiment classification. We summarize the existing techniques in Table <ref type="table" target="#tab_2">2</ref>. Below, we also give a brief description of these existing representative works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Research Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document/Text Representation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neural Networks Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Use Attention Mechanism</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joint Modelling with Sentiment</head><p>Moraes et al. <ref type="bibr" target="#b33">34</ref> made an empirical comparison between Support Vector Machines (SVM) and Artificial Neural Networks (ANN) for document level sentiment classification, which demonstrated that ANN produced competitive results to SVM's in most cases.</p><p>To overcome the weakness of BoW, Le and Mikolov <ref type="bibr" target="#b34">35</ref> proposed Paragraph Vector, an unsupervised learning algorithm that learns vector representations for variable-length texts such as sentences, paragraphs and documents. The vector representations are learned by predicting the surrounding words in contexts sampled from the paragraph.</p><p>Glorot et al. <ref type="bibr" target="#b35">36</ref> studied domain adaptation problem for sentiment classification. They proposed a deep learning system based on Stacked Denoising Autoencoder with sparse rectifier units, which can perform an unsupervised text feature/representation extraction using both labeled and unlabeled data. The features are highly beneficial for domain adaption of sentiment classifiers.</p><p>Zhai and Zhang 37 introduced a semi-supervised autoencoder, which further considers the sentiment information in its learning stage in order to obtain better document vectors, for sentiment classification. More specifically, the model learns a task-specific representation of the textual data by relaxing the loss function in the autoencoder to the Bregman Divergence and also deriving a discriminative loss function from the label information.</p><p>Johnson and Zhang <ref type="bibr" target="#b37">38</ref> proposed a CNN variant named BoW-CNN that employs bag-of-word conversion in the convolution layer. They also designed a new model, called Seq-CNN, which keeps the sequential information of words by concatenating the one-hot vector of multiple words.</p><p>Tang et al. <ref type="bibr" target="#b38">39</ref> proposed a neural network to learn document representation, with the consideration of sentence relationships. It first learns the sentence representation with CNN or LSTM from word embeddings. Then a GRU is utilized to adaptively encode semantics of sentences and their inherent relations in document representations for sentiment classification.</p><p>Tang et al. <ref type="bibr" target="#b39">40</ref> applied user representations and product representations in review classification. The idea is that those representations can capture important global clues such as individual preferences of users and overall qualities of products, which can provide better text representations.</p><p>Chen et al. <ref type="bibr" target="#b40">41</ref> also incorporated user information and product information for classification but via word and sentence level attentions, which can take into account of the global user preference and product characteristics at both the word level and the semantic level. Likewise, Dou <ref type="bibr" target="#b41">42</ref> used a deep memory network to capture user and product information. The proposed model can be divided into two separate parts. In the first part, LSTM is applied to learn a document representation. In the second part, a deep memory network consisting of multiple computational layers (hops) is used to predict the review rating for each document.</p><p>Xu et al. <ref type="bibr" target="#b42">43</ref> proposed a cached LSTM model to capture the overall semantic information in a long text.</p><p>The memory in the model is divided into several groups with different forgetting rates. The intuition is to enable the memory groups with low forgetting rates to capture global semantic features and the ones with high forgetting rates to learn local semantic features.</p><p>Yang et al. <ref type="bibr" target="#b43">44</ref> proposed a hierarchical attention network for document level sentiment rating prediction of reviews. The model includes two levels of attention mechanisms: one at the word level and the other at the sentence level, which allow the model to pay more or less attention to individual words or sentences in constructing the representation of a document.</p><p>Yin et al. <ref type="bibr" target="#b44">45</ref> formulated the document-level aspect-sentiment rating prediction task as a machine comprehension problem and proposed a hierarchical interactive attention-based model. Specifically, documents and pseudo aspect-questions are interleaved to learn aspect-aware document representation.</p><p>Zhou et al. <ref type="bibr" target="#b45">46</ref> designed an attention-based LSTM network for cross-lingual sentiment classification at the document level. The model consists of two attention-based LSTMs for bilingual representation, and each LSTM is also hierarchically structured. In this setting, it effectively adapts the sentiment information from a resource-rich language (English) to a resource-poor language (Chinese) and helps improve the sentiment classification performance.</p><p>Li et al. <ref type="bibr" target="#b46">47</ref> proposed an adversarial memory network for cross-domain sentiment classification in a transfer learning setting, where the data from the source and the target domain are modelled together. It jointly trains two networks for sentiment classification and domain classification (i.e., whether a document is from the source or target domain).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SENTENCE LEVEL SENTIMENT CLASSIFICATION</head><p>Sentence level sentiment classification is to determine the sentiment expressed in a single given sentence. As discussed earlier, the sentiment of a sentence can be inferred with subjectivity classification <ref type="bibr" target="#b47">48</ref> and polarity classification, where the former classifies whether a sentence is subjective or objective and the latter decides whether a subjective sentence expresses a negative or positive sentiment. In existing deep learning models, sentence sentiment classification is usually formulated as a joint three-way classification problem, namely, to predict a sentence as positive, neural, and negative.</p><p>Same as document level sentiment classification, sentence representation produced by neural networks is also important for sentence level sentiment classification. Additionally, since a sentence is usually short compared to a document, some syntactic and semantic information (e.g., parse trees, opinion lexicons, and part-of-speech tags) may be used to help. Additional information such as review ratings, social relationship, and cross-domain information can be considered too. For example, social relationships have been exploited in discovering sentiments in social media data such as tweets.</p><p>In early research, parse trees (which provide some semantic and syntactic information) were used together with the original words as the input to neural models, so that the sentiment composition can be better inferred. But lately, CNN and RNN become more popular, and they do not need parse trees to extract features from sentences. Instead, CNN and RNN use word embeddings as input, which already encode some semantic and syntactic information. Moreover, the model architecture of CNN or RNN can help learn intrinsic relationships between words in a sentence too. The related works are introduced in detail below.</p><p>Socher et al. <ref type="bibr" target="#b48">49</ref> first proposed a semi-supervised Recursive Autoencoders Network (RAE) for sentence level sentiment classification, which obtains a reduced dimensional vector representation for a sentence. Later on, Socher et al. <ref type="bibr" target="#b49">50</ref> proposed a Matrix-vector Recursive Neural Network (MV-RNN), in which each word is additionally associated with a matrix representation (besides a vector representation) in a tree structure. The tree structure is obtained from an external parser. In Socher et al. <ref type="bibr" target="#b50">51</ref> , the authors further introduced the Recursive Neural Tensor Network (RNTN), where tensorbased compositional functions are used to better capture the interactions between elements. Qian et al. <ref type="bibr" target="#b32">33</ref> proposed two more advanced models, Tag-guided Recursive Neural Network (TG-RNN), which chooses a composition function according to the part-of-speech tags of a phrase, and Tagembedded Recursive Neural Network / Recursive Neural Tenser Network (TE-RNN/RNTN), which learns tag embeddings and then combines tag and word embeddings together.</p><p>Kalchbrenner et al. <ref type="bibr" target="#b51">52</ref> proposed a Dynamic CNN (called DCNN) for semantic modelling of sentences. DCNN uses the dynamic K-Max pooling operator as a non-linear subsampling function. The feature graph induced by the network is able to capture word relations. Kim 53 also proposed to use CNN for sentence-level sentiment classification and experimented with several variants, namely CNN-rand (where word embeddings are randomly initialized), CNN-static (where word embeddings are pretrained and fixed), CNN-non-static (where word embeddings are pre-trained and fine-tuned) and CNN-multichannel (where multiple sets of word embeddings are used).</p><p>dos Santos and Gatti <ref type="bibr" target="#b53">54</ref> proposed a Character to Sentence CNN (CharSCNN) model. CharSCNN uses two convolutional layers to extract relevant features from words and sentences of any size to perform sentiment analysis of short texts. Wang et al. <ref type="bibr" target="#b54">55</ref> utilized LSTM for Twitter sentiment classification by simulating the interactions of words during the compositional process.</p><p>Multiplicative operations between word embeddings through gate structures are used to provide more flexibility and to produce better compositional results compared to the additive ones in simple recurrent neural network. Similar to bidirectional RNN, the unidirectional LSTM can be extended to a bidirectional LSTM <ref type="bibr" target="#b55">56</ref> by allowing bidirectional connections in the hidden layer.</p><p>Wang et al. <ref type="bibr" target="#b56">57</ref> proposed a regional CNN-LSTM model, which consists of two parts: regional CNN and LSTM, to predict the valence arousal ratings of text.</p><p>Wang et al. <ref type="bibr" target="#b57">58</ref> described a joint CNN and RNN architecture for sentiment classification of short texts, which takes advantage of the coarse-grained local features generated by CNN and long-distance dependencies learned via RNN.</p><p>Guggilla et al. <ref type="bibr" target="#b58">59</ref> presented a LSTM-and CNN-based deep neural network model, which utilizes word2vec and linguistic embeddings for claim classification (classifying sentences to be factual or feeling).</p><p>Huang et al. <ref type="bibr" target="#b59">60</ref> proposed to encode the syntactic knowledge (e.g., part-of-speech tags) in a treestructured LSTM to enhance phrase and sentence representation.</p><p>Akhtar et al. <ref type="bibr" target="#b60">61</ref> proposed several multi-layer perceptron based ensemble models for fine-gained sentiment classification of financial microblogs and news.</p><p>Guan et al. <ref type="bibr" target="#b61">62</ref> employed a weakly-supervised CNN for sentence (and also aspect) level sentiment classification. It contains a two-step learning process: it first learns a sentence representation weakly supervised by overall review ratings and then uses the sentence (and aspect) level labels for finetuning.</p><p>Teng et al. <ref type="bibr" target="#b62">63</ref> proposed a context-sensitive lexicon-based method for sentiment classification based on a simple weighted-sum model, using bidirectional LSTM to learn the sentiment strength, intensification and negation of lexicon sentiments in composing the sentiment value of a sentence.</p><p>Yu and Jiang 64 studied the problem of learning generalized sentence embeddings for cross-domain sentence sentiment classification and designed a neural network model containing two separated CNNs that jointly learn two hidden feature representations from both the labeled and unlabeled data.</p><p>Zhao et al. <ref type="bibr" target="#b64">65</ref> introduced a recurrent random walk network learning approach for sentiment classification of opinionated tweets by exploiting the deep semantic representation of both user posted tweets and their social relationships.</p><p>Mishra et al. <ref type="bibr" target="#b65">66</ref> utilized CNN to automatically extract cognitive features from the eye-movement (or gaze) data of human readers reading the text and used them as enriched features along with textual features for sentiment classification.</p><p>Qian et al. <ref type="bibr" target="#b66">67</ref> presented a linguistically regularized LSTM for the task. The proposed model incorporates linguistic resources such as sentiment lexicon, negation words and intensity words into the LSTM so as to capture the sentiment effect in sentences more accurately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ASPECT LEVEL SENTIMENT CLASSIFICATION</head><p>Different from the document level and the sentence level sentiment classification, aspect level sentiment classification considers both the sentiment and the target information, as a sentiment always has a target. As mentioned earlier, a target is usually an entity or an entity aspect. For simplicity, both entity and aspect are usually just called aspect. Given a sentence and a target aspect, aspect level sentiment classification aims to infer the sentiment polarity/orientation of the sentence toward the target aspect. For example, in the sentence "the screen is very clear but the battery life is too short." the sentiment is positive if the target aspect is "screen" but negative if the target aspect is "battery life". We will discuss automated aspect or target extraction in the next section.</p><p>Aspect level sentiment classification is challenging because modelling the semantic relatedness of a target with its surrounding context words is difficult. Different context words have different influences on the sentiment polarity of a sentence towards the target. Therefore, it is necessary capture semantic connections between the target word and the context words when building learning models using neural networks.</p><p>There are three important tasks in aspect level sentiment classification using neural networks. The first task is to represent the context of a target, where the context means the contextual words in a sentence or document. This issue can be similarly addressed using the text representation approaches mentioned in the above two sections. The second task is to generate a target representation, which can properly interact with its context. A general solution is to learn a target embedding, which is similar to word embedding. The third task is to identify the important sentiment context (words) for the specified target. For example, in the sentence "the screen of iPhone is clear but batter life is short", "clear" is the important context word for "screen" and "short" is the important context for "battery life". This task is recently addressed by the attention mechanism. Although many deep learning techniques have been proposed to deal with aspect level sentiment classification, to our knowledge, there are still no dominating techniques in the literature. Related works and their main focuses are introduced below.</p><p>Dong et al. <ref type="bibr" target="#b67">68</ref> proposed an Adaptive Recursive Neural Network (AdaRNN) for target-dependent twitter sentiment classification, which learns to propagate the sentiments of words towards the target depending on the context and syntactic structure. It uses the representation of the root node as the features, and feeds them into the softmax classifier to predict the distribution over classes.</p><p>Vo and Zhang <ref type="bibr" target="#b68">69</ref> studied aspect-based Twitter sentiment classification by making use of rich automatic features, which are additional features obtained using unsupervised learning methods. The paper showed that multiple embeddings, multiple pooling functions, and sentiment lexicons can offer rich sources of feature information and help achieve performance gains.</p><p>Since LSTM can capture semantic relations between the target and its context words in a more flexible way, Tang et al. <ref type="bibr" target="#b69">70</ref> proposed Target-dependent LSTM (TD-LSTM) and Target-connection LSTM (TC-LSTM) to extend LSTM by taking the target into consideration. They regarded the given target as a feature and concatenated it with the context features for aspect sentiment classification.</p><p>Ruder et al. <ref type="bibr" target="#b70">71</ref> proposed to use a hierarchical and bidirectional LSTM model for aspect level sentiment classification, which is able to leverage both intra-and inter-sentence relations. The sole dependence on sentences and their structures within a review renders the proposed model language-independent. Word embeddings are fed into a sentence-level bidirectional LSTM. Final states of the forward and backward LSTM are concatenated together with the target embedding and fed into a bidirectional review-level LSTM. At every time step, the output of the forward and backward LSTM is concatenated and fed into a final layer, which outputs a probability distribution over sentiments.</p><p>Considering the limitation of work by Dong et al. <ref type="bibr" target="#b67">68</ref> and Vo and Zhang <ref type="bibr" target="#b68">69</ref> , Zhang et al. <ref type="bibr" target="#b71">72</ref> proposed a sentence level neural model to address the weakness of pooling functions, which do not explicitly model tweet-level semantics. To achieve that, two gated neural networks are presented. First, a bi-directional gated neural network is used to connect the words in a tweet so that pooling functions can be applied over the hidden layer instead of words for better representing the target and its contexts. Second, a three-way gated neural network structure is used to model the interaction between the target mention and its surrounding contexts, addressing the limitations by using gated neural network structures to model the syntax and semantics of the enclosing tweet, and the interaction between the surrounding contexts and the target respectively. Gated neural networks have been shown to reduce the bias of standard recurrent neural networks towards the ends of a sequence by better propagation of gradients.</p><p>Wang et al. <ref type="bibr" target="#b72">73</ref> proposed an attention-based LSTM method with target embedding, which was proven to be an effective way to enforce the neural model to attend to the related part of a sentence. The attention mechanism is used to enforce the model to attend to the important part of a sentence, in response to a specific aspect. Likewise, Yang et al. <ref type="bibr" target="#b73">74</ref> proposed two attention-based bidirectional LSTMs to improve the classification performance. Liu and Zhang <ref type="bibr" target="#b74">75</ref> extended the attention modelling by differentiating the attention obtained from the left context and the right context of a given target/aspect. They further controlled their attention contribution by adding multiple gates.</p><p>Tang et al. <ref type="bibr" target="#b75">76</ref> introduced an end-to-end memory network for aspect level sentiment classification, which employs an attention mechanism with an external memory to capture the importance of each context word with respect to the given target aspect. This approach explicitly captures the importance of each context word when inferring the sentiment polarity of the aspect. Such importance degree and text representation are calculated with multiple computational layers, each of which is a neural attention model over an external memory.</p><p>Lei et al. <ref type="bibr" target="#b76">77</ref> proposed to use a neural network approach to extract pieces of input text as rationales (reasons) for review ratings. The model consists of a generator and a decoder. The generator specifies a distribution over possible rationales (extracted text) and the encoder maps any such text to a task-specific target vector. For multi-aspect sentiment analysis, each coordinate of the target vector represents the response or rating pertaining to the associated aspect.</p><p>Li et al. <ref type="bibr" target="#b77">78</ref> integrated the target identification task into sentiment classification task to better model aspect-sentiment interaction. They showed that sentiment identification can be solved with an endto-end machine learning architecture, in which the two sub-tasks are interleaved by a deep memory network. In this way, signals produced in target detection provide clues for polarity classification, and reversely, the predicted polarity provides feedback to the identification of targets.</p><p>Ma et al. <ref type="bibr" target="#b78">79</ref> proposed an Interactive Attention Network (IAN) that considers both attentions on target and context. That is, it uses two attention networks to interactively detect the important words of the target expression/description and the important words of its full context.</p><p>Chen et al. <ref type="bibr" target="#b79">80</ref> proposed to utilize a recurrent attention network to better capture the sentiment of complicated contexts. To achieve that, their proposed model uses a recurrent/dynamic attention structure and learns non-linear combination of the attention in GRUs.</p><p>Tay et al. <ref type="bibr" target="#b80">81</ref> designed a Dyadic Memory Network (DyMemNN) that models dyadic interactions between aspect and context, by using either neural tensor compositions or holographic compositions for memory selection operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ASPECT EXTRACTION AND CATEGORIZATION</head><p>To perform aspect level sentiment classification, one needs to have aspects (or targets), which can be manually given or automatically extracted. In this section, we discuss existing work for automated aspect extraction (or aspect term extraction) from a sentence or document using deep learning models. Let us use an example to state the problem. For example, in the sentence "the image is very clear" the word "image" is an aspect term (or sentiment target). The associated problem of aspect categorization is to group the same aspect expressions into a category. For instance, the aspect terms "image", "photo" and "picture" can be grouped into one aspect category named Image. In the review below, we include the extraction of both aspect and entity that are associated with opinions.</p><p>One reason why deep learning models can be helpful for this task is that, deep learning is essentially good at learning (complicated) feature representations. When an aspect is properly characterized in some feature space, for example, in one or some hidden layer(s), the semantics or correlation between an aspect and its context can be captured with the interplay between their corresponding feature representations. In other words, deep learning provides a possible approach to automated feature engineering without human involvement.</p><p>Katiyar and Cardie 82 investigated the use of deep bidirectional LSTMs for joint extraction of opinion entities and the IS-FORM and IS-ABOUT relationships that connect the entities. Wang et al. <ref type="bibr" target="#b82">83</ref> further proposed a joint model integrating RNN and Conditional Random Fields (CRF) to co-extract aspects and opinion terms or expressions. The proposed model can learn high-level discriminative features and double-propagate information between aspect and opinion terms simultaneously. Wang et al. <ref type="bibr" target="#b83">84</ref> further proposed a Coupled Multi-Layer Attention Model (CMLA) for co-extracting of aspect and opinion terms. The model consists of an aspect attention and an opinion attention using GRU units. An improved LSTM-based approach was reported by Li and Lam <ref type="bibr" target="#b84">85</ref> , specifically for aspect term extraction. It consists of three LSTMs, of which two LSTMs are for capturing aspect and sentiment interactions. The third LSTM is to use the sentiment polarity information as an additional guidance.</p><p>He et al. <ref type="bibr" target="#b85">86</ref> proposed an attention-based model for unsupervised aspect extraction. The main intuition is to utilize the attention mechanism to focus more on aspect-related words while deemphasizing aspect-irrelevant words during the learning of aspect embeddings, similar to the autoencoder framework.</p><p>Zhang et al. <ref type="bibr" target="#b86">87</ref> extended a CRF model using a neural network to jointly extract aspects and corresponding sentiments. The proposed CRF variant replaces the original discrete features in CRF with continuous word embeddings, and adds a neural layer between the input and output nodes.</p><p>Zhou et al. <ref type="bibr" target="#b87">88</ref> proposed a semi-supervised word embedding learning method to obtain continuous word representations on a large set of reviews with noisy labels. With the word vectors learned, deeper and hybrid features are learned by stacking on the word vectors through a neural network. Finally, a logistic regression classifier trained with the hybrid features is used to predict the aspect category.</p><p>Yin et al. <ref type="bibr" target="#b88">89</ref> first learned word embedding by considering the dependency path connecting words. Then they designed some embedding features that consider the linear context and dependency context information for CRF-based aspect term extraction.</p><p>Xiong et al. <ref type="bibr" target="#b89">90</ref> proposed an attention-based deep distance metric learning model to group aspect phrases. The attention-based model is to learn feature representation of contexts. Both aspect phrase embedding and context embedding are used to learn a deep feature subspace metric for Kmeans clustering.</p><p>Poria et al. <ref type="bibr" target="#b90">91</ref> proposed to use CNN for aspect extraction. They developed a seven-layer deep convolutional neural network to tag each word in opinionated sentences as either aspect or nonaspect word. Some linguistic patterns are also integrated into the model for further improvement.</p><p>Ying et al. <ref type="bibr" target="#b91">92</ref> proposed two RNN-based models for cross-domain aspect extraction. They first used rule-based methods to generate an auxiliary label sequence for each sentence. They then trained the models using both the true labels and auxiliary labels, which shows promising results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OPINION EXPRESSION EXTRACTION</head><p>In this and the next few sections, we discuss deep learning applications to some other sentiment analysis related tasks. This section focuses on the problem of opinion expression extraction (or opinion term extraction, or opinion identification), which aims to identify the expressions of sentiment in a sentence or a document.</p><p>Similar to the aspect extraction, opinion expression extraction using deep learning models is workable because their characteristics could be identified in some feature space as well.</p><p>Irsoy and Cardie <ref type="bibr" target="#b92">93</ref> explored the application of deep bidirectional RNN for the task, which outperforms traditional shallow RNNs with the same number of parameters and also previous CRF methods. <ref type="bibr" target="#b93">94</ref> Liu et al. <ref type="bibr" target="#b94">95</ref> presented a general class of discriminative models based on the RNN architecture and word embedding. The authors used pre-trained word embeddings from three external sources in different RNN architectures including Elman-type, Jordan-type, LSTM and their variations.</p><p>Wang et al. <ref type="bibr" target="#b82">83</ref> proposed a model integrating recursive neural networks and CRF to co-extract aspect and opinion terms. The aforementioned CMLA is also proposed for co-extraction of aspect and opinion terms. <ref type="bibr" target="#b83">84</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SENTIMENT COMPOSITION</head><p>Sentiment composition claims that the sentiment orientation of an opinion expression is determined by the meaning of its constituents as well as the grammatical structure. Due to their particular treestructure design, RecNN is naturally suitable for this task. <ref type="bibr" target="#b50">51</ref> Irsoy and Cardie <ref type="bibr" target="#b95">96</ref> reported that the RecNN with a deep architecture can more accurately capture different aspects of compositionality in language, which benefits sentiment compositionality. Zhu et al. <ref type="bibr" target="#b96">97</ref> proposed a neural network for integrating the compositional and non-compositional sentiment in the process of sentiment composition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OPINION HOLDER EXTRACTION</head><p>Opinion holder (or source) extraction is the task of recognizing who holds the opinion (or whom/where the opinion is from). <ref type="bibr" target="#b0">1</ref> For example, in the sentence "John hates his car", the opinion holder is "john". This problem is commonly formulated as a sequence labelling problem like opinion expression extraction or aspect extraction. Notice that opinion holder can be either explicit (from a noun phrase in the sentence) or implicit (from the writer) as shown by Yang and Cardie <ref type="bibr" target="#b97">98</ref> . Deng and Wiebe <ref type="bibr" target="#b98">99</ref> proposed to use word embeddings of opinion expressions as features for recognizing sources of participant opinions and non-participant opinions, where a source can be the noun phrase or writer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TEMPORAL OPINION MINING</head><p>Time is also an important dimension in problem definition of sentiments analysis (see Liu's book <ref type="bibr" target="#b0">1</ref> ). As time passes by, people may maintain or change their mind, or even give new viewpoints.</p><p>Therefore, predicting future opinion is important in sentiment analysis. Some research using neural networks has been reported recently to tackle this problem.</p><p>Chen et al. <ref type="bibr" target="#b99">100</ref> proposed a Content-based Social Influence Model (CIM) to make opinion behaviour predictions of twitter users. That is, it uses the past tweets to predict users' future opinions. It is based on a neural network framework to encode both the user content and social relation factor (one's opinion about a target is influenced by one's friends).</p><p>Rashkin et al. <ref type="bibr" target="#b100">101</ref> used LSTMs for targeted sentiment forecast in the social media context. They introduced multilingual connotation frames, which aim at forecasting implied sentiments among world event participants engaged in a frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SENTIMENT ANALYSIS WITH WORD EMBEDDING</head><p>It is clear that word embeddings played an important role in deep learning based sentiment analysis models. It is also shown that even without the use of deep learning models, word embeddings can be used as features for non-neural learning models for various tasks. The section thus specifically highlights word embeddings' contribution to sentiment analysis.</p><p>We first present the works of sentiment-encoded word embeddings. For sentiment analysis, directly applying regular word methods like CBOW or Skip-gram to learn word embeddings from context can encounter problems, because words with similar contexts but opposite sentiment polarities (e.g., "good" or "bad") may be mapped to nearby vectors in the embedding space. Therefore, sentimentencoded word embedding methods have been proposed. Mass el al. <ref type="bibr" target="#b101">102</ref> learned word embeddings that can capture both semantic and sentiment information. Bespalov et al. <ref type="bibr" target="#b102">103</ref> showed that an n-gram model combined with latent representation would produce a more suitable embedding for sentiment classification. Labutov and Lipson 104 re-embed existing word embeddings with logistic regression by regarding sentiment supervision of sentences as a regularization term.</p><p>Le and Mikolov <ref type="bibr" target="#b34">35</ref> proposed the concept of paragraph vector to first learn fixed-length representation for variable-length pieces of texts, including sentences, paragraphs and documents. They experimented on both sentence and document-level sentiment classification tasks and achieved performance gains, which demonstrates the merit of paragraph vectors in capturing semantics to help sentiment classification. Tang et al. <ref type="bibr" target="#b104">105,</ref><ref type="bibr" target="#b105">106</ref> presented models to learn Sentiment-specific Word Embeddings (SSWE), in which not only the semantic but also sentiment information is embedded in the learned word vectors. Wang and Xia 107 developed a neural architecture to train a sentimentbearing word embedding by integrating the sentiment supervision at both the document and word levels. Yu et al. <ref type="bibr" target="#b107">108</ref> adopted a refinement strategy to obtain joint semantic-sentiment bearing word vectors.</p><p>Feature enrichment and multi-sense word embeddings are also investigated for sentiment analysis. Vo and Zhang 69 studied aspect-based Twitter sentiment classification by making use of rich automatic features, which are additional features obtained using unsupervised learning techniques.</p><p>Li and Jurafsky 109 experimented with the utilization of multi-sense word embeddings on various NLP tasks. Experimental results show that while such embeddings do improve the performance of some tasks, they offer little help to sentiment classification tasks. Ren et al. <ref type="bibr" target="#b109">110</ref> proposed methods to learn topic-enriched multi-prototype word embeddings for Twitter sentiment classification.</p><p>Multilinguistic word embeddings have also been applied to sentiment analysis. Zhou et al. <ref type="bibr" target="#b110">111</ref> reported a Bilingual Sentiment Word Embedding (BSWE) model for cross-language sentiment classification. It incorporates the sentiment information into English-Chinese bilingual embeddings by employing labeled corpora and their translation, instead of large-scale parallel corpora. Barnes et al. <ref type="bibr" target="#b111">112</ref> compared several types of bilingual word embeddings and neural machine translation techniques for cross-lingual aspect-based sentiment classification.</p><p>Zhang et al. <ref type="bibr" target="#b112">113</ref> integrated word embeddings with matrix factorization for personalized review-based rating prediction. Specifically, the authors refine existing semantics-oriented word vectors (e.g., word2vec and GloVe) using sentiment lexicons. Sharma et al. <ref type="bibr" target="#b113">114</ref> proposed a semi-supervised technique to use sentiment bearing word embeddings for ranking sentiment intensity of adjectives. Word embedding techniques have also been utilized or improved to help address various sentiment analysis tasks in many other recent studies. <ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b86">87,</ref><ref type="bibr" target="#b88">89,</ref><ref type="bibr" target="#b94">95</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SARCASM ANALYSIS</head><p>Sarcasm is a form verbal irony and a closely related concept to sentiment analysis. Recently, there is a growing interest in NLP communities in sarcasm detection. Researchers have attempted to solve it using deep learning techniques due of their impressive success in many other NLP problems.</p><p>Zhang et al. <ref type="bibr" target="#b114">115</ref> constructed a deep neural network model for tweet sarcasm detection. Their network first uses a bidirectional GRU model to capture the syntactic and semantic information over tweets locally, and then uses a pooling neural network to extract contextual features automatically from history tweets for detecting sarcastic tweets.</p><p>Joshi et al. <ref type="bibr" target="#b115">116</ref> investigated word embeddings-based features for sarcasm detection. They experimented four past algorithms for sarcasm detection with augmented word embeddings features and showed promising results.</p><p>Poria et al. <ref type="bibr" target="#b116">117</ref> developed a CNN-based model for sarcasm detection (sarcastic or non-sarcastic tweets classification), by jointly modelling pre-trained emotion, sentiment and personality features, along with the textual information in a tweet.</p><p>Peled and Reichart <ref type="bibr" target="#b117">118</ref> proposed to interpret sarcasm tweets based on a RNN neural machine translation model.</p><p>Ghosh and Veale 119 proposed a CNN and bidirectional LSTM hybrid for sarcasm detection in tweets, which models both linguistic and psychological contexts.</p><p>Mishra et al. <ref type="bibr" target="#b65">66</ref> utilized CNN to automatically extract cognitive features from the eye-movement (or gaze) data to enrich information for sarcasm detection. Word embeddings are also used for irony recognition in English tweets <ref type="bibr" target="#b119">120</ref> and for controversial words identification in debates. <ref type="bibr" target="#b120">121</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EMOTION ANALYSIS</head><p>Emotions are the subjective feelings and thoughts of human beings. The primary emotions include love, joy, surprise, anger, sadness and fear. The concept of emotion is closely related to sentiment. For example, the strength of a sentiment can be linked to the intensity of certain emotion like joy and anger. Thus, many deep learning models are also applied to emotion analysis following the way in sentiment analysis.</p><p>Wang et al. <ref type="bibr" target="#b121">122</ref> built a bilingual attention network model for code-switched emotion prediction. A LSTM model is used to construct a document level representation of each post, and the attention mechanism is employed to capture the informative words from both the monolingual and bilingual contexts.</p><p>Zhou et al. <ref type="bibr" target="#b122">123</ref> proposed an emotional chatting machine to model the emotion influence in largescale conversation generation based on GRU. The technique has also been applied in other papers. 39,72,115   Abdul-Mageed and Ungar 124 first built a large dataset for emotion detection automatically by using distant supervision and then used a GRU network for fine-grained emotion detection.</p><p>Felbo et al. <ref type="bibr" target="#b124">125</ref> used millions of emoji occurrences in social media for pretraining neural models in order to learn better representations of emotional contexts.</p><p>A question-answering approach is proposed using a deep memory network for emotion cause extraction. <ref type="bibr" target="#b125">126</ref> Emotion cause extraction aims to identify the reasons behind a certain emotion expressed in text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MULTIMODAL DATA FOR SENTIMENT ANALYSIS</head><p>Multimodal data, such as the data carrying textual, visual, and acoustic information, has been used to help sentiment analysis as it provides additional sentiment signals to the traditional text features. Since deep learning models can map inputs to some latent space for feature representation, the inputs from multimodal data can also be projected simultaneously to learn multimodal data fusion, for example, by using feature concatenation, joint latent space, or other more sophisticated fusion approaches. There is now a growing trend of using multimodal data with deep learning techniques.</p><p>Poria et al. <ref type="bibr" target="#b126">127</ref> proposed a way of extracting features from short texts based on the activation values of an inner layer of CNN. The main novelty of the paper is the use of a deep CNN to extract features from text and the use of multiple kernel learning (MKL) to classify heterogeneous multimodal fused feature vectors.</p><p>Bertero et al. <ref type="bibr" target="#b127">128</ref> described a CNN model for emotion and sentiment recognition in acoustic data from interactive dialog systems.</p><p>Fung et al. <ref type="bibr" target="#b128">129</ref> demonstrated a virtual interaction dialogue system that have incorporated sentiment, emotion and personality recognition capabilities trained by deep learning models.</p><p>Wang et al. <ref type="bibr" target="#b129">130</ref>  Yang et al. <ref type="bibr" target="#b130">131</ref> developed two algorithms based on a conditional probability neural network to analyse visual sentiment in images.</p><p>Zhu et al. <ref type="bibr" target="#b131">132</ref> proposed a unified CNN-RNN model for visual emotion recognition. The architecture leverages CNN with multiple layers to extract different levels of features (e.g., colour, texture, object, etc.) within a multi-task learning framework. And a bidirectional RNN is proposed to integrate the learned features from different layers in the CNN model.</p><p>You et al. <ref type="bibr" target="#b132">133</ref> adopted the attention mechanism for visual sentiment analysis, which can jointly discover the relevant local image regions and build a sentiment classifier on top of these local regions.</p><p>Poria et al. <ref type="bibr" target="#b133">134</ref> proposed some a deep learning model for multi-modal sentiment analysis and emotion recognition on video data. Particularly, a LSTM-based model is proposed for utterance-level sentiment analysis, which can capture contextual information from their surroundings in the same video.</p><p>Tripathi et al. <ref type="bibr" target="#b134">135</ref> used deep and CNN-based models for emotion classification on a multimodal dataset DEAP, which contains electroencephalogram and peripheral physiological and video signals.</p><p>Zadeh et al. <ref type="bibr" target="#b135">136</ref> formulated the problem of multimodal sentiment analysis as modelling intra-modality and inter-modality dynamics and introduced a new neural model named Tensor Fusion Network to tackle it.</p><p>Long et al. <ref type="bibr" target="#b136">137</ref> proposed an attention neural model trained with cognition grounded eye-tracking data for sentence-level sentiment classification. A Cognition Based Attention (CBA) layer is built for neural sentiment analysis.</p><p>Wang et al. <ref type="bibr" target="#b137">138</ref> proposed a Select-Additive Learning (SAL) approach to tackle the confounding factor problem in multimodal sentiment analysis, which removes the individual specific latent representations learned by neural networks (e.g., CNN). To achieve it, two learning phases are involved, namely, a selection phase for confounding factor identification and a removal phase for confounding factor removal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RESOURCE-POOR LANGUAGE AND MULTILINGUAL SENTIMENT ANALYSIS</head><p>Recently, sentiment analysis in resource-poor languages (compared to English) has also achieved significant progress due to the use of deep learning models. Additionally, multilingual features also can help sentiment analysis just like multimodal data. In the same way, deep learning has been applied to the multilingual sentiment analysis setting.</p><p>Akhtar et al. <ref type="bibr" target="#b138">139</ref> reported a CNN-based hybrid architecture for sentence and aspect level sentiment classification in a resource-poor language, Hindi.</p><p>Dahou et al. <ref type="bibr" target="#b139">140</ref> used word embeddings and a CNN-based model for Arabic sentiment classification at the sentence level.</p><p>Singhal and Bhattacharyya 141 designed a solution for multilingual sentiment classification at review/sentence level and experimented with multiple languages, including Hindi, Marathi, Russian, Dutch, French, Spanish, Italian, German, and Portuguese. The authors applied machine translation tools to translate these languages into English and then used English word embeddings, polarities from a sentiment lexicon and a CNN model for classification.</p><p>Joshi et al. <ref type="bibr" target="#b141">142</ref> introduced a sub-word level representation in a LSTM architecture for sentiment classification of Hindi-English code-mixed sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OTHER RELATED TASKS</head><p>There are also applications of deep learning in some other sentiment analysis related tasks.</p><p>Sentiment Intersubjectivity: Gui et al. <ref type="bibr" target="#b142">143</ref> tackled the intersubjectivity problem in sentiment analysis, where the problem is to study the gap between the surface form of a language and the corresponding abstract concepts, and incorporate the modelling of intersubjectivity into a proposed CNN.</p><p>Lexicon Expansion: Wang et al. <ref type="bibr" target="#b143">144</ref> proposed a PU learning-based neural approach for opinion lexicon expansion.</p><p>Financial Volatility Prediction: Rekabsaz et al. <ref type="bibr" target="#b144">145</ref> made volatility predictions using financial disclosure sentiment with word embedding-based information retrieval models, where word embeddings are used in similar word set expansion.</p><p>Opinion Recommendation: Wang and Zhang <ref type="bibr" target="#b145">146</ref> introduced the task of opinion recommendation, which aims to generate a customized review score of a product that the particular user is likely to give, as well as a customized review that the user would have written for the target product if the user had reviewed the product. A multiple-attention memory network was proposed to tackle the problem, which considers users' reviews, product's reviews, and users' neighbours (similar users).</p><p>Stance Detection: Augenstein et al. <ref type="bibr" target="#b146">147</ref> proposed a bidirectional LSTMs with a conditional encoding mechanism for stance detection in political twitter data. Du et al. <ref type="bibr" target="#b147">148</ref> designed a target-specific neural attention model for stance classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONCLUSION</head><p>Applying deep learning to sentiment analysis has become a popular research topic lately. In this paper, we introduced various deep learning architectures and their applications in sentiment analysis. Many of these deep learning techniques have shown state-of-the-art results for various sentiment analysis tasks. With the advances of deep learning research and applications, we believe that there will be more exciting research of deep learning for sentiment analysis in the near future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Feedforward neural network</figDesc><graphic url="image-1.png" coords="2,178.43,409.20,238.45,170.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Feature hierarchy by deep learning</figDesc><graphic url="image-2.png" coords="4,72.00,620.56,440.97,121.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Autoencoder neural network</figDesc><graphic url="image-3.png" coords="5,208.40,499.56,178.10,169.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>8</head><label>8</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Convolutional neural network</figDesc><graphic url="image-4.png" coords="7,112.50,209.28,370.30,123.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>23</head><label>23</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Bidirectional RNN (left) and deep bidirectional RNN (right) Researchers have developed more sophisticated types of RNN to deal with the shortcomings of the standard RNN model: Bidirectional RNN, Deep Bidirectional RNN and Long Short Term Memory network.Bidirectional RNN is based on the idea that the output at each time may not only depend on the previous elements in the sequence, but also depend on the next elements in the sequence. For instance, to predict a missing word in a sequence, we may need to look at both the left and the right context. A bidirectional RNN 24 consists of two RNNs, which are stacked on the top of each other. The one that processes the input in its original order and the one that processes the reversed input sequence. The output is then computed based on the hidden state of both RNNs. Deep bidirectional RNN is similar to bidirectional RNN. The only difference is that it has multiple layers per time step,</figDesc><graphic url="image-6.png" coords="8,83.25,412.06,428.80,185.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Long Short Term Memory network Figure 7 shows an example of LSTM. At time step 𝑡, LSTM first decides what information to dump from the cell state. This decision is made by a sigmoid function/layer 𝜎, called the "forget gate". The function takes ℎ !!! (output from the previous hidden layer) and 𝑥 ! (current input), and outputs a number in [0, 1], where 1 means "completely keep" and 0 means "completely dump" in Equation (7).</figDesc><graphic url="image-7.png" coords="9,116.58,242.07,362.15,206.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>focal point over time. In NLP, the attention mechanism allows the model to learn what to attend to based on the input text and what it has produced so far, rather than encoding the full source text into a fixed-length vector like standard RNN and LSTM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Attention mechanism in bidirectional recurrent neural network</figDesc><graphic url="image-8.png" coords="10,221.15,470.06,152.44,204.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Recursive Neural network</figDesc><graphic url="image-9.png" coords="12,154.40,72.00,285.94,195.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Training algorithm: stochastic gradient descent via backpropagation Initialize weights 𝑊 and biases 𝑏 of the neural network 𝑁 with random values do for each training example (𝑥 ! , 𝑦 ! ) 𝑝 ! = neural-network-prediction (𝑁, 𝑥 ! ) calculate gradients of loss function ( 𝑝 ! , 𝑦 ! ) with respect to 𝑤 ! at layer 𝐿 ! get ∆𝑤 ! for all weights from hidden layer 𝐿 ! to output layer 𝐿 ! calculate gradient with respect to 𝑤 ! by chain rule at layer 𝐿 ! get ∆𝑤 ! for all weights from input layer 𝐿 ! to hidden layer 𝐿 ! update ( 𝑤 ! , 𝑤 ! ) until all training examples are classified correctly or other stopping criteria are met return the trained neural network Training the neural network in Figure</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>introduced the concept of Memory Networks (MemNN) for the question answering problem. It works with several inference components combined with a large long-term memory. The components can be neural networks. The memory acts as a dynamic knowledge base. The four learnable/inference components function as follows: I component coverts the incoming input to the internal feature representation; G component updates old memories given the new input; O component generates output (also in the feature representation space); R component converts the output into a response format. For instance, given a list of sentences and a question for question answering, MemNN finds evidences from those sentences and generates an answer. During inference, the I component reads one sentence at a time and encodes it into a vector representation.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Deep learning methods for document level sentiment classification</figDesc><table><row><cell cols="9">Moraes et al.</cell><cell>34</cell><cell>BoW</cell><cell>ANN</cell><cell>(Artificial</cell><cell>Neural</cell><cell>No</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Network)</cell></row><row><cell>Le</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>and</cell><cell>Learning dense vector at</cell><cell>Paragraph Vector</cell><cell>No</cell><cell>-</cell></row><row><cell cols="4">Mikolov</cell><cell cols="3">35</cell><cell></cell><cell>sentence,</cell><cell>paragraph,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>document level</cell></row><row><cell cols="9">Glorot et al.</cell><cell>36</cell><cell>BoW to dense document</cell><cell>SDA (Stacked Denoising</cell><cell>No</cell><cell>Unsupervised data</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>vector</cell><cell>Autoencoder)</cell><cell>representation from</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>target domains (in</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>transfer</cell><cell>learning</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>settings)</cell></row><row><cell cols="2">Zhai</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>and</cell><cell>BoW to dense document</cell><cell>DAE</cell><cell>(Denoising</cell><cell>No</cell><cell>-</cell></row><row><cell cols="2">Zhang</cell><cell cols="2">37</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>vector</cell><cell>Autoencoder)</cell></row><row><cell cols="9">Johnson and</cell><cell>BoW to dense document</cell><cell>BoW-CNN and Seq-CNN</cell><cell>No</cell><cell>-</cell></row><row><cell cols="2">Zhang</cell><cell cols="2">38</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>vector</cell></row><row><cell cols="7">Tang et al.</cell><cell cols="2">39</cell><cell>Word embeddings to</cell><cell>CNN/LSTM</cell><cell>(to</cell><cell>learn</cell><cell>No</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>dense document vector</cell><cell>sentence representation) +</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>GRU (to learn document</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>representation)</cell></row><row><cell cols="7">Tang et al.</cell><cell cols="2">40</cell><cell>Word embeddings to dense document vector</cell><cell>UPNN (User Product Neutral Network) based on CNN</cell><cell>No</cell><cell>User information and product information</cell></row><row><cell cols="9">Chen et al.</cell><cell>41</cell><cell>Word embeddings to dense document vector</cell><cell>UPA Attention) based on LSTM (User Product</cell><cell>Yes</cell><cell>User information and product Information</cell></row><row><cell>Dou</cell><cell>42</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Word embeddings to</cell><cell>Memory Network</cell><cell>Yes</cell><cell>User information and</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>dense document vector</cell><cell>product Information</cell></row><row><cell cols="5">Xu et al.</cell><cell cols="2">43</cell><cell></cell><cell>Word embeddings to</cell><cell>LSTM</cell><cell>No</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>dense document vector</cell></row><row><cell cols="7">Yang et al.</cell><cell cols="2">44</cell><cell>Word embeddings to</cell><cell>GRU-based</cell><cell>sequence</cell><cell>Hierarchical</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>dense document vector</cell><cell>encoder</cell><cell>attention</cell></row><row><cell cols="6">Yin et al.</cell><cell>45</cell><cell></cell><cell>Word embeddings to</cell><cell>Input encoder and LSTM</cell><cell>Hierarchical</cell><cell>Aspect/target</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>dense document vector</cell><cell>attention</cell><cell>information</cell></row><row><cell cols="8">Zhou et al.</cell><cell>46</cell><cell>Word embeddings to</cell><cell>LSTM</cell><cell>Hierarchical</cell><cell>Cross-lingual</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>dense document vector</cell><cell>attention</cell><cell>information</cell></row><row><cell cols="3">Li et al.</cell><cell cols="4">47</cell><cell></cell><cell>Word embeddings to</cell><cell>Memory Network</cell><cell>Yes</cell><cell>Cross-domain</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>dense document vector</cell><cell>information</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>reported a CNN structured deep network, named Deep Coupled Adjective and Noun (DCAN) neural network, for visual sentiment classification. The key idea of DCAN is to harness the adjective and noun text descriptions, treating them as two (weak) supervision signals to learn two intermediate sentiment representations. Those learned representations are then concatenated and used for sentiment classification.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Bing Liu and Shuai Wang's work was supported in part by National Science Foundation (NSF) under grant no. IIS1407927 and IIS-1650900, and by Huawei Technologies Co. Ltd with a research gift.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Sentiment analysis: mining opinions, sentiments, and emotions</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>The Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Sentiment analysis and opinion mining (introduction and survey)</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-05">May 2012</date>
			<publisher>Morgan &amp; Claypool</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS 2011)</title>
				<meeting>the International Conference on Artificial Intelligence and Statistics (AISTATS 2011)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning representations by back-propagating errors. Cognitive modelling</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A primer on neural network models for natural language processing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Representation learning: a review and new perspectives</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
				<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hierarchical probabilistic neural network language model</title>
		<author>
			<persName><forename type="first">F</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Artificial Intelligence and Statistics</title>
				<meeting>the International Workshop on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dean</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations (ICLR 2013)</title>
				<meeting>International Conference on Learning Representations (ICLR 2013)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dean</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference on Advances in Neural Information Processing Systems (NIPS 2013)</title>
				<meeting>the Annual Conference on Advances in Neural Information Processing Systems (NIPS 2013)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning word embeddings efficiently with noise-contrastive estimation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference on Advances in Neural Information Processing Systems (NIPS 2013)</title>
				<meeting>the Annual Conference on Advances in Neural Information Processing Systems (NIPS 2013)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL 2012)</title>
				<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL 2012)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">GloVe: global vectors for word representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods on Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference on Advances in Neural Information Processing Systems</title>
				<meeting>the Annual Conference on Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page">2006</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<date type="published" when="2006-07">July 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
				<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Traffic sign recognition with multi-scale convolutional networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Neural Networks (IJCNN 2011)</title>
				<meeting>the International Joint Conference on Neural Networks (IJCNN 2011)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName><forename type="first">Tai</forename><forename type="middle">K S</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL 2015)</title>
				<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL 2015)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modelling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.3916</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">Memory networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fergus</forename><forename type="middle">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Conference on Neural Information Processing Systems (NIPS 2015)</title>
				<meeting>the 29th Conference on Neural Information Processing Systems (NIPS 2015)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">Neural Turing Machines. preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning tag embeddings and tag-specific composition functions in the recursive neural network</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL 2015)</title>
				<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL 2015)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Document-level sentiment classification: an empirical comparison between SVM and ANN. Expert Systems with Applications</title>
		<author>
			<persName><forename type="first">R</forename><surname>Moraes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Valiati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Neto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
				<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Domain adaption for large-scale sentiment classification: a deep learning approach</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML 2011</title>
				<meeting>the International Conference on Machine Learning (ICML 2011</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Semisupervised autoencoder for sentiment analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">(</forename><surname>Zhongfei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">)</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Conference on Artificial Intelligence (AAAI 2016)</title>
				<meeting>AAAI Conference on Artificial Intelligence (AAAI 2016)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Effective use of word order for text categorization with convolutional neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter</title>
				<meeting>the Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies (NAACL-HLT</publisher>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Document modelling with gated recurrent neural network for sentiment classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning semantic representations of users and products for document level sentiment classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL 2015)</title>
				<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL 2015)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Neural sentiment classification with user and product attention</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Capturing user and product Information for document level sentiment analysis with deep memory network</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Y</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods on Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cached long short-term memory neural networks for document-level sentiment classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter</title>
				<meeting>the Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Document-level multi-aspect sentiment classification as machine comprehension</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Attention-based LSTM network for cross-lingual sentiment classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wan</forename><forename type="middle">X</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">End-to-end adversarial memory network for cross-domain sentiment classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><forename type="middle">Q</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence</title>
				<meeting>the International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Development and use of a gold standard data set for subjectivity classifications</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL 1999)</title>
				<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL 1999)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods on Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods on Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL 2014)</title>
				<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL 2014)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL 2014)</title>
				<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL 2014)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for sentiment analysis for short texts</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Dos Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gatti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Linguistics (COLING 2014)</title>
				<meeting>the International Conference on Computational Linguistics (COLING 2014)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Predicting polarities of tweets by composing word embeddings with long short-term memory</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><forename type="middle">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL 2015)</title>
				<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL 2015)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional LSTM and other neural network architectures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Dimensional sentiment analysis using a regional CNN-LSTM model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L-C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL 2016)</title>
				<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL 2016)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Combination of convolutional and recurrent neural network for sentiment analysis of short texts</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Linguistics (COLING 2016)</title>
				<meeting>the International Conference on Computational Linguistics (COLING 2016)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">CNN-and LSTM-based claim classification in online user comments</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guggilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Linguistics (COLING 2016)</title>
				<meeting>the International Conference on Computational Linguistics (COLING 2016)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Encoding syntactic knowledge in neural networks for sentiment classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A multilayer perceptron based ensemble technique for fine-grained financial sentiment analysis</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ekbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bhattacharyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods on Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Weakly-supervised deep learning for customer review sentiment classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI 2016)</title>
				<meeting>the International Joint Conference on Artificial Intelligence (IJCAI 2016)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Context-sensitive lexicon features for neural sentiment analysis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D-T</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Learning sentence embeddings with auxiliary tasks for cross-domain sentiment classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Microblog sentiment classification via recurrent random walk network learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Internal Joint Conference on Artificial Intelligence</title>
				<meeting>the Internal Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning cognitive features from gaze data for sentiment and sarcasm classification using convolutional neural network</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bhattacharyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Linguistically regularized LSTM for sentiment classification</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Adaptive recursive neural network for target-dependent Twitter sentiment classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL 2014)</title>
				<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL 2014)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Target-dependent twitter sentiment classification with rich automatic features</title>
		<author>
			<persName><forename type="first">D-T</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Internal Joint Conference on Artificial Intelligence (IJCAI 2015)</title>
				<meeting>the Internal Joint Conference on Artificial Intelligence (IJCAI 2015)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Effective LSTMs for target-dependent sentiment classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Linguistics (COLING 2016)</title>
				<meeting>the International Conference on Computational Linguistics (COLING 2016)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">A hierarchical model of reviews for aspect-based sentiment analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ghaffari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Breslin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods on Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Gated neural networks for targeted sentiment analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D-T</forename><surname>Vo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Conference on Artificial Intelligence (AAAI 2016)</title>
				<meeting>AAAI Conference on Artificial Intelligence (AAAI 2016)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Attention-based LSTM for aspect-level sentiment classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Attention-based LSTM for target-dependent sentiment classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Conference on Artificial Intelligence</title>
				<meeting>AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Attention modeling for targeted sentiment</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the European Chapter</title>
				<meeting>the Conference of the European Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Aspect-level sentiment classification with deep memory network</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08900</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Rationalizing neural predictions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods on Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Deep memory networks for attitude Identification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Web Search and Data Mining</title>
				<meeting>the ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Interactive attention networks for aspect-Level sentiment classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Internal Joint Conference on Artificial Intelligence</title>
				<meeting>the Internal Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Recurrent attention network on memory for aspect sentiment analysis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><forename type="middle">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods on Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Dyadic memory networks for aspect-based sentiment analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Hui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Information and Knowledge Management</title>
				<meeting>the International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Investigating LSTMs for joint extraction of opinion entities and relations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL 2016)</title>
				<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL 2016)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Recursive neural conditional random fields for aspect-based sentiment analysis</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><forename type="middle">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Coupled multi-Layer attentions for co-extraction of aspect and opinion terms</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><forename type="middle">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Conference on Artificial Intelligence</title>
				<meeting>AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Deep multi-task learning for aspect term extraction with memory Interaction</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods on Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">An unsupervised neural attention model for aspect extraction</title>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dahlmeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Neural networks for open domain targeted sentiment</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D-T</forename><surname>Vo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Representation learning for aspect category detection in online reviews</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wan</forename><forename type="middle">X</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of AAAI Conference on Artificial Intelligence (AAAI 2015)</title>
				<meeting>eeding of AAAI Conference on Artificial Intelligence (AAAI 2015)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Unsupervised word and dependency path embeddings for aspect term extraction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI 2016)</title>
				<meeting>the International Joint Conference on Artificial Intelligence (IJCAI 2016)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Distance metric learning for aspect phrase grouping</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lou</forename><forename type="middle">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Linguistics (COLING 2016)</title>
				<meeting>the International Conference on Computational Linguistics (COLING 2016)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Aspect extraction for opinion mining with a deep convolutional neural network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Knowledge-based Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Recurrent neural networks with auxiliary labels for cross-domain opinion target extraction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Conference on Artificial Intelligence</title>
				<meeting>AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Opinion mining with deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">O</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods on Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Extracting opinion expressions with semi-markov conditional random fields</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Fine-grained opinion mining with recurrent neural networks and word embeddings</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Deep recursive neural networks for compositionality in language</title>
		<author>
			<persName><forename type="first">O</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference on Advances in Neural Information Processing Systems (NIPS 2014)</title>
				<meeting>the Annual Conference on Advances in Neural Information Processing Systems (NIPS 2014)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Neural networks for integrating compositional and non-compositional sentiment in sentiment composition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sobhani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter</title>
				<meeting>the Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies (NAACL-HLT</publisher>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Joint Inference for fine-grained opinion extraction</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL 2013)</title>
				<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL 2013)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Recognizing opinion sources based on a new categorization of opinion types</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI 2016)</title>
				<meeting>the International Joint Conference on Artificial Intelligence (IJCAI 2016)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Content-based influence modelling for opinion behaviour Prediction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><forename type="middle">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Linguistics (COLING 2016)</title>
				<meeting>the International Conference on Computational Linguistics (COLING 2016)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Multilingual connotation frames: a case study on social media for targeted sentiment analysis and forecast</title>
		<author>
			<persName><forename type="first">H</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Volkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Mass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL 2011)</title>
				<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL 2011)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Sentiment classification based on supervised latent n-gram analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bespalov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shokoufandeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Information and Knowledge Management (CIKM 2011)</title>
				<meeting>the International Conference on Information and Knowledge Management (CIKM 2011)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Re-embedding words</title>
		<author>
			<persName><forename type="first">I</forename><surname>Labutov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL 2013)</title>
				<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL 2013)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Learning sentiment-specific word embedding for twitter sentiment classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL 2014)</title>
				<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL 2014)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Sentiment embeddings with applications to sentiment analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhoug</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Sentiment Lexicon construction with representation Learning based on hierarchical sentiment Supervision</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods on Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Refining word embeddings for sentiment analysis</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods on Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Do multi-sense embeddings improve natural language understanding?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Improving Twitter sentiment classification using topic-enriched multiprototype word embeddings</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of AAAI Conference on Artificial Intelligence (AAAI 2016)</title>
				<meeting>eeding of AAAI Conference on Artificial Intelligence (AAAI 2016)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Learning bilingual sentiment word embeddings for cross-language sentiment classification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL 2015)</title>
				<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL 2015)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Exploring distributional representations and machine translation for aspectbased cross-lingual sentiment classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Badia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics (COLING 2016)</title>
				<meeting>the 27th International Conference on Computational Linguistics (COLING 2016)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Collaborative multi-Level embedding learning from reviews for rating prediction</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI 2016)</title>
				<meeting>the International Joint Conference on Artificial Intelligence (IJCAI 2016)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Sentiment intensity ranking among adjectives using sentiment bearing word embeddings</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Somani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bhattacharyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods on Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Tweet sarcasm detection using deep neural network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Linguistics (COLING 2016)</title>
				<meeting>the International Conference on Computational Linguistics (COLING 2016)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Are word embedding-based features useful for sarcasm detection?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carman</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods on Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">A deeper look into sarcastic tweets using deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Linguistics (COLING 2016)</title>
				<meeting>the International Conference on Computational Linguistics (COLING 2016)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Sarcasm SIGN: Interpreting sarcasm with sentiment based monolingual machine translation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Peled</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Reichart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Magnets for sarcasm: making sarcasm detection timely, contextual and very personal</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Veale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods on Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Monday mornings are my fave:)# not exploring the automatic recognition of irony in english tweets</title>
		<author>
			<persName><forename type="first">C</forename><surname>Van Hee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lefever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Linguistics (COLING 2016)</title>
				<meeting>the International Conference on Computational Linguistics (COLING 2016)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">WordForce: visualizing controversial words in debates</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">W</forename><surname>Ku</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">A bilingual attention network for code-switched emotion prediction</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Linguistics (COLING 2016)</title>
				<meeting>the International Conference on Computational Linguistics (COLING 2016)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<monogr>
		<title level="m" type="main">Emotional chatting machine: emotional conversation generation with internal and external memory</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01074</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">EmoNet: fine-grained emotion detection with gated recurrent neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abdul-Mageed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm</title>
		<author>
			<persName><forename type="first">B</forename><surname>Felbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mislove</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Rahwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods on Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">A question answering approach to emotion cause extraction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Du</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods on Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Deep convolutional neural text features and multiple kernel learning for utterance-level multimodal sentiment analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods on Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Real-time speech emotion and sentiment recognition for interactive dialogue systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bertero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">B</forename><surname>Siddique</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wan</forename><forename type="middle">Y</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Chan</forename><forename type="middle">R H</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Zara: a virtual interactive dialogue system incorporating emotion, sentiment and personality recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">B</forename><surname>Siddique</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bertero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Linguistics (COLING 2016)</title>
				<meeting>the International Conference on Computational Linguistics (COLING 2016)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Beyond object recognition: visual sentiment analysis with deep coupled adjective and noun neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mei</forename><forename type="middle">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Internal Joint Conference on Artificial Intelligence (IJCAI 2016)</title>
				<meeting>the Internal Joint Conference on Artificial Intelligence (IJCAI 2016)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Learning visual sentiment distributions via augmented conditional probability neural network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Conference on Artificial Intelligence</title>
				<meeting>AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Dependency exploitation: a unified CNN-RNN approach for visual emotion recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Internal Joint Conference on Artificial Intelligence</title>
				<meeting>the Internal Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Visual sentiment analysis by attending on local image regions</title>
		<author>
			<persName><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Conference on Artificial Intelligence</title>
				<meeting>AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Context-dependent sentiment analysis in user-generated videos</title>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Using deep and convolutional neural networks for accurate emotion classification on DEAP dataset</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhattacharya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Conference on Artificial Intelligence</title>
				<meeting>AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Tensor fusion network for multimodal sentiment analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods on Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">A cognition based attention model for sentiment analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods on Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Select-additive learning: improving generalization in multimodal sentiment analysis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Meghawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><forename type="middle">E X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Multimedia and Expo</title>
				<meeting>the International Conference on Multimedia and Expo</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">A hybrid deep learning architecture for sentiment analysis</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ekbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bhattacharyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Linguistics (COLING 2016)</title>
				<meeting>the International Conference on Computational Linguistics (COLING 2016)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Word embeddings and convolutional neural network for Arabic sentiment classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Haddoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duan</forename><forename type="middle">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Linguistics (COLING 2016)</title>
				<meeting>the International Conference on Computational Linguistics (COLING 2016)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Borrow a little from your rich cousin: using embeddings and polarities of english words for multilingual sentiment classification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bhattacharyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Linguistics (COLING 2016)</title>
				<meeting>the International Conference on Computational Linguistics (COLING 2016)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Towards sub-word level compositions for sentiment analysis of Hindi-English code mixed text</title>
		<author>
			<persName><forename type="first">A</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Linguistics (COLING 2016)</title>
				<meeting>the International Conference on Computational Linguistics (COLING 2016)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Intersubjectivity and sentiment: from language to knowledge</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><forename type="middle">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI 2016)</title>
				<meeting>the International Joint Conference on Artificial Intelligence (IJCAI 2016)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Sentiment lexicon expansion based on neural PU Learning, double dictionary lookup, and polarity association</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods on Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Volatility prediction using financial disclosures sentiments with word embedding-based IR models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Rekabsaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lupu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baklanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dür</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anderson</forename><forename type="middle">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Opinion recommendation using a neural model</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods on Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Stance detection with bidirectional conditional encoding</title>
		<author>
			<persName><forename type="first">I</forename><surname>Augenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bontcheva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Stance classification with target-specific neural attention networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Internal Joint Conference on Artificial Intelligence</title>
				<meeting>the Internal Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
