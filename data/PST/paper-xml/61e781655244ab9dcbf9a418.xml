<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Audio-Driven Talking Face Video Generation with Dynamic Convolution Kernels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-01-16">16 Jan 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zipeng</forename><surname>Ye</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mengfei</forename><surname>Xia</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ran</forename><surname>Yi</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Juyong</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Yu-Kun</forename><surname>Lai</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xuwei</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Guoxin</forename><surname>Zhang</surname></persName>
						</author>
						<author role="corresp">
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Yong-Jin</forename><surname>Liu</surname></persName>
							<email>liuy-ongjin@tsinghua.edu.cn.</email>
						</author>
						<author>
							<persName><forename type="first">Y.-K</forename><surname>Lai</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Mathematical Sciences</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">School of Computer Science and Informatics</orgName>
								<orgName type="institution">Cardiff University</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Audio-Driven Talking Face Video Generation with Dynamic Convolution Kernels</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-01-16">16 Jan 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TMM.2022.3142387</idno>
					<idno type="arXiv">arXiv:2201.05986v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>dynamic kernel</term>
					<term>convolutional neural network</term>
					<term>multi-modal generation task</term>
					<term>audio-driven talking-face generation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present a dynamic convolution kernel (DCK) strategy for convolutional neural networks. Using a fully convolutional network with the proposed DCKs, highquality talking-face video can be generated from multi-modal sources (i.e., unmatched audio and video) in real time, and our trained model is robust to different identities, head postures, and input audios. Our proposed DCKs are specially designed for audio-driven talking face video generation, leading to a simple yet effective end-to-end system. We also provide a theoretical analysis to interpret why DCKs work. Experimental results show that our method can generate high-quality talking-face video with background at 60 fps. Comparison and evaluation between our method and the state-of-the-art methods demonstrate the superiority of our method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T ALKING-FACE video refers to video which mainly focuses on head or upper body of the speaker given audio or text signals. It has wide range of applications in news, TV shows, commercials, online chat, online courses, etc. According to the types of input signals, there are text-driven (e.g., <ref type="bibr" target="#b0">[1]</ref>), audio-driven (e.g, <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b10">[11]</ref>) and video-driven (e.g., <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b16">[17]</ref>) talking-face systems. In this paper, we propose an audiodriven talking-face system, capable of transferring the input talking-face video to a generated one corresponding to the input audio. It is a naturally cross-modal task with video and audio (i.e., of visual and auditory modalities) as input. The two modalities are strongly correlated <ref type="bibr" target="#b17">[18]</ref>, and thus it is possible to drive the talking-face video using an audio.</p><p>In this paper, we consider multi-modal fusion in a generation task, i.e., audio-driven talking-face video generation. For this task, a direct way is to treat multi-modal input as different features. To align these features, we can rearrange the audio features as additional channels of image frames and concatenate them with the image features. However, this method maps the audio feature elements to fixed locations, and as we will later show in our experiments presented in Section VI-D, it only works under special conditions where all the frames are aligned (i.e., each frame containing a frontal face at a fixed position), which are difficult to meet in practice. Another possible way is to use landmark points or parametric models <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref> as a prior, which can be inferred from the audio sequence. Facial landmarks are highly correlated to expression but also sensitive to head pose, view angle and scale. Therefore, it is necessary to align input photo/frames with a standard face, which has challenges dealing with the following: (1) facial image fusion with background, (2) head motion and (3) extreme head pose. 3D parametric models can be used as a strict and precise prior, which preserves almost all the information of expression and lip motion, and we can render an image using the parametric model. However, parametric models only contain low frequency information and the rendered images are often not photo-realistic. Therefore, post-processing is needed, which makes the pipeline complex and time-consuming. On the other hand, using these priors, it is difficult to design an end-to-end system with a fully convolutional neural network (FCNN), which is desired to ensure generalizability.</p><p>To overcome these drawbacks, in this paper, we propose a novel dynamic convolution kernel (DCK) technique that works well with FCNN for multi-modal generation tasks. Our key idea is to use a network to infer DCKs in a FCNN from audio modality. Then this FCNN can work with diverse input videos that have different head poses. Our model, i.e., FCNN with DCKs, is a network with dynamic parameters. In the literature, a few dynamic CNN parameter methods existed <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b24">[25]</ref>. However, they were all proposed for processing single modal information and due to limited adaptivity, they are difficult to be extended to handle cross-modal applications. See Section II-B for more details. Our DCKs are totally different in both purpose and content: (1) DCKs are for multi-modal tasks, where the kernel is inferred from input audio, and (2) DCKs use completely flexible kernels, and are linear once the kernels are determined.</p><p>In this paper, we consider the following characteristics in our audio-driven talking-face system: (1) Real time: the video can be generated online when the audio signal is available; <ref type="bibr" target="#b1">(2)</ref> High quality: the quality of generated video should be good enough such that people cannot easily distinguish between real video and generated video; (3) Identity preserving: the identity of the generated video should be preserved with the input video or photo; (4) Expression and voice synchronization: the expression and lip motion of the generated video should be synchronized with the input audio; <ref type="bibr" target="#b4">(5)</ref> Head motion: the head pose and head motion of the generated video should be natural.</p><p>To address these characteristics, we propose DCKs and use them to build an end-to-end and one-for-all system, which only needs to be trained once and can work for different identities. To make better use of the multi-modal inputs which are difficult to fuse, we design DCKs which are different from traditional static convolution kernels. Once the model is trained, traditional convolution kernels no longer change. In contrast, our DCK will change with different inputs. We use the pre-trained audio network <ref type="bibr" target="#b25">[26]</ref> to extract audio features and train a fully connected network to infer the DCKs from the input audio, and therefore we can design a fully convolutional network for video with different audio inputs well handled. We adapt the U-net <ref type="bibr" target="#b26">[27]</ref> for DCKs by replacing convolutional kernels at selected layers to DCKs. Furthermore, we propose a novel dataset (including real videos and synthetic videos) to train our model in a supervised way.</p><p>In summary, the main technical contributions of our work include:</p><p>• We propose DCKs as an effective way to generate highquality talking-face video from multi-modal input in real time with background and natural head motion, which is simple yet effective. • We provide a theoretical analysis to explain DCKs' effectiveness.</p><p>• We propose a novel mixed dataset, including both real videos and synthetic videos, to supervise the training of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Multi-modal Fusion</head><p>One key challenge in tasks with multi-modal input is how to effectively fuse features in them. In various engineering fields, many algorithms have been proposed for fusing features collected from different types of sensors, which may have different modalities, rates, formats or confidence levels. The Kalman filter <ref type="bibr" target="#b27">[28]</ref> is a classical algorithm for multi-sensor fusion. Bayesian inference <ref type="bibr" target="#b28">[29]</ref> is another classic technique to fuse different features. For full details of existing fusion methods, the reader is referred to recent surveys <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref> and references therein.</p><p>Our study focuses on the neural network techniques. In this domain, a simple way for feature fusion is to directly concatenate features. The other simple way is to use different networks for extracting features of different modalities and use late feature fusion. The two simple strategies work well in classification and regression tasks, and achieve successes in many applications (e.g., <ref type="bibr" target="#b31">[32]</ref>- <ref type="bibr" target="#b33">[34]</ref>). Video is the most common input with different modalities. For classification and regression tasks of video, some learning-based methods <ref type="bibr" target="#b34">[35]</ref>- <ref type="bibr" target="#b37">[38]</ref> are proposed, which design network structures for fusing multi-modal input. On the other hand, the talking face video generation is a generation task, which is quite different from classification and regression tasks, and the simple concatenation strategy often fails. The methods which are designed for classification and regression tasks also fail. In some image generation tasks, using landmarks and 3D models as priors is useful to fuse multi-modal input (e.g., <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b38">[39]</ref>). However, as we mention in Section I, it is difficult to design a fully CNN and an end-to-end system using 2D or 3D prior. In this paper, we propose a novel fully convolutional network with DCKs for the talking face video generation task with multi-modal input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Neural Networks with Dynamic Parameters</head><p>The new model proposed in this paper, i.e., the fully convolutional network with DCKs, is a neural network with dynamic parameters. In recent years, several research works on designing neural networks with dynamic parameters have been proposed. The HyperNetworks <ref type="bibr" target="#b20">[21]</ref> uses a hypernetwork to generate the weights for the other network, which has the similar idea as ours, but their motivations (for language modeling) and network structures (using recurrent neural network) are completely different from ours.</p><p>For CNNs, although fixed kernels are dominant in most research, there exist adaptions of CNNs ( <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b24">[25]</ref>) whose kernels can be dynamically adjusted. However, all these methods can only handle single mode information as input. The work <ref type="bibr" target="#b21">[22]</ref> is designed for a classification task, which estimates a set of weights from input and these weights are used for balancing the output of nine sub-network-structures. Although the weights can be dynamically set, only adjusting the weights of nine sub-structures has limited capacity (that is suitable for single mode input); while our DCKs can adaptively set up to 10 4 parameters, which are more powerful and suitable for multi-modal input. The work <ref type="bibr" target="#b24">[25]</ref> is similar to <ref type="bibr" target="#b21">[22]</ref> in the spirit of using dynamic weights to adjust the linear combinations of a few convolution layers. The works <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> predict a feature from input (different input may lead to different feature) and use this feature to do convolution at fixed positions in the network. As a comparison, our DCKs can predict parameters in different positions in the network and thus are more flexible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Audio-driven Talking-face Video Generation</head><p>Audio-driven talking-face video generation is a task that uses an audio to drive a specified face (from a face photo or a talking face video) and produce a talking face video, with focus on fine talking details in head or upper body of the speaker <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b38">[39]</ref>. It is a typical task that uses multimodal input. Many previous works use facial landmarks or 3D morphable models (3DMM) <ref type="bibr" target="#b18">[19]</ref> as priors to bridge the two modalities. Inferring the prior from audio and using it to generate talking face video have been used in some practical methods <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b38">[39]</ref>. However, it is difficult to use 2D or 3D prior to design an end-to-end system which can address the five characteristics summarized in Section I.</p><p>In <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> two end-to-end methods are proposed that do not use 2D or 3D priors. The work <ref type="bibr" target="#b4">[5]</ref> generates talking face video by the composition of a subject-related part and a speech-related part. Based on this composition, they propose an encoder-decoder model to disentangle the two parts from the video and use the input audio as the speech-related part to generate video. The work <ref type="bibr" target="#b3">[4]</ref> proposes a conditional recurrent generation network which uses an audio and an image as input, and output a video. Both methods can only work with a fixed standard head pose and output a video without head motion. As a comparison, our method can work with different poses and generate natural head motion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DYNAMIC CONVOLUTION KERNELS (DCKS)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Motivation</head><p>In this paper, we deal with a talking-face video generation task whose input is a pair of unmatched audio and video. The input contains entirely different modalities which have different forms and contents. How to fuse them together to effectively guide the training process is not easy and many works have studied this. As discussed in Section I, our target is to generate high-quality video with head motion. How to design the fusion is key to achieving these targets. Currently, there are two popular strategies to perform the fusion:</p><p>The first strategy for multi-modal input is to extract their features, and concatenate them or input them together to a network <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref> (Figure <ref type="figure" target="#fig_0">1a</ref>). For example, we can use encoders to transform them into vectors and use a decoder to generate a video, which is not a fully convolutional network. We can also reshape the features of the input audio as channels of an image and concatenate them with the image, which works in a special case that the images are aligned so each pixel position has a fixed content. However, it is hard to achieve success in general because images are usually not aligned strictly and there is no fixed semanteme at each pixel. Our experimental results in Section VI-D demonstrate this observation. The second strategy is to use a parametric model as 3D prior (e.g., <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b38">[39]</ref>) or use facial landmarks as 2D prior (e.g., <ref type="bibr" target="#b1">[2]</ref>). As shown in Figure <ref type="figure" target="#fig_0">1b</ref>, they use audio to predict the prior and then use the prior to conduct the generation of videos. Using facial landmarks requires alignment of images and using 3D prior usually leads to high time cost, as demonstrated by our experimental results in Section VI-E.</p><p>To directly output high-quality video frames, a fully convolutional network is usually preferred to ensure generalizability. In this work, we design dynamic convolution kernels (DCKs), which are different from traditional static convolution kernels (Figure <ref type="figure" target="#fig_0">1c</ref>). Once the model is trained, traditional convolution kernels no longer change. In contrast, our DCK is designed to infer from different inputs and therefore can change during the inference process (Figure <ref type="figure" target="#fig_1">2</ref>). We use the convolution kernel as part of the generative network, which is dynamic for different input audios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Structure of DCKs</head><p>A traditional fully convolutional network f (x), whose convolution kernels are K(f ) = {k 1 , k 2 , . . . , k n }, is a transformation that iteratively applies the convolution operation to the input x. Denote by y 0 , y 1 , . . . , y n the intermediate results where x = y 0 is the input and y n is the output, we have:</p><formula xml:id="formula_0">y i = g i (k i y i−1 ) f or i = 1, 2, . . . , n,<label>(1)</label></formula><p>where k i is the the ith convolution operator whose kernel is k i and g i is the ith combination of normalization and activation function. In this case, all convolution kernels are static which are learned from the training set and will not change in inference.</p><p>We propose a fully convolutional network using DCKs. Some selected convolutional layers K d ⊂ K(f ) are no longer fixed after training, but instead are determined based on the input audio via a neural network, i.e. k j = h j (A), for k j ∈ K d , where h j is a neural network to determine the jth DCK given the input audio A. More generally, denote by Θ(f ) all the parameters of a fully convolutional network, i.e. parameters of all convolution kernels, and some selected parameters Θ d (f ) ⊂ Θ(f ) are no longer fixed after training. A traditional fully convolutional network f (x) can be written as f (x; Θ(f )). The corresponding network with DCKs is </p><formula xml:id="formula_1">f (x; Θ s (f ), Θ d (f )), where Θ s (f ) = Θ(f ) \ Θ d (f ) is the rest</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generating video</head><p>Pre-trained AudioNet Fig. <ref type="figure">3</ref>. The pipeline of our system and the architecture of our model. We adapt the U-net by incorporating dynamic convolution kernels as our generative network. We use the pre-trained audio network <ref type="bibr" target="#b25">[26]</ref> to extract audio features and train a fully connected network to infer the dynamic convolution kernels from the input audio and use them to replace some traditional static convolution kernels. We detect the facial area from input video and crop it as the input of the generative network. The outputs of the generative network are an attention mask and a motion image. We blend them with the input to obtain the generation result.</p><p>static convolution kernels. We use a network to dynamically generate Θ d (f ) based on features of the input audio. We (1) use the pre-trained audio network in Wav2Lip <ref type="bibr" target="#b25">[26]</ref>, which consists of 2D convolutional layers and residual blocks, to extract audio features from Mel Spectrogram of input audio and (2) train a fully connected network to infer Θ d (f ) from the audio features. We have:</p><formula xml:id="formula_2">Θ d (f ) = h 2 (h 1 (A)),<label>(2)</label></formula><p>where A is the Mel Spectrogram of input audio, h 1 is the pre-trained audio network <ref type="bibr" target="#b25">[26]</ref> and h 2 is the fully connected network. We reshape the output of the audio network into the shape of convolution kernels and use them to complete the fully convolutional network. Therefore, the fully convolutional network is dynamic with different input audios. The advantages of using DCKs include the following aspects: (1) We can design a fully convolutional network for the input video, leading to real-time performance; (2) The convolution kernel is dynamic and can effectively fuse features from multi-modal inputs; (3) There is no binding between features and positions of pixels so it can work in different poses and different translations.</p><p>We present a theoretical interpretation for DCKs in Section V, to explain why it is useful for the cross-modal talking face video generation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. THE SYSTEM</head><p>We propose an audio-driven talking face video generation system, whose inputs are unmatched audio and video, and the output is a synthetic video. The pipeline of our system is shown in Figure <ref type="figure">3</ref>. It is an end-to-end approach by directly outputting the synthetic video without intermediate results.</p><p>Our system can generate high quality results in real time by efficiently incorporating the DCK technique and a supervised training scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Fully Convolutional Network with DCKs</head><p>Our system deals with a multi-modal generation task whose input includes both audio and video. We use the pre-trained audio network in Wav2Lip <ref type="bibr" target="#b25">[26]</ref> to extract audio features from Mel Spectrogram of input audio, and train (1) a fully connected network to generate DCKs from audio features, and (2) a fully convolutional network with DCKs. For training the two networks, we propose a novel method to train our model in a supervised way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training</head><p>In our task, the inputs are a pair of unmatched video and audio, and the output is a synthetic video. Denote by V the space of talking-face video and A the space of audio of talking-face video. An audio-driven talking-face system is a function f : V × A → V. For any A ∈ A and V ∈ V, f (V, A) is a synthetic video, which have the same identity as V and the same expression (including lip motion) as A.</p><p>We use a supervised training scheme to train our model. Ideally, we need a training set consisting of pairs of talkingface videos which have different lip motions and the same other attributes (including identity and head motion) to train our model. However, it is difficult to obtain this kind of training dataset of real videos because the condition is too strict: even in a real talking face video without head movement, it is hard to extract two frames with exactly the same head pose. We take an alternative approach that synthesizes videos and pairs them with real videos to build this kind of dataset. Some talking face generation methods <ref type="bibr" target="#b25">[26]</ref> can generate a talking-face video from a reference video and an audio, where the generated video has the same identity and head motion as the reference video. We use the method <ref type="bibr" target="#b25">[26]</ref> to generate a new training dataset by the following steps: (1) we collect N r = 550 real talking-face videos {V 0 1 , V 0 2 , . . . , V 0 Nr } from video websites and collect m = 550 audios from talking-face videos, whose lengths are about 60 seconds; (2) for each real video V 0 i , i = 1, 2, . . . , N r , we use the method <ref type="bibr" target="#b25">[26]</ref> to generate</p><formula xml:id="formula_3">m videos V 1 i , V 2 i , . . . , V m i</formula><p>which has the same identity and head motion with the video V 0 i and different lip motions; (3) we combine V 0 i with V j i as a pair of videos and then we have mN r pairs of videos.</p><p>We use the dataset obtained above (including real videos and synthetic videos) to train our model. In the training set, all talking-face videos have their corresponding audios and we denote their relation by an operator A(V ) : V → A which maps a talking-face video V to its corresponding audio A.</p><p>For each batch, we randomly select a pair of videos V 1 , V 2 and their corresponding audios</p><formula xml:id="formula_4">A 1 = A(V 1 ), A 2 = A(V 2 ). We use our model to generate video f (V 1 , A 2 ) from V 1 and A 2 and f (V 2 , A 1 ) from V 2 and A 1 .</formula><p>Reconstruction Loss. We consider</p><formula xml:id="formula_5">f (V 1 , A 2 ), f (V 2 , A 1</formula><p>) are generation results and V 2 , V 1 are their ground truth. Ideally, the generation results should be exactly the same as the ground truth. We use reconstruction loss to constrain our model to generate talking face videos similar to the ground truth. The loss term is calculated as the L 1 norm of the difference between generation results and the ground truth, i.e.,</p><formula xml:id="formula_6">L rec (f ) = E V1,V2∼V ( f (V 1 , A 2 ) − V 2 1 + f (V 2 , A 1 ) − V 1 1 ).</formula><p>(3) Adversarial Loss. We use adversarial loss to ensure that f (V, A) has the same distribution as V, which can improve the quality of generation results. We adapt the adversarial loss of LSGAN <ref type="bibr" target="#b39">[40]</ref> as:</p><formula xml:id="formula_7">L adv (f, D) = E V1,V2∼V ( D(f (V 1 , A 2 )) 2 2 ) + E V ∼V ( 1 − D(V ) 2 2 ).<label>(4)</label></formula><p>The overall loss function is in the following form:</p><formula xml:id="formula_8">L total (f, D) = L adv (f, D) + λ rec L rec (f ),<label>(5)</label></formula><p>where λ rec is the weight for balancing the multiple objectives.</p><p>For all experiments, we set λ rec = 10. The optimization target is:</p><formula xml:id="formula_9">min f max D L total (f, D).<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Blending</head><p>Instead of directly generating the synthetic frames, the output of our method (shown in Fig. <ref type="figure">4</ref>) is an attention mask α which is a grayscale image, and a motion image M which is a color image that presents the change. α determines at each pixel how much the output should be influenced by the motion image M . Denote by I the input image and I the synthetic image, we have:</p><formula xml:id="formula_10">I = I ⊗ (1 − α) + M ⊗ α, (<label>7</label></formula><formula xml:id="formula_11">)</formula><p>where ⊗ is pixel-wise multiplication.</p><p>Compared with directly generating the synthetic frames, our method has the following advantages. It can not only enforce the network to focus on audiovisual-correlated regions but also offers an efficient post-processing to produce desired output, avoiding expensive image fusion. It also increases the interpretability of the network and we can be informed of where the network focuses. In practice, it is difficult to train the network with DCKs by directly generating the synthetic frames. In Figure <ref type="figure">9</ref>, the generation results of directly generating the synthetic frames have different skin color from inputs whereas those of blending have the same skin color as inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Adding Background in Real Time</head><p>Our system does not need to use image fusion to integrate the generated results with the background, which usually takes a long time. We directly cover the generated results onto the background instead of image fusion and this helps save time. The boundaries of generation results are the same as those of inputs, because we do not change the head pose and non-face area. Due to the attention mechanism and the mask loss, the direct covering has good performance and there are no visible artifacts on the boundary between the face and background. A frame with background generated by direct covering is shown in Figure <ref type="figure">5</ref>.</p><p>As a comparison, other state-of-the-art methods (e.g., <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b40">[41]</ref>) either cannot keep the head pose or change the non-face area, and therefore, the face and background have different color values on the boundary. We note that in these methods, directly covering would cause inconsistency between the generated region and background. Therefore, they cannot adopt our directly covering scheme as efficient as ours; e.g., Wav2Lip <ref type="bibr" target="#b25">[26]</ref> also generates the facial region and uses the direct covering to add background, but as we will show in Section VI-E, their results exhibit clear boundaries between the generated region and background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. THEORETICAL INTERPRETATION FOR DCKS</head><p>We can understand the dynamic convolution kernels (DCKs) in the following way. Denote by T the space of tasks such as all expressions. For a fixed task t ∈ T such as smiling, we can train a network to transfer an image to a new image with smile. For different tasks in T , we can train different networks. We believe these networks are mostly the same with slight distinction. We use static convolution kernels to learn the common characteristics and use dynamic convolution kernels to learn the distinction. Therefore, the network with dynamic kernels can handle all tasks in the space T .</p><p>We present the following formulations to provide a theoretical interpretation of the above statement. Some experimental validations are presented in Section VI-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Interpretation from set approximation</head><p>Assumption 1: A multi-modal task can be fulfilled by solving a set T = {t i } of simpler tasks, each of which t i can be fulfilled by a fully convolutional network f i with fixed parameters. All the networks {f i } have the same structure and most of their parameters are the same. Now we show that a single fully convolutional network with DCKs can well approximate the set of networks {f i } with bounded output errors. Lemma 1: Let the activation function g be any one of Leaky ReLU, tanh or Sigmoid. For any input x 1 , x 2 ∈ R n , the following inequality holds:</p><formula xml:id="formula_12">g(x 1 ) − g(x 2 ) p x 1 − x 2 p .<label>(8)</label></formula><p>Proof 1: For any a ∈ R, a − g(a) increases monotonically; hence for a 1 a 2 , a 1 , a 2 ∈ R, we have a 1 − g(a 1 ) a 2 − g(a 2 ), i.e., |a 1 − a 2 | = a 1 − a 2 g(a 1 ) − g(a 2 ) = |g(a 1 ) − g(a 2 )|. That completes the proof.</p><p>Given two fully convolutional networks f 1 and f 2 corresponding to two sets of convolution kernels {k</p><formula xml:id="formula_13">1 1 , k 1 2 , • • • , k 1 n } and {k 2 1 , k 2 2 , • • • , k 2 n }</formula><p>, respectively, where each pair of (k 1 i , k 2 i ) has the same kernel size, let {y 1 0 , • • • , y 1 n } and {y 2 0 , • • • , y 2 n } be the two sets of intermediate results of two convolution networks f 1 and f 2 , where y 1 0 = y 2 0 = x. Then we have:</p><formula xml:id="formula_14">y j i = g i (k j * i y j i−1 ), i = 1, 2, • • • , n, j = 1, 2,<label>(9)</label></formula><p>where * is the convolution operator and g i is the activation function. The following theorem gives an upper bound of the difference between the two outputs f 1 (x), f 2 (x) in terms of the L p norm. Theorem 1: If all convolution kernels have a uniform upper bound of their L p norm, i.e., k j i p M p for ∀i, j and some M p &gt; 0, the following inequality holds:</p><formula xml:id="formula_15">y 1 n − y 2 n p M n−1 p x p n i=1 k 1 i − k 2 i p . (<label>10</label></formula><formula xml:id="formula_16">)</formula><p>Proof 2: We prove this theorem by induction. First we consider the case n = 1, i.e., there is only one convolution kernel for each set. By calculating the L p loss, we have</p><formula xml:id="formula_17">y 1 1 − y 2 1 p = g(k 1 * 1 x) − g(k 2 * 1 x) p k 1 * 1 x − k 2 * 1 x p = (k 1 1 − k 2 1 ) * x p k 1 1 − k 2 1 p • x p</formula><p>, where the last inequality comes directly from the Cauchy inequality. Now, suppose the inequality (10) holds for n m−1.</p><p>For n = m, we have</p><formula xml:id="formula_18">y 1 m − y 2 m p = g m (k 1 * m y 1 m−1 ) − g m (k 2 * m y 2 m−1 ) p (k 1 * m y 1 m−1 − k 2 * m y 2 m−1 ) p k 1 * m y 1 m−1 − k 1 * m y 2 m−1 p + k 1 * m y 2 m−1 − k 2 * m y 2 m−1 p k 1 m p • y 1 m−1 − y 2 m−1 p + k 1 m − k 2 m p • y 2 m−1 p M p • M m−2 p x p m−1 i=1 k 1 i − k 2 i | p + k 1 m − k 2 m p x p m−1 i=1 k 2 i p M m−1 p x p m i=1 k 1 i − k 2 i | p</formula><p>That completes the proof.</p><p>In practice, the constant M p is usually small, e.g., in all experiments in Section VI, M p = 0.4559. Then Theorem 1 says that for two networks f 1 and f 2 with a fixed number n of convolution layers,</p><formula xml:id="formula_19">f 1 (x) − f 2 (x) p C p x p n i=1 k 1 i − k 2 i p ,<label>(11)</label></formula><p>where C p is a constant independent of the input x and the convolution networks.</p><p>Note that although all the networks in the set {f i } have the same structure and most of their parameters are the same, the remaining parameters can be significantly different. Then any fully convolutional network with fixed parameters cannot well approximate all the networks in {f i }. Let f ∈ {f i } and f be a fully convolutional network with DCKs which are inferred from the audio modality. If the inference makes the parameters in DCKs well approximate the parameters in the corresponding layers of f , f can well approximate any f in {f i }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Interpretation from loss values</head><p>The value of the objective loss function can reflect the quality of generation results of the system to some extent. Next we present the error bounds of two loss terms in our objective function, showing the loss value is approximately optimal 1 using a network with DCKs. In addition, we also present the error bounds of cycle loss <ref type="bibr" target="#b41">[42]</ref>, which is a useful loss term.</p><p>Corollary 1: Let f j D , j = 1, 2, be the discriminator network, which is a fully convolutional network. Let {k j n+1 , • • • , k j n+r } be the convolution kernels of the discriminator D j (Here, the indexes follow those of the generator). The adversarial loss can be expressed as:</p><formula xml:id="formula_20">L adv (f j , D j ) =E x∼X ( D j (f j (x)) 2 2 ) + E x∼X ( 1 − D j (x) 2 2 ) =E x∼X ( f j D • f j (x) 2 2 ) + E x∼X ( 1 − f j D (x) 2 2 ).</formula><p>Then there exist constants A 1 , A 2 &gt; 0, such that the following inequality holds:</p><formula xml:id="formula_21">|L adv (f 1 , D 1 ) − L adv (f 2 , D 2 )| A 1 (DK 2 ) 2 + A 2 • DK 2 E x∼X ( x 2 2 ),</formula><p>where</p><formula xml:id="formula_22">DK 2 = n+r i=1 k 1 i − k 2 i 2 . Proof 3:</formula><p>We can regard the discriminator D j as another convolution network together with convolution kernels {k j n+1 , • • • , k j n+r } and activation functions g n+1 , • • • , g n+m . Notice that there exist C 1 , C 2 &gt; 0, such that the inequalities hold:</p><formula xml:id="formula_23">f 2 D • f 2 (x) 2 C 1 x 2 , and 1 − f 2 D (x) 2 C 2 x 2 .</formula><p>We denote</p><formula xml:id="formula_24">f 1 D • f 1 (x) − f 2 D • f 2 (x) 2 ∆ 1 x 2 , f 1 D (x) − f 2 D (x) 2 ∆ 2 x 2 ,</formula><p>where</p><formula xml:id="formula_25">∆ 1 ∝ n+r i=1 k 1 i − k 2 i 2 , ∆ 2 ∝ r i=1 k 1 n+i − k 2 n+i 2 .</formula><p>1 Assume that the optimal loss value is provided by a network f ∈ {f i }.</p><p>Then we have</p><formula xml:id="formula_26">|L adv (f 1 , D 1 ) − L adv (f 2 , D 2 )| E x∼X (| f 1 D • f 1 (x) 2 2 − f 2 D • f 2 (x) 2 2 |) + E x∼X (| 1 − f 1 D (x) 2 2 − 1 − f 2 D (x) 2 2 |) E x∼X ( f 2 D • f 2 (x) 2 + ∆ 1 x 2 ) 2 − f 2 D • f 2 (x) 2 2 + E x∼X ( 1 − f 2 D (x) 2 + ∆ 2 x 2 ) 2 − 1 − f 2 D (x) 2 2 E x∼X (((C 1 + ∆ 1 ) 2 − C 2 1 ) x 2 2 ) + E x∼X (((C 2 + ∆ 2 ) 2 − C 2 2 ) x 2 2 ) =2(C 1 ∆ 1 + C 2 ∆ 2 )E x∼X ( x 2 2 ) + (∆ 2 1 + ∆ 2 2 )E x∼X ( x 2 2 ) A 1 (DK 2 ) 2 + A 2 • DK 2 E x∼X ( x 2 2 ),</formula><p>where A 1 , A 2 &gt; 0 are two constants. That completes the proof. Corollary 2: Let f P be a fully convolutional network with kernels {k 1 , • • • , k l }. The perceptual loss, including the VGG loss <ref type="bibr" target="#b42">[43]</ref> and LPIPS loss <ref type="bibr" target="#b43">[44]</ref>, can be expressed as:</p><formula xml:id="formula_27">L pcpt (f j , f P ) = E x∼X ( f P (f j (x)) − f P (y) 1 ),</formula><p>where y is the ground truth. Then there exist constants C pcpt &gt; 0 such that the following inequality holds:</p><formula xml:id="formula_28">|L pcpt (f 1 , f P ) − L pcpt (f 2 , f P )| C pcpt ( n i=1 k 1 i − k 2 i 1 )E x∼X ( x 1 ).</formula><p>Specially, if f P degenerates into an identity transformation, the perceptual loss degenerates into L 1 loss in Eq. 3.</p><p>Proof 4: It is a direct consequence of Theorem 1. Corollary 3: Let f j X →Y and f j Y→X , j = 1, 2, be the generators from the domain X to the domain Y and from Y to X , respectively, which are fully convolutional networks. The cycle loss term <ref type="bibr" target="#b41">[42]</ref> can be expressed by</p><formula xml:id="formula_29">L cyc (f j ) = E x∼X ( f j Y→X (f j X →Y (x))) − x 1 ). (<label>12</label></formula><formula xml:id="formula_30">)</formula><p>Then by considering f j Y→X • f j X →Y as a 2n-layer fully convolutional network, we have:</p><formula xml:id="formula_31">|L cyc (f 1 ) − L cyc (f 2 )| C cyc n i=1 ( Dk i,X →Y 1 + Dk i,Y→X 1 ) E x∼X ( x 1 ), where C cyc is a constant, Dk i,X →Y = k 1 i,X →Y − k 2 i,X →Y and Dk i,Y→X = k 1 i,Y→X − k 2 i,Y→X . Proof 5: We have |L cyc (f 1 ) − L cyc (f 2 )| E x∼X ( f 1 Y→X • f 1 X →Y (x) − x 1 − f 2 Y→X • f 2 X →Y (x) − x 1 ) E x∼X ( f 1 Y→X • f 1 X →Y (x) − f 2 Y→X • f 2 X →Y (x) 1 ) C cyc n i=1 ( Dk i,X →Y 1 + Dk i,Y→X 1 ) E x∼X ( x 1 ).</formula><p>That completes the proof. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTS A. Implementation Details</head><p>We implemented our method with PyTorch <ref type="bibr" target="#b44">[45]</ref> and OpenCV. We trained the model on a server with an Intel Xeon Gold 6126 (2.60 GHz) and a NVIDIA TITAN RTX GPU. We also tested it on the same server. We use a mixed video dataset (including real videos and synthetic videos) described in Sec. IV-B to train our model.</p><p>Our system starts with video pre-processing (Figure <ref type="figure">3</ref>). For a video with background, we crop the facial area (detected by Dlib <ref type="bibr" target="#b45">[46]</ref>) from the video and resize it to 256 × 256 as the input of our system. At the end of the pipeline, we cover the facial area with generated results directly without image fusion, which is a major advantage of our method that helps achieve real-time performance.</p><p>We use a U-net with DCKs, called Adapted U-net, as the generator, which has 5 down-sampling layers, 4 middle layers and 5 up-sampling layers, where all middle layers are with DCKs. We use the pre-trained audio network <ref type="bibr" target="#b25">[26]</ref>, which consists of 2D convolutional layers and residual blocks, to extract audio features from Mel Spectrogram of input audio, where the parameters of Mel Spectrogram are the same as <ref type="bibr" target="#b25">[26]</ref>. For each layer with a dynamic convolution kernel, we train a fully connected network to infer the DCK from the audio features. We reshape the output of this module to the shape l × c 1 × (c 2 × ks × ks + 1), where l is the length of video sequence, ks is the kernel size, c 1 and c 2 are the numbers of channels of output and input of the corresponding convolution layers of the adapted U-net. In all our experiments, ks = 1, c 1 = 256 and c 2 = 256. We implement the DCKs by convolution operators with the group parameter in PyTorch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Validation of DCK understanding</head><p>Our system is a fully convolutional network f with DCKs. The theoretical interpretation in Section V indicates that our system f can well approximate a set of networks {f i } with fixed parameters, such that according to different audio input, the system f can adaptively choose the desired f ∈ {f i }. Some results are shown in Figure <ref type="figure" target="#fig_4">6</ref>. Given different audio inputs, our DCKs can successfully infer different parameters that drive the system f to approximate different f ∈ {f i }, e.g., transforming a face (with arbitrary expression in any input frame) into other expressions with different mouth shapes. Another advantage of the system f with DCKs is that for any finite set of {f i }, each input frame can only be transferred into other expressions in a finite expression space, while our DCKs can infer parameters in a continuous space, so that our system can provide better mechanism to secure the inter-frame continuity.  <ref type="figure">9</ref>. Ablation study. The first row shows several frames of real videos as input frames. We use the same audio features as input audio for all the input frames. The syllable of input audio is the word 'were'. The second to the fourth rows show the generation results of our method with one of the modules disabled along with our full model. The generation results without DCKs sometimes have wrong lip motion. The generation results of directly generating the synthetic frames without blending are almost the same as input frames (mouth shape in particular), which shows our method without blending (i.e. the attention mechanism) is difficult to generate good talking-face videos. Only the whole method can generate good results in all cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Visualization of DCKs</head><p>A fully convolutional network with DCKs is a black box. Visualization of DCKs helps us understand it how to work. In all our experiment, kernel sizes of DCKs are 1 so we can visualize each DCKs layer inferred from an audio segment as an image whose weight and height are the numbers of input and output channels of the DCKs layer. The DCKs layer could be regarded as the correlation coefficient between different channels, i.e. their covariance matrix. Four DCKs layer inferred from two audio segments are showed in Figure <ref type="figure" target="#fig_5">7</ref>. Results show different audio inputs lead different parameters of convolution kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Study</head><p>The ablation study focuses on the novel DCKs and blending. First, we design a network f w/o DCKs without DCK, which reshapes the audio features to 16 × H × W (i.e., 16 channels and the same resolution H × W as the image), and use the spatial attention fusion module of the method <ref type="bibr" target="#b46">[47]</ref> to fuse audio features and image features. As shown in Figure <ref type="figure" target="#fig_6">8</ref>, we compare f w/o DCKs with our fully convolutional network f with DCKs in the following test scenario: the face region in the input frame is moved upwards (defined by a translation rate which is the number of upwards translation pixels over the frame height). The results show that f is invariant for translation and outputs similar results for different translation rates, while f w/o DCKs outputs different results for different translation rates, which leads to bad lip synchronization for slightly larger translation rate (Figure <ref type="figure" target="#fig_6">8</ref> middle row, 15% and 20%). The reason is possibly that reshaping the audio features to image space requires fixed semanteme at each pixel.</p><p>To demonstrate the effectiveness of our blending scheme, we train a network to directly generate frames without blending. The qualitative results are shown in Figure <ref type="figure">9</ref>. We use the same audio features as input audio for all the input frames, whose syllable is the word 'were'. We compare the generation results of our method with and without the two modules, i.e. DCKs and blending. The generation results without DCKs sometimes have wrong lip motion. The generation results of directly generating the synthetic frames without blending are almost the same as input frames (mouth shape in particular), which shows our method without blending (i.e. the attention mechanism) is difficult to generate good talking-face videos.</p><p>Only the whole method can generate good results in all cases. We use the test set of LRW dataet <ref type="bibr" target="#b47">[48]</ref> for quantitative metric evaluation. For each video in the test set, we input its first frame and audio signal to the network, and generate a talking-face video for each comparison method. We compare the results with the ground-truth videos, after aligning them according to the way used in ATVGnet <ref type="bibr" target="#b1">[2]</ref>. We use Peak Signal to Noise Ratio (PSNR) and Structural Similarity Index Metrics (SSIM) to evaluate the quality of images, and use Landmark Distance (LMD) <ref type="bibr" target="#b48">[49]</ref> to evaluate the accuracy of lip movement. The results of quantitative comparison are summarized in Table <ref type="table" target="#tab_0">I</ref>. We can see that DCKs and blending are helpful for generating talking-face videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Comparison with State of the Arts</head><p>In this section, we compare our model with state-of-theart methods, including ATVGnet <ref type="bibr" target="#b1">[2]</ref>, You Said That <ref type="bibr" target="#b2">[3]</ref>, Wav2Lip <ref type="bibr" target="#b25">[26]</ref>, X2Face <ref type="bibr" target="#b5">[6]</ref>, DAVS <ref type="bibr" target="#b4">[5]</ref>, SDA <ref type="bibr" target="#b40">[41]</ref>, Yi's Method <ref type="bibr" target="#b8">[9]</ref>. We first introduce and discuss these methods.</p><p>ATVGnet. ATVGnet <ref type="bibr" target="#b1">[2]</ref> generates talking-face video in real time from input photo and audio by hierarchical networks. It crops the facial area from the input photo and aligns it by affine transformation based on facial landmarks extracted from Dlib <ref type="bibr" target="#b45">[46]</ref>. Because the alignment operation changes and fixes the view angle, results of ATVGnet are talking-face videos without background and head motion. The resolution of its generation results is 128 × 128, which is lower than ours. Its results has neither head motion nor eye blink.</p><p>You Said That. You Said That <ref type="bibr" target="#b2">[3]</ref> generates talking-face video from input photo and audio using two CNNs to extract features from audio (spectrum) and photo separately and then concatenating them in channels. It aligns the input photo by applying spatial registration so results of You Said That are talking-face videos without background, head motion and eye blink. The resolution of its generation results is 112 × 112, which is lower than ours.</p><p>Wav2Lip. Wav2Lip <ref type="bibr" target="#b25">[26]</ref> generates talking-face video in real time from input photo (or input video) and audio using CNNs with encoder-decoder structure. It uses a lip-sync expert model to train the generation model in order to achieve high   <ref type="figure" target="#fig_0">10</ref>. The qualitative comparison between the STOA methods (Wav2Lip <ref type="bibr" target="#b25">[26]</ref>, X2Face <ref type="bibr" target="#b5">[6]</ref>, You said that <ref type="bibr" target="#b2">[3]</ref>, SDA <ref type="bibr" target="#b40">[41]</ref>, DAVS <ref type="bibr" target="#b4">[5]</ref>, ATVGnet <ref type="bibr" target="#b1">[2]</ref> and Yi's Method <ref type="bibr" target="#b8">[9]</ref>) and ours. Results of SDA are apparently worse than others. X2Face distorts the shape of face and changes the identity. ATVGnet, DAVS and You Said That results have no head motion and no eye blink. ATVGnet results are of low resolution and degraded visual quality. Wav2Lip results are of low definition in facial area, which sometimes causes an obvious boundary (red boxes) between facial part and other parts when directly covering the generated region onto the background. Yi's Method can not generate talking-head videos in real time. While our method can generate high-quality talking-head videos with head motion in real time. The syllables of input audios are the phrase 'I am' in the left and the word 'really' in the right. lip synchronization. However, the resolution of its generation results is 96 × 96, which is lower than ours.</p><p>X2Face. X2Face <ref type="bibr" target="#b5">[6]</ref> controls a source frame using another frame with different identities to produce generated frame with the identity of the source frame but the pose and expression of the other frame. It uses an auto-encoder to edit a frame in hidden space so the generation process can be driven by audio. However, the model can not totally disentangle different attributes of multi-modal inputs in hidden space which leads to unstable and discontinuous generation results. It can not keep the head pose so results of X2Face are talking-face videos without background.</p><p>DAVS. DAVS <ref type="bibr" target="#b4">[5]</ref> generates talking-face video from input photo and audio using an auto-encoder to disentangle subject-related information and speech-related information via an associative-and-adversarial training process. However, the model can not totally disentangle speech-related information in hidden space, which leads to low audio-visual consistency. It cannot keep the boundary of the cropped face so results of DAVS are talking-face videos without background. It also has  <ref type="bibr" target="#b25">[26]</ref>, X2FACE <ref type="bibr" target="#b5">[6]</ref>, YOU SAID THAT <ref type="bibr" target="#b2">[3]</ref>, SDA <ref type="bibr" target="#b40">[41]</ref>, DAVS <ref type="bibr" target="#b4">[5]</ref>,</p><p>ATVGNET <ref type="bibr" target="#b1">[2]</ref> AND YI'S METHOD <ref type="bibr" target="#b8">[9]</ref>). neither head motion nor eye blink. SDA. SDA <ref type="bibr" target="#b40">[41]</ref> generates talking-face video from input video and audio using a temporal GAN with 2 discriminators, i.e. frame discriminator and sequence discriminator, which are designed for different aspects of a video. However, the quality of generation results decreases over time. It can not keep the boundary of the cropped face so results of SDA are talking-face videos without background. The resolution of its generation results is 96 × 128, which is lower than ours.</p><p>Rendering-based Methods. In recent years, several rendering-based methods (e.g. <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b38">[39]</ref>) for generating talking-face video from a pair of unmatched video and audio have been proposed. Their pipelines are similar, which use the 3D parametric model as a prior, whose parameters are composed of identity components and expression components. They recover identity parameters from the video input by face reconstruction methods and predict expression parameters from the audio input by neural networks. Then they render the reconstructed face model and obtain a rendered frame. Most methods are trained for only one person, and need a large number of training data of one specified person. Only Yi's method <ref type="bibr" target="#b8">[9]</ref> trained a general model and works for arbitrary identity. The model is trained on a dataset with many identities and fine-tuned on a short video of the person. It also works without fine-tuning. Therefore, we only compare our method with Yi's method. In quantitative comparison, Yi's Method is trained on LRW dataset without fine-tuning.</p><p>Qualitative Comparison. The qualitative comparison between the STOA methods (Wav2Lip <ref type="bibr" target="#b25">[26]</ref>, X2Face <ref type="bibr" target="#b5">[6]</ref>, You said that <ref type="bibr" target="#b2">[3]</ref>, SDA <ref type="bibr" target="#b40">[41]</ref>, DAVS <ref type="bibr" target="#b4">[5]</ref>, ATVGnet <ref type="bibr" target="#b1">[2]</ref> and Yi's Method <ref type="bibr" target="#b8">[9]</ref>) and ours is shown in Figure <ref type="figure" target="#fig_0">10</ref>. Results of SDA are apparently worse than others. X2Face distorts the shape of face and changes the identity. The results of ATVGnet, DAVS and You Said That have no head motion and no eye blink. ATVGnet results are of low resolution and degraded visual quality. Wav2Lip results are of low definition in facial area, which sometimes causes an obvious boundary between facial part and other parts when directly covering the generated region onto the background. Yi's Method can not generate talking-head videos in real time. While our method can generate high-quality talking-head videos with head motion in real time.</p><p>Quantitative Comparison. We use the test set of LRW dataet <ref type="bibr" target="#b47">[48]</ref> for quantitative metric evaluation. For each video in the test set, we take its first frame and audio signal as inputs, and generate a talking-face video for each comparison method. We compare the results with the ground-truth videos, after aligning them according to the way used in ATVGnet. We use Peak Signal to Noise Ratio (PSNR) and Structural Similarity Index Metrics (SSIM) to evaluate the quality of images, and use Landmark Distance (LMD) to evaluate the accuracy of lip movement. The results of quantitative comparison between ours and the SOTA methods (Wav2Lip <ref type="bibr" target="#b25">[26]</ref>, X2Face <ref type="bibr" target="#b5">[6]</ref>, You said that <ref type="bibr" target="#b2">[3]</ref>, SDA <ref type="bibr" target="#b40">[41]</ref>, DAVS <ref type="bibr" target="#b4">[5]</ref>, ATVGnet <ref type="bibr" target="#b1">[2]</ref> and Yi's Method <ref type="bibr" target="#b8">[9]</ref>) are summarized in Table <ref type="table" target="#tab_1">II</ref>, showing that our method has better performance than most of the SOTA methods on the three metrics above.</p><p>Perceptual Study. There is no universal metric to evaluate the visual quality of generated video. The metrics used above are also limited in predicting visual quality. Therefore, it is a good way to use a perceptual study for measuring the visual quality. We collected 10 videos in the wild with different head poses and 10 audios as inputs, and combined them to generate 100 videos. We used ATVGnet, Wav2Lip, X2Face, YST and our method to generate talking-face videos without background. A group of five videos generated from the same input was presented in a random order to the participants and they were asked to select the video with the best visual quality (VQ), lip synchronization (LS), inter-frame continuity (IFC) and overall quality (Overall): (1) visual quality is to measure the definition and naturalness of videos, (2) lip synchronization is to measure the correspondence between lip movements and audios, (3) inter-frame continuity is to measure continuity between successive frames of videos, and (4) overall quality is to measure videos by combining all the three metrics. 20 participants attended the perceptual study and each of them compared 20 random groups of video. The statistics of the user study are summarized in Table <ref type="table" target="#tab_3">III</ref>. Our method has the best visual quality, lip synchronization, inter-frame continuity and overall quality. In addition to the figures illustrated in this section, video examples are presented in the accompanying demo video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Running time</head><p>Our method can generate talking-face videos in real time. For generating a 6s video with 159 frames, it takes 0.75s in generating facial video with 256 × 256 resolution and it takes 2.62s in total in generating a video with 1280 × 720  <ref type="bibr" target="#b25">[26]</ref>, X2FACE <ref type="bibr" target="#b5">[6]</ref>, YOU SAID THAT <ref type="bibr" target="#b2">[3]</ref>, SDA <ref type="bibr" target="#b40">[41]</ref>, DAVS <ref type="bibr" target="#b4">[5]</ref>, ATVGNET <ref type="bibr" target="#b1">[2]</ref> AND YI'S METHOD <ref type="bibr" target="#b8">[9]</ref>). YOU SAID THAT, ATVGNET AND X2FACE VII. CONCLUSION We propose a novel fully convolutional network with DCKs for the multi-modal task of audio-driven taking face video generation. Our simple yet effective system can generate highquality talking-face video from unmatched video and audio in real time. Our solution is end-to-end, one-for-all and robust to different identities, head postures and audios. For preserving identities in both input and output talking-head videos, we propose a novel supervised training scheme. The results show our method can generate high-quality 60 fps talkinghead video with background in real time. Comparison and evaluation between our method and state-of-the-art methods show that our method achieves a good balance between various criteria such as running time, qualitative and quantitative qualities. Our novel DCK technique can potentially be applied to other multi-modal generation tasks, and meanwhile, our theoretical interpretation of DCK can be extended from fully convolutional network to forward networks involving ResNet modules, which we will investigate in future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Three strategies for fusing multi-modal input. (a) A simple and direct strategy is to extract the features of different modes, and concatenate them to feed into a network [3], [5]. (b) The second strategy is to use 2D [2] or 3D [9]-[11], [39] prior. It infers the prior's parameters from the audio input. (c) We propose a novel strategy, which extracts features from audio input and reshape features as DCKs of fully convolutional network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of a CNN layer with DCKs. The blue are static convolution kernels and the green are dynamic convolution kernels. We reshape the features of audio modality into the shape of convolution kernels and use them to complete the CNN layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig. 4. Results of attention masks and motion images generated by our method. The audio input is 'for many of us', and the video input is obtained by repeating a face photo multiple times.</figDesc><graphic url="image-90.png" coords="6,220.62,270.56,80.20,80.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Given different audio inputs (successive 3 frames of the word 'for'), our system f can infer different DCKs (i.e., DCKs#1,#2 and #3), such that f with DCKs#1,#2 and #3 can transfer a face (with arbitrary expression in any input frame) into expressions with different mouth shapes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Four DCKs layer inferred from two audio segments are showed. Different audio inputs lead different parameters of convolution kernel. For better observation, we only visualize part of parameters of DCKs (i.e., the first 16 × 16 parameters).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig.8. Ablation study for comparing f w/o DCKs and our system f . The phoneme of input audio is /ba:/ of the word 'Obama'. See text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig.</head><label></label><figDesc>Fig.10. The qualitative comparison between the STOA methods (Wav2Lip<ref type="bibr" target="#b25">[26]</ref>, X2Face<ref type="bibr" target="#b5">[6]</ref>, You said that<ref type="bibr" target="#b2">[3]</ref>, SDA<ref type="bibr" target="#b40">[41]</ref>, DAVS<ref type="bibr" target="#b4">[5]</ref>, ATVGnet<ref type="bibr" target="#b1">[2]</ref> and Yi's Method<ref type="bibr" target="#b8">[9]</ref>) and ours. Results of SDA are apparently worse than others. X2Face distorts the shape of face and changes the identity. ATVGnet, DAVS and You Said That results have no head motion and no eye blink. ATVGnet results are of low resolution and degraded visual quality. Wav2Lip results are of low definition in facial area, which sometimes causes an obvious boundary (red boxes) between facial part and other parts when directly covering the generated region onto the background. Yi's Method can not generate talking-head videos in real time. While our method can generate high-quality talking-head videos with head motion in real time. The syllables of input audios are the phrase 'I am' in the left and the word 'really' in the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I QUANTITATIVE</head><label>I</label><figDesc>COMPARISON BETWEEN OURS AND ABLATION STUDY METHODS.</figDesc><table><row><cell>Metric</cell><cell>w/o Blending</cell><cell>w/o DCKs</cell><cell>Ours</cell></row><row><cell>PSNR ↑</cell><cell>29.29</cell><cell>31.08</cell><cell>31.98</cell></row><row><cell>SSIM ↑</cell><cell>0.74</cell><cell>0.78</cell><cell>0.81</cell></row><row><cell>LMD ↓</cell><cell>1.65</cell><cell>1.61</cell><cell>1.44</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II QUANTITATIVE</head><label>II</label><figDesc>COMPARISON BETWEEN OURS AND THE STOA METHODS (WAV2LIP</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>THE NUMBER IN BRACKET IS THE RANKING OF METHOD.</figDesc><table><row><cell>Metric</cell><cell>Wav2Lip [26]</cell><cell>X2Face [6]</cell><cell>YST [3]</cell><cell>SDA [41]</cell><cell>DAVS [5]</cell><cell>ATVGnet [2]</cell><cell>Yi's Method [9]</cell><cell>Ours</cell></row><row><cell>PSNR ↑</cell><cell>30.72</cell><cell>29.82</cell><cell>29.91</cell><cell>29.44</cell><cell>29.81</cell><cell>30.91</cell><cell>30.85</cell><cell>31.98 (1)</cell></row><row><cell>SSIM ↑</cell><cell>0.76</cell><cell>0.75</cell><cell>0.77</cell><cell>0.68</cell><cell>0.73</cell><cell>0.81</cell><cell>0.75</cell><cell>0.81 (1)</cell></row><row><cell>LMD ↓</cell><cell>1.61</cell><cell>1.60</cell><cell>1.63</cell><cell>2.32</cell><cell>1.73</cell><cell>1.37</cell><cell>1.58</cell><cell>1.44 (2)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III PERCEPTUAL</head><label>III</label><figDesc>STUDY ON VISUAL QUALITY, LIP SYNCHRONIZATION, AND INTER-FRAME CONTINUITY.</figDesc><table><row><cell>Methods</cell><cell>VQ</cell><cell>LS</cell><cell>IFC</cell><cell>Overall</cell></row><row><cell>ATVGnet [2]</cell><cell>1.0%</cell><cell>1.5%</cell><cell>1.0%</cell><cell>1.0%</cell></row><row><cell>Wav2Lip [26]</cell><cell>18.0%</cell><cell>41.2%</cell><cell>29.5%</cell><cell>24.5%</cell></row><row><cell>X2Face [6]</cell><cell>0.0%</cell><cell>0.2%</cell><cell>0.0%</cell><cell>0.0%</cell></row><row><cell>YST [3]</cell><cell>0.8%</cell><cell>1.2%</cell><cell>2.0%</cell><cell>1.5%</cell></row><row><cell>Ours</cell><cell>80.2%</cell><cell>55.8%</cell><cell>67.5%</cell><cell>73.0%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV RUNNING</head><label>IV</label><figDesc>TIME OF OURS AND THE STOA METHODS (WAV2LIP</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>CHANGE THE HEAD POSE SO THEY CANNOT GENERATE RESULTS WITH BACKGROUND. SDA AND DAVS CANNOT KEEP THE BOUNDARY OF THE CROPPED FACE SO THEY ALSO CANNOT GENERATE RESULTS WITH BACKGROUND. For generating a video with the same input, (1) ATVGnet<ref type="bibr" target="#b1">[2]</ref> takes 1.07s to generate video without background, (2) You Said That<ref type="bibr" target="#b2">[3]</ref> takes 14.13s to generate video without background, (3) X2Face<ref type="bibr" target="#b5">[6]</ref> takes 15.10s to generate video without background, (4) DAVS<ref type="bibr" target="#b4">[5]</ref> takes 17.05s to generate video without background, and (5) SDA<ref type="bibr" target="#b40">[41]</ref> takes 5.17s to generate video without background. Yi's method<ref type="bibr" target="#b8">[9]</ref> takes 90.22s to generate video without background and 127.24s to generate video with background, which also takes about 1 hour to fine-tune their network. Wav2Lip<ref type="bibr" target="#b25">[26]</ref> takes 1.58s to generate video without background and takes 3.83s to direct cover it with background. However, sometimes the definition of facial area is lower than other parts which causes an obvious boundary between foreground and background. Therefore, only our method and Wav2Lip can generate talkingface video with background in real time (over 25 fps). We show running time of all above methods in TABLE IV for a direct comparison.</figDesc><table><row><cell>Metric</cell><cell>Wav2Lip [26]</cell><cell>X2Face [6]</cell><cell>YST [3]</cell><cell>SDA [41]</cell><cell>DAVS [5]</cell><cell>ATVGnet [2]</cell><cell>Yi's Method [9]</cell><cell>Ours</cell></row><row><cell>Time Cost per Frame (w/o BG) (ms)</cell><cell>9.9</cell><cell>95.0</cell><cell>88.9</cell><cell>32.5</cell><cell>107.2</cell><cell>6.7</cell><cell>567.4</cell><cell>4.7</cell></row><row><cell>Maximum FPS (w/o BG)</cell><cell>100</cell><cell>11</cell><cell>11</cell><cell>31</cell><cell>9</cell><cell>16</cell><cell>2</cell><cell>212</cell></row><row><cell>Maximum FPS (with BG)</cell><cell>42</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>1</cell><cell>60</cell></row><row><cell>background.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>This work was supported by the Natural Science Foundation of China (61725204), Tsinghua University Initiative Scientific Research Program and Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Text-based editing of talking-head video</title>
		<author>
			<persName><forename type="first">O</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Genova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hierarchical crossmodal talking face generation with dynamic pixel-wise loss</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7832" to="7841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">You said that?</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jamaludin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="109" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Talking face generation by conditional recurrent adversarial network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI)</title>
				<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="919" to="925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Talking face generation by adversarially disentangled audio-visual representation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence (AAAI)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9299" to="9306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">X2face: A network for controlling face generation using images, audio, and pose codes</title>
		<author>
			<persName><forename type="first">O</forename><surname>Wiles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Koepke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="690" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Photo-real talking head with deep bidirectional LSTM</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">K</forename><surname>Soong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4884" to="4888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Realistic speech-driven facial animation with gans</title>
		<author>
			<persName><forename type="first">K</forename><surname>Vougioukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Audio-driven talking face video generation with natural head pose</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2002">2002.10137. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Everybody&apos;s talkin&apos;: Let me talk as you want</title>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2001">2001.05201. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural voice puppetry: Audio-driven facial reenactment</title>
		<author>
			<persName><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="716" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep video portraits</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Face2face: Real-time face capture and reenactment of RGB videos</title>
		<author>
			<persName><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2387" to="2395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bringing portraits to life</title>
		<author>
			<persName><forename type="first">H</forename><surname>Averbuch-Elor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">GANimation: Anatomically-aware facial animation from a single image</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pumarola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Martínez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sanfeliu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="835" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Few-shot adversarial learning of realistic neural talking head models</title>
		<author>
			<persName><forename type="first">E</forename><surname>Zakharov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shysheya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Burkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1905">1905.08233, 2019</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">One-shot face reenactment</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="page">3251</biblScope>
			<date type="published" when="1908">1908. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Auditory versus visual learning of temporal patterns</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Nazzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Nazzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="477" to="478" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A morphable model for the synthesis of 3D faces</title>
		<author>
			<persName><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH)</title>
				<meeting>the 26th Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="187" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Facewarehouse: A 3D facial expression database for visual computing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="413" to="425" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Hypernetworks</surname></persName>
		</author>
		<title level="m">5th International Conference on Learning Representations, ICLR 2017</title>
				<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">April 24-26, 2017. 2017</date>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adaptive convolutional kernels</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Esquivel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Vargas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tickoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1998" to="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">High performance visual tracking with siamese region proposal network</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8971" to="8980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dynamic kernel distillation for efficient pose estimation in videos</title>
		<author>
			<persName><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6942" to="6950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dynamic convolution: Attention over convolution kernels</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">39</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A lip sync expert is all you need for speech to lip generation in the wild</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Prajwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Namboodiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 28th ACM International Conference on Multimedia (MM)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="484" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<biblScope unit="page" from="234" to="241" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A New Approach to Linear Filtering and Prediction Problems</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Kalman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Basic Engineering</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="45" />
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An introduction to multisensor data fusion</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Llinas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="6" to="23" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multimodal fusion for multimedia analysis: a survey</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Atrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">El</forename><surname>Saddik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia systems</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="345" to="379" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multimodal machine learning: A survey and taxonomy</title>
		<author>
			<persName><forename type="first">T</forename><surname>Baltrusaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="423" to="443" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention-based multimodal fusion for video description</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Harsham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sumi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings</title>
				<meeting>null</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4193" to="4202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep multimodal fusion for persuasiveness prediction</title>
		<author>
			<persName><forename type="first">B</forename><surname>Nojavanasghari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gopinath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Koushik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Baltrušaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM International Conference on Multimodal Interaction</title>
				<meeting>the 18th ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="284" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Orthogonalizationguided feature fusion network for multimodal 2d+ 3d facial expression recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1581" to="1591" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Knowledge-augmented multimodal deep regression bayesian networks for emotion video tagging</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1084" to="1097" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep multimodality learning for uav video aesthetic quality assessment</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2623" to="2634" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Mixedemotions: An open-source toolbox for multimodal emotion analysis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Buitelaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">D</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Negi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arcan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Mccrae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Robin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Andryushechkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ziad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sagha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2454" to="2465" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Modeling multimodal clues in a hybrid deep learning framework for video classification</title>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3137" to="3147" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Photorealistic audio-driven video portraits</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Y</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Paul</forename><surname>Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
				<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">End-to-end speech-driven facial animation with temporal GANs</title>
		<author>
			<persName><forename type="first">K</forename><surname>Vougioukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision (ICCV</title>
				<meeting>the IEEE international conference on computer vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2017 Autodiff Workshop: The Future of Gradientbased Machine Learning Software and Techniques</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dlib-ml: A machine learning toolkit</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1755" to="1758" />
			<date type="published" when="2009-07">Jul. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Davd-net: Deep audioaided video decompression of talking heads</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">344</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Lip reading in the wild</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th Asian Conference on Computer Vision (ACCV)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="87" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Lip movements generation at a glance</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="538" to="553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">He won the silver medal twice in 30th and 31st National Mathematical Olympiad of China. His research interests include mathematical foundation in deep learning, image processing and computer vision</title>
		<imprint>
			<date type="published" when="2016">2017. 2020. 2020. 2016 and 2021</date>
			<pubPlace>China; China; China; China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Zipeng Ye is a Ph.D student with Department of Computer Science and Technology, Tsinghua University. He received her B.Eng. degree from Tsinghua University ; Computer Science and Technology, Tsinghua University ; Tsinghua University ; Computer Science and Engineering, Shanghai Jiao Tong University. She received her B.Eng. degree and PhD degree from Tsinghua University</orgName>
		</respStmt>
	</monogr>
	<note>Ran Yi is an assistant professor in the Department of. Her research interests include computational geometry, computer vision and computer graphics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
