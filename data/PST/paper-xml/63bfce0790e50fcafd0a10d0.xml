<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hermes: Accelerating Long-Latency Load Requests via Perceptron-Based O -Chip Load Prediction</title>
				<funder>
					<orgName type="full">VMware</orgName>
				</funder>
				<funder>
					<orgName type="full">Microsoft</orgName>
				</funder>
				<funder>
					<orgName type="full">Semiconductor Research Corporation</orgName>
				</funder>
				<funder>
					<orgName type="full">Intel</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-09-30">30 Sep 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Rahul</forename><surname>Bera</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ETH Z?rich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Konstantinos</forename><surname>Kanellopoulos</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ETH Z?rich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shankar</forename><surname>Balachandran</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Architecture Research Lab</orgName>
								<orgName type="institution">Intel Processor</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Novo</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">LIRMM</orgName>
								<orgName type="institution" key="instit2">Univ. Montpellier</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ataberk</forename><surname>Olgun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ETH Z?rich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mohammad</forename><surname>Sadrosadati</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ETH Z?rich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ETH Z?rich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Hermes: Accelerating Long-Latency Load Requests via Perceptron-Based O -Chip Load Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-09-30">30 Sep 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2209.00188v3[cs.AR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Long-latency load requests continue to limit the performance of modern high-performance processors. To increase the latency tolerance of a processor, architects have primarily relied on two key techniques: sophisticated data prefetchers and large on-chip caches. In this work, we show that: (1) even a sophisticated stateof-the-art prefetcher can only predict half of the o -chip load requests on average across a wide range of workloads, and (2) due to the increasing size and complexity of on-chip caches, a large fraction of the latency of an o -chip load request is spent accessing the on-chip cache hierarchy to solely determine that it needs to go o -chip.</p><p>The goal of this work is to accelerate o -chip load requests by removing the on-chip cache access latency from their critical path. To this end, we propose a new technique called Hermes, whose key idea is to: (1) accurately predict which load requests might go o -chip, and (2) speculatively fetch the data required by the predicted o -chip loads directly from the main memory, while also concurrently accessing the cache hierarchy for such loads.</p><p>To enable Hermes, we develop a new lightweight, perceptronbased o -chip load prediction technique that learns to identify o -chip load requests using multiple program features (e.g., sequence of program counters, byte o set of a load request). For every load request generated by the processor, the predictor observes a set of program features to predict whether or not the load would go o -chip. If the load is predicted to go o -chip, Hermes issues a speculative load request directly to the main memory controller once the load's physical address is generated. If the prediction is correct, the load eventually misses the cache hierarchy and waits for the ongoing speculative load request to nish, and thus Hermes completely hides the on-chip cache hierarchy access latency from the critical path of the correctly-predicted o -chip load. Our extensive evaluation using a wide range of workloads shows that Hermes provides consistent performance improvement on top of a state-of-the-art baseline system across a wide range of con gurations with varying core count, main memory bandwidth, high-performance data prefetchers, and on-chip cache hierarchy access latencies, while incurring only modest storage overhead. The source code of Hermes is freely available at: https://github.com/CMU-SAFARI/Hermes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Long-latency load requests signi cantly limit the performance of high-performance out-of-order (OOO) processors. A load request that misses in the on-chip cache hierarchy and goes to the o -chip main memory (i.e., an o -chip load) often stalls the processor core by blocking the instruction retirement from the reorder bu er (ROB), thus limiting the core's performance <ref type="bibr" target="#b87">[88,</ref><ref type="bibr" target="#b90">91,</ref><ref type="bibr" target="#b91">92]</ref>. To increase the latency tolerance of a core, computer architects primarily rely on two key techniques. First, they employ increasingly sophisticated hardware prefetchers that can learn complex memory address patterns and fetch data required by future load requests before the core demands them <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b74">75]</ref>. Second, they signi cantly scale up the size of the on-chip cache hierarchy with each new generation of processors <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>Key problem. Despite recent advances in processor core design, we observe two key trends in new processor designs that leave a signi cant opportunity for performance improvement on the table. First, even a sophisticated state-of-the-art prefetcher can only predict half of the long-latency o -chip load requests on average across a wide range of workloads (see ?2). This is because even the most sophisticated prefetchers cannot easily learn the irregular access patterns in programs. Second, a large fraction of the latency of an o -chip load request is spent on accessing the multi-level on-chip cache hierarchy. This is primarily due to the increasing size of the on-chip caches <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>. To cater to workloads with ever increasing data footprints, on-chip caches in recent processors are growing in size and complexity <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b115">116]</ref>. A larger onchip cache, on the one hand, improves a core's performance by reducing the fraction of load requests that go o -chip <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b98">99,</ref><ref type="bibr" target="#b121">122]</ref>. On the other hand, a larger cache comes with longer cache access latency, which increases the latency of each ochip load request <ref type="bibr" target="#b17">[18]</ref>.</p><p>Our goal in this work is to accelerate long-latency o -chip load requests by removing on-chip cache access latency from their critical path. To this end, we introduce a new technique called Hermes, whose key idea is to predict which load requests might go o -chip and start fetching their corresponding data directly from the main memory, while also concurrently accessing the cache hierarchy for such a load. <ref type="foot" target="#foot_0">1</ref> By doing so, Hermes hides the on-chip cache access latency under the shadow of the main memory access latency (as illustrated in Fig. <ref type="figure" target="#fig_8">1</ref>), thereby signi cantly reducing the overall latency of an o -chip load request. Hermes works in tandem with any hardware data prefetcher and reduces the long memory access latency of o -chip load requests that otherwise could not have been prefetched by sophisticated state-of-the-art prefetchers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L1 L2 LLC Main Memory</head><p>Main Memory</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L1 L2 LLC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Processor is stalled Latency tolerance limit of ROB</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Saved stall cycles</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Start fetching data from main memory a6ter physical address is available</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hermes</head><p>Predict whether the load will go o=f-chip Key challenge. Although Hermes can potentially improve performance by removing the on-chip cache access latency from the critical path of a correctly-predicted o -chip load request, its performance gain signi cantly depends on how accurately it can identify the o -chip load requests. This is because a false-positive o -chip prediction (i.e., a load request that is predicted to go o -chip but hits in the cache) generates an unnecessary main memory request, incurring additional main memory bandwidth and latency overheads, which can easily diminish the performance bene t gained by the load latency reduction.</p><p>We identify two key challenges in designing an accurate o -chip load prediction mechanism. First, in a system with state-of-the-art high-performance prefetchers, only 1 out of 20 load requests generated by a program on average eventually goes o -chip (see ?3.2). Such a small fraction of o -chip loads makes it di cult for an o -chip load predictor to accurately and robustly learn from program behavior to produce highlyaccurate predictions. Second, the accuracy of the o -chip prediction of a load can change in the presence of sophisticated prefetching techniques, making it even harder for an o -chip load predictor to learn from both the program's and the prefetcher's behavior.</p><p>Limitations of prior works. Several prior works <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b103">104,</ref><ref type="bibr" target="#b125">126]</ref> propose predicting the cache level that would serve a given load request to enable various performance optimizations (e.g., better instruction scheduling). However, most of these works su er from two key limitations that make them unsuitable for o -chip load prediction. First, prior predictors su er from low prediction accuracy (i.e., the fraction of predicted o -chip load requests that actually went o -chip) <ref type="bibr" target="#b83">[84,</ref><ref type="bibr" target="#b125">126]</ref>, which increases the bandwidth overhead in main memory. As a result, they often lose the performance bene t gained by the load latency reduction and might lower performance than the baseline system (see ?8.1.1 and ?8.2.2). Second, prior o -chip load prediction mechanisms often incur impractical metadata overhead (e.g., an operating-system-managed metadata storage inside the physical main memory <ref type="bibr" target="#b59">[60]</ref>, extending each TLB and cache entry with additional metadata for tracking cache residence and coherence of data <ref type="bibr" target="#b103">[104,</ref><ref type="bibr" target="#b104">105]</ref>), which hinders adoption in commercial processors.</p><p>Key mechanism. To enable Hermes, we introduce a new lightweight perceptron-based o -chip predictor, called POPET, that learns to identify o -chip load requests using multiple program features (e.g., sequence of program counters, byte o set of a load request). For every load generated by the processor, POPET observes a set of program features to predict whether or not the load would go o -chip. If the load is predicted to go o -chip, Hermes issues a speculative load request (called a Hermes request) directly to the main memory controller once the load's physical address is generated. This Hermes request is serviced by the main memory controller concurrently with the regular load request (i.e., the load issued by the processor that generated the Hermes request) that accesses the on-chip cache hierarchy. If the prediction is correct, the regular load request eventually misses the cache hierarchy and waits for the ongoing Hermes request to nish, and thus Hermes completely hides the on-chip cache hierarchy access latency from the critical path of a correctly-predicted o -chip load.</p><p>Results summary. We evaluate Hermes with a diverse set of 110 single-core and 220 multi-core workloads spanning SPEC CPU2006 <ref type="bibr" target="#b21">[22]</ref>, SPEC CPU2017 <ref type="bibr" target="#b22">[23]</ref>, PARSEC <ref type="bibr" target="#b19">[20]</ref>, Ligra <ref type="bibr" target="#b107">[108]</ref> graph processing workloads, and commercial workloads <ref type="bibr" target="#b20">[21]</ref>. Our evaluation yields ve key results that demonstrate Hermes's e ectiveness. First, POPET achieves on average 77.1% accuracy and 74.3% coverage (i.e., the fraction of o -chip load requests of a workload that are successfully predicted), both of which are signi cantly higher (1.6? higher accuracy, 3.3? higher coverage) than that of the prior best-accuracy o -chip predictor, HMP <ref type="bibr" target="#b125">[126]</ref>. Second, Hermes improves performance on average by (up to) 5.4% (23.4%), 5.1% (25.7%), and 6.2% (32.2%) in single-core, eight-core, and bandwidth-constrained system con guration, on top of the best-performing state-of-the-art data prefetcher Pythia <ref type="bibr" target="#b31">[32]</ref>. Third, Hermes consistently improves performance when combined with any baseline hardware data prefetcher. When implemented combined with four recently-proposed highperformance prefetchers (SPP <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b74">75]</ref>, Bingo <ref type="bibr" target="#b27">[28]</ref>, MLOP <ref type="bibr" target="#b105">[106]</ref>, and SMS <ref type="bibr" target="#b109">[110]</ref>) in single-core system, Hermes improves performance on average by (up to) 5.1% (27%), 6.2% (22.4%), 7.6% (26.7%), and 7.7% (25.7%). Fourth, Hermes provides better performance-to-overhead bene t than traditional prefetchers due to its highly-accurate o -chip predictions. For every 1% performance increase, Hermes increases the main memory requests by only 0.5%, whereas Pythia increases them by 2%. Fifth, all of Hermes's bene ts come at a very modest storage overhead of only 4 KB per core, while the state-of-the-art prefetcher Pythia consumes 25.5 KB per core.</p><p>We make the following contributions in this paper: ? We identify two key opportunities for performance improvement in modern processors: (1) a signi cant fraction of the load requests continues to go o -chip even in the presence of sophisticated data prefetchers, and (2) an increasing fraction of the o -chip load latency is spent accessing on-chip caches due to the increasing size of the on-chip cache hierarchy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Motivation</head><p>High main memory access latency continues to limit the performance of modern out-of-order (OOO) processors. A load request that misses the on-chip cache hierarchy and goes to ochip main memory often blocks instruction retirement from the reorder bu er (ROB), preventing the processor from allocating new instructions into the ROB <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b87">88,</ref><ref type="bibr" target="#b90">91,</ref><ref type="bibr" target="#b91">92]</ref>, limiting performance.</p><p>To tolerate long memory latency, recent high-performance OOO cores have primarily relied on two key techniques. First, modern cores have signi cantly scaled up their on-chip cache size (e.g., each Intel Alder Lake core <ref type="bibr" target="#b9">[10]</ref> employs 4.3MB onchip cache (including L1, L2 and a per-core last-level cache (LLC) slice), which is 1.88? larger than the on-chip cache in the previous-generation Skylake core <ref type="bibr" target="#b3">[4]</ref>). Second, modern cores employ increasingly sophisticated hardware prefetchers <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16]</ref> that can more e ectively predict the addresses of load requests in advance and fetch their corresponding data to onchip caches before the program demands it, thereby completely or partially hiding the long o -chip load latency for a fraction of o -chip loads <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35]</ref>.</p><p>Despite these advances, we observe two key trends in processor design that leave a signi cant performance improvement opportunity on the table : (1) a large fraction of load requests continues to go o -chip even in the presence of state-of-the-art prefetchers, and (2) an increasing fraction of the latency of an o -chip load request is spent accessing the increasingly larger on-chip caches.</p><p>A large fraction of loads is still uncovered by state-ofthe-art prefetchers. Over the past decades, researchers have proposed many hardware prefetching techniques that have consistently pushed the limits of performance improvement (e.g., <ref type="bibr">[26, 32, 33, 35, 37, 45, 46, 56, 70, 75-77, 79, 85, 94, 98, 106, 107, 110, 111]</ref>). We observe that state-of-the-art prefetchers provide a large performance gain by accurately predicting future load addresses. Yet, a large fraction of o -chip load requests cannot be predicted even by the most advanced prefetchers. These uncovered requests limit the processor's performance by blocking instruction retirement in the ROB. Fig. <ref type="figure" target="#fig_1">2</ref> shows a stacked graph of total number o -chip load requests in a no-prefetching system and a system with the recently-proposed hardware data prefetcher Pythia <ref type="bibr" target="#b31">[32]</ref>, normalized to the no-prefetching system, across 110 workload traces categorized into ve workload categories. <ref type="foot" target="#foot_1">2</ref> Each bar further categorizes load requests into two classes: loads that block instruction retirement from the ROB (called blocking) and loads that do not (called non-blocking). ?7 discusses our evaluation methodology. The distribution of ROB-blocking and non-blocking load requests (on the left y-axis), and LLC misses per kilo instructions (on the right y-axis) in the absence and presence of a state-of-the-art hardware data prefetcher <ref type="bibr" target="#b31">[32]</ref>.</p><p>We make two key observations from Fig. <ref type="figure" target="#fig_1">2</ref>. First, on average, Pythia accurately prefetches nearly half of all o -chip load requests in the no-prefetching system, thereby improving the overall performance (not shown here; see ?8.2.1). Second, the remaining half of the o -chip loads are not prefetched even by a sophisticated prefetcher like Pythia. 71.4% of these nonprefetched o -chip loads block instruction retirement from the ROB, signi cantly limiting performance. We conclude that, state-of-the-art prefetchers, while e ective at improving performance, still leave a signi cant performance improvement opportunity on the table.</p><p>An increasing fraction of o -chip load latency is spent accessing the on-chip cache hierarchy. We observe that the on-chip cache hierarchy has not only grown tremendously in size but also in design complexity (e.g., sliced last-level cache organization <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b73">74]</ref>) in recent processors, in order to cater to workloads with large data footprints. A larger on-chip cache hierarchy, on the one hand, improves a core's performance by preventing more load requests from going ochip. On the other hand, all on-chip caches need to be accessed to determine if a load request should be sent o -chip. As a result, on-chip cache access latency signi cantly contributes to the total latency of an o -chip load. With increasing on-chip cache sizes, and the complexity of the cache hierarchy design and the on-chip network <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b115">116]</ref>, the on-chip cache access latency is increasing in processors <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18]</ref>. An analysis of the Intel Alder Lake core suggests that the load-to-use latency of an LLC access has increased to 14 ns (which is equivalent to 55 cycles for a core running at 4 GHz) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>To demonstrate the e ect of long on-chip cache access la-tency on the total latency of an o -chip load, Fig. <ref type="figure" target="#fig_2">3</ref> plots the average number of cycles a core stalls due to an o -chip load blocking any instruction from retiring from the ROB, averaged across each workload category in our baseline system with Pythia. Each bar further shows the average number of cycles an o -chip load spends for accessing the on-chip cache hierarchy. Our simulation con guration faithfully models an Intel Alder Lake performance-core with a large ROB, large on-chip caches and publicly-reported cache access latencies (see ?7). As Fig. <ref type="figure" target="#fig_2">3</ref> shows, an o -chip load stalls the core for an average of 147.1 cycles. 40.1% of these stall cycles (i.e., 58.9 cycles) can be completely eliminated by removing the on-chip cache access latency from the o -chip load's critical path. We conclude that a large and complex on-chip cache hierarchy is directly responsible for a large fraction of the overall stall cycles caused by an o -chip load request. We envision that this problem will only get exacerbated with new processor designs as on-chip caches continue to grow in size and complexity <ref type="bibr" target="#b17">[18]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Goal and Key Idea</head><p>Our goal is to improve processor performance by removing the on-chip cache access latency from the critical path of ochip load requests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The Key Idea and Potential Bene ts</head><p>To this end, we propose a new technique called Hermes, whose key idea is to predict which load requests might go o -chip and start fetching their corresponding data directly from the main memory, while also concurrently accessing the cache hierarchy for such a load. To understand the potential performance bene ts of Hermes, we model an Ideal Hermes system in simulation where we reduce the main memory access latency of every o -chip load request by the post-L1 on-chip cache hierarchy access latency (which includes L2 and LLC access, and interconnect latency). In other words, in the Ideal Hermes system, we (1) magically and perfectly know if a load request would go o -chip after its physical address is available (i.e., after the translation lookaside bu er access, which happens in parallel with the L1 data cache access in modern processors <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b94">95,</ref><ref type="bibr" target="#b120">121]</ref>), and (2) directly access the o -chip main memory for such a load, eliminating the non-L1-cache related on-chip cache hierarchy access latency from such a load's total latency. Fig. <ref type="figure" target="#fig_3">4</ref>(a) shows the speedup of Ideal Hermes by itself and when combined with Pythia normalized to the no-prefetching system in single-core workloads. We make two key observations from Fig. <ref type="figure" target="#fig_3">4</ref>(a). First, Ideal Hermes combined with Pythia outperforms Pythia alone by 8.3% on average across all workloads. Second, Ideal Hermes by itself provides nearly 80% of the performance improvement that Pythia provides. Fig. <ref type="figure" target="#fig_3">4</ref>(b) shows the speedup of Ideal Hermes when combined with four other recently-proposed high-performance prefetchers: Bingo <ref type="bibr" target="#b27">[28]</ref>, SPP <ref type="bibr" target="#b74">[75]</ref> (with perceptron lter <ref type="bibr" target="#b34">[35]</ref>), MLOP <ref type="bibr" target="#b105">[106]</ref>, and SMS <ref type="bibr" target="#b109">[110]</ref>. Ideal Hermes improves performance by 9.4%, 8.2%, 10.9%, and 13.3% on top of four state-of-the-art prefetchers Bingo, SPP, MLOP, and SMS, respectively. Based on these results, we conclude that Hermes has high potential performance bene t not only when implemented alone but also when combined with a wide variety of high-performance prefetchers.  Ideal Hermes when combined with four recently-proposed prefetchers: Bingo <ref type="bibr" target="#b27">[28]</ref>, SPP <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b74">75]</ref>, MLOP <ref type="bibr" target="#b105">[106]</ref>, and SMS <ref type="bibr" target="#b109">[110]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Key Challenge</head><p>Even though Hermes has a signi cant potential to improve performance, Hermes's performance gain heavily depends on the accuracy (i.e., the fraction of predicted o -chip loads that actually go o -chip) and the coverage (i.e., the fraction of ochip loads that are successfully predicted) of the o -chip load prediction. A low-accuracy o -chip load predictor generates useless main memory requests, which incur both latency and bandwidth overheads, and causes interference to the useful requests in the main memory. A low-coverage predictor loses opportunity to improve performance. We identify two key challenges in designing an o -chip load predictor with high accuracy and high coverage. First, only a small fraction of the total loads generated by a workload goes o -chip in presence of a sophisticated data prefetcher. As shown in Figure <ref type="figure" target="#fig_4">5</ref>, on average 7.9 loads per kilo instructions miss the LLC and go o -chip in our baseline system with Pythia.</p><p>However, these loads constitute only 5.1% of the total loads generated by a workload. This small fraction of o -chip loads makes it di cult for an o -chip load predictor to accurately learn from the workload behavior to produce highly-accurate predictions. Second, the o -chip predictability of a workload can change in the presence of modern sophisticated data prefetchers. This is because in the presence of a sophisticated prefetcher, the likelihood of a load request going o -chip not only depends on the program behavior but also on the prefetcher's ability to successfully prefetch for the load.</p><p>In this work, we overcome these two key challenges by designing a new o -chip load prediction technique, called POPET, based on perceptron learning <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b102">103]</ref>. By learning to identify o -chip loads using multiple program features (e.g., sequence of program counters, byte o set of a load request, page number of the load address), POPET provides both higher accuracy and coverage than a prior cache hit-miss prediction technique <ref type="bibr" target="#b125">[126]</ref> and higher accuracy than another o -chip load prediction technique that we develop (see ?7.2), in the presence of modern sophisticated prefetchers, without requiring large metadata storage overhead. With small changes to the existing on-chip datapath design, we demonstrate that Hermes with POPET signi cantly outperforms the baseline system with a state-of-the-art prefetcher across a wide range of workloads and system con gurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Key Related Works</head><p>The key idea of load hit-miss prediction (HMP) was proposed in <ref type="bibr" target="#b125">[126]</ref> and demonstrated as a method to improve the instruction scheduler e ciency and performance. By predicting which load instructions will miss the L1 data cache, <ref type="foot" target="#foot_2">3</ref> HMP enables the instruction scheduler to delay the scheduling of dependent instructions until the data is fetched. This scheduler optimization improves processor performance by scheduling a load-dependent instruction to execute at the time when the data in available. Even though Hermes leverages o -chip load prediction in a very di erent way than HMP, we compare the accuracy and coverage of our perceptron-based o -chip load predictor POPET, against those of HMP in ?8.1.1 and show that Hermes with POPET greatly outperforms Hermes with HMP in ?8.2.2.</p><p>Cache-level hit/miss prediction (i.e., predicting which cache level a load might hit) has also been explored in three works: Direct-to-Data cache (D2D <ref type="bibr" target="#b103">[104]</ref>), Direct-to-Master cache (D2M <ref type="bibr" target="#b104">[105]</ref>), and Level Prediction (LP <ref type="bibr" target="#b59">[60]</ref>). All three works employ di erent mechanisms to track cacheline addresses present in the cache hierarchy along with the cache-level(s) a cacheline is present in. For a given load, these works predict which cache-level the load would likely hit. If the cache-level hit/miss prediction is correct, the processor fetches the data of the correctly-predicted load by only accessing the predicted memory level (L1, L2, LLC, or the main memory) and bypassing all other memory levels. By doing so, a cache-level hit/miss predictor reduces the latency of a correctly-predicted load and improves the processor's energy e ciency. However, if the predictor incorrectly bypasses a memory level that has more up to date data (e.g., if the data is present in L2, but the predictor suggests fetching the data from LLC), the processor needs to detect such mispredictions and access the correct memory level to maintain correct execution, which comes with performance overhead and additional complexity. Hermes di ers from all these prior works in three major ways:</p><p>Prediction via tracking addresses vs. learning from program context. To make accurate cache-level prediction, D2D, D2M, and LP rely on tracking cacheline addresses present in the on-chip caches by using e ciently-managed metadata structures. For example, D2D and D2M extend each translation look-aside bu er (TLB) entry (called eTLB) to keep additional cache-level-related metadata. LP manages the cache-level metadata as an in-memory table and caches the metadata on-chip using a metadata prefetching mechanism. These metadata structures need to be updated for almost all cache operations (e.g., cache insertions, evictions) in order to faithfully track the cache contents and to provide accurate cache-level predictions. In contrast, POPET is built on the insight that the correlation between di erent types of program context information and o -chip loads can be accurately learned without tracking cache contents. As a result, POPET does not explicitly track addresses present in the cache hierarchy, but learns to predict o -chip loads by aggregating various program context information.</p><p>Lower complexity and hardware overhead. To enable accurate cache-level hit/miss prediction and to recover from a misprediction, prior works need relatively intrusive changes to multiple components on the on-chip datapath. Both D2D and D2M cache designs require extending the TLB and operating system support to store the way and cache-level information for every cacheline within the reach of eTLB. LP requires 2-bit metadata for every 64B chunk of the physical main memory. LP manages this metadata as an in-memory table in the systemreserved physical main memory space <ref type="bibr" target="#b59">[60]</ref>, which necessitates support from the operating system. In the worst case, to make a cache-level hit/miss prediction, LP needs to fetch the metadata from main memory, which can take longer than the cache lookup LP is designed to avoid. In contrast, POPET incurs a very modest (4 KB) total storage overhead and small changes to the existing datapath.</p><p>No misprediction detection and recovery. LP requires cache level misprediction detection and recovery to prevent the program from using any stale, incoherent data from the memory subsystem. On the other hand, Hermes never brings any data to the on-chip cache hierarchy unless the data is required by a LLC miss request (see ?6.2.2). Hence, Hermes does not require any misprediction detection and recovery mechanism to maintain correct execution.</p><p>We design an address tag-tracking based o -chip load predictor (called TTP; see ?7.2) inspired by these prior works <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b103">104]</ref> and evaluate it against POPET (see ?8). TTP tracks address tags present in the entire cache hierarchy in its metadata structure (see ?7.2) and predicts that a load would go o -chip if the tag of the load address is not present in its metadata structure. We open-source the implementation of TTP in our repository <ref type="bibr" target="#b12">[13]</ref>. Our results show that POPET provides both higher accuracy and higher performance than TTP (see ?8.1.1 and ?8.2.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Hermes: Design Overview</head><p>Fig. <ref type="figure" target="#fig_5">6</ref> shows a high-level overview of Hermes. POPET is the key component of Hermes that is responsible for making highlyaccurate o -chip load predictions. For every demand load request generated by the processor, POPET predicts whether or not the load request would go o -chip <ref type="bibr" target="#b0">( 1 )</ref>. If the load is predicted to go o -chip, Hermes issues a speculative memory request (called a Hermes request) directly to the main memory controller once the load's physical address is generated to start fetching the corresponding data from the main memory ( 2 ). This Hermes request is serviced by the main memory controller concurrently with the regular load request (i.e., the load issued by the processor that generated the Hermes request) that accesses the on-chip cache hierarchy. If the prediction is correct, the regular load request to the same address eventually misses the LLC and waits for the ongoing Hermes request to nish, thereby completely hiding the on-chip cache hierarchy access latency from the critical path of the correctly-predicted o -chip load ( 3 ). If a Hermes request returns from the main memory but there has been no regular load request to the same address, Hermes drops the request and does not ll the data into the cache hierarchy. By doing so, Hermes keeps the on-chip cache hierarchy fully coherent even in case of a misprediction. For every regular load request returning to the core, Hermes trains POPET based on whether or not this load has actually gone o -chip ( 4 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Hermes: Detailed Design</head><p>We rst describe the design of POPET in ?6.1, followed by the changes introduced by Hermes to the on-chip cache access datapath in ?6.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">POPET Design</head><p>The purpose of POPET is to accurately predict whether or not a load request generated by the processor will go o -chip. We design POPET using the multi-feature perceptron learning mechanism <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b102">103,</ref><ref type="bibr" target="#b112">113]</ref>. What is perceptron learning? Perceptron learning, whose roots are in <ref type="bibr" target="#b81">[82]</ref> and was demonstrated by Rosenblatt <ref type="bibr" target="#b102">[103]</ref>, is a simpli ed learning model to mimic biological neurons. Fig. <ref type="figure" target="#fig_7">7</ref> shows a single-layer perceptron network where each input is connected to the output via an arti cial neuron. Each arti cial neuron is represented by a numeric value, called weight. The perceptron network as a whole iteratively learns a binary classi cation function f (x) (shown in Eq. 1), a function that maps the input X (a vector of n values) to a binary output.</p><formula xml:id="formula_0">f (x) = 1 if w 0 + n i=1 w i x i &gt; 0 0 otherwise (1)</formula><p>The perceptron learning algorithm starts by initializing the weight of each neuron and iteratively trains the weights using each input vector from the training dataset in two steps. First, for an input vector X, the perceptron network computes a binary output using Eq. 1 and the current weight values of its neurons. Second, if the computed output di ers from the desired output for that input vector provided by the dataset, the weight of each neuron is updated <ref type="bibr" target="#b102">[103]</ref>. This iterative process is repeated until the error between the computed and desired output falls below a user-speci ed threshold.  Jimenez et al. <ref type="bibr" target="#b63">[64]</ref> have applied the perceptron learning algorithm to design a lightweight, high-performance branch predictor in a processor core. Today, multiple commercial processors also use perceptron learning for making various microarchitectural predictions (e.g., Samsung Exynos <ref type="bibr" target="#b48">[49]</ref>, and AMD Ryzen <ref type="bibr" target="#b4">[5]</ref>).</p><p>We design POPET using an existing microarchitectural perceptron model, known as the hashed perceptron <ref type="bibr" target="#b111">[112]</ref>. A hashed perceptron model hashes multiple feature values to retrieve weights of each feature from small tables. If the sum of these weights exceeds a threshold, the model makes a positive prediction. Hashed perceptron, as compared to other perceptron models, is lightweight and easy to implement in hardware. Prior works successfully apply hashed perceptron for various microarchitectural predictions, e.g., branch outcome <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b63">64]</ref>, LLC reuse <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b112">113]</ref>, prefetch usefulness <ref type="bibr" target="#b34">[35]</ref>. This is the rst work that applies hashed perceptron to o -chip load prediction.</p><p>Why perceptron? We choose to design POPET based on perceptron learning for two key reasons. First, by learning using multiple program features, perceptron learning can provide highly accurate predictions that could not be otherwise provided by simple history-based learning prediction (e.g., HMP <ref type="bibr" target="#b125">[126]</ref>, described in ?4). Second, perceptron learning can be implemented with low storage overhead, without requiring any impractical metadata support (e.g., extending TLB <ref type="bibr" target="#b103">[104,</ref><ref type="bibr" target="#b104">105]</ref> or in-memory metadata storage <ref type="bibr" target="#b59">[60]</ref>, described in ?4).</p><p>POPET overview. POPET is organized as a collection of one-dimensional tables (each called a weight table ), where each table corresponds to a single program feature. Each table entry stores a weight value, implemented using a 5-bit saturating signed integer, that represents the correlation between the corresponding program feature value and the true outcome (i.e., whether a given load actually went o -chip). A weight value saturated near the maximum (i.e., +15) or the minimum (i.e., -16) value represents a strong positive or negative correlation between the program feature value and the true outcome, respectively. A weight value closer to zero signi es a weak correlation. The weights are adjusted during training (step 4 in Fig. <ref type="figure" target="#fig_5">6</ref>) to update POPET's prediction with the true outcome. Each weight table is sized di erently based on its corresponding program feature (see Table <ref type="table" target="#tab_5">3</ref>).</p><p>6.1.1. Making a Prediction. During load queue (LQ) allocation for a load generated by the core (step 1 in Fig. <ref type="figure" target="#fig_5">6</ref>), POPET makes a binary prediction on whether or not the load request would go o -chip. The prediction happens in three stages as shown in Fig. <ref type="figure" target="#fig_9">8</ref>. In the rst stage, POPET extracts a set of program features from the current load request and a history of prior requests ( ?6.1.3 shows the list of program features used by POPET). In the second stage, each feature value is hashed and used as an index to retrieve a weight value from the weight table of the corresponding feature. In the third stage, all weight values from individual features are accumulated to generate the cumulative perceptron weight (W ? ). If W ? exceeds a predened threshold (called the activation threshold, ? act ), POPET makes a positive prediction (i.e., it predicts that the current load request would go o -chip). Otherwise, POPET makes a negative prediction. The hashed feature values, the cumulative perceptron weight W ? , and the predicted outcome are stored in the LQ entry to be reused to train POPET when the load request returns to the processor core (step 4 in Fig. <ref type="figure" target="#fig_5">6</ref>).    list of feature sets, each containing n features, at every iteration n in the following way. In the rst iteration, we design POPET with each of the 16 initial program features and test its prediction accuracy in 10 randomly-selected workload traces (called testing workloads). We select the top-10 features that produce the highest prediction accuracy for the second iteration. In the second iteration, we create 160 two-combination feature sets (meaning, each feature set contains two initial features from Table <ref type="table" target="#tab_3">1</ref>) by combining each of the 16 initial features with each of the 10 winning feature sets from the last iteration, and test the prediction accuracy on the testing workloads. We select the top-10 two-combination feature sets that produce the highest prediction accuracy for the third iteration. This iterative process repeats until the maximum prediction accuracy gets saturated (i.e., the di erence in accuracy of two successive iterations is less than 3%). <ref type="foot" target="#foot_3">4</ref> Table <ref type="table" target="#tab_4">2</ref> shows the nal list of program features selected by the automated feature selection process. Rationale for selected features. Each selected feature correlates with the likelihood of observing an o -chip load request with a di erent program context information. We explain the rationale for each selected feature below.</p><p>(1) PC ? cacheline o set. This feature is computed by XOR-ing the load PC value with the cacheline o set of the load address in the virtual page of the load request. The goal of this feature is to learn the likelihood of a load request going o -chip when a given load PC touches a certain cacheline o set in a virtual page. The use of cacheline o set information, instead of load virtual address or virtual page number, enables this feature to apply the learning across di erent virtual pages.</p><p>(2) PC ? cacheline byte o set. This feature is computed by XOR-ing the load PC with the byte o set of the load cacheline address. This feature is particularly useful in accurately predicting o -chip load requests when a program has a streaming access pattern over a linearly allocated data structure. For example, when a program streams through a large array of 4B integers, every 16 th load (as a 64B cacheline stores 16 integers) generated by a load PC that is iterating over the array will go o -chip, and the remaining loads will hit in on-chip caches. In this case, this feature learns to identify only those loads that have a byte o set of 0 to go o -chip.</p><p>(3) PC + rst access. This feature is computed by leftshifting the load PC and adding the rst access hint at the most-signi cant bit position. The rst access hint is a binary value that represents whether or not a cacheline has been recently touched by the program. The hint is computed using a small 64-entry bu er (called the page bu er) that tracks the demanded cachelines from last 64 virtual pages. Each page bu er entry holds two pieces of information: a virtual page tag, and a 64-bit bitmap, where each bit represents one cacheline in the virtual page. During every load request generation, POPET searches the page bu er with the virtual page number of the load address. If a matching entry is found, POPET uses the value of the bit corresponding to the cacheline o set in the matching page bu er entry's bitmap as the rst access hint. If the bit is set (or unset), it signi es that the corresponding cacheline has (not) been recently accessed by the program. If the bit is unset, POPET sets the bit in the page bu er entry's bitmap. The rst access hint provides a crude estimate of a cacheline's reuse in a short temporal window. However, it alone cannot determine the cacheline's residency in on-chip caches, as the memory footprint tracked by the page bu er is much smaller than the total cache size.</p><p>(4) Cacheline o set + rst access. This feature is similar to the PC + rst access feature, except that it learns the likelihood of a load request going o -chip when a given cacheline o set is recently touched by the program.</p><p>(5) Last-4 load PCs. This feature value is computed as a shifted-XOR of last four load PCs. It represents the execution path of a program and correlates it with the likelihood of observing an o -chip load request whenever the program follows the same execution path.</p><p>6.1.4. Parameter Threshold Tuning. POPET has three tunable parameters: negative and positive training thresholds (T N and T P , respectively), and the activation threshold (? act ). Properly tuning the values of all these three parameters is also critical to POPET's performance, since both POPET's accuracy and coverage are sensitive to parameter values.</p><p>We employ a three-step grid search technique to tune each of the three parameters separately. In the rst stage, we uniformly sample values from a parameter's range. For example, ? act can take values in the range [-80, 75]. <ref type="foot" target="#foot_4">5</ref> We uniformly sample values from this range with a grid size of 5. In the second stage, we run Hermes with the randomly-selected 10 test workloads (as mentioned in ?6.1.3) for each of the sampled values and pick the top-10 values that provide the highest performance gain. In the third stage, we run Hermes with all single-core workload traces using the selected 10 parameter values from the second stage. We nally select the value that provides the highest average performance gain. Table <ref type="table" target="#tab_4">2</ref> shows the selected threshold values of each parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Hermes Datapath Design</head><p>In this section, we describe the key changes introduced to the existing well-optimized on-chip cache access datapath to incorporate Hermes. First, we show how the core issues a Hermes request directly to the main memory controller if POPET predicts the load would go o -chip and how a regular load request that misses the LLC waits for an ongoing Hermes request (see ? 6.2.1). Second, we discuss how the data fetched from main memory is properly sent back to the core in presence of Hermes while maintaining cache coherence (see ?6.2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1.">Issuing a Hermes Request.</head><p>For every load request predicted to go o -chip, Hermes issues a Hermes request directly to the main memory controller (step 2 in Fig. <ref type="figure" target="#fig_5">6</ref>) once the load's physical address is generated. The main memory controller enqueues the Hermes request in its read queue (RQ) and starts fetching the corresponding data from the main memory as dictated by its scheduling policy, while the regular load request is concurrently accessing the on-chip cache hierarchy. If the o -chip prediction is correct, the regular load request eventually misses the LLC and checks the main memory controller's RQ for any ongoing main memory access to the same load address (step 3 ). If the address is found, the regular load request waits for the ongoing Hermes request to nish before sending the Hermes-fetched data back to the core.</p><p>Hermes's performance gain depends on the latency to directly issue a Hermes request to the main memory controller (called Hermes request issue latency). Although a Hermes request experiences a signi cantly shorter latency to arrive at the main memory controller than its corresponding regular load request because a Hermes request bypasses the cache hierarchy and on-chip queueing delays, a Hermes request nonetheless pays for a latency to route through the on-chip network. We model two variants of Hermes using an optimistic and a pessimistic estimate of Hermes request issue latency to take into account a wide range of potential di erences in on-chip interconnect designs (see ?7.2). In ?8.4.3, we also evaluate Hermes with a wide range of Hermes request issue latencies (from 0 cycle to 24 cycles) and show that Hermes consistently provides performance bene t even with the most pessimistic Hermes request issue latency. 6.2.2. Returning Data to the Core. For every Hermes request returning from main memory, Hermes checks the RQ of the main memory controller and returns the fetched data back to the LLC if there is a regular load request already waiting for the same load address. If there is no regular load request waiting for the completed Hermes request, Hermes drops the request and does not ll the data into the cache hierarchy, which keeps the on-chip cache hierarchy internally coherent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Storage Overhead</head><p>Table <ref type="table" target="#tab_5">3</ref> shows the total storage overhead of Hermes. Hermes requires only 4 KB of metadata storage per processor core. POPET consumes 3.2 KB, whereas the metadata stored in LQ for POPET training consumes 0.8 KB. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Methodology</head><p>We use the ChampSim trace-driven simulator <ref type="bibr" target="#b8">[9]</ref> to evaluate Hermes. We faithfully model the latest-generation Intel Alder Lake performance-core <ref type="bibr" target="#b10">[11]</ref> with its large ROB, large caches with publicly-reported on-chip cache access latencies <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>, and the state-of-the-art prefetcher Pythia <ref type="bibr" target="#b31">[32]</ref> at the LLC. Table <ref type="table" target="#tab_6">4</ref> shows the key microarchitectural parameters. For single-core simulations, we warm up the core using 100M instructions and simulate the next 500M instructions. For multiprogrammed simulations, we use 50M and 100M instructions from each workload for warmup and simulation, respectively. If a core nishes early, the workload is replayed until every core has nished executing at least 100M instructions. The source code of Hermes, along with all workload traces and scripts to reproduce our results are freely available at <ref type="bibr" target="#b12">[13]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Workloads</head><p>We evaluate Hermes using a wide range of memory-intensive workloads spanning SPEC CPU2006 <ref type="bibr" target="#b21">[22]</ref>, SPEC CPU2017 <ref type="bibr" target="#b22">[23]</ref>, PARSEC <ref type="bibr" target="#b19">[20]</ref>, Ligra graph processing workload suite <ref type="bibr" target="#b107">[108]</ref>, and commercial workloads from the 2nd data value prediction championship (CVP <ref type="bibr" target="#b20">[21]</ref>). For SPEC CPU2006 and SPEC CPU2017 workloads, we reuse the instruction traces provided by the 2nd and the 3rd data prefetching championships (DPC <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>). For PARSEC and Ligra workloads, we reuse the instruction traces open-sourced by Pythia <ref type="bibr" target="#b31">[32]</ref>. The CVP workload traces are collected by the Qualcomm Datacenter Technologies and capture complex program behavior from various integer, oating-point, cryptographic, and server applications in the eld. We only consider workload traces in our evaluation that have at least 3 LLC misses per kilo instructions (MPKI) in the no-prefetching system. In total, we evaluate Hermes using 110 single-core workload traces from 73 workloads, which are summarized in Table <ref type="table" target="#tab_7">5</ref>. All these traces can be freely downloaded using a script as mentioned in Appendix A.5. For multiprogrammed simulations, we create both homogeneous and heterogeneous trace mixes. For an eight-core homogeneous multi-programmed simulation, we run eight copies of each trace from our single-core trace list, one trace in each core. For heterogeneous multi-programmed simulation, we randomly select any eight traces from our single-core trace list and run one trace in each core. In total, we evaluate Hermes using 110 homogeneous and 110 heterogeneous eight-core workloads. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Evaluated System Con gurations</head><p>For a comprehensive analysis, we compare Hermes with various o -chip load prediction mechanisms, as well as in combination with various recently proposed prefetchers. Table <ref type="table" target="#tab_8">6</ref> compares the storage overhead of all evaluated mechanisms.</p><p>(1) Various o -chip prediction mechanisms. We compare POPET against two cache hit/miss prediction techniques: (1) HMP, proposed by Yoaz et al. <ref type="bibr" target="#b125">[126]</ref>, and (2) a simple cacheline tag-tracking based predictor, called TTP, which we design. HMP uses three predictors similar to a hybrid branch predictor: local <ref type="bibr" target="#b124">[125]</ref>, gshare <ref type="bibr" target="#b82">[83,</ref><ref type="bibr" target="#b124">125]</ref>, and gskew <ref type="bibr" target="#b85">[86]</ref>, each of which individually predicts o -chip loads using a di erent prediction mechanism. For a given load, HMP consults each individual predictor and selects the majority prediction. We design TTP by taking inspiration from prior cacheline address trackingbased mechanisms <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b103">104]</ref> (see ?4). TTP tracks partial tags of cacheline addresses that are likely to be present in the entire on-chip cache hierarchy in a separate metadata structure. For every cache ll (LLC eviction), the partial tag of the lled (evicted) cacheline address is inserted into (evicted from) TTP's metadata. To predict whether or not a given load would go o -chip, TTP searches the metadata structure with the partial tag of the load address. If the tag is not present in the metadata structure, TTP predicts the load would go o -chip. We open-source TTP in our repository <ref type="bibr" target="#b12">[13]</ref>.</p><p>(2) Various data prefetchers. We evaluate Hermes combined with ve recently-proposed high-performance prefetch-ing techniques: Pythia <ref type="bibr" target="#b31">[32]</ref>, Bingo <ref type="bibr" target="#b27">[28]</ref>, SPP <ref type="bibr" target="#b74">[75]</ref> (with perceptron lter <ref type="bibr" target="#b34">[35]</ref>), MLOP <ref type="bibr" target="#b105">[106]</ref>, and SMS <ref type="bibr" target="#b109">[110]</ref>. As mentioned in Table <ref type="table" target="#tab_6">4</ref>, Pythia is incorporated in our baseline system. Pythia <ref type="bibr" target="#b31">[32]</ref> with the same con guration in <ref type="bibr" target="#b31">[32]</ref> 25.5 KB Bingo <ref type="bibr" target="#b27">[28]</ref> with the same con guration in <ref type="bibr" target="#b27">[28]</ref> 46 KB SPP <ref type="bibr" target="#b74">[75]</ref> with perceptron-based prefetch lter <ref type="bibr" target="#b34">[35]</ref> 39.3 KB MLOP <ref type="bibr" target="#b105">[106]</ref> with the same con guration in <ref type="bibr" target="#b105">[106]</ref> 8 KB SMS <ref type="bibr" target="#b109">[110]</ref> with the same con guration in <ref type="bibr" target="#b109">[110]</ref> 20 KB</p><p>Hermes with POPET (this work)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">KB</head><p>We evaluate two variants of Hermes: Hermes-O and Hermes-P. These two variants di er only in Hermes request issue latency. Hermes-O (i.e., the optimistic Hermes) and Hermes-P (i.e., the pessimistic Hermes) use a request issue latency of 6 cycles and 18 cycles, respectively. Unless stated otherwise, Hermes represents the optimistic variant Hermes-O.  <ref type="figure" target="#fig_11">9</ref> shows the comparison of POPET's o -chip load prediction accuracy and coverage against those of HMP and TTP in the baseline system. The key takeaway is that POPET has signi cantly higher accuracy and coverage than HMP. POPET provides 77.1% accuracy with 74.3% coverage on average across all single-core workloads, whereas HMP provides 47% accuracy with 22.3% coverage. TTP, with a metadata budget of 1.5 MB, provides the highest coverage (94.8%) but with a signi cantly lower accuracy (16.6%). POPET's superior accuracy and coverage directly translates to performance bene ts both in single-core and eight-core system con guration (see ?8.2 and ?8.3). 8.1.2. E ect of Di erent POPET Features. Fig. <ref type="figure" target="#fig_12">10</ref> shows the accuracy and coverage of POPET using the ve selected program features used individually and in various combinations. We make two key observations. First, each program feature individually produces predictions with a wide range of accuracy and coverage. The PC ? cacheline o set feature produces the lowest-quality predictions with only 53.4% accuracy and 14.5% coverage, whereas the cacheline o set + rst access feature produces the highest-quality predictions with 70.6% accuracy and 48.1% coverage. Second, by stacking multiple features together, the nal POPET design achieves both higher accuracy and coverage than those provided by any single individual program feature. We conclude that POPET is capable of learning from multiple program features to achieve both higher o -chip load prediction accuracy and coverage than any individual program feature can provide. , where no single program feature individually provides the highest coverage across all workloads. This large variability of accuracy/coverage with di erent features in di erent workloads warrants learning using all features in unison to provide higher accuracy and coverage than any individual program feature across a wide range of workloads.  8.2. Single-core Performance Analysis 8.2.1. Performance Improvement. Fig. <ref type="figure" target="#fig_14">12</ref> shows performance of Hermes (O and P), Pythia, and Hermes combined with Pythia normalized to the no-prefetching system in single-core workloads. We make three key observations. First, Hermes provides nearly half of the performance bene t of Pythia with only 1 5 ? the storage overhead. On average, Hermes-O improves performance by 11.5% over a no-prefetching system, whereas Pythia improves performance by 20.3%. Second, Hermes-O (Hermes-P) combined with Pythia outperforms Pythia by 5.4% (4.3%). Third, Hermes combined with Pythia consistently outperforms Pythia in every workload category. To better understand Hermes's performance improvement, Fig. <ref type="figure" target="#fig_10">13</ref> shows the performance line graph of Hermes, Pythia, and Hermes combined with Pythia for every single-core workload trace. The traces are sorted in ascending order of performance gains by Hermes combined with Pythia over the no-prefetching system. We make four key observations from Fig. <ref type="figure" target="#fig_10">13</ref>. First, Hermes combined with Pythia outperforms the no-prefetching system in all but three single-core workload traces. The compute _ int _ 539 and 605.mcf _ s-782B traces experience the highest and the lowest speedup (2.3? and 0.8?, respectively). Second, unlike Pythia, Hermes always improves performance over the no-prefetching system in every workload trace. Third, Hermes outperforms Pythia by 7.9% on average in 51 traces (e.g., streamcluster-6B, Ligra _ PageRank-79B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Evaluation</head><p>In the remaining 59 traces, Pythia outperforms Hermes by 26% on average. Fourth, Hermes combined with Pythia consistently outperforms both Hermes and Pythia alone in almost every workload trace.</p><p>Based on our performance results, we conclude that, Hermes provides signi cant and consistent performance improvements over a wide range of workloads both by itself and when combined with the state-of-the-art prefetcher Pythia.   <ref type="figure" target="#fig_16">14</ref> shows the performance of Hermes with POPET, Hermes-HMP, Hermes-TTP, and the Ideal Hermes (see ?3.1) combined with Pythia normalized to the no-prefetching system in singlecore workloads. We make two key observations. First, Hermes with POPET outperforms both Hermes-HMP and Hermes-TTP. On average, Hermes-HMP, Hermes-TTP, and Hermes with POPET combined with Pythia provide 0.8%, 1.7%, and 5.4% performance improvement over Pythia, respectively. Second, Hermes-POPET provides nearly 90% of the performance improvement provided by the Ideal Hermes that employs an ideal o -chip load predictor with 100% accuracy and coverage. We conclude that Hermes provides performance gains due to both the high o -chip load prediction accuracy and coverage of POPET. Thus, designing a good o -chip predictor is critical for Hermes to improve performance. 8.2.3. E ect on Stall Cycles. Fig. <ref type="figure" target="#fig_17">15</ref>(a) plots the distribution of the percentage reduction in stall cycles due to o -chip load requests in a system with Hermes over the baseline system in single-core workloads as a box-and-whiskers plot. <ref type="foot" target="#foot_5">6</ref> The key observation is that Hermes reduces the stall cycles caused by o -chip loads by 16.2% on average (up to 51.8%) across all workloads. PARSEC workloads experience the highest average stall cycle reduction of 23.8%. 90 out of 110 workloads experience at least 10% stall cycle reduction. We conclude that Hermes considerably reduces the stall cycles due to o -chip load requests, which leads to performance improvement.  shows the percentage increase in main memory requests in Hermes, Pythia, and Hermes combined with Pythia over the no-prefetching system in all single-core workloads. We make two key observations. First, Hermes increases main memory requests by only 5.5% (on average) over the no-prefetching system, whereas Pythia by 38.5%. This means that, every 1% performance gain (see Fig. <ref type="figure" target="#fig_14">12</ref>) comes at a cost of only 0.5% increase in main memory requests in Hermes, whereas nearly 2% increase in main memory requests in Pythia. We attribute this result to the highly-accurate predictions made by POPET, as compared to less-accurate prefetch decisions made by Pythia. Second, Hermes combined with Pythia further increases main memory requests by only 5.9% over Pythia. This means that, every 1% performance bene t by Hermes on top of Pythia comes at a cost of only 1% overhead in main memory requests. We conclude that, Hermes, due to its underlying high-accuracy prediction mechanism, adds considerably lower overhead in main memory requests while providing signi cant performance improvement both by itself and when combined with Pythia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.">Eight-core Performance Analysis</head><p>Fig. <ref type="figure" target="#fig_18">16</ref> shows the performance of Pythia, Hermes-HMP, Hermes-TTP, and Hermes-POPET combined with Pythia normalized to the no-prefetching system in all eight-core workloads. The key takeaway is that due to the highly-accurate predictions by POPET, Hermes-POPET combined with Pythia consistently outperforms Pythia in every workload category. On average, Hermes-HMP, Hermes-TTP, and Hermes-POPET combined with Pythia provide 0.6%, -2.1%, and 5.1% higher performance on top of Pythia, respectively. Due to its inaccurate predictions, TTP generates many unnecessary main memory requests, which reduce the performance of Hermes-TTP combined with Pythia as compared to Pythia alone in the bandwidth-constrained four-core con guration. We conclude that provides signi cant and consistent performance improvement in the bandwidth-constrained eight-core system due to its highly-accurate o -chip load prediction.  Pythia over the no-prefetching system in single-core workloads by scaling the main memory bandwidth. We make two key observations. First, Hermes combined with Pythia consistently outperforms Pythia in every main memory bandwidth con guration from 1  16 ? to 4? of the baseline system. Hermes combined with Pythia outperforms Pythia alone by 6.2% and 5.5% in the main memory bandwidth con guration  with 200 and 12800 million transfers per second (MTPS), respectively. Second, Hermes by itself outperforms Pythia in highly-bandwidth-constrained con gurations. This is due to the highly-accurate o -chip load predictions made by POPET, which incurs less main memory bandwidth overhead than the aggressive, less-accurate prefetching decisions made by Pythia. Hermes outperforms Pythia by 2.8% and 8.9% in 400 and 200 MTPS con gurations, respectively. 8.4.2. E ect of the Baseline Prefetcher. We evaluate Hermes combined with four recently-proposed data prefetchers: Bingo <ref type="bibr" target="#b27">[28]</ref>, SPP <ref type="bibr" target="#b74">[75]</ref> (with perceptron lter <ref type="bibr" target="#b34">[35]</ref>), MLOP <ref type="bibr" target="#b105">[106]</ref>, and SMS <ref type="bibr" target="#b109">[110]</ref>. For each experiment, we replace the baseline LLC prefetcher Pythia with a new prefetcher and measure the performance improvement of the prefetcher by itself and Hermes combined with the prefetcher. Fig. <ref type="bibr">17(b)</ref> shows the performance of the baseline prefetcher, and Hermes-P/O combined with the baseline prefetcher, normalized to the no-prefetching system in single-core workloads. The key takeaway is that Hermes combined with any baseline prefetcher consistently outperforms the baseline prefetcher by itself for all four evaluated prefetching techniques. Hermes+prefetcher outperforms the prefetcher alone by 6.2%, 5.1%, 7.6%, and 7.7%, for Bingo, SPP, MLOP, and SMS as the baseline prefetcher.</p><p>8.4.3. E ect of the Hermes Request Issue Latency. To analyze the performance bene t of Hermes over a wide range of processor designs with simple or complex on-chip datapath, we perform a performance sensitivity study by varying the Hermes request issue latency. Fig. <ref type="figure" target="#fig_21">17(c</ref>) shows the performance of Hermes combined with Pythia normalized to the no-prefetching system in single-core workloads as Hermes request issue latency varies from 0 cycles to 24 cycles. The dashed-line represents the performance of Pythia alone. We make two key observations. First, the speedup of Hermes combined with Pythia decreases as the Hermes request issue latency increases. Second, even with a pessimistic Hermes request issue latency of 24 cycles, Hermes combined with Pythia outperforms Pythia. Pythia+Hermes outperforms Pythia by 5.7% and 3.6% with 0-cycle and 24-cycle Hermes request issue latency, respectively. 8.4.4. E ect of the On-chip Cache Hierarchy Access Latency. We evaluate Hermes by varying the on-chip cache hierarchy access latency. For each experiment, we keep the L1 and L2 cache access latencies unchanged and vary the LLC access la-tency from 25-cycles to 50-cycles, to mimic the access latencies of a wide range of sliced LLC designs with simple or complex on-chip networks. Fig. <ref type="figure" target="#fig_21">17(d)</ref> shows the performance of Pythia, and Hermes (O and P) combined with Pythia, normalized to the no-prefetching system in single-core workloads. We make two key observations. First, Hermes combined with Pythia consistently outperforms Pythia for every on-chip cache hierarchy latency. Hermes-O combined with Pythia outperforms Pythia alone by 3.6% and 6.2% in system with 40-cycle and 65-cycle on-chip cache hierarchy access latency, respectively. Second, the performance improvement by Hermes combined with Pythia increases as the on-chip cache hierarchy access latency increases. Thus, we posit that Hermes can provide even higher performance bene t in future processors with longer on-chip cache access latencies. 8.4.5. E ect of the Activation Threshold. We evaluate the impact of the activation threshold (? act ) on Hermes's performance by varying ? act . Fig. <ref type="figure" target="#fig_20">17</ref> shows POPET's accuracy and coverage (as line graphs on the left y-axis) and the performance of Hermes combined with Pythia over the no-prefetching system (as a bar graph on the right y-axis) across all single-core workloads as ? act varies from -38 to 2. The key takeaway from Fig. <ref type="figure" target="#fig_20">17</ref> is that POPET's accuracy (coverage) increases (decreases) as ? act increases. However, Hermes's performance gain peaks near ? act = -26, which favors higher coverage by trading o accuracy. As POPET's accuracy directly impacts Hermes's main memory request overhead (and hence its performance in bandwidth-constrained con gurations), we set ? act = -18 in POPET. Doing so simultaneously optimizes both POPET's accuracy and coverage.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.5.">Power Overhead</head><p>To accurately estimate Hermes's dynamic power consumption, we model our single-core con guration in McPAT <ref type="bibr" target="#b77">[78]</ref> and compute processor power consumption using statistics from performance simulations. Fig. <ref type="figure" target="#fig_10">18</ref> shows the runtime dynamic power consumed by Hermes, Pythia, and Hermes combined with Pythia, normalized to the no-prefetching system for all single-core workloads. We make two key observations. First, Hermes increases processor power consumption by only 3.6% on average over the no-prefetching system, whereas Pythia increases power consumption by 8.7%. Second, Hermes combined with Pythia incurs only 1.5% additional power overhead on top of Pythia. We conclude that Hermes incurs only a modest power overhead and is more e cient than Pythia alone. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Other Related Work</head><p>To our knowledge, this is the rst work that proposes (1) a lightweight perceptron-based o -chip load predictor (POPET) that makes accurate predictions without having large metadata overhead, and (2) a mechanism (Hermes) that takes advantage of such a predictor to improve processor performance by eliminating on-chip cache access latency from the critical path of a correctly-predicted o -chip load. We have already quantitatively and qualitatively compared (1) POPET with HMP <ref type="bibr" target="#b125">[126]</ref> and (2) Hermes with multiple state-of-the-art hardware data prefetchers: Pythia <ref type="bibr" target="#b31">[32]</ref>, Bingo <ref type="bibr" target="#b27">[28]</ref>, SPP <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b74">75]</ref>, MLOP <ref type="bibr" target="#b105">[106]</ref>, and SMS <ref type="bibr" target="#b109">[110]</ref>. In this section, we discuss other related works.</p><p>Hit/miss prediction. Peir et al. <ref type="bibr" target="#b95">[96]</ref> propose a Bloom-lterbased hit/miss predictor to optimize instruction scheduling in an out-of-order processor. Memik et al. <ref type="bibr" target="#b83">[84]</ref> propose ve di erent heuristics to keep track of cache contents using simple tables. Prior works <ref type="bibr" target="#b80">[81,</ref><ref type="bibr" target="#b99">100]</ref> also explore hit/miss prediction in DRAM caches. Loh and Hill <ref type="bibr" target="#b80">[81]</ref> propose MissMap, which uses bitvectors to track the residency of cachelines in a large DRAM cache. Adapting MissMap to o -chip load prediction poses two key challenges. First, MissMap can have high falsepositive prediction rate (as discussed in <ref type="bibr" target="#b59">[60]</ref>). Second, the size of MissMap can grow very large, leading to large latency and storage overheads. POPET, on the other hand, requires only 4 KB of storage overhead, while producing highly-accurate o -chip load predictions.</p><p>Cache bypassing. High-performance cache management policies (e.g., <ref type="bibr">[38, 43, 48, 53, 58, 63, 65-68, 72, 73, 80, 97, 101, 102, 114, 115, 120, 122]</ref>) dynamically bypass cache levels during a cache ll operation if the incoming cacheline is not expected to be used by the program in the future. This expectation (i.e., reuse prediction) can be provided by either a hardware-based reuse prediction mechanism <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b96">97,</ref><ref type="bibr" target="#b100">101,</ref><ref type="bibr" target="#b101">102,</ref><ref type="bibr" target="#b119">120]</ref> or a software hint <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b113">114,</ref><ref type="bibr" target="#b114">115]</ref>, e.g., non-temporal load instructions employed by modern processors <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19]</ref>. The goal of these cache bypassing techniques is to better utilize the cache space by avoiding the insertion of useless cachelines into the cache. Hermes's goal is di erent and orthogonal to these techniques: to reduce the latency of a long-latency o -chip load by eliminating the on-chip cache hierarchy access latency from its critical path. As such, Hermes can be combined with any cache bypassing technique.</p><p>Data prefetching. Prior prefetching techniques can be broadly categorized into three classes: (1) precomputationbased prefetchers that pre-execute program code to generate future loads <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b86">[87]</ref><ref type="bibr" target="#b87">[88]</ref><ref type="bibr" target="#b88">[89]</ref><ref type="bibr" target="#b89">[90]</ref><ref type="bibr" target="#b90">[91]</ref><ref type="bibr" target="#b91">[92]</ref><ref type="bibr" target="#b92">[93]</ref>, (2) temporal prefetchers that predict future load addresses by memorizing long sequence of demanded cacheline addresses <ref type="bibr">[27, 31, 39-41, 44, 54, 57, 69, 71, 109, 117-119, 123, 124]</ref>, and (3) spatial prefetchers that predict future load addresses by learning program access patterns over di erent memory regions <ref type="bibr">[26, 28, 32, 33, 35, 37, 46, 56, 70, 75-77, 85, 94, 98, 106, 107, 110, 111]</ref>. As we show in ?8.4.2, Hermes can be combined with any baseline prefetcher to provide higher performance than just the prefetcher alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Conclusion</head><p>We introduce Hermes, a technique that accelerates longlatency o -chip load requests by eliminating the on-chip cache hierarchy access latency from their critical path. To enable Hermes, we propose a perceptron-learning based o -chip load predictor (POPET) that accurately predicts which load requests might go o -chip. Our extensive evaluations using a wide range of workloads and system con gurations show that Hermes provides signi cant performance bene ts over a baseline system with a state-of-the-art prefetcher. As on-chip cache hierarchy continues to grow in size and complexity in future processors, we believe and hope that Hermes's key observation and o -chip load prediction mechanism would inspire future works to explore a multitude of other memory system optimizations.   <ref type="figure" target="#fig_24">19</ref> shows the performance of Hermes, Pythia, and Hermes combined with Pythia normalized to the no-prefetching system in singlecore workloads as the size of reorder bu er (ROB) varies from 256 entries to 1024 entries. The key takeaway is that Hermes combined with Pythia outperforms Pythia alone in every ROB size con guration. Pythia+Hermes outperforms Pythia by 6.7% and 5.3% in a system with 256-entry and 1024-entry ROB.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Performance Sensitivity to LLC Size</head><p>Fig. <ref type="figure" target="#fig_24">19</ref> shows the performance of Hermes, Pythia, and Hermes combined with Pythia normalized to the no-prefetching system in singlecore workloads as the per-core last-level cache (LLC) size varies from 3 MB to 24 MB. The key takeaway is that Hermes combined with Pythia outperforms Pythia alone in every LLC size con guration. Even in a system with a 12 MB and 24 MB LLC per core, Pythia+Hermes provides 2.5% and 1.3% performance bene t over Pythia alone, respectively.   We make two key observations. First, POPET's accuracy and coverage varies widely based on the baseline data prefetcher. When combined with Pythia, Bingo, SPP, MLOP, and SMS, POPET provides accuracy of 77.3%, 78.1%, 73.4%, 79.9%, and 76.0%, while providing coverage of 74.2%, 77.6%, 65.9%, 81.7%, and 84.7%, respectively. Second, in a system without any baseline data prefetcher, POPET provides signi cantly higher accuracy (88.9%) and coverage (93.6%) than any con guration with a baseline prefetcher. This shows that, the prefetch requests generated by a sophisticated data prefetcher interfere with the o -chip load prediction. This is why POPET's accuracy and coverage increases in absence of a data prefetcher. Fig. <ref type="figure" target="#fig_1">22</ref> shows the percentage increase in the main memory requests over the no-prefetching system by di erent types of data prefetchers alone, and in combination with Hermes in all single-core workloads.</p><p>Combining Hermes with the baseline prefetcher increases the main memory request overhead by 5.9%, 7.6%, 5.9%, 8.6%, and 15.6% for the baseline prefetchers Pythia, Bingo, SPP, MLOP, and SMS, respectively. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 2Figure 1 :</head><label>11</label><figDesc>Figure 1: Comparison of the execution timeline of an o -chip load request in a conventional processor and in Hermes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: The distribution of ROB-blocking and non-blocking load requests (on the left y-axis), and LLC misses per kilo instructions (on the right y-axis) in the absence and presence of a state-of-the-art hardware data prefetcher<ref type="bibr" target="#b31">[32]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The average number of cycles a core stalls due to an o -chip load blocking any instruction from retiring from the ROB across all workload categories. The dark portion in each bar shows the cycles that can be completely eliminated by removing the on-chip cache access latency from an o -chip load's critical path.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: (a) Speedup of Ideal Hermes by itself and when combined with Pythia in single-core workloads. (b) Speedup ofIdeal Hermes when combined with four recently-proposed prefetchers: Bingo<ref type="bibr" target="#b27">[28]</ref>, SPP<ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b74">75]</ref>, MLOP<ref type="bibr" target="#b105">[106]</ref>, and SMS<ref type="bibr" target="#b109">[110]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Percentage of loads that miss the LLC and goes ochip (on the left y-axis) and the LLC MPKI (on the right y-axis) in the baseline system with Pythia.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>4Figure 6 :</head><label>6</label><figDesc>Figure 6: Overview of Hermes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Overview of a single-layer perceptron model. Each blue circle denotes an input and the green circle denotes the output of the perceptron.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Feature 1</head><label>1</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Stages to make a prediction by POPET 6.1.2. Training the Predictor. POPET training is invoked when a demand load request returns to the core and prepares to release its corresponding LQ entry (step 4 in Fig. 6). Every demand load that misses the LLC and goes to the main memory controller is marked as a true o -chip load request. This true o -chip outcome, along with the predicted outcome stored in the LQ entry of the demand load, are used to appropriately train the feature weights of POPET. The training happens in two stages. In the rst stage, the W ? (computed during prediction) is retrieved from the LQ entry. If W ? is neither positively nor negatively saturated (i.e., W ? lies within a negative and a positive training threshold, T N and T P , respectively), the weight training is triggered. This saturation check prevents the individual feature weight values from getting over-saturated, thereby helping POPET to quickly adapt its learning to program phase changes. In the second stage, if the weight training is triggered, the weights for each individual program feature are retrieved from their corresponding weight table using the hashed feature indices stored in the LQ entry. If the true outcome is positive (meaning the load actually went o -chip), the weight value for each feature is incremented by one. If the true outcome is negative, the weight values are decremented by one. This simple weight update mechanism moves each individual feature weight towards the direction of the true outcome, thus gradually increasing the prediction accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>8. 1 .</head><label>1</label><figDesc>POPET Prediction Analysis 8.1.1. Accuracy and Coverage of POPET. Fig.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Comparison of (a) accuracy and (b) coverage of POPET against those of HMP [126] and TTP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: The accuracy and coverage of POPET using each program feature individually and in various combinations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Line graph of POPET's (a) accuracy and (b) coverage using each of the ve program features individually across all 110 single-core workloads. No single feature can provide the best accuracy or coverage across all workloads.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Speedup in single-core workloads</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Speedup of Hermes with three o -chip load predictors (HMP, TTP, and POPET) and the Ideal Hermes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: (a) Reduction in stall cycles caused by o -chip loads. (b) Overhead in the main memory requests. 8.2.4. Overhead in Main Memory Requests. Fig. 15(b) shows the percentage increase in main memory requests in Hermes, Pythia, and Hermes combined with Pythia over the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: Speedup in eight-core workloads</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>(a)  shows the speedup of Hermes, Pythia, and Hermes combined with</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 17 :</head><label>17</label><figDesc>Figure17: Performance sensitivity to (a) main memory bandwidth, (b) di erent prefetching techniques, (c) Hermes request issue latency, and (d) on-chip cache hierarchy access latency. The baseline system con guration is highlighted in green. Other highlighted con gurations closely match with various commercial processors<ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b16">17]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: E ect of the activation threshold on POPET's accuracy and coverage (on the left y-axis) and Hermes's speedup (on the right y-axis) for all single-core workloads.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Fig.</head><label></label><figDesc>Fig.19shows the performance of Hermes, Pythia, and Hermes combined with Pythia normalized to the no-prefetching system in singlecore workloads as the size of reorder bu er (ROB) varies from 256 entries to 1024 entries. The key takeaway is that Hermes combined with Pythia outperforms Pythia alone in every ROB size con guration. Pythia+Hermes outperforms Pythia by 6.7% and 5.3% in a system with 256-entry and 1024-entry ROB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 19 :</head><label>19</label><figDesc>Figure 19: Performance sensitivity to reorder bu er size. The baseline con guration is marked in green.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Figure 20 :</head><label>20</label><figDesc>Figure 20: Performance sensitivity to LLC size. The baseline con guration is marked in green. B.3. Variation of Prediction Accuracy and Coverage with Di erent Prefetchers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Fig. 21 shows</head><label>21</label><figDesc>Fig.21shows the o -chip load prediction accuracy and coverage when Hermes is combined with di erent baseline data prefetchers. We make two key observations. First, POPET's accuracy and coverage varies widely based on the baseline data prefetcher. When combined with Pythia, Bingo, SPP, MLOP, and SMS, POPET provides accuracy of 77.3%, 78.1%, 73.4%, 79.9%, and 76.0%, while providing coverage of 74.2%, 77.6%, 65.9%, 81.7%, and 84.7%, respectively. Second, in a system without any baseline data prefetcher, POPET provides signi cantly higher accuracy (88.9%) and coverage (93.6%) than any con guration with a baseline prefetcher. This shows that, the prefetch requests generated by a sophisticated data prefetcher interfere with the o -chip load prediction. This is why POPET's accuracy and coverage increases in absence of a data prefetcher.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Figure 21 :</head><label>21</label><figDesc>Figure 21: Variation of o -chip load prediction accuracy and coverage with di erent data prefetchers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>concurrently accessing the on-chip cache hierarchy for such loads.? We design a new perceptron-based o -chip load predictor, called POPET, that accurately identi es and predicts the o -chip load requests using multiple program features. ? We show that Hermes signi cantly improves performance across a wide range of workloads and system con gura-</figDesc><table><row><cell>tions with varying core count, main memory bandwidth,</cell></row><row><cell>high-performance data prefetchers, and on-chip cache ac-</cell></row><row><cell>cess latencies.</cell></row><row><cell>? We open-source Hermes and all necessary traces and scripts</cell></row><row><cell>to reproduce results in https://github.com/CMU-SAFARI/</cell></row><row><cell>Hermes.</cell></row></table><note><p>? We introduce Hermes, a new technique that reduces long memory access latency by predicting o -chip load requests and fetching their corresponding data directly from the main memory, while</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head># Weight Table 1 hash index Feature 2 # Weight Table 2 hash index</head><label>Weight1index2Weight2index</label><figDesc></figDesc><table><row><cell>Stage 1</cell><cell></cell><cell cols="2">Stage 2</cell><cell></cell><cell>Stage 3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>weight 1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>weight 2</cell><cell>? ?act ? ?act</cell><cell>Predict to go o8f-chip</cell></row><row><cell>. . . . .</cell><cell></cell><cell></cell><cell>. . .</cell><cell>weight n</cell><cell>Sum weights</cell><cell>Activation</cell></row><row><cell>Feature N</cell><cell>#</cell><cell>index</cell><cell>Weight Table N</cell><cell></cell></row><row><cell></cell><cell>hash</cell><cell></cell><cell></cell><cell></cell></row></table><note><p>(e.g., PC + o8fset)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>6.1.3. Automated Feature Selection. The selection of the program features used to make the o -chip load prediction is critical to POPET's performance. A carefully-crafted and selected set of features can signi cantly improve the accuracy and the coverage of POPET. In this section we propose an automated, o ine, performance-driven methodology to nd a set of program features for POPET.We initially select a set of 16 individual program features using our domain expertise that can correlate well with a load going o -chip. Table1shows the initial feature set.The automated feature selection process happens o ine during the design time of POPET. The process starts with the initial set of 16 individual program features and iteratively creates a The initial set of program features used for automated feature selection. ? represents a bitwise XOR operation.</figDesc><table><row><cell>Features without control-ow</cell><cell>Features with control-ow</cell></row><row><cell>information</cell><cell>information</cell></row><row><cell></cell><cell>8. Load PC</cell></row><row><cell>1. Load virtual address</cell><cell>9. PC ? load virtual address</cell></row><row><cell>2. Virtual page number</cell><cell>10. PC ? virtual page number</cell></row><row><cell>3. Cacheline o set in page</cell><cell>11. PC ? cacheline o set</cell></row><row><cell>4. First access</cell><cell>12. PC + rst access</cell></row><row><cell>5. Cacheline o set + rst access</cell><cell>13. PC ? byte o set</cell></row><row><cell>6. Byte o set in cacheline</cell><cell>14. PC ? word o set</cell></row><row><cell>7. Word o set in cacheline</cell><cell>15. Last-4 load PCs</cell></row><row><cell></cell><cell>16. Last-4 PCs</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>POPET con guration parameters</figDesc><table><row><cell></cell><cell>? PC ? cacheline o set</cell></row><row><cell></cell><cell>? PC ? byte o set</cell></row><row><cell>Selected features</cell><cell>? PC + rst access</cell></row><row><cell></cell><cell>? Cacheline o set + rst access</cell></row><row><cell></cell><cell>? Last-4 load PCs</cell></row></table><note><p>Threshold values ?act = -18, TN = -35, TP = 40</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Storage overhead of Hermes</figDesc><table><row><cell>Structure</cell><cell>Description</cell><cell>Size</cell></row><row><cell></cell><cell>? Perceptron weight tables</cell><cell></cell></row><row><cell></cell><cell>-PC ? cacheline o set: 1024 ? 5b</cell><cell></cell></row><row><cell></cell><cell>-PC ? byte o set: 1024 ? 5b</cell><cell></cell></row><row><cell>POPET</cell><cell>-PC + rst access: 1024 ? 5b</cell><cell>3.2 KB</cell></row><row><cell></cell><cell>-Cacheline o set + rst access: 128 ? 5b</cell><cell></cell></row><row><cell></cell><cell>-Last-4 load PCs: 1024 ? 5b</cell><cell></cell></row><row><cell></cell><cell>? Page bu er: 64 ? 80b</cell><cell></cell></row><row><cell>LQ Metadata</cell><cell>Hashed PC: 128 ? 32b; Last-4 PC: 128 ? 10b; 128 ? 5b; prediction: 128 ? 1b First access: 128 ? 1b; perceptron weight:</cell><cell>0.8 KB</cell></row><row><cell>Total</cell><cell></cell><cell>4.0 KB</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Simulated system parameters</figDesc><table><row><cell></cell><cell>1 and 8 cores, 6-wide fetch/execute/commit, 512-entry ROB,</cell></row><row><cell>Core</cell><cell>128/72-entry LQ/SQ, Perceptron branch predictor [61] with</cell></row><row><cell></cell><cell>17-cycle misprediction penalty</cell></row><row><cell>L1/L2</cell><cell>Private, 48KB/1.25MB, 64B line, 12/20-way, 16/48 MSHRs,</cell></row><row><cell>Caches</cell><cell>LRU, 5/15-cycle round-trip latency [25]</cell></row><row><cell>LLC</cell><cell>3MB/core, 64B line, 12 way, 64 MSHRs/slice, SHiP [122], 55-cycle round-trip latency [24, 25], Pythia prefetcher [32]</cell></row><row><cell></cell><cell>1C: 1 channel, 1 rank per channel; 8C: 4 channels, 2 ranks</cell></row><row><cell>Main</cell><cell>per channel; 8 banks per rank, DDR4-3200 MTPS, 64b data-</cell></row><row><cell>Memory</cell><cell>bus per channel, 2KB row bu er per bank, tRCD=12.5ns,</cell></row><row><cell></cell><cell>tRP=12.5ns, tCAS=12.5ns</cell></row><row><cell>Hermes</cell><cell>Hermes-O/P: 6/18-cycle Hermes request issue latency</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Workloads used for evaluation</figDesc><table><row><cell>Suite</cell><cell>#Workloads</cell><cell>#Traces</cell><cell>Example Workloads</cell></row><row><cell>SPEC06</cell><cell>14</cell><cell>22</cell><cell>gcc, mcf, cactusADM, lbm, ...</cell></row></table><note><p>SPEC17 11 23 gcc, mcf, pop2, fotonik3d, ... PARSEC 4 12 canneal, facesim, raytrace, ... Ligra 11 20 BFS, PageRank, Radii, ... CVP 33 33 integer, oating-point, server, ...</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Storage overhead of all evaluated mechanisms HMP<ref type="bibr" target="#b125">[126]</ref> with local, gshare, and gskew predictors 11 KB TTP with a metadata budget similar to the L2 cache 1536 KB</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Figure 13: Single-core performance of all 110 workloads 8.2.2. E ect of the O -chip Load Prediction Mechanism.Fig.</figDesc><table><row><cell>Speedup over the No-prefetching system</cell><cell>1.50 1.75 2.00 2.25 2.50</cell><cell cols="25">Hermes-O 623.xalancbmk_s-10B Pythia (baseline) streamcluster-6B Ligra_PageRank-79B Ligra_Components-22B compute_int_264 Pythia + Hermes-O server_612 605.mcf_s-782B Ligra_Triangle-25B</cell><cell></cell><cell></cell><cell cols="8">compute_int_539 602.gcc_s-2226B</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell>4</cell><cell>7</cell><cell>10</cell><cell>13</cell><cell>16</cell><cell>19</cell><cell>22</cell><cell>25</cell><cell>28</cell><cell>31</cell><cell>34</cell><cell>37</cell><cell>40</cell><cell>43</cell><cell>46</cell><cell>49</cell><cell>52</cell><cell>55</cell><cell>58</cell><cell>61</cell><cell>64</cell><cell>67</cell><cell>70</cell><cell>73</cell><cell>76</cell><cell>79</cell><cell>82</cell><cell>85</cell><cell>88</cell><cell>91</cell><cell>94</cell><cell>97</cell><cell>100</cell><cell>103</cell><cell>106</cell><cell>109</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">Workload number</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>2. If you have copied the CSV le using an CSV application that already automatically separates the columns, then go to Step 4. 3. Immediately after pasting, convert the comma-separated rows into columns by going to Data ? Text-to-Columns ? Select comma as a delimiter. This replaces the already existing data in the sheet with the newly collected data. 4. Refresh each pivot table in each sheet by clicking on them and then clicking Pivot-Table-Analyse ? Refresh. The reader can also use any other data processor (e.g., Python pandas) to reproduce the same result. A.8. Expected Results ? Accuracy and coverage comparison. POPET should show 77.1% accuracy, whereas HMP and TTP should show 46.8% and 16.6% accuracy, respectively. POPET should show 74.4% coverage, whereas HMP and TTP should show 22.3% and 94.8% coverage, respectively. ? Performance comparison with Pythia. Hermes-P, Hermes-O, Pythia, Pythia+Hermes-P, and Pythia+Hermes-O should provide 8.9%, 11.5%, 20.5%, 24.7%, and 25.6% geomean performance improvement, respectively, across all single-core workloads. ? Performance comparison with prior predictors. Pythia, Pythia+Hermes-HMP, Pythia+Hermes-TTP, and Pythia+Hermes-POPET should provide 20.5%, 21.1%, 22.09%, and 25.6% geomean performance improvement, respectively, across all singlecore workload traces. ? Performance comparison with varying prefetchers. Both Hermes-P and Hermes-O improves performance than the prefetcher by itself for every baseline prefetcher type: Pythia, Bingo, SPP, MLOP, and SMS. The con guration of each prefetcher can be customized by changing the ini les inside the config directory. ? The exp les can be customized to run new experiments with di erent prefetcher combinations. More experiment les can be found inside experiments/extra directory. One can use the same instructions mentioned in Appendix A.6.1 to launch experiments.Please check the FAQ section in GitHub (https://github.com/ CMU-SAFARI/Hermes#frequently-asked-questions) for quick troubleshooting tips.</figDesc><table><row><cell>A.11. Methodology</cell></row><row><cell>Submission, reviewing and badging methodology:</cell></row><row><cell>? https://www.acm.org/publications/policies/</cell></row><row><cell>artifact-review-badging</cell></row><row><cell>? http://cTuning.org/ae/submission-20201122.html</cell></row><row><cell>? http://cTuning.org/ae/reviewing-20201122.html</cell></row><row><cell>B. Extended Results</cell></row><row><cell>B.1. Performance Sensitivity to ROB Size</cell></row></table><note><p><p>A.9. Experiment Customization</p>? A.10. Quick Troubleshooting</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Hence named after Hermes, the Olympian deity<ref type="bibr" target="#b11">[12]</ref> who can quickly move between the realms of the divine (i.e., the processor) and the mortals (i.e., the main memory).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We select Pythia as the baseline prefetcher as it provides the highest prefetch coverage and performance bene t among the ve contemporary prefetchers considered in this paper (see ?7.2 and ?8.4.2). Nonetheless, our qualitative observation holds equally true for other prefetchers considered in this work (see ?7.2).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Even though HMP was originally proposed to predict loads that miss L1 data cache, it can be extended to predict loads that miss the entire multi-level on-chip cache hierarchy.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>For simplicity, our automated feature selection process optimizes for accuracy. A more comprehensive feature selection process can also include coverage or directly optimize for performance (i.e., execution time).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>As POPET uses ve program features (see ?6.1.3), the sum of all ve weights (each represented by a 5-bit saturating signed integer as described in ?6.1) can take a maximum and minimum value of 75 and -80, respectively.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>Each box is lower-bounded by the rst quartile (i.e., the middle value between the lowest value and the median value of the data points) and upperbounded by the third quartile (i.e., the middle value between the median and the highest value of the data points). The inter-quartile range (IQR) is the distance between the rst and the third quartile (i.e., the length of the box). Whiskers extend an additional 1.5 ? IQR on the either side of the box. Any outlier values that falls outside the range of whiskers are marked by dots. The cross marked value within each box represents the mean.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>This appendix focuses on reproducing four key results mentioned here. Nonetheless, the artifact contains les and necessary scripts to reproduce all results mentioned in the paper.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank <rs type="person">Anant Nori</rs> and <rs type="person">Sreenivas Subramoney</rs> for their valuable feedback on this work. We thank the anonymous reviewers of MICRO 2022 for their encouraging feedback. We thank the <rs type="institution">SAFARI Research Group members</rs> for providing a stimulating intellectual environment. We acknowledge the generous gifts from our industrial partners: Google, <rs type="person">Huawei</rs>, <rs type="funder">Intel</rs>, <rs type="funder">Microsoft</rs>, and <rs type="funder">VMware</rs>. This work is supported in part by the <rs type="funder">Semiconductor Research Corporation</rs> and the <rs type="institution">ETH Future Computing Laboratory</rs>. The rst author thanks his departed father, whom he lost in COVID-19 pandemic.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Artifact Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Abstract</head><p>We implement Hermes using the ChampSim simulator <ref type="bibr" target="#b8">[9]</ref>. In this artifact, we provide the source code of Hermes and necessary instructions to reproduce its key performance results. 7  We identify four key results to demonstrate Hermes's novelty: ? Comparison of accuracy and coverage of POPET against HMP and TTP (Fig. <ref type="figure">9</ref>). ? Workload category-wise performance comparison in singlecore workloads (Fig. <ref type="figure">12</ref>). ? Workload category-wise performance comparison of Hermes with POPET with Hermes-HMP and Hermes-TTP in singlecore workloads (Fig. <ref type="figure">14</ref>). ? Performance sensitivity to di erent prefetching techniques (Fig. <ref type="figure">17(b)</ref>).</p><p>The artifact can be executed in any machine with a generalpurpose CPU and 36 GB disk space. However, we strongly recommend running the artifact on a compute cluster with slurm <ref type="bibr" target="#b126">[127]</ref> support for bulk experimentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Artifact Check-list (Meta-information)</head><p>? Compilation: GCC 6.3.0 or above.</p><p>? Data set: Download traces using the supplied script.  <ref type="bibr" target="#b12">[13]</ref> or Zenodo <ref type="bibr" target="#b13">[14]</ref>.</p><p>A.3.2. Hardware Dependencies. Hermes can be run on any system with a general-purpose CPU and at least 36 GB of free disk space.</p><p>A.3.4. Data Sets. The ChampSim traces required to evaluate Hermes can be downloaded using the supplied script. Our implementation of Hermes is fully compatible with prior ChampSim traces that are used in previous cache replacement (CRC-2 <ref type="bibr" target="#b0">[1]</ref>), data prefetching (DPC-3 <ref type="bibr" target="#b2">[3]</ref>) and value-prediction (CVP-2 <ref type="bibr" target="#b20">[21]</ref>) championships. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Installation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6. Experimental Work ow</head><p>This section describes the steps to generate and execute necessary experiments. We recommend the reader to follow script/README.md to know more about each script used in this section.</p><p>A.6.1. Launching Experiments. The following instructions enable launching all experiments required to reproduce key results in a local machine. We strongly recommend using a compute cluster with slurm support to e ciently launch experiments in bulk. To launch experiments using slurm, please provide -local 0 (tested using slurm v16.05.9).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Create the job le for the experiments as follows:</head><p>$ cd $HERMES _ HOME/experiments $ perl $HERMES _ HOME/scripts/create _ jobfile.pl -exe $HERMES _ HOME/bin/glc-perceptron-nomulti-multi-multi-multi-1core-1ch -tlist MICRO22 _ AE.tlist -exp MICRO22 _ AE.exp -local 1 &gt; jobfile.sh 2. Please make sure the paths used in tlist and exp les are appropriately changed before creating the job le.</p><p>3. If you are creating jobs for slurm, please set the slurm partition name appropriately (set as slurm _ part by default). You might also want to set some key parameters for slurm con guration as follows: (a) max memory per node: 2 GB, (b) timeout: 12 hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Finally, launch the experiments as follows:</head><p>$ cd $HERMES _ HOME/outputs/ $ source ../jobfile.sh A.6.2. Rolling-up Statistics. The rollup.pl script parses a set of output les of a given experiment and dumps all statistics in a commaseparated-value (CSV) format.</p><p>To automate the roll-up process, we use the following set of instructions which enable the creation of four CSV les in experiments directory. These CSV les are later used for comparison in Appendix A.7.</p><p>$ cd $HERMES _ HOME/experiments $ bash automate _ rollup.sh</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7. Evaluation</head><p>We use three metrics for comparing Hermes with previous works: (1) performance, (2) accuracy and (3) coverage of the o -chip predictor.</p><p>The performance gain of a simulation con guration is measured with respect to the no-prefetching system using Eq. 2.</p><p>We measure the accuracy and coverage of each evaluated o -chip prediction mechanism using Eq. 3 and 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AccuracyX = T P T P + F P</head><p>(3)</p><p>Here, TP represents the number of predicted o -chip requests that actually go o -chip, FP represents the number of predicted o -chip requests that do not go o -chip, and FN represents the number of requests that are not predicted by the o -chip predictor but go ochip.</p><p>To easily calculate the metrics, we provide a Microsoft Excel template to post-process the rolled-up CSV les generated in Appendix A.6.2. The template has ve sheets. The rst sheet (named metadata) contains the metadata to identify the category of each workload trace and experiment. Each of the remaining sheets shares the same name of the rolled-up CSV le. Each of these sheets is already populated with our collected results, necessary formulas, pivot tables, and charts to reproduce the results presented in the paper. Only the blue-highlighted columns in each sheet need to be populated by your own CSV les. Please follow these instructions to reproduce the results from your own CSV statistics les: 1. Copy and paste each CSV le into its corresponding sheet's top left corner (i.e., cell A1).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">2nd Cache Replacement Championship</title>
		<ptr target="https://crc2.ece.tamu.edu" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">2nd Data Prefetching Championship</title>
		<ptr target="http://comparch-conf.gatech.edu/dpc2/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">3rd Data Prefetching Championship</title>
		<ptr target="https://dpc3.compas.cs.stonybrook.edu" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">6th Generation Intel? Processor Family</title>
		<ptr target="https://www.intel.com/content/www/us/en/processors/core/desktop-6th-gen-core-family-spec-update.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">AMD Gives More Zen Details: Ryzen</title>
		<ptr target="https://bit.ly/3ACs9ES" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">AMD Ryzen Threadripper 3990X</title>
		<ptr target="https://en.wikichip.org/wiki/amd/ryzen_threadripper/3990x" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">AMD Zen2 EPYC 7702P</title>
		<ptr target="https://en.wikichip.org/wiki/amd/epyc/7702p" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Caching of Temporal vs. Non-Temporal Data -Intel? 64 and IA-32 Architectures Developer&apos;s Manual</title>
		<ptr target="https://www.intel.com/content/www/us/en/architecture-and-technology/64-ia-32-architectures-software-developer-vol-1-manual.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">ChampSim</title>
		<ptr target="https://github.com/ChampSim/ChampSim" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Golden Cove -Microarchitectures -Intel</title>
		<ptr target="https://en.wikichip.org/wiki/intel/microarchitectures/golden_cove" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Golden Cove Microarchitecture (P-Core) Examined</title>
		<ptr target="https://www.anandtech.com/show/16881/a-deep-dive-into-intels-alder-lake-microarchitectures/3" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Hermes -Wikipedia</title>
		<ptr target="https://en.wikipedia.org/wiki/Hermes" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Hermes GitHub repository</title>
		<ptr target="https://github.com/CMU-SAFARI/Hermes" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Hermes Zenodo repository</title>
		<idno type="DOI">10.5281/zenodo.6909799</idno>
		<ptr target="https://doi.org/10.5281/zenodo.6909799" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Intel Core i5-12600K DDR4 Alder Lake CPU Review</title>
		<ptr target="https://www.thefpsreview.com/2021/12/08/intel-core-i5-12600k-ddr4-alder-lake-cpu-review/6/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Intel Details Golden Cove</title>
		<ptr target="https://fuse.wikichip.org/news/6111/intel-details-golden-cove-next-generation-big-core-for-client-and-server-socs/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Intel Xeon Gold 6258R</title>
		<ptr target="https://en.wikichip.org/wiki/intel/xeon_gold/6258r" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">L3 Cache Latency Comparison at Base Frequency</title>
		<ptr target="https://www.cpuagent.com/cpu/intel-core-i9-10900k/benchmarks/l3-cache-latency-at-base-frequency/nvidia-geforce-rtx-2080-ti?res=1&amp;quality=ultra" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">MOVNTI -x86 ISA</title>
		<ptr target="https://www.felixcloutier.com/x86/movnti" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">PARSEC</title>
		<ptr target="http://parsec.cs.princeton.edu/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Second Championship Value Prediction (CVP-2)</title>
		<ptr target="https://www.microarch.org/cvp1/cvp2/rules.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">SPEC CPU</title>
		<ptr target="https://www.spec.org/cpu2006/" />
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">SPEC CPU 2017</title>
		<ptr target="https://www.spec.org/cpu" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Surprisingly High Latency Discovered in Alder Lake</title>
		<ptr target="https://bit.ly/3RhlK8z" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<ptr target="https://bit.ly/3wFRM6l" />
		<title level="m">The Intel 12th Gen Core i9-12900K Review</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">An E ective On-chip Preloading Scheme to Reduce Data Access Penalty</title>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Baer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-F</forename><surname>Chen</surname></persName>
		</author>
		<editor>SC</editor>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Domino Temporal Data Prefetcher</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bakhshalipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lot -Kamran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sarbazi-Azad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bingo Spatial Data Prefetcher</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bakhshalipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shakerinava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lot -Kamran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sarbazi-Azad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Reducing Memory Reference Energy with Opportunistic Virtual Caching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Swift</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Managing Wire Delay in Large Chip-Multiprocessor Caches</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Correlated Load-Address Predictors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bekerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jourdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ronen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kirshenboim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rappoport</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yoaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Weiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Pythia: A Customizable Hardware Prefetching Framework Using Online Reinforcement Learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kanellopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shahroodi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Subramoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">DSPatch: Dual Spatial Pattern Prefetcher</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Nori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Subramoney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Besta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yalamanchili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ausavarungnirun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hoeer</surname></persName>
		</author>
		<title level="m">Slim NoC: A Low-diameter On-chip Network Topology for High Energy Efciency and Scalability</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>ASPLOS</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Perceptron-Based Prefetch Filtering</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chacon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Teran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Gratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Virtual-Address Caches. Part 1: Problems and Solutions in Uniprocessors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cekleov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dubois</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">E ective Hardware-Based Data Prefetching for High-Performance Processors</title>
		<author>
			<persName><forename type="first">T.-F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Baer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Improving Cache Performance by Selective Cache Bypass</title>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dietz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">HICSS</title>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dynamic Hot Data Stream Prefetching for General-Purpose Programs</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Chilimbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hirzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLDI</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Low-cost Epoch-based Correlation Prefetching for Commercial Applications</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">A Stateless, Content-Directed Data Prefetching Mechanism</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cooksey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jourdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grunwald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>ASPLOS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Improving Data Cache Performance by Pre-executing Instructions Under a Cache Miss</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dundas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mudge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICS</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Enhancing Last-Level Cache Performance by Block Bypassing and Early Miss Determination</title>
		<author>
			<persName><forename type="first">H</forename><surname>Dybdahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stenstr?m</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACSAC</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Last-touch Correlated Data Streaming</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPASS</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Spatial Memory Streaming with Rotated Patterns</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Somogyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JILP Data Prefetching Championship</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Stride Directed Prefetching in Scalar Processors</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Janssens</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Bit-level Perceptron Prediction for Indirect Branches</title>
		<author>
			<persName><forename type="first">E</forename><surname>Garza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mirbagher-Ajorpaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jimenez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Bypass and Insertion Algorithms for Exclusive Last-Level Caches</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Subramoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Grayson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rupley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Z</forename><surname>Zuraski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Quinnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nakra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kitchin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hensley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brekelbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sinha</surname></persName>
		</author>
		<title level="m">Evolution of the Samsung Exynos CPU Microarchitecture</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>ISCA</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Reactive NUCA: Near-Optimal Block Placement and Replication in Distributed Caches</title>
		<author>
			<persName><forename type="first">N</forename><surname>Hardavellas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Continuous Runahead: Transparent Hardware Acceleration for Memory Intensive Workloads</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Filtered Runahead Execution with a Runahead Bu er</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MICRO</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Timekeeping in the Memory System: Predicting and Optimizing Memory Behavior</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaxiras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martonosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">TCP: Tag Correlating Prefetchers</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martonosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaxiras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">E ective Stream-Based and Execution-Based Data Prefetching</title>
		<author>
			<persName><forename type="first">S</forename><surname>Iacobovici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Spracklen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kadambi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Abraham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICS</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Access Map Pattern Matching for Data Cache Prefetch</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Inaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hiraki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISC</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Linearizing Irregular Memory Accesses for Improved Correlated Prefetching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Back to the Future: Leveraging Belady&apos;s Algorithm for Improved Cache Replacement</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">High Performance Cache Replacement Using Re-Reference Interval Prediction (RRIP)</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Theobald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Steely</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Reducing Load Latency with Cache Level Prediction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Erez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Dynamic Branch Prediction with Perceptrons</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Fast Path-Based Neural Branch Prediction</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Dead Block Replacement and Bypass with a Sampling Predictor</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JWAC</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Neural Methods for Dynamic Branch Prediction</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOCS</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Multiperspective Reuse Prediction</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Teran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Run-Time Adaptive Cache Management</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Connors</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-M</forename><forename type="middle">W</forename><surname>Hwu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">HICSS</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Run-Time Cache Bypassing</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Connors</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Merten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-M</forename><forename type="middle">W</forename><surname>Hwu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>IEEE Trans. Computers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Run-Time Adaptive Cache Hierarchy Management via Reference Analysis</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-M</forename><forename type="middle">W</forename><surname>Hwu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Prefetching using Markov Predictors</title>
		<author>
			<persName><forename type="first">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grunwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Improving Direct-mapped Cache Performance by the Addition of a Small Fully-associative Cache and Prefetch Bu ers</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">A Prefetching Technique for Irregular Accesses to Linked Data Structures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Karlsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dahlgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stenstrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Sampling Dead Block Prediction for Last-Level Caches</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jimenez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Counter-Based Cache Replacement and Bypassing Algorithms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kharbutli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Solihin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TC</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">An Adaptive, Non-Uniform Cache Structure for Wire-Delay Dominated On-Chip Caches</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>ASPLOS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Path Con dence Based Lookahead Prefetching</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Gratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chishti</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Division of Labor: A More E ective Approach to Prefetching</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kondguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Exploiting Spatial Locality in Data Caches using Spatial Footprints</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Mc-PAT: An Integrated Power, Area, and Timing Modeling Framework for Multicore and Manycore Architectures</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Strong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">CRISP: Critical Slice Prefetching</title>
		<author>
			<persName><forename type="first">H</forename><surname>Litz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ayers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>in ASP-LOS</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Cache Bursts: A New Approach for Eliminating Dead Blocks and Increasing Cache E ciency</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">E ciently Enabling Conventional Block Sizes for Very Large Die-Stacked DRAM Caches</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">A Logical Calculus of the Ideas Immanent in Nervous Activity</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Mcculloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pitts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The bulletin of mathematical biophysics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1943">1943</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Digital Western Research Laboratory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mcfarling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep</note>
	<note>Combining Branch Predictors</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Just Say No: Bene ts of Early Cache Miss Determination</title>
		<author>
			<persName><forename type="first">G</forename><surname>Memik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Reinman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Mangione-Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Best-O set Hardware Prefetching</title>
		<author>
			<persName><forename type="first">P</forename><surname>Michaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Trading Con ict and Capacity Aliasing in Conditional Branch Predictors</title>
		<author>
			<persName><forename type="first">P</forename><surname>Michaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Uhlig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Address-Value Delta (AVD) Prediction: Increasing the E ectiveness of Runahead Execution by Exploiting Regular Memory Allocation Patterns</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Techniques for E cient Processing in Runahead Execution Engines</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">E cient Runahead Execution: Power-E cient Memory Latency Tolerance</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>IEEE Micro</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">On Reusing the Results of Pre-Executed Instructions in a Runahead Execution Processor</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CAL</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Runahead Execution: An Alternative to Very Large Instruction Windows for Out-of-order Processors</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Runahead Execution: An E ective Alternative to Large Instruction Windows</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>IEEE Micro</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Vector Runahead</title>
		<author>
			<persName><forename type="first">A</forename><surname>Naithani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ainsworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Bouquet of Instruction Pointers: Instruction Pointer Classi er-based Spatial Hardware Prefetching</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pakalapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">Computer Organization and Design: The Hardware Software Interface</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Hennessy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Morgan kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Bloom Filtering Cache Misses for Accurate Data Speculation and Prefetching</title>
		<author>
			<persName><forename type="first">J.-K</forename><surname>Peir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICS</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Exploiting Single-Usage for E ective Memory Management</title>
		<author>
			<persName><forename type="first">T</forename><surname>Piquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Rochecouste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACSAC</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chishti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>-F. Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Balasubramonian</surname></persName>
		</author>
		<title level="m">Sandbox Prefetching: Safe Run-Time Evaluation of Aggressive Prefetchers</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>HPCA</note>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Adaptive Insertion Policies for High Performance Caching</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Steely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Fundamental Latency Trade-o in Architecting DRAM Caches: Outperforming Impractical SRAM-Tags with a Simple and Practical Design</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Reducing Con icts in Direct-Mapped Caches with a Temporality-Based Design</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Rivers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Davidson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICPP</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Utilizing Reuse Information in Data Cache Management</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Rivers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Tyson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Farrens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICS</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain</title>
		<author>
			<persName><forename type="first">F</forename><surname>Rosenblatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">The Direct-to-Data (D2D) Cache: Navigating the Cache Hierarchy with a Single Lookup</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sembrant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hagersten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Black-Scha Er</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">A Split Cache Hierarchy for Enabling Data-Oriented Optimizations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sembrant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hagersten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Black-Scha Er</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title level="m" type="main">Multi-Lookahead O set Prefetching</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shakerinava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bakhshalipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lot -Kamran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sarbazi-Azad</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<title level="m" type="main">E ciently Prefetching Complex Address Patterns</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shevgoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Koladiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Balasubramonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chishti</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Ligra: A Lightweight Graph Processing Framework for Shared Memory</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Blelloch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PPoPP</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<author>
			<persName><forename type="first">S</forename><surname>Somogyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spatio-Temporal Memory Streaming</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Spatial Memory Streaming</title>
		<author>
			<persName><forename type="first">S</forename><surname>Somogyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Feedback Directed Prefetching: Improving the Performance and Bandwidth-E ciency of Hardware Prefetchers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Srinath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Merging Path and Gshare Indexing in Perceptron Branch Prediction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tarjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Skadron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACO</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<title level="m" type="main">Perceptron Learning for Reuse Prediction</title>
		<author>
			<persName><forename type="first">E</forename><surname>Teran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<title level="m" type="main">A Modi ed Approach to Data Cache Management</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tyson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Farrens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pleszkun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Managing Data Caches using Selective Cache Line Replacement</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tyson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Farrens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Pleszkun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJPP</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Stream Floating: Enabling Proactive and Decentralized Cache Optimizations</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lowe-Power</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nowatzki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Practical O -chip Meta-data for Temporal Memory Streaming</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Making Address-Correlated Prefetching Practical</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE micro</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Temporal Streaming of Shared Memory</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Somogyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hardavellas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Modi ed LRU Policies for Improving Second-level Cache Behavior</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Baer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Eggers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Pendleton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">An In-Cache Address Translation Mechanism</title>
		<imprint>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
	<note>ISCA</note>
</biblStruct>

<biblStruct xml:id="b121">
	<monogr>
		<title level="m" type="main">SHiP: Signature-based Hit Predictor for High Performance Caching</title>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hasenplaugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martonosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Steely</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">E cient Metadata Management for Irregular Data Prefetching</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nathella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sunwoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<monogr>
		<title level="m" type="main">Temporal Prefetching Without the O -Chip Metadata</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nathella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pusdesris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sunwoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<monogr>
		<title level="m" type="main">Two-level Adaptive Training Branch Prediction</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
	<note>in MI-CRO</note>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Speculation Techniques for Improving Load Related Instruction Scheduling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yoaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ronen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jourdan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Slurm: Simple Linux Utility for Resource Management</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Jette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grondona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on job scheduling strategies for parallel processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="44" to="60" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
