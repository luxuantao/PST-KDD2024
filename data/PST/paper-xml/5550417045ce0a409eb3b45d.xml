<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Weakly-supervised Discovery of Visual Pattern Configurations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hyun</forename><surname>Oh</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Song</forename><surname>Yong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jae</forename><surname>Lee</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Davis</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Weakly-supervised Discovery of Visual Pattern Configurations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">04C65992C8DFD60A9DB76A05D2C77287</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The prominence of weakly labeled data gives rise to a growing demand for object detection methods that can cope with minimal supervision. We propose an approach that automatically identifies discriminative configurations of visual patterns that are characteristic of a given object class. We formulate the problem as a constrained submodular optimization problem and demonstrate the benefits of the discovered configurations in remedying mislocalizations and finding informative positive and negative training examples. Together, these lead to state-of-the-art weakly-supervised detection results on the challenging PASCAL VOC dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The growing amount of sparsely and noisily labeled image data demands robust detection methods that can cope with a minimal amount of supervision. A prominent example of this scenario is the abundant availability of labels at the image level (i.e., whether a certain object is present or absent in the image); detailed annotations of the exact location of the object are tedious and expensive and, consequently, scarce. Learning methods that can handle image-level labels circumvent the need for such detailed annotations and therefore have the potential to effectively use the vast textually annotated visual data available on the Web. Moreover, if the detailed annotations happen to be noisy or erroneous, such weakly supervised methods can even be more robust than fully supervised ones.</p><p>Motivated by these developments, recent work has explored learning methods that decreasingly rely on strong supervision. Early ideas for weakly supervised detection <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b31">32]</ref> paved the way by successfully learning part-based object models, albeit on simple object-centric datasets (e.g., Caltech-101). Since then, a number of approaches <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29]</ref> have aimed at learning models from more realistic and challenging data sets that feature large intra-category appearance variations and background clutter. These approaches typically generate multiple candidate regions and retain the ones that occur most frequently in the positively-labeled images. However, due to intra-category variations and deformations, the identified (single) patches often correspond to only a part of the object, such as a human face instead of the entire body. Such mislocalizations are a frequent problem for weakly supervised detection methods.</p><p>Mislocalization and too large or too small bounding boxes are problematic in two respects. First, detection is commonly phrased as multiple instance learning (MIL) and solved by non-convex optimization methods that alternatingly guess the location of the objects as positive examples (since the true location is unknown) and train a detector based on those guesses. This procedure is heavily affected by the initial localizations. Second, the detector is often trained in stages; in each stage one adds informative "hard" negative examples to the training data. If we are not given accurate true object localizations in the training data, these hard examples must be derived from the detections inferred in earlier rounds. The higher the accuracy of the initial localizations, the more informative is the augmented training data -and this is key to the accuracy of the final learned model.</p><p>In this work, we address the issue of mislocalizations by identifying characteristic, discriminative configurations of multiple patches (rather than a single one). This part-based approach is motivated by the observation that automatically discovered single "discriminative" patches often correspond to object parts. In addition, while background patches (e.g., of water or sky) can also occur throughout the positive images, they will re-occur in arbitrary rather than "typical" configurations. We develop an effective method that takes as input a set of images with labels of the form "the object is present/absent", and automatically identifies characteristic part configurations of the given object.</p><p>To identify such configurations, we use two main criteria. First, useful patches are discriminative, i.e., they occur in many positively-labeled images, and rarely in the negatively labeled ones. To identify such patches, we use a discriminative covering formulation similar to <ref type="bibr" target="#b28">[29]</ref>. Second, the patches should represent different parts, i.e., they may be close but should not overlap too much. In covering formulations, one may rule out overlaps by saying that for two overlapping regions, one "covers" the other, i.e., they are treated as identical and picking one is as good as picking both. But identity is a transitive relation, and the density of possible regions in detection would imply that all regions are identical, strongly discouraging the selection of more than one part per image. Partial covers face the problem of scale invariance. Hence, we instead formulate an independence constraint. This second criterion ensures that we select regions that may be close but are non-redundant and sufficiently non-overlapping. We show that this constrained selection problem corresponds to maximizing a submodular function subject to a matroid intersection constraint, which leads to approximation algorithms with theoretical worst-case bounds. Given candidate parts identified by these two criteria, we effectively find frequently co-occurring configurations that take into account relative position, scale, and viewpoint.</p><p>We demonstrate multiple benefits of the discovered configurations. First, we observe that configurations of patches can produce more accurate spatial coverage of the full object, especially when the most discriminative pattern corresponds to an object part. Second, any overlapping region between co-occurring visual patterns is likely to cover a part (but not the full) of the object of interest. Thus, they can be used to generate mis-localized positives as informative hard negatives for training (see white boxes in Figure <ref type="figure" target="#fig_3">3</ref>), which can further reduce localization errors at test time.</p><p>In short, our main contribution is a weakly-supervised object detection method that automatically discovers frequent configurations of discriminative visual patterns to train robust object detectors. In our experiments on the challenging PASCAL VOC dataset, we find the inclusion of our discriminative, automatically detected configurations to outperform all existing state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Weakly-supervised object detection. Object detectors have commonly been trained in a fullysupervised manner, using tight bounding box annotations that cover the object of interest (e.g., <ref type="bibr" target="#b9">[10]</ref>). To reduce laborious bounding box annotation costs, recent weakly-supervised approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32]</ref> use image-level object-presence labels with no information on object location.</p><p>Early efforts <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b31">32]</ref> focused on simple datasets that have a single prominent object in each image (e.g., Caltech-101). More recent approaches <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29]</ref> work with the more challenging PASCAL dataset that contains multiple objects in each image and large intra-category appearance variations. Of these, Song et al. <ref type="bibr" target="#b28">[29]</ref> achieve state-of-the-art results by finding discriminative image patches that occur frequently in the positive images but rarely in the negative images, using deep Convolutional Neural Network (CNN) features <ref type="bibr" target="#b16">[17]</ref> and a submodular cover formulation. We build on their approach to identify discriminative patches. But, contrary to <ref type="bibr" target="#b28">[29]</ref> which assumes patches to contain entire objects, we assume patches to contain either full objects or merely object parts, and automatically piece together those patches to produce better full-object estimates. To this end, we change the covering formulation and identify patches that are both representative and explicitly mutually different. This leads to more robust object estimates and further allows our system to intelligently select "hard negatives" (mislocalized objects), both of which improve detection performance.</p><p>Visual data mining. Existing approaches discover high-level object categories <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b27">28]</ref>, mid-level patches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24]</ref>, or low-level foreground features <ref type="bibr" target="#b17">[18]</ref> by grouping similar visual patterns (i.e., images, patches, or contours) according to their texture, color, shape, etc. Recent methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16]</ref> use weakly-supervised labels to discover discriminative visual patterns. We use related ideas, but formulate the problem as a submodular optimization over matroids, which leads to approximation algorithms with theoretical worst-case guarantees. Covering formulations have also been used in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, but after running a trained object detector. An alternative discriminative approach is to use spectral methods <ref type="bibr" target="#b33">[34]</ref>.</p><p>Modeling co-occurring visual patterns. It is known that modeling the spatial and geometric relationship between co-occurring visual patterns (objects or object-parts) often improves visual recognition performance <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>. Co-occurring patterns are usually represented as doublets <ref type="bibr" target="#b23">[24]</ref>, higher-order constellations <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b31">32]</ref> or star-shaped models <ref type="bibr" target="#b9">[10]</ref>. Among these, our work is most inspired by <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b31">32]</ref>, which learn part-based models with weak supervision. We use more informative deep CNN features and a different formulation, and show results on more difficult datasets. Our work is also related to <ref type="bibr" target="#b18">[19]</ref>, which discovers high-level object compositions ("visual phrases" <ref type="bibr" target="#b7">[8]</ref>), but with ground-truth bounding box annotations. In contrast, we aim to discover part compositions to represent full objects and do so with less supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>Our goal is to find a discriminative set of patches that co-occur in the same configuration in many positively-labeled images. We address this goal in two steps. First, we find a set of patches that are discriminative; i.e., they occur frequently in positive images and rarely in negative images. Second, we efficiently find co-occurring configurations of pairs of such patches. Our approach easily extends beyond pairs; for simplicity and to retain configurations that occur frequently enough, we here restrict ourselves to pairs. Discriminative candidate patches. For identifying discriminative patches, we begin with a construction similar to that of Song et al. <ref type="bibr" target="#b28">[29]</ref>. Let P be the set of positively-labeled images. Each image I contains candidate boxes {b I,1 , . . . , b I,m } found via selective search <ref type="bibr" target="#b29">[30]</ref>. For each b I,i , we find its closest matching neighbor b I ,j in each other image I (regardless of the image label). The K closest of those neighbors form the neighborhood N (b I,i ); the remaining ones are discarded.</p><p>Discriminative patches have neighborhoods mainly within images in P, i.e., if B(P) is the set of all patches from images in P, then |N (b) ∩ B(P)| ≈ K. To identify a small, diverse and representative set of such patches, like <ref type="bibr" target="#b28">[29]</ref>, we construct a bipartite graph G = (U, V, E), where both U and V contain copies of B(P). Each patch b ∈ V is connected to the copy of its nearest neighbors in U (i.e., N (b) ∩ B(P)). These will be K or fewer, depending on whether the K nearest neighbors of b occur in B(P) or in negatively-labeled images. The most representative patches maximize the covering function</p><formula xml:id="formula_0">F (S) = |Γ(S)|,<label>(1)</label></formula><p>where Γ(S) = {u ∈ U | (b, u) ∈ E for some b ∈ S} ⊆ U is the neighborhood of S ⊆ V in the bipartite graph. Figure <ref type="figure" target="#fig_0">1</ref> shows a cartoon illustration. The function F is monotone and submodular, and the C maximizing elements (for a given C) can be selected greedily <ref type="bibr" target="#b19">[20]</ref>.</p><p>However, if we aim to find part configurations, we must select multiple, jointly informative patches per image. Patches selected to merely maximize coverage can still be redundant, since the most frequently occurring ones are often highly overlapping. A straightforward modification would be to treat highly overlapping patches as identical. This identification would still admit a submodular cover model as in Equation ( <ref type="formula" target="#formula_0">1</ref>). But, in our case, the candidate patches are very densely packed in the image, and, by transitivity, we would have to make all of them identical. In consequence, this would completely rule out the selection of more than one patch in an image and thereby prohibit the discovery of any co-occurring configurations. Instead, we directly constrain our selection such that no two patches b, b ∈ V can be picked whose neighborhoods overlap by more than a fraction θ. By overlap, we mean that the patches in the neighborhoods of b, b overlap significantly (they need not be identical). This notion of diversity is reminiscent of NMS and similar to that in <ref type="bibr" target="#b4">[5]</ref>, but we here phrase and analyze it as a constrained submodular optimization problem. Our constraint can be expressed in terms of a different graph G C = (V, E C ) with nodes V. In G C , there is an edge between b and b if their neighborhoods overlap prohibitively, as illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. Our family of feasible solutions is</p><formula xml:id="formula_1">M = {S ⊆ V | ∀ b, b ∈ S there is no edge (b, b ) ∈ E C }.<label>(2)</label></formula><p>In other words, M is the family of all independent sets in G C . We aim to maximize</p><formula xml:id="formula_2">max S⊆V F (S) s.t. S ∈ M.<label>(3)</label></formula><p>V U The proof relies on phrasing M as an intersection of matroids. Definition 1 (Matroid). A matroid (V, I k ) consists of a ground set V and a family I k ⊆ 2 V of "independent sets" that satisfy three axioms: Proof. (Lemma 1) We will argue that Problem (2) is the problem of maximizing a monotone submodular function subject to the constraint that the solution lies in the intersection of ∆ + 1 matroids. With this insight, the approximation factor of the greedy algorithm for submodular F follows from <ref type="bibr" target="#b11">[12]</ref> and that for non-intersecting Γ(b) from <ref type="bibr" target="#b14">[15]</ref>, since in the latter case the problem is that of finding a maximum weight vector in the intersection of ∆ + 1 matroids.</p><p>It remains to argue that M is an intersection of matroids. Our matroids will be partition matroids (over the ground set V) whose independent sets are of the form</p><formula xml:id="formula_3">I k = {S | |S ∩ e| ≤ 1, for all e ∈ E k }.</formula><p>To define those, we partition the edges in G C into disjoint sets E k , i.e., no two edges in E k share a common node. The E k can be found by an edge coloring -one E k and I k for each color k. By Vizing's theorem <ref type="bibr" target="#b30">[31]</ref>, we need at most ∆+1 colors. The matroid I k demands that for each edge e ∈ E k , we may only select one of its adjacent nodes. All matroids together say that for any edge e ∈ E, we may only select one of the adjacent nodes, and that is the constraint in Equation (2), i.e. M = ∆+1 k=1 I k . We do not ever need to explicitly compute E k and I k ; all we need to do is check membership in the intersection, and this is equivalent to checking whether a set S is an independent set in G C , which is achieved implicitly via the deletions in the algorithm.</p><p>From the constrained greedy algorithm, we obtain a set S ⊂ V of discriminative patches. Together with its neighborhood Γ(b), each patch b ∈ V forms a representative cluster. Figure <ref type="figure" target="#fig_2">2</ref> shows some example patches derived from the labels "aeroplane" and "motorbike". The discovered patches intuitively look like "parts" of the objects, and are frequent but sufficiently different.</p><p>Finding frequent configurations. The next step is to find frequent configurations of co-occurring clusters, e.g., the head patch of a person on top of the torso patch, or a bicycle with visible wheels. A "configuration" consists of patches from two clusters C i , C j , their relative location, and their viewpoint and scale. In practice, we give preference to pairs that by themselves are very relevant and maximize a weighted combination of co-occurrence count and coverage max{Γ(C i ), Γ(C j )}.</p><p>All possible configurations of all pairs of patches amount to too many to explicitly write down and count. Instead, we follow an efficient procedure for finding frequent configurations. Our approach is inspired by <ref type="bibr" target="#b18">[19]</ref>, but does not require any supervision. We first find configurations that occur in at least two images. To do so, we consider each pair of images I 1 , I 2 that have at least two co-occurring clusters. For each correspondence of cluster patches across the images, we find a corresponding transform operation (translation, scale, viewpoint change). This results in a point in a 4D transform space, for each cluster correspondence. We quantize this space into B bins. Our candidate configurations will be pairs of cluster correspondences</p><formula xml:id="formula_4">((b I1,1 , b I2,1 ), (b I1,2 , b I2,2 )) ∈ (C i ×C i )×(C j ×C j )</formula><p>that fall in the same bin, i.e., share the same transform and have the same relative location. Between a given pair of images, there can be multiple such pairs of correspondences. We keep track of those via a multi-graph G P = (P, E P ) that has a node for each image I ∈ P. For each correspondence ((b I1,1 , b I2,1 ), (b I1,2 , b I2,2 )), we draw an edge (I 1 , I 2 ) and label it by the clusters C i , C j and the common relative position. As a result, there can be multiple edges (I 1 , I j ) in G P with different edge labels.</p><p>The most frequently occurring configuration can now be read out by finding the largest connected component in G P induced by retaining only edges with the same label. We use the largest component(s) as the characteristic configurations for a given image label (object class). If the component is very small, then there is not enough information to determine co-occurrences, and we simply use the most frequent single cluster. The final single "correct" localization will be the smallest bounding box that contains the full configuration.</p><p>Discovering mislocalized hard negatives. Discovering frequent configurations can not only lead to better localization estimates of the full object, but they can also be used to generate mislocalized estimates as "hard negatives" when training the object detector. We exploit this idea as follows. Let b 1 , b 2 be a discovered configuration within a given image. These patches typically constitute co-occurring parts or a part and the full object. Our foreground estimate is the smallest box that includes both b 1 and b 2 . Hence, any region within the foreground estimate that does not overlap simultaneously with both b 1 and b 2 will capture only a fragment of the foreground object. We extract the four largest such rectangular regions (see white boxes in Figure <ref type="figure" target="#fig_3">3</ref>) as hard negative examples. Specifically, we parameterize any rectangular region with [x l , x r , y t , y b ], i.e., its x-left, x-right, y-top, and y-bottom coordinate values. Let the bounding box of b</p><formula xml:id="formula_5">i (i = 1, 2) be [x l i , x r i , y t i , y b i ], the foreground estimate be [x l f , x r f , y t f , y b f ], and let x l = max(x l 1 , x l 2 ), x r = min(x r 1 , x r 2 ), y t = max(y t 1 , y t 2 ), y b = min(y b 1 , y b 2 )</formula><p>. We generate four hard negatives:</p><formula xml:id="formula_6">[x l f , x l , y b f , y t f ], [x r , x r f , y b f , y t f ], [x l f , x r f , y t f , y t ], [x l f , x r f , y b , y b f ]. If either b 1 or b 2 is</formula><p>very small in size relative to the foreground, the resulting hard negatives can have high overlap with the foreground, which will introduce undesirable noise (false negatives) when training the detector. Thus, we shrink any hard negative that overlaps with the foreground estimate by more than 50%, until its overlap is 50% (we adjust the boundary that does not coincide with any of the foreground estimation boundaries). Note that simply taking arbitrary rectangular regions that overlap with the foreground estimation box by some threshold will not always generate useful hard negatives (as we show in the experiments). If the overlap threshold is too low, the selected regions will be uninformative, and if the overlap threshold is too high, the selected regions will cover too much of the foreground. Our approach selects informative hard negatives more robustly by ruling out the overlapping region between the configuration patches, which is very likely be part of the foreground object but not the full object.</p><p>Mining positives and training the detector. While the discovered configurations typically lead to better foreground localization, their absolute count can be relatively low compared to the total number of positive images. This is due to inaccuracies in the initial patch discovery stage: for a frequent configuration to be discovered, both of its patches must be found accurately. Thus, we also mine additional positives from the set of remaining positive images P that did not produce any of the discovered configurations.</p><p>To do so, we train an initial object detector, using the foreground estimates derived from our discovered configurations as positive examples, and the corresponding discovered hard negative regions as negatives. In addition, we mine negative examples in negative images as in <ref type="bibr" target="#b9">[10]</ref>. We run the detector on all selective search regions in P and retain the region in each image with the highest detection score as an additional positive training example. Our final detector is trained on this augmented training data, and iteratively improved by latent SVM (LSVM) updates (see <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b28">29]</ref> for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we analyze: (1) detection performance of the models trained with the discovered configurations, and (2) impact of the discovered hard negatives on detection performance.</p><p>Implementation details. We employ a recent region based detection framework <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b28">29]</ref> and use the same fc7 features from the CNN model <ref type="bibr" target="#b5">[6]</ref> on region proposals <ref type="bibr" target="#b29">[30]</ref> throughout the experiments. For discriminative patch discovery, we use K = |P|/2, θ = K/20. For correspondence detection, we discretize the 4D transform space of {x: relative horizontal shift, y: relative vertical shift, s: relative scale, p: relative aspect ratio} with ∆x = 30 px, ∆y = 30 px, ∆s = 1 px/px, ∆p = 1 px/px. We chose this binning scheme by examining a few qualitative examples so that scale and aspect ratio agreement between the two paired instances are more strict, while their translation agreement is more loose, in order to handle deformable objects. More details regarding the transform space binning can be found in <ref type="bibr" target="#b21">[22]</ref>. Discovered configurations. Figure <ref type="figure" target="#fig_5">5</ref> shows the discovered configurations (solid green and yellow boxes) and foreground estimates (dashed magenta boxes) that have high degree in graph G P for all 20 classes in the PASCAL dataset. Our method consistently finds meaningful combinations such as a wheel and body of bicycles, face and torso of people, locomotive basement and upper body parts of trains/buses, and window and body frame of cars. Some failures include cases where the algorithm latches onto different objects co-occurring in consistent configurations such as the lamp and sofa combination (right column, second row from the bottom in Figure <ref type="figure" target="#fig_5">5</ref>).</p><p>Weakly-supervised object detection. Following the evaluation protocol of the PASCAL VOC dataset, we report detection results on the PASCAL test set using detection average precision. For a direct comparison with the state-of-the-art weakly-supervised object detection method <ref type="bibr" target="#b28">[29]</ref>, we do not use the extra instance level annotations such as pose, difficult, truncated and restrict the supervision to the image-level object presence annotations. Table <ref type="table" target="#tab_0">1</ref> compares our detection results against two baseline methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b28">29]</ref> on the full dataset. Our method improves detection performance on 15 of the 20 classes. It is worth noting that our method yields significant improvement on the person aero bike bird boat btl bus car cat chr cow tble dog horse mbk pson plnt shp sofa train tv mAP <ref type="bibr" target="#b24">[25]</ref> 13.4 44.0 3.1 3.1 0.0 31.2 43.9 7.1 0.1 9.3 9.9 1. <ref type="bibr" target="#b4">5</ref>   class, which is arguably the most important category in the PASCAL dataset. Figure <ref type="figure" target="#fig_4">4</ref> shows some example high scoring detections on the test set. Our method produces more complete detections since it is trained on better localized instances of the object-of-interest. Impact of discovered hard negatives. To analyze the effect of our discovered hard negatives, we compare to two baselines: (1) not adding any negative examples from positives images, and (2) adding image regions around the foreground estimate, as conventionally implemented in fully supervised object detection algorithms <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref>. For the latter, we use the criterion from <ref type="bibr" target="#b12">[13]</ref>, where all image regions in positive images with overlap score (intersection over union with respect to any foreground region) less than 0.3 are used as "neighboring" negative image regions on positive images. Table <ref type="table" target="#tab_1">2</ref> shows the effect of our hard negative examples on detection mean average precision for all classes (mAP). We also added neighboring negative examples to <ref type="bibr" target="#b28">[29]</ref>, but this decreases its mAP from 20.3% to 20.2% (before latent updates) and from 22.7% to 21.8% (after latent updates). These experiments show that adding neighboring negative regions does not lead to noticeable improvement over not adding any negative regions from positive images, while adding our automatically discovered hard negative regions improves detection performance more substantially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion.</head><p>We developed a weakly-supervised object detection method that discovers frequent configurations of discriminative visual patterns. We showed that the discovered configurations provide more accurate spatial coverage of the full object and provide a way to generate useful hard negatives. Together, these lead to state-of-the-art weakly-supervised detection results on the challenging PASCAL VOC dataset. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Left: bipartite graph G that defines the utility function F and identifies discriminative patches; right: graph G C that defines the diversifying independence constraints M. We may pick C 1 (yellow) and C 3 (green) together, but not C 2 (red) with any of those.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( 1 )</head><label>1</label><figDesc>∅ ∈ I k ; (2) downward closedness: if S ∈ I k then T ∈ I k for all T ⊆ S; and (3) the exchange property: if S, T ∈ I k and |S| &lt; |T |, then there is an element v ∈ T \ S such that S ∪ {v} ∈ I k .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Examples of discovered patch "clusters" for aeroplane, motorbike, and cat. The discovered patches intuitively look like object parts, and are frequent but sufficiently different.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Automatically discovered foreground estimation box (magenta), hard negative (white), and the patch (yellow) that induced the hard negative. Note that we are only showing the largest one out of (up to) four hard negatives per image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: Example detections on test set. Green: our method, red:<ref type="bibr" target="#b28">[29]</ref> </figDesc><graphic coords="7,407.33,371.91,90.17,56.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Example configurations that have high degree in graph G P . The solid green and yellow boxes show the discovered discriminative visual parts, and the dashed magenta box shows the bounding box that tightly fits their configuration.</figDesc><graphic coords="8,310.08,601.74,90.17,55.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>29.4 38.3 4.6 0.1 0.4 3.8 34.2 0.0 13.9 [29] 27.6 41.9 19.7 9.1 10.4 35.8 39.1 33.6 0.6 20.9 10.0 27.7 29.4 39.2 9.1 19.3 20.5 17.1 35.6 7.1 22.7 ours 1 31.9 47.0 21.9 8.7 4.9 34.4 41.8 25.6 0.3 19.5 14.2 23.0 27.8 38.7 21.2 17.6 26.9 12.8 40.1 9.2 23.4 ours 2 36.3 47.6 23.3 12.3 11.1 36.0 46.6 25.4 0.7 23.5 12.5 23.5 27.9 40.9 14.8 19.2 24.2 17.1 37.7 11.6 24.6 Detection average precision (%) on full PASCAL VOC 2007 test set. ours 1 : before latent updates. ours 2 : after latent updates</figDesc><table><row><cell></cell><cell>w/o hard negatives</cell><cell>neighboring hard negatives</cell><cell>discovered hard negatives</cell></row><row><cell>ours + SVM</cell><cell>22.5</cell><cell>22.2</cell><cell>23.4</cell></row><row><cell>ours + LSVM</cell><cell>23.7</cell><cell>23.9</cell><cell>24.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Effect of our hard negative examples on full PASCAL VOC 2007 test set.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. This work was supported in part by DARPA's MSEE and SMISC programs, by NSF awards IIS-1427425, IIS-1212798, IIS-1116411, and by support from Toyota.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">On detection of multiple object instances using hough transforms</title>
		<author>
			<persName><forename type="first">O</forename><surname>Barinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>IEEE TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Active detection via adaptive submodularity</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shioi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fuentes-Montesinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Localizing objects while learning their appearance</title>
		<author>
			<persName><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weakly supervised localization and learning with generic knowledge</title>
		<author>
			<persName><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">What Makes Paris Look like Paris</title>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<title level="m">DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Clustering by Composition Unsupervised Discovery of Image Categories</title>
		<author>
			<persName><forename type="first">A</forename><surname>Faktor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recognition Using Visual Phrases</title>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sadeghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<title level="m">A Discriminatively Trained, Multiscale, Deformable Part Model</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Object Detection with Discriminatively Trained Part Based Models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Object Class Recognition by Unsupervised Scale-Invariant Learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An analysis of approximations for maximizing submodular set functions -II</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Nemhauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolsey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Prog. Study</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="73" to="87" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised learning of categories from sets of partially matching image features</title>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The efficacy of the &quot;greedy&quot; algorithm</title>
		<author>
			<persName><forename type="first">T</forename><surname>Jenkyns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 7th South Eastern Conference on Combinatorics, Graph Theory and Computing</title>
		<meeting>of 7th South Eastern Conference on Combinatorics, Graph Theory and Computing</meeting>
		<imprint>
			<date type="published" when="1976">1976</date>
			<biblScope unit="page" from="341" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Blocks that Shout: Distinctive Parts for Scene Classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Juneja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Foreground Focus: Unsupervised Learning From Partially Matching Images</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatic Discovery of Groups of Objects for Scene Understanding</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An analysis of approximations for maximizing submodular set functions-I</title>
		<author>
			<persName><forename type="first">G</forename><surname>Nemhauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="265" to="294" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Scene recognition and weakly supervised object localization with deformable part-based models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">From Appearance to Context-Based Recognition: Dense Labeling in Small Images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient Mining of Frequent and Distinctive Feature Configurations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Quack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised Discovery of Mid-level Discriminative Patches</title>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Weakly supervised object detector learning with model drift detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">In defence of negative mining for annotating weakly labelled data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Video Data Mining Using Configurations of Viewpoint Invariant Regions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Discovering object categories in image collections</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On learning to localize objects with minimal supervision</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Selective search for object recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
		<editor>IJCV</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On an estimate of the chromatic class of a p-graph</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vizing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Diskret. Analiz</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="25" to="30" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised Learning of Models for Recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Efficient Kernels for Identifying Unbounded-order Spatial Features</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Contrastive learning using spectral methods</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
