<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Augmentations in Hypergraph Contrastive Learning: Fabricated and Generative</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tianxin</forename><surname>Wei</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
							<email>yuning.you@tamu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Texas A&amp;M University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
							<email>tianlong.chen@utexas.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
							<email>yshen@tamu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Texas A&amp;M University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jingrui</forename><surname>He</surname></persName>
							<email>jingrui@illinois.edu</email>
						</author>
						<author>
							<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Augmentations in Hypergraph Contrastive Learning: Fabricated and Generative</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper targets at improving the generalizability of hypergraph neural networks in the low-label regime, through applying the contrastive learning approach from images/graphs (we refer to it as HyperGCL). We focus on the following question: How to construct contrastive views for hypergraphs via augmentations? We provide the solutions in two folds. First, guided by domain knowledge, we fabricate two schemes to augment hyperedges with higher-order relations encoded, and adopt three vertex augmentation strategies from graph-structured data. Second, in search of more effective views in a data-driven manner, we for the first time propose a hypergraph generative model to generate augmented views, and then an end-to-end differentiable pipeline to jointly learn hypergraph augmentations and model parameters. Our technical innovations are reflected in designing both fabricated and generative augmentations of hypergraphs. The experimental findings include: (i) Among fabricated augmentations in HyperGCL, augmenting hyperedges provides the most numerical gains, implying that higher-order information in structures is usually more downstream-relevant; (ii) Generative augmentations do better in preserving higher-order information to further benefit generalizability; (iii) HyperGCL also boosts robustness and fairness in hypergraph representation learning. Codes are released at https://github.com/weitianxin/HyperGCL. N e=1 N e possibilities for one hyperedge on N vertices, versus N 2 for one edge in graphs. To date, the only way of contrasting is between the representations * Equal contribution. 36th Conference on Neural Information Processing Systems (NeurIPS 2022).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Hypergraphs have raised a surge of interests in the research community <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref> due to their innate capability of capturing higher-order relations <ref type="bibr" target="#b3">[4]</ref>. They offer a powerful tool to model complicated topological structures in broad applications, e.g., recommender systems <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, financial analyses <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, and bioinformatics <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10]</ref>. Concomitant with the trend, hypergraph neural networks (HyperGNNs) have recently been developed <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref> for hypergraph representation learning. This paper focuses on the few-shot scenarios of hypergraphs, i.e., task-specific labels are scarce, which are ubiquitous in real-world applications of hypergraphs <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9]</ref> and empirically restrict the generalizability of HyperGNNs. Inspired by the emerging self-supervised learning on images/graphs <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>, especially the contrastive approaches <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>, we set out to leverage contrastive self-supervision to address the problem.</p><p>Nevertheless, one challenge stands out: How to build contrastive views for hypergraphs? The success of contrastive learning hinges on the appropriate view construction, otherwise it would result in "negative transfer" <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14]</ref>. However, it is non-trivial to build hypergraph views due to their overly intricate topology, i.e., there are of hypergraphs and their clique-expansion graphs <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>, which is computationally expensive as multiple neural networks of different modalities (hypergraphs and variants of expanded graphs) need to be optimized. More importantly, contrasting on clique expansion has the risk of losing higher-order information via pulling representations of hypergraphs and graphs close.</p><p>Contributions. Motivated by <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14]</ref> that appropriate data augmentations suffice for the effective contrastive views, and intuitively they are more capable of preserving higher-order relations in hypergraphs compared to clique expansion, we explore on the question in this paper, how to design augmented views of hypergraphs in contrastive learning (HyperGCL). Our answers are in two folds.</p><p>We first assay whether fabricated augmentations guided by domain knowledge are suited for Hy-perGCL. Since hypergraphs are composed of hyperedges and vertices, to augment hyperedges, we propose two strategies that (i) directly perturb on hyperedges, and (ii) perturb on the "edges" between hyperedges and vertices in the converted bipartite graph; To augment vertices, we adopt three schemes of vertex dropping, attribute masking and subgraph from graph-structured data <ref type="bibr" target="#b13">[14]</ref>. Our finding is that, different from the fact that vertex augmentations benefit more on graphs, hypergraphs mostly benefit from hyperedge augmentations (up to 9% improvement), revealing that higher-order information encoded in hyperedges is usually more downstream-relevant (than information in vertices).</p><p>Furthermore, in search of even better augmented views but in a data-driven manner, we study whether/how augmentations of hypergraphs could be learned during contrastive learning. To this end, for the first time, we propose a novel variational hypergraph auto-encoder architecture, as a hypergraph generative model, to parameterize a certain augmentation space of hypergraphs. In addition, we propose an end-to-end differentiable pipeline utilizing Gumbel-Softmax <ref type="bibr" target="#b27">[28]</ref>, to jointly learn hypergraph augmentations and model parameters. Our observation is that generative augmentations can better capture the higher-order information and achieve state-of-the-art performance on most of the benchmark data sets (up to 20% improvement).</p><p>The aforementioned empirical evidences (for generalizability) are drawn from comprehensive experiments on 13 datasets. Moreover, we introduce the robustness and fairness evaluation for hypergraphs, and show that HyperGCL in addition boosts robustness against adversarial attacks and imposes fairness with regard to sensitive attributes.</p><p>The rest of the paper is organized as follows. We discuss the related work in Section 2, introduce HyperGCL in Section 3, present the experimental results in Section 4, and conclude in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Hypergraph neural networks. Hypergraphs, which are able to encode higher-order relationships, have attracted significant attentions in recent years. In the machine learning community, hypergraph neural networks are developed for effective hypergraph representations. HGNN <ref type="bibr" target="#b0">[1]</ref> adopt the clique expansion technique and designs the weighted hypergraph Laplacian for message passing. HyperGCN <ref type="bibr" target="#b1">[2]</ref> proposes the generalized hypergraph Laplacian and explores adding the hyperedge information through mediators. The attention mechanism <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref> is also designed to learn the importance within hypergraphs. However, the expanded graph will inevitably cause distortion and lead to unsatisfactory performance. There is also another line of works such as UniGNN <ref type="bibr" target="#b30">[31]</ref> and HyperSAGE <ref type="bibr" target="#b31">[32]</ref> which try to perform message passing directly on the hypergraph to avoid the information loss. A recent work <ref type="bibr" target="#b2">[3]</ref> provides an AllSet framework to unify the existing studies with high expressive power and achieves state-of-the-art performance on comprehensive benchmarks. The work utilizes deep multiset functions <ref type="bibr" target="#b32">[33]</ref> to identify the propagation and aggregation rules in a data-driven manner.</p><p>Contrastive self-supervised learning. Contrastive self-supervision <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref> has achieved unprecedented success in computer vision. The core idea is to learn an embedding space where samples from the same instance are pulled closer and samples from different instances are pushed apart. Recent works start to cross-pollinate between contrastive learning and graph neural networks to for more generalizable graph representations. Typically, they design some fabricated augmentations guided by domain knowledge, such as edge perturbation, feature masking or vertex dropping, etc. Nevertheless, contrastive learning on hypergraphs remains largely unexplored. Most existing works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b36">37]</ref> design pretext tasks for hypergraphs and mainly focus on recommender systems <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref>, via contrasting between graphs and hypergraphs which might lose important higher-order information. In this work, we explore on the structure of hypergraph itself to construct contrastive views. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Hypergraph Contrastive Learning</head><p>A hypergraph is denoted as G = {V, E} ∈ G where V = {v 1 , ..., v |V| } is the set of vertices and E = {e 1 , ..., e |E| } is the set of hyperedges. Each hyperedge e n = {v 1 , ..., v |en| } represents the higher-order interaction among a set of vertices. State-of-the-art approaches to encode such complex structures are hypergraph neural networks (HyperGNNs) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>, mapping the hypergraph to a D-dimension latent space via f : G → R D with higher-order message passing.</p><p>Motivated from learning on images/graphs, we adopt contrastive learning to further improve the generalizability of HyperGNNs in the low-label regime (HyperGCL). Main components of our Hy-perGCL, similar to images/graphs <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14]</ref> include: (i) hypergraph augmentations for contrastive views, (ii) HyperGNNs as hypergraph encoders, (iii) projection head h(•) for representations, and (iv) contrastive loss for optimization. The overall pipeline is shown in Figure <ref type="figure" target="#fig_0">1</ref>. Detailed descriptions and training procedure are shown in Appendix B. The main challenge here is how to effectively augment hypergraphs to build contrastive views. We first explore whether manually designed augmentations are suited for Hyper-GCL. Since hyperedges and vertices compose a hypergraph, augmentations are fabricated with regards to topology and node features, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Fabricated Augmentations for Hypergraphs</head><note type="other">Vertex Hyperedge Transform</note><p>A1. Perturbing hyperedges. The most direct augmentation on higher-order interactions is to perturb on the set of hyperedges. Since adding a hyperedge is confronted with the combinatorial challenge (see Sec. 1 of introduction), here we focus on randomly removing the existing hyperedges following an i.i.d. Bernoulli distribution. The underlying assumption is that the partially missing higher-order relations do not significantly affect the semantic meaning of hypergraphs.</p><p>A2. Perturbing edges in equivalent bipartite graph. To augment higher-order relations in a more fine-grained way, we first convert the hypergraph into the equivalent bipartite graph, where two disjoint sets of vertices represent vertices and hyperedges in the hypergraph, respectively (see Figure <ref type="figure" target="#fig_1">2</ref>). On top of the bipartite graph, we perform random removal of edges. A2 disrupts the higher-order relations via randomly kicking out vertices from hyperedges, enforcing the semantics of hypergraph representations to be robust to such disruption. A2 is essentially the generalized version of A1.</p><p>Moreover, we find that vertex augmentations for graph-structured data <ref type="bibr" target="#b13">[14]</ref> are applicable to hypergraphs. Therefore, we adopt three additional schemes of vertex dropping (A3), attribute masking (A4) and subgraph (A5) into our experiments, with similar prior knowledge incorporated as in <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Generative Models for Hypergraph Augmentations</head><p>Manually designing augmentation operators requires a wealth of domain knowledge, and might lead to sub-optimal solutions even with extensive trial-and-errors. We next study whether/how augmentations of hypergraphs could be learned during contrastive learning. Two questions need to be answered here: (i) How to parameterize the augmentation space of hypergraphs? (ii) How to incorporate the learnable augmentations into contrastive learning?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Hypergraph Generative Models for Augmentations</head><p>Considering an augmentation operator defined as the stochastic mapping between two hypergraph manifolds that g : G → G, a natural thought is to adopt the generative model to parameterize the augmentation space, which in general is composed of a deterministic encoder</p><formula xml:id="formula_0">h 1 : G → R D ′</formula><p>and a stochastic decoder (or sampler)</p><formula xml:id="formula_1">h 2 : R D ′ → G. In this way, g = h 1 • h 2 .</formula><p>Following this thought, inspired by the well-studied generative models with variational inference <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref>, we propose a novel variational hypergraph auto-encoder architecture (VHGAE). To the best of our knowledge, this is the first hypergraph generative model for generating augmentations of hypergraphs, which will be used as A6. Notice that here it only parametrizes the augmentation space of edge perturbation, and in the future node perturbation would be included. VHGAE consists of the encoder and decoder neural networks. The overall framework is shown in Figure <ref type="figure" target="#fig_2">3</ref>.</p><p>Encoder. The encoder embeds hypergraphs into latent representations. Instead of embedding a hypergraph into a single vector, we follow VGAE <ref type="bibr" target="#b42">[43]</ref> to embed it into a set of vertex and in additional hyperedge representations, to facilitate the further decoding process of non-Euclidean structures. We adopt two HyperGNNs, h µ 1 and h σ 1 , to encode the mean and the logarithmic standard deviation for variational distributions of vertex and hyperedge representations z</p><formula xml:id="formula_2">V ∼ q ϕ (z V |G) = N (µ V , σ 2 V ), z E ∼ q ϕ (z E |G) = N (µ E , σ<label>2</label></formula><p>E ) as follows (please refer to Appendix B for the detailed computing pipeline):</p><formula xml:id="formula_3">µ V , µ E = h µ 1 (G), log(σ V ), log(σ E ) = h σ 1 (G),<label>(1)</label></formula><p>where µ ∈ R D ′ ×|V| , log(σ) ∈ R D ′ ×|V| . We here leverage the higher-order message passing in HyperGNNs for a better encoding capability.</p><p>Decoder. With the learned vertex and hyperedge variational distributions, the decoder attempts to reconstruct the higher-order relations of hypergraphs. However, modeling the space of higher-order interactions encounters the combinatorial challenge (see Section 1). Adopting the similar strategy as in the augmentation A2 (see Section 3.2), we designate the decoder to recover the relations on the converted bipartite graph G = { Ṽ, Ẽ} for approximation. Mathematically, we formulate decoding as:</p><formula xml:id="formula_4">p(G|z V , z E ) ≈ p( G|z V , z E ) = |E| e=1 |V| v=1 p( Ẽv,e |z v , z e ) = |E| e=1 |V| v=1 Sigmoid(z T v z e ),<label>(2)</label></formula><p>where w ve = z T v z e is the learned edge logit. On the decoded topological distribution of the bipartite graph, we perform sampling and then convert the sample back to the hypergraph (the conversion between hypergraphs and bipartite graphs is lossless).</p><p>Generator optimization. With variational inference <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45]</ref>, we optimize the hypergraph generator on the evidence lower bound (ELBO) as follows: where q ϕ (z E |G) and q ϕ (z V |G) are their variational distribution, p(z V ) and p(z E ) are default Gaussian priors with p(z V ) ∼ N (0, I), p(z E ) ∼ N (0, I). When generating hypergraphs, the generator would sample the relations on the converted bipartite graph with probability p( G|z V , z E ).</p><formula xml:id="formula_5">ELBO = E q ϕ (z E |G) E q ϕ (z V |G) [log p θ (G|z v , z e )] − KL[q ϕ (z V | G) | p(z V )] − KL[q ϕ (z E | G) | p(z E )],<label>(3)</label></formula><note type="other">Encoder Sample Decoder</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Jointly Augmenting and Contrasting with Gumbel-Softmax</head><p>With hypergraph augmentations parametrized with generative models, the next step is to incorporate augmentation learning into HyperGCL. The main barrier results from the discrete sampling of hyperedges which is non-differentiable. To tackle it, we leverage the Gumbel-Softmax trick <ref type="bibr" target="#b27">[28]</ref> for the hyperedge distribution as:</p><formula xml:id="formula_6">T (G) = Gumbel-Softmax(p(G | z V , z E )) = Sigmoid((w VE + log(δ) − log(1 − δ))/τ ) Ggen = T (G) • G,<label>(4)</label></formula><p>where w VE denotes the learned edge logits (before Sigmoid) and δ ∼ Uniform(0, 1). When hyperparameter temperature τ → 0, the results get closer to being binary. T is the sampled one-hot vector for each hyperedge-vertex interaction in the hypergraph G. Then the sampled vector will be applied to perform augmentation. During the Gumbel-Softmax, we leverage the reparametrization trick <ref type="bibr" target="#b41">[42]</ref> to smooth the gradient and make the sample operation differentiable. Thus, this objective can be optimized in an end-to-end manner as:</p><formula xml:id="formula_7">min ϕ L gen (ϕ) − β • L cl (G, Ggen | θ, ϕ),<label>(5)</label></formula><p>where L gen = −ELBO is the generator loss to be minimized, β is the tradeoff factor. Due to the computational cost of collaboratively optimizing two generative views, we train one VHGAE to produce one generative view, with the other view Gp is kept as fabricated. To be specific, (i) independently optimizing two hypergraph generators is of reasonable budgets but would lead to distribution collapse (i.e., two hypergraph generators output the same distribution) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> which results in less effective generative views, while (ii) the collaborative optimization techniques for graph generators (e.g. REINFORCE on the rewards of generative graph structures) are not directly applicable to HyperGCL due to the combinatorial challenge of hypergraph structures (which is computationally expensive). The goal of this multi-task loss is to generate stronger augmentation (maximize contrastive loss) to push HyperGNN to avoid capturing redundant information during the representation learning, while at the same time learning the hypergraph data distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>We examine our methods on the most comprehensive hypergraph benchmarks with 13 data sets, with statistics shown in Table <ref type="table" target="#tab_0">1</ref>. Please refer to Appendix C for detailed information. We focus on semi-supervised vertex classification in the transductive setting. Different from the existing work  <ref type="bibr" target="#b2">[3]</ref> that leverages 50% of all vertexes as the training set, we focus on the more low-label regime of challenging and practical applications. By default, we split the data into training/validation/test samples using (10%/10%/80%) splitting percentages. Each experiment is run for 20 different data splits and initialization with mean and standard deviation reported. We adopt state-of-the-art SetGNN <ref type="bibr" target="#b2">[3]</ref> as the backbone HyperGNN architecture. For baselines, we compare two existing hypergraph self-supervised approaches <ref type="bibr" target="#b35">[36]</ref> and <ref type="bibr" target="#b25">[26]</ref> in recommender systems, denoted as Self and Con.  <ref type="table" target="#tab_3">3</ref>. In general, generalized hyperedge augmentation (A2) works the best among fabricated augmenting operators, but not naïvely perturbing hyperedge (A1). Specifically, among all fabricated augmentations, A2 performs the best in 10 of 13 data sets. This indicates the nature that higherorder information in structures is usually more downstream-relevant.</p><p>For our generative augmentation (A6), we find it performs the best in all the data sets. In our joint augmenting and contrasting framework, we generate stronger augmentation while keeping the hypergraph distribution with adversarial learning. This illustrates the importance of exploring the hypergraph structure. We also test our method on 1% label setting in Table <ref type="table" target="#tab_4">4</ref>. In this setting, Zoo and NTU2012 data sets are not shown because of the extremely small data size (each case has less than one training sample). We can find that in the 1% label setting A4 (mask) method performs the best in Cora, Citeseer and DBLP-CA. These data sets are all originally graphs, and are constructed as hypergraphs in different ways. So on these data sets, relatively little higher-order structural information can be explored with hypergraph structure perturbation-based contrastive learning.</p><p>Comparison between multi-task learning and pretraining. We then compare the multi-task training method with the pretraining method in Table <ref type="table" target="#tab_5">5</ref>. Pretrain_L adopts the linear evaluation protocol where a linear classifier is trained on top of the fixed pretrained representations. Pretrain_F follows a fully finetuning protocol that uses the weights of the learned hypergnn encoder as initialization while finetuning all the layers. MTL denotes the multi-task learning method which trains the supervised classification loss and contrastive loss together. For all the methods, we use A2 (generalized hyperedge perturbation) as it performs the best among fabricated augmentations. From the table, we find MTL achieves the best performance in nearly all data sets. Pretrain_L and Pretrain_F can only obtain better performance on two small data sets: Zoo and House. We find on most data sets, Pretrain_L makes the model perform worse, which shows that the linear classifier is not enough to represent the higher-order information in the hypergraph. Pretrain_F has a much better performance compared  with Pretrain_L, which indicates the effects of using contrastive learning. However, the method switches the objective during finetuning, which would lead to the memorization problem and the loss of pre-trained knowledge. Therefore, all our other experiments adopt MTL setting to incorporate contrastive self-supervision.</p><p>Comparison to the converted graph. Next we investigate on which graph should we contrast on. We first convert the original hypergraph into a conventional graph using the clique expansion technique, and we choose the representative HGNN <ref type="bibr" target="#b0">[1]</ref> as the backbone network for learning on the converted graph. We compare it with two representative augmentations: A2 (Here, edge perturbation on the conventional graph) and A4 (feature masking). In Table <ref type="table" target="#tab_6">6</ref>, we find that HGNN performs poorly compared with SetGNN. Apparently contrastive self-supervision on converted graphs does not bring much benefit. The reason is that part of structural information, important to hypergraph representation learning, is lost when hypergraphs are converted to graphs. These results indicate the importance of designing HyperGNN and contrastive strategies directly on hypergraphs.  Analysis of generative augmentation. Here we analyze our proposed generative hypergraph augmentation (A6). We select ModelNet40 and Yelp as the representatives of high-homophily and low-homophily data sets, respectively.</p><p>First, we examine the training dynamics of keep ratio and find that they are highly related to the data set homophily (Figure <ref type="figure" target="#fig_3">4</ref> (a)). For ModelNet40, the generator keeps only 20% relations in the early training stage. This is because the homophily of the data set is very high (0.92/0.88) and deleting a lot of edges relatively randomly at the beginning would not have a large impact on the model but rather can help model learn structural information. Then at a later stage, the generator begins to keep more than 80% relations, which means that the model has learned higher-order information and only removes the unnecessary relations. For Yelp, a similar conclusion holds. Specifically, as its homophily is pretty low (0.57/0.26), the generator keeps most of the relations at the early stage for training and then just keeps a very low ratio of related relations at the later training stage.</p><p>Next, we investigate what our generator has learned. We visualize the hyperedges in the Yelp data set in Figure <ref type="figure" target="#fig_3">4</ref> (b). Yelp is a restaurant-rating data set and the restaurants visited by the same user are connected by a hyperedge. We find that some vertices with different labels are removed, which could remove extraneous information and improve the hypergraph homophily. This indicates that our generator does grasp the higher-order information in the hypergraph.</p><p>Adversarial robustness. Besides generalizability, we here show hypergraph contrastive learning also boosts robustness. Since there is no existing work developing adversarial attack algorithms designated for hypergraphs, we adapt two state-of-the-art attackers from the graph domain. We presume the graph attackers are applicable to hypergraphs to a certain extent, both of which are non-Euclidean data structures with sharing properties. The experiments are performed on five real-world data sets: Cora, Citeseer, ModelNet40, NTU2012 and House. We regard each hypergraph as a bipartite graph and leverage those algorithms to conduct attacks to the vertices and hyperedges. The methods include an untargeted attack method, minmax attack <ref type="bibr" target="#b45">[46]</ref> which poisons the graph structures by adding and removing relations to reduce the overall performance; and a targeted attack method, nettack <ref type="bibr" target="#b46">[47]</ref> which leads the HyperGNN to mis-classify target vertices. Beyond these, we also include a random hypergraph perturbation baseline which will randomly drop numerous relations in the hypergraph. These three methods are denoted as Net, Minmax and Random. Following previous works, for each attack, we perturb 10% of vertices/relations. The results in Table <ref type="table" target="#tab_7">7</ref> show that Random and Minmax attacks can decrease the performance of the original model a little on all the data sets, while net attack can decrease the performance on most data sets and surprisingly increase the performance on  the House data set. This performance gain indicates HyperGNN is more robust to structure attack compared with GNN as it leverages higher-order information. Based on these, HyperGCL with generalized hyperedge augmentation (A2) performs better than feature perturbation (A4), and our proposed generative augmentation (A6) can surpass these fabricated baselines on all the data sets and thus is the best to defend attacks. We believe this'll be a beneficial complement to our main experiments and we hope for more works on hypergraph attacks.</p><p>Fairness. Furthermore, we claim that hypergraph contrastive self-supervision also benefits fairness.</p><p>There was no related data set before. So we introduce three newly curated hypergraph data sets: German <ref type="bibr" target="#b47">[48]</ref>, Recidivism <ref type="bibr" target="#b48">[49]</ref> and Credit <ref type="bibr" target="#b49">[50]</ref>. The hypergraph construction follows the setting in <ref type="bibr" target="#b0">[1]</ref>. The top 5 similar objects in each data set are built as a hyperedge. For the accuracy metrics, we use F1-score and AUROC value for the binary classification task. For measuring fairness, we adopt the statistical parity ∆ SP and equalized odds ∆ EO . Please refer to Appendix C for detailed information about the data sets and metrics. The experimental results in Table <ref type="table" target="#tab_8">8</ref> show that our generative method still achieves better or comparable performances while imposing more fairness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In the paper, we study the problem of how to construct contrastive views of hypergraphs via augmentations. We provide the solutions by first studying domain knowledge-guided fabrication schemes. Then, in search of more effective views in a data-driven manner, we are the first to propose hypergraph generative models to generate augmented views, as well as an end-to-end differentiable pipeline to jointly perform hypergraph augmentation and contrastive learning. We find that generative augmentations perform better at preserving higher-order information to further benefit generalizability. The proposed framework also boosts robustness and fairness of hypergraph representation learning. In the future, we plan to design more powerful hypergraph generator and HyperGNN while addressing more real-world hypergraph data challenges and more hypergraph learning models. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>×Figure 1 :</head><label>1</label><figDesc>Figure 1: The framework of hypergraph contrastive learning (HyperGCL). The ellipses represent the hyperedges. Two contrastive views are generated by hypergraph augmentations A1 and A2 from the augmentation collection A. f (•) and h(•) are shared encoder and projection head respectively. In the figure, we show two examples of hypergraph augmentations. At the top, the dotted ellipse denotes the deleted hyperedge. At the bottom, one vertex in the dotted hyperedge is removed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Conversion from hypergraph to equivalent bipartite graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Framework of the proposed variational hypergraph auto-encoder (VHGAE). The green lines indicate these modules participated in the optimization process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: (a) Training dynamics of the relation keep ratio. (b) Illustration of our proposed generative augmentation on the Yelp data set. Each icon represents a restaurant in the data set, and the number near the icon is the label of this restaurant. The ellipse denotes the hyperedge.</figDesc><graphic url="image-39.png" coords="9,131.57,74.08,171.34,114.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Data statistics: h e , h v are the node homophily and hyperedge homophily in hypergraph. Higher value indicate the hypergraph is more homogeneous. Details can be found in Appendix C.</figDesc><table><row><cell></cell><cell cols="13">Cora Citeseer Pubmed Cora-CA DBLP-CA Zoo 20News Mushroom NTU2012 ModelNet40 Yelp House Walmart</cell></row><row><cell>|V|</cell><cell cols="2">2708 3312</cell><cell>19717</cell><cell>2708</cell><cell>41302</cell><cell cols="2">101 16242</cell><cell>8124</cell><cell>2012</cell><cell>12311</cell><cell cols="3">50758 1290 88860</cell></row><row><cell>|E|</cell><cell cols="2">1579 1079</cell><cell>7963</cell><cell>1072</cell><cell>22363</cell><cell>43</cell><cell>100</cell><cell>298</cell><cell>2012</cell><cell>12311</cell><cell cols="2">679302 341</cell><cell>69906</cell></row><row><cell cols="3"># feature 1433 3703</cell><cell>500</cell><cell>1433</cell><cell>1425</cell><cell>16</cell><cell>100</cell><cell>22</cell><cell>100</cell><cell>100</cell><cell>1862</cell><cell>100</cell><cell>100</cell></row><row><cell># class</cell><cell>7</cell><cell>6</cell><cell>3</cell><cell>7</cell><cell>6</cell><cell>7</cell><cell>4</cell><cell>2</cell><cell>67</cell><cell>40</cell><cell>9</cell><cell>2</cell><cell>11</cell></row><row><cell>he</cell><cell>0.86</cell><cell>0.83</cell><cell>0.88</cell><cell>0.88</cell><cell>0.93</cell><cell cols="2">0.66 0.73</cell><cell>0.96</cell><cell>0.87</cell><cell>0.92</cell><cell>0.57</cell><cell>0.58</cell><cell>0.75</cell></row><row><cell>hv</cell><cell>0.84</cell><cell>0.78</cell><cell>0.79</cell><cell>0.79</cell><cell>0.88</cell><cell cols="2">0.35 0.49</cell><cell>0.87</cell><cell>0.81</cell><cell>0.88</cell><cell>0.26</cell><cell>0.52</cell><cell>0.55</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>They conduct self-supervised learning between the hypergraph and conventional graph. By default, we adopt multi-task training to incorporate contrastive self-supervision because it performs the best as shown in the comparison in Section 4.2. All the implementation details are listed in Appendix C. More experiments of the hyperparameters study are given in Appendix A. The proposed augmentation operations for contrastive learning framework HyperGCL and their corresponding names.</figDesc><table><row><cell>4.2 Results</cell><cell></cell></row><row><cell>Comparison among different hypergraph aug-</cell><cell></cell></row><row><cell>mentations. The augmentation operations are</cell><cell></cell></row><row><cell>summarized in</cell><cell></cell></row><row><cell cols="2">Name Operation</cell></row><row><cell>A0</cell><cell>Identity</cell></row><row><cell>A1</cell><cell>Naïve Hyperedge Perturbation</cell></row><row><cell>A2</cell><cell>Generalized Hyperedge Perturbation</cell></row><row><cell>A3</cell><cell>Vertex Dropping</cell></row><row><cell>A4</cell><cell>Attribute Masking</cell></row><row><cell>A5</cell><cell>Subgraph</cell></row><row><cell>A6</cell><cell>Generative Augmentation</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Please refer to Appendix C.5 for detailed descriptions. We first conduct experiments to compare different contrastive operations on hypergraphs, with results shown in Table</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results on the test data sets: Mean accuracy (%) ± standard deviation. Bold values indicate the best result. Underlined values indicate the second best. 10% of all vertices are used for training.</figDesc><table><row><cell></cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell><cell>Cora-CA</cell><cell>DBLP-CA</cell><cell>Zoo</cell><cell>20Newsgroups Mushroom</cell></row><row><cell cols="8">SetGNN 67.93 ± 1.27 63.53 ± 1.32 84.33 ± 0.36 72.21 ± 1.51 89.51 ± 0.18 65.06 ± 12.82 79.37 ± 0.35 99.75 ± 0.11</cell></row><row><cell>Self</cell><cell cols="7">68.24 ± 1.12 62.49 ± 1.48 84.38 ± 0.38 72.74 ± 1.53 89.51 ± 0.23 57.35 ± 18.32 79.45 ± 0.32 95.83 ± 0.23</cell></row><row><cell>Con</cell><cell cols="7">68.89 ± 1.80 62.82 ± 1.21 84.56 ± 0.34 73.22 ± 1.65 89.59 ± 0.13 61.05 ± 14.54 79.49 ± 0.45 95.85 ± 0.31</cell></row><row><cell>A0</cell><cell cols="7">68.59 ± 1.33 62.25 ± 2.15 84.54 ± 0.42 71.85 ± 1.62 89.62 ± 0.24 62.57 ± 13.84 79.07 ± 0.46 99.77 ± 0.17</cell></row><row><cell>A1</cell><cell cols="7">72.39 ± 1.34 66.28 ± 1.27 85.17 ± 0.37 75.45 ± 1.54 89.83 ± 0.21 65.80 ± 13.31 79.47 ± 0.32 99.80 ± 0.14</cell></row><row><cell>A2</cell><cell cols="7">72.58 ± 1.09 66.40 ± 1.35 85.16 ± 0.38 75.62 ± 1.42 90.22 ± 0.23 66.35 ± 13.26 79.56 ± 0.42 99.80 ± 0.17</cell></row><row><cell>A3</cell><cell cols="7">72.33 ± 1.23 65.79 ± 1.18 85.24 ± 0.28 75.34 ± 1.40 89.85 ± 0.16 65.79 ± 14.05 79.47 ± 0.34 99.81 ± 0.10</cell></row><row><cell>A4</cell><cell cols="7">72.95 ± 1.19 66.22 ± 0.95 84.88 ± 0.38 75.29 ± 1.56 90.10 ± 0.18 62.59 ± 12.77 79.45 ± 0.48 99.80 ± 0.14</cell></row><row><cell>A5</cell><cell cols="7">67.96 ± 0.99 63.21 ± 1.25 84.48 ± 0.40 72.61 ± 1.86 89.75 ± 0.24 62.47 ± 12.39 79.42 ± 0.52 99.79 ± 0.10</cell></row><row><cell>A6</cell><cell cols="7">73.12 ± 1.48 66.94 ± 1.00 85.72 ± 0.38 76.21 ± 1.26 90.28 ± 0.19 66.89 ± 12.44 79.78 ± 0.40 99.86 ± 0.10</cell></row><row><cell></cell><cell cols="2">NTU2012 ModelNet40</cell><cell>Yelp</cell><cell cols="4">House (0.6) House (1.0) Walmart (0.6) Walmart (1.0) Avg. Rank</cell></row><row><cell cols="8">SetGNN 73.86 ± 1.62 95.85 ± 0.38 28.78 ± 1.51 68.54 ± 1.89 58.34 ± 2.25 74.97 ± 0.22 59.13 ± 0.20</cell><cell>7.71</cell></row><row><cell>Self</cell><cell cols="7">73.41 ± 1.65 95.83 ± 0.23 23.49 ± 4.15 67.75 ± 3.29 58.54 ± 2.16 74.76 ± 0.20 58.83 ± 0.21</cell><cell>8.64</cell></row><row><cell>Con</cell><cell cols="7">73.27 ± 1.53 95.85 ± 0.31 26.14 ± 1.86 68.50 ± 2.52 58.56 ± 2.42 75.17 ± 0.21 59.39 ± 0.20</cell><cell>7.07</cell></row><row><cell>A0</cell><cell cols="7">73.54 ± 1.93 95.92 ± 0.18 29.43 ± 1.42 67.48 ± 3.21 57.39 ± 2.37 73.14 ± 0.21 56.49 ± 0.60</cell><cell>8.21</cell></row><row><cell>A1</cell><cell cols="7">74.71 ± 1.81 95.87 ± 0.27 27.18 ± 0.71 68.64 ± 2.99 58.10 ± 3.22 75.42 ± 0.13 60.09 ± 0.25</cell><cell>4.50</cell></row><row><cell>A2</cell><cell cols="7">74.88 ± 1.66 96.56 ± 0.34 31.39 ± 2.45 69.73 ± 2.60 58.90 ± 1.97 75.50 ± 0.18 60.19 ± 0.20</cell><cell>2.29</cell></row><row><cell>A3</cell><cell cols="7">74.68 ± 1.74 96.48 ± 0.29 27.57 ± 1.00 67.88 ± 2.90 58.51 ± 2.22 75.29 ± 0.23 60.19 ± 0.20</cell><cell>4.71</cell></row><row><cell>A4</cell><cell cols="7">74.83 ± 1.75 95.86 ± 0.28 29.64 ± 1.93 69.56 ± 2.89 58.91 ± 2.69 75.43 ± 0.18 59.90 ± 0.24</cell><cell>4.14</cell></row><row><cell>A5</cell><cell cols="7">74.41 ± 1.86 96.46 ± 0.33 29.24 ± 1.42 68.14 ± 2.97 57.70 ± 2.98 75.26 ± 0.18 59.81 ± 0.22</cell><cell>6.71</cell></row><row><cell>A6</cell><cell cols="7">75.34 ± 1.91 96.93 ± 0.33 34.64 ± 0.39 70.96 ± 2.27 59.93 ± 1.99 75.62 ± 0.16 60.46 ± 0.20</cell><cell>1.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Results on the test data sets: Mean accuracy (%) ± standard deviation. Bold values indicate the best result. 1% of all vertexes are used for training.</figDesc><table><row><cell></cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell><cell>Cora-CA</cell><cell>DBLP-CA 20Newsgroups Mushroom</cell></row><row><cell cols="6">SetGNN 46.48 ± 3.62 47.01 ± 4.31 76.13 ± 1.19 52.29 ± 4.18 85.52 ± 0.54 73.83 ± 1.40</cell><cell>97.73 ± 1.18</cell></row><row><cell>Self</cell><cell cols="5">45.79 ± 5.34 44.22 ± 4.43 76.71 ± 0.90 51.64 ± 5.37 84.42 ± 0.37 73.91 ± 0.90</cell><cell>92.25 ± 0.89</cell></row><row><cell>Con</cell><cell cols="5">49.20 ± 4.38 48.56 ± 4.88 77.51 ± 1.08 52.37 ± 4.41 86.47 ± 0.35 74.39 ± 1.23</cell><cell>92.43 ± 0.87</cell></row><row><cell>A0</cell><cell cols="5">48.50 ± 4.77 46.43 ± 4.24 78.83 ± 1.79 49.87 ± 5.08 87.34 ± 0.73 74.43 ± 1.11</cell><cell>97.32 ± 1.33</cell></row><row><cell>A1</cell><cell cols="5">56.42 ± 5.02 55.63 ± 3.96 80.13 ± 1.44 60.86 ± 5.91 87.53 ± 0.30 74.68 ± 1.31</cell><cell>97.95 ± 1.15</cell></row><row><cell>A2</cell><cell cols="5">56.81 ± 4.49 56.10 ± 2.86 80.22 ± 1.24 60.96 ± 6.31 88.10 ± 0.35 74.72 ± 1.16</cell><cell>98.05 ± 1.18</cell></row><row><cell>A3</cell><cell cols="5">55.94 ± 3.67 55.82 ± 3.40 80.13 ± 1.02 60.51 ± 4.55 87.47 ± 0.36 74.63 ± 1.00</cell><cell>98.04 ± 0.98</cell></row><row><cell>A4</cell><cell cols="5">58.55 ± 5.14 57.16 ± 4.62 80.11 ± 1.02 60.91 ± 5.15 88.91 ± 0.29 74.67 ± 1.39</cell><cell>97.72 ± 1.12</cell></row><row><cell>A5</cell><cell cols="5">46.23 ± 3.44 45.07 ± 4.89 75.95 ± 1.32 53.26 ± 4.86 87.12 ± 0.43 74.81 ± 1.04</cell><cell>97.72 ± 1.25</cell></row><row><cell>A6</cell><cell cols="5">57.45 ± 5.00 56.23 ± 3.27 81.10 ± 0.80 61.76 ± 4.94 88.55 ± 0.41 75.52 ± 0.93</cell><cell>98.28 ± 1.03</cell></row><row><cell></cell><cell>ModelNet40</cell><cell>Yelp</cell><cell cols="3">House (0.6) House (1.0) Walmart (0.6) Walmart (1.0) Avg. Rank (↓)</cell></row><row><cell cols="6">SetGNN 88.34 ± 2.69 27.64 ± 1.10 53.69 ± 2.20 51.85 ± 1.64 65.48 ± 0.45 51.15 ± 0.52</cell><cell>7.62</cell></row><row><cell>Self</cell><cell cols="5">86.85 ± 3.03 20.77 ± 5.15 53.42 ± 2.25 51.14 ± 1.75 65.23 ± 0.43 51.00 ± 0.41</cell><cell>9.69</cell></row><row><cell>Con</cell><cell cols="5">87.00 ± 2.99 24.23 ± 0.43 53.58 ± 3.04 51.96 ± 1.87 65.47 ± 0.44 51.13 ± 0.46</cell><cell>7.31</cell></row><row><cell>A0</cell><cell cols="5">88.75 ± 2.78 27.43 ± 0.60 53.60 ± 2.73 51.70 ± 2.13 65.41 ± 0.47 51.10 ± 0.49</cell><cell>7.46</cell></row><row><cell>A1</cell><cell cols="5">89.34 ± 2.66 26.18 ± 0.51 54.12 ± 3.29 52.23 ± 2.46 65.96 ± 0.36 51.22 ± 0.35</cell><cell>4.08</cell></row><row><cell>A2</cell><cell cols="5">89.37 ± 2.69 27.67 ± 0.91 54.42 ± 2.83 52.31 ± 1.44 66.01 ± 0.41 51.32 ± 0.30</cell><cell>2.69</cell></row><row><cell>A3</cell><cell cols="5">89.31 ± 2.62 26.98 ± 0.66 53.71 ± 2.71 52.11 ± 2.24 65.88 ± 0.50 51.35 ± 0.53</cell><cell>4.38</cell></row><row><cell>A4</cell><cell cols="5">89.03 ± 2.66 27.45 ± 0.81 53.64 ± 2.61 51.77 ± 2.20 65.55 ± 0.51 51.04 ± 0.47</cell><cell>4.54</cell></row><row><cell>A5</cell><cell cols="5">89.43 ± 2.68 28.09 ± 0.96 54.07 ± 3.09 51.94 ± 1.84 65.52 ± 0.39 50.97 ± 0.47</cell><cell>6.00</cell></row><row><cell>A6</cell><cell cols="5">90.22 ± 2.72 29.61 ± 0.71 56.27 ± 4.18 52.55 ± 2.18 66.42 ± 0.40 51.82 ± 0.39</cell><cell>1.23</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Results of different self-supervised mechanisms: Mean accuracy (%) ± standard deviation. Bold values indicate the best result. 10% of all vertexes are used for training. .27 63.53 ± 1.32 84.33 ± 0.36 72.21 ± 1.51 89.51 ± 0.18 65.06 ± 12.82 79.37 ± 0.35 Pretrain_L 52.59 ± 2.33 53.29 ± 2.01 69.90 ± 0.41 48.00 ± 4.79 87.59 ± 0.43 66.82 ± 13.48 71.93 ± 2.99 Pretrain_F 68.39 ± 1.20 63.83 ± 1.68 84.47 ± 0.40 73.12 ± 1.37 89.75 ± 0.23 65.43 ± 13.38 79.44 ± 0.39 MTL 72.58 ± 1.10 66.40 ± 1.35 85.16 ± 0.38 75.82 ± 1.42 90.22 ± 0.23 66.35 ± 13.26 79.56 ± 0.42</figDesc><table><row><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell><cell>Cora-CA</cell><cell>DBLP-CA</cell><cell>Zoo</cell><cell>20Newsgroups</cell></row><row><cell cols="2">SetGNN 67.93 ± 1Mushroom NTU2012 ModelNet40</cell><cell>Yelp</cell><cell cols="4">House (0.6) House (1.0) Walmart (0.6) Walmart (1.0)</cell></row><row><cell cols="7">99.75 ± 0.11 73.86 ± 1.62 95.85 ± 0.38 28.78 ± 1.51 68.54 ± 1.89 58.34 ± 2.25 74.97 ± 0.22 59.13 ± 0.20</cell></row><row><cell cols="7">93.77 ± 2.20 70.06 ± 2.42 96.23 ± 0.31 26.68 ± 0.30 61.22 ± 3.09 54.81 ± 2.39 40.35 ± 4.30 33.30 ± 2.72</cell></row><row><cell cols="7">99.77 ± 0.15 74.03 ± 1.86 95.88 ± 0.34 28.19 ± 1.42 69.02 ± 4.02 59.20 ± 2.54 75.01 ± 0.27 59.87 ± 0.28</cell></row><row><cell cols="7">99.80 ± 0.17 74.88 ± 1.66 96.56 ± 0.34 31.39 ± 2.45 69.73 ± 2.60 58.90 ± 1.97 75.50 ± 0.18 60.19 ± 0.20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Results on converted conventional graphs: Mean accuracy (%) ± standard deviation. Bold values indicate the best result. 10% of all vertices are used for training.</figDesc><table><row><cell></cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell><cell>Cora-CA</cell><cell>DBLP-CA</cell><cell>Zoo</cell><cell>20Newsgroups</cell></row><row><cell>HGNN</cell><cell cols="7">67.37 ± 1.45 62.76 ± 1.42 82.16 ± 0.38 66.80 ± 1.79 85.28 ± 0.29 47.84 ± 6.87 70.27 ± 0.73</cell></row><row><cell>A2</cell><cell cols="7">67.18 ± 1.42 63.52 ± 2.35 82.37 ± 0.34 67.14 ± 1.79 85.22 ± 0.26 46.85 ± 10.15 70.46 ± 1.21</cell></row><row><cell>A4</cell><cell cols="7">67.24 ± 1.51 63.37 ± 2.56 82.25 ± 0.43 66.88 ± 2.07 85.16 ± 0.25 46.85 ± 9.93 69.35 ± 1.24</cell></row><row><cell>Mushroom</cell><cell cols="2">NTU2012 ModelNet40</cell><cell>Yelp</cell><cell cols="4">House (0.6) House (1.0) Walmart (0.6) Walmart (1.0)</cell></row><row><cell cols="8">97.15 ± 0.47 70.26 ± 1.70 87.60 ± 0.36 26.91 ± 0.37 58.01 ± 2.47 57.65 ± 2.69 59.48 ± 0.19 53.97 ± 0.29</cell></row><row><cell cols="8">97.29 ± 0.45 69.91 ± 1.59 87.75 ± 0.33 26.72 ± 0.36 58.08 ± 3.28 57.53 ± 2.80 59.49 ± 0.22 54.04 ± 0.24</cell></row><row><cell cols="8">97.15 ± 0.55 69.94 ± 1.54 87.65 ± 0.36 26.66 ± 0.45 58.47 ± 2.99 57.73 ± 2.84 59.53 ± 0.22 53.98 ± 0.26</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Results on the test data sets with regard to robustness. Bold values indicate the best result. 10% of all vertexes are used for training.</figDesc><table><row><cell></cell><cell></cell><cell>Cora</cell><cell></cell><cell></cell><cell>Citeseer</cell><cell></cell><cell></cell><cell>ModelNet40</cell></row><row><cell></cell><cell>Random</cell><cell>Net</cell><cell>Minmax</cell><cell>Random</cell><cell>Net</cell><cell>Minmax</cell><cell>Random</cell><cell>Net</cell><cell>Minmax</cell></row><row><cell cols="10">SetGNN 66.87 ± 1.33 66.26 ± 1.54 66.58 ± 1.02 62.89 ± 1.57 62.81 ± 1.32 62.21 ± 1.64 95.74 ± 0.22 95.41 ± 0.28 93.33 ± 0.26</cell></row><row><cell>A2</cell><cell cols="9">71.90 ± 1.63 71.16 ± 0.92 70.86 ± 1.22 66.41 ± 1.08 65.38 ± 1.47 64.69 ± 0.98 96.09 ± 0.17 95.52 ± 0.24 93.64 ± 0.26</cell></row><row><cell>A4</cell><cell cols="9">72.11 ± 1.60 70.49 ± 1.29 70.52 ± 1.39 65.94 ± 1.24 65.15 ± 1.70 64.12 ± 1.19 95.79 ± 0.27 95.44 ± 0.25 93.35 ± 0.24</cell></row><row><cell>A6</cell><cell cols="9">72.15 ± 1.70 71.94 ± 1.48 71.98 ± 1.36 66.60 ± 1.61 65.68 ± 1.09 65.51 ± 1.13 96.58 ± 0.24 96.23 ± 0.23 94.82 ± 0.33</cell></row><row><cell></cell><cell></cell><cell>NTU2012</cell><cell></cell><cell></cell><cell>House (0.6)</cell><cell></cell><cell></cell><cell>House (1.0)</cell></row><row><cell></cell><cell>Random</cell><cell>Net</cell><cell>Minmax</cell><cell>Random</cell><cell>Net</cell><cell>Minmax</cell><cell>Random</cell><cell>Net</cell><cell>Minmax</cell></row><row><cell cols="10">SetGNN 73.84 ± 2.18 73.38 ± 1.36 70.71 ± 1.89 67.16 ± 2.55 68.88 ± 2.68 64.78 ± 2.20 56.86 ± 1.93 59.95 ± 1.92 56.52 ± 2.52</cell></row><row><cell>A2</cell><cell cols="9">74.50 ± 2.03 73.86 ± 1.84 71.40 ± 1.64 67.71 ± 2.94 69.59 ± 2.32 65.23 ± 2.89 57.74 ± 2.70 60.73 ± 2.30 57.00 ± 1.94</cell></row><row><cell>A4</cell><cell cols="9">73.73 ± 1.59 73.72 ± 1.59 71.06 ± 1.53 67.55 ± 2.41 68.85 ± 1.38 64.97 ± 3.35 57.47 ± 2.72 60.10 ± 1.74 56.65 ± 2.26</cell></row><row><cell>A6</cell><cell cols="9">75.06 ± 1.97 74.37 ± 1.99 72.09 ± 1.98 69.88 ± 3.27 73.14 ± 2.71 68.84 ± 2.71 60.06 ± 2.07 62.41 ± 1.77 58.76 ± 2.24</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Results on the test data sets with regard to fairness. 10% of all vertexes are used for training. For fairness metrics ∆ SP and ∆ EO , lower values indicate better performance. German Credit SetGNN 59.16 ± 2.51 81.84 ± 0.93 2.65 ± 5.62 4.06 ± 6.76 A2 59.81 ± 3.00 82.26 ± 0.13 0.55 ± 0.95 0.78 ± 0.70 A4 59.66 ± 3.83 80.54 ± 3.52 3.03 ± 6.54 5.07 ± 7.81 A6 59.88 ± 3.04 82.36 ± 0.38 0.95 ± 0.92 0.47 ± 0.56 .19 87.92 ± 0.25 2.84 ± 1.14 1.38 ± 0.32 A6 73.78 ± 0.16 88.03 ± 0.14 2.58 ± 0.91 0.81 ± 0.37 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] (b) Did you mention the license of the assets? [Yes] We discuss it in Appendix C. (c) Did you include any new assets either in the supplemental material or as a URL? [Yes] We submit the code in the supplementary materials. (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] We discuss it in Appendix C. (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes] The data sets we use don't contain any personally identifiable information. 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] We don't conduct research with human subjects. (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]</figDesc><table><row><cell>data set</cell><cell>Method</cell><cell>AUROC</cell><cell>F1</cell><cell>∆SP (↓)</cell><cell>∆EO(↓)</cell></row><row><cell></cell><cell cols="5">SetGNN 96.51 ± 0.48 89.84 ± 0.97 8.63 ± 0.50 4.16 ± 0.51</cell></row><row><cell>Recidivism</cell><cell>A2 A4</cell><cell cols="4">96.34 ± 0.39 90.09 ± 0.53 8.53 ± 0.52 3.92 ± 0.68 96.45 ± 0.35 89.75 ± 0.68 8.49 ± 0.27 3.49 ± 0.66</cell></row><row><cell></cell><cell>A6</cell><cell cols="4">96.55 ± 0.54 89.22 ± 0.55 8.51 ± 0.25 3.13 ± 0.64</cell></row><row><cell></cell><cell cols="5">SetGNN 73.46 ± 0.17 87.91 ± 0.27 2.79 ± 0.99 0.98 ± 0.69</cell></row><row><cell>Credit defaulter</cell><cell>A2 A4</cell><cell cols="4">73.43 ± 0.27 87.82 ± 0.24 2.64 ± 1.32 0.93 ± 0.87 73.58 ± 0</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>This work is supported by National Science Foundation under Award No. IIS-1947203, IIS-2117902, IIS-2137468, CCF-1943008; US Army Research Office Young Investigator Award W911NF2010240; National Institute of General Medical Sciences under grant R35GM124952; and Agriculture and Food Research Initiative grant no. 2020-67021-32799/project accession no.1024178 from the USDA National Institute of Food and Agriculture. The views and conclusions are those of the authors and should not be interpreted as representing the official policies of the government agencies.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Checklist</head><p>The checklist follows the references. Please read the checklist guidelines carefully for information on how to answer these questions. For each question, change the default [TODO] to [Yes] , <ref type="bibr">[No]</ref> , or [N/A] . You are strongly encouraged to include a justification to your answer, either by referencing the appropriate section of your paper or providing a brief inline description. For example:</p><p>• Did you include the license to the code and datasets? [Yes] See Section.</p><p>• Did you include the license to the code and datasets? <ref type="bibr">[No]</ref> The code and the data are proprietary. • Did you include the license to the code and datasets? [N/A] Please do not modify the questions and only use the provided macros for your answers. Note that the Checklist section does not count towards the page limit. In your paper, please delete this instructions block and only keep the Checklist section heading above along with the questions/answers below. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hypergraph neural networks</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3558" to="3565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hypergcn: A new method for training graph convolutional networks on hypergraphs</title>
		<author>
			<persName><forename type="first">Naganand</forename><surname>Yadati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madhav</forename><surname>Nimishakavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prateek</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anand</forename><surname>Vikram Nitin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">You are allset: A multiset function framework for hypergraph neural networks</title>
		<author>
			<persName><forename type="first">Eli</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olgica</forename><surname>Milenkovic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Hypergraph theory. An introduction. Mathematical Engineering</title>
		<author>
			<persName><forename type="first">Alain</forename><surname>Bretto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Springer</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dual channel hypergraph collaborative filtering</title>
		<author>
			<persName><forename type="first">Shuyi</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xibin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanwan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2020" to="2029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Self-supervised multi-channel hypergraph convolutional network for social recommendation</title>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinyong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung</forename><surname>Viet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangliang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
				<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="413" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spatiotemporal hypergraph convolution network for stock movement forecasting</title>
		<author>
			<persName><forename type="first">Ramit</forename><surname>Sawhney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivam</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnav</forename><surname>Wadhwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajiv Ratn</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Data Mining (ICDM)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="482" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cross-modality and self-supervised protein embedding for compound-protein affinity and contact prediction</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multiscale and integrative single-cell hi-c analysis with higashi</title>
		<author>
			<persName><forename type="first">Ruochi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature biotechnology</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="254" to="261" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youzhi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kanji</forename><surname>Uchino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koji</forename><surname>Maruhashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.09410</idno>
		<title level="m">Generating 3d molecules for target protein binding</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scaling and benchmarking self-supervised visual representation learning</title>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ieee/cvf International Conference on computer vision</title>
				<meeting>the ieee/cvf International Conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6391" to="6400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Strategies for pre-training graph neural networks</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12265</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5812" to="5823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">When does self-supervision help graph convolutional networks?</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09136</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Self-supervised learning on graphs: Deep insights and new direction</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Derr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haochen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10141</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Self-supervised learning of graph neural networks: A unified review</title>
		<author>
			<persName><forename type="first">Yaochen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingtun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Hjelm</surname></persName>
		</author>
		<title level="m">Deep graph infomax. ICLR (Poster)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Graph contrastive learning automated</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12121" to="12132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Bringing your own view: Graph contrastive learning without prefabricated data augmentations</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.01702</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04131</idno>
		<title level="m">Deep graph contrastive representation learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Augmentations in graph contrastive learning: Current methodological flaws &amp; towards better practices</title>
		<author>
			<persName><forename type="first">Puja</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekdeep</forename><surname>Singh Lubana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaoqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Web Conference 2022</title>
				<meeting>the ACM Web Conference 2022</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1538" to="1549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Augmentation-free graph contrastive learning</title>
		<author>
			<persName><forename type="first">Haonan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.04874</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adversarial graph contrastive learning with information regularization</title>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoyu</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Web Conference 2022</title>
				<meeting>the ACM Web Conference 2022</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1362" to="1371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Group contrastive self-supervised learning on graphs</title>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaochen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Lianghao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><forename type="middle">Xiangji</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.12200</idno>
		<title level="m">Hypergraph contrastive collaborative filtering</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hypergraph contrastive learning for electronic health records</title>
		<author>
			<persName><forename type="first">Derun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moxian</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baofeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenda</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 SIAM International Conference on Data Mining (SDM)</title>
				<meeting>the 2022 SIAM International Conference on Data Mining (SDM)</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="127" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Hyper-sagnn: a self-attention based graph neural network for hypergraphs</title>
		<author>
			<persName><forename type="first">Ruochi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuesong</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02613</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hypergraph convolution and hypergraph attention</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feihu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Hs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torr</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page">107637</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Unignn: a unified framework for graph and hypergraph neural networks</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.00956</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Devanshu</forename><surname>Arya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Deepak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stevan</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcel</forename><forename type="middle">Worring</forename><surname>Rudinac</surname></persName>
		</author>
		<author>
			<persName><surname>Hypersage</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04558</idno>
		<title level="m">Generalizing inductive representation learning on hypergraphs</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deep sets. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
				<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent-a new approach to self-supervised learning</title>
		<author>
			<persName><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaohan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Gheshlaghi Azar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="21271" to="21284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Self-supervised hypergraph convolutional networks for session-based recommendation</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinyong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangliang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="4503" to="4511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Hypergraph pre-training with graph neural networks</title>
		<author>
			<persName><forename type="first">Boxin</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Barton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Neiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.10862</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Neural collaborative filtering</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th international conference on world wide web</title>
				<meeting>the 26th international conference on world wide web</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Model-agnostic counterfactual reasoning for eliminating popularity bias in recommender system</title>
		<author>
			<persName><forename type="first">Tianxin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1791" to="1800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Comprehensive fair meta-learned recommender system</title>
		<author>
			<persName><forename type="first">Tianxin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingrui</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1989" to="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Causal intervention for leveraging popularity bias in recommendation</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianxin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chonggang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guohui</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">Auto-encoding variational bayes. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">An introduction to variational autoencoders</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Diederik P Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations and Trends® in Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="307" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Bayesian modeling and uncertainty quantification for learning to optimize: What, why, and how</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Adversarial attacks on graph neural networks via meta learning</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Zügner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.08412</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Adversarial attacks on neural networks for graph data</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Zügner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Akbarnejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2847" to="2856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Uci machine learning repository</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Asuncion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The effect of race/ethnicity on sentencing: Examining sentence type, jail length, and prison length</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kareem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><forename type="middle">L</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><surname>Freiburger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Ethnicity in Criminal Justice</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="179" to="196" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients. Expert systems with applications</title>
		<author>
			<persName><forename type="first">I-Cheng</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Che-Hui</forename><surname>Lien</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="2473" to="2480" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
