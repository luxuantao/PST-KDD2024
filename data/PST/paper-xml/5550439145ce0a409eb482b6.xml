<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Traffic sign detection via interest region extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2014-06-04">June 4, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Samuele</forename><surname>Salti</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Bologna</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Bologna</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Bologna</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alioscia</forename><surname>Petrelli</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Bologna</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Bologna</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Bologna</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Federico</forename><surname>Tom- Bari</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Bologna</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nicola</forename><surname>Fioraio</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Bologna</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Bologna</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Bologna</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luigi</forename><surname>Di</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Bologna</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Bologna</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Federico</forename><surname>Tombari</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Bologna</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Bologna</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luigi</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Bologna</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Traffic sign detection via interest region extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2014-06-04">June 4, 2014</date>
						</imprint>
					</monogr>
					<idno type="MD5">73E2ECA32F53D2DE88E172ACD846DA91</idno>
					<idno type="DOI">10.1016/j.patcog.2014.05.017</idno>
					<note type="submission">Received date: 27 December 2013 Revised date: 29 May 2014 Accepted date: 30 May 2014 Preprint submitted to Pattern Recognition</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Pattern Recognition Traffic sign detection via interest region extraction</term>
					<term>Pattern Recognition</term>
					<term>mobile mapping</term>
					<term>traffic sign detection</term>
					<term>local features</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Mobile mapping systems acquire massive amount of data in uncontrolled conditions and pose new challenges to the development of robust computer vision algorithms. In this work, we show how a combination of solid image analysis and pattern recognition techniques can be used to tackle the problem of traffic sign detection in mobile mapping data. Differently from the majority of existing systems, our pipeline is based on interest regions extraction rather than sliding window detection. Thanks to the robustness of local features, the proposed pipeline can withstand great appearance variations, which typically occur in outdoor data, especially dramatic illumination and scale changes. The proposed approach has been specialized and tested in three variants, each aimed at detecting one of the three categories of Mandatory, Prohibitory and Danger traffic signs, according to the experimental setup of the recent German Traffic Sign Detection Benchmark competition.</p><p>Besides achieving very good performance in the on-line competition, our proposal has been successfully evaluated on a novel, more challenging dataset of Italian signs, thereby proving its robustness and suitability to automatic analysis of real-world mobile mapping data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Mobile mapping systems gain increasing interest as cost-effective acquisition tools to collect registered geo-referenced data, which in turn enables new applications, such as location-aware mobile applications, emergency response planning, self-driving cars (e.g., the famous Google car, which locates itself thanks to pre-acquired 2D/3D maps of the route), road mapping and facility management. Mobile mapping systems consist of an integrated array of synchronized inertial, positioning and imaging sensors mounted on a mobile platform. Thanks to the presence of GPS and inertial information, collected data are easily registered without complex and costly post-processing operations.</p><p>Among the automatic analyses of mobile mapping data, Traffic Sign Recognition (TSR) plays a key role as it is one of the enabling technologies in several of the above mentioned applications. As such, TSR has been an actively developed area of research during the last years. Although traffic signs present a rigid and simple shape as well as uniform and known colors, the wide appearance variability of traffic signs captured in uncontrolled environments results in challenging working conditions for computer vision algorithms.</p><p>Traffic Sign Recognition is usually tackled via a two-step approach: traffic sign detection and traffic sign classification. In detection, the aim is to identify the image regions (bounding boxes) that tightly contain a traffic sign, indeed a similar computer vision problem as detecting faces <ref type="bibr" target="#b1">[1]</ref> or pedestrians <ref type="bibr" target="#b2">[2]</ref>. Classification aims at labeling bounding boxes according to the enclosed traffic signs, similarly to the image classification problem. So far, evaluation on public benchmarks of traffic sign recognition algorithms has focused more on classification than on detection. There are several public datasets to assess the pros and cons of classification algorithms. In particular, the German Traffic Sign Recognition Benchmark (GTSRB) <ref type="bibr" target="#b3">[3]</ref> provides a huge dataset and an interesting evaluation of the performance of computer vision algorithms versus the human visual system. Conversely, evaluation of traffic sign detection on publicly available datasets is less explored. Recently, though, the organizers of the GTSRB proposed a follow-up competition targeting traffic sign detection, the German Traffic Sign Detection Benchmark (GTSDB) <ref type="bibr" target="#b4">[4]</ref>. The competition addresses detection of three categories of signs: prohibitory (circular red-and-white signs), mandatory (circular blue-and-white signs) and danger signs (upper-triangular red-andwhite signs). This paper describes an extension of the traffic sign detection pipeline that we developed and submitted for evaluation to the on-line GTSDB competition <ref type="bibr" target="#b5">[5]</ref>. In particular, we present a more detailed description of the ideas and algorithms building up our pipeline, an automatic process to estimate the interest regions rescaling factors, the combination of more HOGs sources to improve its discriminative power, a thorough analysis on some of the key pipeline steps that provides useful insights on possible design choices, improved figures on the GTSDB dataset as well as results on a novel, more challenging mobile mapping dataset.</p><p>The remainder of the paper is organized as follows: sec. 2 presents an overview of recent work on the subject; sec. 3 details all the steps of the proposed detection pipeline, i.e. image preprocessing (Sec. 3.1), interest region detection (Sec. 3.2), HOGs classification (sec. 3.3), context-aware filter (sec. <ref type="bibr">3.4)</ref> and traffic light removal (sec. <ref type="bibr">3.5)</ref>. Sec. 4 discusses the insights gained by tuning our pipeline on the GTSDB training set (sec. 4.1) and provides the experimental results attained on the test sets of three categories of the benchmark (sec. 4.2) as well as on a novel mobile mapping dataset (sec. 4.3). Sec. 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Traffic Sign Detection (TSD) usually relies upon two key observations: signs have a well defined shape and uniform and distinctive colors. The approaches to TSD can then be categorized as geometry-based, segmentationbased or hybrid according to the cue or cues they try to exploit. Geometrybased algorithms employ either Haar-like features in frameworks inspired by the popular Viola-Jones detector <ref type="bibr" target="#b1">[1]</ref> or the orientation and intensity of image gradients in frameworks inspired by the Generalized Hough Transform . The first sub-category comprises the works by Bahlmann et al. <ref type="bibr" target="#b6">[6]</ref> and by Brkic et al. <ref type="bibr" target="#b7">[7]</ref>, whereas in the second we find the Regular Polygon Detector <ref type="bibr" target="#b8">[8]</ref>,</p><p>the Radial Symmetry Detector <ref type="bibr" target="#b9">[9]</ref>, the Vertex Bisector Transform <ref type="bibr" target="#b10">[10]</ref>, the Bilateral Chinese Transform <ref type="bibr" target="#b11">[11]</ref> and, alike, the two schemes of Single Target Voting for triangles and circles proposed by Houben <ref type="bibr" target="#b12">[12]</ref>.</p><p>Segmentation-based algorithms tend to follow a common scheme: the image is transformed into a color space that highlights the signs of interest and then thresholded. Several color spaces have been proposed accordingly, such as RGB, normalized RGB, HSV, CIE L*a*b. Gomez-Moreno et al. an interesting experimental analysis. A variant to this common approach is represented by definition of a specific color transformation that highlights the colors of interest: for example, the transformations defined in <ref type="bibr" target="#b14">[14]</ref>, which uses the difference between the most characteristic RGB channel of a signal and the other two channels, and that proposed in <ref type="bibr" target="#b15">[15]</ref>, which uses the maximum between the normalized R and B channels. A more complex approach than simply switching color-space is to develop a saliency-based visual attention model, biologically inspired by selective attention of human vision, such as that proposed in Itti et al. <ref type="bibr" target="#b16">[16]</ref>. Such models are based on center-surround differences implemented via operations on Gaussian pyramids and combine into the same saliency map edge magnitude and color. Finally, another variant has been proposed, which pushes further the effort of defining a proper color space for the specific task: in <ref type="bibr" target="#b17">[17]</ref>, the authors learn from training data via Integer Linear Programming a set of good color transformations together with the optimal threshold for each.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Traffic Sign Detection Pipeline</head><p>An overview of our detection pipeline is depicted in Fig. <ref type="figure" target="#fig_4">1</ref>. The use of interest region detectors to select candidate regions is the main difference between our proposal and the majority of the approaches presented in previous section, which instead rely either on the Hough Transform or a sliding window detector trained to learn distinctive cues from data. The only other paper relying on interest regions extraction is the recent traffic sign recognition system described in <ref type="bibr" target="#b15">[15]</ref>, which employs Maximally Stable Extremal Regions (MSERs) <ref type="bibr" target="#b18">[18]</ref> for the detection stage and has been a source of inspiration for our work. Sliding window approaches require a cascade of classifiers <ref type="bibr" target="#b1">[1]</ref> to perform detection at acceptable frame-rates, where the first classifiers in the hierarchy quickly discard background regions and let deeper, more specialized but slower, classifiers analyze only a subset of candidate regions. Although the cascade of classifiers works pretty well in practice, its main purpose is to improve the efficiency of the detector, not its effectiveness. Indeed, if a traffic sign gets discarded by higher level classifiers, it cannot be recovered by deeper ones (in other words, recall may only decrease as long as candidate regions move down through the cascade). Given how crucial the first stage turns out in determining the recall of the overall system, we advocate accomplishing candidate regions selection by a robust and fast approach deploying strong prior knowledge on the appearance of the objects of interest.</p><p>As highlighted in the previous section, the two main cues for traffic sign detection are color and shape: in our pipeline we exploit both, relying purposely on two complementary interest regions detectors. In particular, we extract as candidate patches of the image those regions that exhibit either a uniform value of the main colors of a sign, as found by the MSER algorithm, or show a strong symmetry, as found by a recently proposed detector based on the wave equation (WaDe) <ref type="bibr" target="#b19">[19]</ref>. Color is further exploited by preprocessing the image to enhance the main color of each sign, likewise segmentationbased TSD algorithms. Preprocessing also tries to correct over and under exposure of signs that are common in outdoor pictures and set forth serious challenges for automatic detection.</p><p>The candidate regions provided by the interest region detectors are then classified as either traffic sign or background. We deploy Support Vector Machines (SVMs) to carry out classification; as for features, we rely on HOGs <ref type="bibr" target="#b2">[2]</ref>, as it is widely recognized as an effective general purpose descriptor in a variety of scenarios, including traffic sign detection. Moreover, quite peculiarly with respect to e.g. SIFT <ref type="bibr" target="#b20">[20]</ref> and SURF <ref type="bibr" target="#b21">[21]</ref>, which are paired with a dedicated detector, it turns out quite straightforward to compute HOG on multi-scale interest regions provided by any kind of detector.</p><p>Finally, two filters help further pruning out false positives: a generative context-aware filter discards regions that are unlikely to correspond to traffic signs given the relationship between their size and their position in the image; a traffic light detector checks whether regions corresponding to traffic light lamps have been erroneously classified as traffic signs by previous filters, so to discard them. Next subsections present all the steps in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Image preprocessing</head><p>Road scenes are affected by significant variations in lighting conditions, so that an initial preprocessing step is necessary to enhance traffic sign regions and fade background. We experimented with the following preprocessing steps. First, linear contrast stretching is applied separately to the three RGB channels (3CH-CS ) so as to deal with under or over exposed images.</p><p>Then, a single channel image to be fed to the interesting region extractor is obtained by enhancing the channel C that characterizes the sought signal (blue in the case of mandatory signs, red otherwise). We experimented with two enhancing methods. The first is RGB normalization:</p><formula xml:id="formula_0">C = C R + G + B , C ∈ {R, B}<label>(1)</label></formula><p>The second method has been suggested in <ref type="bibr" target="#b14">[14]</ref>. In the case of Danger and Prohibitory signs, red is enhanced according to:</p><formula xml:id="formula_1">R = max(0, min(R -B, R -G) R + G + B )<label>( 2 )</label></formula><p>whereas, for mandatory signs, the blue channel is enhanced according to:</p><formula xml:id="formula_2">B = max(0, B -R R + G + B )<label>( 3 )</label></formula><p>In turn, the enhancement in (3) differs from the original proposal in <ref type="bibr" target="#b14">[14]</ref>,</p><p>which is the exact dual of (2). Indeed, we found that with very dark or bright mandatory signs the blue and green channels tend to have similar values and cancel each other: we do not consider thus the strength of the blue with respect to the green (see Fig. <ref type="figure" target="#fig_1">2</ref> for an example). Finally, a further contrast stretching step is applied to the resulting one channel image (1CH-CS ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Interest regions extraction</head><p>We substantiate the intuition of relying on multiple interest region detectors by investigating on the use of two complementary algorithms: the well known Maximally Stable Extremal Regions detector (MSER) <ref type="bibr" target="#b18">[18]</ref> and the recently proposed Wave-based Detector (WaDe) <ref type="bibr" target="#b19">[19]</ref>.</p><p>MSER detects high-contrast regions of approximately uniform gray tone and arbitrary shape, and is therefore likely to fire at evenly coloured regions within traffic signs. MSERs are found by binarizing each frame at all the possible threshold levels, and then analyzing the connected components at each level: the connected components that maintain their area through several threshold values are selected as MSERs. We can distinguish between dark connected components on a brighter background (MSER+) and bright ones surrounded by a darker background (MSER-).</p><p>WaDe detects symmetric regions at various scales by sharp spatio-temporal extrema of a novel scale-space-like family of images, which are obtained as solutions of the wave partial differential equation at consecutive time-steps.</p><p>The initial condition is given by the gray-scale image under analysis, with approximately absorbing boundary conditions enforced to limit spurious interferences between traveling waves. By letting the image evolve according to the wave equation, contributions from edges of a symmetric structure, such as a circle or square, would reach the center of the symmetry in phase, thereby creating a sharp extremum. WaDe is therefore peculiarly suited to detection of round traffic signs. Similarly to MSER, minima highlight bright symmetric structures on a darker background (WaDe-), maxima the opposite polarity (WaDe+).</p><p>For danger and prohibitory signs we consider: MSER+ and WaDe+ regions extracted from the red channel of the preprocessed image, so as to detect the red border of the signs, as well as MSER-and WaDe-regions extracted from the preprocessed image to detect the white inner part. As the white part of the signs is the hardest to segment as a uniform color because of its achromaticity <ref type="bibr" target="#b13">[13]</ref>, we also include MSER-regions from the original image converted to grey-scale to detect the white inner part, as proposed in <ref type="bibr" target="#b15">[15]</ref>. For mandatory signs, instead, we consider MSER-and WaDe-regions extracted from the blue channel of the preprocessed image, so to detect the uniform blue region within the signs.</p><p>All regions extracted by MSER-and WaDe-when looking for the inner part of prohibitory or danger signs are rescaled to include the whole traffic sign. We also rescale MSER+ and WaDe+ regions as they tend to be too conservative in estimating the actual sign size. In <ref type="bibr" target="#b5">[5]</ref> rescaling factors were chosen manually. In this work, instead, suitable rescaling factors are learned from data to avoid mis-detecting some signs due to imprecise tuning of trivial geometric constraints. The estimation procedure is as follows:</p><p>• regions are extracted without rescaling;</p><p>• only regions whose overlap with a ground-truth bounding box is higher than a threshold and that are not bigger than the corresponding groundtruth bounding box are kept (we used 0.8 overlap for MSER+ and WaDE+ regions, as they are associated with the whole sign, and 0.4 for regions corresponding to the white inner part of signs);</p><p>• we robustly regress an affine rescaling model for the new height of the bounding box h according to</p><formula xml:id="formula_3">h = mh + q (4)</formula><p>by using the detected bounding boxes and the corresponding groundtruth bounding boxes with Iteratively Re-weighted Least Squares <ref type="bibr" target="#b22">[22]</ref>;</p><p>• at run-time, we rescale both the bounding box width and height according to the regressed mapping, i.e. h = mh + q and w = mw + q.</p><p>( 5 )</p><p>Finally, we pass down to the next stage of the pipeline only those bounding boxes that satisfy the constraints reported in Tab. 1, obtained from ground-truth statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">HOG and SVM classification</head><p>Given a set of candidate regions detected in the previous step, Histogram of Oriented Gradients (HOG) <ref type="bibr" target="#b2">[2]</ref> features are computed to exploit their wellknown ability to robustly capturing key shape traits in spite of large intraclass variance.</p><p>In <ref type="bibr" target="#b5">[5]</ref>, we already evaluated the discriminative power of HOGs extracted from different image kinds such as the input grey-scale image, the preprocessed grey-scale image (which better highlights the shape of evenly coloured regions within signs), the input color image (as proposed in <ref type="bibr" target="#b2">[2]</ref>, in such a case the gradient used at each pixel being the gradient with the largest magnitude between those computed on each channel). The best source turned out to be the input color image, which simultaneously considers both shape as well as color cues. In this work we also evaluate the juxtaposition of the HOGs extracted from the color image and the preprocessed images, as these two input images may be robust to different nuisances. Unlike the HOGs parameters used in the GTSRB benchmark reported in <ref type="bibr" target="#b3">[3]</ref>, we found that V * , to the midpoint of the range (i.e. 128) and joining it to (0, 0) and (255, 255) through two linear mappings (Fig. <ref type="figure" target="#fig_2">3</ref>, Top Row). This function has been devised to ensure that underexposed or overexposed patches redistribute their mass more evenly around the midpoint of the range (Fig. <ref type="figure" target="#fig_2">3</ref>, Top Row), thereby attaining new patches which are more amenable to classification by subsequent stages (Fig. <ref type="figure" target="#fig_2">3</ref>, Bottom Row).</p><p>Finally, HOGs extracted from the compensated patches are classified as either the traffic sign of interest or background by a binary SVM with RBF kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Context-aware filter</head><p>Bounding boxes classified as traffic signs are then passed down to an additional stage of the pipeline aimed at filtering out wrong detections (i.e. reducing the number of false positives). The idea is to enforce spatial constraints based on context information: indeed, context has not been exploited by previous pipeline stages for the detection of traffic signs, although it could represent an important source of discriminative information.</p><p>The proposed filtering stage exploits the typical position of traffic signs in urban environments, with roadsigns appearing not too close to the ground floor nor too above the horizon line. More specifically, the devised feature relies on two main cues: the size and height of a traffic sign in the image. It is indeed intuitive to observe that there is typically a strong correlation between the two cues: the smaller the size of the sign, the more its height will tend to approximate the horizon in the current image frame, due to roadsigns of the same category having approximately the same size. This observation relies also on the assumption that the camera is mounted in a fixed position on the roaming vehicle, which is reasonable in mobile mapping applications and allows simplifying the formulation by considering a constant horizon position in each frame. Hence, the limited range that the traffic sign height tends to assume when the size gets smaller is due to the fact that a smaller projection onto the image plane means that the sign is physically far away from the vehicle, while when its size increases the possible positions along the vertical image dimension tend to be more varied and thus less predictable. </p><formula xml:id="formula_4">h (b i ) = y b tl i H ,<label>( 6 )</label></formula><p>while, as for its size, r, we have chosen the number of rows in its enclosing bounding box normalized by r min and r max , i.e. the minimum and maximum sizes assumable by a traffic sign, which can be computed from the geometric constraints listed in Tab. 1:</p><formula xml:id="formula_5">r(b i ) = y b bl i -y b tl i r max -r min . (<label>7</label></formula><formula xml:id="formula_6">)</formula><p>These two normalization steps let the two parameters r and h span the range </p><p>In turn, we also propose to explicitly model the parameter set Θ = {µ(r), σ(r)}</p><p>and to estimate this model from training data. To perform robust parameter estimation, we assume that both the means as well as the standard deviations of the estimated distributions follow a parametric law of the independent variable r:</p><formula xml:id="formula_8">µ(r) = a µ • r + b µ ,<label>( 9 )</label></formula><formula xml:id="formula_9">σ(r) = a σ • r 2 + b σ • r + c σ . (<label>10</label></formula><formula xml:id="formula_10">)</formula><p>We have thus assumed linear dependency between r and µ(r), and a quadratic relationship for σ(r). Nonetheless, different parametric models could be employed, as the whole approach is general enough to be easily customized (in turn, using a linear model for both parameters would not change significantly the results on the GTSDB dataset). Accordingly, the set of parameters Θ includes the coefficients of the parametric models for the mean and the standard deviation of the Gaussian family we are trying to estimate:</p><formula xml:id="formula_11">Θ = {a µ , b µ , a σ , b σ , c σ }.</formula><p>Given the whole training set of (r, h) pairs, and once µ(r) and σ(r) have been computed for all r values spanned by the training data, an over-determined linear system can be defined so as to estimate in closed-form via least-squares the five parameters. In particular, given the non-uniform distribution of (r, h) pairs over the domain spanned by r, we employ weighted least-square estimation, to render the coefficient estimation process less biased by outliers. The weights in this case are proportional to the available training population for each value of the variable r.</p><p>At run-time, the posterior probability that a test sample (r, h) represents a traffic sign according to our model is:</p><p>p (S|r, h) ∝ p (h|r, S) p (S|r) p (h|r, S) <ref type="bibr" target="#b11">(11)</ref> where we have used Bayes rule and assumed an uninformative prior p (S|r).</p><p>Therefore, to decide if a bounding box represents a sign we can threshold the likelihood</p><formula xml:id="formula_12">p (h|r, S) = 1 √ 2πσ (r) exp - (r -µ (r)) 2 2σ 2 (r) . (<label>12</label></formula><formula xml:id="formula_13">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Traffic light filter</head><p>The presence of traffic lights in urban images represents a nuisance for automatic traffic sign detection due to shape, position and luminosity of traffic light lamps being often similar to that of certain traffic signs (in particular round-shaped ones, such as Mandatory and Prohibitory). This problem is worsened by the widespread presence of traffic lights within road scenery, which may induce a high number of false positives in a traffic sign detection pipeline. For this reason, we have designed a specific stage aimed at rejecting potential false positives due to traffic lights wrongly recognized as traffic signs. This module can also be seen as a traffic light detector, so that it may be deployed within similar application scenarios for other purposes than pruning falsely detected traffic sign.</p><p>The proposed approach follows a two step procedure based on some working hypotheses in order to examine and potentially discard each previously detected Region Of Interest (ROI). First of all, each ROI is assumed centered at one of the lamps of a traffic light. Secondly, we assume that all traffic lights consist of three lamps, with at most one of them switched on at a certain time instant. This obviously do not consider other configurations, such as e.g. two-lamp traffic lights, which are anyway much less frequent, and two-lights-on configuration. In any case, we wish to point out that the heuristics deployed by the proposed traffic light filter could be adapted to deal also with these two cases.</p><p>The goal of the first stage is to detect the most probable status of the lamp on which the current ROI is centered, choosing between Red, Yellow, Green and Off. This is done simply by thresholding the average red and green values of the pixels within the circle inscribed in the ROI. The status of the lamp as well as its size (represented by the size of the current ROI) allows us to define the relative position of the traffic light and of the remaining lamps with respect to the current ROI.</p><p>Successively, six additional auxiliary ROIs are determined. As shown in </p><formula xml:id="formula_14">δ (ROI ref , ROI in ) min i∈{l,r,u,d} δ (ROI ref , ROI i ) &lt; τ δ (<label>13</label></formula><formula xml:id="formula_15">)</formula><p>The role of the four external ROIs is twofold. On the one hand, they provide an adaptive term of comparison to rescale the distance between internal ROIs under different lighting conditions, thus allowing for using a fixed threshold τ δ . On the other hand, in case the input ROI lays on an actual sign, it is likely that ROI ref and ROI in lay on the same background and would report, in this case, a small relative distance. By using four different external ROIs, each placed along a different direction, the chance of having one external ROI laying on the same background part as that of ROI in is increased, this allowing to robustly discriminate real traffic signs from traffic lights even in presence of disparate background conditions. By thresholding the relative distances between inner and outer ROIs, the whole traffic light detection is normalized with respect to the photometric conditions of the image.</p><p>To increase the robustness of the filter, additional constraints that need to be satisfied in order for a ROI to be classified as a traffic light are enforced.</p><p>In particular, the mean and variance of the pixels within the circles inscribed in ROI ref , ROI in must be smaller than a certain threshold, so to check that they correspond to switched-off lamps:</p><formula xml:id="formula_16">µ (ROI ref ) &lt; τ µ ∧µ (ROI in ) &lt; τ µ ∧ σ (ROI ref ) &lt; τ σ ∧σ (ROI in ) &lt; τ σ ) (14)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>The proposed pipeline has been tuned and tested on the dataset made publicly available for the German Traffic Sign Detection Benchmark <ref type="bibr" target="#b4">[4]</ref>. It has also been evaluated on data from a real mobile mapping acquisition campaign carried out in Verona, Italy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Pipeline tuning on the GTSDB training dataset</head><p>The German Traffic Sign Detection Benchmark <ref type="bibr" target="#b4">[4]</ref>  As mentioned in the Introduction, participants were required to detect signs belonging to three categories characterized by differences in shape and colour: prohibitory (circular red-and-white signs), mandatory (circular blueand-white signs) and danger signs (upper-triangular red-and-white signs).</p><p>When tuning our pipeline on the GTSDB dataset, for each category we first selected the best parameters for the image preprocessing and interest region detection steps. As the best preprocessing can only be defined in terms of the quality of the regions found by the detectors, the two stages were tuned jointly. Interest region detection is the only stage where the bounding boxes associated with traffic signs are injected into the pipeline: subsequent stages can thus only prune false detections. Therefore, the main aim in tuning jointly preprocessing and interest region detection was to minimize the number of missed traffic sign detections (false negatives, if we consider detection as a binary classification of all the possible bounding boxes). When the number of false negatives is on par, we prefer a tuning that yields less false positives.</p><p>We simultaneously tuned the rescaling factor according to the procedure described in sec. 3.2 as well as whether to use or not the 3 preprocessing steps. The best combinations of preprocessing and rescaling are reported in Tab. 2 and 3, along with the number of false negatives and false positives over the whole training set. To avoid combinatorial explosion of the parameter space, we instead manually selected the detector parameters, according to the typical size of signs in the GTSDB dataset and the idea of using loose thresholds (such as ρ for WaDe and δ for MSER) so to try not to miss even the most challenging, poorly contrasted signs. The selected parameter tunings are: for MSER, δ = 2, max variation = 0.5, and min diversity = 0.2;</p><p>for WaDe, ρ = 0.1, T = 120, and first scale = 8. What is interesting about the results in Tab. 2 is that the use of complementary region detectors turns out very important to let the first stage of the pipeline achieve the highest recall, thus corroborating our reasoning of Sec. 3.2: should only MSER or WaDe be used, sub-optimal results would be achieved. This is because different signs call for different detector strengths:</p><p>for instance, in the case of mandatory signs, the white arrows can create separated blue regions out of a single mandatory sign, especially if the scale of the sign is small, which in turn leads to detection of several MSERs with wrong scales. On the other hand, the circular symmetry of the sign remains evident and is captured effectively by WaDe (Fig. <ref type="figure">6</ref>).</p><p>Next, we tuned on the training set the context-aware filter and the traffic light detector. Given the three categories of traffic signs that need to be de- pruned thanks to the proposed filters are reported in Fig. <ref type="figure">8</ref>.</p><p>Given the best preprocessing and rescaling parameters for each traffic sign category and the tuned final filters, we then estimated the impact of using as feature for the SVM classifier the justapoxition of HOGs extracted from the preprocessed and the color image, instead of using only HOGs from the color image, as done in <ref type="bibr" target="#b5">[5]</ref>. To estimate the performance of different input HOGs+SVM classifier, we perform 10-fold cross-validation on the training set and run the overall pipeline, so as to evaluate the effect of the different features on performance. Results are reported in Tab. 5 in terms of Areas Under the Curves (AUCs) of the precision-recall curves obtained by varying the SVM decision threshold. We can see that, although HOGs from the preprocessed image is not always better than HOGs from the color image, their combination achieves always a better AUC. This is especially true for Mandatory signs, which is indeed the most challenging category. From the cross-validation we also obtained as best parameters for the SVM classifier the values C = 1 and γ = 0.01, i.e. the same values as already used in our online submission <ref type="bibr" target="#b5">[5]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results on the GTSDB test dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results on mobile mapping data</head><p>Although the images of the GTSDB dataset are hard to classify automatically, they do not fully represent the challenges of real mobile mapping applications. In particular, only one frontal camera is used in the GTSDB dataset, and therefore signs never appear particularly skewed with respect to the camera, whereas this is quite common in a mobile mapping scenario, where more cameras are often used and the same sign can, therefore, appear e.g. parallel to the image plane in the frontal view and highly skewed in a lateral camera, (or viceversa for signs in lateral roads, on gates, etc... ).</p><p>Moreover, traffic sign sizes in real data spans a much greater range than in the GTSDB: in particular, very small signs are more common than in the GTSDB, perhaps because the latter were not included in the dataset by the competition organizers to ease the detection task. On the other hand, as the data are correctly geo-referenced, it is sufficient to detect a sign in one of the camera views to consider it as detected : there is no practical advantage in detecting the same physical traffic sign in more than one view. Therefore, also the evaluation methodology is different: we define a sign as detected (true positive) if it is detected in at least one of the images where it appears, false negative otherwise.</p><p>The dataset that we use to test our pipeline was provided by Qonsult We used as training data all the images of the GTSDB dataset. We expect to obtain better performance when a training set of Italian signs will be available, as there are significant formatting differences in traffic signs even between European countries. Nonetheless, we chose not to use as training data a subset of the mobile mapping data so as not to reduce the size of the test data and, hence, the statistical significance of the results. In spite of the differences between the training and testing data, our pipeline was able to achieve promising results, as vouched by Fig. <ref type="figure" target="#fig_4">11</ref>, left. Indeed, we could achieve 78.21% AUC for the prohibitory class, 82.13% for the danger class and 72.78% for the mandatory class. The results also suggest that our pipeline can withstand changes in appearance of traffic signs that are frequent in real-world mobile mapping data, detecting dark, skewed and very small signs, as shown by the results in Fig. <ref type="figure" target="#fig_4">11</ref>, right. The main source of errors are signs that were not included in the ground-truth of the testing dataset, but are nevertheless detected by the pipeline, such as the "no parking" signs, together with extremely small or skewed signs. Some detection errors on the mobile mapping dataset are shown in Fig. <ref type="figure" target="#fig_13">12</ref>.</p><p>Finally, we report a quantitative comparison on this dataset. Due to the difficulty in implementing the other proposals participating in the online competition, and the unavailability of public implementations, we opted for comparing our pipeline against the standard, sliding window approach to detect objects deploying HOG <ref type="bibr" target="#b2">[2]</ref> and a state-of-the-art sliding window detector based on Integral Channel Features (ICF) <ref type="bibr" target="#b23">[23]</ref>, by using the implementation provided by the CCV library (http://libccv.org) and setting its parameters according to how the pipeline was used in <ref type="bibr" target="#b24">[24]</ref>. Results are reported again in Fig. <ref type="figure" target="#fig_4">11</ref>  be dramatically improved by deploying description and classification only at promising locations identified by interest region detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>The traffic sign detection system we propose demonstrates good performance under challenging conditions such as varying illumination, partial occlusions and large scale variations. Evaluation on the German Traffic Sign Detection Benchmark shows the effectiveness of the proposed approach: our system is able to yield nearly optimal performance on two classes and very good results on the most challenging class of mandatory signs. This can be ascribed to the deployment of complementary interest region detectors, which allows for injecting into the pipeline almost all regions corresponding to traffic signs while notably reducing the number of candidates with respect to a sliding window approach, as well as to the development of effective filters based on traffic light detection and context information. Results on a challenging mobile mapping dataset of Italian signs show the robustness of the proposed approach: our pipeline can successfully be deployed in real and challenging application scenarios.</p><p>Future directions of research concerns experimenting with and developing novel representations for interest regions which could be more inherently</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>[ 13 ]Figure 1 :</head><label>131</label><figDesc>Figure 1: Proposed traffic sign detection pipeline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Enhancement of blue channel. Left, original image (a detail): a very bright mandatory sign; center, enhanced image with blue/green comparison: the sign is not visible; right, enhanced image without blue/green comparison: visibility improves significantly.</figDesc><graphic coords="10,126.36,125.79,116.60,116.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illumination compensation. Top row: input patch and histogram of value V (left); piecewise linear mapping (center); output patch and new histogram of value V (right). Central row: original signs. Bottom row: patches obtained by the proposed illumination compensation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Let B = {b 1 ,Figure 4 :</head><label>14</label><figDesc>Figure 4: Features exploited by the context-aware filter: h is the height of the sign in the image, i.e. the vertical coordinate of the top side of the sign bounding box; r is the height of the sign bounding box</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>[0, 1 ]</head><label>1</label><figDesc>. A graphic illustration of the two values h and r extracted from a real bounding box is shown in Fig. 4. As our goal is defining a classification problem whereby true traffic signs and a false positives are told apart based on context, it turns out hard to model the negative class due to the dependence of negative samples on the actual classification method and parameters employed throughout the previous stages of the pipeline. Due to this difficulty of defining -and thus learning -the negative class, we rely on a generative classifier, with a parametric model aimed at explaining positives samples (i.e. true traffic signs) learned from training data. The idea is to learn the relationship between r and h by means of a parametric model, represented by a family of one-dimensional Gaussian distributions parametrized by the possible normalized size values r, each Gaussian modeling the probability distribution of the normalized height value h given the size r and the event S that the bounding box represents a traffic sign: p (h|r, S) ∼ N (µ (r) , σ (r)) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 ,</head><label>5</label><figDesc>Fig. 5, two of them, referred to as ROI ref , ROI in , are centered on the two</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Examples of traffic lights detected by the proposed filter. For each example, auxiliary ROIs are depicted respectively in green (ROI l , ROI r , ROI u , ROI d ), blue (ROI ref ) and red (ROI in ).</figDesc><graphic coords="21,152.68,128.95,60.46,79.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>is a publicly available dataset targeting traffic sign detection. An on-line competition was also launched when the dataset was released, at the beginning of December 2012, and the submission phase was closed at the end of February 2013. The dataset made available to participants for download features 900 images (split into 600 training images, released at the start of the competition, and 300 evaluation images without ground-truth, released when the submission phase started) with very tough size and illumination conditions variations . In this subsection, we present the tuning of the pipeline that was carried out on the training images during the first part of the competition. The next subsection presents and discusses the results achieved on the test set by the tuned pipeline .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :Figure 7 :Figure 8 :</head><label>678</label><figDesc>Figure 6: Complementarity of region detectors. Top-row: mandatory sign; bottom row: prohibitory sign. From left to right: input image (a detail); MSER detections (coloured regions); WaDe detections (circles, correct detection in red). Note that preprocessing is different for the two detectors and on the last image there are no detections by WaDe due to poor contrast.</figDesc><graphic coords="25,188.56,204.49,77.72,77.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Left chart: results of the proposed pipeline on the three categories of the test GTSDB dataset. Right chart: our results in the competition [5].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: All errors yielded by the proposed pipeline at its best working point on the GTSDB test set: false positives in green, false negatives in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Figure 11: Left: results of the proposed pipeline vs a sliding window HOG and sliding window ICF on the mobile mapping dataset. Right: some correct detections of the proposed pipeline at the best working point.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>, right. The AUCs for sliding window HOG are 30.98% for the Prohibitory class, 46.8% for the Danger class and 5.5% for the Mandatory class; for ICF are 52.94% for the Prohibitory class, 62.92% for the Danger class and 33.19% for the Mandatory class. The relative ranking among the different sign categories is the same for all detectors. The comparison between sliding window classifiers and our pipeline shows that performance can</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Some errors on the mobile mapping dataset at the best working point: false positives in green, false negatives in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Constraints on bounding box size and aspect ratio</figDesc><table><row><cell>Min. area</cell><cell>225</cell></row><row><cell>Max area</cell><cell>27300</cell></row><row><cell>Min aspect ratio</cell><cell>0.6</cell></row><row><cell>Max. aspect ratio</cell><cell>1.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>MSER and WaDe performance for different sign categories.</figDesc><table><row><cell>Category</cell><cell cols="5">Detector 3CH-CS Enhance/Norm. 1CH-CS FN</cell><cell>FP</cell></row><row><cell>Prohibitory</cell><cell>MSER</cell><cell>No</cell><cell>Norm.</cell><cell>No</cell><cell>0</cell><cell>1,127K</cell></row><row><cell>Prohibitory</cell><cell>WaDe</cell><cell>Yes</cell><cell>Norm.</cell><cell>Yes</cell><cell>2</cell><cell>947K</cell></row><row><cell>Mandatory</cell><cell>MSER</cell><cell>Yes</cell><cell>Norm.</cell><cell>No</cell><cell>6</cell><cell>365K</cell></row><row><cell>Mandatory</cell><cell>WaDe</cell><cell>Yes</cell><cell>Enhance</cell><cell>Yes</cell><cell>1</cell><cell>1,498K</cell></row><row><cell>Danger</cell><cell>MSER</cell><cell>No</cell><cell>Norm.</cell><cell>No</cell><cell>2</cell><cell>1,129K</cell></row><row><cell>Danger</cell><cell>WaDe</cell><cell>No</cell><cell>Norm.</cell><cell>Yes</cell><cell cols="2">12 1,979K</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Rescaling factors of interest regions bounding boxes</figDesc><table><row><cell>Category</cell><cell>m</cell><cell>q</cell></row><row><cell>Prohibitory MSER-Normalized Red</cell><cell cols="2">1.09 -0.3</cell></row><row><cell cols="3">Prohibitory MSER+ Normalized Red 1.43 0.83</cell></row><row><cell>Prohibitory MSER-Grey-scale</cell><cell cols="2">1.43 0.83</cell></row><row><cell>Danger MSER-Normalized Red</cell><cell cols="2">1.1 -0.75</cell></row><row><cell>Danger MSER+ Normalized Red</cell><cell cols="2">1.47 0.73</cell></row><row><cell>Danger MSER-Grey-scale</cell><cell cols="2">1.47 0.73</cell></row><row><cell cols="3">Mandatory MSER+ Normalized Blue 1.09 -0.54</cell></row><row><cell>Prohibitory WaDe-Normalized Red</cell><cell cols="2">1.21 -3.65</cell></row><row><cell cols="3">Prohibitory WaDe+ Normalized Red 1.53 -1.11</cell></row><row><cell>Danger WaDe-Normalized Red</cell><cell cols="2">0.84 6.65</cell></row><row><cell>Danger WaDe+ Normalized Red</cell><cell cols="2">0.95 6.06</cell></row><row><cell>Mandatory WaDe-Enhanced Blue</cell><cell cols="2">1.18 -2.56</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Comparison between HOGs extracted on color images and the juxtaposition of</figDesc><table><row><cell cols="3">HOGs from color and preprocessed images</cell><cell></cell></row><row><cell>Category</cell><cell cols="3">Color AUC Preprocessed AUC Color + preprocessed AUC</cell></row><row><cell>Prohibitory</cell><cell>99.11</cell><cell>99.21</cell><cell>99.43</cell></row><row><cell>Mandatory</cell><cell>89.66</cell><cell>93.27</cell><cell>95.01</cell></row><row><cell>Danger</cell><cell>96.89</cell><cell>96.32</cell><cell>97.22</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">on color information, while still proving robust to the severe photometric variations found in typical outdoor conditions. The fusion of information from registered images in mobile mapping data to enhance the pipeline robustness and effectiveness is also worth to be pursued</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust real-time object detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<imprint>
			<publisher>CVPR</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schlipsing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Detection of traffic signs in real-world images: The German Traffic Sign Detection Benchmark</title>
		<author>
			<persName><forename type="first">S</forename><surname>Houben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schlipsing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Joint Conference on Neural Networks</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A traffic sign detection pipeline based on interest region extraction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Petrelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Fioraio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Joint Conference on Neural Networks (IJCNN)</title>
		<meeting>of International Joint Conference on Neural Networks (IJCNN)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A system for traffic sign detection, tracking, and recognition using color, shape and motion information</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bahlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Symposium on Intelligent Vehicles</title>
		<meeting>IEEE Symposium on Intelligent Vehicles</meeting>
		<imprint>
			<biblScope unit="page" from="255" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Traffic sign detection as a component of an automated traffic infrastructure inventory system</title>
		<author>
			<persName><forename type="first">K</forename><surname>Brkic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Legvic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Workshop of the Austrian Association for Pattern Recognition</title>
		<meeting>Workshop of the Austrian Association for Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast shape-based road sign detection for a driver assistance system</title>
		<author>
			<persName><forename type="first">G</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<biblScope unit="page">7075</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zelinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fletcher</surname></persName>
		</author>
		<title level="m">Real-Time Speed Sign Detection Using the Radial Symmetry Detector, in: Intelligent Transportation Systems, International IEEE Conference on</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="322" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Angle vertex and bisector geometric model for triangular road sign detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Belaroussi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Tarel</surname></persName>
		</author>
		<imprint>
			<publisher>WACV</publisher>
			<biblScope unit="page" from="577" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A real-time road sign detection using bilateral chinese transform</title>
		<author>
			<persName><forename type="first">R</forename><surname>Belaroussi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Tarel</surname></persName>
		</author>
		<imprint>
			<publisher>WACV</publisher>
			<biblScope unit="page" from="1161" to="1170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A single target voting scheme for traffic sign detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Houben</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicles Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="124" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Goal Evaluation of Segmentation Algorithms for Traffic Sign Recognition, ITS</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gomez-Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maldonado-Bascon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gil-Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lafuente-Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="917" to="930" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Real-time traffic sign recognition from video by class-specific discriminative features</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ruta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Rec</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="416" to="430" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Real-Time Detection and Recognition of Road Traffic Signs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Greenhalgh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirmehdi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on ITS</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1498" to="1506" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-view traffic sign detection, recognition, and 3D localisation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MVA</title>
		<imprint>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Robust wide-baseline stereo from maximally stable extremal regions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IVC</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="761" to="767" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Keypoints from symmetries by wave propagation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lanza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
		<editor>CVPR, IEEE</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2898" to="2905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<author>
			<persName><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speeded-up robust features (SURF)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="346" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Robust regression using iteratively reweighted least-squares</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Welsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Statistics: Theory and Methods</title>
		<imprint>
			<biblScope unit="page" from="813" to="827" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<title level="m">Integral channel features</title>
		<imprint>
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Traffic sign recognition -how far are we from the solution?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<publisher>ICJNN</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
