<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Masked Autoencoders As Spatiotemporal Learners</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-05-18">18 May 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">equal contribution Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">equal contribution Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yanghao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">equal contribution Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">equal contribution Facebook AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Masked Autoencoders As Spatiotemporal Learners</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-05-18">18 May 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2205.09113v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper studies a conceptually simple extension of Masked Autoencoders (MAE) <ref type="bibr" target="#b30">[31]</ref> to spatiotemporal representation learning from videos. We randomly mask out spacetime patches in videos and learn an autoencoder to reconstruct them in pixels. Interestingly, we show that our MAE method can learn strong representations with almost no inductive bias on spacetime (only except for patch and positional embeddings), and spacetime-agnostic random masking performs the best. We observe that the optimal masking ratio is as high as 90% (vs. 75% on images <ref type="bibr" target="#b30">[31]</ref>), supporting the hypothesis that this ratio is related to information redundancy of the data. A high masking ratio leads to a large speedup, e.g., &gt; 4× in wall-clock time or even more. We report competitive results on several challenging video datasets using vanilla Vision Transformers <ref type="bibr" target="#b17">[18]</ref>. We observe that MAE can outperform supervised pre-training by large margins. We further report encouraging results of training on real-world, uncurated Instagram data. Our study suggests that the general framework of masked autoencoding (BERT [15], MAE [31], etc.) can be a unified methodology for representation learning with minimal domain knowledge.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The deep learning community is experiencing a trend of unifying methodologies for solving problems in different areas, such as language, vision, speech, and more. For architectures, Transformers <ref type="bibr" target="#b66">[67]</ref> have been successfully introduced into computer vision <ref type="bibr" target="#b17">[18]</ref> and established as a general building block in both language and vision. For self-supervised representation learning, the denoising/masked autoencoding methodology <ref type="bibr" target="#b67">[68]</ref> in BERT <ref type="bibr" target="#b14">[15]</ref> has been shown effective on learning visual representations from images <ref type="bibr" target="#b30">[31]</ref>. Towards unifying methodologies, less domain knowledge ("fewer inductive biases" <ref type="bibr" target="#b17">[18]</ref>) is introduced for a specific problem, which urges the models to learn useful knowledge almost purely from data.</p><p>Following this philosophy, we study extending Masked Autoencoders (MAE) <ref type="bibr" target="#b30">[31]</ref> to the problem of spatiotemporal representation learning. Our method is simple: we randomly mask out spacetime patches in videos and learn an autoencoder to reconstruct them (Fig. <ref type="figure" target="#fig_0">1</ref>). Our method has minimal domain knowledge: the only spacetime-specific inductive bias is on embedding the patches and their positions; all other components are agnostic to the spacetime nature of the problem. In particular, our encoder and decoder are both vanilla Vision Transformers <ref type="bibr" target="#b17">[18]</ref> with no factorization or hierarchy, and our random mask sampling is agnostic to the spacetime structures. Our method predicts pixel values and uses no extra problem-specific tokenizer. In a nutshell, our method is simply MAE applied to the set of spacetime patches. Despite minimal inductive biases, our method achieves strong empirical results, suggesting that useful knowledge can be learned from data.</p><p>It is hypothesized in <ref type="bibr" target="#b30">[31]</ref> that the masking ratio (i.e., percentage of removed tokens) in masked autoencoding methods is related to the information redundancy of the problems. For example, natural images are more information-redundant than languages and thus the optimal masking ratio is higher (e.g., than BERT <ref type="bibr" target="#b14">[15]</ref>). Our observations on video data support this hypothesis. We find that the optimal masking ratio of MAE is 90% for videos (Fig. <ref type="figure">2</ref>), higher than the masking ratio of 75% for its image counterpart <ref type="bibr" target="#b30">[31]</ref>. This can be understood as a consequence of natural video data being temporally correlated. To the extreme, if a video has T identical static frames, randomly sampling 1 /T of all spacetime patches would reveal most of the static frame. Because slow motion is more likely than fast motion in natural videos, the masking ratio can be very high as we observe empirically. We mask a large subset (e.g., 90%) of random patches in spacetime. An encoder operates on the set of visible patches. A small decoder then processes the full set of encoded patches and mask tokens to reconstruct the input. Except for patch and positional embeddings, neither the encoder, the decoder, nor the masking strategy, has any spatiotemporal inductive bias.</p><p>The higher masking ratio leads to a more efficient solution in practice. Following the MAE in <ref type="bibr" target="#b30">[31]</ref> that applies the encoder only on visible tokens, a masking ratio of 90% reduces the encoder time and memory complexity to &lt;1/10. Put together with a small decoder <ref type="bibr" target="#b30">[31]</ref>, the MAE pre-training can achieve a theoretically 7.7× reduction in computation vs. encoding all tokens. In fact, the computation reduction is so large that the data loading time becomes a new bottleneck; even so, we record a 4.1× wall-clock speedup. Such a significant speedup is of great importance for video research that is large-scale and time-consuming.</p><p>We report strong results on a variety of video recognition datasets. Our MAE pre-training greatly improves generalization performance: on Kinetics-400 <ref type="bibr" target="#b34">[35]</ref>, it increases the accuracy of ViT-Large <ref type="bibr" target="#b17">[18]</ref> by absolute 13% vs. training from scratch, while it takes less wall-clock training time overall (pre-training plus fine-tuning). Our MAE pre-training can outperform its supervised pre-training counterpart by big margins. Using vanilla ViT <ref type="bibr" target="#b17">[18]</ref>, our method achieves competitive results with previous state-of-the-art methods that incorporate more domain knowledge. We also report encouraging results using MAE pre-trained on 1 million random, uncurated Instagram videos. These results suggest that self-supervised learning on videos can be tackled in a way similar to its counterparts on language <ref type="bibr" target="#b14">[15]</ref> and images <ref type="bibr" target="#b30">[31]</ref>, under a unified framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Denoising autoencoders. Denoising autoencoders (DAE) <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b68">69]</ref> present a general methodology for learning representations by reconstructing clean signals from corrupted inputs. Masking as a type of noise dates back to at least a decade ago <ref type="bibr" target="#b68">[69]</ref>. One of its most successful developments is BERT <ref type="bibr" target="#b14">[15]</ref>, which is conceptually masked autoencoding on language tokens.</p><p>Denoising/masked autoencoding methods for computer vision have been making continuous progress <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b30">31]</ref>. A series of recent methods are based on Transformer architectures <ref type="bibr" target="#b66">[67]</ref> and are towards a unified solution between vision and language. iGPT <ref type="bibr" target="#b8">[9]</ref> pioneers this direction by training Transformers on pixels as tokens. The ViT paper <ref type="bibr" target="#b17">[18]</ref> makes a revolutionary step forward by using patches as tokens. It not only establishes strong Transformer architectures for vision tasks, but also explores masked prediction with patches. MAE <ref type="bibr" target="#b30">[31]</ref> returns to the basics of the autoencoding concept <ref type="bibr" target="#b67">[68]</ref> and draws attention to the decoding aspect. The presence of a meaningful decoder provides more flexibility, e.g., enabling the encoder to operate only on visible patches and leading to a more efficient solution. It empirically shows that a high masking ratio is essential for image tasks <ref type="bibr" target="#b30">[31]</ref>. Our study follows this line of research.</p><p>Instead of predicting pixels <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b79">80]</ref>, another line of research focuses on the tokenization of the prediction targets <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b76">77]</ref>. BEiT <ref type="bibr" target="#b2">[3]</ref> proposes to use pre-trained dVAE <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b54">55]</ref> as the Figure <ref type="figure">2</ref>: Visualizations on the Kinetics-400 <ref type="bibr" target="#b34">[35]</ref> validation set (masking ratio 90%). We show the original video (top), masked video (middle), and MAE output (bottom) for each sample. This model reconstructs the original pixels. The video size is 16×224×224 and the spacetime patch size is 2×16×16 (the temporal patch size of 2 is not visualized here). Each sample has 8×14×14=1568 tokens with 156 being visible. For better visualizations, the known patches in the output are from the original input. Fig. <ref type="figure" target="#fig_5">7</ref> shows more examples.</p><p>Figure <ref type="figure">3</ref>: Visualizations of the same pre-trained model in Fig. <ref type="figure">2</ref> but with a masking ratio of 95%.</p><p>reconstruction target. The dVAE tokenizer can be improved by perceptual or adversarial losses <ref type="bibr" target="#b16">[17]</ref>.</p><p>MaskFeat <ref type="bibr" target="#b76">[77]</ref> shows that HoG <ref type="bibr" target="#b12">[13]</ref> as prediction targets performs strongly.</p><p>Self-supervised learning on videos. The presence of the temporal dimension is a focus of selfsupervised learning on video data. Related topics include temporal coherence ('slowness') <ref type="bibr" target="#b78">[79,</ref><ref type="bibr" target="#b24">25]</ref>, future prediction <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b15">16]</ref>, object motion <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b75">76]</ref>, temporal ordering <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b80">81]</ref>, spatiotemporal contrast <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b55">56]</ref>, etc.</p><p>Our method also relies on the temporal coherence of videos, but it approaches this goal implicitly. In fact, as our method is largely agnostic to spacetime, the main opportunity for it to make use of the temporal coherence is a higher masking ratio (e.g., 90%), which assumes that videos are more information-redundant than images.  Space-only random sampling, broadcasted to all time steps ("tube" masking <ref type="bibr" target="#b76">[77]</ref>). (c): Time-only random sampling, broadcasted to all spatial locations ("frame" masking <ref type="bibr" target="#b76">[77]</ref>). (d): Block-wise sampling <ref type="bibr" target="#b2">[3]</ref> in spacetime, removing large regions ("cube" masking <ref type="bibr" target="#b76">[77]</ref>). In this illustration, T ×H×W is 8×14×14; green tokens are kept and others are masked out.</p><p>There has been growing interest in masking-based methods for self-supervised learning on videos. Previous works focus on tokenizing the prediction targets for the use of videos <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b76">77]</ref>. Our autoencoding method operates on pixels, which is simpler and requires no extra data or domain knowledge on the tokenizer. Importantly, our method greatly improves the efficiency of learning. The practical speedup is of central importance for video-related research, which is in general larger-scale and more time-consuming.</p><p>Our work is done independently and concurrently with <ref type="bibr" target="#b65">[66]</ref> on a related method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Our method is a simple extension of MAE <ref type="bibr" target="#b30">[31]</ref> to spacetime data (Fig. <ref type="figure" target="#fig_0">1</ref>). Our goal is to develop the method under a general and unified framework, with as little domain knowledge as possible.</p><p>Patch embedding. Following the original ViT <ref type="bibr" target="#b17">[18]</ref>, given a video clip, we divide it into a regular grid of non-overlapping patches in spacetime <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b76">77]</ref>. The patches are flattened and embedded by linear projection <ref type="bibr" target="#b17">[18]</ref>. Positional embeddings <ref type="bibr" target="#b66">[67]</ref> are added to the embedded patches. The patch and positional embedding process is the only process that is spacetime-aware.</p><p>Masking. We sample random patches without replacement from the set of embedded patches. This random sampling is agnostic to the spacetime structure (Fig. <ref type="figure" target="#fig_2">4 (a)</ref>). This structure-agnostic sampling strategy is analogous to that of BERT in 1D <ref type="bibr" target="#b14">[15]</ref> and MAE in 2D <ref type="bibr" target="#b30">[31]</ref>.</p><p>It is hypothesized in <ref type="bibr" target="#b30">[31]</ref> that the optimal masking ratio is related to the information redundancy of the data. With unstructured random masking, BERT <ref type="bibr" target="#b14">[15]</ref> uses a masking ratio of 15% for language and MAE <ref type="bibr" target="#b30">[31]</ref> uses a ratio of 75% for images, suggesting that images are more information-redundant than language. Our empirical results on videos support this hypothesis. The optimal masking ratio we observe is 90%. This is in line with the common assumption that natural videos are more informationredundant than images because of temporal coherence. Fig. <ref type="figure">2 and 3</ref> present our MAE reconstruction results on unseen validation data with a masking ratio of 90% and 95%.</p><p>The spacetime-agnostic sampling can be more effective than structure-aware sampling strategies, e.g., space-only, time-only, or block-wise sampling (Fig. <ref type="figure" target="#fig_2">4 (b-d</ref>)). As neighboring patches in space or in time (Fig. <ref type="figure" target="#fig_2">4(b, c</ref>)) are coherent, with a very high masking ratio, space-only or time-only sampling may retain less information and yield an overly difficult pre-training task. For example, time-only sampling from 8 frames with a masking ratio of 87.5% means keeping only a single frame, which presents an overly challenging task of predicting the future and past given only one frame. We observe that optimal masking ratios for structure-aware sampling are in general lower. In contrast, the spacetime-agnostic sampling better utilizes the limited number of visible patches and thus allows to use a higher masking ratio.</p><p>Autoencoding. Our encoder is a vanilla ViT <ref type="bibr" target="#b17">[18]</ref> applied only on the visible set of embedded patches, following <ref type="bibr" target="#b30">[31]</ref>. This design greatly reduces time and memory complexity and leads to a more practical solution. A masking ratio of 90% reduces the encoder complexity to &lt;1/10 (noting that self-attention is quadratically-complex w.r.t. the token set size).</p><p>Our decoder is another vanilla ViT on the union of the encoded patch set and a set of mask tokens <ref type="bibr" target="#b30">[31]</ref>. Decoder-specific positional embeddings are added to this set <ref type="bibr" target="#b30">[31]</ref>. The decoder is designed to be smaller than the encoder <ref type="bibr" target="#b30">[31]</ref>. Although the decoder processes the full set, its complexity is smaller than the encoder (e.g., ∼1/20 per token). In our default setting, the overall autoencoder has a complexity reduction of 7.7× vs. full encoding (more discussions are in Sec. 5.1 and Table <ref type="table" target="#tab_0">1</ref>).</p><p>The decoder predicts the patches in the pixel space. In principle we can simply predict a full spacetime patch (e.g., t×16×16); in practice, we find it sufficient to predict a single time slice of the patch (16×16), which keeps the prediction layer's size manageable. We predict the original pixels or their per-patch normalized values <ref type="bibr" target="#b30">[31]</ref> (compared in Table <ref type="table" target="#tab_2">2b</ref>). The training loss function is the mean squared error (MSE) between the prediction and its target, averaged over unknown patches <ref type="bibr" target="#b14">[15]</ref>.</p><p>The encoder and decoder are agnostic to the spacetime structure of the problem. There is no hierarchy or spacetime factorization, in contrast to the leading architectures <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b18">19]</ref>. Our method relies on the global self-attention to learn useful knowledge from data, following the spirit of <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementation</head><p>Data pre-processing. For MAE pre-training, our default input size is 16 frames each with 224×224 pixels (i.e., 16×224×224). The 16 frames are sampled from the raw video with a temporal stride of 4 (i.e., 16×4 sampling in the literature <ref type="bibr" target="#b20">[21]</ref>), and the starting frame is randomly sampled. In the spatial domain, we perform random resized cropping <ref type="bibr" target="#b62">[63]</ref> with a scale range of [0.5, 1], and random horizontal flipping. We do not apply other data augmentations unless noted.</p><p>Our MAE pre-training is so fast in computation that data loading becomes a new bottleneck that dominates running time in our setup. We adopt repeated sampling <ref type="bibr" target="#b32">[33]</ref> <ref type="foot" target="#foot_0">1</ref> to alleviate this problem. Each time a raw video is loaded and decompressed, we take multiple (4 by default) samples from it. This reduces the data loading and decompressing time per sample. We note that repeated sampling does not change the number of samples seen; it only influences the orders of the samples seen during training. We always count epochs as "effective epochs", i.e., how many times each raw video is sampled throughout training.</p><p>Architecture. Our encoder and decoder are the vanilla ViT architectures <ref type="bibr" target="#b17">[18]</ref>. We use a temporal patch size of 2 <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b76">77]</ref> and a spatial patch size of 16×16 <ref type="bibr" target="#b17">[18]</ref>, denoted as 2×16×16. We use the same patch size for ViT-B/L/H <ref type="bibr" target="#b17">[18]</ref> for simplicity. For a 16×224×224 input, this patch size produces 8×14×14 tokens.</p><p>We adopt separable positional embeddings for the encoder. We have two positional embeddings, one for space and the other for time. The spacetime positional embeddings are the sum of them. This separable implementation prevents the size of positional embeddings growing too large in 3D. We use learnable positional embeddings; the sin-cos variant <ref type="bibr" target="#b66">[67]</ref> works similarly.</p><p>Settings. Our MAE pre-training configuration mostly follows <ref type="bibr" target="#b30">[31]</ref>. We use the AdamW optimizer <ref type="bibr" target="#b42">[43]</ref> with a batch size of 512. We evaluate the pre-training quality by end-to-end fine-tuning. The choice of evaluating by fine-tuning (instead of linear probing) follows <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b30">31]</ref>. Our inference process follows the common practice of multi-view testing <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b20">21]</ref>: it takes K temporal clips (by default K=7 on Kinetics) to cover the video length, and for each clip it takes 3 spatial views to cover the longer spatial axis (denoted as K×3). The final prediction is the average of all views. The implementation details and hyper-parameters are in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In Sec. 5.1 and Sec. 5.2 we perform ablation experiments on Kinetics-400 (K400) <ref type="bibr" target="#b34">[35]</ref>. We do MAE self-supervised pre-training and then fine-tune the encoder with supervision for evaluation. We report top-1 classification accuracy (%) on the K400 validation set. In Sec. 5.3 we study more pre-training datasets and downstream tasks.   <ref type="table" target="#tab_0">1</ref>. With a masking ratio of 90%, the sparse encoder reduces the FLOPs (floating-point operations) by &gt;10×. After counting the decoder, the sparse design of MAE reduces FLOPs by 7.7×. In our implementation, this reduction should produce a 5.8×computational speedup, if the video data were already pre-processed and loaded in memory. Our speedup ratio is so high that the video pre-processing and loading time becomes a new bottleneck. In our system, the data loading step increases the wall-clock training time from 24.5 hours to 35.8 hours. Nevertheless, this still leads to a significant speedup of 4.1×. <ref type="foot" target="#foot_2">3</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation experiments</head><p>Masking ratio. Fig. <ref type="figure">6</ref> shows the influence of the masking ratio jointly with the pre-training length.</p><p>The ratio of 90% works the best. The ratio of 95% performs surprisingly well, which can catch up if trained long enough (Fig. <ref type="figure">6</ref> left). A higher masking ratio leads to fewer tokens encoded by the encoder; to have a more comprehensive look, we plot the results w.r.t. the total number of encoded tokens (Fig. <ref type="figure">6</ref> right). Under this measure, the ratios of 90% and 95% perform closely.</p><p>The lower masking ratios of 75% and 50% perform worse, even though the encoder sees more tokens and has higher computation cost. The ratio of 75% is optimal for its image counterpart <ref type="bibr" target="#b30">[31]</ref>, but not for videos. This observation can be explained by the assumption that video data is more information-redundant.   <ref type="bibr" target="#b30">[31]</ref>.</p><p>Mask sampling strategy. Our method follows the structure-agnostic random sampling methodology in BERT <ref type="bibr" target="#b14">[15]</ref> and MAE <ref type="bibr" target="#b30">[31]</ref>. Table <ref type="table" target="#tab_2">2a</ref> reports that this simple solution works the best in our method.</p><p>We compare with other strategies as illustrated in Fig. <ref type="figure" target="#fig_2">4</ref>. Space-only sampling, which samples on the 2D spatial axes and broadcasts along the temporal axis, works reasonably well (83.5%). Time-only sampling, with a masking ratio of 75% (i.e., keep 2 time steps out of 8), performs poorly (79.1%); if we increase its masking ratio to 87.5% (keep 1 out of 8), the accuracy drops further to 75.4%. Time-only sampling is related to future/past frame prediction, which can be an overly difficult task in our scenario. Block-wise sampling <ref type="bibr" target="#b2">[3]</ref>, in its spacetime variant <ref type="bibr" target="#b76">[77]</ref>, has 83.2% accuracy with 75% masking ratio (a higher ratio is worse).</p><p>Reconstruction target. Our method performs decently by reconstructing the original, unmodified pixels (83.8%, Table <ref type="table" target="#tab_2">2b</ref>). Using per-patch normalized pixels <ref type="bibr" target="#b30">[31]</ref> improves by 0.6%. This observation is similar to that of its image counterpart <ref type="bibr" target="#b30">[31]</ref>. Using HOG <ref type="bibr" target="#b12">[13]</ref> as the target <ref type="bibr" target="#b76">[77]</ref> works strongly too.</p><p>The autoencoding nature of our method (i.e., predicting pixels) provides a self-contained solution. In contrast, an extra tokenizer (e.g., dVAE <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b8">9]</ref>), as is used in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b72">73]</ref>, may require external data to train and additional domain knowledge to design (e.g., the dVAE used is a ConvNet <ref type="bibr" target="#b36">[37]</ref>). Applying the extra dVAE tokenizer to each frame is computationally heavy, which slows down training by 1.6× in our implementation. Our pixel-based method is simpler and performs better (Table <ref type="table" target="#tab_2">2b</ref>).</p><p>Data augmentation. Temporal data can provide natural augmentation, e.g., on view points, motion, deformation, occlusion. These forms of natural augmentation have been incorporated by random temporal sampling. Table <ref type="table" target="#tab_2">2c</ref> compares additional augmentation on the spatial domain. Even using no spatial augmentation (center crop only) works competitively, similar to the observation on images <ref type="bibr" target="#b30">[31]</ref>. Random cropping with a mild scale range of [0.5, 1] works well, while stronger cropping (range [0.08, 1], <ref type="bibr" target="#b62">[63]</ref>) reduces accuracy; adding color jittering reduces accuracy too, similar to <ref type="bibr" target="#b30">[31]</ref>. It is practically valuable for self-supervised learning methods to be less dependent on data augmentation. There are a variety of applications in which augmentation is not valid or is hard to induce, e.g., medical imaging, hyper-spectral imaging, remote sensing, geometric data (point cloud, key points, etc.), and their temporal extensions. Our method could be generalized to these cases.</p><p>Repeated sampling. As our method is fast in computation, we adopt repeated sampling <ref type="bibr" target="#b32">[33]</ref> to reduce the data loading overhead. Table <ref type="table" target="#tab_2">2d</ref> reports its influence. Using 2 or 4 repetitions increases wall-clock speed by 1.8× or 3.0×, as a loaded and decompressed file is reused multiple times.</p><p>Decoder capacity. Table <ref type="table" target="#tab_2">2e</ref> and 2f report the influence of the decoder width and depth. Using an overly small decoder degrades accuracy by large margins. This is unlike its image counterpart <ref type="bibr" target="#b30">[31]</ref>, in which a 128-d or 1-block decoder has no degradation if fine-tuning is applied. We hypothesize that the higher-dimensional video data are more complex and thus require higher decoding capacity. On the other hand, our optimal decoder (512-d, 4-block) is still substantially smaller than the encoder (1024-d, 24-block). This is similar to the observation on its image counterpart <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Influence of Data</head><p>Transfer learning ablation. Table <ref type="table" target="#tab_3">3</ref> studies pre-training on different datasets and transferring to various downstream tasks. The pre-training datasets include ImageNet-1K (IN1K) <ref type="bibr" target="#b13">[14]</ref> and Kinetics-400, 600, and 700 <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. The downstream tasks include K400, AVA <ref type="bibr" target="#b28">[29]</ref>, and SomethingSomething v2 (SSv2) <ref type="bibr" target="#b26">[27]</ref>. We do not perform any intermediate fine-tuning (see appendix), so the comparison here is influenced by the data scale/distribution but not by the number of their labels.   Table <ref type="table" target="#tab_6">4</ref> presents more results on the dataset size and training epochs. Pre-training on a 240k subset of IG-curated (the same size as K400) performs worse on K400 classification, which can be caused by the domain shift of data. However, increasing the dataset size of IG-curated to 512k and 1M shows good gains: under the same number of pre-training epochs (200 and 400), it can outperform K400 pre-training even when evaluating on K400. IG-uncurated performs similarly well as IG-curated, although the videos are randomly sampled and unrelated to K400 classes. This behavior is not observed on contrastive learning methods for videos: e.g., in <ref type="bibr" target="#b21">[22]</ref> it is empirically shown that data curation has a major impact on contrastive learning <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b27">28]</ref> performance.</p><p>We believe that our exploration with real-world data has encouraging results. It is a more realistic use case of unsupervised learning at scale. We hope this exploration will shed light on future study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">System-level Comparisons</head><p>We provide system-level comparisons with the leading results on K400, AVA, and SSv2. The detailed tables are in the appendix (Table <ref type="table" target="#tab_10">7</ref>, 8, 9). These results are multifaceted, involving architecture designs, computational complexity, model sizes, input resolution, pre-training data and methods, etc., as we summarize in the tables. Our results are competitive and are close to the leading entries. In particular, our results are based only on vanilla ViT architectures, while the leading methods are hierarchical or specialized for videos. Our results demonstrate the potential of using fewer inductive biases and learning more from data, which is a pursuit of self-supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Video Pre-training for Image Recognition</head><p>Finally, we report preliminary results on video pre-training for image recognition. The usage of vanilla ViT allows to convert to 2D easily: we only "deflate" patch embeddings by summing in time.</p><p>Using ViT-L pre-trained by MAE on K400 / IG-uncurated, we obtain 83.7% / 84.1% accuracy on IN1K image classification. This is better than training ViT-L from scratch on IN1K (82.6% <ref type="bibr" target="#b30">[31]</ref>), though lower than MAE pre-training on IN1K (85.9% <ref type="bibr" target="#b30">[31]</ref>). Considering the large domain gap, we believe this result is decent and its improvement over training from scratch is encouraging. We hope it will motivate the community to explore video pre-training for general visual representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have explored a simple extension of MAE <ref type="bibr" target="#b30">[31]</ref> to video data. We have drawn several interesting observations. (i) We find that it is possible to learn strong representations with minimal domain knowledge or inductive biases. This follows the spirit of the ViT paper <ref type="bibr" target="#b17">[18]</ref>. Similar to BERT <ref type="bibr" target="#b14">[15]</ref> and MAE <ref type="bibr" target="#b30">[31]</ref>, we show that self-supervised learning on videos can be tackled in a conceptually unified framework. (ii) We empirically show that the masking ratio is an important factor for general masked autoencoding methods <ref type="bibr" target="#b68">[69]</ref>, and its optimal values may depend on the nature of the data (language, images, videos, etc.). (iii) We report encouraging results of pre-training on real-world, uncurated data. It achieves strong performance, close to pre-training on controlled, curated data (e.g., Kinetics). To the best of our knowledge, promising results on uncurated data are rare in the literature.</p><p>In spite of these observations, open problems remain. The scale of data we have explored is orders of magnitudes smaller than the language counterparts <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b4">5]</ref>. While our method has largely improved the efficiency of self-supervised learning, the high-dimensional video data still present a major challenge for scaling up. We hope our study will provide initial signals for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Implementation Details</head><p>Kinetics action classification. Our settings mainly follow <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b76">77]</ref>. Table <ref type="table" target="#tab_8">5a</ref> summarizes our pretraining settings on Kinetics. Table <ref type="table" target="#tab_8">5b</ref> shows the corresponding fine-tuning settings for ViT-B/L/H. For fine-tuning, we add a linear classifier layer to the encoder's averaged tokens <ref type="bibr" target="#b17">[18]</ref>.</p><p>For fine-tuning the intermediately fine-tuned checkpoints from K600 in Table <ref type="table" target="#tab_10">7</ref>, we use the setting in Table <ref type="table" target="#tab_8">5b</ref> with a lower learning rate (8e-4) and shorter duration (40 epochs for ViT-L; 30 for ViT-H) and an increased drop path rate of 0.3 for ViT-H.</p><p>AVA action detection. Table <ref type="table" target="#tab_9">6a</ref> summarizes our fine-tuning settings on AVA <ref type="bibr" target="#b28">[29]</ref>. The settings mainly follow <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b76">77]</ref>. We follow the detection architecture in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b76">77]</ref> that adapts Faster R-CNN <ref type="bibr" target="#b56">[57]</ref> for video action detection. Only for the AVA results in Table <ref type="table" target="#tab_12">8</ref>, we use relative positions <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b53">54]</ref> (as implemented in <ref type="bibr" target="#b38">[39]</ref>) during fine-tuning.</p><p>SSv2 action classification. Table <ref type="table" target="#tab_9">6b</ref> summarizes our fine-tuning settings on SSv2 <ref type="bibr" target="#b26">[27]</ref>. The settings mainly follow <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b76">77]</ref>. For the frame sampling, we split each video into segments, and sample one frame from each segment to form a clip following <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>Fine-tuning from image pre-training. In Table <ref type="table" target="#tab_3">3</ref>   <ref type="bibr" target="#b85">[86]</ref> 0.8 cutmix <ref type="bibr" target="#b83">[84]</ref> 1.0 label smoothing <ref type="bibr" target="#b63">[64]</ref> 0.1 drop path <ref type="bibr" target="#b33">[34]</ref> 0.1 0.2 0.2 dropout <ref type="bibr" target="#b59">[60]</ref> 0.3 0.3 0.5 layer-wise decay <ref type="bibr" target="#b10">[11]</ref> 0.65 0.75 0.8 (b) Kinetics fine-tuning  <ref type="bibr" target="#b85">[86]</ref> 0.8 cutmix <ref type="bibr" target="#b83">[84]</ref> 1.0 label smoothing <ref type="bibr" target="#b63">[64]</ref> 0.1 drop path <ref type="bibr" target="#b33">[34]</ref> 0.2 dropout <ref type="bibr" target="#b59">[60]</ref> 0.5 layer-wise decay <ref type="bibr" target="#b10">[11]</ref> 0.75 (L) 0.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 System-level Comparisons</head><p>Kinetics-400. Table <ref type="table" target="#tab_10">7</ref> compares on Kinetics-400 (K400). Our results are competitive with the leading ones. Importantly, our method is much simpler than many other entries. Our method is the only leading entry based on vanilla ViT, while others were based on hierarchical or specialized designs for videos. We use the standard 224×224 spatial resolution, while higher-resolution fine-tuning and testing may improve results at a higher cost. Our model does not use relative position embedding, which could have extra gains that are orthogonal to our thesis. Our results can compete with some strong results that were based on in-house data for supervision.</p><p>AVA. Table <ref type="table" target="#tab_12">8</ref> compares on AVA <ref type="bibr" target="#b28">[29]</ref> action detection. Using only a resolution of 16×224 2 , our results are close to those of MaskFeat on higher-resolution inputs (40×312 2 ). Importantly, our architectures are plain ViT models without feature hierarchies, yet they perform strongly on this detection task.</p><p>SSv2.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Masked Autoencoders as spatiotemporal learners. We mask a large subset (e.g., 90%) of random patches in spacetime. An encoder operates on the set of visible patches. A small decoder then processes the full set of encoded patches and mask tokens to reconstruct the input. Except for patch and positional embeddings, neither the encoder, the decoder, nor the masking strategy, has any spatiotemporal inductive bias.</figDesc><graphic url="image-1.png" coords="2,379.67,84.07,105.40,114.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Mask sampling. (a): Random sampling that is spacetime-agnostic. (b):Space-only random sampling, broadcasted to all time steps ("tube" masking<ref type="bibr" target="#b76">[77]</ref>). (c): Time-only random sampling, broadcasted to all spatial locations ("frame" masking<ref type="bibr" target="#b76">[77]</ref>). (d): Block-wise sampling<ref type="bibr" target="#b2">[3]</ref> in spacetime, removing large regions ("cube" masking<ref type="bibr" target="#b76">[77]</ref>). In this illustration, T ×H×W is 8×14×14; green tokens are kept and others are masked out.</figDesc><graphic url="image-17.png" coords="4,230.75,72.14,65.12,79.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: MAE pre-training plus fine-tuning is much more accurate and faster than training from scratch. Here the x-axis is the wall-clock training time (128 A100 GPUs), and the y-axis is the 1-view accuracy on Kinetics-400 validation. The table shows the final accuracy. The model is ViT-L.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>85 (H) (b) SSv2 fine-tuning</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: More visualizations on Kinetics-400 following Fig. 2 (masking ratio 90%).</figDesc><graphic url="image-33.png" coords="14,109.98,604.12,196.00,74.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>table shows the final accuracy. The model is ViT-L. Training time comparison between a dense encoder (w/ [M]) and a sparse encoder (w/o [M]) in MAE. The encoder is ViT-L (1024-d, 24-block); the decoder is our default (512-d, 4-block).With a masking ratio of 90%, the sparse variant reduces FLOPs by 7.7×. This reduces computation time by 5.8×. In our infra, computation is so fast that data loading becomes a bottleneck, which leads to an actual speedup of 4.1×. Profiling is with synchronized SGD over 16 nodes, each with 8 A100 GPUs and 80 CPU cores. The training length is 800 epochs.</figDesc><table><row><cell>MAE w/</cell><cell>acc.</cell><cell>FLOPs</cell><cell>compute</cell><cell>load+compute</cell></row><row><cell>encoder w/ [M]</cell><cell>84.3</cell><cell>627.5 G</cell><cell>141.1 hr</cell><cell>147.5 hr</cell></row><row><cell>encoder w/o [M]</cell><cell>84.4</cell><cell>81.0 G</cell><cell>24.5 hr</cell><cell>35.8 hr</cell></row><row><cell>gain</cell><cell></cell><cell>7.7×</cell><cell>5.8×</cell><cell>4.1×</cell></row></table><note>5.1 PerformanceFig.5compares MAE pre-training vs. no pre-training (i.e., training from scratch), using vanilla ViT-L<ref type="bibr" target="#b17">[18]</ref>. The from-scratch recipe follows<ref type="bibr" target="#b76">[77]</ref> and has 71.4% accuracy.2 As a comparison, using MAE pre-training for 800 epochs, the same vanilla ViT-L achieves 84.4% accuracy, which has a large increase of 13.0% absolute vs. training from scratch. This gap is much larger than that on image recognition tasks (∼3%<ref type="bibr" target="#b30">[31]</ref>), suggesting that MAE pre-training is more helpful for video recognition. In addition to the accuracy gain, MAE pre-training can reduce the overall training cost, as plotted in Fig. 5. The 800-epoch MAE pre-training only takes 35.8 hours. A short fine-tuning (100 epochs here), which takes 16.3 hours, achieves good accuracy thanks to pre-training. The overall training time can be shorter than training from scratch (e.g., 400 epochs, 65.2 hours), which converges more slowly without pre-training. This shows that MAE is a practical solution to video recognition. MAE pre-training is fast because its encoder is only applied on the sparse set of visible patches, without the mask token [M]. We profile the pre-training performance in Table</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Masking ratio. Every point represents a single pre-training and fine-tuning experiment. Left: x-axis is the epochs (proportional to the number of decoded tokens). Right: x-axis is the number of encoded tokens.</figDesc><table><row><cell></cell><cell>85</cell><cell></cell><cell></cell><cell></cell><cell>84.4</cell><cell>84.8</cell><cell></cell><cell>85</cell><cell></cell><cell></cell></row><row><cell></cell><cell>84</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>84</cell><cell></cell><cell></cell></row><row><cell>accuracy (%)</cell><cell>82 83</cell><cell>81.5</cell><cell></cell><cell>83.3</cell><cell cols="2">mask ratio 50% mask ratio 75%</cell><cell>accuracy (%)</cell><cell>82 83</cell><cell></cell><cell></cell><cell>mask ratio 50% mask ratio 75%</cell></row><row><cell></cell><cell>81</cell><cell></cell><cell></cell><cell></cell><cell cols="2">mask ratio 90%</cell><cell></cell><cell>81</cell><cell></cell><cell></cell><cell>mask ratio 90%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">mask ratio 95%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>mask ratio 95%</cell></row><row><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>80</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">200</cell><cell cols="2">400</cell><cell>800</cell><cell>1600</cell><cell></cell><cell></cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>64</cell><cell>128</cell><cell>256</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2"># epochs (log-scale)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2"># encoded tokens (billion, log-scale)</cell></row><row><cell cols="3">Figure 6: case</cell><cell>ratio</cell><cell>acc.</cell><cell>case</cell><cell></cell><cell></cell><cell></cell><cell>acc.</cell><cell></cell><cell>case</cell><cell>acc.</cell></row><row><cell cols="3">agnostic</cell><cell>90</cell><cell>84.4</cell><cell cols="5">pixel (w/o norm) 83.8</cell><cell></cell><cell>center crop</cell><cell>83.9</cell></row><row><cell cols="4">space-only 90</cell><cell>83.5</cell><cell cols="4">pixel (w/ norm)</cell><cell>84.4</cell><cell></cell><cell>rand crop</cell><cell>84.4</cell></row><row><cell cols="3">time-only</cell><cell>75</cell><cell>79.1</cell><cell>HOG</cell><cell></cell><cell></cell><cell></cell><cell>84.0</cell><cell></cell><cell>rand crop (stronger) 83.4</cell></row><row><cell cols="3">block</cell><cell>75</cell><cell>83.2</cell><cell cols="2">dVAE token</cell><cell></cell><cell></cell><cell>83.8</cell><cell></cell><cell>rand crop + color jit 83.8</cell></row><row><cell cols="5">(a) Mask sampling. See also Fig. 4.</cell><cell cols="6">(b) Reconstruction target. Pixels as</cell><cell>(c) Data augmentation. Strong aug-</cell></row><row><cell cols="5">Random sampling that is spacetime-</cell><cell cols="6">reconstruction targets work well with</cell><cell>mentation is unnecessary.</cell></row><row><cell cols="4">agnostic works the best.</cell><cell></cell><cell cols="4">no domain knowledge.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>rep.</cell><cell>acc.</cell><cell>speed</cell><cell></cell><cell>dim</cell><cell></cell><cell>acc.</cell><cell></cell><cell></cell><cell>blocks acc.</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell>83.7</cell><cell>1.0×</cell><cell></cell><cell>128</cell><cell></cell><cell>80.8</cell><cell></cell><cell></cell><cell>1</cell><cell>83.2</cell></row><row><cell></cell><cell></cell><cell>2</cell><cell>84.3</cell><cell>1.8×</cell><cell></cell><cell>256</cell><cell></cell><cell>83.1</cell><cell></cell><cell></cell><cell>2</cell><cell>83.6</cell></row><row><cell></cell><cell></cell><cell>4</cell><cell>84.4</cell><cell>3.0×</cell><cell></cell><cell>512</cell><cell></cell><cell>84.4</cell><cell></cell><cell></cell><cell>4</cell><cell>84.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1024</cell><cell></cell><cell>83.7</cell><cell></cell><cell></cell><cell>8</cell><cell>84.3</cell></row><row><cell cols="5">(d) Repeated sampling. All entries</cell><cell cols="6">(e) Decoder width. Unlike the image</cell></row><row><cell cols="5">see the same # samples. Data loading</cell><cell cols="6">counterpart [31], an overly narrow de-</cell></row><row><cell cols="4">overhead is reduced.</cell><cell></cell><cell cols="6">coder degrades accuracy noticeably.</cell></row></table><note>(f) Decoder depth. Unlike the image counterpart<ref type="bibr" target="#b30">[31]</ref>, an overly shallow decoder degrades accuracy.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation experiments on Kinetics-400. The model is ViT-L, with an input size of 16×224×224 and a spacetime patch size of 2×16×16. The pre-training length is 800 epochs. The entries marked in gray are the same, which specify the default settings. This table format follows</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Influence of pre-training data, evaluated on K400, AVA, and SSv2 as the downstream tasks.</figDesc><table><row><cell>pre-train set</cell><cell cols="2"># pre-train data pre-train method</cell><cell>K400</cell><cell>AVA</cell><cell>SSv2</cell></row><row><cell>-</cell><cell>-</cell><cell>none (from scratch)</cell><cell>71.4</cell><cell>-</cell><cell>-</cell></row><row><cell>IN1K</cell><cell>1.28M</cell><cell>supervised</cell><cell>78.6</cell><cell>17.3</cell><cell>50.2</cell></row><row><cell>IN1K</cell><cell>1.28M</cell><cell>MAE</cell><cell>82.3</cell><cell>26.3</cell><cell>65.6</cell></row><row><cell>K400</cell><cell>240k</cell><cell>supervised</cell><cell>-</cell><cell>21.6</cell><cell>55.7</cell></row><row><cell>K400</cell><cell>240k</cell><cell>MAE</cell><cell>84.8</cell><cell>31.1</cell><cell>72.1</cell></row><row><cell>K600</cell><cell>387k</cell><cell>MAE</cell><cell>84.9</cell><cell>32.5</cell><cell>73.0</cell></row><row><cell>K700</cell><cell>537k</cell><cell>MAE</cell><cell>n/a  †</cell><cell>33.1</cell><cell>73.6</cell></row><row><cell cols="2">IG-uncurated 1M</cell><cell>MAE</cell><cell>84.4</cell><cell>34.2</cell><cell>73.6</cell></row><row><cell cols="6">The MAE pre-training length is 1600 epochs on K400/600/700 and IG-uncurated. No intermediate</cell></row><row><cell cols="6">fine-tuning is used. The model is ViT-L.  † : The K700 training set has 13.9k duplicated videos with the K400</cell></row><row><cell cols="4">validation set (19.9k), so it is not legitimate to train on K700 to get K400 results.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 (</head><label>3</label><figDesc>last row) reports transfer learning results on AVA and SSv2 using IG-uncurated pre-training.</figDesc><table><row><cell>data</cell><cell># videos</cell><cell>200-ep.</cell><cell>400-ep.</cell><cell>800-ep.</cell></row><row><cell>K400</cell><cell>240k</cell><cell>81.5</cell><cell>83.3</cell><cell>84.4</cell></row><row><cell>IG-curated</cell><cell>240k</cell><cell>79.0</cell><cell>81.6</cell><cell>83.2</cell></row><row><cell>IG-curated</cell><cell>512k</cell><cell>81.9</cell><cell>83.5</cell><cell>83.9</cell></row><row><cell>IG-curated</cell><cell>1M</cell><cell>83.5</cell><cell>84.1</cell><cell>84.2</cell></row><row><cell>IG-uncurated</cell><cell>1M</cell><cell>83.2</cell><cell>84.5</cell><cell>84.4</cell></row></table><note>Notably, on AVA, MAE with IG-uncurated is better than MAE with curated Kinetics pre-training (e.g., by 3.1/1.7/1.1% over K400/600/700 pre-training); on SSv2, MAE with IG-uncurated is among the best, on par with the K700 counterpart.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Real-world Instagram data for MAE pre-training. We pre-train MAE on each individual set for 200, 400, and 800 epochs. We compare fine-tuning accuracy on K400. The model is ViT-L.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>we have compared with ImageNet-based supervised/MAE pre-training. When fine-tuning these variants for videos, we inflate the 2D kernel of the patch embedding layer to 3D<ref type="bibr" target="#b7">[8]</ref> and initialize the temporal position embeddings by zero.</figDesc><table><row><cell>config</cell><cell>value</cell><cell>config</cell><cell cols="3">ViT-B ViT-L ViT-H</cell></row><row><cell>optimizer</cell><cell>AdamW [43]</cell><cell>optimizer</cell><cell></cell><cell>AdamW [43]</cell><cell></cell></row><row><cell>optimizer momentum</cell><cell>β1, β2=0.9, 0.95 [9]</cell><cell>optimizer momentum</cell><cell cols="3">β1, β2=0.9, 0.999</cell></row><row><cell>weight decay</cell><cell>0.05</cell><cell>weight decay</cell><cell></cell><cell>0.05</cell><cell></cell></row><row><cell>base learning rate  †</cell><cell>1.6e-3</cell><cell>base learning rate  †</cell><cell cols="3">1.6e-2 4.8e-3 1.6e-3</cell></row><row><cell>learning rate schedule</cell><cell>cosine decay [42]</cell><cell>learning rate schedule</cell><cell cols="3">cosine decay [42]</cell></row><row><cell>warmup epochs [26]</cell><cell>120</cell><cell>warmup epochs [26]</cell><cell></cell><cell>5</cell><cell></cell></row><row><cell>epochs</cell><cell>default 800</cell><cell>epochs</cell><cell>150</cell><cell>100</cell><cell>75</cell></row><row><cell>repeated sampling [33]</cell><cell>4</cell><cell>repeated sampling [33]</cell><cell>2</cell><cell>2</cell><cell>1</cell></row><row><cell>augmentation</cell><cell>hflip, crop [0.5, 1]</cell><cell>augmentation</cell><cell cols="3">RandAug (9, 0.5) [12]</cell></row><row><cell>batch size</cell><cell>512</cell><cell>batch size</cell><cell>768</cell><cell>256</cell><cell>256</cell></row><row><cell>gradient clipping</cell><cell>0.02</cell><cell>mixup</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">(a) Kinetics pre-training</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Settings on Kinetics. † : lr = base_lr×batchsize / 256 per the linear lr scaling rule<ref type="bibr" target="#b25">[26]</ref>.</figDesc><table><row><cell>config</cell><cell>values</cell><cell>config</cell><cell>values</cell></row><row><cell>optimizer</cell><cell>SGD</cell><cell>optimizer</cell><cell>SGD</cell></row><row><cell>weight decay</cell><cell>1e-8</cell><cell>weight decay</cell><cell>1e-4</cell></row><row><cell>learning rate</cell><cell>4.8</cell><cell>learning rate</cell><cell>0.64 (L) 0.32 (H)</cell></row><row><cell>learning rate schedule</cell><cell>cosine decay [42]</cell><cell>learning rate schedule</cell><cell>cosine decay [42]</cell></row><row><cell>warmup epochs [26]</cell><cell>5</cell><cell>warmup epochs [26]</cell><cell>3</cell></row><row><cell>epochs</cell><cell>30</cell><cell>epochs</cell><cell>40</cell></row><row><cell>batch size</cell><cell>128</cell><cell>augmentation</cell><cell>RandAug (9,</cell></row><row><cell>drop path [34]</cell><cell>0.2</cell><cell></cell><cell>0.5) [12]</cell></row><row><cell>dropout [60]</cell><cell>0.5</cell><cell>batch size</cell><cell>256</cell></row><row><cell>layer-wise decay [11]</cell><cell>0.75 (L) 0.85 (H)</cell><cell>mixup</cell><cell></cell></row><row><cell cols="2">(a) AVA fine-tuning</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Settings on AVA and SSv2.</figDesc><table><row><cell>pre-train</cell><cell>extra data</cell><cell>architecture</cell><cell>input size top-1 top-5</cell><cell>FLOPs</cell><cell>param.</cell></row><row><cell>scratch</cell><cell></cell><cell>SlowFast [21]</cell><cell>64 × 224 2 79.8 93.9</cell><cell>234 × 3 × 10</cell><cell>60</cell></row><row><cell>scratch</cell><cell></cell><cell>X3D-XL [20]</cell><cell>16 × 312 2 79.1 93.9</cell><cell>48 × 3 × 10</cell><cell>11</cell></row><row><cell>scratch</cell><cell></cell><cell>MoViNet [36]</cell><cell>120 × 320 2 81.5 95.3</cell><cell>386 × 1 × 1</cell><cell>31</cell></row><row><cell>scratch</cell><cell></cell><cell>MViT-B [19]</cell><cell>64 × 224 2 81.2 95.1</cell><cell>455 × 3 × 3</cell><cell>37</cell></row><row><cell>scratch</cell><cell></cell><cell>MViTv2-B [19]</cell><cell>32 × 224 2 82.9 95.7</cell><cell>255 × 1 × 5</cell><cell>51</cell></row><row><cell>supervised</cell><cell>IN21K</cell><cell>Swin-B [41]</cell><cell>32 × 224 2 82.7 95.5</cell><cell>282 × 3 × 4</cell><cell>88</cell></row><row><cell>supervised</cell><cell>IN21K</cell><cell>Swin-L [41]</cell><cell>32 × 224 2 83.1 95.9</cell><cell cols="2">604 × 3 × 4 197</cell></row><row><cell>supervised</cell><cell>IN21K</cell><cell>Swin-L [41]</cell><cell cols="3">32 × 384 2 84.9 96.7 2107 × 5 × 10 200</cell></row><row><cell>BEVT [73]</cell><cell>IN1K+DALLE</cell><cell>Swin-B [41]</cell><cell>32 × 224 2 81.1 n/a</cell><cell>282 × 3 × 4</cell><cell>88</cell></row><row><cell>MaskFeat [77]</cell><cell></cell><cell>MViTv2-L [39]</cell><cell>16 × 224 2 84.3 96.3</cell><cell cols="2">377 × 1 × 10 218</cell></row><row><cell>MaskFeat [77]</cell><cell></cell><cell>MViTv2-L [39]</cell><cell cols="3">40 × 352 2 86.7 97.3 3790 × 3 × 4 218</cell></row><row><cell>MaskFeat [77]</cell><cell>K600</cell><cell>MViTv2-L [39]</cell><cell cols="3">40 × 352 2 87.0 97.4 3790 × 3 × 4 218</cell></row><row><cell>MAE</cell><cell></cell><cell>ViT-B</cell><cell>16 × 224 2 81.3 94.9</cell><cell>180 × 3 × 7</cell><cell>87</cell></row><row><cell>MAE</cell><cell></cell><cell>ViT-L</cell><cell>16 × 224 2 84.8 96.2</cell><cell cols="2">598 × 3 × 7 304</cell></row><row><cell>MAE</cell><cell></cell><cell>ViT-H</cell><cell cols="3">16 × 224 2 85.1 96.6 1193 × 3 × 7 632</cell></row><row><cell>MAE</cell><cell>K600</cell><cell>ViT-L</cell><cell>16 × 224 2 86.5 97.2</cell><cell cols="2">598 × 3 × 7 304</cell></row><row><cell>MAE</cell><cell>K600</cell><cell>ViT-H</cell><cell cols="3">16 × 224 2 86.8 97.2 1193 × 3 × 7 632</cell></row><row><cell cols="2">using in-house data for supervision:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>supervised</cell><cell>JFT-300M</cell><cell>ViViT-L [2]</cell><cell cols="3">32 × 320 2 83.5 94.3 3980 × 3 × 1 308</cell></row><row><cell>supervised</cell><cell>JFT-300M</cell><cell>ViViT-H [2]</cell><cell cols="3">32 × 320 2 84.9 95.8 3981 × 3 × 4 654</cell></row><row><cell cols="2">supervised + text FLD-900M</cell><cell>Florence [83]</cell><cell>n/a × 384 2 86.5 97.3</cell><cell cols="2">n/a × 3 × 4 647</cell></row><row><cell cols="2">SimMIM [80] + sup. IN21K+70M</cell><cell>SwinV2-G [40]</cell><cell>8 × 384 2 86.8 n/a</cell><cell cols="2">n/a × 5 × 4 3000</cell></row><row><cell>supervised</cell><cell cols="2">JFT-3B+SSv2+MiT+IN CoVeR [85]</cell><cell>16 × 448 2 87.2 n/a</cell><cell>n/a × 3 × 1</cell><cell>n/a</cell></row><row><cell>supervised</cell><cell>WTS-60M</cell><cell>MTV-H [82]</cell><cell cols="2">32 × 280 2 89.9 98.3 6130 × 3 × 4</cell><cell>n/a</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>System-level comparisons on Kinetics-400 action classification. We report top-1 and top-5 accuracy on the validation set. The input size is T ×H×W . FLOPs (in 10 9 ) are presented as "FLOPs per view × spatial views × temporal views", following the literature. Parameters are in 10 6 . The "extra data" column specifies the data used in addition to K400. Entries using spatial resolution &gt;224 2 are noted in gray; entries using in-house data for supervision are in light blue. Our results with K600 are with intermediate fine-tuning.</figDesc><table /><note>* This table does not include results using K700, because the K700 training set has 13.9k videos duplicated with the K400 validation set (19.9k). Results with K700 are in Table8(AVA) and Table9(SSv2).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Table 9 compares on SSv2<ref type="bibr" target="#b26">[27]</ref> action classification. On the resolution of 16×224 2 and using vanilla ViT, our results compare favorably with those of MaskFeat on 40×312 2 inputs.</figDesc><table><row><cell>pre-train</cell><cell>pre-train data</cell><cell>architecture</cell><cell>input size</cell><cell>mAP center</cell><cell>mAP full</cell><cell>FLOPs</cell><cell>param.</cell></row><row><cell>supervised</cell><cell>K400</cell><cell>SlowFast [21]</cell><cell>32 × 224 2</cell><cell>23.8</cell><cell>-</cell><cell>138</cell><cell>53</cell></row><row><cell>supervised</cell><cell>K400</cell><cell>MViTv1-B [19]</cell><cell>64 × 224 2</cell><cell>27.3</cell><cell>-</cell><cell>455</cell><cell>36</cell></row><row><cell>supervised</cell><cell>K400</cell><cell>MViTv2-B [39]</cell><cell>32 × 224 2</cell><cell>28.1</cell><cell>29.0</cell><cell>225</cell><cell>51</cell></row><row><cell cols="2">MaskFeat [77] K400</cell><cell>MViTv2-L [39]</cell><cell>40 × 312 2</cell><cell>36.3</cell><cell>37.5</cell><cell>2828</cell><cell>218</cell></row><row><cell>MAE</cell><cell>K400</cell><cell>ViT-L</cell><cell>16 × 224 2</cell><cell>34.8</cell><cell>35.7</cell><cell>598</cell><cell>304</cell></row><row><cell>MAE</cell><cell>K400</cell><cell>ViT-H</cell><cell>16 × 224 2</cell><cell>35.7</cell><cell>36.2</cell><cell>1193</cell><cell>632</cell></row><row><cell></cell><cell cols="4">(a) AVA results using Kinetics-400 pre-training</cell><cell></cell><cell></cell><cell></cell></row><row><cell>pre-train</cell><cell>pre-train data</cell><cell>architecture</cell><cell>input size</cell><cell>mAP center</cell><cell>mAP full</cell><cell>FLOPs</cell><cell>param.</cell></row><row><cell>supervised</cell><cell>K600</cell><cell>SlowFast [21]</cell><cell>64 × 224 2</cell><cell>27.5</cell><cell>-</cell><cell>296</cell><cell>59</cell></row><row><cell>supervised</cell><cell>K600</cell><cell>X3D-XL [20]</cell><cell>16 × 312 2</cell><cell>27.4</cell><cell>-</cell><cell>48</cell><cell>11</cell></row><row><cell>supervised</cell><cell>K600</cell><cell>MViT-B [19]</cell><cell>32 × 224 2</cell><cell>28.7</cell><cell>-</cell><cell>236</cell><cell>53</cell></row><row><cell>supervised</cell><cell>K600</cell><cell>MViTv2-B [39]</cell><cell>32 × 224 2</cell><cell>29.9</cell><cell>30.5</cell><cell>225</cell><cell>51</cell></row><row><cell>supervised</cell><cell>K600</cell><cell>ACAR [48]</cell><cell>64 × 224 2</cell><cell>-</cell><cell>31.4</cell><cell>n/a</cell><cell>n/a</cell></row><row><cell cols="2">MaskFeat [77] K600</cell><cell>MViTv2-L [39]</cell><cell>40 × 312 2</cell><cell>37.8</cell><cell>38.8</cell><cell>2828</cell><cell>218</cell></row><row><cell>MAE</cell><cell>K600</cell><cell>ViT-L</cell><cell>16 × 224 2</cell><cell>36.5</cell><cell>37.2</cell><cell>598</cell><cell>304</cell></row><row><cell>MAE</cell><cell>K600</cell><cell>ViT-H</cell><cell>16 × 224 2</cell><cell>38.0</cell><cell>39.1</cell><cell>1193</cell><cell>632</cell></row><row><cell></cell><cell cols="4">(b) AVA results using Kinetics-600 pre-training</cell><cell></cell><cell></cell><cell></cell></row><row><cell>pre-train</cell><cell>pre-train data</cell><cell>architecture</cell><cell>input size</cell><cell>mAP center</cell><cell>mAP full</cell><cell>FLOPs</cell><cell>param.</cell></row><row><cell>supervised</cell><cell>K700</cell><cell>MViTv2-B [39]</cell><cell>32 × 224 2</cell><cell>31.3</cell><cell>32.3</cell><cell>225</cell><cell>51</cell></row><row><cell>supervised</cell><cell>K700</cell><cell>ACAR [48]</cell><cell>64 × 224 2</cell><cell>-</cell><cell>33.3</cell><cell>n/a</cell><cell>n/a</cell></row><row><cell>supervised</cell><cell cols="2">K700 + IN21K MViTv2-L [39]</cell><cell>40 × 312 2</cell><cell>33.5</cell><cell>34.4</cell><cell>2828</cell><cell>213</cell></row><row><cell>MAE</cell><cell>K700</cell><cell>ViT-L</cell><cell>16 × 224 2</cell><cell>37.3</cell><cell>38.3</cell><cell>598</cell><cell>304</cell></row><row><cell>MAE</cell><cell>K700</cell><cell>ViT-H</cell><cell>16 × 224 2</cell><cell>38.2</cell><cell>39.0</cell><cell>1193</cell><cell>632</cell></row><row><cell></cell><cell cols="4">(c) AVA results using Kinetics-700 pre-training</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>System-level comparisons on AVA v2.2 action detection. We report mAP using centercrop or full-resolution inference, following the literature. FLOPs (in 10 9 ) are measured with centercrop inference. Parameter numbers are in 10 6 . Only in this table, following MaskFeat<ref type="bibr" target="#b76">[77]</ref>, our results are with intermediate fine-tuning and with relative positions<ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b53">54]</ref> during fine-tuning.</figDesc><table><row><cell>pre-train</cell><cell>pre-train data</cell><cell>architecture</cell><cell>input size</cell><cell cols="2">top-1 top-5</cell><cell>FLOPs</cell><cell>param.</cell></row><row><cell>supervised</cell><cell>K400</cell><cell>SlowFast [21]</cell><cell cols="2">32 × 224 2 63.1</cell><cell>87.6</cell><cell>106 × 3 × 1</cell><cell>53</cell></row><row><cell>supervised</cell><cell>K400</cell><cell>MViTv1-B [19]</cell><cell cols="2">64 × 224 2 67.7</cell><cell>90.9</cell><cell>454 × 3 × 1</cell><cell>37</cell></row><row><cell>supervised</cell><cell>K400</cell><cell>MViTv2-B [39]</cell><cell cols="2">32 × 224 2 70.5</cell><cell>92.7</cell><cell>225 × 3 × 1</cell><cell>51</cell></row><row><cell>supervised</cell><cell cols="2">K400 + IN21K Swin-B [41]</cell><cell cols="2">32 × 224 2 69.6</cell><cell>92.7</cell><cell>321 × 3 × 1</cell><cell>89</cell></row><row><cell>supervised</cell><cell cols="2">K400 + IN21K MViTv2-B [39]</cell><cell cols="2">32 × 224 2 72.1</cell><cell>93.4</cell><cell>225 × 3 × 1</cell><cell>51</cell></row><row><cell>supervised</cell><cell cols="2">K400 + IN21K MViTv2-L [39]</cell><cell cols="2">40 × 224 2 73.3</cell><cell>94.1</cell><cell>2828 × 3 × 1</cell><cell>213</cell></row><row><cell>BEVT [73]</cell><cell>K400 + IN1K</cell><cell>Swin-B [41]</cell><cell cols="2">32 × 224 2 71.4</cell><cell>n/a</cell><cell>321 × 3 × 1</cell><cell>88</cell></row><row><cell cols="2">MaskFeat [77] K400</cell><cell>MViTv2-L [39]</cell><cell cols="2">40 × 312 2 74.4</cell><cell>94.6</cell><cell>2828 × 3 × 1</cell><cell>218</cell></row><row><cell>MAE</cell><cell>K400</cell><cell>ViT-L</cell><cell cols="2">16 × 224 2 72.1</cell><cell>93.9</cell><cell>598 × 3 × 1</cell><cell>304</cell></row><row><cell>MAE</cell><cell>K400</cell><cell>ViT-H</cell><cell cols="2">16 × 224 2 74.1</cell><cell>94.5</cell><cell>1193 × 3 × 1</cell><cell>632</cell></row><row><cell></cell><cell cols="5">(a) SSv2 results using Kinetics-400 pre-training</cell><cell></cell></row><row><cell>pre-train</cell><cell>pre-train data</cell><cell>architecture</cell><cell>input size</cell><cell cols="2">top-1 top-5</cell><cell>FLOPs</cell><cell>param.</cell></row><row><cell>supervised</cell><cell>K600</cell><cell>MViTv1-B [19]</cell><cell cols="2">32 × 224 2 67.7</cell><cell>90.9</cell><cell>454 × 3 × 1</cell><cell>37</cell></row><row><cell cols="2">MaskFeat [77] K600</cell><cell>MViTv2-L [39]</cell><cell cols="2">40 × 312 2 75.0</cell><cell>95.0</cell><cell>2828 × 3 × 1</cell><cell>218</cell></row><row><cell>MAE</cell><cell>K600</cell><cell>ViT-L</cell><cell cols="2">16 × 224 2 73.0</cell><cell>94.2</cell><cell>598 × 3 × 1</cell><cell>304</cell></row><row><cell>MAE</cell><cell>K600</cell><cell>ViT-H</cell><cell cols="2">16 × 224 2 75.2</cell><cell>94.9</cell><cell>1193 × 3 × 1</cell><cell>632</cell></row><row><cell></cell><cell cols="5">(b) SSv2 results using Kinetics-600 pre-training</cell><cell></cell></row><row><cell>pre-train</cell><cell>pre-train data</cell><cell>architecture</cell><cell>input size</cell><cell cols="2">top-1 top-5</cell><cell>FLOPs</cell><cell>param.</cell></row><row><cell>MAE</cell><cell>K700</cell><cell>ViT-L</cell><cell cols="2">16 × 224 2 73.6</cell><cell>94.4</cell><cell>598 × 3 × 1</cell><cell>304</cell></row><row><cell>MAE</cell><cell>K700</cell><cell>ViT-H</cell><cell cols="2">16 × 224 2 75.5</cell><cell>95.0</cell><cell>1193 × 3 × 1</cell><cell>632</cell></row><row><cell></cell><cell cols="5">(c) SSv2 results using Kinetics-700 pre-training</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>System-level comparisons on SSv2 action classification. Notations of FLOPs(10 9 ) and parameters(10 6 ) follow Table7. We do not use intermediate fine-tuning here (see Table10).</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">In our use case, repeated sampling involves data augmentation and mask sampling.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">The ViT-B result is 68.5%<ref type="bibr" target="#b76">[77]</ref> trained from scratch using this recipe.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">The speedup is closer to 5.8× if using slower GPUs (V100 instead of A100) that can hide the loading time.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">The IN1K pre-trained model is from https://github.com/facebookresearch/mae.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Chen Wei, Karttikeya Mangalam, Chao-Yuan Wu, Ross Girshick, Piotr Dollár, and Jitendra Malik for discussions and feedback.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Ablation on Intermediate Fine-tuning</head><p>In Table <ref type="table">3</ref> we have shown results of self-supervised pre-training directly transferred to downstream datasets. Following the literature, we also investigate an another scenario: after self-supervised pretraining, we perform intermediate fine-tuning on the pre-training set using labels, before transferring. Table <ref type="table">10</ref> studies its influence. Intermediate fine-tuning has substantial improvements on AVA, while on SSV2 its effect is marginal.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to see by moving</title>
		<author>
			<persName><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">João</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ViViT: A video vision transformer</title>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Lučić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">BEiT: BERT pre-training of image Transformers</title>
		<author>
			<persName><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Is space-time attention all you need for video understanding</title>
		<author>
			<persName><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ilya Sutskever, and Dario Amodei</title>
				<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">João</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andras</forename><surname>Banki-Horvath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.01340</idno>
		<title level="m">A short note about Kinetics-600</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">João</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06987</idno>
		<title level="m">A short note on the Kinetics-700 human action dataset</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName><forename type="first">João</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">ELECTRA: Pre-training text encoders as discriminators rather than generators</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">RandAugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Ekin D Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional Transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">DynamoNet: Dynamic Action and Motion Network</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">PeCo: Perceptual codebook for BERT pre-training of Vision Transformers</title>
		<author>
			<persName><forename type="first">Xiaoyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.12710</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multiscale Vision Transformers</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Haoqi Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karttikeya</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghao</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">X3D: Expanding architectures for efficient video recognition</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SlowFast networks for video recognition</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A large-scale study on unsupervised spatiotemporal representation learning</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Self-supervised video representation learning with odd-one-out networks</title>
		<author>
			<persName><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Large-scale weakly-supervised pre-training for video action recognition</title>
		<author>
			<persName><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueting</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised learning of spatiotemporally coherent metrics</title>
		<author>
			<persName><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training ImageNet in 1 hour</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The &quot;something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ingo</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent -a new approach to self-supervised learning</title>
		<author>
			<persName><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaohan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Valko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">AVA: A video dataset of spatio-temporally localized atomic visual actions</title>
		<author>
			<persName><forename type="first">Chunhui</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susanna</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Video representation learning by dense predictive coding</title>
		<author>
			<persName><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Large Scale Holistic Video Understanding, ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.06377</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Augment your batch: Improving generalization through instance repetition</title>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itay</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niv</forename><surname>Giladi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">João</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The Kinetics human action video dataset</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">MoviNets: Mobile video networks for efficient video recognition</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Kondratyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangzhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yandong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donnie</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by sorting sequence</title>
		<author>
			<persName><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Improved multiscale vision transformers for classification and detection</title>
		<author>
			<persName><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.01526</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.09883</idno>
		<title level="m">Swin Transformer v2: Scaling up capacity and resolution</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13230</idno>
	</analytic>
	<monogr>
		<title level="j">Video Swin Transformer</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">SGDR: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Deep predictive coding networks for video prediction and unsupervised learning</title>
		<author>
			<persName><forename type="first">William</forename><surname>Lotter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Gabriel Kreiman</surname></persName>
		</author>
		<author>
			<persName><surname>Cox</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Shuffle and learn: Unsupervised learning using temporal order verification</title>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Actorcontext-actor relation network for spatio-temporal action localization</title>
		<author>
			<persName><forename type="first">Junting</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><forename type="middle">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning features by watching objects move</title>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Spatiotemporal contrastive video representation learning</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianjian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huisheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Broaden your views for self-supervised video learning</title>
		<author>
			<persName><forename type="first">Adria</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pauline</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Viorica Pȃtrȃucean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName><surname>Valko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><surname>Sun</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Time-contrastive networks: Self-supervised learning from video</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Self-attention with relative position representations</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02155</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using LSTMs</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabien</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05743</idno>
		<title level="m">Contrastive bidirectional transformer for temporal representation learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">VIMPAC: Video pre-training via masked token prediction and contrastive learning</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Hao Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.11250</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">VideoMAE: Masked autoencoders are data-efficient learners for self-supervised video pre-training</title>
		<author>
			<persName><forename type="first">Zhan</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.12602</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Anticipating visual representations from unlabelled video</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Tracking emerges by colorizing videos</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">An uncertain future: Forecasting from static images using variational autoencoders</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">BEVT: BERT pretraining of video transformers</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Learning correspondence from the cycleconsistency of time</title>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allan</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Masked feature prediction for self-supervised visual pre-training</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.09133</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Learning and using the arrow of time</title>
		<author>
			<persName><forename type="first">Donglai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Slow feature analysis: Unsupervised learning of invariances</title>
		<author>
			<persName><forename type="first">Laurenz</forename><surname>Wiskott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terrence</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Computation</title>
				<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<author>
			<persName><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.09886</idno>
		<title level="m">SimMIM: A simple framework for masked image modeling</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Self-supervised spatiotemporal learning via video clip order prediction</title>
		<author>
			<persName><forename type="first">Dejing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Multiview transformers for video recognition</title>
		<author>
			<persName><forename type="first">Xuehan</forename><surname>Shen Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.04288</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Ling</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yumao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Florence</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.11432</idno>
		<title level="m">A new foundation model for computer</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seong</forename><surname>Joon Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjoon</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Co-training Transformer with videos and images improves action recognition</title>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.07175</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName><surname>Lopez-Paz</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
