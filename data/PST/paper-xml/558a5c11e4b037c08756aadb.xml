<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">A</forename><surname>Mukherjee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Ave</addrLine>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Siewiorek</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Ave</addrLine>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4B229E65878851E76C2F2A872EFB29BD</idno>
					<note type="submission">received May 19, 1994; revised July 19, 1996.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Measuring Software Dependability by Robustness Benchmarking</head><p>Arup Mukherjee and Daniel P. Siewiorek, Fellow, IEEE Abstract-Inability to identify weaknesses or to quantify advancements in software system robustness frequently hinders the development of robust software systems. Efforts have been made to develop benchmarks of software robustness to address this problem, but they all suffer from significant shortcomings. This paper presents the various features that are desirable in a benchmark of system robustness, and evaluates some existing benchmarks according to these features. A new hierarchically structured approach to building robustness benchmarks, which overcomes many deficiencies of past efforts, is also presented. This approach has been applied to building a hierarchically structured benchmark that tests part of the Unix file and virtual memory systems. The resultant benchmark has successfully been used to identify new response class stuctures that were not detected in a similar situation by other less organized techniques.</p><p>Index Terms-System reliability, software dependability, robustness benchmarking, test suite organization, object-oriented benchmarks, software validation, extensible benchmarks.</p><p>----------3 ----------</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>IVEN the current scarcity of tools to measure the robustness of a software system, operating system developers lack the means to focus their attention on issues affecting system robustness. System developers have long used suites of performance tests to aid in the development of high performance machines and application programs; we believe that a suite of robustness tests would be similarly useful in gauging the development of robust systems, and in providing a means to compare robustness among various systems. Throughout this paper many examples are presented in the context of evaluating the robustness of an operating system. However, most of the issues examined arise in evaluating the robustness of any complex software system.</p><p>A robustness benchmark is a suite of robustness tests, or stimuli. The benchmark should address issues that are general enough to apply to a wide range of systems yet specific enough to provide a basis for differentiation according to system robustness. Essentially, a robustness benchmark aims to stimulate the system in ways that are likely to trigger internal errors, and thereby to expose both programming and design errors in the error detection or recovery mechanisms. Differentiation amongst systems should reflect the number of such errors uncovered.</p><p>In attempting to design a useful benchmark with the most general applicability, several issues must be considered. For example, if a benchmark simulates memory faults via fault injection into the supervisor code of an operating system (as in <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>), it is not likely to be easily portable between oper-ating systems, perhaps not even between operating systems that are very similar from an application's point of viewsimilar operating system interfaces are often backed by very different bodies of code. This makes it very difficult to inject faults into supervisor code in such a way that the results can meaningfully be compared across systems. This paper documents several goals that a benchmark of robustness should strive to achieve. Those goals are considered in light of their effect on benchmark implementation decisions. The evolution of robustness benchmarking can be seen as a set of changing priorities within this design space. We document these changes through a set of examples.</p><p>After Section 2 describes the motivation for robustness benchmarking, Section 3 presents the tradeoffs and implementation decisions that must be made in building a benchmark of robustness. Later sections present a series of examples reflecting the evolution of robustness benchmarking efforts, each evaluated in terms of the design goals described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND MOTIVATION</head><p>The development of computer systems has traditionally been motivated by the desire to achieve higher performance. The need to measure progress towards this goal prompted the development of performance benchmarks, which have grown in complexity and sophistication since their inception. The original performance measures of a computer system reflected attempts to compute the average instruction execution time of that system. Later, focus shifted to attempts to measure overall system performance in scenarios designed to reflect common uses of the system. The latter approach led first to synthetic benchmarks, such as Whetstone <ref type="bibr" target="#b4">[5]</ref> and Dhrystone <ref type="bibr" target="#b16">[17]</ref>, and then to applications oriented benchmarks such as the SPEC <ref type="bibr" target="#b13">[14]</ref> suite, which measures performance under prototypical workloads built from a collection of real applications. Similarly, the advent of reliable computing systems is spurring the development of robustness benchmarks to quantify improvements in system reliability. As robustness benchmarking grows, its focus is also shifting from simple measures of hardware characteristics to measures that reflect the overall reliability of a computing environment (i.e., the hardware together with its supporting software).</p><p>To date, much of the effort in building robust systems has been devoted to building robust hardware. Efforts to evaluate the robustness of software systems have become common only recently, and are exemplified by such studies as <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b14">[15]</ref>. These studies both concentrate, like all robustness benchmarks, on studying the behavior produced when a system is subjected to unusual (rather than common-case) stimuli. Both studies perform their evaluations via a collection of isolated tests, and draw conclusions from the collected results. Unfortunately, it is often difficult to evaluate the relative significance of each of the individual results collected through such test suites.</p><p>For example, <ref type="bibr" target="#b10">[11]</ref> examines the behavior of Unix utilities when they are supplied with randomly generated input data. The crash of any given utility must be taken as seriously as the crash of any other utility, even though this weighting may not reflect reality, because the benchmark lacks knowledge of the underlying system's structure, i.e., it cannot know if two utilities are related in any way. Thus, if several utilities all crash due to a single bug in an underlying shared system library, the robustness of the system being measured might be perceived to be unduly low. <ref type="bibr" target="#b0">1</ref> In order to allow more accurate evaluations of overall system robustness, robustness benchmarks must evolve towards a structure that embodies the dependency hierarchy of the system. Benchmarks should allow testing at multiple levels of abstraction, in order to facilitate the isolation of sources of failures, and to help evaluate the severity of any failures encountered. Robustness benchmarks should also be easy to adapt to any new facilities added to an existing system. For example, if a module is added to a software system and is conceptually similar to an older module, test procedures developed for the older module should be easily adaptable to the new module to ensure that such expertise is not lost. Present suites of robustness tests do not address these issues. In this paper we consider how these goals may be achieved without precluding any of the desirable properties found in existing benchmarks of robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BENCHMARK DESIGN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">General Issues</head><p>Several issues must be addressed by the designer of any suite of robustness benchmarks. This section examines benchmark properties that were considered in the evolution of the designs presented in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Portability.</head><p>First and foremost, it must be possible to use the robustness benchmarks to compare different operating environments and computer systems, and thus the benchmarks should be portable across platforms. This goal often 1. Admittedly, a bug in a critical system library should be considered more severe than any single isolated bug in a utility, but it is important to note that a single bug fix or workaround is also likely to be able to remedy all instances of such a bug. restricts the range of tests that can be performed-notably eliminating those that require knowledge specific to one environment, such as is often the case with fault-injection based tests.</p><p>Coverage. Ideally, a benchmark should test all possible uses of every system module being tested. Often, however, the space of such stimuli is too large to permit exhaustive testing. A completely deterministic benchmark may choose to test only the most frequent uses of a module, and hence to function only as a verification suite. However, experience shows that the common case uses are most often those that are properly debugged. A deterministic benchmark might also focus solely on unusual uses of a module, thereby providing a better assessment of the robustness of that module. However, problems usually occur at the intersections of rarely occurring events which, taken together, produce an unexpected state. The set of possible event intersections is often too large to explore systematically, and any benchmark based on systematic exploration is likely to become merely an aid to debugging the uncommon cases. When systematic exploration is intractable, the use of randomized stimulation is often required to obtain a representative sample of the test space.</p><p>Extensibility. An extensible benchmark provides a means to extend its set of stimuli in a consistent manner, i.e., stimuli can be added to the benchmark in such a way that it produces results that are directly comparable with results generated prior to the addition. Extensibility is necessary not only to allow for the addition of stimuli of a different nature to be used in testing an existing system module, but also to allow for an existing benchmark to be extended to apply to new system modules. Extensibility ensures that a benchmark is a consistent measure of progress, rather than simply a verification suite for an isolated module.</p><p>A simple form of extensibility can be achieved through the use of parameterized stimuli. For example, a benchmark suite might consist of a group of stimuli whose behavior is completely determined by the input of a string of random numbers. New sets of tests can be generated (thereby increasing coverage) by varying the input string, whereas every test set generated maintains repeatability. The extensibility of such a benchmark is rather restricted, however, as only limited variation can be achieved in tests by varying only input parameters. Greater extensibility requires the ability to add completely new stimuli while maintaining the consistency of the result processing.</p><p>In general, the extensibilty of a benchmark is determined by the degree to which the test control structure of the benchmark can be extended without affecting the result processing. Hierarchically structured benchmarks provide a general means of achieving extensibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hierarchy of Detail.</head><p>The tests within a set of benchmarks should be organized in order of increasing detail, where detail is inversely proportional to the number of modules that may be exercised by a test. The simplest tests are often applicable across multiple system modules (e.g., there are tests of proper resource allocation and deallocation that are applicable to any system module that manages a resource), whereas the most detailed tests are usually highly specific to a particular module or combination of modules. This organization may be reflected in the tests themselves, such that the most detailed tests can assume that all simpler tests have been passed. In designing a benchmark suite that tests several system modules, it may be desirable to develop a hierarchical interface to those system modules, organized to reflect the hierarchy of their functions-simple tests can then be written in an objectoriented fashion with minimal code duplication (which may lead to divergence) on a per-module basis. An example of this approach to benchmarking is documented in Section 4.4.</p><p>Note that an arrangement of tests in a hierarchy of detail may lead to a higher initial implementation cost. Some effort is required to define the structure of such a benchmark, unlike the construction of a benchmark composed of a collection of ad hoc tests. However, this initial investment is usually worthwhile due to the desirable properties of a hierarchical structure such as the resultant ease of code reuse when the benchmark is extended.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reporting of Results</head><p>. Several characteristics are desirable in the results reported by each test of robustness. As mentioned before, each test result should be repeatable. The results should also be amenable to comparisons between different machines-indeed, rather than simply indicating whether each test of robustness was passed or not, it is desirable to report on failed tests using a scale that reflects the severity of the failure. A closely related issue is the degree of localization of triggering events <ref type="bibr" target="#b15">[16]</ref> that is reflected in the reported results-i.e., the extent to which the results pinpoint the error(s) that were detected, and their possible causes. Good localization is especially valuable to system designers trying to focus on improving the dependability of the operating system.</p><p>Note that detection of a "failure" (i.e., failed test) in itself raises a number of difficulties. As many stimuli exercise the system in ways that may not have been anticipated by its creators, a "gold standard" for correctness is often absent. The developers of a robustness benchmark might choose to overcome this problem by defining a standard of correctness, or even a measure of incorrectness. For example, a scale ranking errors in terms of their severity, ranging from unanticipated error code returns to complete system crashes, could serve as a yardstick of incorrectness. Alternatively, the benchmark itself could be augmented with the ability to learn and record a "correct" result. For example, the developers of a benchmark may define the correct result to be the result most commonly produced in a particular abnormal scenario 2 -the benchmark itself can then be used to determine the most common results. A third possible approach involves defining only the possible "incorrect" results, and assuming that anything else is correct. (For example, the effects of a system call invoked with garbage parameters could be defined as being correct as long as the operating system does not crash, the file system is left intact, and other executing processes are unaffected.)</p><p>2. Such an approach measures behavioral consistency of modules across the domain of the test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Goal</head><p>A set of systems whose robustness can be compared via a benchmark must necessarily be similar at some levelsimilar enough to provide at least comparable abstractions to evaluate. The family of Unix-based multiuser, multitasking operating systems forms such a set, and was the testbed for much of the work presented herein.</p><p>Unix-like operating systems are primarily used to support applications that require support for multiple processes and multiple users. Even in "single user" systems, processes owned by at least two different users are usually present on the system (processes owned by the system administrator, and those owned by one or more users). The function of the operating system is to manage access to hardware resources, and to ensure that the running processes do not affect each other adversely. This suggests that the robustness of an operating system should reflect the system's ability to successfully contain fault conditions generated by any one process (i.e., reflect the ability of the operating system to prevent those faults from affecting other processes). Thus, a system crash is considered to be the extreme case of failure to localize a fault, as it means that all other processes are affected. A Unix benchmark suite should thus attempt to measure the ability of each module to contain errors, on a module by module basis for each of the several testable modules and interfaces existing on most systems.</p><p>An implementation of a robustness benchmark for any multiuser, multitasking operating system must address system-specific issues in addition to those presented in Section 3.1. The next several sections present these issues in the context of Unix-like systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Benchmark Structure</head><p>Unix systems provide at least the following modules together with their interfaces: file system, virtual memory, process management, and signal handling. Most also provide network support and window management. A simple benchmark suite might consist of a series of independent test programs, each of which exercises one module. However, such a test does not accurately reflect the fact that under normal use, each system module must support simultaneous interaction with several programs. Thus multiprogramming, or support for running and monitoring several simple programs simultaneously, is required for a representative test of fault handling scenarios that may arise in regular use. Note that a multiprogrammed benchmark is also able to test the system's ability to handle the propagation of multiple faults simultaneously occurring in distinct modules. Thus, a robustness benchmark for Unix-like systems should ideally have multiple processes-a feature that also proves convenient in measuring the extent to which a fault propagates (described later).</p><p>There is another disadvantage in designing a benchmark suite made up of one test program per system module. A system may have several modules, and the incremental cost of adding a new, widely applicable test to the test suite is high-the new test must be implemented once for every module involved. However, if the new test simply manipulates modules in a manner that can be abstracted be-yond a standardized module interface, the test need only be coded once; module-specific code below the interface handles all interaction with the system modules and is unchanged. This is the motivation for hierarchical structuring of benchmarks. Unix systems support enough modules to justify implementing a hierarchically structured system interface to allow tests to be be coded in an object-oriented manner, without code duplication. (For an example of such an implementation approach, see Section 4.4.) Note also that a hierarchical approach enforces consistency of result reporting simply by eliminating multiple copies of functionally similar testing and reporting code-each test is implemented exactly once at an abstract level, thereby guaranteeing compatibility of test reports across modules.</p><p>Finally, it should be noted that portability considerations often require that the code of a benchmark suite be limited to a user-level implementation. Unix-like kernels often differ substantially in their implementation. Had this not been the case there would presumably be little or no difference in robustness among the various flavors of Unix. Thus, any benchmark that requires kernel-level support is not likely to be easily portable across a wide range of Unix platforms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Fault Classification</head><p>The following criteria might be used to evaluate the seriousness of a fault condition (in increasing order of severity):</p><p>1) Does the fault affect the process causing it? 2) Does the fault affect other executing processes? 3) Does the fault crash the operating system? 4) Does the fault crash the operating system microkernel? 4 An uncontained fault may affect another executing process in one of several ways. It may cause the other process to crash (without having crashed the entire operating system), to produce incorrect output, or simply to execute more slowly than it otherwise would. Note that the process causing the fault may be affected in a similar manner. If the causing process is the benchmark itself, the benchmark may not be able, in some cases, to detect the fault without the aid of an external monitoring agent, e.g., a watchdog started before beginning the tests.</p><p>The possible effects of a fault may be classified according to any of several taxonomies, such as those of <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b14">[15]</ref>, which are summarized in Table <ref type="table" target="#tab_0">1</ref>. All of these taxonomies necessitate a means of measuring the effect on processes other than those owned by the benchmark itself. This may be done by observing a "sacrificial program" which is executed concurrently with the benchmark suite, and checking to see how it is affected by the faults generated by the benchmark suite. Note that almost all of the possible resultant states of the sacrificial program (as enumerated by Table <ref type="table" target="#tab_0">1</ref>) can be detected mechanically by a "watchdog" program that has been previously calibrated to the expected behavior of the sacrificial program. If the fault should produce a complete system crash, however, the watchdog may be unable to observe this fact, unless it is executing on a separate processor that is isolated from the one used to exe-cute the tests. If a separate processor is not available human intervention may be required in the event of a system crash.</p><p>A sacrificial program should, of course, make widespread use of the system to increase the probability that it will reflect any effects of uncontained faults. A robustness benchmark suite might elect to provide a synthetic program to serve as a sacrificial program, or it may choose to make use of any of several performance benchmark suites, such as the SPECmarks, which have the advantage of widespread availability. If a system is being evaluated for its ability to run one particular application without failure, that application itself might well serve as the sacrificial program.</p><p>It should be pointed out that the use of a sacrificial program has a marked effect on some of the properties of the benchmark. In particular, any robustness benchmark employing a sacrificial workload is a multiprogrammed benchmark and as such suffers all the disadvantages of being multiprogrammed (e.g., the resultant benchmark is likely to lose both determinism and repeatability). However, as mentioned earlier, multiprogramming is also more representative of the application computing model supported by the operating system. The use of multiprogramming is exemplified by the first two benchmarks presented in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Recording of Results</head><p>On most Unix-like systems, output paths to permanent storage are buffered. Unfortunately, this buffering may lead to loss of test data if the system crashes, or if the buffer is corrupted, as a side effect of executing the test. Consequently, a benchmark (or its user) may have to compensate. Simple solutions include the use of an unbuffered output channel (e.g., a CRT or a printer) or minimization of the data loss through the use of fine-granularity logging to stable storage. Such workarounds usually involve a substantial performance penalty (which, in some cases, might even change the outcome of the robustness test), and thus are only occasionally warranted in the case of tests that reliably produce total system crashes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Coverage and Repeatability</head><p>In order to provide good coverage of the test space, a robustness benchmark may opt to use tests whose behavior is dependent upon the output of a random number generator. However, such a test may not execute exactly the same actions on different systems if the output of the random number generator changes. Most Unix systems provide a random number generator interface (random()) that is identical across implementations, but which is not guaranteed to produce exactly the same stream of random numbers from machine to machine. 5 This brings into question the value of comparing randomized runs made on two different machines. "Controlled randomness" can be employed to avoid the problem, by using streams of pregenerated random numbers that are fed to the benchmark at run time, ensuring that runs on different machines are parameterized in an identical manner.</p><p>5. In practice, most implementations do produce identical streams, but a system vendor might choose to change this. Nevertheless, repeatability is often difficult to achieve in any realistic robustness benchmark of a Unix system because Unix systems do not usually provide deterministic process schedulers. Thus, any test that presents the system with a multiprogrammed workload (in order to evaluate the system's ability to handle multiple simultaneous requests) will introduce some randomness into the result of the robustness test. Consequently, the results of any such test may not always be repeatable, as the scheduling order is likely to vary from run to run. This problem cannot usually be resolved in a portable manner if a robustness suite wants to test a multitasking environment. Unfortunately, Unix schedulers do not normally provide hooks to allow repeatable or deterministic process scheduling. Given the difficulty of removing this variation by design, a statistical analysis of behavior over several trials is often the most viable solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXAMPLES OF ROBUSTNESS BENCHMARKS</head><p>This section documents the evolution of robustness benchmarks through a series of detailed examples. Each example is evaluated with respect to the design issues presented in Section 3.1. The evaluations in this section are summarized in Table <ref type="table" target="#tab_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">"Crashme"</head><p>Crashme is a simple, publicly available test of robustness for Unix systems. This program allocates an array, and fills this array with random data. Subsequently, it spawns off several child processes that all try to execute this array of data as if it were code. The parent crashme process observes its children and spawns replacements for the children as they take exceptions and die. When crashme is run, the Unix system is subjected to a large number of varied exception conditions in a short period of time-as a result both the error detection and handling capabilities of the operating system are severely tested. Crashme succeeds in crashing a large number of Unix systems. Albeit from a very small sample of machines, we observed that the amount of time an operating system stayed up under crashme appears to be correlated to some degree with our observed reliability of that operating system in day to day use. (Refer to Table <ref type="table">3</ref>.)</p><formula xml:id="formula_0">TABLE 3 TIME TAKEN BY crashme TO CRASH SOME MACHINES</formula><p>Crashme is a very good test of a system's ability to handle a high error rate, but in many ways it is not a good general purpose benchmark of robustness. Although it is portable and has good coverage of the stimulus space (and correspondingly a high ability to discover unanticipated failure modes), the results from crashme are very limited-either the system crashes, or it does not crash. If the system crashes, it is very difficult to determine the cause of the crash (and any such determination could only be a result of error logging external to crashme itself, such as error logging provided by the operating system). Crashme also lacks repeatability-a high degree of randomness is introduced due to the scheduling of a large number of child processes created by the program. If the test is run twice, and the system crashes both times, it cannot be determined whether or not both crashes shared a common cause. Due to the difficulty of interpreting its results, crashme is only of limited usefulness as a measure of progress in building a robust system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">CMU Crashme</head><p>Having observed the aforementioned problems with crashme, we attempted to remedy them by restricting the coverage of the test, hoping to gain repeatability and better localization of triggering events. The spawned child processes were constrained to exercising only a single welldefined system interface-namely, the Unix system calls. It was anticipated that the error checking on parameters passed to system calls would be sufficient to guarantee that system calls made with randomly generated parameters would not be able to crash the operating system. However, many of the systems tested were vulnerable to this limited test. (Refer to Table 4.)  Although this modified version of crashme still exhibits a high degree of nondeterminism, it offers better localization of triggering events than the original version. This localization can be further improved by restricting the tests to only a subset of the Unix system calls. Such restriction, together with the monitoring of system calls via the sentry mechanism described in <ref type="bibr" target="#b12">[13]</ref>, has been used successfully to identify some errors in the Mach 3.0 Unix server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Modular Benchmarks</head><p>Another approach is that of modular benchmarking. Modular benchmarks are separate tests of individual system modules. These benchmarks are constructed by regarding the system as a collection of isolated modules, and writing one or more tests to exercise each module independently. One example of a modular benchmark is documented in <ref type="bibr" target="#b14">[15]</ref>. Another example is a set of robustness benchmarks that was recently constructed at CMU to test the robustness of the Advanced Spaceborne Computer Module (ASCM) <ref type="bibr" target="#b5">[6]</ref>. Although this is an embedded system running a real-time operating system not related to Unix, it can be regarded as a collection of system modules in much the same way as any other operating system. The ASCM test suite consisted of distinct tests to exercise the various system modules (file system, memory system, external communication, locking support and multi-program operations) together with a watchdog program (similar to the parent crashme process) to monitor and collect the results of the tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">An Example of a Modular Benchmark</head><p>The file module benchmark from the ASCM test suite serves as a good example of a modular test. This benchmark stresses the seven calls of the file module (create_file, open_file, close_file, delete_file, read_file, write_file and move_file_pointer) systematically, constructing tests for each call from the interface definition of that call. For example, the read_file call takes three parameters: a file handle, the starting address of a buffer into which data is to be read, and the number of words to be read from the file. The benchmark chooses a value for each of the parameters from a set of values that are based on the parameter's type. For example, a file handle might point at a valid file that is closed, at a valid file that was opened in read-only mode, or at a deleted file, among other possibilities. By choosing all possible test input combinations for all of the parameters of the read_file call, the benchmark generates 252 test cases, as shown in Table <ref type="table">5</ref>.</p><p>The results of the 252 tests are then divided into six groups in increasing order of severity: those that produced the expected result (correct); those that returned with an error code, but not one of those that would be expected given the input parameters (unexpected error); those that returned indicating success in spite of having been given invalid input parameters (bad success); those that caused the operating system to terminate the benchmark (terminated); those that caused a warm restart of the system (warm restart); and those that caused a cold restart of the system (cold restart). The results of these 252 tests on the read_file and write_file calls, which take identical sets of inputs (and thus generate the same test parameter combinations), are shown in Table <ref type="table">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 5 THE VARIOUS POSSIBILITIES FOR INPUT PARAMETERS IN THE read_file AND write_file TESTS</head><p>The advantages of the modular benchmarking approach include relatively low complexity of the individual tests (because intermodule interactions are usually not considered), and the ability to guarantee determinism. Unfortunately, modular benchmarks also have several disadvantages.</p><p>Although the modular benchmark approach applies well to hardware, where system components are manufactured separately, and are often designed for independent testability, the approach does not scale well to large bodies of operating systems software, whose modules are often closely intertwined, making independent testing difficult. Here, this testing paradigm is not matched well to the system being evaluated. A different problem may occur when a modularized system, seemingly well suited to modular testing, is evaluated. In this case, the modular decomposition of the benchmark suite restricts the coverage of individual tests, and eliminates any possibility of stimulating interactions between system modules. In addition, modular benchmarks are also unable to take advantage of the similarities between system modules. While it is quite likely that some similarities will exist between the many modules in a system, modular benchmarking requires that any similar tests applicable to multiple modules be coded once for each applicable module in the appropriate test. Thus, similarities between modules are hidden within the individual tests-a significant loss, because such similarities are the key to extensibility, as explained in the next section.</p><p>Modular benchmarks offer no guarantee that the results of similar tests, or even of two different revisions of the same test, are comparable with each other. In effect, while all comparisons are external to the benchmark (i.e., they are done by the human, or by an automated postprocessor, who collects the results of the individual tests), all information determining their comparability (the abstract nature of the tests) is completely hidden from the evaluator. This problem limits the usefulness, as a measure of progress in the development of a system, of any modular benchmark that goes beyond simple interface verification. For example, improvements to a system module may render a modular benchmark incompatible with its test simply because implementation details changed. The original test may still apply at an abstract level, but modular benchmarks enforce no separation between the abstract test they apply and the interfacing of this test to the module being tested. Consequently, a modular test often needs to be adapted in response to any significant change in a module. When such an adaptation is made, great care must be taken if the results of the adapted test are to remain directly comparable to results from the original version. An alternative benchmark structure, wherein test suites are carefully organized to enforce separation of "testing" code from "system interface" code (rather than to enforce separation along module boundaries) alleviates may of the problems of modular benchmarking schemes. This approach is described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">A Hierarchical Approach</head><p>Given the shortcomings of modular benchmarks, it would seem that decomposition of a system into multiple unrelated modules is not the best approach to organizing a suite of robustness tests. Consider the following set of identical tests, taken from the ASCM benchmarking suite. These tests appear in the benchmarks of both the file system and of the memory buffer system. &lt;object&gt; is used to denote either a file or a memory buffer in the list:</p><p>• Reference &lt;object&gt; before it has been created.</p><p>• Reference &lt;object&gt; after it has been deleted.</p><p>• Delete an active &lt;object&gt;.</p><p>• Write past the end of &lt;object&gt;.</p><p>• Read before the beginning of &lt;object&gt;.</p><p>• Allocate &lt;object&gt;s until resources are exhausted.</p><p>These tests represent just a few examples of stimuli that are applicable to multiple modules, but are hidden within individual modular benchmarks. As a result, similar tests are being performed, and their results are potentially comparable to one another, but this may not be apparent in the results. One way to remedy this problem is to abstract the tests and their associated result processing, separating them from the implementation details of the various modules by a clearly defined interface layer. However, this one-step decomposition is not sufficient, because it does not delineate the range of applicability of any given test. Some tests may be applicable to all modules, while others might only apply to a subset, etc. We believe that the correct way to decompose a software system in order to test it is through the use of a class hierarchy. Once all of a system's features are organized into a hierarchy of classes, a test can be specified to apply to one or more particular classes. One possible class hierarchy that might be used to organize the testing of a Unix system is shown in Fig. <ref type="figure" target="#fig_1">1</ref>. Note that the modular ASCM benchmarks described earlier actually represent an example of a simple hierarchy with only one level of abstraction. The ASCM benchmark generates tests for a module by looking at the interface to that module-sets of test input parameters, selected so as to be of the correct data types, are chosen for each call implemented by the module. For any input parameter of a given type, there is a predetermined list of inputs that may be used to instantiate that parameter. Thus, one can think of the interface to a call in a module as being a class inheriting from a set of base classes, where each base class corresponds to one input parameter type. The class corresponding to a particular call inherits from all of the input parameter types that describe its arguments, and the test applicable to such a class is a composition of tests applicable to each of its base classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">A Proposed Hierarchy</head><p>As the management of various resources is a primary function of any operating system, Fig. <ref type="figure" target="#fig_1">1</ref> is one possible starting point in the construction of a benchmark of robustness. Note that the hierarchy contains many abstract classes (e.g., Storage Object) which do not correspond to any single system module, but rather serve as the fundamental means TABLE <ref type="table">6</ref> THE RESULTS OF RUNNING THE read_file AND write_file TESTS ON AN ASCM SYSTEM [6]   Each test case is classified as having produced the correct result (correct), having returned an unexpected error code (unexpected error), having indicated success in spite of having been given invalid input parameters (bad success), having caused the operating system to terminate the benchmark (terminated), having caused warm restart of the system (warm restart), or having necessitated a cold restart of the system (cold restart).</p><p>of grouping modules by similarity. This explicit grouping is the basis for the organization of a hierarchically structured benchmark suite. It also provides the mechanism for orderly extension of the benchmark suite.</p><p>In order to construct a hierarchical benchmark, a hierarchically structured interface to the operating system must first be implemented. Unix does not provide such an interface, so a hierarchical interface library must be developed to support the subsequent construction of robustness tests. The construction of such an interface proceeds as follows. Having decided upon the hierarchy to be implemented, the designer must choose an appropriate set of methods to be defined in the interface to each class. For example, the methods allo- cate and deallocate might be defined for the top-level class Resource. Shared_Resource, which inherits from Resource, might add the methods lock and unlock to those already defined by Resource. Great care must be taken to define these methods with sufficient generality to apply to all modules that will be their descendants. An instance of a particular class (e.g., File) is required to implement all methods from all abstract classes that are its ancestors. Whenever a method can be implemented in a completely module-independent way, it should be defined in the most general abstract class to which it applies. Subclasses and instances of this class may chose to redefine the default implementation with a more specific one.</p><p>Note that operating systems of the future are likely to provide object-oriented hierarchical interfaces to their facilities and will thereby eliminate the need to construct the interface library. Recent research prototypes that exemplify this trend include the Spring <ref type="bibr" target="#b7">[8]</ref>, Choices <ref type="bibr" target="#b2">[3]</ref>, and Chorus <ref type="bibr" target="#b11">[12]</ref>, operating systems. Note also that although hierarchical interfaces and benchmarks are most easily written in a language that supports object-oriented programming (such as C++), they can also be written in more traditional languages (such as C) with some extra effort on the part of the programmer. The experimental hierarchy has been written in C++.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Using a Hierarchy to Build a Benchmark</head><p>Once a hierarchical interface to the operating system has been built, any operating system test not requiring module-specific knowledge can be written in an abstract manner. The test should be implemented at the most abstract possible level in the class hierarchy to which it applies. For example, a test of resource exhaustion by repeated allocation (mentioned in Section 4.4) requires only an allocate method from the module that it is testing. As such, it should be coded to take a parameter of class Resource, and to use the allocate method provided by all Resources. Once the test has been coded for a particular class, it can automatically be applied to any of the descendants of that class. In the case of the resource overallocation test, the test can be applied to all of the system modules at the base of the hierarchy (including files and memory buffers). In contrast, consider a test that checks for correct system behavior upon writing past the end of an object. This test requires the notion that the object be able to store information, and that it have a beginning and an end. In this case, the test applies to the class Storage_Object and its subclasses.</p><p>Once a test has been encoded at the appropriate level of abstraction, any result processing associated with the test should be encoded at the same level. Thus the result processing is allowed to have knowledge of the specificity of the test (e.g., it might want to use such knowledge to scale the results of the test according to some weight assigned to the severity of a failure in that class). Most importantly, however, both the test and the result processing for the test are coded exactly once, in an abstract way. This guarantees comparability of the results obtained from applying the test to multiple modules. Because exactly one copy of the test is used for its multiple applications, there is no way for inconsistencies to arise, as might occur when multiple copies are present (e.g., with modular benchmarks). Hierarchical benchmarks are also easily extensible in a consistent manner due to their organization. If the benchmark suite is to be extended to include a new module, the interface to that module is encoded as a subclass of the appropriate class within the hierarchical interface to the operating system. If the new module is related in any way to existing modules, its placement reflects these relationships. Any tests that have been developed for the existing modules can be applied to the new module immediately. Again there is no possibility of duplicating test code if the new module is positioned correctly in the hierarchy, and consistency in testing and result processing is guaranteed to have been maintained across the extension.</p><p>Finally, note that while hierarchical structuring offers superior extensibility, reduction of coding costs (through code reuse) and the potential for better organization of result reporting, it is not antagonistic to achieving any of the other goals desirable in a benchmark (portability, coverage, localization of triggering events, etc). Thus hierarchical benchmarks offer some special benefits without sacrificing the desirable qualities offered by other benchmark styles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Hierarchical Testing Using C++</head><p>Hierarchically structured benchmarks are most easily implemented in a language that provides support for inheritance. This section presents some trivial tests that demonstrate the use of C++ in building a hierarchy, and in constructing tests using that hierarchy. The tests presented are intended solely to illustrate the programming paradigm. Readers who are familiar with C++ may wish to skip to the next section, which presents a more complicated example of a hierarchical test which was actually implemented.</p><p>Given the hierarchy from Fig. <ref type="figure" target="#fig_1">1</ref>, the significant parts of a declaration of class Resource might look as follows:</p><formula xml:id="formula_1">class Resource public: virtual int allocate(int n) =0; virtual int deallocate(int id, int n) =0; };</formula><p>Resource is an abstract class (i.e., it does not correspond to any single system module), so the implementations of methods allocate and deallocate are not provided, and are hence marked "=0." The Storage_Object abstract class inherits from Resource, and supplies some additional methods:</p><p>class Storage_Object : public Resource { public: virtual void set_mode (object_mode mode); virtual object_mode get_mode(); virtual int read_data (char *buf, int len) =0; virtual int write_data (char *buf, int len) =0;</p><p>protected: enum object_mode mode_spec; // mode of object };</p><p>The Storage_Object class provides two more methods (read_data and write_data) that will be implemented by classes inheriting from it. Default implementations are also provided of two methods that manage the mode of the storage object. Classes inheriting from Storage_Object may override the definitions of set_mode and get_mode if they so desire, as these methods are declared as being virtual. Finally, the file_object class is declared as follows: The file_object class must implement all of the unimplemented methods that it has inherited from its ancestors. It also chooses to override the default implementation of set_mode, replacing it with an enhanced version that manipulates the mode bits of the underlying file on disk, and then calls the original default implementation.</p><p>Having declared and implemented all the methods of the hierarchical interface, a simple test, applicable to any Resource might be implemented as follows: The resource test can be applied to both the file_object and the process_object, but the storage object test is only applicable to the file_object, because the proc- ess_object is not a Storage_object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.4">An Example Hierarchical Test</head><p>As a simple illustration of a hierarchical benchmark, the hierarchical analog to part of the ASCM benchmark was constructed for a Unix system. Observe that the read_file and write_file tests described in Section 4.3 require only those capabilities that are defined by the class Stor- age_Object. The hierarchical Unix benchmark implements these as tests of the Storage_Object class, making use of the read_data and write_data methods respectively. The file_object and memory_object classes, which are defined in our hierarchical interface library as subclasses of Storage_Object, were evaluated using this benchmark.</p><p>Implementation. Both the read_data and the write_data benchmarks are composed of two parts. The first part generates the objects (i.e., file_objects or mem- ory_objects) which are used in the testing. This part embodies module-specific knowledge, as it must know at least the data type of the object to be generated, and may need to know the internal details of the object being generated. For example, it may need to know that an object encapsulating a closed file is required. The second part of the benchmark embodies the abstracted test routine itself. This part of the test is usually written at the highest possible level of abstraction, and does not embody any module-specific knowledge. For example, the read_data and write_data tests are written to apply to any Storage_Object, and do not require further knowledge of the object they are testing. Thus, the code for testing a file_object looks like: The routine test_storage_object can be applied to any storage object, such as a file or a piece of virtual memory. In this particular case, it conducts tests of reading and writing using all possible combinations of test buffer addresses and I/O request sizes. On the other hand, get_test_obj() is a module-dependent routine, implemented by each of the modules tested, that makes use of local module-specific knowledge to generate the objects to be tested. In the example shown, it returns a file_object, which is passed to test_storage_object and treated like any other Storage_Object.</p><p>Result Processing. The hierarchical benchmark implemented maintains the six result classifications defined by the ASCM modular benchmark with a seemingly slight, but important, change. The ASCM error class terminated has been replaced by an unexpected_termination class, and terminations that are the expected outcome of a test are instead counted as correct. The reason for this change is discussed later.</p><p>The structure of the result processing code resembles that of the testing code in that test results are also processed by a module-dependent and a module-independent routine. A module-specific analysis routine is supplied by each module being tested, and evaluates the outcome of the test in view of the characteristics of the particular object that was tested. In a system without a system-wide integrated error handler, the module-specific routine is also able to examine modulespecific error return mechanisms and incorporate their feedback into its analysis. Unlike the module-specific routine, the module-independent routine, also known as the test-specific routine, has no detailed knowledge of the object being tested, but has detailed knowledge only about the test that was applied. It is aware of the tested object only at the same level of abstraction as the test itself was. The test-specific routine is applicable across all modules that are tested. It evaluates outcomes based on system-wide error handling information, and on the observed behavior of the tested object. Note that some information may be available to both result-processing routines, and should be used by both of them in analyzing the outcome of the test.</p><p>Given the goal of classifying test results into one of the six classes described above, the two analysis routines each return correct, unexpected_error, unexpected_termination, or bad_success. These evaluations correspond to the first four possible result classifications respectively. The modulespecific and test-specific evaluations are then combined to yield the final classification of the outcome, which falls into one of these four classes. In situations where either of the remaining two possible outcomes (warm_restart or cold_restart) occur, the occurrence is detected by an external monitoring agent (such as a human). In these two cases, the outcome is indisputable, and no further modulespecific or test-specific analysis need be performed.</p><p>Consider, for example, a write_data test which attempts to write 1,024 bytes of data to a Storage_Object that encapsulates a closed file, passing in a buffer that points to only 256 valid bytes of data. After the test has run, it is the job of the module-specific routine, which is aware of the internal details of the Storage_Object, to check whether the result indicates an invalid attempt to write to a closed file, as the module-specific routine is aware that the file object being written was actually closed. If the error code returned agrees with this predicted outcome, the module-specific routine indicates a correct return. Otherwise, it indicates bad_success or unexpected_error, as appropriate. The module-independent routine, on the other hand, is aware only of dealing with a Stor- age_Object, but knows that the test attempted to write more data than was actually supplied to the write_data call. It therefore checks to see how much data the test claimed to have written successfully, and once again classifies the outcome as being in one of the four aforementioned categories. The two evaluations are then combined according to the following principles, which are employed to produce the matrix of Table <ref type="table" target="#tab_5">7:</ref> • A call may succeed (complete without returning an error code), fail (complete, returning an error via one or more error return channels 6 ), or produce a termination. • 5 A successful call is either correct or is a bad_success. Both result analyses must agree that success was the expected outcome of a test in order for the result to be deemed correct. This is because an error stimulus can be introduced into the test by either the module-specific object generation, or by the abstract test code initialization, without the knowledge of the other. Thus, if either one of the analyses expects an error as the correct outcome, the correct outcome must be an error, e.g., if write_data is called on a file that is open for writing, but is passed a NULL buffer and asked to write 1,024 bytes, only the test-specific routine will predict an error as being the correct outcome. (This is because the decision to use a NULL buffer would have been made by the testspecific initialization code.) If executing this test does not trigger an error as predicted, the outcome is correctly classified as a bad_success.</p><p>• I Conversely, if a call fails, it has produced either the correct outcome, or an unexpected_error; only one of the analyses need accept this error as the correct outcome in order for the result to be deemed correct. Once again, this is because it is possible (and likely) that only one of the two parts of the testing code is aware of a stimulus that is expected to cause an error condition. In the example above, suppose the call failed, and that the return code indicated that invalid data had been passed to the write_data call. Only the module-independent routine would have sufficient information to classify this as being the correct outcome. To the module-dependent independent routine, knowing only that a valid storage object was being written to, the error return would look like an unexpected_error return.</p><p>• ᭪ If a call produces a termination, that termination is either correct or it is an unexpected_ termina- tion. Only one result analysis needs to accept this termination as correct (i.e., expected) in order for the outcome to be considered correct. The justification for this case is analogous to that of handling the failure of a call.</p><p>• # The module-specific and test-specific result analysis routines cannot disagree on whether or not the test resulted in program termination. Thus, it is not possible for one routine to classify an outcome as an unex- pected_termination while the other reports an un- expected_error or a bad_success.</p><p>• 4 The two analysis routines may disagree on whether a call failed or not. This may seem counterintuitive, and is not a common occurrence. However, it may occur in the presence of error return channels that are not available to both analyses for examination. For example, a module-specific error-return channel might indicate to only the module-specific routine that a test had failed. Alternatively, the test-specific 6. An error return channel is any means by which the operating system can indicate an error condition to an application. Examples include return codes, global status flags, signals and system traps. routine might conclude that a test had failed by observing behavioral data not available to the modulespecific routine. In cases of such disagreement, one analysis may return a bad_success while the other returns unexpected_error. The correct combination of these two results yields unexpected_error because the analysis that detected an error had error indication channels available to it that were not available to the routine that did not indicate an error.</p><p>Note that the analysis routines in this implementation observe and analyze test termination by the operating system. This is done in order to classify each termination as having been correct or having been an unex- pected_termination. Had the original ASCM classification been retained, classifying terminations in a group unto themselves, benchmark termination would not have required further analysis; terminations could simply have been monitored and handled in the same way as cold re- starts or warm restarts. However, the implementation of the hierarchical benchmark exposed a problem with the ASCM result classification that renders this simpler scheme unusable.</p><p>The hierarchical benchmark was initially implemented and run on a file_object module interfacing to part of a Mach 3.0 file system. With minimal effort (restricted to extending the hierarchical OS interface library), the same test was later run on a memory_object module, interfacing to the virtual memory system. The results of running the benchmark are shown in Table <ref type="table" target="#tab_6">8</ref>.</p><p>These results serve to demonstrate one of the valuable benefits of hierarchical testing, namely that of enforcing consistency across modules in result processing, which is especially valuable when a benchmark is extended. The hierarchical benchmark had initially been implemented using the ASCM result classification scheme, which works reasonably well in the context of file management, because process termination is not the expected outcome of any file operation. However, process termination is often the correct means of signaling an error in interactions with the virtual memory system (e.g., as in the case of an attempt to reference an invalid address). When the benchmark was extended to test memory_objects in addition to file_objects, the limitations of the ASCM scheme became apparent; many otherwise correct outcomes were simply classified as ter- minated. As can be seen from Table <ref type="table" target="#tab_6">8</ref> the large numbers of test outcomes that are misclassified by the ASCM scheme might lead to a conclusion that the virtual memory system was of very low quality. (Due to the misclassifications, less than 10 percent of the tests would have appeared to have produced the correct outcome.) However, further analysis revealed that the result classification system, and not the virtual memory system, was at fault. This led to the modified result classifications, wherein terminations are classed as either correct or unexpected_termination. The original shortcoming of the result analysis was made obvious by the hierarchical structure of the benchmark, which enforced comparability of the results across the two modules. A more ad hoc modular approach to the problem might easily have obscured it.</p><p>In addition, it should be noted that this case study exemplifies one of the potential pitfalls of robustness benchmark design-it is difficult to define error classes without knowing the outcomes of all possible tests to which those classes may be applied. The ASCM benchmark, for example, defined the terminated class in the absence of enough data to evaluate its suitability. The ASCM evaluations of  the file module (see Table <ref type="table">6</ref>) show no occurrences of ter- minated tests at all, and only a few occurrences are reported for other modules that were similarly tested. In such cases, hierarchical testing serves, by enforcing consistency in result processing, to maintain a "placeholder" for result classes that are not well characterized. When more data on the vague classification become available, the hierarchical framework serves as a guide toward correct characterization of this new result class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS AND FUTURE WORK</head><p>In this paper, current robustness benchmark efforts have been examined. These approaches all fail to address certain issues that are critical to the long-term success of a robustness test suite. Several such issues have been delineated, and a benchmarking organization that overcomes many of these problems has been described. Our experiences with the introduction of hierarchical structure (as compared to our use of randomness or of multiprogramming) in a robustness benchmark suite are summarized in Table <ref type="table">9</ref>. Hierarchical benchmark organization does not adversely affect other desirable properties of a benchmark. By imposing a hierarchical, extensible structure upon test suites, the technique often carries a higher implementation cost, but promises improved lifespan and maintainability of robustness benchmarks. There remain several opportunities for improvement that have not been explored in the discussion of an extensible hierarchy. For example, hierarchically structured testing may provide a framework by which individual test results can be combined into an overall "index of robustness"-we believe that the organization of the test hierarchy, in conjunction with an appropriate set of weights, could be used towards this end. In addition, the possibility of extending the hierarchical structure to include benchmarks that operate via fault injection has not been explored, mainly because simple fault injection violates the notion that testing procedures should be abstracted from the implementation details whenever possible. The possibility of reconciling this difference (via an abstracted form of fault injection) should be explored.</p><p>It should be noted that the techniques presented depend upon the ability of the benchmark designer to construct an appropriate hierarchy of system facilities. Changes in the hierarchy due to oversights in the original design may require significant changes in the benchmark suite. Fortunately, the object-oriented operating systems community has devoted much effort to exploring such hierarchies, and to using them as the basis for operating system implemen-tation (e.g., <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b0">[1]</ref>). Although the optimal design of resource hierarchies is still a matter of debate, we believe that further experience will show that it does not present an insurmountable problem for hierarchically structured benchmark suites. The implementor of a benchmark has an advantage in that his hierarchy need only encode the structure of the facilities of the subsystems of interest (rather than serve as the basis for the entire operating system structure). In addition, given the trend towards objectoriented design in operating sytems, it is likely that the implementors of an operating system (or family of systems) will already have done much of the most difficult groundwork in defining a hierarchy by the time the system is ready for robustness testing. The cost of reconciling significant differences in system structure, when evaluating multiple systems with different pre-existing hierarchies (as defined by the OS designers), also remains to be determined.</p><p>Much of the work presented herein has been carried out in the context of measuring the robustness of a Unix-like operating system. The kernel of the operating system has been regarded as an opaque, monolithic entity. However, the advent of microkernel-based operating systems presents an opportunity for the lowest levels of such a hierarchical benchmark to focus on the robustness of the layers beneath the operating system server-i.e., on the microkernel itself. Hierarchically structured benchmarks can easily be extended to incorporate the notion of benchmarking one level deeper (simply by adding one or more layers to the bottom of the benchmark hierarchy).</p><p>Finally, this approach to measuring robustness can be applied easily to any large software system that has a modular structure. Such application promises the same improvements in extensibility, maintenance cost, and consistency of benchmarks that are obtained when measuring operating system robustness. In particular, the proliferation of multiplatform object-oriented runtime environments is likely to produce many opportunities for similar robustness evaluation. (In the near future, multiple implementations of the Java <ref type="bibr" target="#b6">[7]</ref> runtime environment are likely to provide one such arena.) The detailed investigation of such applications remains to be pursued. should not be interpreted as representing official policies, either expressed or implied, of CSC or ONR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>3</head><label>3</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Part of one possible hierarchy. Abstract classes, representing abstractions, are in boldface, while ordinary (nonabstract) classes, representing system modules or data types, are not.</figDesc><graphic coords="7,280.36,278.54,258.08,151.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>for i = 1 to num_test_objects { Storage_Object obj; obj = file_test_lib::get_test_obj(i); test_storage_object(obj);} }</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>This research was supported, in part, by the Computer Sciences Corporation under Contract No. G.S. 09K90B HD0001LFP and by the Office of Naval Research under Contract No. N00014-91-J-4139. The views and conclusions contained in this document are those of the authors andTABLE 9 THE EFFECTS OF IMPLEMENTATION CHOICES ON BENCHMARK CHARACTERISTICS, IN THE BENCHMARK EFFORTS EXAMINED ⇑ indicates that the observed correlation, where present, was positive, whereas ⇓ indicates a negative correlation, if any. Empty slots indicate factors that were independent in the benchmarks examined.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="6,-106.07,83.55,513.86,102.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="11,-28.18,73.48,604.52,96.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="11,-29.07,217.76,604.52,96.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 A</head><label>1</label><figDesc>COMPARISON OF FAILURE CLASSIFICATIONS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2 THE</head><label>2</label><figDesc>PROPERTIES OF VARIOUS EXAMPLE BENCHMARKSProperties marked "variable" are unconstrained by the design of the benchmark, and vary between the individual tests in the benchmark.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 4 TIME</head><label>4</label><figDesc>TAKEN BY CMU crashme TO CRASH SOME MACHINES</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>This test attempts to exhaust the supply of a Resource in the hopes of stimulating anomalous behavior. Similarly, a more specific test, applicable only to Storage_Objects, might attempt to stimulate error conditions by writing unusually large segments of data:Finally, the two tests can then be applied to objects of the appropriate types:</figDesc><table><row><cell>{</cell></row><row><cell>resource_test (&amp;p);</cell></row><row><cell>resource_test (&amp;f);</cell></row><row><cell>storage_obj_test (&amp;f);}</cell></row><row><cell>}</cell></row><row><cell>void resource_test(Resource *r)</cell></row><row><cell>{</cell></row><row><cell>for (;;) {</cell></row><row><cell>// keep allocating forever</cell></row><row><cell>r-&gt;allocate(1);</cell></row><row><cell>}</cell></row><row><cell>}</cell></row><row><cell>void storage_obj_test(Storage_Object *s)</cell></row><row><cell>{</cell></row><row><cell>// write out a big data block</cell></row><row><cell>s-&gt;write_data(buf, 99*1024*1024);</cell></row><row><cell>}</cell></row><row><cell>file_object f;</cell></row><row><cell>process_object p;</cell></row><row><cell>main()</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 7 A</head><label>7</label><figDesc>MATRIX SHOWING HOW THE MODULE-AND TEST-SPECIFIC ANALYSES OF A TEST RESULT ARE COMBINED TO DETERMINE THE ULTIMATE CLASSIFICATION OF THAT RESULTThe categories correct, unexpected_error and bad_success correspond to the ASCM result classes with similar names, while the unexpected_termination class supersedes the ASCM terminated class. The derivation of this matrix, keyed to the figure by the subscripts, is discussed at length in the text.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 8 EVALUATION</head><label>8</label><figDesc>OF THE file_object AND memory_object MODULES USING THE HIERARCHICAL BENCHMARK</figDesc><table /><note><p>The result classes used are those defined by the ASCM benchmark described in Section 4.3, modified so that the terminated class has been replaced with a class containing only unexpected terminations. Terminations which were the expected outcome of a test are counted as being correct. The number of correct outcomes that were due to expected terminations is included in parentheses in the table, and represents the number of results that would have been misclassified by the ASCM scheme.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>In practice, this measure is limited by a benchmark's ability to detect changes made to its own state.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>Of course, this applies only to systems that are built on top of a microkernel.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Arup Mukherjee received the BSE degree in computer science and engineering from the University of Pennsylvania in 1990, and the MS degree in computer science from Carnegie Mellon University in 1993. Arup has worked in the Large Systems and Highly Parallel Systems groups at the IBM Thomas J. Watson Research Center and in the Spring Object-Oriented Operating System group at Sun Microsystems Laboratories, Inc. He is currently a doctoral student in computer science at Carnegie Mellon University. His thesis research focuses on the problems of structuring online services to operate efficiently in the face of limited bandwidth. Arup's research interests include operating systems, distributed systems, mobile computing, fault tolerance, electronic commerce, multimedia, and computer security. He is a member of Tau Beta Pi, Phi Beta Kappa, Eta Kappa Nu, and Sigma Xi. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Implementing a Modular Object Oriented Distributed Operating System on Top of CHO-RUS</title>
		<author>
			<persName><forename type="first">P</forename><surname>Amaral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jacquemot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Open Forum &apos;92</title>
		<meeting>Open Forum &apos;92<address><addrLine>Utrecht, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992-11">Nov. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fault Injection Experiments Using FIAT</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Barton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Czeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Z</forename><surname>Segall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Siewiorek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Computers</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="575" to="582" />
			<date type="published" when="1990-04">Apr. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Research Directions in Concurrent Object-Oriented Programming</title>
		<author>
			<persName><forename type="first">R</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Islam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
	<note>Choices: A Parallel Object-Oriented Operating System</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Understanding Fault-Tolerant Distributed Systems</title>
		<author>
			<persName><forename type="first">F</forename><surname>Cristian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comm. ACM</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="56" to="78" />
			<date type="published" when="1991-02">Feb. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Synthetic Benchmark</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Curnow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Computer J</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="49" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Measuring Robustness of a Fault Tolerant Aerospace System</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Dingman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Siewiorek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25th Int&apos;l Symp. Fault-Tolerant Computing</title>
		<meeting>25th Int&apos;l Symp. Fault-Tolerant Computing<address><addrLine>Pasadena, Calif</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995-06">June 1995</date>
			<biblScope unit="page" from="522" to="527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The Java(tm) Language Environment: A White Paper</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gosling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mcgilton</surname></persName>
		</author>
		<ptr target="http://java.sun.com/whitePaper/java-whitepaper-1.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The Spring Nucleus: A Microkernel for Objects</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kougiouris</surname></persName>
		</author>
		<idno>TR-93-14</idno>
		<imprint>
			<pubPlace>Mountain View, Calif.</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Sun Microsystems Laboratories</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Kanawati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Kanawati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Abraham</surname></persName>
		</author>
		<title level="m">FERRARI: A Proc. IEEE Workshop Fault-Tolerant Parallel and Distributed Systems</title>
		<meeting><address><addrLine>Amherst, Mass</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992-07">July 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">FINE: A Fault Injection and Monitoring Environment for Tracing the Unix System Behavior under Faults</title>
		<author>
			<persName><forename type="first">W.-I</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Software Eng</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="105" to="106" />
			<date type="published" when="1993-11">Nov. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An Empirical Study of the Reliability of UNIX Utilities</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fredriksen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>So</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comm. ACM</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="32" to="43" />
			<date type="published" when="1990-12">Dec. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Overview of the Chorus Distributed Operating System</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rozier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Abrossimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Armand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Boule</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guillermont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Langlois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Neuhauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Usenix Workshop Micro-kernels and Other Kernel Architectures</title>
		<meeting>Usenix Workshop Micro-kernels and Other Kernel Architectures<address><addrLine>Seattle</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992-04">Apr. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Open System Fault Management: Fault Tolerant MACH</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Russinovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Segall</surname></persName>
		</author>
		<idno>CMUCDS-92-8</idno>
		<imprint>
			<pubPlace>Pittsburgh, Pa</pubPlace>
		</imprint>
		<respStmt>
			<orgName>CMU Research Center for Dependable Systems, Carnegie Mellon Univ</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Research Report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Waterside Assoc</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1990">1990</date>
			<pubPlace>Fremont, Calif., Spring</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Standard Performance Evaluation Corporation</orgName>
		</respStmt>
	</monogr>
	<note>SPEC Newsletter</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Development of a Benchmark to Measure System Robustness</title>
		<author>
			<persName><forename type="first">B.-H</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hudak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Siewiorek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Segall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fault Tolerant Computing Systems: Proc. 23rd Int&apos;l Symp</title>
		<meeting><address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993-06">June 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Software Defects and their Impact on System Availability-A Study of Field Failures in Operating Systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chillarege</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fault Tolerant Computing Systems: Proc. 21st Int&apos;l Symp</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991-06">June 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dhrystone: A Synthetic Systems Programming Benchmark</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Weicker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comm. ACM</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">30</biblScope>
			<date type="published" when="1984-10">Oct. 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hartstone: Synthetic Benchmark Requirements for Hard Real-Time Applications</title>
		<author>
			<persName><forename type="first">N</forename><surname>Weiderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ada Letters</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="126" to="136" />
			<date type="published" when="1990">1990</date>
			<pubPlace>Winter</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
