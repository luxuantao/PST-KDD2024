<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploiting Edge Features for Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Liyu</forename><surname>Gong</surname></persName>
							<email>liyu.gong@uky.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Biomedical Informatics</orgName>
								<orgName type="institution">University of Kentucky</orgName>
								<address>
									<settlement>Lexington</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qiang</forename><surname>Cheng</surname></persName>
							<email>qiang.cheng@uky.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Biomedical Informatics</orgName>
								<orgName type="institution">University of Kentucky</orgName>
								<address>
									<settlement>Lexington</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Kentucky</orgName>
								<address>
									<settlement>Lexington</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Exploiting Edge Features for Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Edge features contain important information about graphs. However, current state-of-the-art neural network models designed for graph learning, e.g., graph convolutional networks (GCN) and graph attention networks (GAT), inadequately utilize edge features, especially multidimensional edge features. In this paper, we build a new framework for a family of new graph neural network models that can more sufficiently exploit edge features, including those of undirected or multi-dimensional edges. The proposed framework can consolidate current graph neural network models, e.g., GCN and GAT. The proposed framework and new models have the following novelties: First, we propose to use doubly stochastic normalization of graph edge features instead of the commonly used row or symmetric normalization approaches used in current graph neural networks. Second, we construct new formulas for the operations in each individual layer so that they can handle multi-dimensional edge features. Third, for the proposed new framework, edge features are adaptive across network layers. Fourth, we propose to encode edge directions using multi-dimensional edge features. As a result, our proposed new framework and new models are able to exploit a rich source of graph edge information. We apply our new models to graph node classification on several citation networks, whole graph classification, and regression on several molecular datasets. Compared with the current stateof-the-art methods, i.e., GCNs and GAT, our models obtain better performance, which testify to the importance of exploiting edge features in graph neural networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep neural networks have become one of the most successful machine learning techniques in recent years. In many important problems, they achieve state-of-the-art performance, e.g., convolutional neural networks (CNN) <ref type="bibr" target="#b19">[19]</ref> GNN Layer EGNN Layer A GNN layer could be a GCN layer, or a GAT layer, while an EGNN layer is an edge enhanced counterpart of it. EGNN differs from GNN structurally in two folds. Firstly, the adjacency matrix A in GNN is either a binary matrix that indicates merely the neighborhood of each node and is used in GAT layers, or a nonnegative-valued matrix that has one dimensional edge features and is used in GCN layers; in contrast, EGNN uses the multi-dimensional nonnegative-valued edge features represented as a tensor E which may exploit multiple attributes associated with each edge. Secondly, in GNN the same original adjacency matrix A is fed to every layer; in contrast, the edge features in EGNN are adapted at each layer before being fed to next layer.</p><p>in image recognition, and recurrent neural networks (RNN) <ref type="bibr" target="#b12">[12]</ref> and Long Short Term Memory (LSTM) <ref type="bibr" target="#b14">[14]</ref> in natural language processing. In real world, many problems can be naturally modeled with graphs rather than conventional tables, grid type images, or time sequences. Generally, a graph contains nodes and edges, where nodes represent entities in real world, and edges represent interactions or relationships between entities. For example, a social network naturally models users as nodes and friendship relationships as edges. For each node, there is often an associated feature vector describing it, e.g., a user's profile in a social network.</p><p>Similarly, each edge is also often associated with features depicting relationship strengths or other properties. Due to their complex structures, a challenge in learning on graphs is to find effective ways to incorporate different sources of information contained in graphs into computational models such as neural networks.</p><p>Recently, several neural network models have been developed for graph learning, which obtain better performance than traditional techniques. Inspired by graph Fourier transform, Defferrard et al. <ref type="bibr" target="#b11">[11]</ref> propose a graph convolution operation as an analogue to standard convolutions used in CNN. Just like the convolution operation in image spatial domain is equivalent to multiplication in the frequency domain, convolution operators defined by polynomials of a graph Laplacian is equivalent to filtering in the graph spectral domain. Particularly, by applying Chebyshev polynomials to the graph Laplacian, spatially localized filtering is obtained. Kipf et al. <ref type="bibr" target="#b18">[18]</ref> approximate the polynomials using a re-normalized first-order adjacency matrix to obtain comparable results on graph node classification tasks. Those graph convolutional networks (GCNs) <ref type="bibr" target="#b11">[11]</ref>[18] combine graph node features and graph topological structural information to make predictions. Velickovic et al. <ref type="bibr" target="#b28">[27]</ref> adopt attention mechanism into graph learning, and propose a graph attention network (GAT). Unlike GCNs, which use a fixed or learnable polynomial of Laplacian or adjacency matrix to aggregate (filter) node information, GAT aggregates node information by using an attention mechanism on graph neighborhoods. The essential difference between GAT and GCNs is stark: In GCNs the weights for aggregating (filtering) neighbor nodes are defined by the graph topological structure, which is independent of node contents; in contrast, weights in GAT are a function of node contents due to the attention mechanism. Empirical results on graph node classification show that the adaptiveness of GAT makes it more effective to fuse information from node features and graph topological structures.</p><p>One major problem in the current GNN models, such as GAT and GCNs, is that edge features are not fully incorporated. In GAT, graph topological information is injected into the model by forcing the attention coefficient between two nodes to zero if they are not connected. Therefore, the edge information used in GAT is only the indication about whether there is an edge or not, i.e., connectivities. However, graph edges are often in possession of rich information like strengths, types, etc. Instead of being a binary indicator variable, edge features could be continuous, e.g., strengths, or multi-dimensional. GCNs can utilize one-dimensional real-valued edge features, e.g., edge weights, but the edge features are restricted to be one-dimensional. Properly addressing this problem is likely to benefit many graph learning problems. Another problem of GAT and GCNs is that each GAT or GCN layer filters node features based on the original adjacency matrix that is given as an input. The original adjacency matrix is likely to be noisy and not optimal, which will limit the effectiveness of the filtering operation.</p><p>In this paper, we address the above problems by proposing new GNN models to more adequately exploit edge information, which naturally enhance current GCNs and GAT models. Our models construct different formulas from those of GCNs and GAT, so that they are capable of exploiting multi-dimensional edge features. Also our new models can exploit one-dimensional edge features more effectively by making them adaptive across network layers. Moreover, our models leverage doubly stochastic normalization to augment the GCNs and GAT models that use ordinary row or symmetric edge normalization. Doubly stochastic matrices have nice properties that can facilitate the use of edges.</p><p>We conduct experiments on several citation network datasets and molecular datasets. For citation networks, we encode directed edges as three dimensional edge feature vectors. For molecular datasets, different atom bond types are naturally encoded as multi-dimensional edge attributes. By leveraging those multi-dimensional edge features our methods outperform current state-of-the-art approaches. The results confirm that edge features are important for graph learning, and our proposed EGNN models are effective incorporating edge features.</p><p>As a summary, the novelties of our proposed EGNN model include the following:</p><p>• A new framework for adequately exploiting multidimensional edge features. Our new framework is able to incorporate multi-dimensional positive-valued edge features. It eliminates the limitation of GAT which can handle only binary edge indicators and the limitation of GCNs which can handle only one dimensional edge features.</p><p>• Doubly stochastic edge normalization. We propose to normalize edge feature matrices into doubly stochastic matrices which show improved performance in denoising <ref type="bibr" target="#b30">[29]</ref>.</p><p>• Attention based edge adaptiveness across neural network layers. We design a new graph network architecture which can not only filter node features but also adapt edge features across layers. Leveraging this new architecture, in our models the edge features are adaptive to both local contents and the global layers when passing through the layers of the neural network.</p><p>• Multi-dimensional edge features for directed edges. We propose a method to encode edge directions as multi-dimensional edge features. Therefore, our EGNN can effectively learn on directed graphs.</p><p>The rest of this paper is organized as follows: Section 2 briefly reviews the related works. Details of the proposed EGNN architecture and two types of proposed EGNN layers are described in Section 3. Section 4 presents the experimental results, and Section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>A critical challenge in graph learning is the complex non-Euclidean structure of graph data. To address this challenge, traditional machine learning approaches extract graph statistics (e.g., degrees) <ref type="bibr" target="#b5">[5]</ref>, kernel functions <ref type="bibr" target="#b29">[28]</ref> <ref type="bibr" target="#b25">[24]</ref> or other hand-crafted features which measure local neighborhood structures. Those methods lack flexibility in that designing sensible hand-crafted features is time consuming and extensive experiments are needed to generalize to different tasks or settings. Instead of extracting structural information or using hand-engineered statistics as features of the graph, graph representation learning attempts to embed graphs or graph nodes in a low-dimensional vector space using a data-driven approach. One kind of embedding approaches are based on matrix-factorization, e.g., Laplacian Eigenmap (LE) <ref type="bibr" target="#b4">[4]</ref>, Graph Factorization (GF) algorithm <ref type="bibr" target="#b1">[2]</ref>, GraRep <ref type="bibr" target="#b7">[7]</ref>, and HOPE <ref type="bibr" target="#b21">[21]</ref>. Another class of approaches focus on employing a flexible, stochastic measure of node similarity based on random walks, e.g., DeepWalk <ref type="bibr" target="#b22">[22]</ref>, node2vec <ref type="bibr" target="#b1">[2]</ref>, LINE <ref type="bibr" target="#b27">[26]</ref>, and HARP <ref type="bibr" target="#b9">[9]</ref>. There are several limitations in matrix factorization-based and random walkbased graph learning approaches. First, the embedding function which maps to a low-dimensional vector space is linear or overly simple so that complex patterns cannot be captured; Second, they typically do not incorporate node features; Finally, they are inherently transductive, for the whole graph structure is required in the training phase.</p><p>Recently these limitations in graph learning have been addressed by adopting new advances in deep learning. Deep learning with neural networks can represent complex mapping functions and be efficiently optimized by gradientdescent methods. To embed graph nodes to a Euclidean space, deep autoencoders are adopted to extract connectivity patterns from the node similarity matrix or adjacency matrix, e.g., Deep Neural Graph Representations (DNGR) <ref type="bibr" target="#b8">[8]</ref> and Structural Deep Network Embeddings (SDNE) <ref type="bibr" target="#b31">[30]</ref>. Although autoencoder-based approaches are able to capture more complex patterns than matrix factorization based and random walk based methods, they are still unable to leverage node features.</p><p>With celebrated successes of CNN in image recognition, recently, there has been an increased interest in adapting convolutions to graph learning. In <ref type="bibr" target="#b6">[6]</ref>, the convolution operation is defined in the Fourier domain, that is, the spectral space, of the graph Laplacian. The method is afflicted by two major problems: Firstly, the eigen decomposition is computationally intensive; secondly, filtering in the Fourier domain may result in non-spatially localized effects. In <ref type="bibr" target="#b13">[13]</ref>, a parameterization of the Fourier filter with smooth coefficients is introduced to make the filter spatially localized. <ref type="bibr" target="#b11">[11]</ref> proposes to approximate the filters by using a Chebyshev expansion of the graph Laplacian, which produces spatially localized filters, and also avoids computing the eigenvectors of the Laplacian.</p><p>Attention mechanisms have been widely employed in many sequence-based tasks <ref type="bibr" target="#b3">[3]</ref>[33] <ref type="bibr" target="#b16">[16]</ref>. Compared with convolution operators, attention mechanisms enjoy two benefits: Firstly, they are able to aggregate any variable sized neighborhood or sequence; further, the weights for aggregation are functions of the contents of a neighborhood or sequence. Therefore, they are adaptive to the contents. <ref type="bibr" target="#b28">[27]</ref> adapts an attention mechanism to graph learning and proposes a graph attention network (GAT), achieving current state-of-the-art performance on several graph node classification problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Edge feature enhanced graph neural networks 3.1. Architecture overview</head><p>Given a graph with N nodes, let X be an N × F matrix representation of the node features of the whole graph. We denote an element of a matrix or a tensor by indices in the subscript. Specifically, the "•" notation in the subscript is used to select the whole range (slice) of a dimension. Therefore, X ij will represent the value of the j th feature of the i th node. X i• ∈ R F , i = 1, 2, . . . , N , represents the Fdimensional feature vector of the i th node. Similarly, let E be an N ×N ×P tensor representing the edge features of the graph. Then E ij• ∈ R P , i = 1, 2, . . . , N, j = 1, 2, . . . , N , represents the P -dimensional feature vector of the edge connecting the i th and j th nodes, and E ijp denotes the p th channel of the edge feature in E ij• . We use the notation E ij• = 0 to mean that there is no edge between the i th and j th nodes. Let N i , i = 1, 2, . . . , N , denote the index set of neighboring nodes of node i.</p><p>Our proposed network has a multi-layer feed-forward architecture. We use superscript l to denote the output of the l th layer. The inputs to the network are denoted by X 0 and E 0 . After passing through the first EGNN layer, X 0 is filtered to produce an N × F 1 new node feature matrix X 1 . In the mean time, edge features are adapted to E 1 that preserves the dimensionality of E 0 . The adapted E 1 is fed to the next layer as edge features. This procedure is repeated for every subsequent layer. Within each hidden layer, non-linear activations can be applied to the filtered node features X l . The node features X L can be considered as an embedding of the graph nodes in an F L -dimensional space. For a node classification problem, a soft-max operator will be applied to each node embedding vector X L i• along the last dimension. For a whole-graph prediction (classification or regression) problem, a pooling layer is applied to the first dimension of X L so that the feature matrix is reduced to a single vector embedding for the whole graph. Then a fully connected layer is applied to the vector, whose output could be used as predictions for regression, or logits for classification. The weights of the network will be trained with supervision from ground truth labels. Figure <ref type="figure" target="#fig_0">1</ref> gives a schematic illustration of the EGNN architecture with a comparison to the existing GNN architectures. Note that the input edge features in E 0 are already pre-normalized. The normalization method will be described in the next subsection. Two types of EGNN layers, attention based EGNN (EGNN(A)) layer and convolution based EGNN (EGNN(C)) layer will also be presented in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Doubly stochastic normalization of edges</head><p>In graph convolution operations, the edge feature matrices will be used as filters to multiply the node feature matrix. To avoid increasing the scale of output features by multiplication, the edge features need to be normalized. Let Ê be the raw edge features, our normalized features E is produced as follows:</p><formula xml:id="formula_0">Ẽijp = Êijp N k=1 Êikp<label>(1)</label></formula><formula xml:id="formula_1">E ijp = N k=1 Ẽikp Ẽjkp N v=1 Ẽvkp<label>(2)</label></formula><p>Note that all elements in Ê are nonnegative. It can be easily verified that such kind of normalized edge feature tensor E satisfies the following properties:</p><formula xml:id="formula_2">E ijp ≥ 0,<label>(3)</label></formula><formula xml:id="formula_3">N i=1 E ijp = N j=1 E ijp = 1.<label>(4)</label></formula><p>In other words, the edge feature matrices E ••p for p = 1, 2, • • • , P are square nonnegative real matrices with rows and columns summing to 1. Thus, they are doubly stochastic matrices, i.e., they are both left stochastic and right stochastic. Doubly stochastic matrices (DSMs) have several nice properties, e.g., they are symmetric, positive semidefinite and having the largest eigen-value 1. The graph convolution has an effect similar to passing information through edges in a diffusion process by iteratively multiplying the previous result with the edge matrix. Since taking the power of a DSM preserves the three mentioned properties, it prevents the edge matrix from exploding or shrinking to zero during diffusion, thus can help stabilize the process, compared with the previously used row normalization as in GAT <ref type="bibr" target="#b28">[27]</ref>:</p><formula xml:id="formula_4">E ijp = Êijp N j=1 Êijp<label>(5)</label></formula><p>or symmetric normalization as in GCN <ref type="bibr" target="#b18">[18]</ref>:</p><formula xml:id="formula_5">E ijp = Êijp N i=1 Êijp N j=1 Êijp<label>(6)</label></formula><p>Further, the powering (or diffusion) has an effect of increasing the gaps between large egien-values, which denoises the edges. The effectiveness of doubly stochastic matrix has been recently demonstrated for graph edges denoising <ref type="bibr" target="#b30">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">EGNN(A): Attention based EGNN layer</head><p>We describe the attention based EGNN layer. The original GAT model <ref type="bibr" target="#b28">[27]</ref> is only able to handle one dimensional binary edge features, i.e., the attention mechanism is defined on the node features of the neighborhood, which does not take the real valued edge features, e.g., weights, into account. To exploit multi-dimensional nonnegative-valued edge features, we propose a new attention mechanism. In our new mechanism, feature vector X l i• will be aggregated from the feature vectors of the neighboring nodes of the i th node, i.e., {X j , j ∈ N i }, by simultaneously incorporating the corresponding edge features. Utilizing the tensor and the notation that zero valued edge features mean no edge connections, the aggregation operation is defined as follows:</p><formula xml:id="formula_6">X l = σ   P p=1 α l ••p (X l−1 , E l−1 ••p )g l (X l−1 )   . (<label>7</label></formula><formula xml:id="formula_7">)</formula><p>Here, is the concatenation operator; σ is a non-linear activation; α is a function which produces an N × N × P tensor; g is a transformation which maps the node features from the input space to the output space, and usually a linear mapping is used:</p><formula xml:id="formula_8">g l (X l−1 ) = X l−1 W l ,<label>(8)</label></formula><p>where W l is an F l−1 × F l weight matrix. In Eq. ( <ref type="formula" target="#formula_6">7</ref>), α l contains the attention coefficients, whose specific entry α l ijp is a function of</p><formula xml:id="formula_9">X l−1 i• , X l−1 j•</formula><p>and E ijp . In existing attention mechanisms <ref type="bibr" target="#b28">[27]</ref>, the attention coefficient depends on X i• and X j• only. Our mechanism allows the attention operation to be guided by edge features. For multiple dimensional edge features, we consider them as multi-channel signals, and each channel will guide a separate attention operation. The results from different channels are combined by the concatenation operation. For a specific channel of edge features, our attention function is chosen to be the following:</p><formula xml:id="formula_10">αl ijp = f l (X l−1 i• , X l−1 j• )E l−1 ijp ,<label>(9)</label></formula><formula xml:id="formula_11">α l ••p = DS(α l ••p ), (<label>10</label></formula><formula xml:id="formula_12">)</formula><p>where DS is the doubly stochastic normalization operator defined in Eqs. ( <ref type="formula" target="#formula_0">1</ref>) and ( <ref type="formula" target="#formula_1">2</ref>). In principle, f l can be any ordinary attention function which produces a scalar value from two input vectors. In this paper, we use a linear function as the attention function for simplicity:</p><formula xml:id="formula_13">f l (X l−1 i• , X l−1 j• ) = exp L a T [X l−1 i• W l X l−1 j• W l ] ,<label>(11)</label></formula><p>where L is the LeakyReLU activation function; W l is the same mapping as in <ref type="bibr" target="#b8">(8)</ref>.</p><p>The attention coefficients will be used as new edge features for the next layer:</p><formula xml:id="formula_14">E l = α l . (<label>12</label></formula><formula xml:id="formula_15">)</formula><p>By doing so, EGNN adapts the edge features across the network layers, which helps capture essential edge features as determined by our novel attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">EGNN(C): Convolution based EGNN layer</head><p>By regarding the graph convolution operation as a special case of the graph attention operation, we derive our EGNN(C) layer from the formula of EGNN(A) layer. Indeed, the essential difference between GCN <ref type="bibr" target="#b18">[18]</ref> and GAT <ref type="bibr" target="#b28">[27]</ref> is whether we use the attention coefficients (i.e., matrix α) or the adjacency matrix to aggregate node features. With this view, we derive EGNN(C) by replacing the attention coefficient matrices α ••p with the corresponding edge feature matrices E ••p . The resulting formula for EGNN(C) is given as follows:</p><formula xml:id="formula_16">X l = σ   P p=1 E ••p X l−1 W l   ,<label>(13)</label></formula><p>where the notations are the same as in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Edge features for directed graph</head><p>In real world, many graphs are directed. Often times, edge direction contains important information about the graph. For example, in a citation network, machine learning papers sometimes cite mathematics papers or other theoretical papers. However, mathematics papers may seldom cite machine learning papers. In many previous studies including GCNs and GAT, edge directions are not considered. In their experiments, directed graphs such as citation networks are simply treated as undirected graphs. In this paper, we show in the experiment part that discarding edge directions will lose important information. By viewing directions of edges as a kind of edge features, we encode a directed edge channel E ijp to be Êijp Êjip Êijp + Êjip . Therefore, each directed channel is augmented to three channels. Note that the three channels define three types of neighborhoods: forward, backward, and undirected. As a result, EGNN will aggregate node information from these three different types of neighborhoods, which contains the direction information. Taking the citation network for instance, EGNN will apply the attention mechanism or convolution operation on the papers that a specific paper cited, the papers cited this paper, and the union of the former two. With such edge features, discriminative patterns in various types of neighborhoods can be effectively captured.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental results</head><p>For all the experiments, we implement the algorithms in Python on the TensorFlow platform <ref type="bibr" target="#b0">[1]</ref>. In all the experiments, models are trained with a Nvidia Tesla K40 graphics card with 12 Gigabyte graphics memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Citation networks</head><p>To benchmark the effectiveness of our proposed models, we apply them to the network node classification problem. Three datasets are tested: Cora <ref type="bibr" target="#b23">[23]</ref>, Citeseer <ref type="bibr" target="#b23">[23]</ref>, and Pubmed <ref type="bibr" target="#b20">[20]</ref>. Some basic statistics about these datasets are listed in Table <ref type="table" target="#tab_0">1</ref>. All three datasets are directed graphs, where edge directions represent the directions of citations. For Cora and Citeseer, node features contain binary indicators representing the occurrences of predefined keywords in a paper. For Pubmed, term frequency-inverse document frequency (TF-IDF) features are employed to describe the network nodes (i.e., papers). The three citation network datasets are also used in <ref type="bibr" target="#b33">[32]</ref> [18] <ref type="bibr" target="#b28">[27]</ref>. However, they all use a pre-processed version which discards the edge directions. Since our EGNN models require the edge directions to construct edge features, we use the original version from <ref type="bibr" target="#b23">[23]</ref> and <ref type="bibr" target="#b20">[20]</ref>. For each of the 82.5 ± 0.3% 88.4 ± 0.3% 69.4 ± 0.4% 76.5 ± 0.3% 85.7 ± 0.1% 86.7 ± 0.1% EGNN(A)[ADMW] 83.1 ± 0.4% 88.4 ± 0.3% 69.3 ± 0.3% 76.3 ± 0.5% 85.6 ± 0.2% 85.7 ± 0.2% three datasets, we split nodes into 3 subsets for training, validation and testing. Two splittings are tested. One splitting has 5%, 15% and 80% sized subsets for training, validation and test, respectively. Since it has a small training set, we call it "sparse" splitting. Another splitting has 60%, 20% and 20% sized subsets, which is called "dense" splitting.</p><p>Following the experiment settings of <ref type="bibr" target="#b18">[18]</ref>[27], we use two layers of EGNN in all of our experiments for fair comparison. Throughout the experiments in this subsection, we use the Adam optimizer <ref type="bibr" target="#b17">[17]</ref> with learning rate 0.005. An early stopping strategy with a window size of 100 is adopted for the citation networks; i.e., we stop training if the validation loss does not decrease for 100 consecutive epochs. We set the output dimension of W to 64 for hidden layers. We apply dropout <ref type="bibr" target="#b26">[25]</ref> with dropout rate 0.6 to both input features and normalized attention coefficients. L 2 regularization with weight decay 0.0005 is applied to the weights W and a. Moreover, exponential linear unit (ELU) <ref type="bibr" target="#b10">[10]</ref> is employed as nonlinear activations for hidden layers.</p><p>We notice that the class distributions of the training subsets of the three datasets are not balanced. To test the effects of dataset imbalance, we train each algorithm with two different loss functions, i.e., unweighted and weighted losses. The weight of a node belonging to class k is calculated as</p><formula xml:id="formula_17">K k=1 n k Kn k ,<label>(14)</label></formula><p>where K and n k are the numbers of classes and nodes belonging to the k th class in the training subset, respectively. Thus, nodes in a minority class are given larger weights and are penalized more in the loss than a majority class.</p><p>The baseline methods we used are GCN <ref type="bibr" target="#b18">[18]</ref> and GAT <ref type="bibr" target="#b28">[27]</ref>. To investigate the effectivenesses of each components, we perform ablation study of EGNN(A) and EGNN(C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>We denote a specific version of EGNN(A) or EGNN(C) by EGNN(A)[•] or EGNN(C)[•]</head><p>, where the letters in the square bracket represent different combinations of components. We denote doubly stochastic normalization, multidimensional edge features, edge adaptiveness, and weighted loss by "D", "M", "A" and "W", respectively. For example, EGNN(C)[M] means the EGNN(C) model with the component of the multi-dimensional edge features only. Totally, 18 models are tested, including the two baselines. We run each model 20 times, and record the mean and standard deviation of the classification accuracies, which are listed in Table <ref type="table" target="#tab_1">2</ref>. We can observe several interesting phenomena, some of which warrant further investigations:</p><p>• Overall, almost all EGNN variants outperform their corresponding baselines, which indicates that all the three components are able to incorporate useful information for classification. Particularly, multidimensional edge features and doubly stochastic normalization improve more than edge adaptiveness.  • The two baselines underperform on both the sparse and dense splittings of the Cora dataset. This is caused by the class imbalance of the Cora dataset. We illustrate the class distributions of the three datasets in Figure <ref type="figure" target="#fig_1">2</ref> • Performances on dense splittings are consistently higher than on sparse splitting. It is not unexpected because more training data gives an algorithm more information to tune parameters.</p><p>• We noticed that the Pubmed graph is much bigger than Cora and Citeseer. Also, its node features (TF-IDF) are more sophisticated than the bag-of-words features of the other two datasets. Finally, Pubmed is the least imbalanced. These characteristics may be the reason that the sparse and dense splittings of Pubmed get close accuracies as the sparse splitting already provides sufficient information for training.</p><p>• Another characteristic we noticed is that the average degree of Citeseer is the smallest among the citation networks. Therefore, a very limited number of edges can be utilized in a sparse splitting. This restricts the ability of our models to incorporate edge features; thus, we do not observe much performance gain on the sparse splitting of Citeseer. To compare the computational efficiency of the models, we record the average training time on the Cora dataset with dense splitting in Table <ref type="table" target="#tab_5">3</ref>. Note that EGNN(C) and EGNN(A) are full models with all components implemented. According to Table <ref type="table" target="#tab_5">3</ref>, the proposed EGNN(C) and EGNN(A) are a little higher but still essentially comparable to GCN and GAT in terms of time complexity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Molecular analysis</head><p>One promising application of graph learning is molecular analysis. A molecule can be represented as a graph, where each atom is a node and chemical bonds are edges. Unlike citation network analysis in Section 4.1, the problem here is whole-graph prediction, either classification or regression. For example, given a graph representation of a molecule, the goal may be to classify it as toxic or not, or to predict the solubility (regression). In other words, we need to predict one value for the whole graph, rather than one value for a graph node. Usually, for each chemical bond, there are several attributes associated with it, e.g., Atom Pair Type, Bond Order, and Ring Status. Therefore, the graphs intrinsically contain multi-dimensional edge features.</p><p>Three datasets, Tox21, Lipophilicity and Freesolv, are used to test our algorithms. Tox21 contains 7831 environmental compounds and drugs. Each compound is associated with 12 labels, e.g., androgen receptor, estrogen receptor, and mitochondrial membrane potential, which defines For both EGNN(A) and EGNN(C), we implement a network consisting of 2 graph processing layers, a global maxpooling layer, and a fully connected layer. For each graph processing layer, the output dimensions of the linear mapping g are fixed to be 16. For Tox21, the sigmoid cross entropy loss is applied to the output logits of the fully connected layer. For Lipo and Freesolv, the mean squared error loss is employed. The networks are trained by Adam optimizer <ref type="bibr" target="#b17">[17]</ref> with learning rate 0.0005. An early stopping strategy with a window size of 200 is adopted. L 2 regularization with weight decay 0.0001 is applied to parameters of the models except for bias parameters. Moreover, exponential linear unit (ELU) <ref type="bibr" target="#b10">[10]</ref> is employed as nonlinear activations for hidden layers.</p><p>Our models are compared with two baseline models which are shown in MoleculeNet <ref type="bibr" target="#b32">[31]</ref>: Random Forest and Weave. Random Forest is a traditional learning model which is widely applied to various problems. Weave model <ref type="bibr" target="#b15">[15]</ref> is similar to graph convolution but specifically designed for molecular analysis.</p><p>All the three datasets are split into training, validation and test subsets with a ratio of 8:1:1. We run our models 5 times, and record the means and standard deviations of performance scores. For classification task (i.e., Tox21), Area Under Curve (AUC) scores of the receiver operating characteristic (ROC) curve are recorded. Since it is a multi-label classification problem, we record the AUCs of each class and take the averaged value as the final score. For regression (i.e., Lipo and Freesolv), root mean square error (RMSE) is used as the evaluation metric. The scores are given in Table <ref type="table" target="#tab_6">4</ref>. The results show that EGNN(C) and EGNN(A) outperform the two baselines with considerable margins. On the Tox21 dataset, the AUC scores are improved by more than 0.2 compared with the Weave model. For the two regression tasks, RMSEs are improved by about 0.1 and 0.3 on the Lipo and Freesolv datasets, respectively. The scores of EGNN(C) and EGNN(A) are close on the three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we propose a new framework to address the existing problems in the current state-of-the-art graph neural network models. Specifically, we propose a new attention mechanism to incorporate multi-dimensional nonnegative-valued edge features. Then, we propose a new graph neural network architecture that adapts edge features across neural network layers. Our framework admits a formula that allows for extending convolutions to multidimensional edge features. Further, we propose to use doubly stochastic normalization, as opposed to the ordinary row normalization or symmetric normalization used in the existing graph neural network models. Finally, we propose a method to design multi-dimensional edge features for directed edges to effectively handle directed graphs. Extensive experiments are conducted on three citation network datasets for graph node classification evaluation, and on three molecular datasets to test the performance on whole graph classification and regression tasks. Experimental results show that our new framework outperforms current state-of-the-art models such as GCN and GAT significantly and consistently on all the datasets. Detailed ablation study also shows the effectiveness of each individual component in our framework.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Schematic illustration of the proposed edge enhanced graph neural network (EGNN) architecture (right), compared with the original graph neural network (GNN) architecture (left). A GNN layer could be a GCN layer, or a GAT layer, while an EGNN layer is an edge enhanced counterpart of it. EGNN differs from GNN structurally in two folds. Firstly, the adjacency matrix A in GNN is either a binary matrix that indicates merely the neighborhood of each node and is used in GAT layers, or a nonnegative-valued matrix that has one dimensional edge features and is used in GCN layers; in contrast, EGNN uses the multi-dimensional nonnegative-valued edge features represented as a tensor E which may exploit multiple attributes associated with each edge. Secondly, in GNN the same original adjacency matrix A is fed to every layer; in contrast, the edge features in EGNN are adapted at each layer before being fed to next layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Node class distribution of the training subsets of the three citation networks. The Cora dataset is more imbalanced than the other two.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of citation network datasets</figDesc><table><row><cell></cell><cell cols="3">Cora Citeseer Pubmed</cell></row><row><cell># Nodes</cell><cell>2708</cell><cell>3327</cell><cell>19717</cell></row><row><cell># Edges</cell><cell>5429</cell><cell>4732</cell><cell>44338</cell></row><row><cell cols="2"># Node Features 1433</cell><cell>3703</cell><cell>500</cell></row><row><cell># Classes</cell><cell>7</cell><cell>6</cell><cell>3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Classification accuracies on citation networks. Doubly stochastic normalization, multi-dimensional edge features, edge adaptiveness and weighted loss components are denoted by "D", "M", "A" and "W", respectively, in square brackets.</figDesc><table><row><cell>Dataset</cell><cell>Cora</cell><cell></cell><cell>CiteSeer</cell><cell></cell><cell>Pubmed</cell><cell></cell></row><row><cell>Splitting</cell><cell>Sparse</cell><cell>Dense</cell><cell>Sparse</cell><cell>Dense</cell><cell>Sparse</cell><cell>Dense</cell></row><row><cell>GCN</cell><cell cols="6">72.9 ± 0.8% 72.0 ± 1.2% 69.2 ± 0.7% 75.3 ± 0.4% 83.3 ± 0.4% 83.4 ± 0.2%</cell></row><row><cell>GAT</cell><cell cols="6">75.5 ± 1.1% 79.0 ± 1.0% 69.5 ± 0.5% 74.9 ± 0.5% 83.4 ± 0.1% 83.4 ± 0.2%</cell></row><row><cell>EGNN(C)[W]</cell><cell cols="6">82.7 ± 0.6% 87.6 ± 0.6% 69.3 ± 0.6% 76.0 ± 0.5% 84.5 ± 0.2% 84.3 ± 0.4%</cell></row><row><cell>EGNN(A)[W]</cell><cell cols="6">82.7 ± 0.6% 86.6 ± 0.6% 69.4 ± 0.5% 74.9 ± 0.8% 83.1 ± 0.2% 82.7 ± 0.2%</cell></row><row><cell>EGNN(C)[D]</cell><cell cols="6">81.8 ± 0.5% 85.1 ± 0.5% 70.6 ± 0.3% 75.0 ± 0.3% 84.3 ± 0.1% 84.1 ± 0.1%</cell></row><row><cell>EGNN(C)[DW]</cell><cell cols="6">83.2 ± 0.3% 87.4 ± 0.4% 70.3 ± 0.3% 75.4 ± 0.5% 84.1 ± 0.1% 84.1 ± 0.1%</cell></row><row><cell>EGNN(C)[M]</cell><cell cols="6">80.2 ± 0.4% 86.1 ± 0.5% 69.4 ± 0.3% 76.8 ± 0.4% 86.2 ± 0.2% 86.7 ± 0.1%</cell></row><row><cell>EGNN(C)[MW]</cell><cell cols="6">82.3 ± 0.4% 87.2 ± 0.4% 69.4 ± 0.3% 77.1 ± 0.4% 86.2 ± 0.1% 86.4 ± 0.3%</cell></row><row><cell>EGNN(C)[DM]</cell><cell cols="6">83.0 ± 0.3% 88.8 ± 0.3% 69.5 ± 0.3% 76.7 ± 0.4% 86.0 ± 0.1% 86.0 ± 0.1%</cell></row><row><cell>EGNN(C)[DMW]</cell><cell cols="6">83.4 ± 0.3% 88.5 ± 0.4% 69.5 ± 0.3% 76.6 ± 0.4% 85.8 ± 0.1% 85.6 ± 0.2%</cell></row><row><cell>EGNN(A)[A]</cell><cell cols="6">76.0 ± 1.0% 79.1 ± 1.0% 69.5 ± 0.4% 74.6 ± 0.3% 83.4 ± 0.1% 83.6 ± 0.2%</cell></row><row><cell>EGNN(A)[AW]</cell><cell cols="6">82.6 ± 0.6% 86.3 ± 0.9% 69.4 ± 0.4% 74.9 ± 0.4% 83.7 ± 0.2% 82.8 ± 0.3%</cell></row><row><cell>EGNN(A)[D]</cell><cell cols="6">80.1 ± 1.0% 85.4 ± 0.5% 70.1 ± 0.4% 74.7 ± 0.4% 84.3 ± 0.2% 84.2 ± 0.1%</cell></row><row><cell>EGNN(A)[DW]</cell><cell cols="6">82.7 ± 0.4% 87.2 ± 0.5% 69.5 ± 0.3% 74.5 ± 0.5% 83.9 ± 0.2% 83.3 ± 0.2%</cell></row><row><cell>EGNN(A)[M]</cell><cell cols="6">81.7 ± 0.4% 87.9 ± 0.4% 69.4 ± 0.3% 75.7 ± 0.3% 85.5 ± 0.1% 86.0 ± 0.1%</cell></row><row><cell>EGNN(A)[MW]</cell><cell cols="6">82.8 ± 0.3% 87.0 ± 0.6% 69.1 ± 0.3% 76.3 ± 0.5% 85.2 ± 0.2% 85.3 ± 0.3%</cell></row><row><cell>EGNN(A)[ADM]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>, from which we can see that Cora is more imbalanced than Citeseer and Pubmed. On the Cora dataset, EGNN(A)[W] and EGNN(C)[W], which add the weighted loss component only, perform much better than GAT and GCN. This verifies that the underperformance of GAT and GCN are caused by the class imbalance. Our proposed models are highly resilient to class imbalance. Without weighted training, our framework obtains high accuracies on the Cora dataset. Weighted training does not always improve performance, especially on less imbalanced datasets, e.g., Pubmed. This indicates that simply weighting the classes is not sufficient to fully solve the class imbalance problem. More sophisticated methods need to be designed to address this problem in the future.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Average training time (in milliseconds) per epoch on the Cora dataset with dense splitting.</figDesc><table><row><cell>Model</cell><cell cols="4">GCN GAT EGNN(C) EGNN(A)</cell></row><row><cell>Time (ms)</cell><cell>22</cell><cell>49</cell><cell>48</cell><cell>159</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Performance on molecular datasets For all the three datasets, compounds are converted to graphs. For all the three datasets, nodes are described by 25-dimensional feature vectors. The dimensionality of edge feature vectors are 42, 21, and 25 for Tox21, Lipo, and Freesolv, respectively.</figDesc><table><row><cell></cell><cell cols="2">Tox21 (AUC)</cell><cell cols="2">Lipo (RMSE)</cell><cell cols="2">Freesolv (RMSE)</cell></row><row><cell>Dataset</cell><cell>Validation</cell><cell>Test</cell><cell>Validation</cell><cell>Test</cell><cell>Validation</cell><cell>Test</cell></row><row><cell>RF</cell><cell cols="6">0.78 ± 0.01 0.75 ± 0.03 0.87 ± 0.02 0.86 ± 0.04 1.98 ± 0.07 1.62 ± 0.14</cell></row><row><cell>Weave</cell><cell cols="6">0.79 ± 0.02 0.80 ± 0.02 0.88 ± 0.06 0.89 ± 0.04 1.35 ± 0.22 1.37 ± 0.14</cell></row><row><cell cols="7">EGNN(C) 0.82 ± 0.01 0.82 ± 0.01 0.80 ± 0.02 0.75 ± 0.01 1.07 ± 0.08 1.09 ± 0.08</cell></row><row><cell cols="7">EGNN(A) 0.82 ± 0.01 0.81 ± 0.01 0.79 ± 0.02 0.75 ± 0.01 1.09 ± 0.12 1.01 ± 0.12</cell></row><row><cell cols="4">a multi-label classification problem. Lipophilicity contains</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">4200 compounds. The goal is to predict compound solubil-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">ity, which is a regression task. Freesolv includes a set of</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">642 neutral molecules, which similarly defines a regression</cell><cell></cell><cell></cell><cell></cell></row><row><cell>task.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TensorFlow: A System for Large-scale Machine Learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation, OSDI&apos;16</title>
				<meeting>the 12th USENIX Conference on Operating Systems Design and Implementation, OSDI&apos;16<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Josifovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Distributed Large-scale Natural</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Graph Factorization</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22Nd International Conference on World Wide Web, WWW &apos;13</title>
				<meeting>the 22Nd International Conference on World Wide Web, WWW &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<title level="m">Neural Machine Translation by Jointly Learning to Align and Translate</title>
				<imprint>
			<date type="published" when="2014-09">Sept. 2014</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Bhagat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1101.3291</idno>
		<title level="m">Node Classification in Social Networks</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="115" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral Networks and Locally Connected Networks on Graphs</title>
				<imprint>
			<date type="published" when="2013-12">Dec. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">GraRep: Learning Graph Representations with Global Structural Information</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, CIKM &apos;15</title>
				<meeting>the 24th ACM International on Conference on Information and Knowledge Management, CIKM &apos;15<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="891" to="900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep Neural Networks for Learning Graph Representations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">HARP: Hierarchical Representation Learning for Networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)</title>
		<author>
			<persName><forename type="first">D.-A</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Finding Structure in Time</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990-03">Mar. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep Convolutional Networks on Graph-Structured Data</title>
				<imprint>
			<date type="published" when="2015-06">June 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">1735</biblScope>
			<date type="published" when="1997-11">Nov. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Molecular graph convolutions: Moving beyond fingerprints</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Berndl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer -Aided Molecular Design</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="595" to="608" />
			<date type="published" when="2016-08">Aug. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Semantic Sentence Matching with Densely-connected Recurrent and Coattentive Information</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11360</idno>
		<imprint>
			<date type="published" when="2018-05">May 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998-11">Nov. 1998</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Querydriven Active Surveying for Collective Classification</title>
		<author>
			<persName><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>London</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Mining and Learning with Graphs</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Asymmetric Transitivity Preserving Graph Embedding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;16</title>
				<meeting>the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1105" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">DeepWalk: Online Learning of Social Representations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;14</title>
				<meeting>the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;14<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Collective Classification in Network Data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">AI Magazine</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">La Canada</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="106" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Weisfeiler-Lehman Graph Kernels</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011-09">Sep. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014-01">Jan. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">LINE: Large-scale Information Network Embedding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web, WWW &apos;15</title>
				<meeting>the 24th International Conference on World Wide Web, WWW &apos;15</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Graph Kernels</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1201" to="1242" />
			<date type="published" when="2010-04">Apr. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Network Enhancement: A general method to denoise weighted biological networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pourshafeie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Bustamante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Batzoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.03327</idno>
		<imprint>
			<date type="published" when="2018-05">May 2018</date>
		</imprint>
	</monogr>
	<note>cs, q-bio</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Structural Deep Network Embedding</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;16</title>
				<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1225" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">MoleculeNet: A benchmark for molecular machine learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="513" to="530" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Revisiting Semi-Supervised Learning with Graph Embeddings</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2016-06">June 2016</date>
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06978[cs,stat]</idno>
		<title level="m">Deep Interest Network for Click-Through Rate Prediction</title>
				<imprint>
			<date type="published" when="2017-06">June 2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
