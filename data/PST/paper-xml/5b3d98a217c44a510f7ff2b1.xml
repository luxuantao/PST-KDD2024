<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Contents lists available at ScienceDirect Neurocomputing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-05-04">4 May 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Uri</forename><surname>Shaham</surname></persName>
							<email>uri.shaham@yale.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Outcome Research</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<addrLine>200 Church st</addrLine>
									<postCode>06510</postCode>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yutaro</forename><surname>Yamada</surname></persName>
							<email>yutaro.yamada@yale.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<addrLine>24 Hillhouse st</addrLine>
									<postCode>06511</postCode>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sahand</forename><surname>Negahban</surname></persName>
							<email>sahand.negahban@yale.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<addrLine>24 Hillhouse st</addrLine>
									<postCode>06511</postCode>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Contents lists available at ScienceDirect Neurocomputing</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-05-04">4 May 2018</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1016/j.neucom.2018.04.027</idno>
					<note type="submission">Received 27 August 2017 Revised 2 April 2018 Accepted 6 April 2018</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Adversarial examples Robust optimization Non-parametric supervised models Deep learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We show that adversarial training of supervised learning models is in fact a robust optimization procedure. To do this, we establish a general framework for increasing local stability of supervised learning models using robust optimization. The framework is general and broadly applicable to differentiable non-parametric models, e.g., Artificial Neural Networks (ANNs). Using an alternating minimizationmaximization procedure, the loss of the model is minimized with respect to perturbed examples that are generated at each parameter update, rather than with respect to the original training data. Our proposed framework generalizes adversarial training, as well as previous approaches for increasing local stability of ANNs. Experimental results reveal that our approach increases the robustness of the network to existing adversarial examples, while making it harder to generate new ones. Furthermore, our algorithm improves the accuracy of the networks also on the original test data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Machine learning models might be very unstable locally, and have significantly different outputs on inputs which only slightly differ from one another. This may be the case even for models with high generalization ability (estimated by performance on test data). For example, Szegedy et al. <ref type="bibr" target="#b33">[34]</ref> , showed that highly performing vision ANNs mis-classify examples that have only barely perceivable (by a human eye) differences from correctly classified examples. Such examples are called adversarial examples , and although usually mentioned in the context of ANNs, they are not unique to this model family.</p><p>Adversarial examples do not tend to exist naturally in training and test data. Yet, the local instability manifested by their existence is disturbing, for several reasons. First, state-of-the-art models, including, for example, ANNs with super-human performance, may assign examples which are indistinguishable in the natural "human eye" metric to different classes, indicating that the models are far from learning the true class 'concept'. Second, adversarial examples can be generated in structured and automated ways. Third, it has been shown that different models with different architectures which are trained on different training sets tend to mis-classify the same adversarial examples in a similar fashion. This can be used to perform attacks on models by making them fail easily and consistently <ref type="bibr" target="#b12">[13]</ref> and poses serious security issues.</p><p>In recent years, the field of adversarial attacks and defenses has become a highly active research area, primarily associated with deep learning, and many attacks and defenses have been proposed <ref type="bibr" target="#b40">[41]</ref> . Improving robustness of neural nets to adversarial examples by adding such examples to the training data is arguably among the first and fundamental defense techniques against adversarial attacks, see, for example, <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b33">34]</ref> . We refer to the usage of adversarial examples during training as "adversarial training". To the best of our knowledge, despite making intuitive sense and yielding impressive empirical results, a more rigor mathematical understanding of adversarial training is lacking. The goal of this manuscript, which appeared in pre-print in 2015, is to provide a framework that yields a full theoretical understanding of adversarial training, as well as new optimization schemes, based on robust optimization. Specifically, we show that generating and using adversarial examples during training of supervised machine learning models (and ANNs in particular) can be derived from the powerful notion of robust optimization, which has many applications in machine learning and is closely related to regularization. We propose a general algorithm for robustification of non-parametric machine learning models, and show that it generalizes several previously proposed approaches for training of ANNs.</p><p>Essentially, our algorithm increases the stability of supervised models with respect to perturbations in the input data, through an iterative minimization-maximization procedure, in which the network parameters are updated with respect to worst-case data, rather than to the original training data. Furthermore, we show connections between our method and existing methods for generating adversarial examples and adversarial training, demonstrating that those methods are special instances of the robust optimization framework. This point yields a principled connection highlighting the fact that the existing adversarial training methods aim to robustify the parameter optimization process. The main application we consider in this manuscript is training of ANNs, to which our algorithm applies naturally and has an efficient implementation. Yet, the algorithm is general and can be applied to any nonparametric supervised model that is trained using gradient-based optimization.</p><p>The structure of this paper is as follows: in Section 2 we provide background on adversarial examples and robust optimization. In Section 3, we present our training framework, some of its possible variants and its practical version. Experimental results on ANNs and boosting models are given in Section 4 . Some related works are mentioned in Section 5 . Section 6 briefly concludes this manuscript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head><p>In this section, we provide elementary background on adversarial examples and robust optimization, required to justify our proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Notation</head><p>We denote a labeled training set by { (x i , y i ) } m i =1 where x i ∈ R d is a set of features and y i ∈ { 1 , . . . , K} is a label. The loss of a model with parameters θ on ( x, y ) is denoted by J ( θ , x, y ) and is a function that quantifies the goodness-of-fit between the parameters θ and the observations ( x, y ). When holding θ and y fixed and viewing J ( θ , x, y ) as a function of x we occasionally write J θ , y ( x ).</p><p>x ∈ R d corresponds to a small additive adversarial perturbation, that is to be added to x . By adversarial example we refer to the perturbed example, i.e., ˜</p><p>x i = x + x , along with the original label y . We denote the p norm for 1 ≤ p &lt; ∞ to be</p><formula xml:id="formula_0">x p p = d j=1 | x (i ) | p and denote the ∞ norm of a vector x to be x ∞ = max i {| x (i ) |} .</formula><p>Given two vectors x and y , the Euclidean inner-product is denoted</p><p>x, y = x T y = i x i y i . We denote the gradient of a function f ( x, y ) with respect to the vector x by ∇ x f ( x, y ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Adversarial examples</head><p>To this day, adversarial examples were primarily discussed in the context of ANNs. They were first introduced by Szegedy et al. <ref type="bibr" target="#b33">[34]</ref> , who generated an adversarial perturbation x for a given training point ( x, y ) by using L-BFGS <ref type="bibr" target="#b37">[38]</ref> to solve the boxconstrained optimization problem</p><formula xml:id="formula_1">min x c x 2 + J(θ , x + x , y ) subject to x + x ∈ [0 , 1] d ,</formula><p>and y = y . The fundamental idea here is to construct a small perturbation of the data point x in order to force the method to misclassify the training example x with some incorrect label y .</p><p>Goodfellow et al. <ref type="bibr" target="#b11">[12]</ref> point out that when the dimension d is large, changing each entry of x by a small value yields a perturbation x (such that x ∞ = ), which can significantly change the inner product w T x of x with a weight vector w . They propose to use an adversarial perturbation defined by</p><formula xml:id="formula_2">x = sign (∇ x J(θ , x, y )) .</formula><p>(1)</p><p>Eq. ( <ref type="formula">1</ref>) is also known as the "fast gradient sign (FGS) method". We present a simple alternative formulation of the problem to naturally show how the adversarial perturbation in (1) was obtained. If we take a first-order approximation of the loss function around the true training example x with a small perturbation x J θ ,y (x + x ) ≈ J θ ,y (x ) + ∇ J θ ,y (x ) , x , and maximize the right hand size with respect to x restricted to an ∞ ball of radius , we see that the choice that maximizes the right-hand side is exactly the quantity in Eq. <ref type="bibr" target="#b0">(1)</ref> . Replacing the ∞ ball with a 2 ball yields a perturbation in the direction of the gradient, coined "fast gradient value" <ref type="bibr" target="#b29">[30]</ref> . Since in the case of ANN, the gradient ∇ x J ( θ , x, y ) can be computed efficiently using backpropagation <ref type="bibr" target="#b30">[31]</ref> , this approach for generating adversarial examples is rather fast. In the sequel we will show how the above computation is an example of the framework that we present in this manuscript.</p><p>It is reported in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b33">34]</ref> that adversarial examples that were generated for a specific network were mis-classified in a similar fashion by other networks, with possibly different architectures and using different subsets of the data for training. This phenomenon is known as the "transferability" of adversarial examples <ref type="bibr" target="#b25">[26]</ref> , and is used to create "black-box" attacks (i.e., where the attacker has no access to the parameters and gradients of the target network).</p><p>Goodfellow et al. <ref type="bibr" target="#b11">[12]</ref> propose the following adversarial training loss function:</p><formula xml:id="formula_3">˜ J (θ , x, y ) = αJ(θ , x, y ) + (1 − α) J(θ , x + x , y ) ,<label>(2)</label></formula><p>with x as in Eq. <ref type="bibr" target="#b0">(1)</ref> . They report that the resulting net had improved test set accuracy, as well as better performance on new adversarial examples. They further give intuitive explanations of this training procedure being an adversary game, and a min-max optimization over ∞ balls. In Section 3.2 , we will attempt to make the second interpretation rigor, by deriving it from a Robust Optimization framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Robustification through random perturbations</head><p>Let f ( x ) be the output of a machine learning model on input x . A possible approach to robustification of models is to smooth f (see, for example, <ref type="bibr" target="#b23">[24]</ref> ). A naive approach to obtain such smoothing is to perturb the model input x at test time. To see this, consider a case where at test time, given input x , the model outputs f (x + w ) where w ∼ N (0, σ 2 I ). In this case</p><formula xml:id="formula_4">E w ∼N(0 ,σ 2 I) f (x + w ) ∝ w exp − w 2 σ 2 f (x + w ) dw = f * N(0 , σ 2 I) , .</formula><p>i.e, in expectation, the model output is convolved with a Gaussian. In Section 4, we will demonstrate experimentally that such mechanism indeed improves the stability of a neural net to adversarial examples. However, the approach presented in this manuscript, performs significantly better. In our approach, the robustification of the model is obtained through a modified training procedure, which is based on robust optimization . In the remainder of this section, we therefore turn to describe the main ideas of robust optimization, and several applications of it in machine learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Robust optimization</head><p>Solutions to optimization problems can be very sensitive to small perturbations in the input data of the optimization problem, in the sense that an optimal solution given the current data may turn into a highly sub-optimal or even infeasible solution given a slight change in the data. A desirable property of an optimal solution is to remain nearly optimal under small perturbations of the data. Since measurement data is typically precision-limited and might contain errors, the requirement for a solution to be stable to input perturbations becomes essential.</p><p>Robust Optimization (RO, see <ref type="bibr" target="#b0">[1]</ref> , for example) is an area of optimization theory which aims to obtain solutions which are stable under some level of uncertainty the data. The uncertainty has a deterministic and worst-case nature. The assumption is that the perturbations of the data can be drawn from specific sets U i called uncertainty sets . The uncertainty sets are often defined in terms of the type of the uncertainty and a parameter controlling the size of the uncertainty set. The Cartesian product of the sets U i is usually denoted by U.</p><p>The goal in Robust Optimization is to obtain solutions which are feasible and well-behaved under any realization of the uncertainty from U; among feasible solutions, an optimal one would be such that has the minimal cost given the worst-case realization from U. Robust Optimization problems thus usually have a min-max formulation, in which the objective function is being minimized with respect to a worst-case realization of a perturbation. For example, consider the standard linear programming problem min</p><formula xml:id="formula_5">x { c T x : Ax ≤ b} .</formula><p>The given data in this case is ( A, b, c ) and the goal is to obtain a solution x which is robust to perturbations in the data. Clearly, no solution can be well-behaved if the perturbations of the data can be arbitrary. Hence, we restrict ourselves to only allowing the perturbations to exist in the uncertainty set U. The corresponding Robust Optimization formulation is min</p><formula xml:id="formula_6">x sup (A,b,c) ∈U { c T x : Ax ≤ b} .</formula><p>Thus, the goal of the above problem is to pick an x that can work well for all possible instances of the problem parameters within the uncertainty set.</p><p>The robust counterpart of an optimization problem can sometimes be more complicated to solve than the original problem. Mutapcic and Boyd <ref type="bibr" target="#b21">[22]</ref> and Ben-Tal et al. <ref type="bibr" target="#b1">[2]</ref> propose algorithms for approximately solving the robust problem, which are based only on the algorithm for the original problem. This approach is closely related to the algorithm we propose in this manuscript.</p><p>Robust Optimization is applied in various settings in statistics and machine learning, including, for example, several parameter estimation applications. In Section 5 we discuss some connections between Robust Optimization and regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Robust optimization through stochastic optimization</head><p>Inspired by the Robust Optimization paradigm, we propose a loss function for training of non-parametric supervised models, which are trained via stochastic gradient methods, e.g., ANNs. Our approach is designed to make the model output stable in a small neighborhood around every training point x i ; this neighborhood corresponds to the uncertainty set U i . For example, we may set</p><formula xml:id="formula_7">U i = B ρ (x i , r</formula><p>) , a ball with radius r around x i with respect to some norm ρ. To do so, we select from this neighborhood a representative ˜</p><p>x i = x i + x i , which is the point on which the model output will induce the greatest loss; we then require the model output on ˜ x i to be y i , the target output for x i . This requirement acts as a regularizer, biasing the learned model towards being smooth. We expect that the increased smoothness of the model output will consequently increase the robustness of the model to adversarial examples.</p><p>We propose to train the model using a minimizationmaximization approach to optimize:</p><formula xml:id="formula_8">min θ ˜ J (θ , x, y ) = min θ m i =1 max ˜ x i ∈U i J(θ , ˜ x i , y i ) , (3)</formula><p>where U i is the uncertainty set corresponding to example i . This can be viewed as optimizing the model parameters θ with respect to a worst-case data { ( ˜ x i , y i ) } , rather than to the original training data; the i th worst-case data point is chosen from the uncertainty set U i . The uncertainty sets are determined by the type of uncertainty and can be selected based on the problem at hand. Optimization of Eq. ( <ref type="formula">3</ref>) can be done in a standard iterative fashion, where in each iteration of the algorithm two optimization subprocedures are performed. First, the model parameters θ are held fixed and for every training example x i , an additive adversarial perturbation x i is selected such that x i + x i ∈ U i and</p><formula xml:id="formula_9">x i = arg max : x i + ∈U i J θ ,y i (x i + ) . (<label>4</label></formula><formula xml:id="formula_10">)</formula><p>The model parameters θ are then updated with respect to the perturbed data { ( ˜ x i , y i ) } , where ˜</p><formula xml:id="formula_11">x i = x i + x i .</formula><p>Clearly, finding the exact x i in Eq. ( <ref type="formula" target="#formula_9">4</ref>) is intractable in general. Furthermore, performing a full optimization process in each of these sub-procedures in each iteration is not practical. Hence, we propose to minimize a surrogate to ˜ J , in which each sub-procedure is reduced to a single ascent / descent step; that is, in each iteration, we perform a single ascent step (for each i ) to find an approximation ˆ x i for x i , followed by a single descent step to update θ .</p><p>The surrogate that we consider is the first-order Taylor expansion of the loss around the example, which yields:</p><formula xml:id="formula_12">ˆ x i ∈ arg max : x i + ∈U i J θ ,y i (x i ) + ∇ J θ ,y i (x i ) , .<label>(5)</label></formula><p>This training procedure is formalized in Algorithm 1 . In words, the algorithm performs alternating ascent and descent steps, where we first ascend for each i with respect to the training example x i and descend with respect to model parameters θ .</p><p>Note that under this procedure, θ is never updated based on the original training data; rather, it is always updated based on worst-case examples which are close to the original training points (i.e., in the uncertainty sets U i ). In the sequel, we will remark on how to solve Eq. ( <ref type="formula" target="#formula_12">5</ref>) for special cases of U i . In general, one could use an algorithm like L-BFGS or projected gradient descent <ref type="bibr" target="#b22">[23]</ref> . Finally, note that in the case of neural networks, in each iteration of the algorithm, two forward and backward passes through the network are performed, one using the original training data to compute the adversarial perturbations ˜</p><p>x i and one using the perturbed data to compute the update for θ ; hence, we expect the training time to be twice as long, comparing to standard training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Examples of uncertainty sets</head><p>One may find several specific choices of uncertainty sets U i as particularly useful. One example is when U i = B ρ (x i , r) , a norm ball centered at x i with radius r with respect to the norm ρ. In such case, x i can be approximated using normalized steepest ascent step with respect to the norm ρ <ref type="bibr" target="#b3">[4]</ref> . Some interesting choices for ρ are the ∞ , 1 and 2 norms. The steepest ascent step with respect to the ∞ ball (i.e., box) is obtained by the sign of the gradient sign ∇ J θ ,y i (x i ) .</p><p>Choosing x i from an ∞ ball will therefore yield a perturbation in which every entry of x is changed by the same amount r . The steepest ascent with respect to the 2 ball coincides with the direction of the gradient ∇J θ ,y i (x i ) . Choosing x i from an 1 ball will yield a sparse perturbation, in which only one or a small number of the entries of x i are changed (those of largest magnitude in ∇J θ ,y i (x i ) ). Observe that in all three cases the steepest ascent direction is derived from the gradient ∇J θ ,y i (x i ) (in particular, in the case of ANNs, this gradient can be computed efficiently using backpropagation). In Section 4, we use each of the 1 , 2 , ∞ norms to generate adversarial examples by Eq. ( <ref type="formula" target="#formula_12">5</ref>) and compare the performance of Algorithm 1 using each of these types of uncertainty sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Relation to previous works</head><p>The loss function in Eq. ( <ref type="formula" target="#formula_3">2</ref>) , proposed by Goodfellow et al. <ref type="bibr" target="#b11">[12]</ref> , can be viewed as a variant of our approach, in which x i is chosen from an ∞ ball around x i , since θ is updated with respect to adversarial examples generated by Eq. ( <ref type="formula">1</ref>) , which is the steepest ascent step with respect to the ∞ norm. Therefore, we see that Eq. ( <ref type="formula" target="#formula_3">2</ref>) is in fact a robust optimization procedure. In addition, the solution to Eq. ( <ref type="formula" target="#formula_12">5</ref>) for the case that U i = B ∞ (x i , ) is precisely the update presented in Eq. <ref type="bibr" target="#b0">(1)</ref> .</p><p>We may also relate our proposed methodology to the Manifold Tangent Classifier (MTC) loss function <ref type="bibr" target="#b28">[29]</ref> . MTC is based on an assumption that points belonging to different classes tend to concentrate near different sub-manifolds, separated by low density areas. Consequently, the output of a classification network is encouraged to be constant near every training point, by penalizing the dot product of the network's gradient with the basis vectors of the plain that is tangent to the data manifold at every training point. This is done by using the loss function</p><formula xml:id="formula_13">˜ J (θ , x, y ) = J(θ , x, y ) + β u ∈B x ( u, ∇ x f (x ) ) 2 , (<label>6</label></formula><formula xml:id="formula_14">)</formula><p>where f ( x ) is the output of the network at x and B x is a basis of the hyperplane that is tangent to the data manifold at x . Following this assumption, suppose that the data exists on a low-dimension smooth manifold ⊂ R d . Let the uncertainty set for training sample x be U = B 2 (x, r) .</p><p>Recall that being a manifold, is locally Euclidean. Therefore, given that r is sufficiently small, we may rewrite Eq. ( <ref type="formula" target="#formula_12">5</ref>) as arg max</p><formula xml:id="formula_15">∈ span (B x ) , 2 ≤r J θ ,y (x ) + ∇ x J θ ,y (x ) , ,</formula><p>The solution to the above equation is</p><formula xml:id="formula_16">x ∝ B x ∇ x J θ ,y (x ) ,</formula><p>where B x is the orthogonal projection matrix onto the subspace span (B x ) and x should have 2 norm equal to r . Thus, this acts as an 2 regularization of the gradient of the loss with respect to the training sample x , projected along the tangent hyperplane span (B x ) , which is analogous to the regularization presented in Eq. ( <ref type="formula" target="#formula_13">6</ref>) . Put another way, small perturbations of x along the tangent hyperplane span (B x ) should cause very small changes to the loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental results</head><p>In this section we apply our proposed training algorithm to ANNs on two popular benchmark datasets: MNIST <ref type="bibr" target="#b16">[17]</ref> and CIFAR-10 <ref type="bibr" target="#b14">[15]</ref> . In each case we compare the robustness of a network that was trained using Algorithm 1 to that of a network trained in a standard fashion. In addition, we also demonstrate how one may generate adversarial examples for a boosting model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiments on MNIST dataset</head><p>As a baseline, we trained a LeNet-like Convolutional Neural Net (CNN) with ReLU units, two convolutional layers (containing 32 and 64 5 × 5 filters), max pooling (3 × 3 and 2 × 2) after every convolutional layer, and two fully connected layers (of sizes 200 and 10) on top. The net was trained using Stochastic Gradient Descent (SGD) with momentum, and had 99.09% accuracy on the MNIST test set. We refer to this network as "the baseline net". We then used the baseline net to generate a collection of adversarial examples, using Eq. ( <ref type="formula" target="#formula_12">5</ref>) , with 1 , 2 and ∞ norm balls.</p><p>As pointed out in Section 3.1 , the adversarial perturbation was computed by a step in the steepest ascent direction w.r.t the corresponding norm. The step w.r.t to ∞ uncertainty set is the same as the fast method in Eq. ( <ref type="formula">1</ref>) ; the step w.r.t to 2 uncertainty set is in the direction of the gradient; the steepest ascent direction w.r.t to 1 uncertainty sets comes down to changing the pixel corresponding to the entry of largest magnitude in the gradient vector. It is interesting to note that using Eq. ( <ref type="formula" target="#formula_12">5</ref>) with 1 uncertainty, it is possible to make a network mis-classify an image by changing only a single pixel. Several such examples are presented in Fig. <ref type="figure" target="#fig_0">1</ref> .</p><p>Altogether we generated a collection of 1203 adversarial examples, on which the baseline network had zero accuracy and which were generated from correctly classified test points. A sample of the adversarial examples is presented in Fig. <ref type="figure" target="#fig_1">2</ref> . We refer to this collection as A mnist .</p><p>We then used Algorithm 1 to re-train the net with the norm ρ being 1 , 2 and ∞ (each norm in a different experiment). We refer to the resulting nets as the robustified nets . To evaluate the performance of the robustified nets, we also compare their performance to the baseline net, where random Gaussian noise is added to the examples during test time, as in Section 2.3 . Table <ref type="table" target="#tab_0">1</ref> summarizes the accuracy of the baseline, smoothed and each robustified net on the mnist test data athe collection A mnist of adversarial examples.</p><p>As can be seen, all three robustified nets perform significantly better than the smoothed net and classify correctly many of the adversarial examples in A mnist , with the ∞ uncertainty giving the best performance. In addition, all three robustified nets improve the accuracy also on the original test data, i.e., the adversarial training acts as a regularizer, which improves the network's generalization ability. This observation is consistent with the ones made by Szegedy et al. <ref type="bibr" target="#b33">[34]</ref> and Goodfellow et al. <ref type="bibr" target="#b11">[12]</ref> . Fig. <ref type="figure" target="#fig_2">3</ref> shows that the robustified nets take about the same number of epochs to converge as the baseline net.</p><p>Next, we checked whether it is harder to generate new adversarial examples from the robustified nets (i.e., the nets that were trained via Algorithm 1 ) than from the baseline net. To do that, we used the FGS method in Eq. ( <ref type="formula">1</ref>) with various values of (which corresponds to the amount of noise added/subtracted to/from each pixel) to generate adversarial examples for the baseline net, and for the robustified nets. For each we measured the classification accuracy of each net with respect to adversarial examples that were generated from its own parameters. The results are shown in Fig. <ref type="figure">4</ref> . Clearly, all three robustified nets are significantly more robust to generation of new adversarial examples. In addition, Fig. <ref type="figure" target="#fig_4">5</ref> shows the test accuracy of the baseline and ∞ robustified nets on FGS examples and Basic Iterative Method (BIM, <ref type="bibr" target="#b15">[16]</ref> , with 10 iterations)    </p><formula xml:id="formula_17">Input: { (x i , y i ) } m i =1</formula><p>Output: robust parameter vector θ initialize θ while θ not converged do for every mini batch mb do for i = 1 , . . . , | mb| do Compute ˆ x i using a single ascent step to approximate</p><formula xml:id="formula_18">x i via equation (5) Set ˜ x i ← x i + ˆ x i end for</formula><p>Update θ using a single descent step based on the perturbed data { ( ˜</p><formula xml:id="formula_19">x i , y i ) } | mb| i =1</formula><p>end for end while versus the normalized 2 norm of the perturbation, defined as</p><formula xml:id="formula_20">x − ˜ x 2 x 2 .</formula><p>As can be seen, the robustified net outperforms the baseline net on both types if examples. Not surprisingly, the robustified net is somewhat less robust against BIM examples comparing to FGM examples. While this result is much expected, we remark that using BIM examples instead of FGM examples during training, and so improve robustness to BIM attacks, still perfectly fits within our proposed robust optimization framework. A detailed comparison, however, is beyond the scope of this manuscript, whose primary goal is to describe adversarial training as a robust optimization procedure.</p><p>To summarize the MNIST experiment, we observed that networks trained with Algorithm 1 (1) have improved performance on original test data, (2) have improved performance of original adversarial examples that were generated w.r.t to the baseline net, and (3) are more robust to generation of new adversarial examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiments on CIFAR-10 dataset</head><p>As a baseline net, we use a variant of the VGG net, publicly available online <ref type="bibr" target="#b41">[42]</ref> , where we disabled the batch-flip module,  which flips half of the images in every batch. This baseline net achieved accuracy of 90.79% on the test set.</p><p>As in Section 4.1 , we constructed adversarial examples for the baseline net, using Eq. ( <ref type="formula" target="#formula_12">5</ref>) , with 1 , 2 and ∞ uncertainty sets. Altogether we constructed 1712 adversarial examples, all of which were mis-classified by the baseline net, and were constructed from correctly classified test images. We denote this set as A cifar10 . A sample from A cifar10 is presented in Fig. <ref type="figure" target="#fig_5">6</ref> .</p><p>We then used Algorithm 1 to re-train the net with 1 , 2 and ∞ uncertainty. Table <ref type="table" target="#tab_1">2</ref> compares the performance of the baseline and robustified nets on the CIFAR-10 test data and the collection A cifar10 of adversarial examples. Consistently with the results of the MNIST experiment, here as well the robustified nets classify correctly many of the adversarial examples in A cifar10 , and also outperform the baseline net on the original test data.</p><p>As in the MNIST experiment, we continued by checking whether it is harder to generate new adversarial examples from the robustified nets than from the baseline net; we used Eq. (1) (i.e., with ∞ uncertainty) and various values of to generate adversarial examples for each of the the baseline and the ro- bustified nets. The results are shown in Fig. <ref type="figure" target="#fig_6">7</ref> . We can see that new adversarial examples are consistently harder to generate for the robustified net, which is consistent with the observation we had in the MNIST experiment. However, the robustified nets seem to be more prone to adversarial attacks, comparing to the ones in the MNIST experiment.</p><p>To summarize the CIFAR-10 experiment, we observed that here as well, the robustified nets improve the performance on original test data, while making the nets more robust to generation of new adversarial examples. As in the MNIST experiment, the ∞ uncer-  tainty yields the best improvement in test accuracy. In addition, the robustified nets require about the same number of parameter updates to converge as the baseline net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Adversarial examples for boosting</head><p>Adversarial examples are not unique to neural nets. In this section, we experiment with adversarial examples for a Gradient Boosting model. We used the XGBoost R toolbox <ref type="bibr" target="#b6">[7]</ref> to train a gradient boosting model, containing 100 trees of depth 6 on MNIST. Table <ref type="table" target="#tab_2">3</ref> shows the accuracy of the model on the test data, as well as on the adversarial examples of the set A mnist , which was constructed in Section 4.1 . As can be seen, the model performs poorly on the adversarial examples, despite the fact that they were generated with respect to a different model (the baseline net in Section 4.1 ). This supports similar observations made by Goodfellow et al. <ref type="bibr" target="#b11">[12]</ref> , Szegedy et al. <ref type="bibr" target="#b33">[34]</ref> that adversarial examples generalize across models.</p><p>In addition, we trained a binary gradient boosting classifier on the '3' and '8' digits from MNIST, containing 200 trees of depth 6. The model had perfect accuracy on the test data. We then applied Eq. ( <ref type="formula" target="#formula_12">5</ref>) with ∞ uncertainty and radius = . 2 , taking the gradient of a logistic loss wrt the model input x , where the gradient was computed numerically. As a in the neural net case, using this procedure we obtained a set of 673 perturbed examples, which were all predicted accurately before the perturbation and were mis-classified after it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related work</head><p>The search for adversarial attacks against deep learning models is an active research area. Like the FGS method, most proposed attacks are based on access (at test time) to the gradient of either the model or the loss wrt the input, and are usually called "white box" attacks <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b35">36]</ref> . Interestingly, Moosavi-Dezfooli et al. <ref type="bibr" target="#b19">[20]</ref> create universal adversarial perturbations, i.e., perturbations that are adversarial for many examples. "Black box" attacks do not require access to the weights or gradients of the targeted model. Chen et al. <ref type="bibr" target="#b5">[6]</ref> propose a black box attack that approximate the gradients using finite differences. Other black box attacks are based on the transferability of adversarial examples, for example <ref type="bibr" target="#b17">[18]</ref> .</p><p>Along adversarial training, other dominant defense approaches that aim to perform model smoothing are proposed in <ref type="bibr" target="#b7">[8]</ref> and distillation <ref type="bibr" target="#b27">[28]</ref> . In the latter work, the network is trained to mimic the performance of another network with relatively smooth decision boundaries. Despite promising experimental results, distillation was shown to be vulnerable to adversarial attacks as well <ref type="bibr" target="#b4">[5]</ref> . Different popular approaches to handle adversarial examples are based on detection or manipulation of adversarial examples at test time, e.g., Metzen et al. <ref type="bibr" target="#b18">[19]</ref> .</p><p>Nguyen et al. <ref type="bibr" target="#b24">[25]</ref> and Fawzi et al. <ref type="bibr" target="#b9">[10]</ref> both nicely demonstrate that classifiers can achieve very high test accuracy without actually learning the true concepts of the classes they predict. Rather, they can base their predictions on discriminative information, which suffices to obtain accurate predictions on test data, however does not reflect learning of the true concept that defines specific classes. As a result, they can consistently fail in recognizing the class' underlying concept <ref type="bibr" target="#b9">[10]</ref> or confidently give wrong predictions on specifically designed examples <ref type="bibr" target="#b24">[25]</ref> . Fawzi et al. <ref type="bibr" target="#b9">[10]</ref> , further provide theoretical arguments, showing that robustness of any classifier to adversarial examples depends on the distinguishability between the classes; they show that sufficiently large distinguishability is a necessary condition for any classifier to be robust to adversarial perturbations. Distinguishability is expressed, for example, by distance between means in case of linear classifiers and and between covariance matrices in the case of quadratic classifiers.</p><p>Acquiring models robustness to noise has got much attention in the machine learning community; see for example, robustification of SVM <ref type="bibr" target="#b42">[43]</ref> and Boosting <ref type="bibr" target="#b10">[11]</ref> . The case we discuss in this manuscript is different, however, as we are interested in robustness to adversarial noise. Teo et al. <ref type="bibr" target="#b34">[35]</ref> address the case of learning under invariances (i.e, transformation that may change the input x but not the label y associated with it). The learning is performed with respect to a worse case invariances, which is related to the approach presented in this manuscript.</p><p>There is a strong connection between Robust Optimization and regularization; in several cases it was shown that solving a regularized problem is equivalent to obtaining a Robust Optimization solution for a non-regularized problem. For example, Xu et al. <ref type="bibr" target="#b38">[39]</ref> show that a solution to a 1 regularized least squares problem min</p><formula xml:id="formula_21">x Ax − b + λ x 1 is also a solution to the Robust Optimization problem min x max A | ∞ , 2 ≤ρ (A + A ) x − b ,</formula><p>where • ∞ , 2 is the ∞ norm of the 2 norms of the columns <ref type="bibr" target="#b2">[3]</ref> . As a result, it was shown in <ref type="bibr" target="#b38">[39]</ref> that sparsity of the solution x opt is a consequence of its robustness. Regularized Support Vector Machines (SVMs) were also shown to have robustness properties: Xu et al. <ref type="bibr" target="#b39">[40]</ref> show that solutions to SVM with norm regularization can be obtained from non-regularized Robust Optimization problems <ref type="bibr" target="#b2">[3]</ref> .</p><p>Ridge Regression can also be viewed as a variant of a robust optimization problem. Namely, it can be shown that min <ref type="bibr" target="#b31">[32]</ref> . In addition, El Ghaoui and Lebret <ref type="bibr" target="#b8">[9]</ref> showed that a solution to the regularized least squares problem min x Ax − b + ρ x 2 2 + 1 can also be obtained from solving a corresponding Robust Optimization problem. Finally, a recent paper <ref type="bibr" target="#b13">[14]</ref> is similar in spirit to our work; a pre-print version of it was uploaded to arXiv in the same week as this manuscript.</p><formula xml:id="formula_22">x max { : F ≤γ } (A + ) x − b 2 is equivalent to min x Ax − b 2 + γ x 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In an attempt to theoretically understand the successful empirical results of adversarial training, we proposed a framework for robust optimization of non-parametric supervised machine learning problems, and neural nets in particular, in which the model prediction is encouraged to be consistent in a small ball with respect to some norm. The implementation is done using minimizationmaximization approach, where the loss is minimized over worstcase examples, rather than on the original data. Our framework explains previously reported empirical results, showing that incorporating adversarial examples during training improves accuracy on test data. In addition, we showed that the loss function proposed by Goodfellow et al. <ref type="bibr" target="#b11">[12]</ref> is in fact a special case of Algorithm 1 , for certain type of uncertainty, thus explaining intuitive interpretations given in that paper. We also showed a connection between Algorithm 1 and the manifold tangent classifier <ref type="bibr" target="#b28">[29]</ref> , showing that it, too, corresponds to a robustification of ANN training.</p><p>Experimental results on MNIST and CIFAR-10 datasets show that Algorithm 1 indeed acts as a regularizer and improves the prediction accuracy also on the original test examples, and are consistent with previous results in <ref type="bibr" target="#b11">[12]</ref> and <ref type="bibr" target="#b33">[34]</ref> . Furthermore, we showed that new adversarial examples are harder to generate for a network that is trained using our proposed approach, comparing to a network that was trained in a standard fashion. As a by-product, we also showed that one may be able to make a neural net misclassify a correctly-classified an image by changing only a single pixel.</p><p>Explaining the regularization effect that adversarial training is in the same vein that from practical experience, most authors knew that dropout <ref type="bibr" target="#b32">[33]</ref> acts as regularization, without a formal rigor justification. Later, a well-cited work by Wager et al. <ref type="bibr" target="#b36">[37]</ref> created a rigorous connection between dropout and weighted ridge regression.</p><p>The scripts that were used for the experiments are available online at https://github.com/ushaham/adversarialPaper . </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Adversarial examples that were generated for the MNIST dataset w.r.t to the baseline net, via Eq. (5) with 1 uncertainty. Top row: original test examples. Bottom row: adversarial examples, where a single pixel (circled) was changed. All original examples presented here were correctly classified by the baseline net, all adversarial examples were mis-classified.</figDesc><graphic url="image-4.png" coords="5,90.35,60.57,425.28,142.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. A sample from the set A mnist of adversarial examples, generated via Eq. (5) . Top row: original test examples (correctly classified by the baseline net). Bottom row: adversarial examples (mis-classified).</figDesc><graphic url="image-5.png" coords="5,90.35,249.39,425.28,143.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. MNIST dataset experiment: test accuracy vs. epoch number.</figDesc><graphic url="image-6.png" coords="5,112.85,432.20,379.68,292.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .Algorithm 1</head><label>41</label><figDesc>Fig. 4. MNIST dataset experiment: comparison between the baseline net (trained in a standard way) and the robustified nets (trained using Algorithm 1 , with 1 , 2 and ∞ uncertainty sets). Adversarial examples were generated via Eq. (1) with respect to each net for various values of and classification accuracy is plotted. As can be seen, the nets that were trained using Algorithm 1 ) are significantly more robust to adversarial examples.</figDesc><graphic url="image-7.png" coords="6,86.53,56.85,412.80,416.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. TODO: right this.</figDesc><graphic url="image-8.png" coords="7,47.85,57.15,510.00,247.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. A sample from the set A cifar10 of adversarial examples. Some (pre-processed) original CIFAR-10 test examples (top row) and their corresponding adversarial examples (bottom row) for the baseline net. All original test examples are classified correctly by the baseline net while the adversarial examples are all mis-classified. The adversarial examples shown here were generated from the baseline net using Eq. (5) with 2 and ∞ uncertainties.</figDesc><graphic url="image-9.png" coords="7,90.35,332.00,425.28,139.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. CIFAR-10 dataset experiment: comparison between the baseline net (trained in a standard way) and the robustified nets (trained using Algorithm 1 , using 1 , 2 and ∞ uncertainty). Adversarial examples were generated via Eq. (1) with respect to each net for various values of and classification accuracy is plotted. The robustified nets are more robust to generation of new adversarial examples.</figDesc><graphic url="image-10.png" coords="8,86.53,56.85,412.80,416.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Uri</head><label></label><figDesc>Shaham is a Ph.D. student in Statistics at Yale University, New Haven, CT. Uri received B.Sc in Mathematics and M.Sc in Industrial Engineering from Ben Gurion university, Israel, in 2007 and 2008, respectively. Prior to his Ph.D studies, Uri worked as a Machine Learning researcher in several technology companies. His research interests include machine learning, deep learning, data analysis and design of algorithms. Yutaro Yamada has completed a B.Sc in Computer Science and Statistics at Yale University, and is currently a research intern with the program of applied Mathematics at Yale. Sahand Negahban is an Assistant Professor with the department of Statistics at Yale University. Prior to that he worked with Prof. Devavrat Shah at MIT as a postdoc and Prof. Martin J. Wainwright at UC Berkeley as a graduate student.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Accuracy of the baseline net, a smoothed net and each of the three robustified nets on the original MNIST test data, and on the set A mnist of adversarial examples that were generated w.r.t to the baseline net.</figDesc><table><row><cell>Net</cell><cell>MNIST test set (%)</cell><cell>A mnist (%)</cell></row><row><cell>Baseline</cell><cell>99.09</cell><cell>0</cell></row><row><cell>Smoothed</cell><cell>99.03</cell><cell>10.7</cell></row><row><cell>Robust 1</cell><cell>99.16</cell><cell>33.83</cell></row><row><cell>Robust 2</cell><cell>99.28</cell><cell>76.55</cell></row><row><cell>Robust ∞</cell><cell>99.33</cell><cell>79.96</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Accuracy of the baseline, 1 , 2 and ∞ robustified nets on the original CIFAR-10 test data, and on the set A cifar10 of adversarial examples that were generated w.r.t to the baseline net.</figDesc><table><row><cell>Net</cell><cell>CIFAR-10 test set (%)</cell><cell>A cifar10 (%)</cell></row><row><cell>Baseline</cell><cell>90.79</cell><cell>0</cell></row><row><cell>Robust 1</cell><cell>91.11</cell><cell>56.31</cell></row><row><cell>Robust 2</cell><cell>91.04</cell><cell>59.92</cell></row><row><cell>Robust ∞</cell><cell>91.36</cell><cell>65.01</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Accuracy of a gradient boosting model on the MNIST test set and the subsets of adversarial examples from the set A mnist , which was constructed in Section 4.1 .</figDesc><table><row><cell>Set</cell><cell>MNIST test set (%)</cell><cell>1 adv. (%)</cell><cell>2 adv. (%)</cell><cell>∞ adv. (%)</cell></row><row><cell>Accuracy</cell><cell>97.02</cell><cell>14.33</cell><cell>10.81</cell><cell>10.69</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Robust Optimization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ben-Tal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">El</forename><surname>Ghaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nemirovski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Princeton University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Oracle-based robust optimization via online learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ben-Tal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oper. Res</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="628" to="638" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Theory and applications of robust optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bertsimas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Caramanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Rev</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="464" to="501" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex Optimization</title>
				<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Symposium on Security and Privacy (SP)</title>
				<meeting>the IEEE Symposium on Security and Privacy (SP)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="39" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models</title>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security</title>
				<meeting>the 10th ACM Workshop on Artificial Intelligence and Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="15" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Xgboost: extreme gradient boosting, R package version 0</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Benesty</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Parseval networks: Improving robustness to adversarial examples</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
				<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="854" to="863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Robust solutions to least-squares problems with uncertain data</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">El</forename><surname>Ghaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lebret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Matrix Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1035" to="1064" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Analysis of classifiers&apos; robustness to adversarial perturbations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="481" to="508" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<idno>arxiv: 0905.2138</idno>
		<title level="m">A more robust boosting algorithm</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>arxiv: 1412.6572</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rigazio</surname></persName>
		</author>
		<idno>arxiv: 1412.5068</idno>
		<title level="m">Towards deep neural network architectures robust to adversarial examples</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning with a strong adversary</title>
		<author>
			<persName><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvari</surname></persName>
		</author>
		<idno>arxiv: 1511.03034</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno>arxiv: 1607.02533</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Delving into transferable adversarial examples and black-box attacks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<idno>arxiv: 1611.02770</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">On detecting adversarial perturbations</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Genewein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bischoff</surname></persName>
		</author>
		<idno>arxiv: 1702.04267</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">S.-M</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<idno>arxiv: 1610.08401</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
		<respStmt>
			<orgName>Universal adversarial perturbations</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deepfool: a simple and accurate method to fool deep neural networks</title>
		<author>
			<persName><forename type="first">S.-M</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2574" to="2582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cutting-set methods for robust convex optimization with pessimizing oracles</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mutapcic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optim. Methods Softw</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="381" to="406" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Introductory Lectures on Convex Optimization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Smooth minimization of non-smooth functions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Prog</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="127" to="152" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<title level="m">Deep neural networks are easily fooled: High confidence predictions for unrecognizable images</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="427" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Transferability in machine learning: from phenomena to black-box attacks using adversarial samples</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07277</idno>
		<imprint>
			<date type="published" when="2016">2016a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The limitations of deep learning in adversarial settings</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">B</forename><surname>Celik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE European Symposium on Security and Privacy (EuroS&amp;P</title>
				<meeting>the IEEE European Symposium on Security and Privacy (EuroS&amp;P</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="372" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distillation as a defense to adversarial perturbations against deep neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Symposium on Security and Privacy (SP</title>
				<meeting>the IEEE Symposium on Security and Privacy (SP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="582" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The manifold tangent classifier</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rifai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2294" to="2302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adversarial diversity and hard positive generation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rozsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Rudd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning representations by back--propagating errors</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognit. Model</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
		<title level="m">Optimization for Machine Learning</title>
				<imprint>
			<publisher>Mit Press</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Convex learning with invariances</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Teo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Globerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1489" to="1496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Ensemble adversarial training: Attacks and defenses</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<idno>arxiv: 1705.07204</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dropout training as adaptive regularization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="351" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<title level="m">Numerical Optimization</title>
				<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Robust regression and lasso</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Caramanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1801" to="1808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Robustness and regularization of support vector machines</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Caramanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1485" to="1510" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Adversarial examples: Attacks and defenses for deep learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno>arxiv: 1712.07107</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<ptr target="http://torch.ch/blog/2015/07/30/cifar.html" />
		<title level="m">45% on cifar-10 in torch</title>
				<imprint>
			<date type="published" when="2015-11">2015. November-2015</date>
			<biblScope unit="volume">92</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Support vector classification with input data uncertainty</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="161" to="169" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
