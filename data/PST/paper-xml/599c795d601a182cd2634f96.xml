<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep adaptive feature emb e dding with local sample distributions for person re-identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-08-31">31 August 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lin</forename><surname>Wu</surname></persName>
							<email>lin.wu@uq.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Queensland</orgName>
								<address>
									<postCode>4072</postCode>
									<region>Queensland</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Wang</surname></persName>
							<email>wangy@cse.unsw.edu.au</email>
							<affiliation key="aff1">
								<orgName type="institution">The University of New South Wales</orgName>
								<address>
									<postCode>2052</postCode>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junbin</forename><surname>Gao</surname></persName>
							<email>junbin.gao@sydney.edu.au</email>
							<affiliation key="aff2">
								<orgName type="department">Discipline of Business Analytics</orgName>
								<orgName type="institution" key="instit1">The University of Sydney Business School</orgName>
								<orgName type="institution" key="instit2">The University of Sydney</orgName>
								<address>
									<postCode>2006</postCode>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xue</forename><surname>Li</surname></persName>
							<email>xueli@itee.uq.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Queensland</orgName>
								<address>
									<postCode>4072</postCode>
									<region>Queensland</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep adaptive feature emb e dding with local sample distributions for person re-identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-08-31">31 August 2017</date>
						</imprint>
					</monogr>
					<idno type="MD5">9CC88D7C0FF4E4B3A25659D1C812B5E0</idno>
					<idno type="DOI">10.1016/j.patcog.2017.08.029</idno>
					<note type="submission">Received 22 April 2017 Revised 25 August 2017 Accepted 27 August 2017</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Deep feature embedding Person re-identification Local positive mining</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Person re-identification (re-id) aims to match pedestrians observed by disjoint camera views. It attracts increasing attention in computer vision due to its importance to surveillance systems. To combat the major challenge of cross-view visual variations, deep embedding approaches are proposed by learning a compact feature space from images such that the Euclidean distances correspond to their cross-view similarity metric. However, the global Euclidean distance cannot faithfully characterize the ideal similarity in a complex visual feature space because features of pedestrian images exhibit unknown distributions due to large variations in poses, illumination and occlusion. Moreover, intra-personal training samples within a local range which are robust to guide deep embedding against uncontrolled variations cannot be captured by a global Euclidean distance. In this paper, we study the problem of person re-id by proposing a novel sampling to mine suitable positives ( i.e., intra-class) within a local range to improve the deep embedding in the context of large intra-class variations. Our method is capable of learning a deep similarity metric adaptive to local sample structure by minimizing each sample's local distances while propagating through the relationship between samples to attain the whole intra-class minimization. To this end, a novel objective function is proposed to jointly optimize similarity metric learning, local positive mining and robust deep feature embedding. This attains local discriminations by selecting local-ranged positive samples, and the learned features are robust to dramatic intra-class variations. Experiments on benchmarks show state-of-the-art results achieved by our method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The re-identification (re-id) of individuals across spatially disjoint camera views has attracted tremendous attention in computer vision community due to its practice into security and surveillance systems. Despite years of great effort s, person re-id still remains a challenging task due to its large variations in terms of view points, illuminations and different poses (See examples in Fig. <ref type="figure" target="#fig_0">1 (a)</ref>). Existing approaches to person re-id can be summarized into two categories. The first category focuses on developing robust descriptors to describe a person's appearance against challenging factors (lighting, pose, etc) while preserving identity information <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref> . Low-level features such as color <ref type="bibr" target="#b3">[4]</ref> , texture (Local Binary Patterns <ref type="bibr" target="#b0">[1]</ref> , and Gabor <ref type="bibr" target="#b2">[3]</ref> ) are commonly used for this pur-  <ref type="bibr" target="#b10">[11]</ref> . Each column shows two images of the same identity observed by two disjoint camera views. (b) Highly-curved manifolds of 3 identities. Positive samples in a local range (green lines) should be selected to guide deep feature embedding while those in large distance (yellow line with cross) should not be sampled to respect the manifold structure. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) pairwise (by contrastive loss <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref> ), in triplets <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> , or even high-order relationships <ref type="bibr" target="#b17">[18]</ref> . Among these methods, hard sample mining is crucial to ensure the quality and the learning efficiency, due to the fact that there are many more easy examples than those meaningful hard examples. Thus, they usually choose hard samples to compute the convenient Euclidean distance in the embedding space. However, these deep embedding methods suffer from inherent limitations: First, they adopt a global Euclidean distance metric to evaluate the hard samples whereas recent manifold learning in person re-id <ref type="bibr" target="#b18">[19]</ref> suggests that pedestrian samples are distributed as highly-curved manifolds. Euclidean distance can only be adopted in local range to approximate the geodesic distance via graphical relationship between samples (as illustrated in Fig. <ref type="figure" target="#fig_0">1 (b)</ref>). Second, these methods are conditioned on individual samples in term of pairs/triplets to categorize the inputs as depicting either the same or different subjects. Such mapping to a scalar prediction of similarity score based on person identities would make the optimization on CNN parameters over-fitting because the supervision binary similarity labels (0 for dissimilar and 1 for similar) tend to push the scores independently. In practice, the similarity scores of positive and negative pairs live on a 1-D manifold following the distribution on heterogeneous data <ref type="bibr" target="#b19">[20]</ref> . Finally, when training the CNN with contrastive or triplet loss for embedding, existing methods use the Euclidean distance indiscriminately with all the positive samples. Nonetheless, we observe that selecting positive samples within local ranges (pairs in green lines in Fig. <ref type="figure" target="#fig_0">1 (b)</ref>) is critical for training whilst enforce training with the positive samples of long distance may distort the manifold structure (the yellow line with red cross in Fig. <ref type="figure" target="#fig_0">1 (b)</ref>). Moreover, objective functions defined on triplet loss involve sampling on divergent triplets, which is not necessarily consistent, and thus impedes the convergence rate and training efficiency.</p><p>Our Approach. Mitigating the aforementioned issues, in this paper we propose a principled approach to learn a local-adaptive similarity metric, which will be exploited to search for suitable positive samples in a local neighborhood to facilitate a more effective yet efficient deep embedding learning. The key challenge lies in the design of robust feature extraction and the loss function that can jointly consider 1) similarity metric learning; 2) suitable positive sample selection; and 3) deep embedding learning. Existing deep embedding studies <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref> only consider the two later objectives but not jointly with the first important aspect. To this end, we propose a principled approach to train a deep network that transforms the input data into a deep feature space where the local data distribution structure within classes can be captured. We formulate the feature extractor as stacked convolutional Restricted Boltzmann Machines (CRBMs) <ref type="bibr" target="#b22">[23]</ref> to initialize the parameters that define the mapping from input images to their representation space. We remark that CNN has generic parameterization while in person re-id case, body parts exhibit different visual modalities due to the combinations of view points, poses, and photometric settings. Thus, a single/generic CNN filter cannot capture the inter-camera variations while some fine-grained information such as "texture in clothes" and "bags" are very helpful in reducing intra-personal variations. As such, CRBMs serve as hierarchical feature model to faithfully describe pedestrian samples containing dramatic variations. We formulate the training of CRBMs adaptively to search the suitable positive samples within local range so that it learns locally adaptive metric (instead of global Euclidean distance). Furthermore, to improve training efficiency, we employ variance reduced Stochastic Gradient Descent (SGD) <ref type="bibr" target="#b23">[24]</ref> to share and reuse past stochastic gradients across data samples by exploiting their neighborhood structure. As shown in Fig. <ref type="figure" target="#fig_1">2</ref> , the proposed metric yields similarity scores in mini-batch, from which positive samples constituting a hard quadruplet are mined and used to optimize the feature embedding space. The similarity metric learning and embedding learning in the associated CRBMs are jointly optimized via a novel large-margin criterion.</p><p>Contributions. The main contributions of our work are four-fold: (1) An improved deep embedding approach is presented to construct a representation amenable to similarity metric computation in person re-identification by jointly optimizing robust feature embedding, local adaptive similarity learning, and suitable positive mining. <ref type="bibr" target="#b1">(2)</ref> The proposed method enhances the quality of learned representations and the training efficiency by accessing Euclidean distance of samples in local range w.r.t highly-curved structure. This allows adaptive similarity access in local range and achieves minimization of intra-class variations by local-ranged positive sample mining. <ref type="bibr" target="#b2">(3)</ref> We provide alternative to CNN embedding by formulating a stacked CRBMs into local sample structure in deep feature space, and thus enables local adaptive similarity metric learning as well as plausible positive mining. (4) Our method achieves state-of-the-art results on four benchmark datasets: VIPeR <ref type="bibr" target="#b24">[25]</ref> , CUHK03 <ref type="bibr" target="#b10">[11]</ref> , CUHK01 <ref type="bibr" target="#b25">[26]</ref> , and Market-1501 <ref type="bibr" target="#b26">[27]</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Metric learning in person re-identification</head><p>Metric learning algorithms have been extensively applied into person re-identification to learning discriminative distance metrics or subspaces for matching persons across views <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref> . They essentially perform a two-stage pipeline where hand-crafted features are extracted for each image, and then a Mahalanobis form metric is learned. This corresponds to a </p><formula xml:id="formula_0">l via ˆ k = arg max ( ˆ i ,k ) ∈ N S ˆ i ,k and ˆ l = arg min ( ˆ i ,l) ∈ P (S ˆ i ,l &gt; S ˆ i ,k ) , respectively. The selected hard quadruplet [ ˆ i , ˆ j , ˆ l , ˆ k ] is put through the network to calculate their similarities S ˆ i , ˆ j , S ˆ i , ˆ k , S ˆ i , ˆ</formula><p>l which are optimized under large margin criterion so as to jointly learn similarity metric and adapt their embeddings  <ref type="bibr" target="#b6">[7]</ref> to learn a subspace to reduce the dimensionality of the extracted high-dimensional features under which the Fisher discriminant criterion is met <ref type="bibr" target="#b31">[32]</ref> . These methods have a common drawback in terms of the separation on feature extraction and metric learning, making their performance limited by the representation power of low-level features. Moreover, they aim to optimize a linear transformation with a limited number of parameters, which cannot model high-order correlations between original data dimensions.</p><formula xml:id="formula_1">[ f (x ˆ i ) , f (x ˆ j ) , f (x ˆ k ) , f (x ˆ l )]</formula><p>To jointly learn representations and similarity metric for pedestrian samples, deep embedding approaches are developed to allow the interaction between feature extraction and metric learning <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b32">33]</ref> . Euclidean distance is the simplest similarity metric, and widely used by current deep embedding methods where Euclidean feature distances directly correspond to the similarity values. Similarities can be encoded in pairwise with a contrastive loss <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref> , or a flexible triplet loss <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b32">33]</ref> . Alternative to Euclidean metric is parametric Mahalanobis metric, representative works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> minimize the Mahalanobis distances between positive sample pairs while maximizing the distances between negative pairs. They directly optimize the Mahalanobis metric for nearest neighbor classification via large margin nearest neighbor <ref type="bibr" target="#b33">[34]</ref> . However, Mahalanobis metric learning is also a global linear transformation of the input space that precedes k -NN classification using Euclidean distance. Thus, the common drawbacks of Mahalanobis and Euclidean metric is that they are both global and unable to reflect the heterogeneous feature distribution.</p><p>As data samples reside on highly-curved manifold, this indicates the similarity notion should be defined on local Euclidean distance which can be propagated to approximate the whole class structure by enforcing reasonable proximity relationship between samples <ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref> . In this paper, we present an approach to train a deep neural network to learn deep embeddings from the input space to a discriminative feature space under which the similarity is defined as a function of local range structure within a subject such that the intra-personal variations can be reduced substantially. More recent works in person re-id <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref> suggest that transfer learning <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref> can be utilized to learn adaptive viewinvariant features of a person's appearance in the context of crossdataset person matching. For instance, Peng et al. <ref type="bibr" target="#b38">[39]</ref> propose an approach based on dictionary learning which assumes that a person's appearance can be represented as a linear combination of latent factors each corresponding to a dictionary atom. They formulate the problem into a multi-task learning by further assuming that some of the atoms are view/dataset-independent and shared across different datasets/tasks, whilst others are unique to each dataset. We remark that these methods focus on the aspect of feature transferability while in this paper we aim to learn a deep similarity metric adaptive to local sample structures by improving the deep feature embedding in the context of intra-class variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Deep embeddings with hard sample mining in person re-identification</head><p>Person re-id is a challenging task in terms of the large intrapersonal variations ( e.g., viewpoints, poses, occlusion) present in typical surveillance footage. Thus, it requires a fine granularity model to discriminate identities that resemble each other with subtle difference. Deep learning has shown great success in a variety of tasks in image classification <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref> , and frequency domain <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref> . Inspired by these high-capacity models in deep learning, some deep embedding models have been developed for person re-id to learn representations against visual variations <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b46">47]</ref> . They commonly learn a feature embedding from images using a deep CNN, and optimize an embedding objective in an Euclidean distance, which should preserve their semantic relationship encoded by pairwise (contrastive loss <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21]</ref> ), in triplets <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b32">33]</ref> , or even higher order relationships ( e.g., a structured loss <ref type="bibr" target="#b17">[18]</ref> ). An important component of deep embedding approaches is hard sample mining, which is crucial to ensure the learning quality and the efficiency since there would be many easy examples than those meaningful hard examples. In existing deep embedding pipelines of person re-id, hard sample mining is commonly performed to augment a training set progressively with false positive examples with the model learned so far. However, they select false positive/hard negative examples randomly, in which divergent pairs/triplets are not necessarily consistent, and hinders the convergence rate. Moreover, the similarity is defined in a global Euclidean distance, which cannot capture the complex feature structure. To this end, we are motivated to develop a deep neural network to learning a deep transformation from input space to a representation space in which neighborhood structure w.r.t class distributions is adaptively captured. Meanwhile, we characterize the similarity adaptively as function of positive samples in local-ranged feature space, and pursue local large margin as opposed to global.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Deep feature embedding with local adaptive similarities</head><p>In this section, we present our approach to learn adaptive, deep embeddings that transform the input data with real-values into a representation space such that the intra-class variations can be addressed by selecting suitable positive samples in a local range.</p><p>Formally, let X = { x i , y i } be a pedestrian imagery dataset, where y i is the class/identity label of image x i . Our goal is to jointly learn a deep feature embedding f ( x ) from image x into a feature space R d , and a similarity metric S( f (x i ) , f (x j )) ∈ R 1 , such that the metric can robustly select positive samples adaptive to local range to learn a discriminative feature embedding. Ultimately, the learned features ( f ( x i ), f ( x j )) from the set of positive pairs P = { (i, j) | y i = y j } should be close to each other with a large similarity value S i, j , whilst those from the set of negative pairs N = { (i, j) | y i = y j } should be far away with a small similarity score. Importantly, this relationship cannot be reflected in a global Euclidean metric S i, j wherein data samples reside on highly-curved manifolds and Euclidean distance is limited in the local range. To adapt S i, j into local latent structure of feature embeddings, we propose to select suitable positive samples within a local neighborhood to guide deep embedding adaptive to manifold structure.</p><p>The overall architecture is shown in Fig. <ref type="figure" target="#fig_1">2</ref> . Given a mini-batch containing pairs of images represented in the form of ( f W ( x i ), f W ( x j )), we compute their similarity scores S i, j to select one hard quadruplet from the local set of positive pairs P ∈ P, and negative pairs N ∈ N in the batch. Then, each sample in the mined hard quadruplet is fed into four identical Convolutional Restricted Boltzmann Machines (CRBMs) with shared parameters W to extract ddimensional features. To optimize the parameters W , a discriminative local loss is applied to similarity scores based on a large margin criterion ( Section 3.1 ). Note that we initially use CRBMs <ref type="bibr" target="#b22">[23]</ref> to produce features that discovers the structure of complex visual appearance on pedestrian images ( Section 3.2 ). The features from CRBMs can be used to compute similarity scores for the minibatch samples during a particular forward pass.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Joint similarity learning and local positive sample mining</head><p>To enforce similarity into learned representations in which intra-class variations among pedestrian samples are minimized by enforcing reasonable proximity relationship, we are motivated to learn a similarity metric to adapt into local structure where each example is designated only a few number of target neighbors of the same class <ref type="bibr" target="#b33">[34]</ref> . This principle suits to person re-identification datasets where each identity is associated with only a small number of images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Adapting to local sample distributions</head><p>Given a feature pair ( f W ( x i ), f W ( x j )) extracted from images x i and x j by an embedding function f W ( • ) parameterized by W , we aim to learn a similarity score y i, j = 1 if ( i, j ) ∈ P , and y i, j = 0 if ( i, j ) ∈ N . Thus, we seek the optimal similarity metric S * ( • , • ), and feature embedding parameters</p><formula xml:id="formula_2">W * : [ S * (•, •) , W * ] = arg min S(•, •) ,W 1 | P ∪ N| (i, j) ∈ P∪ N L (S( f W (x i ) , f W (x j )) , y i, j ) ,<label>(1)</label></formula><p>where L (•) is a loss function to be defined later. We will omit the parameter notation W from f W ( • ) in the following for brevity. The standard Euclidean or Mahalanobis metric is defined based on the feature difference vector e = | f (x i )f (x j ) | or its linear transformation. However, these metrics are demonstrated to be suboptimal in a heterogeneous embedding space despite of the CRBM's capability of hierarchical feature extraction. By contrast, similarity metric learning should characterize similarity adaptively into local sample structure in the deep feature space. This knowledge can be utilized to reduce large intra-class visual variations in the resulting representation space. Thus, inspired by Huang et al. <ref type="bibr" target="#b19">[20]</ref> , apart from the feature difference vector e , we additionally introduce the feature mean vector u = ( f (x i ) + f (x j )) / 2 to leverage absolute feature position to adapt the metric into local range.</p><p>Formally, as suggested by <ref type="bibr" target="#b19">[20]</ref> , the features f ( x i ) and f ( x j ) are first normalized onto the unit hypersphere, that is,</p><formula xml:id="formula_3">|| f (x ) || 2 = 1 ,</formula><p>in order to maintain feature compatibility in the computation of their relative and absolute positions encoded by e and u , respectively. Thereafter, a sequence of nonlinearities of a fully connected layer, an element-wise ReLU function max (0, x ), and a second 2 -</p><formula xml:id="formula_4">normalization r(x ) = x || x || 2</formula><p>are applied on e and u , respectively. This process can be formulated as:</p><formula xml:id="formula_5">e = | f (x i ) -f (x j ) | , u = f (x i ) + f (x j ) / 2 ; ē = r ( max (0 , W e e + b e ) ) , ū = r ( max (0 , W u u + b u ) ) ;<label>(2)</label></formula><p>where the parameters (</p><formula xml:id="formula_6">W e ∈ R d×d , b e ∈ R d ) and ( W u ∈ R d×d , b u ∈ R d ) are not shared.</formula><p>Then, the vectors ē and ū are concatenated and fed into a fully connected layer, parameterized by</p><formula xml:id="formula_7">W c ∈ R 2 d×d , b c ∈ R d</formula><p>, and the ReLU function, to map to a final similarity score</p><formula xml:id="formula_8">S i, j = S( f (x i ) , f (x j )) ∈ R 1 , parameterized by W s ∈ R d×1 , b s ∈ R 1 .</formula><p>This can be defined as </p><formula xml:id="formula_9">S i, j = W s c + b s , c = max (0 , W c [ ē ; ū ] T + b c ) . Thus,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Local positive sample mining</head><p>To optimize all these parameters, we need to choose an appropriate loss function L (•) , in which the pre-trained multi-layer network initialized by CRBMs are optimized to learn deep embeddings with adaptive local sample structure. One possible solution is to cast the problem as a binary classification problem as some deep embedding methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref> . However, the binary similarity labels y i, j ∈ {0, 1} tend to independently push the scores towards two single points. While some study <ref type="bibr" target="#b19">[20]</ref> has shown that the similarity scores of positive and negative pairs live on a 1-D manifold following some irregular distribution on data space. This motivates us to design a loss function that is able to separate the similarity distribution in a local range along its manifold. One option is to impose the Fisher criterion <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b47">48]</ref> on the similarity scores, which is to maximize the ratio between the interclass and intraclass scatters of scores. However, the optimality of Fisher criteria relies on the assumption that the data of each is of Gaussian distribution, which is not satisfied in our case.</p><p>To this end, we propose a loss function that approximately maximizes the margin between the positive and negative similarity distribution in a local range to reduce the intra-class variations. Specifically, we select a hard quadruplet from each random minibatch during each forward pass. To constitute the hard quadruplet, we first select the most dissimilar positive pair in the batch via ( ˆ i , ˆ j ) = arg min (i, j) ∈ P S i, j , which indicates their similarity score is most likely to cross the "safe margin ", and move towards the negative similarity distribution in the local range (see the illustration in Fig. <ref type="figure" target="#fig_3">3</ref> ). Then, we construct the hard quadruplet by choosing the hard negative ˆ k and hard positive ˆ l w.r.t each anchor sample</p><formula xml:id="formula_10">ˆ i , via ˆ k = arg max ( ˆ i ,k ) ∈ N S ˆ i ,k and ˆ l = arg min ( ˆ i ,l) ∈ P (S ˆ i ,l &gt; S ˆ i ,k ) , respec-</formula><p>tively. Mining the hard negative ˆ k w.r.t the anchor image ˆ i is to ensure the correct relative distances between positive and negative pairs, and thus the sample ˆ k is push away from the safe margin. On the other hand, we choose the positive samples that have larger similarity scores than the hardest negative, and then mine the hardest one amongst these chosen positives as adaptive local positive samples. This is to preserve the local manifold structure by pushing the positive sample towards the safe margin.</p><p>With this hard quadruplet</p><formula xml:id="formula_11">[ ˆ i , ˆ j , ˆ l , ˆ k ] ,</formula><p>we can define the suitable positive samples adaptively within each subject, meanwhile their hard negatives are also involved in case the positive ones are too easy or too hard to be mined. Finally, we design the objective function by discriminating the local similarity distributions under the </p><formula xml:id="formula_12">min L = ˆ i , ˆ j ε ˆ i , ˆ j + η ˆ i , ˆ j , s.t. : ∀ ( ˆ i , ˆ j ) , max 0 , α 1 + S ˆ i , ˆ k -S ˆ i , ˆ j ≤ ε ˆ i , ˆ j , max 0 , α 2 + S ˆ i , ˆ k -S ˆ i , ˆ l ≤ η ˆ i , ˆ j , ( ˆ i , ˆ j ) = arg min (i, j) ∈ P S i, j , ˆ k = arg max ( ˆ i ,k ) ∈ N S ˆ i ,k , ˆ l = arg min ( ˆ i ,l) ∈ P S ˆ i ,l , ε ˆ i , ˆ j ≥ 0 , η ˆ i , ˆ j ≥ 0 ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_13">ε ˆ i , ˆ j , η ˆ i , ˆ</formula><p>j are the slack variables, α 1 and α 2 are the enforced margins. We adopt the different margin thresholds to determine the balance of two terms in our loss function. Specifically, we require that the margin between the pairs w.r.t the same probe</p><formula xml:id="formula_14">( S ˆ i , ˆ k -S ˆ i , ˆ j</formula><p>) should be large enough to enlarge the inter-class variations. And the term of local positive mining as opposed to the safe margin could hold smaller margin to preserve a relatively weak constraint on local manifold structure. Thus, α 1 is set to be larger than α 2 . In this sense, the proposed quadruplet loss not only maintains the correct relative distance between positive and negative pairs but also preserves the local manifold structure by mining positive samples in a local range. The procedure of mining the hard quadruplet is illustrated in Fig. <ref type="figure" target="#fig_3">3</ref> and summarized in Algorithm 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1:</head><p>Local positive sample mining.</p><formula xml:id="formula_15">Input : A mini-batch X = { x i , y i } .</formula><p>Output : One hard quadruplet samples.</p><p>1 Input the images into the CRBMs to obtain their features f (x i ) , and compute their similarity scores S i, j = S( f (x i ) , f (x j )) ; 2 Mine the most dissimilar positive pair in the mini-batch:</p><formula xml:id="formula_16">( ˆ i , ˆ j ) = arg min (i, j) ∈ P S i, j ; 3 Choose the hard negative satisfying ˆ k = arg max ( ˆ i ,k ) ∈ N S ˆ i ,k ; 4 Choose the hard positive satisfying ˆ l = arg min ( ˆ i ,l) ∈ P (S ˆ i ,l &gt; S ˆ i ,k ) ; 5 return One hard quadruplet [ ˆ i , ˆ j , ˆ l , ˆ k ] ;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature extraction using convolutional RBMs</head><p>In this paper, we employ CRBM <ref type="bibr" target="#b22">[23]</ref> to extract features from each sample. The weights in CRBM between hidden layers and visible layers (corresponding to input data, such as image pixels) are shared among all locations in an image (see Fig. <ref type="figure" target="#fig_4">4 (a)</ref>). Capturing 2-D structure of images in this way allows weights that detect a given feature to be replicated across locations, and thus redundancy can be reduced to make it scalable to realistic fullsized images. The basic CRBM consists of two layers: an input layer V and a hidden layer H with real-valued visible input nodes v and binary-valued hidden nodes v . The visible input nodes can be viewed as intensity values in the image of N V × N V pixels, and the hidden nodes are manipulated in 2-D configurations, that it,</p><formula xml:id="formula_17">i.e., v ∈ R N V ×N V and h ∈ { 0 , 1 } N H ×N H . As illustrated in Fig. 4 (b), a</formula><p>CRBM block consists of three sets of parameters: (1) K convolution filter weights between a hidden node and a subset of visible nodes where each filter perceives</p><formula xml:id="formula_18">N W × N W pixels ( i.e., W k ∈ R N W ×N W , k = 1 , . . . , K); (2) hidden biases b k ∈ R that are shared among hidden nodes; (3) visible bias c ∈ R that is shared among visible nodes.</formula><p>After convolutional detection (filtering), probabilistic max-pooling <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b48">49]</ref> is used to incorporate local translation invariance. Specifically, the probabilistic max-pooling CRBM with real-valued visible inputs can be defined as:</p><formula xml:id="formula_19">P (v , h ) = 1 Z exp (-E(v , h )) ; E(v , h ) = - K k =1 N H i, j=1 N W r,s =1 h k i j W k rs v i + r-1 , j+ s -1 + N V i, j=1 1 2 v 2 i j - K k =1 b k N H i, j=1 h k i j -c N V i, j = 1 v i j ; s.t. (i, j) ∈ B α h j i j ≤ 1 , ∀ k, α. (<label>4</label></formula><formula xml:id="formula_20">)</formula><p>where B α refers to a C × C block of locally neighboring ( e.g., 2 × 2) hidden units h k i j that are pooled to a pooling node p k α . In what follows, we discuss sampling the detection layer H and the pooling layer P given the visible layer V . Filter k receives the following bottom-up signals from layer V :</p><formula xml:id="formula_21">I(h k i j ) = b k + ( ˜ W k * v ) i j</formula><p>, where * denotes convolution, ˜ W denotes flipping the original filter W in both upside-down and left-right directions. With the energy function in (4) , suppose h k i j is a hidden unit contained in block α ( i.e., ( i, j ) ∈ B α ) the conditional probabilities can be computed as follows:</p><formula xml:id="formula_22">P (v i j = 1 | h ) = N k W k * h k i j + c, 1 ; P (h k i j = 1 | v ) = exp I(h k i j ) 1 + (i , j ) ∈ B α exp (I(h k i j )) (<label>5</label></formula><formula xml:id="formula_23">)</formula><p>where N (•) is a normal distribution. The pooling node p k α is a stochastic random variable that is defined as thus the marginal posterior can be written as a soft-max function:</p><formula xml:id="formula_24">p k α = (i, j) ∈ B α h k i j , and</formula><formula xml:id="formula_25">P (p k α = 1 | v ) = (i , j ) ∈ B α exp (I(h k i j</formula><p>))</p><formula xml:id="formula_26">1+ (i , j ) ∈ B α exp (I(h k i j</formula><p>))</p><p>.</p><p>We train the RBM parameters by using the contrastive divergence optimization which allows to estimate an approximate gradient efficiently <ref type="bibr" target="#b49">[50]</ref> . Since the model is highly over-complete, an appropriate regularization is required to prevent the model from learning trivial feature representations. To this end, a sparsity regularization <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b48">49]</ref> is added into the log-likelihood objective to encourage each hidden unit group to have a mean activation close to a small constant. To increase its expressive power, CRBMs are stacked to form convolutional deep belief network (CDBN) <ref type="bibr" target="#b48">[49]</ref> , a hierarchical generative model for full-sized images. CDBN consists of several max-pooling CRBMs stacked on top of one another, and the network defines an energy function by summing together the energy functions for all of the individual pairs of layers ( i.e., E = l=1 E(h l , h l+1 ) where h 0 = v ). Training CDBN is accomplished with greedy, layer-wise procedure, that is, once a given layer is trained, its weights are frozen, and its activations are used as inputs to the next layer. Once training is completed, hierarchical representations for each image f W ( x ) ( b is omitted) can be generated with trainable parameters</p><formula xml:id="formula_27">W = { W 1 , W 2 , W 3 } , b = { b 1 , b 2 , b 3 } ,</formula><p>namely feature embedding function f W ( • ). Please refer to Appendix for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Improvement on training efficiency with variance reduced SGD</head><p>To improve training efficiency, we adopt variance reduced SGD <ref type="bibr" target="#b23">[24]</ref> to reuse past gradients across data samples with respect to their neighborhood structure. Generally, given a convex loss L , and a μ-strongly convex regularizer , the training of a network aims at finding a parameter vector w which minimizes the empirical expectation:</p><formula xml:id="formula_28">w * = arg min w f ( w ) , f ( w ) = 1 N N n f n ( w ) ; f n ( w ) : = L ( w , (x n , y n )) + ( w ) . (<label>6</label></formula><formula xml:id="formula_29">)</formula><p>Steepest descent can find the minimizer w * , whereas requires repeated computations of full gradients f ( w ), which is prohibitive for massive datasets. Stochastic Gradient Descent (SGD) is a widely used alternative in the context of large-scale learning, which updates only involving f n ( w ) for an index n chosen uniformly at random, offering an unbiased gradient estimate. However, studies have shown that SGD has property of slow convergence due to its rate of O (1/ t ) ( t is number of epochs), making training inefficient. For the optimization problem in <ref type="bibr" target="#b5">(6)</ref> , a family of SGD algorithms commonly generates an iterate sequence w t with updates as</p><formula xml:id="formula_30">w + = w -γ g n ( w ) , g n ( w ) = f n ( w ) -ε n , ε n := ε n -ε , (<label>7</label></formula><formula xml:id="formula_31">)</formula><p>where ε := 1 N N n ε n . w is the current and w + is the new parameter vector, and γ is the step size. ε n are variance correction terms such at</p><formula xml:id="formula_32">E [ ε n ]= 0, which guarantees unbiasedness E [ g n ( w )] = f ( w ) .</formula><p>In this paper, we employ a neighborhood based gradient memorization method, N-SAGA <ref type="bibr" target="#b23">[24]</ref> , to update ε n from selected neighborhood data points w.r.t n . The q-memorization algorithms evolve iterates w according to <ref type="bibr" target="#b6">(7)</ref> and select in each iteration a random index set I of memory locations to update via:</p><formula xml:id="formula_33">ε + j := f j ( w ) , i f j ∈ I ε j , otherwise<label>(8)</label></formula><p>s.t. any j has the same probability of q/ N of being updated: ∀ j, j∈ I P { I} = q/ N . N-SAGA makes use of a neighborhood system N n = { 1 , . . . , N } and selects neighborhoods uniformly, i.e., P { N n } = 1 N . Using neighborhoods for sharing gradients between close-by data points can avoid an increase in gradient computations but at the expense of an approximation bias. To this end, we employ two types of quantities. First, the gradient memory ε n is defined by using <ref type="bibr" target="#b7">(8)</ref> , and the shared gradient memory state β n is used in a modified update rule in <ref type="bibr" target="#b6">(7)</ref> :</p><formula xml:id="formula_34">w + = w -γ ( f n ( w ) -β n + β ) . Assume</formula><p>an index j is selected for the weight update, then we generalize (8) as</p><formula xml:id="formula_35">β + j := f n ( w ) , i f j ∈ N n β j , otherwise ; β := 1 N N n β n , βn := β n -β.<label>(9)</label></formula><p>In <ref type="bibr" target="#b23">[24]</ref> , a proof is given to show that the error can be controlled in a small value: || ε n -β n || 2 &lt; n . Euclidean distances can be used as the metric for defining neighborhoods while standard approximation methods for finding nearest neighbors can also be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Relation to neighborhood models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Relation to triplet relationship based embedding</head><p>Existing deep metric learning approaches to person re-id stem from contrastive loss <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b46">47]</ref> and triplet loss <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref> . They have the same outline, and for simplicity we use triplet loss as illustration. In a typical triplet training framework, triplet images consisting of a seed example, a positive and negative example to the seed are fed into three network models with shared parameter set to compute their representations f ( x i ), f ( x j ), and f ( x k ) ( i = 1 , . . . , M). Triplet loss demands the distance between mismatched pairs and matched pairs be larger than a pre-defined margin α ∈ R :</p><formula xml:id="formula_36">L triplet (W ) = 1 M M i =1 || f (x i ) -f (x k ) || 2 2 -|| f (x i ) -f (x j ) || 2 2 + α + , (<label>10</label></formula><formula xml:id="formula_37">)</formula><p>where {•} + denote the hinge function, and W the parameter set of the deep embeddings into the representation space. However, training the deep network with contrastive or triplet loss for embedding uses the global Euclidean distance, which cannot faithfully characterize the true feature similarity in a complex visual feature space. Moreover, penalizing individual pairs or </p><formula xml:id="formula_38">= || f (x ˆ i ) -f (x ˆ k ) || 2 2 -|| f (x ˆ i ) -f (x ˆ l ) || 2 2 + α + || f (x ˆ i ) -f (x ˆ k ) || 2 2 - || f (x ˆ i ) -f (x ˆ j ) || 2 2 + α + = 2 || f (x ˆ i ) -f (x ˆ k ) || 2 2 -|| f (x ˆ i ) -f (x ˆ l ) || 2 2 - || f (x ˆ i ) -f (x ˆ j ) || 2 2 + 2 α + .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Relation to neighborhood component analysis</head><p>Neighborhood component analysis (NCA) <ref type="bibr" target="#b50">[51]</ref> can be designed to have an objective to maximize the expected number of correctly classified data samples on the labeled training data. They essentially learn a linear/non-linear transformation that transforms input data into a lower-dimensional space to make the K -nearest neighbor perform well. The NCA objective is defined as</p><formula xml:id="formula_39">L NCA = 1 N N n =1 -log n : C ( f (x n ))= C ( f (x n )) e -|| f (x n ) -f (x n ) || 2 2 N n =1 e -|| f (x n ) -f (x n ) || 2 2 , (<label>11</label></formula><formula xml:id="formula_40">)</formula><p>where C ( x ) is the class membership of x . NCA attempts to model the distribution of data samples by preserving its K -nearest neighbors with respect to each data sample. However, this formulation does not address the concern on local range feature structure. Even though we maintain a neighborhood structure, for each example, a naive retrieval to obtain nearest neighbors would lead to completely different classes with high probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Data sets</head><p>We perform experiments on four benchmarks: VIPeR <ref type="bibr" target="#b24">[25]</ref> , CUHK03 <ref type="bibr" target="#b10">[11]</ref> , CUHK01 <ref type="bibr" target="#b25">[26]</ref> , and Market-1501 <ref type="bibr" target="#b26">[27]</ref> (see examples in Fig. <ref type="figure" target="#fig_5">5</ref> ). <ref type="bibr" target="#b0">(1)</ref> The VIPeR data set <ref type="bibr" target="#b24">[25]</ref> contains 632 individuals taken from two cameras with arbitrary viewpoints and varying illumination conditions. The 632 person's images are randomly divided into two equal halves, one for training and the other for testing. <ref type="bibr" target="#b1">(2)</ref> The CUHK03 data set <ref type="bibr" target="#b10">[11]</ref> includes 13,164 images of 1360 pedestrians. The whole dataset is captured with six surveillance camera. Each identity is observed by two disjoint camera views, yielding an average 4.8 images in each view. This dataset provides both manually labeled pedestrian bounding boxes and bounding boxes automatically obtained by running a pedestrian detector <ref type="bibr" target="#b51">[52]</ref> . In our experiment, we report results on labeled data set. The dataset is randomly partitioned into training, validation, and test with 1160, 100, and 100 identities, respectively. (3) The CUHK01 data set <ref type="bibr" target="#b25">[26]</ref> has 971 identities with 2 images per person in each view. We report results on the setting where 100 identities are used for testing, and the remaining 871 identities used for training, in accordance with FPNN <ref type="bibr" target="#b10">[11]</ref> . ( <ref type="formula" target="#formula_19">4</ref>) The Market-1501 data set <ref type="bibr" target="#b26">[27]</ref> contains 32,643 fully annotated boxes of 1501 pedestrians, making it the largest person re-id dataset to date. Each identity is captured by at most six cameras and boxes of person are obtained by running a state-of-theart detector, the Deformable Part Model (DPM) <ref type="bibr" target="#b52">[53]</ref> . The dataset is randomly divided into training and testing sets, containing 750 and 751 identities, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Experimental settings, optimizations and implementation details</head><p>Each image in training set is resized to be 150 × 150 for the visible layer. We process each image by whitening their pixel intensity values, and extracting LBP and Gabor as inputs. LBP can generate 59 dimensional binary vector at each pixel location. Gabor filters are applied on each pixel to extract features invariant to viewpoint and pose <ref type="bibr" target="#b2">[3]</ref> . For each layer of CDBN, we need to set the size of filters, number of filters, and max-pooling region size. The first layer consists of 40 groups ( K = 40 ) of 12 × 12 pixel filters ( N W × N W = 12 × 12 ), while the second and third layer consists of 100 groups of 10 × 10 filters and 40 groups of 6 × 6 filters, respectively. The deep network of CDBN is optimized on a single GPU GTX 980 using the code available. <ref type="foot" target="#foot_0">1</ref> In training CRBMs, each layer was greedily pretrained 50 epoches through the entire training set. The weights were updated using a learning rate of 0.1, momentum of 0.9, and a weight decay of 0.002 × weight × learning rate. The weights were initialized with small random values sampled from a zero-mean normal distribution with variance 0.01. All trainable parameters of CRBMs are then fine-tuned w.r.t the proposed objective function Eq. ( <ref type="formula" target="#formula_12">3</ref>) to adapt the feature embedding into local manifold structure. For all experiments, we choose by grid search the mini-batch size of m = 64 , and the margins are set to be α 1 = 1 , α 2 = 0 . 5 in Eq. (3) . To find meaningful local positives in hard quadruplets, we perform data augmentation ensure that any class in a mini-batch has at least 4 samples. For each image, we deterministically create five crops of size H × W : four corner crops and one center crop, as well as horizontally flipped copy of each. Activations of three layers (after pooling) are concatenated to be the final feature vector.</p><p>For the evaluation protocol, we adopt the widely used singleshot modality to allow extensive comparison. Each probe image is matched against the gallery set, and the rank of the true match is obtained. The rank-r recognition rate is the expectation of the matches at rank r , and the cumulative values of the recognition rate at all ranks are recorded as the one-trial Cumulative Match-  ing Characteristic (CMC) results. This evaluation is performed ten times, and the average CMC results are reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Architecture analysis 6.3.1. Local positive sample mining</head><p>We evaluate the contribution of positive sample mining by comparing the performance with and without it. Since the embedding of CRBMs which perform a series of convolution and probabilistic pooling is unsupervised encoding, it is lack of capability to maintain identities with large visual variations. Thus, positive mining in local range and the derived objective are proposed to enhance the embedding of CRBMs by reducing intra-personal variations. We performed experiments on CUHK01 data set. The left figure of Fig. <ref type="figure" target="#fig_6">6</ref> shows the CMC curves. We can find that the collaboration of hard negative mining and positive mining achieves the best result at rank-1 value of 73.53%. Hard negative mining alone<ref type="foot" target="#foot_1">2</ref> has a performance drop to 66.48%. This validates the positive role of positive sample mining in learning features adaptive to local manifolds. In the case of no mining method is used, the embedding gives very low identification rate at rank-1 of 56%. The middle and right figures of Fig. <ref type="figure" target="#fig_6">6</ref> show the loss of the training and test sets with and without positive mining. We can see that mining on local positives enables faster convergence because of its manipulation on the local feature structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2.">Convergence rate</head><p>We investigate the convergence rate of local distribution loss and triplet loss, and report the results in Fig. <ref type="figure" target="#fig_7">7</ref> . The triplet loss is implemented by using Eq. ( <ref type="formula" target="#formula_36">10</ref>) , and the architecture of deep embedding is identical to our method. It can be observed that local distribution loss can reach the asymptotic error rate of triplet loss </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.3.">Visualization on deep representations</head><p>The representations are learned by feature embedding through stacked CRBMs, and optimize the objective function with similarity metric defined adaptively to local range. This dynamic process leads to more flexible representations that allow intra-personal variations, and robust to maintain identity information. To this end, 2d visualizations of representations of training with different loss functions are given in Fig. <ref type="figure" target="#fig_8">8</ref> . It can be seen that triplet loss tends  to produce unimodal separation due to the enforcement of semantic similarity in a global Euclidean distance. By contrast, the local distribution loss can more adaptively accept intra-class variations. For example, it captures the intra-class variation between the same person holding a handbag in one camera view while without a handbag in a different view. Some mined-out positive samples in local range are also shown in Fig. <ref type="figure" target="#fig_8">8</ref> . These positives are moderate difficulty than very hard negative examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Experimental results</head><p>In this section, we compare the proposed method with the following state-of-the-art approaches: JointRe-id <ref type="bibr" target="#b12">[13]</ref> , FPNN <ref type="bibr" target="#b10">[11]</ref> , LADF <ref type="bibr" target="#b28">[29]</ref> , SDALF <ref type="bibr" target="#b1">[2]</ref> , eSDC <ref type="bibr" target="#b54">[55]</ref> , KISSME <ref type="bibr" target="#b8">[9]</ref> , kLFDA <ref type="bibr" target="#b29">[30]</ref> , ELF <ref type="bibr" target="#b2">[3]</ref> , SalMatch <ref type="bibr" target="#b56">[57]</ref> , MLF <ref type="bibr" target="#b3">[4]</ref> , DML <ref type="bibr" target="#b11">[12]</ref> , ITML <ref type="bibr" target="#b57">[58]</ref> , DeepRanking <ref type="bibr" target="#b15">[16]</ref> , Multi-channel <ref type="bibr" target="#b16">[17]</ref> , NLML <ref type="bibr" target="#b4">[5]</ref> , NullRe-id <ref type="bibr" target="#b5">[6]</ref> , LMNN <ref type="bibr" target="#b58">[59]</ref> , PersonNet <ref type="bibr" target="#b13">[14]</ref> , DomainDropout <ref type="bibr" target="#b14">[15]</ref> , LOMO+XQDA <ref type="bibr" target="#b9">[10]</ref> , E-Metric <ref type="bibr" target="#b18">[19]</ref> , SI-CI <ref type="bibr" target="#b20">[21]</ref> , GatedCNN <ref type="bibr" target="#b21">[22]</ref> , DeepLDA <ref type="bibr" target="#b31">[32]</ref> , SSM <ref type="bibr" target="#b59">[60]</ref> . Note that not all of these methods report their results in all three data sets, and for fair comparison, we conduct performance with aforementioned method if available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.1.">Experiments on VIPeR data set</head><p>We compare the proposed approach to state-of-the-art methods in terms of CMC values on VIPeR dataset, and report results in Table <ref type="table" target="#tab_1">1</ref> and Fig. <ref type="figure" target="#fig_9">9</ref> . This dataset is relatively small and the number of distinct identities as well as positive pairs per identity for training are very less compared to other datasets. Therefore, we adopt a random translation for the training data augmentation. The images are randomly cropped (0-5 pixels) in horizon and vertical, and stretched to recover the size. Even though our method does not achieve the best matching rate, it outperforms recent deep embedding approaches: JointRe-id <ref type="bibr" target="#b12">[13]</ref> , DeepRanking <ref type="bibr" target="#b15">[16]</ref> , Multi-channel <ref type="bibr" target="#b16">[17]</ref> , NLML <ref type="bibr" target="#b4">[5]</ref> , and SI-CI <ref type="bibr" target="#b20">[21]</ref> . These methods are trained on triplet loss or soft-max loss and their similarity is defined in a global Euclidean distance. For example, our method has performance gain from 47.80% (the best result of deep embedding in state-of-the-art) to 49.04%. The most similar approach to us is E-Metric <ref type="bibr" target="#b18">[19]</ref> (rank-1 rate is 40.91%) which also considers moderate positive mining in local manifold structure. However, our method performs embedding with absolute feature position in deep feature space and the positive mining strategy is different from E-Metric <ref type="bibr" target="#b18">[19]</ref> in the selection of hard quadruplets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.2.">Experiments on CUHK03 data set</head><p>Table <ref type="table" target="#tab_2">2</ref> and Fig. <ref type="figure" target="#fig_9">9</ref> provide CMC results attained from all methods. Our method outperforms all competitors including deep embedding alternatives: PersonNet <ref type="bibr" target="#b13">[14]</ref> , DomainDropout <ref type="bibr" target="#b14">[15]</ref> , Gated-CNN <ref type="bibr" target="#b21">[22]</ref> , E-Metric <ref type="bibr" target="#b18">[19]</ref> and SI-CI <ref type="bibr" target="#b20">[21]</ref> . Compared with Person-Net <ref type="bibr" target="#b13">[14]</ref> that uses more weight layers training on pairwise sampling and joint similarity learning, our method offers advantage in jointly optimizing feature embedding, similarity metric learning, and adaptive positive sampling in local range. Compared with DomainDropout <ref type="bibr" target="#b14">[15]</ref> which combines training samples from multiple domains to seek generic features, our approach mitigates the cross-domain problem by maintaining the distribution of samples, and results in adaptive representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.3.">Experiments on CUHK01 data set</head><p>The CUHK01 data set contains 971 subjects, each of which has 4 images under two camera views. Following the protocol in <ref type="bibr" target="#b25">[26]</ref> , the data set is split into 871 subjects as training and the rest 100 as test set. Since this data set has limited number of training samples regarding each identity, we optimize the embedding on CUHK03, and then fine-tuning on the objective function on CUHK01. The CMC rank rates and curves are shown in Table <ref type="table" target="#tab_2">2</ref> and Fig. <ref type="figure" target="#fig_9">9</ref> , our method consistently achieves performance gain by improving the state-of-the-art result from 69.38% (attained by E-Metric <ref type="bibr" target="#b18">[19]</ref> ) to 71.60% at rank-1 matching rate. This notable improvement can be credited to the expressive representations derived from the deep embeddings of our method. The proposed joint optimization on embedding, similarity metric learning, and more suitable positive mining improves the quality of representation learning in person re-id. </p><formula xml:id="formula_41">Dataset CUHK03 CUHK01 Method R = 1 R = 5 R = 10 R = 20 R = 1 R = 5 R = 10 R = 20</formula><p>JointRe-id <ref type="bibr">[</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.4.">Experiments on Market-1501 data set</head><p>This dataset is the largest and the most realistic dataset with natural detector errors. Since each subject is captured by 6 cameras, the intra-class variations are evidently dominating in samples therein. In this data set, our method outperforms most of competitors in terms of CMC rank-1 rate and the mean average precision (mAP) value. The results are shown in Table <ref type="table" target="#tab_4">3</ref> . In the setting of single query, a number of alternatives are based on classical metric learning, e.g., KISSME <ref type="bibr" target="#b8">[9]</ref> , kLFDA <ref type="bibr" target="#b29">[30]</ref> , LOMO+XQDA <ref type="bibr" target="#b9">[10]</ref> , and NullRe-id <ref type="bibr" target="#b5">[6]</ref> , whereas they are unable to jointly optimize feature embedding and similarity learning. Our approach achieves 68.32% at rank-1 rate and 40.24% as mAP, showing a notable margin against these methods in single query case. It can be observed that two state-of-the-arts i.e., SSM <ref type="bibr" target="#b59">[60]</ref> + re-ranking and K-reciprocal encoding <ref type="bibr" target="#b62">[63]</ref> outperform our method in single query setting. The main reason is SSM <ref type="bibr" target="#b59">[60]</ref> and k-reciprocal encoding <ref type="bibr" target="#b62">[63]</ref> rely on a re-ranking process, which is not a self-contained principle. While deep embedding methods of DeepLDA <ref type="bibr" target="#b31">[32]</ref> , Deep-Hist-Loss <ref type="bibr" target="#b61">[62]</ref> , SCSP <ref type="bibr" target="#b27">[28]</ref> and GatedCNN <ref type="bibr" target="#b21">[22]</ref> can improve their recognition accuracy to some extend, they commonly perform embeddings with a global Euclidean objective. In contrast, our approach achieves a better result on account of jointly optimizing feature embeddings, adaptive similarity learning and local-ranged positive mining. In the setting of multiple query (MQ), the proposed method outperforms the most state-of-the-arts i.e., NullReid <ref type="bibr" target="#b5">[6]</ref> + MQ, GatedCNN <ref type="bibr" target="#b21">[22]</ref> + MQ, S-LSTM <ref type="bibr" target="#b46">[47]</ref> + MQ, by a notable margin. The witnessed performance gain can be ascribed to the reduced intra-class variations yielded by our method with respect to the multiple query structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">More evaluations and analysis</head><p>In this section, we carry out a fair self-evaluation of the proposed deep adaptive feature embedding approach to person reidentification. Our aim to conducting a self-evaluation is to assess each component of our approach to prove the necessarily positive roles that all components have played in the whole framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.1.">Evaluation on hierarchical representations: CNN features v.s. CRBMs</head><p>In this experiment, we test the capacity of our method in learning hierarchical representations for pedestrian images. In the embedding stage, we are using a stacked, three-layer CRBMs, and we retain features from each layer to perform pedestrian matching. As alternative, CNN is widely adopted to be the embedding solution <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref> . However, CNN typically uses shared parameters and filters across the input and feed-forward dimensions. This is unable to learn specific features from the different human body parts of pedestrian images; meanwhile, the morphological information is preserved from each part of human body. Results are shown in Table <ref type="table" target="#tab_5">4</ref> . First, it can be seen that CNN embedding cannot produce representations as robust as CRBMs can. This demonstrates the rational of using CRBMs as the feature embedding. Second, in terms of individual layers, the third layer outperforms the second and the first layer, and the combination of the first and second layers notably improves the matching accuracy relative to the first layer alone but marginal to the second layer alone. Third, the combination of three layers can improve the performance to a very limited extent relative to the third layer alone. Thus, to keep computation efficiency, we use the features extracted from the third layer only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.2.">Evaluation on the proposed quadruplet loss</head><p>To illustrate the effectiveness of the proposed quadruplet loss function, we perform experiments by applying a variety of loss functions into existing feature learning networks. Baselines using AlexNet <ref type="bibr" target="#b42">[43]</ref> , VGGNet <ref type="bibr" target="#b43">[44]</ref> and ResNet <ref type="bibr" target="#b63">[64]</ref> are fine-tuned with the default parameter setting except that the output dimension of the last FC layer is set to the number of training identities. The loss functions include contrastive loss <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b46">47]</ref> , triplet loss <ref type="bibr" target="#b15">[16]</ref> , and the proposed quadruplet loss with local positive mining. The AlexNet and VGGNet are trained for 60 epochs with a learning rate of 0.001 and then for another 20 epochs with a learning rate of 0.0 0 01. As suggested by <ref type="bibr" target="#b64">[65]</ref> , the parameters of the convolutional layers and first two FC layers are initialized by the parameters pre-trained on ImageNet and the parameters of the last fully-connected layer are randomly initialized with a Gaussian distribution ( G ( μ, σ ), μ = 0 , σ = 0 . 01 ).The ResNet is trained for 60 epochs with learning rate of 0.001 initially and reduced by 10 at 25 and 50 epochs. During testing, the FC6 descriptor of AlexNet/VGGNet and the Pool5 descriptor of ResNet are used for feature representation. Experimental results are shown in Table <ref type="table" target="#tab_6">5</ref> . In general, feature embedding with triplet loss outperforms contrastive loss by considering the error of the relative distance between positive and negative pairs while the contrastive loss is based on binary classification mode, which emphasizes on absolute distances of positive/negative pairs and thus leads to poor generalization ability on testing data. However, a model that is trained by a triplet loss would still cause a relatively large intra-class variation, as observed in <ref type="bibr" target="#b16">[17]</ref> . The comparison of the proposed loss to the triplet loss suggests that mining hard negatives together with local positives can effectively reduce intra-class variations and thus improves the recognition rate by a notable margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.3.">Analysis of variance reduced SGD</head><p>More recent studies on variance reduction techniques such as SAGA <ref type="bibr" target="#b65">[66]</ref> , N-SAGA <ref type="bibr" target="#b23">[24]</ref> and SVRG <ref type="bibr" target="#b66">[67]</ref> have been proposed to overcome this weakness, achieving linear convergence with geometric rates. They introduce variance corrections to ensure the convergence for constant learning rates. To realize the training efficiency, we employ the variance reduced SGD with sharing on stochastic gradients computed from neighborhood structure <ref type="bibr" target="#b23">[24]</ref> , and this technique is successfully applied into deep person re-id system. In this experiment, we evaluate the evolution of the suboptimality of the objective as a function in terms of the number of update steps performed i.e., data point evaluation. The step size of γ = q μN is used. Fig. <ref type="figure" target="#fig_10">10</ref> shows the sub-optimality as a function of the number of data point evaluations ( i.e., number of stochastic updates) for the value of μ = 10 -1 . We can see that N-SAGA shows the minimum number of updates in stochastic computations. The constant step-size variant of SGD is faster in the early stages until it converges to a neighborhood of the optimum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion and future work</head><p>In this paper, a principled deep feature embedding approach for person re-identification is presented to learn adaptive deep transformations to a feature space such that the local manifold structure of data is considered. The proposed approach is the first attempt to jointly optimize local similarity metric learning, meaningful positive mining in local manifolds and robust feature embedding. As a result, trainable parameters for feature embedding are optimized on local distribution objective which is designed to seek local manifold structure between positive samples in order to address the large intra-personal variations. To further improve training efficiency, we employ variance reduced SGD to share and reuse computed gradients across data samples in their neighborhood structure. In our future work, we would explore effective feature embeddings with discriminant analysis <ref type="bibr" target="#b67">[68]</ref><ref type="bibr" target="#b68">[69]</ref><ref type="bibr" target="#b69">[70]</ref><ref type="bibr" target="#b70">[71]</ref> to preserve the discriminative information regarding identities while approaching a solution to stable optimization and convergence. we describe a case with one CRBM. The model takes image as input of visible units with whitened pixel intensity values. In person re-id, some hand-crafted features such as Local Binary Pattern (LBP) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b29">30]</ref> and Gabor <ref type="bibr" target="#b2">[3]</ref> are employed to describe a person's appearance from different perspectives. Thus, we additionally learn deep representations from LBP and Gabor which can capture their high-order statistics. Since the generated features are highdimensional, we use PCA to reduce the dimensionality to 500 for each type of representation.</p><p>Lin Wu is Postdoctoral Research Fellow at The University of Queensland, Australia. Prior to joining Queensland, she was working as an ARC Senior Research Fellow at School of Computer Science, The University of Adelaide, Australia. She received a PhD from School of Computer Science and Engineering, The University of New South Wales, Sydney, Australia in 2014. She has published over 30 research papers (including one book chapter) in the fields of pattern recognition, machine learning and multimedia analytics, such as CVPR, ACM Multimedia, ACM SIGIR, IJCAI, IEEE TIP, IEEE TNNLS, IEEE Cybernetics. She is the co-recipient of best research paper runner-up award for PAKDD 2014, and regularly serves as program committee member for proceedings and invited reviewer for journals. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Yang</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) Samples of pedestrian images from the CUHK03 dataset [11] . Each column shows two images of the same identity observed by two disjoint camera views. (b) Highly-curved manifolds of 3 identities. Positive samples in a local range (green lines) should be selected to guide deep feature embedding while those in large distance (yellow line with cross) should not be sampled to respect the manifold structure. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)</figDesc><graphic coords="2,113.03,57.32,360.00,114.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The proposed deep embedding framework. For each feature pair ( f W ( x i ), f W ( x j )) extracted from images x i and x j by an embedding function f W ( • ) parameterized by W , we first select the most dissimilar positive pair in the mini-batch via ( ˆ i , ˆ j ) = arg min (i, j) ∈ P S i, j . where S i, j is the similarity score. Then, a hard quadruplet is constructed by choosing the hard negative ˆ k and hard positive ˆ l via ˆ k = arg max ( ˆ i ,k ) ∈ N S ˆ i ,k and ˆ l = arg min ( ˆ</figDesc><graphic coords="3,92.85,57.67,420.00,95.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>in this way, the seeking of the similarity metric function S ( • , • ) can be transformed into the joint learning of hyper-parameters { W e , W u , W c , W s , b e , b u , b c , b s }, and { W 1 , W 2 , W 3 , b 1 , b 2 , b 3 } for feature embeddings of CRBMs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Positive sample mining in a local range.</figDesc><graphic coords="5,122.84,57.21,360.00,156.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. (a) Pre-training using three stacked convolutional RBMs. (b) A block of convolutional RBM with probabilistic max-pooling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Examples from person re-identification datasets. From left to right: VIPeR, CUHK03, CUHK01, and Market-1501.</figDesc><graphic coords="7,68.84,56.77,468.00,117.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Component analysis of positive sample mining. Left: CMC curves with or without positive sample mining. PM: local positive mining. NM: hard negative mining. Middle and Right: The loss of the training and test sets with/without positive sample mining, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Training curves on three datasets as function of number of iterations. It indicates that local distribution loss reaches the same error to triplet loss in 5-30 times fewer iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. 2D visualization of representations obtained by training triplet loss, and local distribution loss on 10 identities. Different colors correspond to different identities, and the distributions are computed from t-SNE [54] .</figDesc><graphic coords="8,304.53,364.18,246.00,180.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Performance comparison with state-of-the-art approaches using CMC curves on VIPeR, CUHK03 and CUHK01 data sets.</figDesc><graphic coords="9,68.84,56.81,468.00,131.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Comparison of standard SGD, SAGA, and N-SAGA with decreasing and constant step size on three datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Wang is currently a Research Fellow in the School of Computer Science &amp; Engineering, The University of New South Wales, Kensington, Sydney, Australia. He obtained the PhD degree from the University of New South Wales, Sydney, Australia in 2015. So far, he has published 30 research papers including one book chapter, most of which have appeared in major venues, such as ACM Multimedia, ACM SIGIR, IJCAI, IEEE ICDM, ACM CIKM, IEEE TIP, IEEE TNNLS, IEEE TCYB, KAIS etc. Dr Wang was the winner of Best Research Paper Runner-up Award for PAKDD 2014, and was a program committee member for ECML/PKDD 2014 and ECML/PKDD 2015, while regularly served as the invited reviewer for IEEE TIP and IEEE TNNLS.Junbin Gao is the Professor of Big Data Analytics at the University of Sydney. He was a Professor in Computing from 2010 to 2016 and an Associate Professor from 2005 to 2010 at Charles Sturt University (CSU). Previously, between 2001 and 2005, he was a Senior Lecturer from Jan 2005 to July 2005 and a Lecturer from Nov 2001 to Jan 2005 in the School of Mathematics, Statistics and Computer Science at University of New England (UNE). Between 1999 and 2001, he worked as a Research Fellow in the Department of Electronics and Computer Science at University of Southampton, England. His research interests lie in machine learning, pattern recognition and image analysis. Junbin has extensively published the research results on the competitive venues, such as IEEE TPAMI, IEEE TNNLS, IEEE TIP, IEEE TCYB, IEEE CVPR, AAAI, IEEE ICDM, SIAM DM, Neural Computation, Machine Learning, Neural Networks and Pattern recognition. Xue Li is a Professor in the School of Information Technology and Electrical Engineering, The University of Queensland, Australia. His major areas of research interests and expertise include: Data Mining, Social Computing, Database Systems, and Intelligent Web Information Systems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>Rank-1, -5, -10, -20 recognition rate of various methods on the VIPeR data set (test person = 316).</figDesc><table><row><cell>Method</cell><cell>R = 1</cell><cell>R = 5</cell><cell>R = 10</cell><cell>R = 20</cell></row><row><cell>JointRe-id [13]</cell><cell>34.80</cell><cell>63.32</cell><cell>74.79</cell><cell>82.45</cell></row><row><cell>LADF [29]</cell><cell>29.34</cell><cell>61.04</cell><cell>75.98</cell><cell>88.10</cell></row><row><cell>SDALF [2]</cell><cell>19.87</cell><cell>38.89</cell><cell>49.37</cell><cell>65.73</cell></row><row><cell>eSDC [55]</cell><cell>26.31</cell><cell>46.61</cell><cell>58.86</cell><cell>72.77</cell></row><row><cell>KISSME [9]</cell><cell>19.60</cell><cell>48.00</cell><cell>62.20</cell><cell>77.00</cell></row><row><cell>kLFDA [30]</cell><cell>32.33</cell><cell>65.78</cell><cell>79.72</cell><cell>90.95</cell></row><row><cell>ELF [3]</cell><cell>12.00</cell><cell>41.50</cell><cell>59.50</cell><cell>74.50</cell></row><row><cell>PCCA [56]</cell><cell>19.27</cell><cell>48.89</cell><cell>64.91</cell><cell>80.28</cell></row><row><cell>SalMatch [57]</cell><cell>30.16</cell><cell>52.00</cell><cell>62.50</cell><cell>75.60</cell></row><row><cell>MLF [4]</cell><cell>29.11</cell><cell>52.00</cell><cell>65.20</cell><cell>79.90</cell></row><row><cell>DML [12]</cell><cell>34.49</cell><cell>60.13</cell><cell>74.37</cell><cell>84.18</cell></row><row><cell>DeepRanking [16]</cell><cell>38.37</cell><cell>69.22</cell><cell>81.33</cell><cell>90.43</cell></row><row><cell>Multi-channel [17]</cell><cell>47.80</cell><cell>74.70</cell><cell>84.80</cell><cell>91.10</cell></row><row><cell>NLML [5]</cell><cell>42.30</cell><cell>70.99</cell><cell>85.23</cell><cell>94.25</cell></row><row><cell>NullRe-id [6]</cell><cell>42.28</cell><cell>71.46</cell><cell>82.94</cell><cell>92.06</cell></row><row><cell>SCSP [28]</cell><cell>53.54</cell><cell>82.59</cell><cell>91.49</cell><cell>96.65</cell></row><row><cell>SI-CI [21]</cell><cell>35.75</cell><cell>72.33</cell><cell>81.78</cell><cell>97.07</cell></row><row><cell>E-Metric [19]</cell><cell>40.91</cell><cell>73.80</cell><cell>85.05</cell><cell>92.00</cell></row><row><cell>GatedCNN [22]</cell><cell>37.80</cell><cell>66.90</cell><cell>77.40</cell><cell>-</cell></row><row><cell>S-LSTM [47]</cell><cell>42.40</cell><cell>68.70</cell><cell>79.40</cell><cell>-</cell></row><row><cell>Ours</cell><cell>49.04</cell><cell>77.13</cell><cell>86.26</cell><cell>96.20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>Rank-1, -5, -10, -20 recognition rate of various methods on the CUHK03 (test person = 100) and CUHK01 data sets (test person = 100).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>Rank-1,-5, -10, -20 recognition rate and mAP of various methods on the Market-1501 data set (test person = 751). MQ: multiple query.</figDesc><table><row><cell>Method</cell><cell>R = 1</cell><cell>R = 5</cell><cell>R = 10</cell><cell>R = 20</cell><cell>mAP</cell></row><row><cell>SDALF [2]</cell><cell>20.53</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>8 . 2 0</cell></row><row><cell>eSDC [55]</cell><cell>33.54</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1 3 . 5 4</cell></row><row><cell>KISSME [9]</cell><cell>39.35</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1 9 . 1 2</cell></row><row><cell>kLFDA [30]</cell><cell>44.37</cell><cell>67.35</cell><cell>74.82</cell><cell>81.94</cell><cell>23.14</cell></row><row><cell>LOMO + XQDA [10]</cell><cell>43.79</cell><cell>65.27</cell><cell>73.22</cell><cell>80.38</cell><cell>22.22</cell></row><row><cell>BoW [27]</cell><cell>34.40</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1 4 . 0 9</cell></row><row><cell>DeepLDA [32]</cell><cell>48.15</cell><cell>72.46</cell><cell>80.22</cell><cell>86.78</cell><cell>29.94</cell></row><row><cell>Deep-Hist-Loss [62]</cell><cell>59.47</cell><cell>80.73</cell><cell>86.94</cell><cell>91.09</cell><cell>-</cell></row><row><cell>SCSP [28]</cell><cell>51.90</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>26.35</cell></row><row><cell>NullRe-id [6]</cell><cell>55.43</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>29.94</cell></row><row><cell>NullRe-id [6] + MQ</cell><cell>71.56</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>46.03</cell></row><row><cell>GatedCNN [22]</cell><cell>65.88</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>39.55</cell></row><row><cell>GatedCNN [22] + MQ</cell><cell>76.04</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>48.45</cell></row><row><cell>S-LSTM [47] + MQ</cell><cell>61.60</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>35.30</cell></row><row><cell>SSM [60] + re-ranking</cell><cell>82.81</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>68.80</cell></row><row><cell>K-reciprocal encoding [63]</cell><cell>77.11</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>63.63</cell></row><row><cell>Ours</cell><cell>68.32</cell><cell>87.23</cell><cell>94.59</cell><cell>96.71</cell><cell>40.24</cell></row><row><cell>Ours + MQ</cell><cell>84.14</cell><cell>93.25</cell><cell>97.33</cell><cell>98.07</cell><cell>58.80</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc>Rank-1 accuracy on the CUHK01 data set (test person = 486 or 100) using features from CNN embedding and different individual/combination layers of CRBMs.</figDesc><table><row><cell>Method</cell><cell>test = 486</cell><cell>test = 100</cell></row><row><cell>untied CNN embedding [19]</cell><cell>51.79%</cell><cell>69.38%</cell></row><row><cell>CDBN (first layer)</cell><cell>58.32%</cell><cell>61.04%</cell></row><row><cell>CDBN (second layer)</cell><cell>63.67%</cell><cell>66.25%</cell></row><row><cell>CDBN (third layer)</cell><cell>65.85%</cell><cell>71.60%</cell></row><row><cell>CDBN (first + second layers)</cell><cell>66.04%</cell><cell>71.68%</cell></row><row><cell>CDBN (three layers)</cell><cell>66.09%</cell><cell>71.70%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc>Rank-1 accuracy on the CUHK01 data set (test person = 486 or 100) using features from CNNs with varied embedding functions.</figDesc><table><row><cell>Method</cell><cell>test = 486</cell><cell>test = 100</cell></row><row><cell>AlexNet [43] + contrastive loss</cell><cell>41.24%</cell><cell>62.30%</cell></row><row><cell>AlexNet [43] + triplet loss</cell><cell>56.74%</cell><cell>70.02%</cell></row><row><cell>AlexNet [43] + proposed loss</cell><cell>62.69%</cell><cell>72.14%</cell></row><row><cell>VGGNet [44] + contrastive loss</cell><cell>42.56%</cell><cell>64.78%</cell></row><row><cell>VGGNet [44] + triplet loss</cell><cell>58.78%</cell><cell>72.45%</cell></row><row><cell>VGGNet [44] + proposed loss</cell><cell>65.08%</cell><cell>75.93%</cell></row><row><cell>ResNet [64] + contrastive loss</cell><cell>47.28%</cell><cell>65.04%</cell></row><row><cell>ResNet [64] + triplet loss</cell><cell>60.13%</cell><cell>76.05%</cell></row><row><cell>ResNet [64] + proposed loss</cell><cell>67.29%</cell><cell>80.47%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/gwtaylor/imCRBM.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>For each positive pair, e.g., ( i, j ), their hard negatives are retrieved by selecting the samples with the biggest similarity scores: k = arg max i,k ∈ N S i,k , l = arg max j.l∈ N S j,l .</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is partially supported by ARC DP 160104075 . Junbin Gao's research was partially by Australian Research Council Discovery Projects funding scheme (Project No. DP140102270 ) and the University of Sydney Business School ARC Bridging Fund.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Generating hierarchical representations</head><p>This appendix provides the procedure of generating hierarchical representations for each pedestrian sample by using CRBMs. Suppose all parameters in stacked CRBMs are learned, we can generate the representation of an image by sampling from the joint distribution over all hidden layers conditioned on the input. We use block Gibbs sampling to sample from the units of each layer in parallel. We summarize the procedure of sampling in Algorithm 2 where Algorithm 2: Block Gibbs sampling on all hidden layers. Input : A visible layer (input image) V , a detection layer H, a pooling layer P , higher detection layer H above P . A set of shared weights</p><p>where k,l is connecting pooling units P k to H l .</p><p>Output : Hierarchical representations of an image.</p><p>3 for Sample each block independently via conditional probability</p><p>)+ I(p k α )) ; 6 Sample from V given H via Eq (5); </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Person reidentification using spatiotemporal appearance</title>
		<author>
			<persName><forename type="first">N</forename><surname>Gheissari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Person re-identification by symmetry-driven accumulation of local features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Farenzena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Viewpoint invariant pedestrian recognition with an ensemble of localized features</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning mid-level filters for person re-identfiation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Nonlinear local metric learning for person re-identification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning a discriminative null space for person re-identification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Local fisher discriminant analysis for pedestrian re-identification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pedagadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Orwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Velastin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boghossian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<title level="m">Person re-identification by probabilistic relative distance comparison, CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Large scale metric learning from equivalence constraints</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kostinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deepreid: deep filter pairing neural network for person re-identification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep metric learning for person re-identification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An improved deep learning architecture for person re-identification</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Personnet: person re-identification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Hengel</surname></persName>
		</author>
		<idno>CoRR abs/1601.07255</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning deep feature representation with domain guided dropout for person re-identification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep ranking for re-identification via joint representation learning</title>
		<author>
			<persName><forename type="first">S.-Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2353" to="2367" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Person re-identification by multi-channel parts-based cnn with improved triplet loss function</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.04179</idno>
		<title level="m">Structured deep hashing with convolutional neural networks for fast person re-identification</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Embedding deep metric for person re-identification: a study against large variations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Local similarity-aware deep feature embedding</title>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Joint learning of single-image and cross-image representations for person re-identification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Gated siamese convolutional neural network architecture for human re-identification</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Varior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Haloi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised learning of hierarchical representations with convolutional deep belief networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="95" to="103" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Variance reduced stochastic gradient descent with neighbors</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Evaluating appearance models for recognition, reacquisition, and tracking</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Int&apos;l. Workshop on Perf. Eval. of Track. and Surv&apos;l</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Human reidentification with transferred metric learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACCV</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<title level="m">Scalable person re-identification: a benchmark, ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Similarity learning with spatial constraints for person re-identification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning locally-adaptive decision functions for person verification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Person re-identification using kernel-based metric learning methods</title>
		<author>
			<persName><forename type="first">F</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Camps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sznaier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised metric fusion over multiview data by graph random walk-based cross-view diffusion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="70" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep linear discriminant analysis on fisher networks: a hybrid architecture for person re-identification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="238" to="250" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep feature learning with relative distance comparison for person re-identification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2993" to="3003" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Robust hashing for multi-view data: jointly learning low-rank kernelized similarity consensus and hash functions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vision Comput</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="58" to="66" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Stable local dimensionality reduction approaches</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2054" to="2066" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Two-dimensional supervised local similarity and diversity projection</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3359" to="3363" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Locally linear discriminant embedding: an efficient method for face recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3813" to="3821" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Unsupervised cross-dataset transfer learning for person re-identification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Continuous adaptation of multi-camera person identification models through sparse non-redundant representative selection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vision Image Understand</title>
		<imprint>
			<biblScope unit="volume">156</biblScope>
			<biblScope unit="page" from="66" to="78" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Schö&quot;lkopf , Domain adaptation with conditional transferable components</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<title level="m">Cnnpack: Packing convolutional neural networks in the frequency domain, NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6115</idno>
		<title level="m">Compressing deep convolutional networks using vector quantization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">A siamese long short-term memory architecture for human re-identification</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Varior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Enhanced fisher discriminant criterion for image recognition</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3717" to="3724" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Training products of experts by minimizing contrastive divergence</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1771" to="1800" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<title level="m">Neighbourhood components analysis</title>
		<imprint>
			<publisher>NIPS</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Sparsity-based occlusion handling method for person re-identification</title>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Multimedia Modeling</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Unsupervised salience learning for person re-identification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Pcca: a new approach for distance learning from sparse pairwise constraints</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mignon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="2666" to="2672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">Person re-identification by salience matching, ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Information-theoretic metric learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Person re-identification by efficient imposter-based metric learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l</title>
		<title level="s">Conf. on Adv. Vid. and Sig. Surveillance</title>
		<meeting>Int&apos;l</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Scalable person re-identification on supervised smoothed manifold</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Is that you? Metric learning approaches for face identification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<title level="m">Learning deep embeddings with histogram loss, NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Re-ranking person re-identification with k-reciprocal encoding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.5726</idno>
		<title level="m">Cnn: single-label to multi-label</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Defazio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<title level="m">Saga: a fast incremental gradient method with support for non-strongly convex composite objectives, NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Accelerate stochastic gradient descent using predictive variance reduction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Robust subspace clustering for multi-view data by exploiting correlation consensus</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3939" to="3949" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">General tensor discriminant analysis and gabor features for gait recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1700" to="1715" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Geometric mean for subspace selection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="260" to="274" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<title level="m">Large-margin weakly supervised dimensionality reduction, in: ICML</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="865" to="873" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
