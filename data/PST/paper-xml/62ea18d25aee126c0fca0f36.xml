<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Some Practice for Improving the Search Results of E-commerce</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Fanyou</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Forest and Natural Resource</orgName>
								<orgName type="institution">Purdue University West Lafayette</orgName>
								<address>
									<region>Indiana</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
							<email>liuy@chalmers.se</email>
							<affiliation key="aff1">
								<orgName type="department">School of Vehicle and Mobility</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rado</forename><surname>Gazo</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Forest and Natural Resource</orgName>
								<orgName type="institution">Purdue University West Lafayette</orgName>
								<address>
									<region>Indiana</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Benes</forename><surname>Bedrich</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Purdue University West Lafayette</orgName>
								<address>
									<region>Indiana</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaobo</forename><surname>Qu</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">School of Vehicle and Mobility</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Some Practice for Improving the Search Results of E-commerce</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>LR: Logistic Regression FG: Group Feature FC: Four Class Probability FP: Query-Porduct Feature search relevance</term>
					<term>querying</term>
					<term>e-commerce</term>
					<term>semantic matching</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Figure 1: Overall schema of our proposed solution for Amazon KDD CUP 2022 for all three tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">PROBLEM DESCRIPTION</head><p>The organizer provides a dataset called the Shopping Queries Dataset <ref type="bibr" target="#b4">[5]</ref>. It is a large-scale, manually annotated dataset composed of challenging customer queries. The data is multilingual and includes English, Japanese, and Spanish queries. It comprises query-result pairs with annotated four classes of relevance (ESCI labels):</p><p>? Exact (E): the item is relevant for the query and satisfies all the query specifications;</p><p>? Substitute (S): the item is somewhat relevant: it fails to fulfill some aspects of the query, but the item can be used as a functional substitute; ? Complement (C): the item does not fulfill the query but could be used in combination with an exact item; ? Irrelevant (I): the item is irrelevant, or it fails to fulfill a central aspect of the query.</p><p>The primary objective of this competition is to build new ranking strategies and, simultaneously, identify interesting categories of results (i.e., substitutes) that can be used to improve the customer experience when searching for products. The three different tasks for this KDD Cup competition using our Shopping Queries Dataset are: T1. Query-Product Ranking T2. Multiclass Product Classification T3. Product Substitute Identification Task one (T1) aims at ranking the relevance of a subset of the ESCI dataset by using Normalized Discounted Cumulative Gain (nDCG) score to measure the performance. The organizer designed a customized Discounted Cumulative Gain (DCG) of 1.0, 0.1, 0.01 and 0.0, for Exact, Substitute, Complement and Irrelevant respectively.</p><p>Task two (T2) aims to classify each product as being an Exact, Substitute, Complement, or Irrelevant match for the query. The Micro-F1 (equivalent to accuracy here) will be used to evaluate the methods. Task three (T3) is a binary classification problem that arXiv:2208.00108v1 [cs.IR] 30 Jul 2022 tries to distinguish whether a query product pair is a Substitute or not and uses the Micro-F1 score as well to measure the performance.</p><p>T1 uses a subset of ECSI dataset, while T2 and T3 use the same dataset. It is natural to treat this competition as two different problems. In the rest of the paper, we will use short-term T2T3 to represent tasks two and three. This inner difference among tasks is also reflected on the final leaderboard that the final ranking of T1 and T2T3 are not correlated well. The setup also involves a potential data leakage, which we will discuss further in section 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">EXPLORE DATA ANALYSIS</head><p>ECSI dataset contains three tables, which are product_catalogue, train and test. The test is also decomposed into public and private where the latter one is unseen to us. In this section, we make our special observation of product_catalogue and train that play an important role in the final leaderboard.</p><p>T1 and T2T3 use different product_catalogue tables. Figure <ref type="figure" target="#fig_0">2</ref> shows the order of product entries in T2T3. T1 remains a similar pattern unless it is started with es entries (es ? us ? jp). There is a part of products that have not been used in the both training set and the public test set, and we conjecture those entries are unique in the private test set. Let us have another investigation about train. Figure <ref type="figure">3</ref> shows the histogram of products grouped by queries in T1 and T2T3. Generally, most queries will sample 16 and 40 products for training and test sets. And this distribution is slightly different between T1 and T2T3, while we know that there are fewer Exact labels in T1. So associated with this prior knowledge, using this product number as a feature to calibrate the prediction will make some improvement in the leaderboard.</p><p>Another important piece of information in the training set is that the proportion of ESCI labels in T1 and T2T3 are different (Figure <ref type="figure">4</ref>). This difference creates a well-known data leakage for most participants in T2T3 that distinguishing whether a queryproduct pair is in T1 will improve results. However, it isn't easy to use this information directly in the private test set.</p><p>Besides the above-described deep dive, some other patterns will help to improve the scores. Here is the summary of that information:</p><p>E1.  E5. The product id is called ASIN and will be identical to ISBN (starts with digits) if the product has ISBN. E6. Most query products group has fewer unique brand numbers than product numbers and the product with the most frequent brand tends to be labeled as Exact. E7. At least one product in a query-products group will be labeled as Exact, and the label of the query-product pair is affected by other labels in this group as well.</p><p>Combining those explorations, we could significantly improve scores for all three tasks. A detailed feature engineering will be introduced in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED SOLUTION</head><p>Figure <ref type="figure">1</ref> shows the general schema of our proposed solution for Amazon KDD CUP 2022 for all three tasks. As we planned to attend to all three tasks, for efficiency, we have to train the cross-encoders once and use them for all three tasks. This strategy makes this twostage solution the only choice. So we trained all cross-encoders with all data from T1 and T2T3 in two folds and then combined the four class probabilities with other essential features, using lightGBM to fuse and calibrate the prediction and adapt results to different tasks. Figure <ref type="figure">5</ref> shows the milestone of the public leaderboard score for T2. In the following, we will discuss those milestones in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Cross-Encoder Architecture</head><p>In the first stage, we applied the classical cross encoder <ref type="bibr" target="#b5">[6]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Encoder</head><p>Figure <ref type="figure">5</ref>: Our milestone of public leaderboard score for T2. We use red points and blue points to represent the improvement from cross-encoders models or the gain from feature engineering, respectively.</p><p>multiple fields (title, brand, and so on), we use neither the CLS token nor mean (max) pooling to get the latent vector of the query-product pair. Instead, we concatenate the hidden states of a predefined token (query, title, brand color, etc.). This small modification yields about a 0.002 increase in the T2 public leaderboard (0.802 ? 0.804).</p><p>In the final solution, we ensembled three cross-encoders for each language that differ from pre-trained models, the training data, or the input fields. For Engish entries, we used DeBERTaV3 <ref type="bibr" target="#b1">[2]</ref>, BigBird <ref type="bibr" target="#b6">[7]</ref> and COCO-LM <ref type="bibr" target="#b3">[4]</ref>. While for Japanese and Spanish ones, we used a multi-language version of DeBERTaV3. All those pre-trained data could be found at Huggerface. Table <ref type="table">1</ref> shows the average accuracy for each model for T2. By ensemble of all models, the score for the public leaderboard for task two is 0.818.</p><p>Table <ref type="table">1</ref>: Average accuracy for each model for T2. Here we used 2-fold cross-validation. Note for code submission, we used the different seeds to split data, so the accuracy here is not very comparable. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Locale Pretrained Model Accuracy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Feature Engineering</head><p>Once stacking a lot of models has tiny improvements for both local and online tests, we start to do some feature engineering based on our exploration in section 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Leakage Features.</head><p>Combining E1, E2, and E3 together, we designed a feature that measures the percentage of product_id in Task 1 product list grouped by query_id. This feature gives us a 0.005 improvement in the T2 public leaderboard and remains effective for the private test set, and that's why we are extremely closed to first place in T2 (0.0001 difference).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Query product number features.</head><p>Based on E4, we use the product number for each query as a feature and obtain an approximate 0.002 improvement in the T2 public leaderboard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Product ID features.</head><p>Based on E5, we designed features that measure whether the product_id is ISBN or not and whether the query-products group has an ISBN product or not. This feature gives us an approximate 0.001 improvement in the T2 public leaderboard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Brand Features.</head><p>Based on E6, we designed features that measure the unique number of brands in a query-products group and whether the brand of the product is the most frequent one in the group. This feature gives us an approximate 0.001 improvement in the T2 public leaderboard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5">Group Features.</head><p>Based on E7, we designed several stats (min, medium, and max) of the cross encoder output probability grouped by query_id. This feature gives us a 0.008 improvement in the T2 public leaderboard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">LightGBM model</head><p>3.3.1 T1. As the ECSI label distribution is different in T1 and T2T3, for T1, we train the lightGBM model with T1 data only to simulate this distribution and calculate the expected gain for each queryproduct pair as:</p><formula xml:id="formula_0">? = ? ? + 0.1 ? ? ? + 0.01 ? ? ? ,<label>(1)</label></formula><p>where ? ? , ? ? and ? ? are the probability output of lightGBM for label Exact, Substitute and Irrelevant respectively. Then we sort the query-product list by this gain. This method is slightly better than using LambdaRank <ref type="bibr" target="#b0">[1]</ref> with the same label gain (0,0.01,0.1,1) in LightGBM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">T2T3.</head><p>We use full data from T1 and T2T3, and use lightGBM to train either a four-class classifier (T2) or a binary classifier (T3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Model ensemble.</head><p>For T1, T2, and T3, we average the gain or the prediction from 6 models (3 models x 2 folds) for each language to make a final decision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">INFERENCE ACCELERATION</head><p>During the code submission round, the organizer proposed a 120 minutes time limit for all three tasks. This time limit requests us to provide a more efficient solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Knowledge Distillation</head><p>We use knowledge distillation <ref type="bibr" target="#b2">[3]</ref> to improve the model's performance. This knowledge distillation is applied to English entries only. We used all data and trained large versions of DeBERTaV3, BigBird, and COCO-LM. Then we used a linear combination loss of cross-entropy (loss between prediction and ground truth) and mean-square-error (logit difference between student model and teacher model).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Other Inference Acceleration Strategies</head><p>Here are we listed some other inference acceleration strategies: A1. Pre-process product token and save it as an HDF5 file. A2. Transfer all models to ONNX with FP16 precision. A3. Pre-sort the product id to reduce the side impact of batch zero padding. A4. Use a relatively small mini-batch size (= 4) when inference.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The order of product entries in T2T3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: The histogram of products grouped by queries in the training set. 16 and 40 are the typical number of products for each query.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The order of the product catalog is not randomized (training set ? private test set ? public test set). E2. Most products are used once. E3. The ESCI label proportion is different between T1 and T2T3. E4. Most queries have 16 or 40 product numbers and the label distribution of those queries are slightly different.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>architecture with only minor modifications. As the product context has</figDesc><table><row><cell></cell><cell>Single Model</cell><cell>Two Folds</cell><cell>6 Models</cell><cell>Leakage</cell><cell>Group Features</cell></row><row><cell>0.802</cell><cell>0.804</cell><cell>0.814</cell><cell>0.818</cell><cell>0.823</cell><cell>0.831</cell></row><row><cell>Appx. July, 1st</cell><cell>July, 7th</cell><cell>July, 9th</cell><cell>July, 12th</cell><cell>July, 15th</cell><cell>July, 19th</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to rank with nonsmooth cost functions</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Ragno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing</title>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.09543</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Coco-lm: Correcting and contrasting text sequences for language model pretraining</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Payal</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="23102" to="23114" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Chandan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llu?s</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fran</forename><surname>M?rquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Valero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sambaran</forename><surname>Zaragoza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnab</forename><surname>Bandyopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anlu</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><surname>Subbian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.06588</idno>
		<title level="m">Shopping Queries Dataset: A Large-Scale ESCI Benchmark for Improving Product Search</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10084</idno>
		<title level="m">Sentence-bert: Sentence embeddings using siamese bert-networks</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avinava</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifan</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="17283" to="17297" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
