<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Comparative Study of Differential Evolution, Particle Swarm Optimization, and Evolutionary Algorithms on Numerical Benchmark Problems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jakob</forename><surname>Vesterstram</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Ren6</forename><surname>Thomsen</surname></persName>
							<email>thomsen@birc.dk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">BiRC -Bioinformatics Research Center</orgName>
								<orgName type="institution">University of Aarhus</orgName>
								<address>
									<addrLine>Ny Munkegade</addrLine>
									<postBox>Bldg. 540</postBox>
									<postCode>DK-8000</postCode>
									<settlement>Aarhus C</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">BiRC -Bioinformatics Research Center</orgName>
								<orgName type="institution">University of Aarhus</orgName>
								<address>
									<addrLine>Ny Munkegade</addrLine>
									<postBox>Bldg. 540</postBox>
									<postCode>DK-8000</postCode>
									<settlement>Aarhus C</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Comparative Study of Differential Evolution, Particle Swarm Optimization, and Evolutionary Algorithms on Numerical Benchmark Problems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">263ECF5463D3D3B4E8C8C4C11E36A42C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T02:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Several extensions to evolutionary algorithms (EAs) and particle swarm optimization (PSO) have heen suggested dur-and an evolutionary algorithm (EA)' on a selection of 34 numerical benchmark problems. The main obiective was to exing the last decades offering improved performance on selected benchmark problems. Recently, another search heuristic termed differential evolution (DE) has shown superior in seVprs~ r e ~~. w ~r ~d annlieatinns. In this naner we evaluate the amine whether one tested algorithms would outperfolm all others on a majority of the problems. Additionally, since we used a rather large number of benchmark problems, the .~ ~ .~ _. . ..-. .. ~~~ ~~~~~ performance of DE, PSO, and EAs regarding their general a p plicability as numerical optimization techniques. The comparison is performed On a suite Of 34 mdely The results from our study show that DE generally outperforms the other algorithms. However, on two noisy functions, both DE and PSO were Outperformed hy the EA. 'The EA used in this study resembled a real-valued GA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>experiments would also reveal whether the algorithms would have any Overall, the experimental results show that DE was far more efficient and robust (with respect to reproducing the results in several runs) compared to PSO and the EA. This suggests that more emphasis should be put on DE when solving numerical problems with real-valued parameters. However, on two noisy test problems, DE was outperformed by the other algorithms. difficulties or preferences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>benchmark I. INTRODUCTION</head><p>The evolutionary computation (EC) community has shown a significant interest in optimization for many years. In particular, there has been a focus on global optimization of numerical, real-valued 'black-box' problems for which exact and analytical methods do not apply. Since the mid-sixties many general-purpose optimization algorithms have been proposed for finding near-optimal solutions to this class of problems; most notably: evolution strategies (ES) <ref type="bibr" target="#b7">[8]</ref>, evolutionary programming (EP) [3], and genetic algorithms (GA) <ref type="bibr">[6]</ref>.</p><p>Many efforts have also been devoted to compare these algorithms to each other. Typically, such comparisons have been based on artificial numerical benchmark problems. The goal of many studies was to verify that one algorithm outperformed another on a given set of problems. In general, it has been possible to improve a given standard method within a restricted set of benchmark problems by making minor modifications to it.</p><p>Recently, particle swarm optimization (PSO) <ref type="bibr" target="#b6">[7]</ref> and differential evolution (DE) [ 1 I] have been introduced and p aticularly PSO has received increased interest from the EC community. Both techniques have shown great promise in several real-world applications [4], [51, [121, [141. However, to our knowledge, a comparative study of DE, PSO, and GAS on a large and diverse set of problems has never been made.</p><p>In this study, we investigated the performance of DE, PSO,</p><p>The paper is organized as follows. In Section I1 we introduce the methods used in the study: DE, PSO, and the EA. Further, Section 111 outlines the experimental setup, parameter settings, and benchmark problems used. The experimental results are presented in Section IV. Finally, Section V contains a discussion of the experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Differential Evolution</head><p>The DE algorithm was introduced by Storn and Price in 1995 [ 1 I]. It resembles the structure of an EA, but differs from traditional EAs in its generation of new candidate solutions and by its use of a 'greedy' selection scheme. DE works as follows: First, all individuals are randomly initialized and evaluated using the fitness function provided. Afterwards, .the following process will be executed as long as the termination condition is not fulfilled: For each individual in the populatb&gt;n, an offspring is created using the weighted difference of parent solutions. In this study we used the DE/rand/l/exp scheme shown in Figure <ref type="figure">1</ref>. The offspring replaces the parent if it is fitter. Otherwise, the parent survives and is passed on to the next iteration of the algorithm. where il # iz # i 3 # i n = U(0,dim) </p><formula xml:id="formula_0">for (j = 0; j &lt; dim A U(0,l) &lt; CR; j + + ) { ' ' O[il[n] =P[i1l[nl + F . ( P [ i z ] [ n ] -P[i3l[nl) n = (n + 1) mod dim . . } }. ' '</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Particle Swarm Optimization</head><p>PSO was introduced by Kennedy and Eberhart in 1995. It was inspired by the swarming behavior as is displayed by a flock of birds, a school of fish, or even human social behavior being influenced by other individuals <ref type="bibr" target="#b6">[7]</ref>.</p><p>PSO consists of a swarm of particles moving in an ndimensional, real-valued search space of possible problem solutions. Every particle has a position vector Z' encoding a candidate solution to the problem (similar to the genotype in EAs) and a velocity vector C. Moreover, each particle contains a small memory that stores its own best position seen so far @ and a global best position a obtained through communication with its neighbor particles. In this study we used the fully connected network topology for passing on information (see [I51 for more details).</p><p>Intuitively, the information about good solutions spreads through the swarm, and thus the particles tend to move to good areas in the search space. At each time step t , the velocity is updated and the particle is moved to a new position. This new position is calculated as the sum of the previous position and the new velocity:</p><formula xml:id="formula_1">q t + 1) = q t ) + " t + 1)</formula><p>The update of the velocity from the previous velocity to the new velocity is determined by the following equation:</p><p>where U ( a , b ) is a uniformly distributed number between a and b. The parameter w is called the inertia weight <ref type="bibr">[IO]</ref> and controls the magnitude of the old velocity "(t) in the calculation of the new velocity, whereas 41 and $2 determine the significance of $(t) and a(t), respectively. Furthermore, at any time step of the algorithm vi is constrained by the parameter U , , , .</p><p>The swarm in PSO is initialized by assigning each particle to a uniformly and randomly chosen position in the search space. Velocities are initialized randomly in the range [-vma5, v,,,].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1981</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Attractive and Repulsive PSO</head><p>The attractive and repulsive PSO (arPSO) 191, 1151 was introduced by Vesterstrgm and Riget to overcome the problems of premature convergence <ref type="bibr">[I]</ref>. The modification of the basic PSO scheme is to modify the velocity update formula when the swarm diversity becomes less than a value This modification corresponds to repulsion of the particles instead of the usual attraction scheme. Thus, the velocity is updated according to:</p><formula xml:id="formula_2">v'(t+l) = w.&lt;(t)-U(O, $Jl)(?j(t)-qt))-v(o, 42)(g(t)-z(t))</formula><p>This will increase the diversity over some iterations, and eventually when another value dhigh is reached, the commonly used velocity update formula will be used again. Thus, arPSO is able to zoom out when an optimum has been reached, followed by zooming in on another hot spot, possibly discovering a new optimum in the vicinity of the old one. Previously, arPS0 was shown to be more robust than the basic PSO on problems with many optima 191.</p><p>The arPSO algorithm was included in this study as a representative for the large number of algorithmic extensions to PSO that try to avoid the problem of premature convergence. Other PSO extensions could have been chosen but we selected this particular one, since the performance of arPS0 was as good (or better) as many other extensions 1151.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Evolutionary Algorithm</head><p>In this study we used a simple EA (SEA) that was previously found to work well on real-world problems [13]. The SEA works as follows: First, all individuals are randomly initialized and evaluated according to a given fitness function. Afterwards, the following process will be executed as long as the termination condition is not fulfilled: Each individual has a probability of being exposed to either mutation or recombination (or both). Mutation and recombination operators are applied with probability p,,, and p,, respectively. The mutation and recombination operators used are Cauchy mutation using an annealing scheme and arithmetic crossover, respectively. Finally, tournament selection [Z] comparing pairs of individuals is applied to weed out the least fit individuals.</p><p>The Cauchy mutation operator is similar to the well-known Gaussian mutation operator, but the Cauchy distribution has thick tails that enable it to generate considerable changes more frequently than the Gaussian distribution. The Cauchy distribution has the form:</p><p>where a 2 0, / 3 &gt; 0, -CO &lt; z &lt; 00 (a and p are parameters that affect the mean and spread of the distribution). All of the solution parameters are subject to mutation and the variance is scaled with 0.1 x the range of the specific parameter in question.</p><p>Moreover, an annealing scheme was applied to decrease the value of p as a function of the elapsed number of generations t.</p><p>a was fixed to 0. In this study we used the following annealing function:</p><p>In arithmetic crossover the offspring is generated as a weighted mean of each gene of the two parents, i.e., offspring, = r . parentl,</p><formula xml:id="formula_3">+ (1 -T ) 'parent2,,</formula><p>where offspring, is the i'th gene of the offspring and parentl; and parent2; refer to the i'th gene of the two parents, respectively. The weight T is determined by a random value between 0 and 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Additional Remarks</head><p>All methods described above used truncation of infeasible solutions to the nearest boundary of the search space. Further, the termination criterion for all methods was to stop the search process, when the current number of fitness evaluations exceeded a maximum number of evaluations allowed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="111.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Setup and Data Sampling</head><p>The algorithms used for comparison were DE, PSO, arPS0, and the SEA. For all algorithms the parameter settings were manually tuned, based on a few preliminary experiments.</p><p>The specific settings for each of the algorithms are described below. Each algorithm was tested with all of the numerical benchmarks shown in Table <ref type="table">I</ref>. In addition, we tested the algorithms on fl-fil in 100 dimensions, yielding a total of 34 numerical benchmarks. For each algorithm, the maximum number of evaluations allowed was set to 500,000 for the 30dimensional (or less) benchmarks and to 5,000,000 for the 100-dimensional benchmarks. Each of the experiments was repeated 30 times with different random seeds, and the average fitness of the best solutions (e.g. individuals or particles) throughout the optimization run was recorded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. DE Settings</head><p>DE has three parameters: The size of the population (popsize), the crossover constant (CR), and the scaling factor (F). In all experiments they were. set to the following values: popsize = 1.00, CR = 0.9, F = 0.5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. PSO Settings</head><p>PSO has several parameters: The number of particles in the swrrm (swarmsize), maximum velocity (vmaZ), the parameters for attraction towards personal best and the neighborhoods best found solutions ($1 and $z), and the inertia weight (U). For PSO we used these settings: swarmsize = 25, umoz = 15% of the longest axis-parallel interval in the search space, Often the inertia weight is decreased linearly over time. The setting for PSO in this study is a bit unusual, because the inertia weight was held constant during the run. However, it was found that for the easier problems, the chosen settings outperformed the setting where w was annealed. The setting = 1.8, c$2 = 1.8, and w = 0.6. with constant U , on the other hand, performed poorly on multimodal problems. To be fair to PSO, we therefore included the arPSO algorithm, which was known to outperform PSO (even with annealed w ) on multi modal problems [9].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. arPSO Settings</head><p>In addition to the basic PSO settings, arPS0 used the following parameter settings: dlow = O.OoooO5 and dhigh = 0.25. The two parameters were only marginally dependent on the problem, and these settings were consistent with !:he settings found in previous studies <ref type="bibr">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. SEA Settings</head><p>The SEA used a population size of 100. The probability of mutating and crossing over individuals was fixed at p , = 10.9 and p , = 0.7, respectively. Tournament selection with a tournament size of two was used to select the individuals for the next generation. Further, elitism with an elite size of one was used to keep the overall best solution found in the population.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Numerical Benchmark Functions</head><p>For evaluating the four algorithms, we used a test suite of benchmark functions previously introduced by Yao and Liu <ref type="bibr">[16]</ref>. The suite contained a diverse set of problems, including unimodal as well as multimodal functions, and functions with correlated and uncorrelated variables. Additionally, two noisy problems and a single problem with plateaus were included. The dimensionality of the problems originally varied from 2 to 30, but we extended the set with 100-dimensional variants to allow for comparison on more difficult problem instances. Table <ref type="table">I</ref> lists the benchmark problems, the ranges of their search spaces, their dimensionalities, and their global minimum fitnesses. We omitted Jlg and fzo from Yao and Liu's study [I61 because of difficulties in obtaining the definitions of the constants used in these functions (they were not provided in <ref type="bibr" target="#b5">[161)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problems of Dimensionality 30 or Less</head><p>The results for the benchmark problems fl-fz3 are shown in Table <ref type="table">I1</ref> and 111. Moreover, Figure <ref type="figure" target="#fig_3">2</ref> shows the convergence graphs for selected benchmark problems. All results below For functions fl-fd there is a consistent performance pattern across all algorithms: PSO is the best, and DE is almost as good. They both converge exponentially fast toward the fitness optimum (resulting in a straight line when plotted uc.ing a logarithmic y-axis). The SEA is many times slower than the two other methods, and even though it eventually might converge toward the optimum, it requires several hours to find solutions that PSO and DE can reach in a few seconds. This analysis is illustrated in Figure <ref type="figure" target="#fig_3">2 (a)</ref>.</p><p>On function fs, DE is superior to both PSO and arF'SO and the SEA. Only DE converges toward the optimum. After 300,000 evaluations, it commences to fine-tune around the were reported as '0.0000000e+00'. optimum at an exponentially progressing rate. We may note that PSO performs moderately better than the SEA. DE and the SEA easily find the optimum for the f6 function, whereas both PSOs fail. This test function consists of plateaus, and apparently both PSO methods have difficulties with functions of this kind. The average best fitness value of 0.04 for basic PSO comes from failing twice in 30 runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE I NUMERICAL BENCHMARK FUNCTIONS WITH A VARYING NUMBER OF DIMENSIONS (DIM). REMARKS: 1) FUNCTIONS SINE</head><p>Function f7 is a noisy problem. All algorithms seem to converge in a similar pattern, see Figure <ref type="figure" target="#fig_3">2 (d)</ref>. The SEA had the best convergence speed, followed by arPS0, PSO, and finally DE.</p><p>Functions f8-f13 are highly multimodal functions. On all of them, DE clearly performs best and it finds the global optimum in all cases. Neither the SEA nor the PSOs find the global optimum for these functions in any of the runs. Further, the SEA consistently outperforms both PSOs, with fio being an exception.</p><p>Both DE, the SEA, and arPSO come very close to the global optimum on f14 in all runs, but only DE hits the exact optimum every time, making it the best algorithm on this problem. PSO occasionally stagnates at a local optimum, which is the reason for its poor average best fitness.</p><p>On f i 5 both PSOs perform worse than DE and SEA. DE converges very fast to good values near the optimum, but seems to stagnate suboptimally. The SEA converges slowly, but outperforms DE after 500,000 fitness evaluations. To investigate if the SEA would continue its convergence and ultimately reach the optimum, we tried to let it run for 2 million evaluations. In the majority of runs, it actually found the optimum after approximately 1 million evaluations (data not shown).</p><p>Functions f16-fl8 are all easy problems, and all algorithms are able to find near optimum solutions quickly. Both PSO methods have particularly fast convergence. For some reason, the SEA seems to be able to fine-tune its results slightly better than the other algorithms.</p><p>The last three problems are f21-fZ3. Again, DE is superior compared to the other algorithms. It finds optimum in all cases. DE. the SEA, and PSO all converge quickly, but the SEA stagnates before finding the optimum, and both PSO methods converge even earlier. arPSO performs better than PSO, and is almost as good as the SEA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Problenis of Dimensionaliry 100</head><p>On fl, PSO and DE have good exponential convergence to optimum (similar to the results in 30 dimensions) and the SEA is much slower. However, on f2 the picture has changed. DE still has exponential convergence to optimum, but both PSOs fail to find the optimumthey are now performing worse than the SEA. The same pattern occurs for f3 and f4. This contrasts with the 30 dimensional cases, where PSO performed exceptionally well for fl-f4.</p><p>On the difficult f5 problem, DE is superior and finds optimum after 3.5 million evaluations. The other algorithms fail to find the optimum. However, the SEA is slightly better than the basic PSO.</p><p>Both DE and the SEA quickly find the optimum of f6 in all runs. PSO only finds the optimum in 9 out of 30 runs. This is the reason for the average of 2.1 for PSO on this problem.</p><p>The results for the 100 dimensional version of f7 are similar to the results in the 30 dimensional case. The SEA has the best convergence, arPS0 is slightly slower, followed by PSO, and finally DE.</p><p>For problems f8-fl3 the results also resemble those from 30 dimensions. One exception is that arPS0 is now marginally worse than the SEA on fro. Thus. in 100 dimensions the SEA is consistently better than both PSOs on these six problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSION</head><p>Overall, DE is clearly the best performing algorithm in this study. It finds the lowest fitness value for most of the problems. The only exceptions are: 1) The noisy f7 problem, where the nature of the convergence of DE is similar, but still slower than the other algorithms. Apparently, DE faces difficulties on noisy problems. 2) On f15 DE stagnates on a suboptimal value (and both PSOs even earlier). Only the SEA is able to find the optimum on this problem, which only has four problem parameters, but still appears to be very difficult.</p><p>We have not tried to tune DE to these problems. Most likely, one could improve the performance of DE by altering the crossover scheme, varying the parameters C R and F , or using a more 'greedy' offspring generation strategy (e.g.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DElbestlllexp).</head><p>DE is robust; it is able to reproduce the same results consistently over many trials, whereas the performance of PSO and arPS0 is far more dependent on the randomized initialization of the individuals. This difference is profound on the 100 dimensional benchmark problems. As a result, both versions of PSO must be executed several times to ensure good results, whereas one run of DE and the SEA usually suffices.</p><p>PSO is more sensitive to parameter changes than the other algorithms. When changing the problem, one probably need to change parameters as well to sustain optimal performance. This is not the case for the SEA and DE. The 100 dimensional problems illustrate this point. Thus, the settings for both PSOs do not generalize to 100 dimensions, whereas DE and the SEA can be used with the same settings and still give the same t)pe of convergence.</p><p>In general, DE shows great fine-tuning abilities, but on f16 and f17 it fails in comparison to the SEA. We have not determined why DE fails to fine tune these particular problems, but it would be interesting to investigate why.</p><p>Regarding convergence speed, PSO is always the fastest, whereas the SEA or arPS0 are always the slowest. However, the SEA could be further improved with a more 'greedy' selection scheme similar to DE. Especially on the very easy functions f16-f18. PSO has a very fast convergence (3-4 times faster than DE). This may be of practical relevance for some real-world problems where the evaluation is computationally expensive and the search space is relatively simple and of low dimensionality.   To conclude, the performance of DE is outstanding in comparison to the other algorithms tested. It is simple, robust, converges fast, and finds the optimum in almost every run. In addition, it has few parameters to set, and the same settings can be used for many different problems. Previously, the DE has shown its worth on real-world problems, and in this study it outperformed PSO and EAs on the majority of the numerical benchmark problems as well. Among 'the tested algorithms, the DE can rightfully be regarded as an excellent first choice, when faced with a new optimization problem to solve. The results for the two noisy benchmark functions call ,. for further investigations. More experiments are required to determine why and when the DE and PSO methods fail on noisy problems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>procedure create offspring O [ i ] from parent P[i] { O[i] = P(i] // copy parent genotype to offspring ,randomly select parents P [ ~I ] , P[iz], P[i3],</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. I .</head><label>I</label><figDesc>Fig. I . Pseudo-code for creating an offspring in DE. U ( 0 : z ) is a uniformly disrributcd number between 0 and z. CR is the probability of crossover, F is the scaling factor, and dim is Ihe number of problem parameters (problem dimensionality).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>- 5 55 5 -5 5 x, 5 5 =</head><label>555</label><figDesc>A N D COSINE TAKE ARGUMENTS I N RADIANS. 2) THE NOTATION (a)T(b) DENOTES THE DOT PRODUCT BETWEEN VECTORS a AND b. 3) THE FUNCTION U A N D THE VALUES vi REFERRED TO I N Jn A N D JU ARE GIVEN B Y U(%. (I, b, e) = b ( za ) ' I F I &gt; a, u ( x , a, b, c) = b ( -za)" I F z i 0 , u ( x , a, b, e) = 0 I F -a 5 3 5 a, A N D FINALLY yi = 1 + a ( Z , + 1). THE MATRIX a USED I N f14. THE VECTORS D A N D b USED IN fis. A N D THE MATRIX a AND THE VECTOR c USED IN f 2 i -f ~~. ARE ALL DEFINED IN THE APPENDIX. Function Dim Js(8) = c:z; (100. (xi+] -(xi)')'+ (xi A 1)') Xi z) = -10.4Jm(ir z) = -10.5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2.Average best fitness curves for selected benchmark problems. All results are means of 30 runs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE II RESULTS</head><label>II</label><figDesc>FOR ALL ALGORITHMS ON BENCHMARK PROBLEMS OF DIMENSIONALITY 30 OR LESS (MEAN OF 30 RUNS AND STANDARD DEVIATIONS (STDDEV)). FOR EACH PROBLEM, THE BEST PERFORMING ALGORITHM(S) IS EMPHASIZED IN BOLDFACE</figDesc><table><row><cell>Benchmark</cell><cell>DE</cell><cell></cell><cell>rso</cell><cell></cell><cell></cell><cell></cell><cell>SEA</cell><cell></cell></row><row><cell>rroblcm</cell><cell>Me?. "</cell><cell>Stddev</cell><cell>Mean</cell><cell>Stddev</cell><cell>Mea"</cell><cell>Stddev</cell><cell>Mea"</cell><cell>Stddev</cell></row><row><cell>1</cell><cell>0.0000000e+00</cell><cell>O.We+W</cell><cell cols="2">o.oooooolk+oo o.ooe+Oo</cell><cell>6.8081735e-I3</cell><cell>5.30e-13</cell><cell>1.78941 I2e-03</cell><cell>2.77e-04</cell></row><row><cell>2</cell><cell>0.0000000e+00</cell><cell>0.00e+00</cell><cell cols="2">o.oooooooe+00 O.Mle+@o</cell><cell>2.0892037e-02</cell><cell>1.48e-01</cell><cell>1.7207452e-02</cell><cell>1.70.-03</cell></row><row><cell>3</cell><cell>2.02W713e-09</cell><cell>8.26e-I0</cell><cell>0.0000000e+00</cell><cell>O.We+W</cell><cell>O.OOoooOOe+OO</cell><cell>2.13e-25</cell><cell>1.5891817e-02</cell><cell>4.25e-03</cell></row><row><cell>4</cell><cell cols="2">3.8502 I ~7 ~-0 n 9. I 7e-09</cell><cell>2.1070152e-16</cell><cell>8.01e-I6</cell><cell cols="2">1.4183790e-05 . 8.27e-06</cell><cell cols="2">1.9827734~-02 2.07e-03</cell></row><row><cell>5</cell><cell cols="2">0.0000000e+W O.M)e+W</cell><cell>4.0263857e+00</cell><cell>4.99e+00</cell><cell>3.550928&amp;+02</cell><cell>2.15e+03</cell><cell>3.1318954e+OI</cell><cell>1.74e+OI</cell></row><row><cell>6</cell><cell cols="2">0.0000000e+W O.M)e+OO</cell><cell>4.00000We-02</cell><cell>1.98~-01</cell><cell cols="2">1.898OOM)e+O1 6.30e+01</cell><cell cols="2">0.00W000e+M) O.oOe+W</cell></row><row><cell>7</cell><cell cols="2">4.9390831~-03 1.13e-03</cell><cell>1.9082207e-03</cell><cell>1.14e-03</cell><cell cols="2">3.8866682~-04 4.78e-04</cell><cell>7.1062480e.M</cell><cell>3.27~-04</cell></row><row><cell>8</cell><cell>-1.2569481e+04</cell><cell>2.30~-04</cell><cell cols="2">.7.1874076e+03 6.72er02</cell><cell cols="2">-n.5986527e+03 2.07et03</cell><cell cols="2">-I . 166933&amp;+04 2.34e+02</cell></row><row><cell>9</cell><cell>0.0000000e+00</cell><cell>O.M)e+OO</cell><cell>4.9170789e+Ol</cell><cell>1.62e+01</cell><cell cols="2">2.1491414e+W 4.91et00</cell><cell>7.1789575e-01</cell><cell>9.22e-01</cell></row><row><cell>10</cell><cell>-1.1901591e-15</cell><cell>7.03~-I6</cell><cell>1.4046895et00</cell><cell>7.91e-01</cell><cell cols="2">1.8422773~-07 7.15~-08</cell><cell>1.0468 l80e-02</cell><cell>9.0%-04</cell></row><row><cell>I I</cell><cell>0.000OOOOe+00</cell><cell>O.M)e+OO</cell><cell>2.3528934e-02</cell><cell>3.54~-02</cell><cell>9.2344555-02</cell><cell>3.41e-01</cell><cell>4.6366988e-03</cell><cell>3.96e-03</cell></row><row><cell>12</cell><cell>o.000ooooe+OO</cell><cell>O.Mle+M)</cell><cell>3.819961 le-01</cell><cell>8.40~-01</cell><cell>8.5597888e-03</cell><cell>4.79e-02</cell><cell cols="2">4.5626102~-06 8.11e-07</cell></row><row><cell>13</cell><cell>-1.1428244e+00</cell><cell>4.45e-08</cell><cell>-5.9688703e-01</cell><cell>5.17~-01</cell><cell>-9.6263537e-01</cell><cell>5.14e-01</cell><cell>-1.I42742lkcW</cell><cell>1.34e-05</cell></row><row><cell>14</cell><cell>9.980039Oe41</cell><cell>3.75~-08</cell><cell>I.l570484e+00</cell><cell>3.68~-01</cell><cell>9.98W393e-01</cell><cell>2.13e-08</cell><cell>9.98004We-01</cell><cell>4.33e-08</cell></row><row><cell>15</cell><cell>4.1736828e-04</cell><cell>3.01e-04</cell><cell cols="2">1.3378460~-03 3.94e-03</cell><cell>1.2476701 e-03</cell><cell>3.96e-03</cell><cell cols="2">3.7~18~8e.04 8.78~-05</cell></row><row><cell>16</cell><cell>-1.0316285e+00</cell><cell>1.92e-08</cell><cell cols="2">~1.03 16284e+00 3.W-08</cell><cell cols="2">-1.0316284e+00 3.84e-08</cell><cell>-1.0316300e+00</cell><cell>3.16e-08</cell></row><row><cell>17</cell><cell cols="2">3.9788735~-01 1.17~-08</cell><cell cols="2">3.9788736~-01 5.01e-09</cell><cell>3.9788736e-01</cell><cell>5.01e-09</cell><cell cols="2">~. ~7 ~~7 ~e -o i 2.20.-08</cell></row><row><cell>18</cell><cell cols="2">3.000owOe+00 0.We+00</cell><cell cols="2">3.oooOwOe+00 O.Wec00</cell><cell cols="2">3.5 1627 19e+00 3.65e+00</cell><cell cols="2">3.w00000ec00 O.Mle+W</cell></row><row><cell>21</cell><cell cols="2">-1.0153201e+01 4.60.-07</cell><cell cols="2">-5.3944733e+00 3.40e+M)</cell><cell cols="2">-8.1809408e+W 2.60e+00</cell><cell cols="2">-8.4076288e+~ 3.16ec00</cell></row><row><cell>22</cell><cell>-1.0402943e+Ol</cell><cell>3.58~-07</cell><cell cols="2">-6.9460507~+00 3.70e+W</cell><cell cols="2">.8.4352620.+00 2.83e+00</cell><cell cols="2">-8.9125~noe+oo ~.86e+oo</cell></row><row><cell>23</cell><cell>-1.0536412e+01</cell><cell>2.09e-07</cell><cell cols="2">-6.7107552er00 3.77etW</cell><cell cols="2">-8.615504oe+W 2.88e+00</cell><cell cols="2">-9.7995696e+00 2.24e+OO</cell></row><row><cell>Problem</cell><cell>Mean</cell><cell>Stddev</cell><cell>Mean</cell><cell>Slddev</cell><cell>Mean</cell><cell>Stddev</cell><cell>Mean</cell><cell>Stddev</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 111 RESULTS</head><label>111</label><figDesc>FOR ALL ALGORITHMS ON BENCHMARK PROBLEMS OF DIMENSIONALITY 100 (MEAN OF 30 RUNS AND STANDARD DEVIATIONS (STDDEV)). FOR EACH PROBLEM, THE BEST PERFORMING ALGORITHM(S) IS EMPHASIZED IN BOLDFACE.</figDesc><table><row><cell>Benchmark I</cell><cell>DE</cell><cell>I</cell><cell>PSO</cell><cell>I</cell><cell>a m o</cell><cell>I</cell><cell>SEA</cell></row><row><cell>2</cell><cell>0.0000000e+00</cell><cell>O.Mk+(M</cell><cell>1.8045813et01</cell><cell></cell><cell></cell><cell></cell><cell>9.43~-04</cell></row><row><cell>3</cell><cell>5.873478%-IO</cell><cell>1.83~-IO</cell><cell>3.6666668~+03</cell><cell></cell><cell></cell><cell></cell><cell>6.06e-03</cell></row><row><cell>4</cell><cell>1.1284972e-09</cell><cell>1 . 4 2 ~-I 0</cell><cell>5.31218Me+W</cell><cell></cell><cell></cell><cell></cell><cell>5.71e-04</cell></row><row><cell>5</cell><cell>0.0000000e+W</cell><cell>O.We+W</cell><cell>2.0203629e+02</cell><cell></cell><cell></cell><cell></cell><cell>1.29ec01</cell></row><row><cell>6</cell><cell>0.00MM00e+00</cell><cell>O.We+OO</cell><cell>2.1000WOe+W</cell><cell></cell><cell></cell><cell></cell><cell>O.M)e+W</cell></row><row><cell>7</cell><cell>7.6640871~-03</cell><cell>6.58~-04</cell><cell>2.7845728e-02</cell><cell></cell><cell></cell><cell></cell><cell>9.70.-05</cell></row><row><cell>8</cell><cell>-4.1898293ecM</cell><cell>I .Me-03</cell><cell>-2.1579M8e+M</cell><cell></cell><cell></cell><cell></cell><cell>5.3&amp;+02</cell></row><row><cell>9</cell><cell>0.0000000e+00</cell><cell>o . m t 0 0</cell><cell>2.4359139et02</cell><cell></cell><cell></cell><cell></cell><cell>3.04e-01</cell></row><row><cell>10</cell><cell>8.02321 17e-15</cell><cell>I .7&amp;-I 5</cell><cell>4.49343 16e+W</cell><cell></cell><cell></cell><cell></cell><cell>1.47e-04</cell></row><row><cell>I I</cell><cell>5.4210109e-20</cell><cell>O.OOet00</cell><cell>4.1715onoe-01</cell><cell></cell><cell></cell><cell></cell><cell>4.42e-03</cell></row><row><cell>12</cell><cell>0.0000000e+00</cell><cell>0.00e+W</cell><cell>1.1774980e-01</cell><cell></cell><cell></cell><cell></cell><cell>2.76~-08</cell></row><row><cell>13</cell><cell>-1.1428244c+OO</cell><cell>2.74e-08</cell><cell>-3.~604485~-01</cell><cell></cell><cell></cell><cell></cell><cell>2.41e-08</cell></row></table><note><p>................... I 18.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>12 IS.14 (e) f10</head><label></label><figDesc></figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like ,to thank Wouter Boomsma for proofreading the paper. Also, we would like to thank the colleagues at BiRC for valuable comments on early versions of the manuscript. This work has been supported by the Danish Research Council.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>1.0 1.0 1.0 8.0 8.0 8.0 6.0 6.0 6.0 7.0 3.0 7.0 9.0 2.0 9.0 5.0 3.0 3.0 1.0 8.0 1.0 2.0 6.0 2.0 f14 : a = (-32, -W O , 16,32,. . . , -32,-16,0,16,32 -32,. . . , -16,. . , ,O,. . . ,16,. . ,, 32,. . . </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Evolutionary optimization versus particle swarm optimization: Philosophy and perfotmance differences</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">1</forename><surname>Angeline</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Evolulionory Pmgramnling VU</title>
		<editor>
			<persName><forename type="first">V</forename><forename type="middle">W</forename><surname>Porto</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Saravanan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Waagen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Eiben</surname></persName>
		</editor>
		<imprint>
			<publisher>springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page">601410</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Fogel. and 2. Michalewicr, edilors. Handbook of Evolurionory Compurorion, chapter</title>
		<author>
			<persName><forename type="first">T</forename><surname>Biick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>lnstitule of Physics Publishing and Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Artificial intelligence through a simulation of evolution</title>
	</analytic>
	<monogr>
		<title level="m">Biophysics and Cybernetic Systems Pmr. of [he 2nd Cybrmetir Sciences Syntposium</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Maxfield</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">I</forename><surname>Callahan</surname></persName>
		</editor>
		<editor>
			<persName><surname>Fogel</surname></persName>
		</editor>
		<imprint>
			<publisher>Spartan Books</publisher>
			<date type="published" when="1965">1965</date>
			<biblScope unit="page" from="131" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A particle swarm optimization far reactive power and voltage conVal in electric power systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fukuyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Takayama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nskanishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pmcerdingr &lt;f the Generic and Evolutionary C , m p t m t i m Conference</title>
		<editor>
			<persName><forename type="first">W</forename><surname>Banrhaf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Daida</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Eiben</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Garron</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Honavar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="1523" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Particle swarm optimization for reconfigurable phasc-differmlialcd anay design</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rahmat-Samii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Letters</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="0" to="3" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Adpatotion in Natural and Arr#ciol Systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Holland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<publisher>University of Michigan Press</publisher>
			<pubPlace>Ann Arbor, MI</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Panicle swarm optimization. In Pmcerdingr of the</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Eberhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internotional Conferenre on Neural Nelworkr</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1942" to="1948" />
			<date type="published" when="1995">1995. 1995</date>
			<publisher>IEEE Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Evolurion araregy: Optimization of tt&apos;chnicnl syxrmn by meon5 of biological evolution</title>
		<author>
			<persName><forename type="first">I</forename><surname>Rechenberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973">1973</date>
			<publisher>Froman-Halzboag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Riget and 1. S. Vester~m~m. A diversity-guided panicle swarm optimizer -the arPS0</title>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<pubPlace>Denmark</pubPlace>
		</imprint>
		<respStmt>
			<orgName>EVALife. Dept. of Computer Science, University of Aarhus</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A modified panicle swarm optimizer</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Eberhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Evolutionary Computation</title>
		<meeting>the IEEE Conference on Evolutionary Computation</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="69" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Differential evolution -a simple and efficient adaptive scheme for global optimization over continuous spaces. Technical repon</title>
		<author>
			<persName><forename type="first">R</forename><surname>Storn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Price</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
		<respStmt>
			<orgName>Intemational Computer Science Institute, Berkley</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Flexible ligand docking using differential evolution</title>
		<author>
			<persName><forename type="first">R</forename><surname>Thomsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings ofthe 2003 Congress on Evolutionary Cmpputarion</title>
		<meeting>the 2003 Congress on Evolutionary Cmpputarion</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="2354" to="2361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Flexible ligand docking using evolutionary algonthms: investigating the effects of vacation operators and local search hybrids</title>
		<author>
			<persName><forename type="first">R</forename><surname>Thomsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioSyrtems</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="57" to="73" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Parameter identification of induction momn using differential evolution</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Unem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vadsmp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 C O ~~W S S on Evolutionary Computation</title>
		<meeting>the 2003 C O ~~W S S on Evolutionary Computation</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">1151</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Particle swarms: Extensions for improved local, multi-modal, and dynamic search in numerical optimization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Vesterstrgm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Riget</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<pubPlace>Denmark</pubPlace>
		</imprint>
		<respStmt>
			<orgName>EVALife, Dept. of Computer Science, University of Aarhus</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast evolutionary programming</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings .f the 5th Annual Conference on Evolutionary Programming</title>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Fogel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Angeline</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Back</surname></persName>
		</editor>
		<meeting>.f the 5th Annual Conference on Evolutionary Programming</meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1996">1996. c = (0.1,0.2,0.2,0.4,0.4,0.6,0.3,0.7,0.5,0</date>
			<biblScope unit="page">451460</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
