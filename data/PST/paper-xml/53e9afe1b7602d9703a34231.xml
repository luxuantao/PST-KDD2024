<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PBN: Towards Practical Activity Recognition Using Smartphone-Based Body Sensor Networks *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Matthew</forename><surname>Keally</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Gang</forename><surname>Zhou</surname></persName>
							<email>gzhou@cs.wm.edu</email>
						</author>
						<author>
							<persName><forename type="first">Guoliang</forename><surname>Xing</surname></persName>
							<email>glxing@msu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
							<email>jxwu@ntu.edu.sg</email>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><surname>Pyles</surname></persName>
							<email>ajpyles@cs.wm.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science College of William and Mary Williamsburg</orgName>
								<address>
									<postCode>23187</postCode>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Computer Science College of William and Mary Williamsburg</orgName>
								<address>
									<postCode>23187</postCode>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Dept. of Computer Science and Engineering</orgName>
								<orgName type="institution">Michigan State University East Lansing</orgName>
								<address>
									<postCode>48824</postCode>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">School of Computer Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<postCode>639798</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Dept. of Computer Science College of William and Mary Williamsburg</orgName>
								<address>
									<postCode>23187</postCode>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PBN: Towards Practical Activity Recognition Using Smartphone-Based Body Sensor Networks *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">36E0939B9E09A4A7E5D07FDB0159AB67</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>C.3 [Special-Purpose and Application-Based Sys- Algorithms</term>
					<term>Design</term>
					<term>Experimentation</term>
					<term>Performance Body Sensor Networks</term>
					<term>Mobile Phones</term>
					<term>Motes</term>
					<term>Machine Learning</term>
					<term>Sensing</term>
					<term>Activity Recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The vast array of small wireless sensors is a boon to body sensor network applications, especially in the context awareness and activity recognition arena. However, most activity recognition deployments and applications are challenged to provide personal control and practical functionality for everyday use. We argue that activity recognition for mobile devices must meet several goals in order to provide a practical solution: user friendly hardware and software, accurate and efficient classification, and reduced reliance on ground truth. To meet these challenges, we present PBN: Practical Body Networking. Through the unification of TinyOS motes and Android smartphones, we combine the sensing power of on-body wireless sensors with the additional sensing power, computational resources, and user-friendly interface of an Android smartphone. We provide an accurate and efficient classification approach through the use of ensemble learning. We explore the properties of different sensors and sensor data to further improve classification efficiency and reduce reliance on user annotated ground truth. We evaluate our PBN system with multiple subjects over a two week period and demonstrate that the system is easy to use, accurate, and appropriate for mobile devices.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The low cost and wide availability of wireless sensors makes body sensor networks (BSNs) an increasingly attractive solution for a wide range of applications such as personal health care monitoring <ref type="bibr" target="#b2">[3]</ref>, physical fitness assessment <ref type="bibr" target="#b0">[1]</ref>, and context awareness <ref type="bibr" target="#b18">[19]</ref>  <ref type="bibr" target="#b3">[4]</ref>. In all of these applications, and especially in the context awareness and activity recognition domain, the need exists to provide personal control and direct application feedback to the user. For example, a sensing application for activity recognition allows user configuration of the number and placement of sensor nodes as well as runtime notification as to the current activity. Complete control over a body sensor network allows a user to tailor the application to his or her needs as well as exercise discretion over the use and dissemination of activity inferences.</p><p>For context aware and activity recognition applications, the sensing capability of multiple on-body sensor nodes is difficult to capture through the use of smartphones alone <ref type="bibr" target="#b16">[17]</ref>. However, the portability, computational power, and interface of a mobile phone can provide the user personal control and feedback over the sensing application. We envision a practical solution to activity recognition for mobile devices which is entirely portable, under direct control of the user, computationally lightweight, and accurate.</p><p>In this paper, we focus on four major challenges for providing practical activity recognition with mobile devices. First, the hardware and software platform must be userfriendly. The hardware must be portable and easily configurable, while the software must provide an intuitive control interface with adequate system feedback. Second, classi-fication must be performed accurately, handling both easy and difficult to classify activities, environmental changes, and noisy data. Accurate classification must also allow context switching between activities without extensive parameter tuning and different, complex classifiers for each activity. Third, since mobile hardware is often constrained in terms of computation power and energy, classification must be performed efficiently and redundant sensing resources must be timely turned off. Lastly, the system must have a reduced reliance on ground truth, for regular collection and labeling of sensor data can be invasive to the end user.</p><p>While there is significant existing work in the mobile activity recognition domain, most approaches do not offer practical solutions that address the above challenges. Some approaches <ref type="bibr" target="#b8">[9]</ref> [15] <ref type="bibr" target="#b27">[28]</ref> provide multiple on-body sensor nodes but do not provide a portable interface, such as a mobile phone, for the end user to control sampling, configure hardware, or receive real time feedback. Other approaches <ref type="bibr" target="#b17">[18]</ref>  <ref type="bibr" target="#b18">[19]</ref> rely on computationally expensive learning algorithms and a back end server. Still more works <ref type="bibr" target="#b15">[16]</ref>  <ref type="bibr" target="#b16">[17]</ref> rely on specific sensing models for each sensor modality, making sensor collaboration difficult. Lastly, other works <ref type="bibr" target="#b26">[27]</ref> [23] do not provide online training for adaptation to body sensor network (BSN) dynamics. Compared to static sensor networks, body sensor network dynamics include the changing geographical location of the user, user biomechanics and variable sensor orientation, as well as background noise.</p><p>Towards addressing these challenges of activity recognition, we propose PBN: Practical Body Networking. PBN consolidates the flexibility and sensing capability of TinyOSbased motes with the additional sensing power, computational resources, and user-friendly interface of an Android smartphone. Our solution can also be extended beyond TinyOS motes to combine Android with a wide variety of USB devices and wireless sensors. Through the use of ensemble learning, which automates parameter tuning for BSN dynamics, PBN provides a capable, yet lightweight activity recognition solution that does not require a backend server.</p><p>With PBN, we provide an online training solution which detects when retraining is needed by analyzing the information divergence between training and runtime data distributions and integrating this analysis with the ensemble classifier. In this way, PBN determines when retraining is needed without the need to request ground truth from the user. Furthermore, we investigate the properties of sensors and sensor data to identify sensors which are accurate and have diverse classification results. From this analysis, we are able to prevent the ensemble classifier from needlessly consuming computational overhead by using redundant sensors in the online training process. Our main contributions are:</p><p>• We combine the sensing capability of on-body TinyOSbased motes with the sensors, computational power, portability, and user interface of an Android smartphone. • An activity recognition approach appropriate for lowpower wireless motes and mobile phones that does not rely on a backend server. Our approach handles BSN dynamics without sophisticated parameter tuning and also accurately classifies difficult tasks.</p><p>• We provide retraining detection without requesting ground truth from the user, reducing the invasiveness of the system. • We reduce online training costs by detecting redundant sensors and excluding them from the ensemble classifier. • With two weeks of data from two subjects, we demonstrate that we can detect even the most difficult activities with nearly 90% accuracy. We identify 40% of sensors as redundant, excluding them from online training. The rest of this paper is organized as follows: Section 2 presents related work, Section 3 provides an overview of our PBN system design, and Section 4 describes the ensemble classifier. We describe our retraining detection method in Section 5, provide details on how to collect new training data in Section 6, and present a sensor selection method in Section 7. In Section 8, we evaluate our PBN platform and present conclusions and future work in Section 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Many methods perform classification with multiple onbody sensor nodes but have no mobile, on-body aggregator for sensor control and activity recognition feedback. Some works <ref type="bibr" target="#b8">[9]</ref> [15] <ref type="bibr" target="#b27">[28]</ref> [22] use multiple on-body sensor motes to detect user activities, body posture, or medical conditions, but such motes are only used to collect and store data with analysis performed offline. Such approaches limit user mobility due to periodic communication with a fixed base station and also lack real time analysis and feedback to the user.</p><p>Other approaches use mobile phones for sensing and classification, but require the use of backend servers or offline analysis to train or update classification models. The authors of <ref type="bibr" target="#b17">[18]</ref> provide model training with a short amount of initial data, but model updating is performed using a backend server. In <ref type="bibr" target="#b18">[19]</ref>, model training and classification is split between lightweight classifiers on a mobile phone and more powerful classifiers on a server. The authors of <ref type="bibr" target="#b20">[21]</ref> present a sensor mote with an SD card attachment for interface with a mobile phone but do not provide an application which uses such a device. Mobile phone like hardware is used in <ref type="bibr" target="#b6">[7]</ref> to perform pothole detection, but data is analyzed offline. Some works use a limited set of sensor modalities or use a separate classifier for each sensing modality, making classification difficult for deployments with a large number of heterogeneous sensors. Some works <ref type="bibr" target="#b1">[2]</ref> [12] focus extensively on energy aware sensing models specifically for localization or location tracking. In <ref type="bibr" target="#b15">[16]</ref>, while the authors provide an adaptive classification approach, they only make use of a mobile phone microphone to recognize user context and activities, thus eliminating a wide range of user activities that are not sound dependent. The authors of <ref type="bibr" target="#b16">[17]</ref> provide a component-based approach to mobile phone based classification for different sensors and different applications, but each sensor component requires a separate classifier implementation. One approach uses both motes and mobile phones to perform fitness measurement <ref type="bibr" target="#b5">[6]</ref>, but simple sensing models are used that will not work for more general activity recognition applications. Activity recognition and energy expenditure is calculated in <ref type="bibr" target="#b0">[1]</ref> using an accelerometerspecific sensing model for on-body wireless nodes.</p><p>Lastly, several activity recognition methods do not provide online training to account for environmental dynamics or poor initial training data. The authors of <ref type="bibr" target="#b13">[14]</ref> use Ada-Boost to perform activity recognition but use custom hardware with very high sampling rates. In <ref type="bibr" target="#b19">[20]</ref>, the authors also use AdaBoost for activity recognition with mobile phones, but focus mainly on ground truth labeling inconsistencies and do not provide a practical system for long term use. The authors of <ref type="bibr" target="#b26">[27]</ref> focus on duty cycling mobile phone sensors to save energy, but provide a rigid rule-based recognition model that must be defined before runtime. A speaker and emotion recognition system using mobile phones is presented in <ref type="bibr" target="#b22">[23]</ref> which implements adaptive sampling rates but the classification models used are trained offline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">System Overview and Architecture</head><p>In this section, we first present the application requirements. We then present our PBN hardware and application, which unifies TinyOS and Android. Next, we describe our PBN architecture and finally describe our PBN experimental setup which we refer to for the remainder of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Application Requirements</head><p>Our PBN system design is motivated by the requirements of a practical body networking application for activity recognition. Data from multiple on-body sensors is reported to a mobile aggregator which makes classification decisions in real time. The system must be able to accurately and efficiently classify typical daily activities, postures, and environmental contexts, which we present in Table <ref type="table" target="#tab_0">1</ref>. Despite these categories being common for many individuals, previous work <ref type="bibr" target="#b13">[14]</ref> has identified some of them, such as cleaning, as especially difficult to classify. From the table, we break down our target classifications into three categories: Environment, Posture, and Activity. With the Environment and Posture categories, we can provide insight into the physical state of the user for personal health and physical fitness monitoring applications. We also wish to measure typical activities in which a user engages, such as watching TV, driving, meeting with colleagues, and cleaning. Here, we consider cycling and walking to be both postures and activities. Such activity recognition is quite useful for participatory sensing and social networking applications since it eliminates the need for a user to manually update his or her activities online. The requirements to provide such a practical activity recognition system are:</p><p>• User-friendly. Different on-body commercial off-theshelf (COTS) sensor nodes must seamlessly interact with a mobile phone aggregator for simple user configuration and operation. The hardware must be portable and easy to wear, while the software must provide an intuitive interface for adding, removing, and configuring different sensors geared to detect the user's intended activities.</p><p>Labeling training data should also be a simple and noninvasive task, facilitated by the mobile phone. • Accurate classification. The system must accurately handle both easy and difficult to detect activities as well as noisy data and environmental dynamics. The system must also account for the changing geographic location of the user as well as the variable orientation of the individual on-body sensors. • Efficient classification. A body sensor network for activity recognition may often use low power hardware, therefore, the classification algorithm should be computationally efficient in addressing the BSN dynamics of geographic location, user biomechanics, and environmental noise. The system must also be energy efficient: by quantifying the contribution of each sensor towards accurately classifying activities, sensors with minimal contribution can be powered down. Furthermore, the system must avoid extensive parameter tuning as well as avoid unique, complex sensing models for each activity. • Less reliance on ground truth. Activity recognition systems are often deployed with minimal labeled training data, thus the need exists to train a system in an online manner, requesting ground truth labels only when absolutely necessary. A reduced need for ground truth reduces the burden on the user to label training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">PBN Hardware and Application</head><p>To achieve accurate and efficient activity recognition for mobile phones and on-body sensors, we provide an extensive hardware and software support system, which we describe in this section. In Section 3.2.1, we describe the implementation of USB host mode in the Android kernel to allow communication between a base station mote and an Android phone. In Section 3.2.2, we describe our Android application for configuration and activity recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Unification of Android and TinyOS</head><p>Our PBN system consists of Crossbow IRIS on-body sensor motes and a TelosB base station connected to an Android HTC G1 smartphone via USB. While previous work has connected TinyOS motes and mobile phones, such efforts either use energy demanding Bluetooth <ref type="bibr" target="#b5">[6]</ref>, do not provide mobile phone support for TinyOS packets <ref type="bibr" target="#b23">[24]</ref>, or use special hardware <ref type="bibr" target="#b28">[29]</ref>. Instead, we provide a seamless and efficient integration of TinyOS motes and Android smartphones. Our solution can be easily extended beyond the research-based TinyOS devices to work with a wide variety of commercial and more ergonomic USB and wireless sensors. Our challenges with such integration lie in 4 aspects: TinyOS sensing support, Android OS kernel support, Android hardware support, and TinyOS Java library support.</p><p>TinyOS Sensing Support. Illustrated in Figure <ref type="figure" target="#fig_2">3</ref>, we implement a sensing application in TinyOS 2.x for Crossbow IRIS motes with attached MTS310 sensorboards. The sensor node application allows for runtime configuration of active sensors, sampling rates, and local aggregation meth-ods. We develop a communication scheme to pass control and data packets through a TelosB base station connected to an Android smartphone.</p><p>Android OS Kernel Support. To prepare the phone to support external USB devices, a kernel EHCI or host controller driver is required. However, the USB host controller hardware documentation of the Google Developer phone is not publicly available. We incorporate the suggestions from <ref type="bibr" target="#b4">[5]</ref> and modify the Freescale MPC5121 driver to work with the Qualcomm MSM7201A chipset on the Android phone. With these modifications, the host controller is able to recognize USB devices with a caveat: enabling the HOST controller disables the USB client mode for PC SD card access.</p><p>Hardware Support. The EHCI controller of the phone does not provide any power to external devices, such as the sensor motes in our case. To solve this limitation, we build a special USB cable that includes an external 5V battery pack with a peak load of 0.5A. The positive and ground cables are cut on the phone side such that only the USB device, not the phone, receives power. This also has the added benefit of not placing an extra load on the phone battery.</p><p>TinyOS Support on Android. Two challenges exist in providing Android TinyOS communication support: systems implementation and TinyOS software modifications. 1) Each mote has an FTDI USB serial chip that can be used to communicate with the host. The Linux FTDI driver creates a serial port device in the /dev directory. Android includes a minimal device manager that creates devices with insufficient privileges. This device manager is modified so that correct privileges are established. 2) TinyOS uses a binary serialization to communicate bidirectionally between the mote and host with the help of a C++ JNI interface. We modify the TinyOS JNI interface and associated Java libraries to compile and function on the Android platform. With such modifications, Android applications can send and receive TinyOS packets using the same Java interfaces available on a PC.  To provide a user-friendly front end for PBN, we implement an Android app to allow for easy configuration, run-time deployment, ground truth labeling, and data storage and upload for offline analysis. The GUI component allows for user control of both phone and remote wireless sensors. The GUI also receives feedback as to the current activity and whether or not classifier retraining is needed, and also provides ground truth labels to the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Android App</head><p>Sensor Configuration. Our PBN Android app provides an interface to allow for easy configuration of both phone sensors as well as remote TinyOS sensors. Using our software interface, a user can easily add or remove TinyOS nodes as well as phone and TinyOS-based sensors. The user is also able to adjust sampling rates as well as local data aggregation intervals to reduce the number of radio transmissions. A user's sensor and sampling configuration can be stored on the phone in an XML file so that configuration must only be performed once. Users of different phones can also exchange saved sensor configurations.</p><p>Runtime Control and Feedback. With a created sensor configuration, a PBN user is able to quickly start and stop data sampling and activity recognition. As depicted in Figure <ref type="figure" target="#fig_0">1</ref>, the current activity is displayed during runtime along with a configurable histogram of the most recent activities. When our classifier determines that accuracy is dropping and retraining with more labeled ground truth is needed, the PBN app prompts the user to input the current activity, illustrated in Figure <ref type="figure" target="#fig_1">2</ref>. During retraining, an indicator and button on the current activity screen appears, allowing the user to log any changes in the ground truth. Labeled training data can be stored locally for later retraining or uploaded to a server for sharing and offline analysis. Phone and mote sensors (white dotted areas) sample data at a user configured rate and aggregate data from each sensor over a larger aggregation interval. Aggregated data for each sensor is returned at each aggregation interval. For each TinyOS mote, aggregated data is returned wirelessly in a single packet. We provide a reliable communication scheme between the phone and motes and determine through our 2 week experiment that for most activities, packet loss rates are below 10%, with 99% of packets received after one retransmission, even when using the lowest transmission power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">PBN Architecture</head><p>Aggregated data from phone and mote sensors is fed into the PBN classification system (gray dotted area in Figure <ref type="figure" target="#fig_2">3</ref>) at each aggregation interval to make a classification decision. The classifier, AdaBoost, is initially trained with the user labeling a short two minute period of each pre-defined activity (Ground Truth Management) but training can be updated online through Retraining Detection. During initial training and retraining, the Sensor Selection module reduces the training overhead incurred by AdaBoost by choosing only the most capable sensors for performing accurate classification. We now describe the core of our PBN platform:</p><p>Activity Classification. We use AdaBoost <ref type="bibr" target="#b7">[8]</ref>, an ensemble learning algorithm, as our activity recognition classifier which resides entirely on a smartphone. AdaBoost combines an ensemble of weak classifiers together to form a single, more robust classifier. With this approach, we are able to train weak classifiers for each sensor in our deployment and combine them together to recognize activities. Ada-Boost is able to maximize training accuracy by selecting only the most capable sensors for use during runtime. We improve upon AdaBoost by providing online training and retraining detection as well as improve computational overhead through sensor selection.</p><p>Retraining Detection. Since initial training data may not be sufficient to account for body sensor network dynamics, AdaBoost retraining is needed in order to ensure high accuracy throughout the deployment lifetime. We investigate the discriminative power of individual sensors and from our analysis we are able to detect when retraining is needed during runtime without the use of labeled ground truth. For each sensor, we use Kullback-Leibler divergence <ref type="bibr" target="#b12">[13]</ref> to determine when runtime data is sufficiently different from training data and use a consensus-based approach to initiate retraining when enough sensors indicate that retraining is needed.</p><p>Ground Truth Management. When retraining is needed, we investigate how much new data to collect and label in order to ensure BSN dynamics are captured, yet minimize the intrusiveness of the user manually annotating his or her current activities. We also determine how to maintain a balance of training data for each activity to ensure AdaBoost trains properly and provides maximum runtime accuracy.</p><p>Sensor Selection. During training, AdaBoost trains multiple weak classifiers for every sensor in the deployment, even if many sensors are never chosen by AdaBoost when training is complete. Through analysis of the theory behind ensemble learning, we identify both helpful and redundant sensors through the use of the Pearson correlation coefficient <ref type="bibr" target="#b24">[25]</ref>. Based on our analysis, we provide a method to give AdaBoost only the sensors that provide a significant contri-bution towards maximizing accuracy, thus reducing online training overhead.  For the remainder of the paper, we will refer to our activity recognition experiment, in which we collected two weeks of sensor data using two subjects, depicted in Figures <ref type="figure" target="#fig_3">4</ref> and<ref type="figure" target="#fig_4">5</ref>. Each subject wore five Crossbow IRIS motes wirelessly linked to a TelosB base station and Android HTC G1 smartphone. The mote and sensor configuration for our experiment is summarized in Table <ref type="table" target="#tab_1">2</ref>. On the phone, which we attach to the waist, we make use of the 3-axis accelerometer as well as velocity from WiFi and GPS, with GPS active only when PBN detects the user is outdoors. On the mote, we use an MTS310 sensorboard with the following sensors: 2-axis accelerometer, microphone, light, and temperature. In addition to the sensors on the mote, the base station also collects RSSI information from each received packet, which has been previously demonstrated <ref type="bibr" target="#b21">[22]</ref> to provide insight into body posture. During initial and online training, all sensors are active, while only sensors selected during training remain active during the remaining sampling periods. For the microphones and accelerometers, raw ADC values are sampled at 20ms intervals to ensure quick body movements can be captured, with light and temperature ADC readings sampled at 1s intervals, and GPS/WiFi sampled every 10s. To reduce communication overhead, data for each sensor is aggregated locally on each node at 10s intervals, which is well within the time granularity of the activities we classify. To reduce the complexity of local aggregation, data from each accelerometer axis is treated as a separate sensor. During local aggregation, light and temperature sensor readings are averaged since these sensor readings remain relatively stable for each activity. Except for GPS/WiFi, all other sensors compute the difference between the highest and lowest readings for each aggregation interval, for the change in readings indicate body movement or sound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Experimental Setup</head><p>During the two week period, both subjects recorded all activity ground truth in order to evaluate the accuracy of training data (training accuracy) and runtime accuracy. We recorded ground truth for 3 classification categories, illustrated in Table <ref type="table" target="#tab_0">1</ref>: Environment, Posture, and Activity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">AdaBoost Activity Recognition</head><p>The core of our activity recognition approach uses ensemble learning, specifically AdaBoost.M2 <ref type="bibr" target="#b7">[8]</ref>, which we expand and improve upon in subsequent sections. In this section, we explain how we adapt AdaBoost to run on a phone for use with a body sensor network. AdaBoost is lightweight enough for mobile phones, yet previous work <ref type="bibr" target="#b13">[14]</ref> [20], while relying on offline processing with no feedback or user control, has demonstrated AdaBoost to be accurate for classification applications. Furthermore, without user parameter tuning, our implementation of AdaBoost is able to choose the right sensors needed to maximize training accuracy for all activities. Other classifiers commonly used for activity recognition use a combination of complex, specialized classifiers per sensor modality <ref type="bibr" target="#b16">[17]</ref> [19] or per activity <ref type="bibr" target="#b15">[16]</ref> which require extensive parameter tuning. Other techniques use single classifiers which are computationally demanding for mobile phones, such as GMMs <ref type="bibr" target="#b17">[18]</ref> or HMMs <ref type="bibr" target="#b8">[9]</ref>.</p><p>Using AdaBoost, we incrementally build an ensemble of computationally inexpensive weak classifiers, each of which is trained from the labeled training observations of a single sensor. Weak classifiers need only to make classification decisions that are slightly correlated with the ground truth; their capabilities are combined to form a single accurate classifier. The completed ensemble may contain multiple weak classifiers for the same sensor; some sensors may not have trained classifiers in the ensemble at all. AdaBoost incrementally creates such sensor-based weak classifiers by emphasizing the training observations misclassified by previous classifiers, thus ensuring that training accuracy is maximized.</p><p>Using Algorithm 1, we describe AdaBoost training. We define a set of activities A = {a 1 ,... ,a a }, sensors S = {s 1 ,... ,s m }, and observation vectors O j for each sensor s j ∈ S, where each sensor has n training observations. The training output is an ensemble of weak classifiers H = {h 1 ,... ,h T }, where h t ∈ H represents the weak classifier chosen in the t th iteration. We initialize a set of equal weights D 1 for each training observation, where during the training process, greater weights for an observation represent greater classification difficulty.</p><p>During each iteration t, we train a weak classifier h t, j for each sensor s j ∈ S using observations O j and weights D t . We then compute the weighted classifier error ε t, j for each trained sensor classifier, adding only the sensor classifier to H which has the lowest weighted error. Before the next iteration, the observation weights D t are updated based on the current weights and the misclassifications made by the selected classifier. for sensor s j ∈ S do 4:</p><p>Train weak classifier h t, j using obs. O j , weights D t 5:</p><p>Get weighted error ε t, j for h t, j using labels <ref type="bibr" target="#b7">[8]</ref> 6:</p><p>end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>Add the h t, j with least error ε t to H by choosing h t, j with least error ε t 8:</p><p>Set D t+1 using D t , misclassifications made by h t [8] 9: end for Given an observation o, each weak classifier returns a probability vector [0, 1] a with each scalar representing the probability that the current activity is a i . To train a weak classifier h t, j for each sensor s j ∈ S, we use a naive Bayes model where training observations O j are placed into one of 10 discrete bins. The bin interval is based on the minimum and maximum possible readings for each sensor. Each binned training observation from O j is assigned its respective weight from D t and ground truth label in the set of activities A. A new observation is classified by placing it into a bin, with the generated probability output vector corresponding to the weights of training observations present for each activity in the assigned bin. With a weak classifier chosen for each iteration, Equation 1 defines the output of the AdaBoost classifier for each new observation o during runtime:</p><formula xml:id="formula_0">h(o) = argmax a i ∈A T ∑ t=1 log 1 -ε t ε t h t (o, a i )<label>(1)</label></formula><p>In Equation <ref type="formula" target="#formula_0">1</ref>, the activity probability vector for each weak classifier h t is weighted by the inverse of its error ε t . Thus, the weak classifiers with the lowest training error have the most weight in making classification decisions. To put it another way, AdaBoost chooses the sensors with weak classifiers that minimize weighted training error, achieving maximum training accuracy for all activities.</p><p>In Figure <ref type="figure">6</ref>, we depict AdaBoost training and runtime performance for T = 1 to 300 AdaBoost iterations using data from Subject 1 and ground truth from the Activity classification category. For this paper, we use 300 iterations to achieve maximum accuracy. We also show in Figure <ref type="figure">6</ref> that AdaBoost does not choose all of the 34 sensors in our experiment, for even with 300 iterations it only chooses 18. During runtime, any sensor not selected by AdaBoost will be disabled to save energy and communication costs.</p><p>Lastly, in Figure <ref type="figure">7</ref>, we demonstrate that AdaBoost can perform very accurately with a sizable amount of training data: maximum training and runtime accuracy is achieved with roughly 50 observations per activity for Subject 1.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Retraining Detection</head><p>In this section, we propose an online training approach to achieve high accuracy with a limited initial training data set. This approach can also be used to retrain when an existing data set is not accurate enough. First, we investigate how to quantify the discriminative power of each sensor to detect when runtime data is significantly different than training data. Second, we use the insight we gain from the discrimination analysis to detect when AdaBoost retraining is needed during runtime without retrieval of ground truth information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Sensor Data Discrimination Analysis</head><p>We investigate how to quantify the discriminative power of each sensor and predict if it is accurate, employing the use of Kullback-Leibler divergence <ref type="bibr" target="#b12">[13]</ref>. K-L divergence measures the expected amount of information required to transform samples from a distribution P into a second distribution Q. Using trace data from Subject 1, we demonstrate the use of K-L divergence per activity to identify which sensors are best able to distinguish between activities. We also show a clear relationship between K-L divergence per activity and training accuracy. We conclude that K-L divergence can be used to detect when retraining is needed without regular ground truth requests to compute classifier accuracy: sensors need only to compare training data to current runtime data.</p><p>In Figure <ref type="figure">8</ref>, we analyze the ability of each sensor to discriminate between different activities; we calculate K-L divergence as "one against the rest," for one data distribution is calculated for the target activity and another distribution is calculated for all other activities. For all analyses using K-L divergence, we discretize continuous-valued sensor data into 100 bins. The figure shows that some sensors, such as those on the hands (nodes 1 and 2) have poor ability to distinguish between any activity. Conversely, sensors on the feet (nodes 3 and 4) are especially good at detecting activities that involve motion, such as walking and cycling. The GPS/WiFi velocity (0-LOC) has the highest K-L divergence of all for detecting driving, with a value of nearly 14.</p><p>In Figure <ref type="figure" target="#fig_7">9</ref>, we compare K-L divergence per sensor and activity to individual sensor training accuracy using Nearest Centroid <ref type="bibr" target="#b10">[11]</ref>. Nearest Centroid is a variant of k-means clustering and unlike the AdaBoost weak classifiers, it is an inexpensive classifier that does not require a weight for each training observation. The figure shows a clear relationship between K-L divergence and runtime accuracy, indicating that for a given activity, sensors with high K-L divergence are much more likely to have high accuracy compared with sensors with low K-L divergence. As with Figure <ref type="figure">8</ref>, activities involving motion, such as cycling, walking, and driving are most easily distinguished and have the highest accuracy. From Figures <ref type="figure">8</ref> and<ref type="figure" target="#fig_7">9</ref>, we can conclude that K-L divergence will allow sensors to tell activities apart and can even be used to tell if training data and runtime data for the same activity are sufficiently different that retraining is needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Consensus-based Detection</head><p>During runtime, retraining detection is performed in two steps, using the insight gained in Section 5.1. First, at each aggregation interval, each active sensor independently determines if retraining is needed. Second, for some aggregation interval, if enough sensors determine that retraining is needed, PBN prompts the user to record ground truth for a short period. During this period, all sensors are woken up and sample data which is then labeled by the user. When the retraining period completes, a new AdaBoost model is trained using both the old and new training data.</p><p>Step 1. Individual Sensor Retraining Detection. In Section 5.1, we demonstrate in that K-L divergence is a powerful tool for determining sensor classification capability. Here, we use the discriminative power of K-L divergence to determine when retraining is needed for each sensor. For a sensor to determine retraining is needed, two conditions must hold: 1) The runtime data distribution for the current activity must be significantly different from the training data distribution, and 2) The runtime data distribution must have at least as many observations as the training data distribution.</p><p>During training, each sensor chosen by AdaBoost computes K-L divergence for each activity using its labeled sensor training data. Training K-L divergence per activity is used as a baseline to compare against during runtime to determine when retraining is needed. For each activity a i ∈ A, training sensor data distribution T i for activity a i , and training sensor data distribution T o for all activities a j ∈ A \ {a i }, each sensor computes the K-L divergence between T i and T o , D KL (T i , T o ). Then, during runtime, for each new observation, AdaBoost classifies the observation as activity a i ∈ A, and each active sensor adds its data to a runtime distribution R i for activity a i . An active sensor s j determines retraining is needed when the runtime-training K-L divergence is greater than the training K-L divergence for the current activity a i :</p><formula xml:id="formula_1">D KL (R i , T i ) &gt; D KL (T i , T o ).</formula><p>During runtime, to ensure a fair comparison between training and runtime K-L divergence, each sensor does not determine if retraining is needed until the runtime distribution R i has as at least as many observations as the training data distribution T i . We determine through evaluation that collecting fewer runtime observations per activity yields similar accuracy but more retraining instances, which imparts a greater burden on the user. Since Figure <ref type="figure">7</ref> demonstrates that 100 observations per activity is more than sufficient to achieve maximum runtime accuracy (this is also true for Subject 2), we limit training data to 100 observations per activity to reduce delay before the training observations and runtime observations are compared.</p><p>Step 2. Consensus Detection: Combining Individual Sensor Decisions. During each runtime interval, PBN checks to see how many sensors indicate retraining is necessary. If a weighted number of sensors surpasses a threshold, new ground truth is collected and a new AdaBoost classifier is trained. We describe how this consensus threshold for retraining is obtained.</p><p>First, upon completion of AdaBoost training, we can determine the AdaBoost weight of the sensor classifiers used to make correct classification decisions. Through an extension of Equation 1, in Equation <ref type="formula" target="#formula_2">2</ref>, we can output the weight of the correct weak classifiers given an observation o and correct AdaBoost decision a i , w(o, a i ):</p><formula xml:id="formula_2">w(o, a i ) = T ∑ t=1 log 1 -ε t ε t h t (o, a i )<label>(2)</label></formula><p>Next, using training data, a trained AdaBoost classifier, and Equation 2, we determine the average weight w avg for all correct training classification decisions (o, a i ). To do this, we compute a weight w j for each active sensor s j , indicating how important its decisions are relative to other sensors. Depicted in Equation 3, w j is computed as the sum of the inverse error rates for each weak classifier belonging to sensor s j (multiple classifiers for s j may be iteratively trained and chosen by AdaBoost). Function f ( j) maps sensor s j to each AdaBoost iteration t where AdaBoost chooses the weak classifier trained by s j .</p><formula xml:id="formula_3">w j = ∑ ∀t∈ f ( j) log 1 -ε t ε t (3)</formula><p>At each runtime interval, we sum the weights w j for each sensor s j that indicates retraining is necessary. If the sum of the sensor weights is greater than w avg , PBN notifies the user to collect ground truth and retrain a new AdaBoost classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Ground Truth Management</head><p>We address two issues regarding labeling and addition of new sensor data to the existing training dataset during retraining. First, we address how much new data to add to the training set. Second, we show how to maintain a balance between training data set sizes for each activity to ensure Ada-Boost retrains properly and maintains high runtime accuracy.</p><p>When PBN decides retraining is required, it will prompt the user to log ground truth for a window of N aggregation intervals. Through evaluation, we find that recording ground truth retroactively for the data that triggered the retraining results in no change in accuracy since any such significant change in sensor data is persistent. During the ground truth logging period, a user only notes the current activity at the start of the logging period and any activity changes throughout the remainder of the logging period. In the evaluation, we determine that with N = 30 (5 minutes), the retraining ground truth collection window is short enough not to be intrusive to the user but long enough to capture changes in PBN dynamics. When the ground truth labeling period is complete, the new labeled sensor data is added to the existing training set and a new AdaBoost model is trained.</p><p>Since the core of AdaBoost training relies on creating a weight distribution for all training observations based on classification difficulty, each activity must be given nearly equal amounts of training data (within an order of magnitude) for AdaBoost to train properly <ref type="bibr" target="#b9">[10]</ref>. Without a balance in training observations across all activities, AdaBoost will focus on training activities with more training observations, creating poor runtime accuracy for activities with fewer training observations. However, if we are too restrictive in enforcing such a balance, very few new training observations will be added to the training dataset during retraining, resulting in poor adaptation to the new data. In Equation <ref type="formula">4</ref>, we ensure that each activity a i ∈ A has no more than δ times the average number of training observations per activity, where O is the set of training observations.  The limit imposed in Equation <ref type="formula">4</ref>allows AdaBoost to place equal emphasis on training weak classifiers for each activity during retraining. If, during retraining, an activity exceeds this limit, data is removed until the number of observations is under the limit. We remove observations at random to reach the activity limit since we do not know how representative each observation is of its labeled activity; some observations may contain more noise than others.</p><formula xml:id="formula_4">|O i | -1 |A| ∑ ∀a k ∈A |O k | 1 |A| ∑ ∀a k ∈A |O k | ≤ δ (4) 0,P-ACC-X 0,P-ACC-Y 0,P-ACC-Z 1,RSSI 1,ACC-X 1,ACC-Y 1,MIC 1,LIGHT 1,TEMP 2,RSSI 2,ACC-X 2,ACC-Y 2,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Sensor Selection for Efficient Classification</head><p>While AdaBoost, through training, provides a measure of sensor selection by weighting the most accurate sensors, this approach can be computationally demanding: at each AdaBoost training iteration, a weak classifier is trained and evaluated for each sensor. In this section, we focus on identifying redundant sensors and excluding them from AdaBoost training to reduce the number of weak classifiers trained by AdaBoost. To achieve this goal, we first investigate why ensemble learning algorithms are successful: weak classifiers must be relatively accurate and different weak classifiers must have diverse prediction results <ref type="bibr" target="#b29">[30]</ref>. Second, by identifying sensors that satisfy these properties, we provide a method to detect redundant sensors during runtime and exclude them from input to AdaBoost during online retraining. Our method adapts to BSN dynamics during runtime to ensure only helpful sensors are used by AdaBoost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Identifying Sensing Redundancies</head><p>With ensemble learning and AdaBoost, each weak classifier in the ensemble must be slightly correlated with the true classification to achieve high runtime accuracy. Since we use data from a single sensor to train each weak classifier, it follows that sensors chosen by AdaBoost will be similarly correlated both in terms of raw data and in classification decisions made by each weak classifier. While the Pearson correlation coefficient <ref type="bibr" target="#b24">[25]</ref> is also used in previous works <ref type="bibr" target="#b25">[26]</ref> [31] to identify packet loss relationships between different wireless radio links; here we use correlation to identify sensing relationships between different sensors to identify both sensors that are helpful as well as redundant.</p><p>Figure <ref type="figure" target="#fig_8">10</ref> depicts the correlation between each sensor pair using the raw sensor data collected from Subject 1. It is noted that several correlation patterns exist: accelerometers are strongly correlated as are light and temperature sensors. We can use this information to find sensors with redundant data and remove them from the AdaBoost training process to save computational overhead as well as energy consumption. We next illustrate that not only do correlation patterns exist between raw sensor data but also between sensor and sensor cluster classifier decisions. In Figure <ref type="figure" target="#fig_9">11</ref>, with data from Subject 1, we show that when we add a new sensor to an existing sensor cluster, the greatest accuracy increase is achieved when the sensor and cluster have uncorrelated classification decisions. The figure shows the decision correlation and accuracy change for nearly 7,000 randomly generated clusters from size 1 through 19 using data from Subject 1, with individual sensor and sensor cluster classifiers trained using Nearest Centroid <ref type="bibr" target="#b10">[11]</ref>. To compute the decision correlation for a classifier, each correct decision is recorded as 1 and each incorrect decision is recorded as 0. From the figure, it is clear that choosing sensors with a decision correlation close to 0 can help select sensors that will make the most contribution towards an accurate sensor cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Adaptive Runtime Sensor Selection</head><p>We now describe how to reduce the number of sensors given as input to AdaBoost during online retraining, reducing retraining overhead while achieving high runtime accuracy. We perform sensor selection using raw sensor data correlation, also extending the concept to sensor and sensor cluster classifier decision correlation.</p><p>Sensor selection consists of two components: Threshold Adjustment and Selection. During Threshold Adjustment, using a trained AdaBoost classifier, a threshold α is computed which discriminates between sensors chosen by Ada-Boost and unused sensors. During Selection, a previously computed α value is used to select a set of sensors for input to AdaBoost during retraining. Threshold Adjustment is performed during initial training while Selection is performed during subsequent retrainings. To ensure the selection threshold stays current with BSN dynamics, we update the threshold α periodically during runtime retraining using Threshold Adjustment and apply smoothing using a moving window of previous α values.</p><p>Threshold Adjustment. During initial training, we initialize a sensor selection threshold α and update it during runtime retraining to adjust to any changes in user geographical location, biomechanics, or environmental noise. To de- </p><formula xml:id="formula_5">= μ R + nσ R</formula><p>termine the threshold α, we first train AdaBoost with all sensors as input and determine S, the set of sensors selected by AdaBoost. Using Algorithm 2, we determine the correlation coefficient r of raw sensor training data for each combination of sensor pairs in S. With each correlation coefficient r stored in set R for all sensors selected by AdaBoost, we determine the mean correlation coefficient μ R and standard deviation σ R . We then set the threshold α to be n times the standard deviation above the mean correlation. We determine through empirical evaluation that n = 2 is sufficient to include nearly all sensors selected by AdaBoost but exclude unused sensors.</p><p>Selection. During retraining, when the threshold is not being updated, we choose a set of sensors S * from the set of all sensors S using the previously computed threshold α. The selected set S * is given as input to AdaBoost to reduce the overhead incurred by training on the entire set S. In Algorithm 3, we ensure that no two sensors in S * have a correlation coefficient that is greater than or equal to the threshold α, since we demonstrate previously in Section 7.1 that correlation closest to 0 yields the most accurate sensor clusters. To enforce the threshold, we maintain a set E, which contains sensors that have a correlation coefficient above the threshold for some sensor already in S * . For each sensor pair in S, we determine the correlation coefficient r and add pairs to S * where r &lt; α. When r ≥ α for some sensor pair, we add the less accurate sensor to E as determined by Nearest Centroid <ref type="bibr" target="#b10">[11]</ref> and the other to S * as long as it is not already in E.</p><p>Pair DC. We also propose two other correlation metrics with which to select sensors: pair decision correlation (Pair DC) and cluster decision correlation (Cluster DC). With these two metrics, we select sensors based on classification decisions made by individual sensors and sensor clusters. For a sensor or cluster classifier trained using Nearest Centroid, we can convert each training data decision into an integer value as in Section 7.1 to use in computing decision correlation. Sensor selection using sensor pair decision correlation is identical to raw data correlation, except for the use of individual sensor classifier decisions rather than raw data.</p><p>Cluster DC. For sensor cluster decision correlation, we modify Algorithms 2 and 3. Instead of iterating through all sensor pair combinations, we first find an individual sensor classifier with the highest accuracy using Nearest Centroid and add it to a trained sensor cluster C * . We then incremen- end if 16: end for tally add sensors to C * as long as the sensor has a decision correlation with the cluster that is below the threshold. The final cluster is then used by AdaBoost for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Evaluation</head><p>We evaluate our PBN activity recognition system using the configuration and data collection methods described in Section 3.4, using two weeks of activity recognition data and two subjects, one of whom is not an author. While these two subjects have substantially different routines and compose very different evaluation scenarios, we leave a more broadly focused evaluation with more subjects to future work. We first evaluate classification performance with a good initial training dataset in Section 8.1 and show that PBN can achieve good accuracy for even difficult to classify activities. In Section 8.2, we evaluate online training with a limited initial training dataset and compare our PBN retraining detection method to a periodic retraining approach, illustrating the benefits of PBN retraining detection. We then show the effectiveness of our sensor selection approach in Section 8.3 and evaluate our PBN application performance in terms of mobile phone hardware constraints in Section 8.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Classification Performance</head><p>In Figure <ref type="figure" target="#fig_11">12</ref>, we first compare performance for both subjects in the three classification categories depicted in Table <ref type="table" target="#tab_0">1</ref>: Environment, Posture, and Activity, showing that PBN is able to achieve high runtime accuracy with a good initial training dataset (100 observations per activity) along with no online training or sensor selection. Subject 2 does not perform the cycling, lying down, cleaning, or reading categories, hence there is no histogram bar. In addition to total accuracy, we plot accuracy, precision (true positive/(true positive + false positive)), and recall (true positive/(true positive + false negative) for each activity.</p><p>Each classification category in Figure <ref type="figure" target="#fig_11">12</ref> has similar performance characteristics per subject: Subject 1 has total accuracies of 98%, 85%, and 90% for the Environmental, Pos-  ture, and Activity categories, while Subject 2 has respective total accuracies of 81%, 82%, and 76%. Interestingly, Subject 1 performs significantly better than Subject 2 since Subject 2 often performs each activity less cleanly, for example, working with different lights on in a room or eating in different locations, sometimes with friends. Lastly, more complex activities, which involve a combination of different physical movements as well as external sounds and light, perform reasonably well in comparison with more easily classified activities. Subject 1 has over 90% accuracy for complex activities like cleaning, eating, meeting, reading, and watching TV. Subject 2 also has over 80% accuracy for complex activities like eating, meeting, and watching TV. We present a classification timeline for Subject 1 using the Activity category in Figure <ref type="figure" target="#fig_12">13</ref>, comparing PBN classifications with ground truth. Here, it is apparent that activities involving movement, such as cycling, walking, and driving, are classified with few errors. More misclassifications are seen for more complex activities, such as working confused with meeting, cleaning, and watching TV, however, as illustrated in Figure <ref type="figure" target="#fig_11">12</ref>, overall accuracy for each of these complex activities is near or above 90%.</p><p>In Figure <ref type="figure" target="#fig_13">14</ref>, we show the normalized AdaBoost weights for each sensor for Subject 1 and the Activity classification category as calculated in Equation 3. In addition to the weights for the total of all classifications, we also compute the normalized weight of correct decisions made by each sensor for each activity using runtime data. This figure indicates that AdaBoost is able to select the right sensors to maximize training and runtime accuracy and exclude 16 unhelpful sensors (shown in black). The figure also shows heavy reliance on the light and temperature sensors to distinguish between indoor and outdoor activities as well as reliance on the accelerometers to detect activities involving motion, such as walking, driving, and cycling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Online Training</head><p>In this section, we demonstrate that we can use a small training dataset, perform online training through retraining detection, and achieve similar accuracy as if we had a larger initial training data set. In this section, we focus on the Activity classification category and also initialize each runtime configuration with 10 random training data samples for each activity. To limit AdaBoost training overhead, we enforce a maximum of 100 training observations per activity since Figure <ref type="figure">7</ref> demonstrates that this number is more than enough to achieve maximum runtime accuracy. Since we are using random initial training data, we compute the average and standard deviation for 10 runs of each configuration.</p><p>Ground Truth Management. First, we investigate the retraining window size described in Section 6 for collecting new ground truth in Figure <ref type="figure" target="#fig_15">15</ref>. From the figure, larger window sizes mean significantly less retraining and computational overhead but also less runtime accuracy. We use a smaller window size of 30 new data elements per retraining   to balance accuracy and computational overhead. We argue that roughly 20-40 retraining instances per subject over a two week period as per Figure <ref type="figure" target="#fig_15">15</ref> is an inconsequential burden, since the subject must only interact with the phone once per instance to input his or her current activity. Next, we show the ideal training data balance restriction δ, from Equation 4, in Figure <ref type="figure" target="#fig_0">16</ref>. As δ approaches 2.0, runtime accuracy increases and the number of retraining instances decreases. We choose a δ = 2.0 to achieve high accuracy and fewer retraining instances, as larger δ values do not improve accuracy or reduce the number of retraining instances. Furthermore, since we enforce a maximum number of training observations per activity, larger δ values will have no effect on the balance of training data among activities.</p><p>Comparison with Periodic Retraining. In Figure <ref type="figure" target="#fig_0">17</ref>, we compare the best retraining performance with our retraining detection and ground truth management methods to a naive retraining approach: periodic retraining. We implement periodic retraining for periods of 100 to 500 intervals, with each retraining adding 30 new training data elements to the training data set. We also choose δ = 2.0 for training data balance among all activities. The figure demonstrates that PBN with retraining detection achieves high accuracy for both subjects while periodic retraining either has twice as many retraining instances for the same accuracy or especially in the case of Subject 2, lower accuracy for fewer retraining instances.</p><p>We also show that PBN with retraining detection achieves similar accuracy as periodic retraining but incurs fewer retraining instances. In Figure <ref type="figure" target="#fig_0">18</ref>, we present a timeline of runtime accuracy and retraining instances for the first 2500 classification intervals comparing PBN to periodic retraining every 100 classification intervals. Both approaches have very volatile initial accuracy, but eventually converge  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Sensor Selection</head><p>We now evaluate our sensor selection approach in Section 7 and demonstrate that we can exclude 30-40% of all sensors from AdaBoost training, yet achieve similar runtime accuracy as using no sensor selection. Removing this many sensors from training is a significant savings in computational overhead since these sensors no longer have classifiers trained during each of 300 AdaBoost training iterations.</p><p>Using 10 random observations per activity as initial training data and using online training, we compare each of the correlation metrics to determine which is the best in terms of accuracy and sensors excluded. In Figure <ref type="figure" target="#fig_16">19</ref>, we depict the percentage of total sensors excluded from AdaBoost (SS Only), the percentage of total sensors excluded by sensor selection and AdaBoost training (SS + AdaBoost), and average runtime accuracy for each sensor selection method.</p><p>Figure <ref type="figure" target="#fig_16">19</ref> indicates that both sensor pair raw correlation and decision correlation exclude 30-40% of all sensors from training. Both of these methods also have nearly the same accuracy as without sensor selection, indicating that the right sensors are excluded using these methods. While sensor cluster sensor selection excludes 50% of sensors from training for Subject 1, accuracy is also worse. The figure also shows that in combination with AdaBoost training, each sensor selection method excludes more than 10% points more sensors than with no sensor selection, resulting in additional  energy savings since unused nodes are powered down. We suggest that using raw correlation is the best approach to sensor selection, as it requires the least computational overhead of the three metrics, yet excludes a significant number of sensors and maintains high runtime accuracy. During our experiments, the wireless motes had battery life measured in days (4 or 5 days of 8 hour sessions), while the Android HTC G1 phone was unable to last more than 8 hours without recharging. Here, we focus on evaluating the phone performance, for its battery lifetime is much shorter. In Table <ref type="table" target="#tab_4">3</ref>, we measure the CPU, memory, and power consumption of our PBN application to demonstrate that it is practical for mobile phones. We run each configuration for 5 minutes and show the average for system idle, sampling only with GPS or WiFi for localization, sampling with AdaBoost training, and sampling with AdaBoost classification. Since retraining occurs infrequently, during the vast majority of system deployment (Sampling + Classify), PBN incurs roughly 20% CPU use with memory overhead under 10MB. Most of this overhead is due to the TinyOS Java libraries sending and receiving packets to and from sensor motes; we leave it to future work to further optimize these libraries for mobile devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4">Application Performance</head><p>When retraining does occur, it takes between 1 and 10 minutes on the HTC G1, depending on training data size. With newer hardware (HTC Nexus One), retraining time is halved under the same conditions. Since PBN retraining is run as a background process, it can be preempted and has little impact on performance of other applications, such as checking email or making a phone call.</p><p>Depicted in Figure <ref type="figure" target="#fig_17">20</ref>, we evaluate PBN power consumption with the display off using a power meter from Monsoon Technologies. In Table <ref type="table" target="#tab_4">3</ref>, we demonstrate that during sampling and classification, PBN consumes roughly 150mW in addition to system idle. This consumption is about 1/3 of the additional 450mW consumed by the display when it is active. GPS-based localization consumes an additional 200mW, however GPS is enabled only when WiFi localization is not possible. An additional 90mW is consumed during online training, however, as previously mentioned, these periods are short lived and infrequent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion and Future Work</head><p>In this paper, we present PBN, a significant effort towards a practical solution for daily activity recognition. PBN provides a user-friendly experience for the wearer as well as strong classification performance. Through integration of Android and TinyOS, we provide a software and hardware support system which couples the sensing power of on-body wireless sensors with an easy to use mobile phone application. Our approach is computationally appropriate for mobile phones and wireless motes and also chooses the sensors that maximize accuracy. We improve AdaBoost through online training and enhanced sensor selection, analyzing the properties of sensors and sensor data to identify helpful and redundant sensors as well as indicators for when retraining is needed. We show in our evaluation that PBN can perform accurately for classifying a wide array of activities and postures even when using a limited training dataset coupled with online training. In future work, we intend to provide an extensive usability study with a diverse array of subjects as well as improve energy use on the phone.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: PBN activity status view.</figDesc><graphic coords="4,53.98,494.98,117.41,176.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Ground truth logging.</figDesc><graphic coords="4,173.98,494.98,117.41,176.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: PBN System Architecture. Illustrated in Figure 3, our Practical Body Networking (PBN) system resides solely on TinyOS-based motes and on an Android smartphone with no reliance on a backend server. Multiple TinyOS-based motes, each containing one or more sensors of different modalities, are attached on-body. Each mote communicates wirelessly (dotted lines) with a TinyOS mote base station, which is connected via USB (solid lines) to an Android smartphone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Subject 1.Figure 5: Subject 2.</figDesc><graphic coords="5,452.62,122.74,88.13,187.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 4: Subject 1.Figure 5: Subject 2.</figDesc><graphic coords="5,332.62,122.74,88.13,187.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :Figure 7 :Figure 8 :</head><label>678</label><figDesc>Figure 6: Training and runtime accuracy for 1-300 Ada-Boost iterations.</figDesc><graphic coords="7,81.82,253.54,178.85,63.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Sensor K-L divergence and training accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Raw data correlation for Subject 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Decision correlation between individual sensors and sensor clusters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Algorithm 2 3 :</head><label>23</label><figDesc>Raw Correlation Threshold for Sensor Selection Input: Set of sensors S selected by AdaBoost, training observations for all sensors O, multiplier n Output: Sensor selection threshold α 1: R = / 0 // set of correlation coefficients 2: for all combinations of sensors s i and s j in S do Compute correlation coefficient r = |r O i ,O j | 4: R = R ∪ {r} 5: end for 6: // compute threshold as avg + (n * std. dev. ) of R 7: α</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Runtime performance comparison for multiple subjects and multiple classification category sets. Subject 2 does not engage in the cycling, lying down, cleaning, and reading categories, so the corresponding bars are not presented.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: PBN decision and ground truth timeline for Subject 1.</figDesc><graphic coords="12,81.58,216.58,177.17,62.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: AdaBoost sensor weights per activity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: PBN retraining instances and runtime accuracy for new data window sizes N = 10-100. by 2000 intervals. The figures also demonstrate that by reducing the number of retraining instances, PBN retraining detection consequently reduces the amount of ground truth logging by the user.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 19 :</head><label>19</label><figDesc>Figure 19: Sensor selection comparison with online training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 20 :</head><label>20</label><figDesc>Figure 20: Power Consumption Evaluation.</figDesc><graphic coords="13,459.34,252.58,96.05,134.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>PBN Classification Categories.</figDesc><table><row><cell cols="2">Environment Indoors, Outdoors</cell></row><row><cell>Posture</cell><cell>Cycling, Lying Down, Sitting, Standing, Walk-</cell></row><row><cell></cell><cell>ing</cell></row><row><cell>Activity</cell><cell>Cleaning, Cycling, Driving, Eating, Meeting,</cell></row><row><cell></cell><cell>Reading, Walking, Watching TV, Working</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>PBN Deployment Configuration.    </figDesc><table><row><cell>Node</cell><cell cols="2">ID Location</cell><cell>Sensors</cell></row><row><cell>Phone</cell><cell>0</cell><cell cols="2">R. Waist 3-Axis Acc., GPS/WiFi (velocity)</cell></row><row><cell>IRIS</cell><cell>1</cell><cell>L. Wrist</cell><cell>2-Axis Acc., Mic., Light, Temp.</cell></row><row><cell>IRIS</cell><cell>2</cell><cell>R. Wrist</cell><cell>2-Axis Acc., Mic., Light, Temp.</cell></row><row><cell>IRIS</cell><cell>3</cell><cell>L. Ankle</cell><cell>2-Axis Acc., Mic., Light, Temp.</cell></row><row><cell>IRIS</cell><cell>4</cell><cell>R. Ankle</cell><cell>2-Axis Acc., Mic., Light, Temp..</cell></row><row><cell>IRIS</cell><cell>5</cell><cell>Head</cell><cell>2-Axis Acc., Mic., Light, Temp.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Max iterations T , training obs. vector O j for each sensor s j ∈ S, obs. ground truth labels Output: Set of weak classifiers H 1: Initialize observation weights D 1 to 1/n for all obs.</figDesc><table><row><cell>Algorithm 1 AdaBoost Training</cell></row><row><cell>Input:</cell></row></table><note><p>2: for t = 1 to T do 3:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Algorithm 3</head><label>3</label><figDesc>Sensor Selection Using Raw Correlation Input: Set of all sensors S, training observations for all sensors O, threshold α Output: Selected sensors S * to give as input to AdaBoost 1: S * = / 0 2: E = / 0 // set of sensors we exclude 3: for all combinations of sensors s i and s j in S do</figDesc><table><row><cell>4: 5: 6: 7:</cell><cell>Compute correlation coefficient r = |r O i ,O j | if r &lt; α then if s i / ∈ E then S  *  = S  *  ∪ {s i } if s j / ∈ E then S  *  = S  *  ∪ {s j }</cell></row><row><cell>8: 9: 10: 11:</cell><cell>else if r ≥ α and acc(s i ) &gt; acc(s j ) then // use accuracy to decide which to add to S  *  if s i / ∈ E then S  *  = S  *  ∪ {s i } E = E ∪ {s j }; S  *  = S  *  \ {s j }</cell></row><row><cell>12: 13:</cell><cell>else if s j / ∈ E then S  *  = S  *  ∪ {s j }</cell></row><row><cell>14:</cell><cell></cell></row></table><note><p>E = E ∪ {s i }; S * = S * \ {s i } 15:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>PBN CPU, memory, and power benchmarks.</figDesc><table><row><cell>Mode</cell><cell>CPU</cell><cell>Memory</cell><cell>Power</cell></row><row><cell>Idle (No PBN)</cell><cell>&lt;1%</cell><cell cols="2">4.30MB 360.59mW</cell></row><row><cell>Sampling (WiFi)</cell><cell>19%</cell><cell cols="2">8.16MB 517.74mW</cell></row><row><cell>Sampling (GPS)</cell><cell>21%</cell><cell cols="2">8.47MB 711.74mW</cell></row><row><cell>Sampling (WiFi) + Train</cell><cell cols="3">100% 9.48MB 601.02mW</cell></row><row><cell>Sampling (WiFi) + Classify</cell><cell>21%</cell><cell cols="2">9.45MB 513.57mW</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>* This work is supported in part by NSF grants ECCS-0901437, CNS-0916994, and CNS-0954039.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Using Wearable Activity Type Detection to Improve Physical Activity Energy Expenditure Estimation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Albinali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Intille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Haskell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rosenberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UbiComp &apos;10</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="311" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Surround-Sense: Mobile Phone Localization via Ambience Fingerprinting</title>
		<author>
			<persName><forename type="first">M</forename><surname>Azizyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Constandache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Choudhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MobiCom &apos;09</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="261" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reliable Clinical Monitoring using Wireless Sensor Networks: Experience in a Step-down Hospital Unit</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chipara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-C</forename><surname>Roman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SenSys &apos;10</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="155" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bridging the Gap Between Physical Location and Online Social Networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cranshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Toch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kittur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Saleh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UbiComp &apos;10</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="119" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">HTC Hero (MSM7201) USB Host Mode</title>
		<author>
			<persName><forename type="first">A</forename><surname>De Quincey</surname></persName>
		</author>
		<ptr target="http://adq.livejournal.com/95689.html" />
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The BikeNet Mobile Sensing System for Cyclist Experience Mapping</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Eisenman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Miluzzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SenSys &apos;07</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="87" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The Pothole Patrol: Using a Mobile Sensor Network for Road Surface Monitoring</title>
		<author>
			<persName><forename type="first">J</forename><surname>Eriksson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Girod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Newton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Madded</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Balakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MobiSys &apos;08</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="29" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Decision-Theoretic Generalization of On-line Learning and an Application to Boosting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JCSS</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="336" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">SATIRE: A Software Architecture for Smart AtTIRE</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Ganti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jayachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Abdelzaher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stankovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mo-biSys &apos;06</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="110" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning from Imbalanced Data</title>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TKDE</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1263" to="1284" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exploiting Sensing Diversity for Confident Sensing in Wireless Sensor Networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Keally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INFOCOM &apos;11</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1719" to="1727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SensLoc: Sensing Everyday Places and Paths using Less Energy</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Estrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SenSys &apos;10</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="43" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On Information and Sufficiency</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kullback</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Leibler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="86" />
			<date type="published" when="1951">1951</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Hybrid Discriminative/Generative Approach for Modeling Activities</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Borriello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hannaford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="766" to="772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mercury: A Wearable Sensor Network Platform for High-Fidelity Motion Analysis</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lorincz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Challen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bonato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welsh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SenSys &apos;09</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="183" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">SoundSense: Scalable Sound Sensing for People-Centric Sensing Applications on Mobile Phones</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MobiSys &apos;09</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="165" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The Jigsaw Continuous Sensing Engine for Mobile Phone Applications</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SenSys &apos;10</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="71" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Darwin Phones: The Evolution of Sensing and Inference on Mobile Phones</title>
		<author>
			<persName><forename type="first">E</forename><surname>Miluzzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cornelius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MobiSys &apos;10</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="5" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sensing Meets Mobile Social Networks: The Design, Implementation and Evaluation of the CenceMe Application</title>
		<author>
			<persName><forename type="first">E</forename><surname>Miluzzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fodor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Musolesi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SenSys &apos;08</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="337" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Community-Guided Learning: Exploiting Mobile Sensor User to Model Human Behavior</title>
		<author>
			<persName><forename type="first">D</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The PSI Board: Realizing a Phone-Centric Body Sensor Network</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chaudhri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Anokwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Want</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BSN</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="26" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Body Posture Identification using Hidden Markov Model with a Wearable Sensor Network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Quwaider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biswas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bodynets &apos;08</title>
		<imprint>
			<publisher>ICST</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">EmotionSense: A Mobile Phones based Adaptive Platform for Experimental Social Psychology Research</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rachuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Musolesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mascolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rentfrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Longworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acuinas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UbiComp &apos;10</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="281" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">BodyT2: Throughput and Time Delay Performace Assurance for Heterogeneous BSNs</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pyles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Keally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INFOCOM &apos;11</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2750" to="2758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Thirteen Ways to Look at the Correlation Coefficient</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Nicewander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="66" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The k Factor: Inferring Protocol Performance Using Inter-Link Reception Correlation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Azim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Levis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Krishnamachari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MobiCom &apos;10</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="317" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A Framework of Energy Efficient Mobile Sensing for Automatic User State Recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Annavaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Krishnamachari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MobiSys &apos;09</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="179" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Activity Recognition from On-Body Sensors: Accuracy-Power Trade-Off by Dynamic Sensor Selection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zappi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lombriser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Steifmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Farella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roggen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Benini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Troster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EWSN &apos;08</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="17" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">ZiFi: Wireless LAN Discovery via ZigBee Interference Signatures</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MobiCom &apos;10</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="49" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Ensembling Neural Networks: Many Could Be Better Than All. Artificial Intelligence</title>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">137</biblScope>
			<biblScope unit="page" from="239" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Exploring Link Correlation for Efficient Flooding in Wireless Sensor Networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI &apos;10. USENIX</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
