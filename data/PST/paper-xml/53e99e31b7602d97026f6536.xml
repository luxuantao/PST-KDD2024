<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Semantic Typicality Measure for Natural Scene Categorization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Julia</forename><surname>Vogel</surname></persName>
							<email>vogel@inf.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Perceptual Computing and Computer Vision Group</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Perceptual Computing and Computer Vision Group</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Multimodal Interactive Systems</orgName>
								<address>
									<settlement>Darmstadt</settlement>
									<region>TU</region>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Semantic Typicality Measure for Natural Scene Categorization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">103808BAA9DDAD0C68F8A027678998B8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose an approach to categorize real-world natural scenes based on a semantic typicality measure. The proposed typicality measure allows to grade the similarity of an image with respect to a scene category. We argue that such a graded decision is appropriate and justified both from a human's perspective as well as from the image-content point of view. The method combines bottomup information of local semantic concepts with the typical semantic content of an image category. Using this learned category representation the proposed typicality measure also quantifies the semantic transitions between image categories such as coasts, rivers/lakes, forest, plains, mountains or sky/clouds. The method is evaluated quantitatively and qualitatively on a database of natural scenes. The experiments show that the typicality measure well represents the diversity of the given image categories as well as the ambiguity in human judgment of image categorization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Scene categorization or scene classification is still a challenge on the way to reduce the semantic gap between "the information that one can extract from visual data and the users' interpretation for the same data in a given situation" <ref type="bibr" target="#b0">[1]</ref>. In the context of this paper, scene categorization refers to the task of grouping images into semantically meaningful categories. But what are "semantically meaningful" categories? In image retrieval, meaningful categories correspond to those basic-level image categories that act as as a starting point when users describe verbally the particular image they are searching for. In general however, any natural scene category will be characterized by a high degree of diversity and potential ambiguities. The reason is that those categories depend strongly on the subjective perception of the viewer.</p><p>We argue that high categorization accuracies should not be the primary evaluation criterion for categorization. Since many natural scenes are in fact ambiguous, the categorization accuracy only reflects the accuracy with respect to the opinion of the particular person that performed the annotation. Therefore, the attention should also be directed at modeling the typicality of a particular scene. Here, typicality can be seen as a measure for the uncertainty of annotation judgment. Research in psychophysics especially addresses the concept of typicality in categorization. In each category, typical and less typical items can be found with typicality differences being the most reliable effect in categorization research <ref type="bibr" target="#b1">[2]</ref>. We propose a semantic typicality measure that grades the similarity of natural realworld image with respect to six scene categories. Furthermore, the typicality measure allows to categorize the images into one of those categories. Images are represented through the frequency of occurrence of nine local semantic concepts. Based on this information, a prototypical category representation is learned for each scene category. The proposed typicality measure is evaluated both qualitatively and quantitatively using cross-validation on an image database of 700 natural scenes.</p><p>Previous research in scene classification usually aims for high classification accuracies by using very "clean" databases. Early research covers city/landscape- <ref type="bibr" target="#b2">[3]</ref>, indoor/outdoor- <ref type="bibr" target="#b3">[4]</ref> and indoor/outdoor-, city/landscape-, sunset/mountain/forest-classification <ref type="bibr" target="#b4">[5]</ref>. These approaches employ only global image information rather than localized information. The goal of more recent work is to automatically annotate local image regions <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b8">[9]</ref>, but the majority does not try to globally describe the retrieved images. Oliva and Torralba <ref type="bibr" target="#b9">[10]</ref> attach global labels to images based on local and global features, but do not use any intermediate semantic annotation.</p><p>In the next section, we discuss the selection of image categories. The image and category representations are introduced in Section 3 and 4. We present our typicality measure and the categorization based on it in Section 5. Section 6 summarizes the categorization results using automatically classified image subregions as input. Finally, Section 7 shows the categorization performance visually on new, unseen images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Basic Level Image Categories</head><p>Our selection of scene categories has been inspired by work in psychophysics. In search of a taxonomy of environmental scenes, Tversky and Hemenway <ref type="bibr" target="#b10">[11]</ref> found indoors and outdoors to be superordinate-level categories, with the outdoors category being composed of the basic-level categories city, park, beach and mountains. The experiments of Rogowitz et al. <ref type="bibr" target="#b11">[12]</ref> revealed two main axes in which humans sort photographic images: human vs. non-human and natural vs. artificial. For our experi-ments, we selected the non-human/natural coordinate as superordinate and extended the natural, basic-level categories of <ref type="bibr" target="#b10">[11]</ref> to coasts, rivers/lakes, forests, plains, mountains and sky/clouds. The diversity of those categories is illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. It displays a sample of images for each category. The top three lines correspond to typical examples for each category. The bottom line shows images which are far less typical but which are -arguably -still part of the respective category. Obviously, those examples are more difficult to classify and literally correspond to borderline cases. In the following, we aim for a semantic typicality measure based on the global composition of local semantic concepts which reflects that those less typical images in the bottom part of Figure <ref type="figure" target="#fig_0">1</ref> are less similar to the semantic category than those, more typical images in the upper part of the figure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Image Representation</head><p>Many studies have shown that in categorization, members and non-members form a continuum with no obvious break in people's membership judgment. Quite importantly, typicality differences are probably the strongest and most reliable effect in the categorization literature <ref type="bibr" target="#b1">[2]</ref>. For example it has been found that typical items were more likely to serve as cognitive reference points <ref type="bibr" target="#b12">[13]</ref> and that learning of category representations is faster if subjects are taught on mostly typical items than if they are taught on less typical items <ref type="bibr" target="#b13">[14]</ref>. In our opinion, any successful category representation has to take these findings into account and should be consistent with them.</p><p>A representation which is predictive of typicality is the so-called "attribute score" <ref type="bibr" target="#b14">[15]</ref>. That is, items that are most typical have attributes that are very common in the category. In this approach each attribute is weighted in order to take into account their respective importance for the category. In our case, it is the local semantic concepts that act as scene category attributes. By analyzing the semantic similarities and dissimilarities of the aforementioned categories, the following set of nine local semantic concepts emerged as being most discriminant: sky, water, grass, trunks, foliage, field, rocks, flowers and sand. In our current implementation, the local semantic concepts are extracted on an arbitrary regular 10x10 grid of image subregions. For each local semantic concept, its frequency of occurrence in a particular image is determined and each image is represented by a so-called concept occurrence vector. Figure <ref type="figure">2</ref> shows an exemplary image with its local semantic concepts and its concept occurrence vector. Since the statistics of the local semantic concepts vary significantly when analyzing certain image areas separately (e.g. top/middle/bottom), we evaluate the concept occurrence vector for at least three image areas.</p><p>Database. The database consists of 700 images in the categories coasts, forests, rivers/lakes, plains, mountains and sky/clouds. All image subregions have been manually annotated with the above mentioned nine local semantic concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Prototypical Representation of Scene Categories</head><p>The representation of the scene categories should take into account the typicality effect and the prototype phenomenon. A category prototype is an example which is most typical for the category, even though the prototype is not necessarily an existing category member. Given the image representation presented in the previous section, a prototypical representation for the six scene categories can be learned. This prototypical representation allows to grade the different members of the category by an appropriate semantic typicality measure. The measure takes into account the occurrence statistics of the semantic concepts and weights them according to their variance within the category. The prototypical representation corresponds to the means over the concept occurrence vectors of all category members. Figure <ref type="figure" target="#fig_1">3</ref> displays those prototypes and the standard deviations for all categories. From this figure, it becomes apparent which local semantic concepts are especially discriminant. For example, forests are characterized through a large amount of foliage and trunks, whereas mountains can be differentiated when a large amount of rocks is detected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Typicality and Categorization</head><p>The proposed category representation has the advantage of not representing binary decisions about the semantic concepts being present in the image or not ("Yes, there are rocks." vs. "No, there are no rocks."). Instead it represents soft decisions about the degree to which a particular semantic concept is present. The distances of the category members to the prototypical representation thus allow to assess the typicality of these images without excluding them from the category. There might be mountains scenes that hardly contain any rocks, but quite some foliage. They do belong to the mountains category, but they are much less typical than mountains scenes that contain a larger amount of rocks. In fact, they might be quite close to the borderline of being forest scenes.</p><p>The image typicality is measured by computing the Mahalanobis distance between the images' concept occurrence vector and the prototypical representation. All experiments have been 10-fold cross-validated. Hence, the category prototypes are computed on 90% of the database. All following depicted images belong to the respective test sets. Figures <ref type="figure">4,</ref><ref type="figure">5</ref>, and 6 show the transitions between two categories with the typicality distance measure printed below the images, normalized to the range [0, 1]. A value close to 0 corresponds to a close similarity of the particular image to the first of the two categories and vice versa. For example Figure <ref type="figure">4</ref> shows clearly the increase in "forest-ness" from left to right. Figure <ref type="figure">5</ref> depicts the transition from forests to mountains and Figure <ref type="figure">6</ref> the transition from mountains back to rivers/lakes.</p><p>With the typicality measure, also the categorization of unseen images can be carried out. For a new image, the similarity to the prototypical representation of each category is computed and the image is assigned to the category with the smallest distance. Table <ref type="table" target="#tab_1">2</ref>(a) shows the confusion matrix for the categorization of the annotated database images (10fold cross-validated) resulting in an overall categorization rate of 89.3%. The analysis of the mis-categorized images shows that most of the confusions can be explained due to similarities of the different categories. Another way to evaluate the performance is to use the rank statistics of the categorization shown in Table <ref type="table" target="#tab_1">2</ref>(a). Using both the best and the second best match the categorization rate raises to 98.0%. This proves that images which are incorrectly categorized as first match are on the borderline between two similar categories and therefore most often correctly categorized with the second best match. It is also true, that the typicality values of those two matches are often very close to each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Categorization of Classified Images</head><p>The categorization experiment of the previous section was carried out using the manually annotated images of our database. In this section, we discuss the categorization results when the 70'000 image subregions have automatically been classified into one of the    Neighbor classifier. Features and classifier have been selected through an extensive series of experiments. These pre-tests also showed that the use of neighborhood information in face decreases the overall classification accuracy since it penalizes concepts that appear as "singularities" in the image instead of as contiguous regions (e.g. trunks or grass).</p><p>The classification accuracy of the concept classification is 68.9%.</p><p>The prototypical representation of the categories is computed on ten image areas (ten rows from top to bottom, see Section 3). Both concept classification and categorization are 10-fold cross-validated on the same test and training set. That is, a particular training set is used to train the concept classifier and to learn the prototypes. The images of the corresponding test set are classified locally with the learned concept classifier and subsequently categorized.</p><p>The overall categorization rate of the classified images is 67.2%. The corresponding confusion matrix is displayed in <ref type="bibr">Table 3(a)</ref>. A closer analysis of the confusions leads to the following insights. Good and less good categorization is strongly correlated with the performance of the concept classifier that is most discriminant for the particular category. Three of the six categories have been categorized with high accuracy: forest, mountains and sky/clouds. The reason is that the important local concepts for those categories, that is sky, foliage and rocks have been classified with high accuracy and thus lead to a better categorization. Critical for the categorization especially of the category plains is the classification of fields. Since fields is frequently confused with either foliage or rocks, plains is sometimes mis-categorized as forests or mountains.  Another semantic concept that is critical for the categorization is water. If not enough water is classified correctly, rivers/lakes images are confused with forests or mountains depending on the amount of foliage and rocks in the image. If too much water has incorrectly been detected in rivers/lakes images, they are confused with coasts.</p><p>Table <ref type="table">3</ref>(b) displays the rank statistics for the categorization problem. When using both the best and the second best match, the categorization rate is 83.1%. As before with the labeled data, there is a large jump in categorization accuracy from the first to the second rank. This leads to the conclusion that the wrong classifications on subregion level move many images closer to the borderline between two categories and thus cause mis-categorizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">More Categorization Results</head><p>In order to verify the results of Section 6, both concept classification and scene categorization were tested on new images that do not belong to the cross-validated data sets. Exemplary categorization results are displayed in Figure <ref type="figure" target="#fig_5">7</ref> and Figure <ref type="figure" target="#fig_6">8</ref>. "Correctly" and "incorrectly" are quoted on purpose since especially Figure <ref type="figure" target="#fig_6">8</ref> exemplifies how difficult it is to label the respective images. When does a forest-scene become a rivers/lakesscene or a mountains-scene a forest-scene? The reason for the "mis-categorization" of the first image in Figure <ref type="figure" target="#fig_6">8</ref> is that a large amount of water has been classified as foliage thus moving the scene closer to the forest-prototype. The reason for the other two "mis-categorizations" is the ambiguity of the scenes. The typicality measure returned for all three images in Figure <ref type="figure" target="#fig_6">8</ref> low confidence values for either of the two relevant categories whereas the typicality value for the scenes in Figure <ref type="figure" target="#fig_5">7</ref> is higher. This shows that we are able to detect difficult or ambiguous scenes using our image representation in combination with the typicality measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion and Conclusion</head><p>In this paper, we have presented a novel way to categorize natural scenes based on a semantic typicality measure. We have shown that it is indispensable both from a human's perspective and from a system's point of view to model the local content and thus the diversity of scene categories. With our typicality measure ambiguous images can be marked as being less typical for a particular image category, or the transition between two categories can be determined. This behavior is of interest for image retrieval systems since humans are often interested in searching for images that are somewhere "between mountains and rivers/lakes, but have no flowers".</p><p>Considering the diversity of the images and scene categories used, classification rates of 89.3% with annotated concept regions and 67.2% using semantic concept classifiers are convincing. By also including the second best match in the categorization, an increase to 98.0% and 83.1%, respectively, could be reached. In particular this latter result reveals that many of the images misclassified with the first match are indeed at the borderline between two semantically related categories. This supports the claim that we are able to model the typicality and ambiguity of unseen images. The results also show that the categorization performance is strongly dependent on the performance of the individual concept classifiers which will be the topic of further research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Images of each category. Top three rows: typical images. Bottom row: less typical image.</figDesc><graphic coords="2,47.73,53.58,334.94,166.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 2. Image representation</figDesc><graphic coords="4,48.97,77.45,124.34,88.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .Fig. 5 .Fig. 6 .</head><label>456</label><figDesc>Fig. 4. Transition from rivers/lakes to forests with normalized typicality value</figDesc><graphic coords="5,42.06,137.94,334.28,69.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>83.8 95.1 99.3 100.0 100.0 41.6 80.5 96.5 98.2 100.0 100.0 94.1 95.1 97.1 97.1 100.0 100.0 43.8 59.2 78.5 99.2 100.0 100.0 84.3 91.6 96.1 99.4 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 67.2 83.1 93.0 98.8 99.9 100.0 (b) Rank Statistics semantic concept classes. The subregions are represented by a combined 84-bin linear HSI color histogram and a 72-bin edge direction histogram, and classified by a k-Nearest</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Examples for "correctly" categorized images.</figDesc><graphic coords="7,42.06,169.46,335.13,87.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Examples for "incorrectly" categorized images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Categorization Confusion Matrix and Rank Statistics -Annotated Image Regions</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Categorization Confusion Matrix and Rank Statistics -Classified Image Regions</figDesc><table><row><cell>coasts</cell><cell>59.9 12.0 3.5 6.3 10.6 7.7</cell></row><row><cell>rivers/lakes</cell><cell>15.9 41.6 13.</cell></row><row><cell>forests</cell><cell></cell></row><row><cell>plains</cell><cell></cell></row><row><cell>mountains</cell><cell></cell></row><row><cell>sky/clouds</cell><cell></cell></row><row><cell>OVERALL</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>C.E. Rasmussen et al. (Eds.): DAGM 2004, LNCS 3175, pp. 195-203, 2004. c Springer-Verlag Berlin Heidelberg 2004</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This work is part of the CogVis project, funded by the Comission of the European Union (IST-2000-29375) and the Swiss Federal Office for Education and Science (BBW 00.0617).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Content-based image retrieval at the end of the early years</title>
		<author>
			<persName><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Santini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on PAMI</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The Big Book of Concepts</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Texture orientation for sorting photos at a glance</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gorkani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Pattern Recognition ICPR</title>
		<meeting><address><addrLine>Jerusalem, Israel</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Indoor-outdoor image classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Szummer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Access of Image and Video Databases</title>
		<meeting><address><addrLine>Bombay, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image classification for content-based indexing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vailaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multiple-instance learning for natural scene classification</title>
		<author>
			<persName><forename type="first">O</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Ratan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Machine Learning</title>
		<meeting><address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Content based image retrieval using semantic visual categories</title>
		<author>
			<persName><forename type="first">C</forename><surname>Town</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sinclair</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000. 2000</date>
			<biblScope unit="volume">14</biblScope>
		</imprint>
		<respStmt>
			<orgName>AT&amp;T Laboratories Cambridge</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A probabilistic framework for semantic video indexing, filtering, and retrieval</title>
		<author>
			<persName><forename type="first">M</forename><surname>Naphade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Multimedia</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Object recognition as machine translation -part 2: Exploiting image data-base clustering models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Duygulu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. on Computer Vision</title>
		<meeting><address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: A holistic representation of the spatial envelope</title>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Categories of environmental scenes</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tversky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hemenway</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Perceptual image similarity experiments</title>
		<author>
			<persName><forename type="first">B</forename><surname>Rogowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Frese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bouman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kalin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE Conf. Human Vision and Electronic Imaging</title>
		<meeting><address><addrLine>San Jose, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cognitive representations of semantic categories</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rosch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Exp. Psychology</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On the genesis of abstract ideas</title>
		<author>
			<persName><forename type="first">M</forename><surname>Posner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Keele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Exp. Psychology</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Family resemblance: Studies in the internal structure of categories</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rosch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mervis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
