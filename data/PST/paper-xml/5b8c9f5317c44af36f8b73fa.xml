<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Context-Aware Visual Policy Network for Sequence-Level Image Captioning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Daqing</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
							<email>hanwangzhang@ntu.edu.sg</email>
						</author>
						<author>
							<persName><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Feng</forename><surname>Wu</surname></persName>
							<email>fengwu@ustc.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Context-Aware Visual Policy Network for Sequence-Level Image Captioning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8D8B94BEE6511990157BB3D6C20E29AA</idno>
					<idno type="DOI">10.1145/3240508.3240632</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>image captioning</term>
					<term>reinforcement learning</term>
					<term>visual context</term>
					<term>policy network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many vision-language tasks can be reduced to the problem of sequence prediction for natural language output. In particular, recent advances in image captioning use deep reinforcement learning (RL) to alleviate the "exposure bias" during training: ground-truth subsequence is exposed in every step prediction, which introduces bias in test when only predicted subsequence is seen. However, existing RL-based image captioning methods only focus on the language policy while not the visual policy (e.g., visual attention), and thus fail to capture the visual context that are crucial for compositional reasoning such as visual relationships (e.g., "man riding horse") and comparisons (e.g., "smaller cat"). To fill the gap, we propose a Context-Aware Visual Policy network (CAVP) for sequence-level image captioning. At every time step, CAVP explicitly accounts for the previous visual attentions as the context, and then decides whether the context is helpful for the current word generation given the current visual attention. Compared against traditional visual attention that only fixes a single image region at every step, CAVP can attend to complex visual compositions over time. The whole image captioning model -CAVP and its subsequent language policy network -can be efficiently optimized end-to-end by using an actor-critic policy gradient method with respect to any caption evaluation metric. We demonstrate the effectiveness of CAVP by state-of-the-art performances on MS-COCO offline split and online server, using various metrics and sensible visualizations of qualitative visual context. The code is available at https://github.com/daqingliu/CAVP</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Visual and natural language comprehension -the ever-lasting goal in the multimedia community -are rapidly evolving with the help of deep learning based AI technologies <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b18">18]</ref>. A prime example is image captioning -the task of generating natural language descriptions for an image, relying solely on the visual input -which demonstrates a machine's visual comprehension in terms of its ability of natural language modeling <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b39">39]</ref>. As an AI-complete task <ref type="bibr" target="#b20">[20]</ref>, researchers attempts to combine the most advanced computer vision (CV) techniques like object recognition <ref type="bibr" target="#b28">[28]</ref>, relationship detection <ref type="bibr" target="#b42">[42]</ref>, and scene parsing <ref type="bibr" target="#b38">[38]</ref>, as well as the modern natural language processing (NLP) techniques such as language generative models <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b41">41]</ref>. In a nutshell, the CV-end acts as an encoder and the NLP-end plays as an decoder, translating from "source" image to "target" sentences. This encoder-decoder architecture is trained using human-annotated image and sentence pairs in a fully-supervised way, that is, the decoder is supervised to maximize the posterior probability of each ground-truth word given the previous ground-truth subsequence and "source" image. Unfortunately, due to the exponentially large search space of language compositions, recent studies demonstrate that this conventional supervised training tends to learn dataset bias but not machine reasoning <ref type="bibr" target="#b13">[13]</ref><ref type="bibr" target="#b14">[14]</ref><ref type="bibr" target="#b15">[15]</ref>.</p><p>An emerging line of training strategy is the sequence-level training using deep reinforcement learning (RL) <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b44">44]</ref>. To see this, as illustrated in Figure <ref type="figure">2a</ref>, we first frame the traditional encoder-decoder image captioning into the decision-making process, where the visual encoder can be viewed as Visual Policy (VP): deciding where to fix a gaze in the image, and the language decoder can be viewed as Language Policy (LP): deciding what the next word is. As highlighted in Figure <ref type="figure">2b</ref>, the sequence-level RL-based framework directly injects the previously sampled word (sampling but not greedy argmax) to influence the next prediction. This brings two benefits: 1) training supervision is delayed to the whole sequence generated, thus we can use non-differentiable sequence-level metric such as CIDEr <ref type="bibr" target="#b35">[35]</ref> and SPICE <ref type="bibr">[1]</ref>, which are more suitable than word-level cross entropy loss for language quality evaluation; 2) it avoids the exposure bias <ref type="bibr" target="#b27">[27]</ref>, i.e., the LP has never been exposed to the ground-truth at training, allowing exploration over the large sequence compositions under current policy, and thus generating more fruitful sentences without severe overfitting.</p><p>However, existing RL-based framework neglects to turn VP into decision-making, e.g., the input of VP is identical in every step as shown in Figure <ref type="figure">2b</ref>. This disrespects the nature of sequence prediction, where the history visual actions (e.g., previously attended regions) should explicitly influence the current visual policy. To this end, we develop a Context-Aware Visual Policy (CAVP) network for sequence-level image captioning. As shown in Figure <ref type="figure">2c</ref>, CAVP allows the previous visual features, i.e., the previous output of CAVP, to serve as the visual context for the current action. Different from the conventional visual attention <ref type="bibr" target="#b39">[39]</ref>, where the visual context is implicitly encoded in a hidden RNN state vector from LP, our visual context is explicitly considered in a sequence prediction process. Our motivation is in line with the cognitive evidences that the visual memory recall plays a crucial role in compositional reasoning <ref type="bibr" target="#b32">[32]</ref>. For example, as illustrated in Figure <ref type="figure">1</ref>, it is necessary to consider the contextual regions, e.g., the previously selected "man" object, when generating the composition "man riding a horse".</p><p>We decompose CAVP into 4 sub-policy networks, which together accomplish the visual decision-making task (cf. Figure <ref type="figure" target="#fig_1">3</ref>), each of which is an Recurrent Neural Network (RNN) control with shared Long Short-Term Memory (LSTM) parameters and outputs a soft visual attention map. As we will show in Section 3.2, this CAVP design stabilizes the conventional Monte Carlo policy rollout and reduces the exponentially large search complexity to linear time. It is worth noting that CAVP and its subsequent language policy network can efficiently model higher-order compositions over time, e.g., relationships among multiple objects mentioned in the generated sub-sequence. The whole framework is trained end-to-end using an actor-critic policy gradient with a self-critic baseline <ref type="bibr" target="#b30">[30]</ref>. In fact, our CAVP can be seamlessly integrated into any policybased RL models <ref type="bibr" target="#b33">[33]</ref>. We show the effectiveness of the proposed CAVP through extensive experiments on the MS-COCO image captioning benchmark <ref type="bibr" target="#b22">[22]</ref>. By using only a single model, we can beat all the published RL-based methods on MS-COCO server. In particular, we significantly improve every SPICE [1] compositional scores such as object, relation, and attribute without optimizing on it. We also show promising qualitative results of visual policy reasoning over the time of generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Image Captioning</head><p>Inspired by the recent advances in machine translation <ref type="bibr" target="#b4">[4]</ref>, current image captioning approaches <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b39">39]</ref> typically follow an encoder-decoder framework, which can be considered as a neural translation task from image to text. It uses CNN-RNN architectures that encodes an image as feature vectors by CNN <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b19">19]</ref> and decodes such vectors to a sentence by RNN <ref type="bibr" target="#b12">[12]</ref>.</p><p>More recently, attention mechanisms which allows dynamic feature vectors have been introduced to the encoder-decoder framework. Xu et al. <ref type="bibr" target="#b39">[39]</ref> incorporated soft and hard attention mechanism to automatically focus on salient objects when generating corresponding words. Chen et al. <ref type="bibr" target="#b7">[7]</ref> introduced channel-wise attention besides spatial attention mechanism and Lu et al. <ref type="bibr" target="#b24">[24]</ref> proposed a visual sentinel to deal with the non-visual words when generating captions. Besides the spatial information comes from CNN feature maps, Anderson et al. <ref type="bibr" target="#b2">[2]</ref> used an object detection network to propose salient image regions with an associated feature vector as bottom-up attention mechanism. However, all those captioning approaches only focus on the current time step's visual attention and neglect to consider the visual context over time, which is the key for language compositions. To this end, we introduce the context-aware visual policy network which incorporates history visual attentions to current time step and generate more effective feature vectors fed into word policy network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Sequential Decision-Making</head><p>Most recent captioning approaches are typically trained by maximizing the likelihood of the ground-truth word at each time steps. This causes the exposure bias <ref type="bibr" target="#b27">[27]</ref> between the training and the testing phases. To mitigate it, reinforcement learning have been applied to image captioning which introduces the notion of sequential decision-making. The idea of making a series of decisions forces the agent to take into account future sequences of actions, states, and rewards. In the case of image captioning, the state is the image, previously generated words and visual context, the action is choosing next word and visual representation, and the reward can be any metric of interest.</p><p>Several attempts have been made to apply sequential decisionmaking framework to image captioning. The first work <ref type="bibr" target="#b27">[27]</ref>  where policy gradient was used to optimize the sentence-level reward. Rennie et al. <ref type="bibr" target="#b30">[30]</ref> used the classic REINFORCE algorithm <ref type="bibr" target="#b37">[37]</ref> and applied a relative baseline obtained by the current model to the final reward. In result, for each sampled caption, it has a sentence level advantage indicating how good or bad this sentence is. It assume every token makes the same contribution towards the whole sentence. Actor-Critic based method <ref type="bibr" target="#b44">[44]</ref> was also applied to image captioning by utilized two networks as Actor and Critic respectively. After that, Ren et al. <ref type="bibr" target="#b29">[29]</ref> introduced decision-making framework utilized a policy network and a value network with embedding reward to collaboratively generate captions.</p><p>In our work, we formulate the image captioning task into a sequence-level training framework where each word-level prediction policy is based on the action performed by the proposed CAVP. Our framework is optimized using policy gradient with a self-critic value which can directly optimize non-differentiable quality metrics of interest such as CIDEr <ref type="bibr" target="#b35">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">APPROACH</head><p>In this section, we explain our approach in more detail. The overview of the proposed RL-based image captioning framework is illustrated in Figure <ref type="figure" target="#fig_1">3</ref>. We first define our problem formulation, then we detail the Context-Aware Visual Policy network (CAVP) and the language policy network. Finally we discuss the sequence-level training strategy for the entire framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>We formulate the task of image captioning into a sequential decisionmaking process. In decision-making, there is an agent interacts with the environment, and then executes a series of actions, after each action, a state is observed, so as to optimize the reward when accomplishing a goal.</p><p>In image captioning, given an image I, the goal is to generate a sequential visual representations {v 1 , v 2 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , v T }, where v t is the t-th action of visual policy network, and a corresponding sentence</p><formula xml:id="formula_0">Y = {y 1 , y 2 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , y T }, y t ‚àà Œ£,</formula><p>where Œ£ is the vocabulary, y t is the t-th action of language policy network, i.e., generated word.</p><p>Our model, including the visual policy network and the language policy network, can be viewed as the agent. At time step t, the state includes the given image I, visual context {v i &lt;t } and the generated sequence so far {y i &lt;t }. An action is to predict the next visual representation v t and word y t . The reward is any sequence evaluation between the ground-truth Y –¥t and the generated Y .</p><p>For image representation, we use Faster R-CNN <ref type="bibr" target="#b28">[28]</ref> to extract image region features {r 1 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , r k } from image I. We select top k ROIs ranked according to their ROI scores in each image. For every selected region i, r i is the mean-pooling convolutional feature from this region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Context-Aware Visual Policy Network</head><p>The proposed CAVP is designed to select the most informative image features for the subsequent language policy network (LP). A brute force search of all possible contextual regions requires O(2 N ) complexity for multinomial combinations of N image regions. For linear efficiency, we approximate the overall visual policy network into 4 sub-policy networks: 1) single sub-policy network œÄ s , 2) composition sub-policy network œÄ p , 3) context sub-policy network œÄ c , and 4) output sub-policy network œÄ o .</p><p>Each sub-policy network uses a RNN with the LSTM cell <ref type="bibr" target="#b12">[12]</ref> to encode the environment state. Moreover, we use soft-attention features as the real-valued action performed by the sub-policy networks, which can be considered as the deterministic approximation for the Monte Carlo policy search <ref type="bibr" target="#b43">[43]</ref>, to reduce the sample variance caused by the diverse image regions. Here, without loss of generality, we first introduce the general structure of the sub-policy networks denoted as œÄ without any superscripts.</p><p>At time t, a sub-policy network as an agent receives environment state s t , then take an action a t ‚àº œÄ (s t |h t ; Œ∏ ), where h t is it's LSTM hidden state. Then the sub-policy network according to this action sampling a representation f from query inputs. Query inputs Q t = {q t,1 , q t,2 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , q t,d } are a set of features which are difference depending on specific sub-policy network and we will determine it in the next section. The general formulation is given by:  with the LSTM hidden state transition function:</p><formula xml:id="formula_1">f = d i=1 œÄ (a t = i)q t,i ,<label>(1)</label></formula><formula xml:id="formula_2">h t +1 = LSTM(s t , h t ).<label>(2)</label></formula><p>To compute the probability of taking action a t , we follow the additive attention mechanism <ref type="bibr" target="#b39">[39]</ref> as:</p><formula xml:id="formula_3">œÄ (a t = i) = softmax(w T a tan(W h h t + W q q t,i )),<label>(3)</label></formula><p>where œÄ (a t = i) ‚àà [0, 1], and w a , W h and W q are trainable parameters.</p><p>In this way, if we determine the environment state s t and query inputs Q t at each time steps of a sub-policy network, the sequence decision-making is known. Next, we will implement each sub-policy networks by introducing the corresponding state and query inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.1</head><p>Single Sub-policy Network. For single sub-policy network, the environment state at each time steps consist of the previous LSTM hidden state h l t -1 of the language policy network, concatenated with the mean-pooled region features r = 1 k k i=1 r i , and word embedding of the previously generated word:</p><formula xml:id="formula_4">s s t = [h l t -1 , r,W e Œ† t -1 ],<label>(4)</label></formula><p>where W e ‚àà R E√óŒ£ is a word embedding matrix of a vocabulary size Œ£, which is learned from random initialization, and Œ† t -1 is one-hot encoding of the output action (i.e. generated word) of the language policy network at time step t -1. The query inputs at each time steps are the detected region features, i.e.Q s t = {r 1 , r 2 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , r k }. We take the output of single sub-policy network as single feature at time t: v s t ‚Üê f s t . Single feature will be used in output sub-policy network (Section 3.2.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Context</head><p>Sub-policy Network. At time step t, we have the visual contexts {v i &lt;t }; however, not all contexts are helpful to generate the current time step's word. Therefore, we introduce the context sub-policy network to choose the most informative context and combine it with the detected region features. In detail, we define the environment state as:</p><formula xml:id="formula_5">s c t = [h l t -1 , r ,W e Œ† t -1 ],<label>(5)</label></formula><p>and the query inputs as Q c t = {v i &lt;t }. Then we fuse visual context representation f c t at step t, i.e. the output from context sub-policy network, with every region features to concatenate features c t,i as:</p><formula xml:id="formula_6">c t,i = W T c [f c t ; r i ], i = 1, 2, ‚Ä¢ ‚Ä¢ ‚Ä¢ , k,<label>(6)</label></formula><p>where [‚Ä¢; ‚Ä¢] indicates concatenating two vectors and W T c projecting the concatenated features to the original dimension as region features. Concatenate features will be used in composition sub-policy network (Section 3.2.3). Besides, we can also use a simple but effective way that only considering the visual context from the last time step t -1, i.e.:</p><formula xml:id="formula_7">c t,i = W T c [v t -1 , r i ], i = 1, 2, ‚Ä¢ ‚Ä¢ ‚Ä¢ , k,<label>(7)</label></formula><p>which will reduce the computation without sacrificing performance. We will discuss this approximation in Section 4.4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Composition</head><p>Sub-policy Network. The composition sub-policy network is similar to the single sub-policy network which takes the previous hidden state of the language policy network, the meanpooled region features, and an encoding of the previously generated word as environment state:</p><formula xml:id="formula_8">s p t = [h l t -1 , r,W e Œ† t -1 ].<label>(8)</label></formula><p>The query inputs of the composition sub-policy network takes concatenate features from the context sub-policy network:</p><formula xml:id="formula_9">Q p t = {c t,1 , c t,2 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , c t,k }.<label>(9)</label></formula><p>Then we take the output of composition sub-policy network as composition features at time t: v</p><formula xml:id="formula_10">p t ‚Üê f p t .</formula><p>3.2.4 Output Sub-policy Network. After obtaining the single and compositional visual features from Single SP and Comp. SP, we given the environment state of output sub-policy network by:</p><formula xml:id="formula_11">s o t = [h l t -1 , r ,W e Œ† t -1 ].<label>(10)</label></formula><p>And determined query inputs as</p><formula xml:id="formula_12">Q o t = {v s t , v p t }.</formula><p>The output of this sub-policy is thus the overall CAVP's output visual representation, i.e. v t = f o t , which will be used in language policy network at each time steps and also will be seen as a part of visual context in subsequent time steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5">Weights</head><p>Sharing. We noticed that although the above subpolicy network are difference in query inputs, but some of them have the same environment state like:</p><formula xml:id="formula_13">s c t = s s t = s p t = s o t = [h l t -1 , r ,W e Œ† t -1 ].<label>(11)</label></formula><p>To reduce the complexity of our model and alleviate over-fittings, we shared the LSTM parameters among those sub-policy networks in experiments. More ablative studies of the weight sharing is detailed in Section 4.4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Language Policy Network</head><p>At each time step, CAVP generates a context-aware visual representation most fitting to current words. Language policy network take the visual representation and the hidden state h s t of Single SP as input, then use them to update LSTM hidden state:</p><formula xml:id="formula_14">h l t +1 = LSTM([h s t , v t ], h l t ).<label>(12)</label></formula><p>To compute the distribution over all words in vocabulary, we apply a FC layer to hidden state, and after softmax layer it outputs the probability distribution of each words, given by:</p><formula xml:id="formula_15">œÄ l (y t |y 1:t -1 ) = softmax(W y h l t + b y ),<label>(13)</label></formula><p>where W y and b y are learnable weights and biases. For complete output sentences, the distribution is calculated as the product of all time step's conditional distributions:</p><formula xml:id="formula_16">œÄ l (y 1:T ) = T t =1</formula><p>œÄ l (y t |y 1:t -1 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Sequence-Level Training</head><p>For sequence-level training, we use two learning stages: 1) standard supervised learning with cross entropy loss and 2) reinforcement learning by policy gradient method using a self-critical relative base reward <ref type="bibr" target="#b30">[30]</ref>. As described in the previous section, traditional captioning models are trained using the cross entropy loss. Given a target ground-truth sequence y –¥t 1:T and a captioning model with parameters Œ∏ , the traditional supervised learning approach is used to train this network by minimizing the cross entropy loss (XE):</p><formula xml:id="formula_18">L X E (Œ∏ ) = - T t =1 log(œÄ Œ∏ (y –¥t t | y –¥t 1:t -1 )). (<label>15</label></formula><formula xml:id="formula_19">)</formula><p>This corresponds to the imitation learning of a perfect teacher in RL context, and we use the pre-trained model as the initial policy network.</p><p>To directly optimize the NLP metrics and address the exposure bias issue, we use the policy gradient method to maximize the expected reward, for instance, the CIDEr score of the generated sequences. Initializing from the cross-entropy trained model, we minimize the negative expected score:</p><formula xml:id="formula_20">L R (Œ∏ ) = -E y‚àºœÄ Œ∏ r (y 1:T ), (<label>16</label></formula><formula xml:id="formula_21">)</formula><p>where r is the metric evaluation (e.g. BCMR). Following the <ref type="bibr" target="#b30">[30]</ref> approach, the gradient of this loss can be approximated to:</p><formula xml:id="formula_22">‚àá Œ∏ L R (Œ∏ ) ‚âà -(r (y s 1:T ) -r ( ≈∑1:T ))‚àá Œ∏ log œÄ Œ∏ (y s 1:T ),<label>(17)</label></formula><p>where y s 1:T is a caption sampled according to the word distribution (i.e. Monte-Carlo sampling) and ≈∑1:T is a greedy searched caption. The REINFORCE <ref type="bibr" target="#b37">[37]</ref> algorithms explores the space of captions by sampling captions from the policy network. While training, this gradient tends to increase the probability of each words in the sampled captions if r (y s 1:T ) higher than r ( ≈∑1:T ), which can been seen as the relative baseline score, and vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Behavior</head><p>Cloning. The learning would be easier if we have some additional knowledge of the output policy. While there no any additional knowledge in the caption datasets e.g. MS-COCO, we can use a language parser <ref type="bibr" target="#b34">[34]</ref> as an existing expert output policy that can be used to provide additional supervision. More generally, if there is an expert output policy œÄ e that predicts a reasonable output policy œÄ o , we can first pre-train our model by behavioral cloning from œÄ e . This can be done by minimizing the KL-divergence D K L (œÄ e ||œÄ o ) between the expert output policy œÄ e and our output policy œÄ o , and simultaneously minimizing the captioning loss L X E with expert output policy œÄ e . This supervised behavioral cloning from the expert output policy can provide a good set of initial parameters in our output sub-policy network. Note that the above behavioral cloning procedure is only done at cross-entropy training time to obtain a supervised initialization for our model, and the expert output policy is not used at test time.</p><p>The expert output policy is not necessarily optimal, for behavioral cloning itself is not sufficient for learning the most suitable output policy for each image. After learning a good initialization by cloning the expert output policy, our model is further trained end-to-end with gradient ‚àá Œ∏ L R (Œ∏ ) computed using Eqn. <ref type="bibr" target="#b17">(17)</ref>, where the output policy œÄ o is sampled from the output policy network in our model, and the expert output policy œÄ e can be discarded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we first describe the datasets used in our experiments and the widely used evaluation metrics. Then, we go through the implementation details and the comparing methods. Finally, we analyze both of the quantitative and qualitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We used the most popular benchmark MS-COCO <ref type="bibr" target="#b22">[22]</ref> for image captioning, which contains 82,783 images for training and 40,504 for validation. Each image is human-annotated with 5 captions. As the annotations of the official test set are not publicly available, for validation of model hyperparameters and offline testing, we follow the widely used "Karpathy" splits <ref type="bibr" target="#b16">[16]</ref> in most prior works, containing 113,287 images for training, 5,000 for validation, and 5,000 for testing. We reported the results both on "Karpathy" offline split and MS-COCO online test server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Metrics</head><p>The most common metrics for caption evaluation are based on ngram similarity of reference and candidate descriptions. BLEU <ref type="bibr" target="#b26">[26]</ref> is defined as the geometric mean of n-gram precision scores, with a sentence-brevity penalty. In CIDEr <ref type="bibr" target="#b35">[35]</ref>, n-grams in the candidate and reference sentences are weighted by term frequency-inverse document frequency weights (i.e. tf-idf). Then, the cosine similarity between them are computed. METEOR <ref type="bibr" target="#b5">[5]</ref> is defined as the harmonic mean of precision and recall of exact, stem, synonym, and paraphrase matches between sentences. ROUGE <ref type="bibr" target="#b21">[21]</ref> is a measures for automatic evaluation for summarization systems via F-measures. We followed <ref type="bibr" target="#b29">[29]</ref> to call them them BCMR and used all of them as metrics.</p><p>All the above metrics are originally developed for the evaluation of text summaries or machine translations. It has been shown that there are obvious bias between those metrics and human judgment <ref type="bibr">[1]</ref>. Therefore, we further evaluated our model using SPICE [1] metric, which is defined over tuples that are divided into semantically meaningful categories such as objects, relations and attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>4.3.1 Data Pre-processing. We performed standard minimal text pre-processing: first tokenizing on white space, second converting all words into lower case, then filtering out words that occur less than 5 times, finally resulting in a vocabulary of 10,369 words. Captions are trimmed to a maximum of 16 words for computational efficiency. To generate a set of image region features r i in image captioning, we take the final output of the region proposed network <ref type="bibr" target="#b28">[28]</ref> and perform non-maximum suppression. In our implementation we used an IoU threshold of 0.7 for region proposal non-maximum suppression, and 0.3 for object class non-maximum suppression. To select salient image regions, we simply selected the top k = 36 features in each image for computation consider.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Parameter Settings.</head><p>We set the number of hidden units of each LSTM to 1,300, the number of LSTM layers to 1, the number of hidden units in the attention-like mechanism we described in Eqn. (3) to 1,024, and the size of word embedding to 1000. During the supervised learning for the cross-entropy process, we used Adam optimizer <ref type="bibr" target="#b17">[17]</ref> with base learning rate of 5e-4 and shrink it by 0.8 every 3 epochs. We start reinforcement learning after 37 epochs, we used Adam optimizer with base learning rate of 5e-5 and shrink it by 0.1 every 55 epochs. We set the batch size to 100 images and trained for up to 100 epochs. During inferencing stage, we use a beam search size of 5. While training Faster R-CNN, we follow <ref type="bibr" target="#b2">[2]</ref> and first initialize it with ResNet-101 <ref type="bibr" target="#b11">[11]</ref> pretrained with classification on ImageNet <ref type="bibr" target="#b31">[31]</ref>, then fine-tune it on Visual Genome <ref type="bibr" target="#b18">[18]</ref> with attribute labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparing Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Traditional</head><p>Framework. We first compared our models with classic methods including Google NIC <ref type="bibr" target="#b36">[36]</ref>, Hard Attention <ref type="bibr" target="#b39">[39]</ref>,</p><p>Adaptive Attention <ref type="bibr" target="#b24">[24]</ref> and LSTM-A <ref type="bibr" target="#b40">[40]</ref>. These methods follow the popular encoder-decoder architecture, trained with crossentropy loss between the predicted and ground-truth words, that is, no sequence-level training is applied. 4.4.2 RL-based Framework. We also compared our models with the RL-based methods including PG-SPIDEr-TAG <ref type="bibr" target="#b23">[23]</ref>, SCST <ref type="bibr" target="#b30">[30]</ref>, Embedding-Reward <ref type="bibr" target="#b29">[29]</ref>, and Actor-Critic <ref type="bibr" target="#b44">[44]</ref>. These methods use sequence-level training with various reward returns such as the metrics mentioned above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Ablative Models.</head><p>We extensively investigated ablative structures and settings of our model to gain insights about how and why it works: 1) Single: we only used the single sub-policy network as the visual policy network without context. 2) CAVP 4c : we shared the LSTM weights among all 4 sub-policy networks (i.e.context, single, compositional and output sub-policy networks) with fully context-aware. Attention that we only shared LSTM parameters and not all parameters in a sub-policy network. 3) CAVP 4p : we shared LSTM weights among all 4 sub-policy networks (i.e., context, single, compositional and output sub-policy networks) and simply used the previous visual feature as visual context. 4) CAVP 3p+scratch : we shared LSTM weights among 3 sub-policy networks (i.e.context, single, and composition sub-policy networks) and simply used the previous visual feature as visual context. And while training output sub-policy network, we did not use expert policy and trained from scratch. 5) CAVP 3p+cloning : we shared the weights among 3 sub-policy networks (i.e.context, single and composition sub-policy networks) and simply used the previous visual feature as visual context. And the output sub-policy network is cloning from expert policy.  <ref type="table">2</ref>, we evaluated our model compared with the state-of-the-art methods. We found that almost all RL-based methods outperform traditional methods. The reason is that RL addresses the loss-evaluation mismatch problem and included the inference process in training to address the exposure bias problem. We can also find that our CAVP outperforms other non-context methods. This is because the visual context information is useful for current word generation and the policy makes better decisions. In particular, we achieved state-of-the-art performance under all metrics on "Karpathy" test split. Besides the BCMR metrics, our SPICE scores also outperform all the other methods. SPICE correlates with human judgment better and can be detailed into semantic categories. Our CAVP 4p model improves all category scores compared with the Up-down <ref type="bibr" target="#b2">[2]</ref> model shown in Figure <ref type="figure" target="#fig_3">4</ref>. 4.5.2 Qualitative Analysis. To better understand our CAVP, we show some qualitative visualizations as well as the output subpolicy network's predictions in Figure <ref type="figure" target="#fig_4">5</ref>. Take Figure <ref type="figure" target="#fig_4">5a</ref> as an example, after we generated "a young boy", we first focus on the visual context, the boy's hand which is holding something, and then we want to find visual regions that the boy is holding, so we focus on the toothbrush in the boy's mouth. Note that both hand and toothbrush we generated is exactly the word "brushing". In Figure <ref type="figure" target="#fig_4">5c</ref> and 5d, although they generate the same words, the context of these two words are different from Figure <ref type="figure" target="#fig_4">5c</ref> where the context is "kite";  <ref type="table">2</ref>: Performance comparisons on MS-COCO "Karpathy" offline split. B@n is short for BLEU-n, M is short for ME-TEOR, R is short for ROUGE, C is short for CIDEr, and S is short for SPICE. while in Figure <ref type="figure" target="#fig_4">5d</ref>, the context is "people" when we want to caption what the people are doing. By applying our CAVP, we can generate those captions both successfully with deep understanding of image scenes. Besides showing a single important word of the generated sequence, we also visualize the whole policy decision across the whole sentence generation in Figure <ref type="figure" target="#fig_5">6</ref>. Take the first sentence as an example, we notice that our context-aware model can not only focus on some single objects such as "man", "skis", and "snow", but also the compositional word "standing", connecting "man standing in snow".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Comparisons with State-of-The-Art</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3">Online Results</head><p>. Table <ref type="table" target="#tab_1">1</ref> reports the performances of our single model without any ensemble on the official MS-COCO evaluation server<ref type="foot" target="#foot_0">1</ref> . Note that the highest results are from industrial companies while we are academic groups who lack of abundant computing sources. Moreover, we only used a single model while   <ref type="table">4</ref>: Ablative performances on the MS-COCO "Karpathy" offline split with respect to various metrics as the reward.</p><p>theirs are fused models. With more computing resource for parameter fine-tuning, we believe that there will be a large space for improving our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Ablative Studies</head><p>4.6.1 Architecture. Under the framework there are a lot of variants, we do some ablative studies for insights into our framework. As shown in Table <ref type="table" target="#tab_2">3</ref>, we found that sharing the weights between all the 4 sub-policy networks gains the best result. It is because without weight sharing, the model is easily overfitted. We also found that CAVP 4p is similar to the CAVP 4c which indicates that the most informative visual context is from the last time step and little comes from older ones. Comparing the CAVP 3p+scrach model and CAVP 3p+cloning model, with the expert policy guiding, there is little improvement. This indicates that the off-the-sheaf language parser is not very suitable to the visual-language task and the output sub-policy network can learned from scratch without any initialization.</p><p>4.6.2 Reward. For sequence-level training by policy gradient, the reward function r can be any metrics, e.g. BLEU, ROUGE, METEOR, CIDEr and SPIDEr <ref type="bibr" target="#b23">[23]</ref> (which combining the CIDEr and SPICE scores equally as the reward). Optimizing for different metrics leads to different performance. In general, as shown in Table <ref type="table">4</ref>, we found that optimizing for a specific metric results in the best performance on the same metric. And optimizing for CIDEr and SPIDEr gives the best overall performance, but the SPIDEr is more time consuming as the SPICE metric evaluation is very slow. Thus, we chose the CIDEr as the optimizing objective in most our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we presented a sequence-level image captioning framework using a novel Context-Aware Visual Policy network (CAVP). Unlike existing RL-based methods, our framework takes the advantage of visual context in compositional visual reasoning, which is beneficial for image captioning. Compared against traditional visual attention which only fixes a single image region at every step, CAVP can attend to complex visual compositions over time. To the best of our knowledge, we are the first RL-based image captioning model which incorporates visual context into sequential visual reasoning. We conducted extensive experiments as well as ablative studies to show the effectiveness of CAVP. Our framework can significantly boost the performances of the RL-based image captioning methods and achieves top ranking performances on MS-COCO server. Moving forward, we are going to 1) integrate the visual policy and language policy into a Monte Carlo tree search strategy for sentence generation, and 2) apply CAVP in other sequential decision-making tasks such as visual Q&amp;A and visual dialog.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Overview of the proposed RL-based image captioning framework. It consists of the proposed CAVP for visual feature composition and the language policy for sentence generation. CAVP contains 4 sub-policy (SP) networks: Single SP (Section 3.2.1), Context SP (Section 3.2.2), Composition SP (Section 3.2.3), and Output SP (Section 3.2.4). The notations used include visual context: {v i &lt;t }, query inputs: {q t,i }, output visual feature: v t , and predicted words: y t .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>4. 5 . 1</head><label>51</label><figDesc>Quantitative Analysis. As shown in Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The performance of our CAVP 4p model and Up-Down model which is very similar to our Single policy model. All SPICE category scores are improved by CAVP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Qualitative examples where top matrix shows the output policy network action probabilities and the bottom image shows the decision with maximum probability for composition features. The blue bounding boxes are the context regions and the red bounding boxes are the current regions which concatenated with context regions.</figDesc><graphic coords="7,350.40,393.64,79.27,96.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: For each generated word, we visualized the attended image regions, outlining the region with the maximum policy probability in bounding box. The blue bounding boxes are the visual context representation regions and the red bounding boxes are the regions decided by single policy network.</figDesc><graphic coords="8,307.67,90.99,240.70,109.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>proposed by Ranzato et al. trained RNN-based sequence model by policy gradient method based on a Monte Carlo search technique, as input to step t for sentence generation. (b) RL-based framework focuses on sequence-level training by directly input the predicted word y t -1 to LP at step t. (c) our framework explicitly takes the history visual actions {v i &lt;t } as visual context to step t.</figDesc><table><row><cell></cell><cell>ùë¶ ùë°-1</cell><cell>ùë¶ ùë°-1 ùëîùë°</cell><cell>ùë¶ ùë°</cell><cell>ùë¶ ùë° ùëîùë°</cell><cell>ùë¶ ùë°+1</cell><cell></cell><cell></cell><cell>ùë¶ ùë°-1</cell><cell>ùë¶ ùë°</cell><cell>ùë¶ ùë°+1</cell><cell></cell><cell></cell><cell>ùë¶ ùë°-1</cell><cell>ùë¶ ùë°</cell><cell>ùë¶ ùë°+1</cell></row><row><cell></cell><cell>LP</cell><cell></cell><cell>LP</cell><cell></cell><cell>LP</cell><cell></cell><cell></cell><cell>LP</cell><cell>LP</cell><cell>LP</cell><cell></cell><cell></cell><cell>LP</cell><cell>LP</cell><cell>LP</cell></row><row><cell>‚ãØ</cell><cell>ùë£ ùë°-1</cell><cell></cell><cell>ùë£ ùë°</cell><cell></cell><cell>ùë£ ùë°+1</cell><cell>‚ãØ</cell><cell>‚ãØ</cell><cell>ùë£ ùë°-1</cell><cell>ùë£ ùë°</cell><cell>ùë£ ùë°+1</cell><cell>‚ãØ</cell><cell>‚ãØ</cell><cell>ùë£ ùë°-1</cell><cell>ùë£ ùë°</cell><cell>ùë£ ùë°+1</cell><cell>‚ãØ</cell></row><row><cell></cell><cell>VP</cell><cell></cell><cell>VP</cell><cell></cell><cell>VP</cell><cell></cell><cell></cell><cell>VP</cell><cell>VP</cell><cell>VP</cell><cell></cell><cell></cell><cell>VP</cell><cell>VP</cell><cell>VP</cell></row><row><cell></cell><cell>ùë£ 0</cell><cell></cell><cell>ùë£ 0</cell><cell></cell><cell>ùë£ 0</cell><cell></cell><cell></cell><cell>ùë£ 0</cell><cell>ùë£ 0</cell><cell>ùë£ 0</cell><cell></cell><cell></cell><cell>{ùë£ ùëñ&lt;ùë°-1 }</cell><cell>{ùë£ ùëñ&lt;ùë° }</cell><cell>{ùë£ ùëñ&lt;ùë°+1 }</cell></row><row><cell></cell><cell cols="5">(a) Traditional Framework</cell><cell></cell><cell></cell><cell cols="3">(b) RL-based Framework</cell><cell></cell><cell></cell><cell cols="3">(c) Our Framework</cell></row><row><cell cols="16">Figure 2: The evolution of the encoder-decoder framework for image captioning. LP: language policy. VP: visual policy. v t : visual feature at time t. y t : predicted word at time t. y –¥t t : ground-truth word at time t. (a) traditional framework only focuses –¥t on the word-level prediction by exposing the ground-truth word y t -1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Highest ranking published image captioning results on the online MSCOCO test server. Except for BLUE-1 which is of little interest, our single model optimized with CIDEr, outperforms previously published works using all the other metrics.</figDesc><table><row><cell>Model</cell><cell>B@4</cell><cell>M</cell><cell>R</cell><cell>C</cell><cell>S</cell></row><row><cell>Google NIC[36]</cell><cell cols="2">32.1 25.7</cell><cell>-</cell><cell>99.8</cell><cell>-</cell></row><row><cell>Hard-Attention[39]</cell><cell cols="2">24.3 23.9</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Adaptive[24]</cell><cell cols="5">33.2 26.6 54.9 108.5 19.4</cell></row><row><cell>LSTM-A[40]</cell><cell cols="3">32.5 25.1 53.8</cell><cell>98.6</cell><cell>-</cell></row><row><cell>PG-SPIDEr[23]</cell><cell cols="4">32.2 25.1 54.4 100.0</cell><cell>-</cell></row><row><cell>Actor-Critic[44]</cell><cell cols="4">34.4 26.7 55.8 116.2</cell><cell>-</cell></row><row><cell cols="4">EmbeddingReward[29] 30.4 25.1 52.5</cell><cell>93.7</cell><cell>-</cell></row><row><cell>SCST[30]</cell><cell cols="4">35.4 27.1 56.6 117.5</cell><cell>-</cell></row><row><cell>StackCap[10]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>36.1 27.4 56.9 120.4 20.9 Up-Down[2] 36.3 27.7 56.9 120.1 21.4 Ours 38.6 28.3 58.5 126.3 21.6 Table</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>CAVP 3p+cloning 38.3 27.8 58.0 124.6 21.4 CAVP Ablative performance comparisons on MS-COCO.B@n is short for BLEU-n, M is short for METEOR, R is short for ROUGE, C is short for CIDEr, and S is short for SPICE.</figDesc><table><row><cell>Model</cell><cell></cell><cell>B@4</cell><cell>M</cell><cell>R</cell><cell>C</cell><cell>S</cell></row><row><cell>Single</cell><cell></cell><cell cols="5">37.5 27.7 57.9 121.9 21.0</cell></row><row><cell cols="2">CAVP 3p+scrach</cell><cell cols="5">37.8 28.0 58.2 124.5 21.3</cell></row><row><cell>Training</cell><cell></cell><cell cols="4">Evaluation Metric</cell></row><row><cell>Metric</cell><cell cols="6">BLEU4 ROUGE METEOR CIDEr SPICE</cell></row><row><cell>BLEU</cell><cell>38.8</cell><cell>57.7</cell><cell></cell><cell>27.3</cell><cell>114.5</cell><cell>20.7</cell></row><row><cell>ROUGE</cell><cell>38.1</cell><cell>59.1</cell><cell></cell><cell>27.8</cell><cell>120.0</cell><cell>20.8</cell></row><row><cell>METEOR</cell><cell>33.6</cell><cell>57.6</cell><cell></cell><cell>29.6</cell><cell>113.0</cell><cell>22.8</cell></row><row><cell>CIDEr</cell><cell>38.3</cell><cell>58.4</cell><cell></cell><cell>28.2</cell><cell>126.4</cell><cell>21.6</cell></row><row><cell>SPIDEr</cell><cell>37.8</cell><cell>58.0</cell><cell></cell><cell>27.8</cell><cell>125.3</cell><cell>23.1</cell></row><row><cell>Table</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>4p 38.3 28.2 58.4 126.4 21.6 CAVP 4c 38.6 28.3 58.5 126.3 21.6</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://competitions.codalab.org/competitions/3221#results</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>MM'18, October 22-26, 2018, Seoul, Republic of Korea</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported by the National Key R&amp;D Program of China under Grant 2017YFB1300201, the National Natural Science Foundation of China (NSFC) under Grants 61622211, 61472392 and 61620106009 as well as the Fundamental Research Funds for the Central Universities under Grant WK2100100030.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Pg-Spider-Tag</forename></persName>
		</author>
		<idno>75.1 91.6</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Spice: Semantic propositional image caption evaluation</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Bottom-up and top-down attention for image captioning and VQA</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07998</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">METEOR: An automatic metric for MT evaluation with improved correlation with human judgments</title>
		<author>
			<persName><forename type="first">Satanjeev</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</title>
		<meeting>the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Structcap: Structured semantic embedding for image captioning</title>
		<author>
			<persName><forename type="first">Fuhai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongjian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunsheng</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Multimedia Conference</title>
		<meeting>the 2017 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="46" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sca-cnn: Spatial and channel-wise attention in convolutional networks for image captioning</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khushi</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avi</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deshraj</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Jos√©</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><surname>Batra</surname></persName>
		</author>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Hao Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Forrest</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rupesh</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Doll√°r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><surname>Platt</surname></persName>
		</author>
		<title level="m">From captions to visual concepts and back</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Jiuxiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsuhan</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.03376</idno>
		<title level="m">Stack-captioning: Coarse-to-fine learning for image captioning</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J√ºrgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to Reason: End-To-End Module Networks for Visual Question Answering</title>
		<author>
			<persName><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Revisiting visual question answering baselines</title>
		<author>
			<persName><forename type="first">Allan</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inferring and Executing Programs for Visual Reasoning</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>In NIPS</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Building machines that learn and think like people</title>
		<author>
			<persName><surname>Brenden M Lake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tomer D Ullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Text Summarization Branches Out</title>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll√°r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitnick</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Improved image captioning via policy gradient optimization of spider</title>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhai</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00370</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Knowing when to look: Adaptive attention via a visual sentinel for image captioning</title>
		<author>
			<persName><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6632</idno>
		<title level="m">Deep captioning with multimodal recurrent neural networks (m-rnn)</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Aurelio</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06732</idno>
		<title level="m">Sequence level training with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning-based image captioning with embedding reward</title>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xutao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03899</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Self-critical sequence training for image captioning</title>
		<author>
			<persName><forename type="first">Etienne</forename><surname>Steven J Rennie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youssef</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jarret</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaibhava</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><surname>Goel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00563</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Toward memory-based reasoning</title>
		<author>
			<persName><forename type="first">Craig</forename><surname>Stanfill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Waltz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<date type="published" when="1986">1986. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT press Cambridge</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Feature-rich part-of-speech tagging with a cyclic dependency network</title>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter</title>
		<meeting>the 2003 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics on Human Language Technology</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Samy Bengio, and Dumitru Erhan. 2015. Show and tell: A neural image caption generator</title>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">Williams</forename><surname>Ronald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
	<note>Reinforcement Learning</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Scene graph generation by iterative message passing</title>
		<author>
			<persName><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Rich Zemel, and Yoshua Bengio. 2015. Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Boosting image captioning with attributes</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yehao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenReview</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient</title>
		<author>
			<persName><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Visual translation embedding network for visual relation detection</title>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zawlin</forename><surname>Kyaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Grounding Referring Expressions in Images by Variational Context</title>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09601</idno>
		<title level="m">Actor-Critic Sequence Training for Image Captioning</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
