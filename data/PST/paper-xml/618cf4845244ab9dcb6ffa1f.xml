<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep graph learning of inter-protein contacts</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ziwei</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Technological Institute at Chicago</orgName>
								<address>
									<postCode>60637</postCode>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Jinbo</forename><surname>Xu</surname></persName>
							<email>jinboxu@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Technological Institute at Chicago</orgName>
								<address>
									<postCode>60637</postCode>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep graph learning of inter-protein contacts</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btab761</idno>
					<note type="submission">Received on August 13, 2021; revised on October 6, 2021; editorial decision on October 26, 2021; accepted on November 4, 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Motivation: Inter-protein (interfacial) contact prediction is very useful for in silico structural characterization of protein-protein interactions. Although deep learning has been applied to this problem, its accuracy is not as good as intra-protein contact prediction. Results: We propose a new deep learning method GLINTER (Graph Learning of INTER-protein contacts) for interfacial contact prediction of dimers, leveraging a rotational invariant representation of protein tertiary structures and a pretrained language model of multiple sequence alignments. Tested on the 13th and 14th CASP-CAPRI datasets, the average top L/10 precision achieved by GLINTER is 54% on the homodimers and 52% on all the dimers, much higher than 30% obtained by the latest deep learning method DeepHomo on the homodimers and 15% obtained by BIPSPI on all the dimers. Our experiments show that GLINTER-predicted contacts help improve selection of docking decoys.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Proteins perform functions by interacting with other molecules or forming protein complexes. As a result, the full characterization of protein-protein interactions with structural details is crucial to atom-level understanding of protein functions. The in silico structural characterization of protein complexes, or quaternary protein structure prediction, is a longstanding challenge in computational structural biology. Given individual protein chains (and possibly their structures), interfacial contact prediction aims to predict which pairs of residues on the protein surface are geometrically close to each other after the protein chains bind together. Interfacial contacts may facilitate generating and filtering docking decoys <ref type="bibr" target="#b0">(Baldassi et al., 2014;</ref><ref type="bibr" target="#b8">Geng et al., 2020;</ref><ref type="bibr" target="#b12">Hopf et al., 2014;</ref><ref type="bibr" target="#b21">Ovchinnikov et al., 2014)</ref>, and reveal important biophysical properties and evolutionary information of protein interfaces <ref type="bibr" target="#b36">(Uguzzoni et al., 2017)</ref>. They are also useful for the redesign of protein-protein interfaces <ref type="bibr" target="#b17">(Laine and Carbone, 2015)</ref> and prediction of binding affinity <ref type="bibr" target="#b37">(Vangone and Bonvin, 2015)</ref>.</p><p>Co-evolution analysis by global statistical methods <ref type="bibr" target="#b2">(Burger and van Nimwegen, 2008;</ref><ref type="bibr" target="#b39">Weigt et al., 2009)</ref> has been used for interprotein contact prediction. A recent study <ref type="bibr" target="#b3">(Cong et al., 2019)</ref> showed that co-evolution-based in silico protein-protein interaction screening methods produced more true protein-protein interactions than high-throughput experimental techniques. Nevertheless, accurate co-evolution analysis needs a large number of sequence homologs and thus, may not work well on a large portion of heterodimers for which it is very challenging to find sufficient number of interacting paralogs (interlogs) <ref type="bibr" target="#b1">(Bitbol et al., 2016;</ref><ref type="bibr" target="#b9">Gueudre ´et al., 2016;</ref><ref type="bibr" target="#b45">Zeng et al., 2018)</ref>. On the other hand, protein language models, which are trained on individual protein sequences or multiple sequence alignment (MSAs), are shown to perform similarly as or better than global statistical methods on intra-chain contact prediction when few sequence homologs are available <ref type="bibr" target="#b25">(Rao et al., 2021;</ref><ref type="bibr" target="#b26">Rives et al., 2021)</ref>. It was shown before that a deep learning model trained by individual protein chains works fine on protein complex contact prediction <ref type="bibr" target="#b45">(Zeng et al., 2018;</ref><ref type="bibr" target="#b47">Zhou et al., 2018)</ref>. Therefore, we hypothesize that a deep language model trained on individual protein chains may also generalize well to protein-protein interactions, reducing the required number of interlogs. Protein language models are also much faster since they require only one-time forward computation during inference and thus, more suitable for proteome-scale screening of protein-protein interactions.</p><p>RaptorX ComplexContact <ref type="bibr" target="#b45">(Zeng et al., 2018;</ref><ref type="bibr" target="#b47">Zhou et al., 2018</ref>) possibly is the first deep learning method for interfacial contact prediction. It is mainly developed for heterodimers, although can be used for homodimers. Nevertheless, its deep models are purely trained on individual protein chains instead of protein complexes. Further, ComplexContact does not make use of any (experimental or predicted) structures of constituent monomers of a dimer. Recently, some deep learning methods are developed specifically for contact prediction of a homodimer, e.g. DNCON_inter (Quadir  <ref type="bibr">et al., 2021)</ref> and DeepHomo <ref type="bibr" target="#b44">(Yan and Huang, 2021)</ref>, both using ResNet originally implemented in RaptorX <ref type="bibr" target="#b38">(Wang et al., 2017)</ref>. In addition to evolution information, DeepHomo uses docking maps, native intra-chain contacts, and experimental structural features derived from monomers to achieve state-of-the-art performance. However, it is slow in calculating docking maps and thus, cannot scale well to proteome-scale prediction. Some deep learning methods also use learned representations of tertiary structures, including voxels <ref type="bibr" target="#b5">(Derevyanko and Lamoureux, 2019;</ref><ref type="bibr" target="#b35">Townshend et al., 2019)</ref> and radial/point cloud representations on protein surfaces <ref type="bibr" target="#b4">(Dai and Bailey-Kellogg, 2021;</ref><ref type="bibr" target="#b7">Gainza et al., 2020;</ref><ref type="bibr" target="#b34">Sverrisson et al., 2020)</ref>. Meanwhile, some representations include anisotropy information in the structures <ref type="bibr" target="#b6">(Fout et al., 2017;</ref><ref type="bibr" target="#b23">Pittala and Bailey-Kellogg, 2020)</ref> while others do not.</p><p>Given the tremendous progress in protein structure prediction <ref type="bibr">(Jing and Xu, 2020;</ref><ref type="bibr" target="#b15">Jumper et al., 2021;</ref><ref type="bibr" target="#b38">Wang et al., 2017;</ref><ref type="bibr" target="#b41">Xu, 2019;</ref><ref type="bibr" target="#b42">Xu et al., 2021)</ref> and the fast growing number of protein sequences, it is important to leverage predicted structures of constituent monomers and large sequence corpus to produce accurate, proteome-scale interfacial contact predictions. An interfacial contact prediction method shall effectively extract coevolution signals from a small number of interlogs, and make use of predicted structures of constituent monomers. Here, we propose a new supervised deep learning method GLINTER for interfacial contact prediction that integrates representations learned from (experimental and predicted) monomer structures and attentions generated by the MSA Transformer (ESM-MSA) <ref type="bibr" target="#b25">(Rao et al., 2021)</ref> from interlogs of the dimer under prediction. GLINTER applies to both heterodimers and homodimers, outperforming ComplexContact, DeepHomo and BIPSPI on the 13th and 14th CASP-CAPRI datasets. The contacts predicted by GLINTER may also improve the ranking of the HDOCK-generated docking decoys <ref type="bibr" target="#b43">(Yan et al., 2017)</ref>. Further, our method runs very quickly, which makes it suitable for proteomescale study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Materials and methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Network architecture</head><p>As shown in Figure <ref type="figure" target="#fig_0">1</ref>, our network, denoted as GLINTER, consists of two major modules: a Siamese graph convolutional network (GCN) <ref type="bibr" target="#b10">(Hashemifar et al., 2018)</ref> and a 16-block ResNet <ref type="bibr" target="#b11">(He et al., 2016)</ref>. The GCN extracts local features from three types of graphs derived from monomer structures. The ResNet takes as input the outputs of the GCN module and the attention weights generated by the MSA Transformer <ref type="bibr" target="#b25">(Rao et al., 2021)</ref> and yields interfacial contact prediction. One ResNet block has two convolutional layers, each with 96 filters and a 333 kernel. ELU and BatchNorm are used in each block. ResNet is connected to a fully connected layer and a softmax layer for contact probability prediction.</p><p>At each graph convolution layer (denoted as CaConv), we calculate the message for a graph edge and node as follows. For an edge e, we feed its feature and the features of its two ends to a subnetwork to generate a message. For a node q, we first aggregate all messages of its adjacent nodes using max pooling, and then pass the result to a subnetwork to generate a message of q, i.e. gq¼g(maxv2Nqf <ref type="bibr">([xq,xv,e(q,v)</ref>]))where x q is the feature of node q, v is a node in the neighborhood N q of q, x v is the feature of v, e(q,v) is the feature of edge (q, v) and the non-linear functions g and f are two fully connected layers of 128 hidden units with BatchNorm and ReLU.</p><p>Both coordinates and normals are used to represent the geometric properties of a monomer structure <ref type="bibr" target="#b34">(Sverrisson et al., 2020)</ref>. We standardize the geometric features so that they are invariant to the coordinate system used by the monomer structure. While calculating an message for any node q (i.e. computation of f), all the adjacent nodes of q are first translated using q as the origin, and then rotated using its predefined local reference frame <ref type="bibr" target="#b22">(Page `s et al., 2019;</ref><ref type="bibr" target="#b28">Sanyal et al., 2020)</ref>. The standardized features are then concatenated with other features to form the actual inputs of function f.</p><p>We use a separate graph convolution network (GCN) module to process each graph. When multiple graphs are used for a monomer, the outputs of all its GCN modules are concatenated to form a single output vector of this monomer. The outputs of two monomers are then outer-concatenated to form a pairwise representation of this dimer. When the ESM row attention weight is used, the attention matrix generated by Facebook's MSA Transformer is concatenated to the pairwise representation, which is then fed to the ResNet for interfacial contact probability prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph representation of protein structures</head><p>We build three different graphs from one protein structure: residue graph, atom graph and surface graph. In a residue graph, a node is a residue represented by its CA atom, and there is an edge between two residue nodes if and only if the Euclidean distance between their CA atoms is within a certain cutoff, e.g. 8 A ˚. In an atom graph, a node is a heavy atom or a residue represented by its CA atom, and there is one edge between one residue node and one atom node if and only if their Euclidean distance is within a certain cutoff.</p><p>We use Reduce <ref type="bibr" target="#b40">(Word et al., 1999)</ref>, MSMS <ref type="bibr" target="#b27">(Sanner et al., 1996)</ref> and trimesh <ref type="bibr" target="#b5">(Dawson-Haggerty,M et al., 2019)</ref> to construct the triangulated surface of a protein structure (detailed in Supplementary File). The surface can be essentially interpreted as a mesh enclosing the protein. Two neighboring triangles in the surface share either one edge or at least one vertex. In a surface graph, one node represents one residue or one vertex on the triangulated surface. There is one edge between one residue node and one triangle vertex if and only if their Euclidean distance is within a certain cutoff. It takes only a few seconds to build a surface graph and thus, our method scales well on large-scale prediction <ref type="bibr" target="#b3">(Cong et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Features</head><p>Supplementary Table <ref type="table" target="#tab_2">S2</ref> summarizes all the features. The geometric features of a residue node include its coordinates and a local reference frame derived from the N-CA-C plane. As shown in Supplementary Figure <ref type="figure">S3</ref>, it uses the CA-C bond as the x-axis, the vector perpendicular to the plane formed by the N-CA and CA-C bonds as the z-axis, and their cross-product as the y-axis. Such a representation is rotation invariant and thus, may generalize well without data augmentation in contrast to the network that is not rotation invariant. The other features of a residue node include position-specific scoring matrix (PSSM), residue solvent accessible surface areas (summation of the solvent accessible surface areas of all atoms in the residue), the one-hot encoding of amino acid type, and the sequence index of the residue divided by the protein sequence length (which is used to provide order information for neural network architectures that are order invariant) <ref type="bibr">(Jing and Xu, 2020)</ref>.</p><p>In an atom graph, an edge has a binary feature called 'edge type'. It is equal to 1 if the nodes of this edge belong to the same residue. An atom is encoded by a 10-dimensional 1-hot vector, indicating four backbone atom types (CA, N, C, O) and six side chain atom types (CB, C, N, O, S, H).</p><p>In a surface graph, we use the coordinates and normals generated by MSMS as the features of a triangle vertex <ref type="bibr" target="#b7">(Gainza et al., 2020)</ref>, which indicate the contour and orientation of some local patches on the surfaces. Normals are initially computed by MSMS, then validated by trimesh's default protocol.</p><p>Coevolution signals generated by Facebook's MSA transformer <ref type="bibr" target="#b25">(Rao et al., 2021)</ref> We use the row attention weights generated by the MSA Transformer as interfacial co-evolution signals. We build a joint MSA for a heterodimer using the protocol proposed by ComplexContact <ref type="bibr" target="#b45">(Zeng et al., 2018)</ref>. For a homodimer, we simply concatenate each sequence in the MSA with itself. We then select a diverse set of sequences from the joint MSA as the input of the MSA Transformer. That is, we filter the MSA with HHfilter <ref type="bibr" target="#b32">(Steinegger et al., 2019)</ref> and assign Henikoff weights to sequences, as detailed in the Supplementary File. We further symmetrized the generated inter-chain attentions, following the MSA Transformer's protocol <ref type="bibr" target="#b25">(Rao et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Datasets</head><p>Following DeepHomo (Yan and Huang, 2021), we say there is one true contact between two residues (of two monomers) if in the experimental complex structure, the minimal distance between their respective heavy atoms is less than 8 A ˚. We define the interfacial contact density of a given dimer by N/(L1L2), where N is the number of inter-protein contacts and L 1 and L 2 are the respective lengths of the constituent monomers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CASP-CAPRI data</head><p>We use all 32 dimers (23 homodimers and 9 heterodimers) with at most 1000 residues in the 13th and 14th CASP-CAPRI datasets 40 as our test set. We do not include the dimers with more than 1000 residues since Facebook's MSA Transformer cannot handle such a large protein. To avoid redundancy between our training and test sets and to fairly compare GLINTER with recently published methods, we do not use the 11th and 12th CASP-CAPRI data. We run HHblits on the 'uniclust30_2016_09' database to build MSAs for individual chains and then concatenate two MSAs to form a joint MSA for a heterodimer using the method described in ComplexContact <ref type="bibr" target="#b45">(Zeng et al., 2018)</ref>. We use monomer (bound) experimental structures as inputs since their unbound structures are unavailable. We also tested the 3D structure models of individual chains predicted by AlphaFold <ref type="bibr" target="#b14">(Jumper et al., 2020;</ref><ref type="bibr">Senior et al., 2020)</ref> in CASP13 and 14, except for T0974s2 which did not have a predicted 3D model. The median interfacial contact density of this dataset is 1.79%. Calculated by FreeSASA <ref type="bibr" target="#b19">(Mitternacht, 2016)</ref>, the median buried solvent accessible surface area (SASA) of this dataset is 2507A ˚2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D complex data</head><p>Our training set has 5306 homodimers and 1036 heterodimers derived from 3DComplex <ref type="bibr" target="#b18">(Levy et al., 2006)</ref>. We do not include the dimers with more than 1000 residues due to MSA Transformer's limit <ref type="bibr" target="#b25">(Rao et al., 2021)</ref>. We say two dimers are at most x% similar, if the maximum sequence identity between their constituent monomers is no more than x% and build a joint MSA as described in the previous subsection.</p><p>The median interfacial contact density of the training set is 0.76%. The median buried SASA of the training set is 2393.A ˚2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PDB2018 data</head><p>We build two more test sets from the complexes released to PDB after January 1, 2018. One test set (denoted as 'HomoPDB2018') has 165 homodimers and the other one (denoted as 'HeteroPDB2018') has 72 heterodimers. We define homodimers and heterodimers in the same way as the 3DComplex data. We exclude dimers similar to the training set, judged by MMseqs2 E-value &lt; 1. We cluster dimers using the 40% sequence identity threshold and also remove dimers with interfacial contact density &lt; 0.7%, which is slightly lower than the median interfacial contact density of the training set. The medians of the buried SASAs of 'HomoPDB2018' and 'HeteroPDB2018' are 2557 and 2346 A ˚2, respectively. The medians of the interfacial contact densities of 'HomoPDB2018' and 'HeteroPDB2018' are 2.41% and 3.52%, respectively. It should be noted that although we remove dimers similar to our training set, there may be some redundancy between our test dimers and the training sets used by the other competing methods. Therefore, the estimated performance of the competing methods on the PDB2018 data may be overly optimistic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Training and evaluation</head><p>We use weighted cross-entropy as the loss function since the interfacial contact density is very small (the median of the training set is 0.76%). We initially trained our network on a small training subset using weights 5, 10, 50 and 100 and found that the weight 5 yields the best average top-10 precision in the first few epochs. So in the formal training, we set the weight of a contact to be five times that of a non-contact. We trained our deep models using Adam as the optimizer <ref type="bibr" target="#b16">(Kingma et al., 2014)</ref>, with the hyperparameters b1¼0.9,b2¼0.9999,¼1eÀ8. The learning rate is initialized to 0.0001 and reduced by half every four epochs. All models are trained for 20 epochs on two Titan X GPUs, with minibatch size 1 on each GPU. It takes 20-40 min to train one epoch. For a given hyperparameter setting, we select the model with the best top-10 precision on the validation data as the final model. Since our deep network is rotation invariant, we do not augment the training set by rotating a monomer multiple times. Nevertheless, we randomly rotate a monomer once before training to prevent our deep network from learning unexpected artifacts in the dataset. For a heterodimer, we use both of the orders of its two proteins in training. For evaluation, we predict two contact maps for one heterodimer by exchanging the order of its two proteins, and then compute the geometric average of the two predicted contact map probability matrices as the final prediction.</p><p>We evaluate contact prediction in terms of top k precision where k¼10,25,50,L/10 and L/5 and L is the length of the shorter protein in a dimer. When the number of native contacts is less than k, we still use k as the denominator while computing the top k precision. Inter-chain contact maps are more sparse than intra-chain contact maps, so we evaluate a smaller number of predicted inter-chain contacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Methods to compare</head><p>We compare GLINTER with DeepHomo, ComplexContact and BIPSPI. DeepHomo is a ResNet-based method developed for only homodimers. ComplexContact is a sequence-only and ResNet-based method developed mainly for heterodimers. Both DeepHomo and ComplexContact take as input the coevolution information computed by CCMpred <ref type="bibr" target="#b29">(Seemayer et al., 2014)</ref> while GLINTER does not. BIPSPI works for both homodimers and heterodimers and can take both structures and MSAs as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>We test our method with the bound experimental structures while comparing it with BIPSPI, DeepHomo and ComplexContact, as mentioned in Section 2.5. We also study the impact of the quality of predicted structures on our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Evaluation of interfacial contact prediction</head><p>Performance on the CASP-CAPRI data As shown in Table <ref type="table" target="#tab_1">1</ref>, tested on the 23 test homodimers, GLINTER has 54% top 10 precision and 51% top L/10 precision where L is the sum of the two monomer protein sequence lengths, while DeepHomo has 30% top 10 precision and 27% top L/10 precision. Tested on the nine heterodimers, GLINTER has 44% top 10 precision and 48% top L/10 precision, while ComplexContact has 14% top 10 precision and 14% top L/10 precision. Even using the monomer structures predicted by AlphaFold-1 and AlphaFold-2 as input, GLINTER has 43% top 10 precision on the homodimers and 24% top 10 precision on the heterodimers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance on the PDB2018 data</head><p>As shown in Table <ref type="table" target="#tab_1">1</ref>, tested on the 165 HomoPDB2018 homodimers, GLINTER has 48% top 10 precision, while BIPSPI and DeepHomo have 20 and 24% top 10 precision, respectively. Tested on the 72 HeteroPDB2018 targets, GLINTER has 47% top 10 precision, while BIPSPI and ComplexContact have 18 and 14% top 10 precision, respectively. See detailed results in Supplementary Tables <ref type="table">S5 and S6</ref>.</p><p>In summary, GLINTER consistently outperforms DeepHomo and ComplexContact by a large margin no matter which test sets are evaluated and whether experimental or predicted monomer structures are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Ablation study</head><p>We train the GLINTER models under eight different settings (different sets of input features). Supplementary Tables <ref type="table" target="#tab_3">S3 and S4</ref> show their test results with monomer experimental structures and AlphaFold-predicted monomer structures, respectively. We have studied the following eight settings: 'Residue', 'ResidueþESM', 'ResidueþAtom', 'ResidueþAtomþESM', 'ResidueþSurface', 'ResidueþSurfaceþESM', 'ResidueþAtomþSurface' and 'ResidueþAtomþSurfaceþESM' models. Here, 'Residue', 'Atom' and 'Surface' represent the residue, atom and surface graphs, respectively. 'ESM' means that the ESM row attention weights are used. Using the ESM row attention weights does not change the network architecture, but increases the input dimension of the first ResNet block, as shown in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>To evaluate the contribution of the ESM row attention weights, we test a sequence-only model called 'ESM-Attention' that uses only the ESM row attention weights as input. As shown in Supplementary Figure <ref type="figure" target="#fig_0">S1</ref>, its major module is a 2D ResNet with the same architecture as the one used in the ResidueþESM model.</p><p>To evaluate the contribution of the graph convolution module, we develop a sequence-structure-hybrid model denoted as 'CNNþESM-Attention', which uses an 1D convolutional network (CNN) and the same set of input features. Similar to the ResidueþESM model, the CNNþESM-Attention model consists of two major modules: a Siamese 1D CNN and a ResNet. The 1D CNN has four convolution layers (each with 128 filters and kernel size 5) and the ResNet is the same as that used in the ResidueþESM model (Supplementary Fig. <ref type="figure" target="#fig_1">S2</ref>). Both the ESM-Attention and the CNNþESM-Attention models are trained on the same dataset using the same protocols as the GLINTER models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contribution of the graph convolution module</head><p>As shown in Table <ref type="table" target="#tab_2">2</ref> and Supplementary Table <ref type="table" target="#tab_3">S3</ref>, the CNNþESM-Attention model has similar performance as the ESM-Attention model. The best CNNþESM-Attention model has 35% top-10 precision and 24% top-L/10 precision, while the ESM-Attention model has 31% top-10 precision and 29% top-L/10 precision. In contrast, the ResidueþESM model has 43% top-10 precision and 42% top-L/ 10 precision, which suggests that the residue graph (derived from monomer structures) used by GLINTER is indeed very helpful for interfacial contact prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dependency on distance cutoff</head><p>The distance cutoff used to define graph edges is an important hyperparameter. According to our observation, a model with a Note: 'Native' means the experimental monomer structures are used. 'None' means that tertiary structures are not used at all. 'Pred' means the monomer structures predicted by AlphaFold are used. HomoCASP represents the set of 23 homodimers in the CASP-CAPRI data. HeteroCASP represents the set of nine heterodimers in the CASP-CAPRI data. For the PDB2018 test sets (HomoPDB and HeteroPDB), only the native monomer structures are used. 'DH' represents DeepHomo and 'CC' represents ComplexContact. In each column, the entry of the best performance is in bold. larger distance cutoff tends to have a lower training loss, although its prediction performance may not be as good. A model with a smaller distance cutoff may have a higher training loss and much worse prediction performance. As shown in Table <ref type="table" target="#tab_3">3</ref> and Supplementary Table <ref type="table" target="#tab_3">S3</ref>, the top k precision of GLINTER models increases along with the distance cutoff until reaching the optimal value. For example, the top-10 precision of the ResidueþAtom model increases from 22 to 33% as the distance cutoff increases from 4 to 6 A ˚, and then decreases to 27% when the distance cutoff is 8 A ˚. This saturation effect on the distance cutoffs is also observed in <ref type="bibr" target="#b35">Townshend et al. (2019)</ref>.</p><p>Different types of graphs may rely on distance cutoffs differently. For example, the top 10 precision of the ResidueþSurface model is around 33% when the distance cutoff defining the surface graph ranges from 4 to 10 A ˚, while the precision of the 'ResidueþAtom' model changes a lot with respect to the distance cutoff. Here, we determine the optimal distance cutoff using the experimental monomer structures, which may not have the optimal performance when predicted monomer structures are used.</p><p>Dependency on the quality of predicted monomer structures GLINTER models are trained with monomer experimental structures. Here, we study their prediction performance when the AlphaFold-predicted monomer structures are used. We use the lower TMscore <ref type="bibr" target="#b46">(Zhang and Skolnick, 2005)</ref> of the two constituent monomer models to measure the structure quality of a dimer under test. We exclude the test dimers without any correct top k predicted contacts when their structures are used as input. Since there are only dozens of test targets, we divide them into four groups according to their TMscores: low quality (0.2 TMscore &lt; 0.5), acceptable quality (0.5 TMscore &lt; 0.7), medium quality (0.7 TMscore &lt; 0.9) and high quality (0.9 TMscore &lt; 1.0).</p><p>Supplementary Figure <ref type="figure">S6</ref> shows that even trained on bound experimental structures, our methods work well on predicted structures with medium or high quality (i.e. TMscore &gt; 0.7). When the predicted monomer structures have lower quality (TMscore &lt; 0.7), GLINTER models perform better with experimental structures than predicted structures. By comparing Supplementary Figure <ref type="figure">S6D and  E</ref>, we find that the ESM row attention weight may not be able to reduce the precision gap incurred by predicted structures. This suggests that the ESM row attention weight derived purely from MSAs may not necessarily improve the robustness of our structure-based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contribution of the ESM row attention weight</head><p>As shown in Table <ref type="table" target="#tab_1">1</ref> and Supplementary Table <ref type="table" target="#tab_3">S3</ref>, on the 32 dimer targets, the ESM-Attention model has top 10 and L/10 precision 31 and 29%, respectively, greatly outperforming BIPSPI, which has top 10 and L/10 precision 15 and 14%, respectively. That is, even though the MSA Transformer is pre-trained with the MSAs of single-chain protein sequences, it works for inter-chain contact prediction. Over the nine heterodimer targets, the top 10 precision of ComplexContact and ESM-Attention is 14 and 28%, respectively. As shown in Supplementary Tables <ref type="table" target="#tab_3">S3 and S4</ref>, no matter whether native or predicted monomer structures are used the ESM row attention weight consistently improves the performance of GLINTER models, which confirms that coevolution signals are very useful for inter-chain contact predictions.</p><p>Figure <ref type="figure" target="#fig_1">2A</ref> compares the performance of the ESM-Attention model (which is a sequence-only model) and the ResidueþAtomþSurface model (which is a structure-only model) when the native structures are used. They have similar overall performance, but perform very differently on individual test targets, which suggests that the ESM row attention weight and structure information are highly complementary to each other. On the majority of test targets, the ResidueþAtomþSurfaceþESM model outperforms the ESM-Attention model (Fig. <ref type="figure" target="#fig_1">2B</ref>) and the ResidueþAtomþSurface model (Fig. <ref type="figure" target="#fig_1">2C</ref>). Figure <ref type="figure" target="#fig_1">2A</ref> and B differs only in the y-axis by an ESM feature, so their comparison shows the impact of the ESM features. Figure <ref type="figure" target="#fig_1">2A</ref> and C differs only in the x-axis by ResidueþAtomþSurf, so their comparison shows the impact of the ResidueþAtomþSurf features. Detailed performance comparison among the three models is shown in Supplementary Table <ref type="table" target="#tab_1">S1</ref>. A case study on target T0997 is in Supplementary File.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dependency on the depth of MSAs</head><p>It is known that intra-chain contact prediction precision correlates with the depth of MSAs denoted as Meff (defined in Supplementary File). Here, we study the impact of MSA depth on interfacial contact prediction when the ESM row attention weight is used. To remove the impact of inaccurate predicted structures, here, we test GLINTER models with native monomer structures. Supplementary Figure <ref type="figure">S7</ref> shows that there is certain correlation (R2¼0.3093) between the number of correct top-10 predictions by the ESM-Attention model and the ln(Meff) of the input MSA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Application to selection of docking decoys</head><p>A simple application of predicted interfacial contacts is to select the docking decoys. We use the top k (k ¼ 10, 25, 50) contacts predicted Note: The first row shows the distance cutoffs used to define graph edges. For example, '8,6' for 'ResidueþAtom' indicates that the residue graph and atom graph use 8 and 6 A ˚to define edges, respectively. by the ResidueþAtomþSurfaceþESM model to rank the docking decoys generated by HDOCK. The quality of a docking decoy is calculated by comparing it with its experimental complex structure using MMalign <ref type="bibr" target="#b20">(Mukherjee and Zhang, 2009)</ref>. For each target, we select top N decoys ranked by the predicted interfacial contacts and define their highest TMscore as the 'TMscore of the top N decoys'. In Figure <ref type="figure">3</ref>, the y-axis shows the average TMscore of the top N decoys of all the test dimers. Generally speaking, predicted contacts may improve the quality of top decoys by 5-8%. Except when N 10, generally speaking using more top predicted contacts may select better decoys than using only top 10 predicted contacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We have presented an interfacial contact prediction method, GLINTER, that predicts inter-protein contacts by integrating attention information generated by protein language models and graph modeling of monomer (experimental and predicted) structures. The attention may capture evolutionary and coevolutionary information encoded in MSA. We demonstrate that GLINTER outperforms existing methods and even if trained with experimental structures, it generalizes well to predicted structures. The interfacial contacts predicted by our method may help improve selection of docking decoys. Our ablation study shows that the attention information and structural features are complementary and important for interfacial contact prediction. The features used by GLINTER can be calculated very efficiently and GLINTER is applicable to both heterodimers and homodimers. Therefore, potentially GLINTER is applicable to the proteome-scale study of protein-protein interactions and complexes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Overview of the GLINTER architecture. L1and L2 are the lengths of the two protein chains, K is the number of channels in a CaConv layer and 144 is the total number of heads in the row attention weights generated by Facebook's MSA Transformer<ref type="bibr" target="#b25">(Rao et al., 2021)</ref> </figDesc><graphic url="image-1.png" coords="3,110.95,59.47,390.33,304.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Comparison of top-10 precision of three models: ESM, ResidueþAtomþSurface and ResidueþAtomþSurfaceþESM. (A) compares ResidueþAtomþSurface and ESM, (B) compares ResidueþAtomþSurfaceþESM and ESM, and (C) compares ResidueþAtomþSurface and ResidueþAtomþSurfaceþESM.</figDesc><graphic url="image-2.png" coords="5,317.99,494.25,228.36,192.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Average contact prediction precision (%) on the CASP-CAPRI and PDB data</figDesc><table><row><cell></cell><cell cols="2">HomoCASP HeteroCASP HomoPDB HeteroPDB</cell></row><row><cell cols="3">No. of top predictions 10 L10 L5 10 L10 L5 10 L10 L5 10 L10 L5</cell></row><row><cell>BIPSPI (native)</cell><cell cols="2">16 16 14 11 11 14 20 21 19 18 18 19</cell></row><row><cell>DH (native)</cell><cell>30 27 23</cell><cell>24 25 24</cell></row><row><cell>CC (none)</cell><cell>14 14 11</cell><cell>14 13 14</cell></row><row><cell>GLINTER (native)</cell><cell cols="2">54 51 47 44 48 37 48 48 47 47 47 46</cell></row><row><cell>GLINTER (pred)</cell><cell>43 40 37 24 30 23</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Average interfacial contact precision (%) of the ESM-Attention, CNNþESM-Attention and ResidueþESM models on the CASP-CAPRI data The ESM-Attention model only uses MSAs as inputs, while the CNNþESM-Attention and ResidueþESM models use MSAs and experimental monomer structures as inputs.</figDesc><table><row><cell>No. of top predictions</cell><cell>10</cell><cell>25</cell><cell>50</cell><cell>L10</cell><cell>L5</cell></row><row><cell>ESM-attention</cell><cell>31</cell><cell>27</cell><cell>24</cell><cell>29</cell><cell>28</cell></row><row><cell>CNNþESM-attention</cell><cell>35</cell><cell>28</cell><cell>20</cell><cell>24</cell><cell>34</cell></row><row><cell>ResidueþESM</cell><cell>43</cell><cell>37</cell><cell>34</cell><cell>42</cell><cell>32</cell></row><row><cell>Note:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Average top-10 interfacial contact precision (%) of the 'ResidueþAtom' and 'ResidueþSurface' models on the CASP-CAPRI data when experimental monomer structures are used</figDesc><table><row><cell></cell><cell>8,4</cell><cell>8,6</cell><cell>8,8</cell><cell>8,10</cell></row><row><cell>Residueþatom</cell><cell>22</cell><cell>33</cell><cell>27</cell><cell>30</cell></row><row><cell>Residueþsurface</cell><cell>33</cell><cell>34</cell><cell>33</cell><cell>33</cell></row></table></figure>
		</body>
		<back>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The software is available at https://github.com/zw2x/glinter. The datasets are available at https://github.com/zw2x/glinter/data.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Funding</head><p>This work was supported by National Institutes of Health [R01GM089753 to J.X.].</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conflict of Interest: none declared.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast and accurate multivariate Gaussian modeling of protein families: predicting residue contacts and protein-interaction partners</title>
		<author>
			<persName><forename type="first">C</forename><surname>Baldassi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">e92721</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Inferring interaction partners from protein sequences</title>
		<author>
			<persName><forename type="first">A.-F</forename><surname>Bitbol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl. Acad. Sci. USA</title>
				<meeting>Natl. Acad. Sci. USA</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="12180" to="12185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Accurate prediction of protein-protein interactions from sequence alignments using a Bayesian method</title>
		<author>
			<persName><forename type="first">L</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Van Nimwegen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mol. Syst. Biol</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">165</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Protein interaction networks revealed by proteome coevolution</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">365</biblScope>
			<biblScope unit="page" from="185" to="189" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Protein interaction interface region prediction by geometric deep learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bailey-Kellogg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2580" to="2588" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Protein-protein docking using learned three-dimensional representations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dawson-Haggerty</surname></persName>
		</author>
		<idno type="DOI">10.1101/738690</idno>
		<ptr target="https://github.com/mikedh/trimesh" />
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page">738690</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Protein interface prediction using graph convolutional networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="6533" to="6542" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deciphering interaction fingerprints from protein molecular surfaces using geometric deep learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gainza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="184" to="192" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">iScore: a novel graph kernel-based function for scoring protein-protein docking models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="112" to="121" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Simultaneous identification of specifically interacting paralogs and interprotein contacts by direct coupling analysis</title>
		<author>
			<persName><forename type="first">´</forename><surname>Gueudre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl. Acad. Sci. USA</title>
				<meeting>Natl. Acad. Sci. USA</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="12186" to="12191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Predicting protein-protein interactions through sequence-based deep learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hashemifar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="802" to="810" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Sequence co-evolution gives 3D contacts and structures of protein complexes</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Hopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">e03430</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast and effective protein model refinement by deep graph neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1038/s43588-021-00098-9</idno>
	</analytic>
	<monogr>
		<title level="j">Nat Comput Sci</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="462" to="469" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">High accuracy protein structure prediction using deep learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jumper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fourteenth Crit. Assess. Tech. Protein Struct. Predict</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Highly accurate protein structure prediction with AlphaFold</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jumper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">596</biblScope>
			<biblScope unit="page" from="583" to="589" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Local geometry and evolutionary conservation of protein surfaces reveal the multiple recognition patches in protein-protein interactions</title>
		<author>
			<persName><forename type="first">E</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Carbone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Comput. Biol</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">e1004580</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">3D complex: a structural classification of protein complexes</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Comput. Biol</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">e155</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">FreeSASA: an open source C library for solvent accessible surface area calculations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mitternacht</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1000">2016. F1000Res, 5, 189</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">MM-align: a quick algorithm for aligning multiple-chain protein complex structures using iterative dynamic programming</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">e83</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Robust and accurate prediction of residue-residue interactions across protein interfaces using evolutionary information</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ovchinnikov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">e02030</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Protein model quality assessment using 3D oriented convolutional neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Page ´s</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="3313" to="3319" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning context-aware structural representations to predict antigen and antibody binding interfaces</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bailey-Kellogg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="3996" to="4003" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">DNCON2_Inter: predicting interchain contacts for homodimeric and homomultimeric protein complexes using multiple sequence alignments of monomers and deep learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Quadir</surname></persName>
		</author>
		<idno type="DOI">12295.10.1038/s41598-021-91827-7</idno>
	</analytic>
	<monogr>
		<title level="j">Sci Rep</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Rao</surname></persName>
		</author>
		<idno type="DOI">10.1101/2021.02.12.430858</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl. Acad. Sci. USA</title>
				<meeting>Natl. Acad. Sci. USA</meeting>
		<imprint>
			<date type="published" when="2021">2021. e2016239118</date>
			<biblScope unit="volume">118</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Reduced surface: an efficient way to compute molecular surfaces</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Sanner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biopolymers</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="305" to="320" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">ProteinGCN: protein model quality assessment using Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sanyal</surname></persName>
		</author>
		<idno type="DOI">10.1101/2020.04.06.028266</idno>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">CCMpred-fast and precise prediction of protein residue-residue contacts from correlated mutations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Seemayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3128" to="3130" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName><surname>Fig</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The average quality (measured by TMscore) of the selected decoys by top predicted contacts. The x-axis is the number of top decoys selected. In the legend, &apos;top-10&apos;, &apos;top-25&apos; and &apos;top-50&apos; represent that top 10, 25 and 50 predicted contacts are used to select docking decoys, respectively. &apos;best decoy&apos; indicates the quality of the best decoys generated by HDOCK Senior</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">577</biblScope>
			<biblScope unit="page" from="706" to="710" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Improved protein structure prediction using potentials from deep learning</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">HH-suite3 for fast remote homology detection and deep protein annotation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Steinegger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">473</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">UDSMProt: universal deep sequence models for protein classification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Strodthoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="2401" to="2409" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fast end-to-end learning on protein surfaces</title>
		<author>
			<persName><forename type="first">F</forename><surname>Sverrisson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="15272" to="15281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">End-to-end learning on 3D protein structure for interface prediction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Townshend</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="15642" to="15651" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Large-scale identification of coevolution signals across homo-oligomeric protein interfaces by direct coupling analysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Uguzzoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl. Acad. Sci. USA</title>
				<meeting>Natl. Acad. Sci. USA</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="E2662" to="E2671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Contacts-based prediction of binding affinity in protein-protein complexes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vangone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Bonvin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">e07454</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Accurate de novo prediction of protein contact map by ultra-deep learning model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Comput. Biol</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">e1005324</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Identification of direct residue contacts in protein-protein interaction by message passing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Weigt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl. Acad. Sci. USA</title>
				<meeting>Natl. Acad. Sci. USA</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="67" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Asparagine and glutamine: using hydrogen atom contacts in the choice of side-chain amide orientation 1 1Edited by</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Word</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mol. Biol</title>
		<editor>J. Thornton</editor>
		<imprint>
			<biblScope unit="volume">285</biblScope>
			<biblScope unit="page" from="1735" to="1747" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Distance-based protein folding powered by deep learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl. Acad. Sci. USA</title>
				<meeting>Natl. Acad. Sci. USA</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="16856" to="16865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Improved protein structure prediction by deep learning irrespective of co-evolution information</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1038/s42256-021-00348-5</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="601" to="609" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">HDOCK: a web server for protein-protein and protein-DNA/RNA docking based on a hybrid strategy</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="W365" to="W373" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Accurate prediction of inter-protein residue--residue contacts for homo-oligomeric protein complexes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">22.10.1093/bib/bbab038</idno>
	</analytic>
	<monogr>
		<title level="j">Brief. Bioinform</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">ComplexContact: a web server for inter-protein contact prediction using deep learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="W432" to="W437" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">TM-align: a protein structure alignment algorithm based on the TM-score</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Skolnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2302" to="2309" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Deep learning reveals many more inter-protein residue-residue contacts than direct coupling analysis</title>
		<author>
			<persName><forename type="first">T.-M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1101/240754</idno>
		<ptr target="https://doi.org/10.1101/240754" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
