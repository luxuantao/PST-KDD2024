<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Object Detection with Grammar Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of Chicago Chicago</orgName>
								<address>
									<postCode>60637</postCode>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pedro</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Engineering and Dept. of Computer Science Brown University Providence</orgName>
								<address>
									<postCode>02912</postCode>
									<region>RI</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">David</forename><surname>Mcallester</surname></persName>
							<email>mcallester@ttic.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">TTI-Chicago Chicago</orgName>
								<address>
									<postCode>60637</postCode>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Object Detection with Grammar Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F0060DE472D563BA2EB7367C39790C41</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Compositional models provide an elegant formalism for representing the visual appearance of highly variable objects. While such models are appealing from a theoretical point of view, it has been difficult to demonstrate that they lead to performance advantages on challenging datasets. Here we develop a grammar model for person detection and show that it outperforms previous high-performance systems on the PASCAL benchmark. Our model represents people using a hierarchy of deformable parts, variable structure and an explicit model of occlusion for partially visible objects. To train the model, we introduce a new discriminative framework for learning structured prediction models from weakly-labeled data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The idea that images can be hierarchically parsed into objects and their parts has a long history in computer vision, see for example <ref type="bibr" target="#b14">[15]</ref>. Image parsing has also been of considerable recent interest <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24]</ref>. However, it has been difficult to demonstrate that sophisticated compositional models lead to performance advantages on challenging metrics such as the PASCAL object detection benchmark <ref type="bibr" target="#b8">[9]</ref>. In this paper we achieve new levels of performance for person detection using a grammar model that is richer than previous models used in high-performance systems. We also introduce a general framework for learning discriminative models from weakly-labeled data.</p><p>Our models are based on the object detection grammar formalism in <ref type="bibr" target="#b10">[11]</ref>. Objects are represented in terms of other objects through compositional rules. Deformation rules allow for the parts of an object to move relative to each other, leading to hierarchical deformable part models. Structural variability provides choice between multiple part subtypes -effectively creating mixture models throughout the compositional hierarchy -and also enables optional parts. In this formalism parts may be reused both within an object category and across object categories.</p><p>Our baseline and departure point is the UoC-TTI object detector <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12]</ref>. This system represents a class of objects with three different pictorial structure models. Although these models are learned automatically, making semantic interpretation unclear, it seems that the three components for the person class differ in how much of the person is taken to be visible -just the head and shoulders, the head and shoulders together with the upper body, or the whole standing person. Each of the three components has independently trained parts. For example, each component has a head part trained independently from the head part of the other components.</p><p>Here we construct a single grammar model that allows more flexibility in describing the amount of the person that is visible. The grammar model avoids dividing the training data between different components and thus uses the training data more efficiently. The parts in the model, such as the head part, are shared across different interpretations of the degree of visibility of the person. The grammar model also includes subtype choice at the part level to accommodate greater appearance variability across object instances. We use parts with subparts to benefit from high-resolution image data, while also allowing for deformations. Unlike previous approaches, we explicitly model the source of occlusion for partially visible objects.</p><p>Our approach differs from that of Jin and Geman <ref type="bibr" target="#b12">[13]</ref> in that theirs focuses on whole scene interpretation with generative models, while we focus on discriminatively trained models of individual objects. We also make Markovian restrictions not made in <ref type="bibr" target="#b12">[13]</ref>. Our work is more similar to that of Zhu et al. <ref type="bibr" target="#b20">[21]</ref> who impose similar Markovian restrictions. However, our training method, image features, and grammar design are substantially different.</p><p>The model presented here is designed to accurately capture the visible portion of a person. There has been recent related work on occlusion modeling in pedestrian and person images <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18]</ref>. In <ref type="bibr" target="#b6">[7]</ref>, Enzweiler et al. assume access to depth and motion information in order to estimate occlusion boundaries. In <ref type="bibr" target="#b17">[18]</ref>, Wang et al. rely on the observation that the scores of individual filter cells (using the Dalal and Triggs detector <ref type="bibr" target="#b4">[5]</ref>) can reliably predict occlusion in the INRIA pedestrian data. This does not hold for the harder PASCAL person data.</p><p>In addition to developing a grammar model for detecting people, we develop new training methods which contribute to our boost in performance. Training data for vision is often assigned weak labels such as bounding boxes or just the names of objects occurring in the image. In contrast, an image analysis system will often produce strong predictions such as a segmentation or a pose. Existing structured prediction methods, such as structural SVM <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> and latent structural SVM <ref type="bibr" target="#b18">[19]</ref>, do not directly support weak labels together with strong predictions. We develop the notion of a weaklabel structural SVM which generalizes structural SVMs and latent-structural SVMs. The key idea is to introduce a loss L(y, s) for making a strong prediction s when the weak training label is y.</p><p>A formalism for learning from weak labels was also developed in <ref type="bibr" target="#b1">[2]</ref>. One important difference is that <ref type="bibr" target="#b1">[2]</ref> generalizes ranking SVMs. <ref type="foot" target="#foot_0">1</ref> Our framework also allows for softer relations between weak labels and strong predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Grammar models</head><p>Object detection grammars <ref type="bibr" target="#b10">[11]</ref> represent objects recursively in terms of other objects. Let N be a set of nonterminal symbols and T be a set of terminal symbols. We can think of the terminals as the basic building blocks that can be found in an image. The nonterminals define abstract objects whose appearance are defined in terms of expansions into terminals.</p><p>Let Ω be a set of possible locations for a symbol within an image. A placed symbol, Y (ω), specifies a placement of Y ∈ N ∪ T at a location ω ∈ Ω. The structure of a grammar model is defined by a set, R, of weighted productions of the form</p><formula xml:id="formula_0">X(ω 0 ) s -→ { Y 1 (ω 1 ), . . . , Y n (ω n ) },<label>(1)</label></formula><p>where X ∈ N , Y i ∈ N ∪ T , ω i ∈ Ω and s ∈ R is a score. We denote the score of r ∈ R by s(r).</p><p>We can expand a placed nonterminal to a bag of placed terminals by repeatedly applying productions. An expansion of X(ω) leads to a derivation tree T rooted at X(ω). The leaves of T are labeled with placed terminals, and the internal nodes of T are labeled with placed nonterminals and with the productions used to replace those symbols.</p><p>We define appearance models for the terminals using a function score(A, ω) that computes a score for placing the terminal A at location ω. This score depends implicitly on the image data. We define the score of a derivation tree T to be the sum of the scores of the productions used to generate T , plus the score of placing the terminals associated with T 's leaves in their respective locations.</p><formula xml:id="formula_1">score(T ) = r∈internal(T ) s(r) + A(w)∈leaves(T ) score(A, ω)<label>(2)</label></formula><p>To generalize the models from <ref type="bibr" target="#b9">[10]</ref> we let Ω be positions and scales within a feature map pyramid H. We define the appearance models for terminals by associating a filter F A with each terminal A. Then score(A, ω) = F A • φ(H, ω) is the dot product between the filter coefficients and the features in a subwindow of the feature map pyramid, φ(H, ω). We use the variant of histogram of oriented gradient (HOG <ref type="bibr" target="#b4">[5]</ref>) features described in <ref type="bibr" target="#b9">[10]</ref>.</p><p>We consider models with productions specified by two kinds of schemas (a schema is a template for generating productions). A structure schema specifies one production for each placement ω ∈ Ω,</p><formula xml:id="formula_2">X(ω) s -→ { Y 1 (ω ⊕ δ 1 ), . . . , Y n (ω ⊕ δ n ) }.<label>(3)</label></formula><p>Here the δ i specify constant displacements within the feature map pyramid. Structure schemas can be used to define decompositions of objects into other objects.</p><p>Let ∆ be the set of possible displacements within a single scale of a feature map pyramid. A deformation schema specifies one production for each placement ω ∈ Ω and displacement δ ∈ ∆,</p><formula xml:id="formula_3">X(ω) α•φ(δ) -→ { Y (ω ⊕ δ) }.<label>(4)</label></formula><p>Here φ(δ) is a feature vector and α is a vector of deformation parameters. Deformation schemas can be used to define deformable models. We define φ(δ) = (dx, dy, dx 2 , dy 2 ) so that deformation scores are quadratic functions of the displacements.</p><p>The parameters of our models are defined by a weight vector w with entries for the score of each structure schema, the deformation parameters of each deformation schema and the filter coefficients associated with each terminal. Then score(T ) = w • Φ(T ), where Φ(T ) is the sum of (sparse) feature vectors associated with each placed terminal and production in T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">A grammar model for detecting people</head><p>Each component in the person model learned by the voc-release4 system <ref type="bibr" target="#b11">[12]</ref> is tuned to detect people under a prototypical visibility pattern. Based on this observation we designed, by hand, the structure of a grammar that models visibility by using structural variability and optional parts. For clarity, we begin by describing a shallow model (Figure <ref type="figure" target="#fig_0">1</ref>) that places all filters at the same resolution in the feature map pyramid. After explaining this model, we describe a deeper model that includes deformable subparts at higher resolutions.</p><p>Fine-grained occlusion Our grammar model has a start symbol Q that can be expanded using one of six possible structure schemas. These choices model different degrees of visibility ranging from heavy occlusion (only the head and shoulders are visible) to no occlusion at all.</p><p>Beyond modeling fine-grained occlusion patterns when compared to the mixture models from <ref type="bibr" target="#b6">[7]</ref> and <ref type="bibr" target="#b11">[12]</ref>, our grammar model is also richer in a number of ways. In Section 5 we show that each of the following modeling choices improves detection performance.</p><p>Occlusion model If a person is occluded, then there must be some cause of the occlusion -either the edge of the image or an occluding object, such as a desk or dinner table. We use a nontrivial model to capture the appearance of the stuff that occludes people.</p><p>Part subtypes The mixture model from <ref type="bibr" target="#b11">[12]</ref> has two subtypes for each mixture component. The subtypes are forced to be mirror images of each other and correspond roughly to left-facing versus right-facing people. Our grammar model has two subtypes for each part, which are also forced to be mirror images of each other. But in the case of our grammar model, the decision of which part subtype to instantiate at detection time is independent for each part.</p><p>The shallow person grammar model is defined by the following grammar. The indices p (for part), t (for subtype), and k have the following ranges: p ∈ {1, . . . , 6}, t ∈ {L, R} and k ∈ {1, . . . , 5}.</p><formula xml:id="formula_4">Q(ω) s k -→ { Y 1 (ω ⊕ δ 1 ), . . . , Y k (ω ⊕ δ k ), O(ω ⊕ δ k+1 ) } Q(ω) s6 -→ { Y 1 (ω ⊕ δ 1 ), . . . , Y 6 (ω ⊕ δ 6 ) } Y p (ω) 0 -→ { Y p,t (ω) } Y p,t (ω) αp,t•φ(δ) -→ { A p,t (ω ⊕ δ) } O(ω) 0 -→ { O t (ω) } O t (ω) αt•φ(δ) -→ { A t (ω ⊕ δ) }</formula><p>The grammar has a start symbol Q with six alternate choices that derive people under varying degrees of visibility (occlusion). Each part has a corresponding nonterminal Y p that is placed at some ideal position relative to Q. Derivations with occlusion include the occlusion symbol O. A derivation selects a subtype and displacement for each visible part. The parameters of the grammar (production scores, deformation parameters and filters) are learned with the discriminative procedure described in Section 4. Figure <ref type="figure" target="#fig_0">1</ref> illustrates the filters in the resulting model and some example detections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deeper model</head><p>We extend the shallow model by adding deformable subparts at two scales: (1) the same as, and (2) twice the resolution of the start symbol Q. When detecting large objects, high-resolution subparts capture fine image details. However, when detecting small objects, highresolution subparts cannot be used because they "fall off the bottom" of the feature map pyramid. The model uses derivations with low-resolution subparts when detecting small objects.</p><p>We begin by replacing the productions from Y p,t in the grammar above, and then adding new productions. Recall that p indexes the top-level parts and t indexes subtypes. In the following schemas, the indices r (for resolution) and u (for subpart) have the ranges: r ∈ {H, L}, u ∈ {1, . . . , N p }, where N p is the number of subparts in a top-level part Y p . -→ {A p,t,r,u (ω ⊕ δ)} We note that as in <ref type="bibr" target="#b22">[23]</ref> our model has hierarchical deformations. The part terminal A p,t can move relative to Q and the subpart terminal A p,t,r,u can move relative to A p,t .</p><formula xml:id="formula_5">Y p,t (ω) αp,t•φ(δ) -→ { Z p,t (ω ⊕ δ) } Z p,t (ω) 0 -→ {A p,t (ω), W p,t,</formula><p>The displacements δ p,t,H,u place the symbols W p,t,H,u one octave below Z p,t in the feature map pyramid. The displacements δ p,t,L,u place the symbols W p,t,L,u at the same scale as Z p,t . We add subparts to the first two top-level parts (p = 1 and 2), with the number of subparts set to N 1 = 3 and N 2 = 2. We find that adding additional subparts does not improve detection performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Inference and test time detection</head><p>Inference involves finding high scoring derivations. At test time, because images may contain multiple instances of an object class, we compute the maximum scoring derivation rooted at Q(ω), for each ω ∈ Ω. This can be done efficiently using a standard dynamic programming algorithm <ref type="bibr" target="#b10">[11]</ref>.</p><p>We retain only those derivations that score above a threshold, which we set low enough to ensure high recall. We use box(T ) to denote a detection window associated with a derivation T . Given a set of candidate detections, we apply nonmaximal suppression to produce a final set of detections.</p><p>We define box(T ) by assigning a detection window size, in feature map coordinates, to each structure schema that can be applied to Q. This leads to detections with one of six possible aspect ratios, depending on which production was used in the first step of the derivation. The absolute location and size of a detection depends on the placement of Q. For the first five production schemas, the ideal location of the occlusion part, O, is outside of box(T ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning from weakly-labeled data</head><p>Here we define a general formalism for learning functions from weakly-labeled data. Let X be an input space, Y be a label space, and S be an output space. We are interested in learning functions f : X → S based on a set of training examples {(x 1 , y 1 ), . . . , (x n , y n )} where (x i , y i ) ∈ X × Y. In contrast to the usual supervised learning setting, we do not assume that the label space and the output space are the same. In particular there may be many output values that are compatible with a label, and we can think of each example as being only weakly labeled. It will also be useful to associate a subset of possible outputs, S(x) ⊆ S, with an example x. In this case f (x) ∈ S(x).</p><p>A connection between labels and outputs can be made using a loss function L : Y × S → R. L(y, s) associates a cost with the prediction s ∈ S on an example labeled y ∈ Y. Let D be a distribution over X × Y. Then a natural goal is to find a function f with low expected loss E D [L(y, f (x))].</p><p>A simple example of a weakly-labeled training problem comes from learning sliding window classifiers in the PASCAL object detection dataset. The training data specifies pixel-accurate bounding boxes for the target objects while a sliding window classifier reports boxes with a fixed aspect ratio and at a finite number of scales. The output space is, therefore, a subset of the label space.</p><p>As usual, we assume f is parameterized by a vector of model parameters w and generates predictions by maximizing a linear function of a joint feature map Φ(x, s), f (x) = argmax s∈S(x) w • Φ(x, s).</p><p>We can train w by minimizing a regularized risk on the training set. We define a weak-label structural SVM (WL-SSVM) by the following training equation,</p><formula xml:id="formula_6">E(w) = 1 2 ||w|| 2 + C n i=1 L (w, x i , y i ).<label>(5)</label></formula><p>The surrogate training loss L is defined in terms of two different loss augmented predictions.</p><formula xml:id="formula_7">L (w, x, y) = max s∈S(x) [w • Φ(x, s) + L margin (y, s)] (6a) -max s∈S(x) [w • Φ(x, s) -L output (y, s)] (6b)<label>(6)</label></formula><p>L margin encourages high-loss outputs to "pop out" of (6a), so that their scores get pushed down. L output suppresses high-loss outputs in (6b), so the score of a low-loss prediction gets pulled up.</p><p>It is natural to take L margin = L output = L. In this case L becomes a type of ramp loss <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14]</ref>. Alternatively, taking L margin = L and L output = 0 gives the ramp loss that has been shown to be consistent in <ref type="bibr" target="#b13">[14]</ref>. As we discuss below, the choice of L output can have a significant effect on the computational difficulty of the training problem. Several popular learning frameworks can be derived as special cases of WL-SSVM. For the examples below, let I(a, b) = 0 when a = b, and</p><formula xml:id="formula_8">I(a, b) = ∞ when a = b.</formula><p>Structural SVM Let S = Y, L margin = L and L output (y, ŷ) = I(y, ŷ). Then L (w, x, y) is the hinge loss used in a structural SVM <ref type="bibr" target="#b16">[17]</ref>. In this case L is convex in w because the maximization in (6b) disappears. We note, however, that this choice of L output may be problematic and lead to inconsistent training problems. Consider the following situation. A training example (x, y) may be compatible with a different label ŷ = y, in the sense that L(y, ŷ) = 0. But even in this case a structural SVM pushes the score w • Φ(x, y) to be above w • Φ(x, ŷ). This issue can be addressed by relaxing L output to include a maximization over labels in (6b).</p><p>Latent structural SVM Now let Z be a space of latent values, S = Y × Z, L margin = L and L output (y, (ŷ, ẑ)) = I(y, ŷ). Then L (w, x, y) is the hinge loss used in a latent structural SVM <ref type="bibr" target="#b18">[19]</ref>.</p><p>In this case L is not convex in w due to the maximization over latent values in (6b). As in the previous example, this choice of L output can be problematic because it "requires" that the training labels be predicted exactly. This can be addressed by relaxing L output , as in the previous example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Training grammar models</head><p>Now we consider learning the parameters of an object detection grammar using the training data in the PASCAL VOC datasets with the WL-SSVM framework. For two rectangles a and b let overlap(a, b) = area(a ∩ b)/ area(a ∪ b). We will use this measure of overlap in our loss functions.</p><p>For training, we augment our model's output space (the set of all derivation trees), with the background output ⊥. We define Φ(x, ⊥) to be the zero vector, as was done in <ref type="bibr" target="#b0">[1]</ref>. Thus the score of a background hypothesis is zero independent of the model parameters w.</p><p>The training data specifies a bounding box for each instance of an object in a set of training images. We construct a set of weakly-labeled examples {(x 1 , y 1 ), . . . , (x n , y n )} as follows. For each training image I, and for each bounding box B in I, we define a foreground example (x, y), where y = B, x specifies the image I, and the set of valid predictions S(x) includes:</p><p>1. Derivations T with overlap(box(T ), B) ≥ 0.1 and overlap(box(T ), B ) &lt; 0.5 for all B in I such that B = B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The background output ⊥.</head><p>The overlap requirements in (1) ensure that we consider only predictions that are relevant for a particular object instance, while avoiding interactions with other objects in the image.</p><p>We also define a very large set of background examples. For simplicity, we use images that do not contain any bounding boxes. For each background image I, we define a different example (x, y) for each position and scale ω within I. In this case y = ⊥, x specifies the image I, and S(x) includes derivations T rooted at Q(ω) and the background output ⊥. The set of background examples is very large because the number of positions and scales within each image is typically around 250K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Loss functions</head><p>The PASCAL benchmark requires a correct detection to have at least 50% overlap with a groundtruth bounding box. We use this rule to define our loss functions. First, define L l,τ (y, s) as follows</p><formula xml:id="formula_9">L l,τ (y, s) =      l if y = ⊥ and s = ⊥ 0 if y = ⊥ and s = ⊥ l if y = ⊥ and overlap(y, s) &lt; τ 0 if y = ⊥ and overlap(y, s) ≥ τ .<label>(7)</label></formula><p>Following the PASCAL VOC protocol we use L margin = L 1,0.5 . For a foreground example this pushes down the score of detections that don't overlap with the bounding box label by at least 50%.</p><p>Instead of using L output = L margin , we let L output = L ∞,0.7 . For a foreground example this ensures that the maximizer of (6b) is a detection with high overlap with the bounding box label. For a background example, the maximizer of (6b) is always ⊥. Later we discuss how this simplifies our optimization algorithm. While our choice of L output does not produce a convex objective, it does tightly limit the range of outputs, making our optimization less prone to reaching bad local optima.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Optimization</head><p>Since L is not convex, the WL-SSVM objective (5) leads to a nonconvex optimization problem. We follow <ref type="bibr" target="#b18">[19]</ref> in which the CCCP procedure <ref type="bibr" target="#b19">[20]</ref> was used to find a local optima of a similar objective.</p><p>CCCP is an iterative algorithm that uses a decomposition of the objective into a sum of convex and concave parts E(w) = E convex (w) + E concave (w).</p><formula xml:id="formula_10">E convex (w) = 1 2 ||w|| 2 + C n i=1 max s∈S(xi) [w • Φ(x i , s) + L margin (y i , s)]<label>(8)</label></formula><formula xml:id="formula_11">E concave (w) = -C n i=1 max s∈S(xi) [w • Φ(x i , s) -L output (y i , s)]<label>(9)</label></formula><p>In each iteration, CCCP computes a linear upper bound to E concave based on a current weight vector w t . The bound depends on subgradients of the summands in <ref type="bibr" target="#b8">(9)</ref>. For each summand, we take the subgradient Φ(x i , s i (w t )), where</p><formula xml:id="formula_12">s i (w) = argmax s∈S(xi) [w • Φ(x i , s) -L output (y i , s)] is a loss augmented prediction.</formula><p>We note that computing s i (w t ) for each training example can be costly. But from our definition of L output , we have that s i (w) = ⊥ for a background example independent of w. Therefore, for a background example Φ(x i , s i (w t )) = 0. </p><formula xml:id="formula_13">w t+1 = argmin w 1 2 ||w|| 2 + C n i=1 max s∈S(xi) [w • Φ(x i , s) + L margin (y i , s)] -w • Φ(x i , s i (w t )) . (<label>10</label></formula><formula xml:id="formula_14">)</formula><p>The optimization subproblem defined by equation ( <ref type="formula" target="#formula_13">10</ref>) is similar in form to a structural SVM optimization. Given the size and nature of our training dataset we opt to solve this subproblem using stochastic subgradient descent and a modified form of the data mining procedure from <ref type="bibr" target="#b9">[10]</ref>. As in <ref type="bibr" target="#b9">[10]</ref>, we data mine over background images to collect support vectors for background examples. However, unlike in the binary LSVM setting considered in <ref type="bibr" target="#b9">[10]</ref>, we also need to apply data mining to foreground examples. This would be slow because it requires performing relatively expensive inference (more than 1 second per image) on thousands of images. Instead of applying data mining to the foreground examples, each time we compute s i (w t ) for a foreground example, we also compute the top M scoring outputs s ∈ S(x i ) of w t • Φ(x i , s) + L margin (y i , s), and place the corresponding feature vectors in the data mining cache. This is efficient since much of the required computation is shared with computation already necessary for computing s i (w t ). While this is only a heuristic approximation to true data mining, it leads to an improvement over training with binary LSVM (see <ref type="bibr">Section 5)</ref>. In practice, we find that M = 1 is sufficient for improved performance and that increasing M beyond 1 does not improve our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Initialization</head><p>Using CCCP requires an initial model or heuristic for selecting the initial outputs s i (w 0 ). Inspired by the methods in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12]</ref>, we train a single filter for fully visible people using a standard binary SVM. To define the SVM's training data, we select vertically elongated examples. We apply the orientation clustering method in <ref type="bibr" target="#b11">[12]</ref> to further divide these examples into two sets that approximately correspond to left-facing versus right-facing orientations. Examples from one of these two sets are then anisotropically rescaled so their HOG feature maps match the dimensions of the filter. These form the positive examples. For negative examples, random patches are extracted from background images. After training the initial filter, we slice it into subfilters (one 8 × 8 and five 3 × 8) that form the building blocks of the grammar model. We mirror these six filters to get subtypes, and then add subparts using the energy covering heuristic in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental results</head><p>We evaluated the performance of our person grammar and training framework on the PASCAL VOC 2007 and 2010 datasets <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. We used the standard PASCAL VOC comp3 test protocol, which measures detection performance by average precision (AP) over different recall levels. Figure <ref type="figure" target="#fig_1">2</ref> shows some qualitative results, including failure cases.</p><p>PASCAL VOC 2010 Our results on the 2010 dataset are presented in Table <ref type="table" target="#tab_1">1</ref> in the context of two strong baselines. The first, UoC-TTI, won the person category in the comp3 track of the 2010 competition <ref type="bibr" target="#b8">[9]</ref>. The 2010 entry of the UoC-TTI method extended <ref type="bibr" target="#b11">[12]</ref> by adding an extra octave to the HOG feature map pyramid, which allows the detector to find smaller objects. We report the AP score of the UoC-TTI "raw" person detector, as well as the scores after applying the bounding We also applied the two post-processing steps to the grammar model, and found that unlike with the mixture model, the grammar model does not benefit from bounding box prediction. This is likely because our fine-grained occlusion model reduces the number of near misses that are fixed by bounding box prediction. To test context rescoring, we used the UoC-TTI detection data for the other 19 object classes. Context rescoring boosts our final score to 49.5.</p><p>The second baseline is the poselets system described in <ref type="bibr" target="#b2">[3]</ref>. Their system requires detailed pose and visibility annotations, in contrast to our grammar model which was trained only with bounding box labels. Prior to context rescoring, our model scores one point lower than the poselets model, and after rescoring it scores one point higher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structure and training</head><p>We evaluated several aspects of our model structure and training objective on the PASCAL VOC 2007 dataset. In all of our experiments we set the regularization constant to C = 0.006. In Table <ref type="table" target="#tab_2">2</ref> we compare the WL-SSVM framework developed here with the binary LSVM framework from <ref type="bibr" target="#b9">[10]</ref>. WL-SSVM improves performance of the grammar model by 1.4 AP points over binary LSVM training. WL-SSVM also improves results obtained using a mixture of part-based models by 0.6 points. To investigate model structure, we evaluated the effect of part subtypes and occlusion modeling. Removing subtypes reduces the score of the grammar model from 46.7 to 45.5. Removing the occlusion part also decreases the score from 46.7 to 45.5. The shallow model (no subparts) achieves a score of 40.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>Our results establish grammar-based methods as a high-performance approach to object detection by demonstrating their effectiveness on the challenging task of detecting people in the PASCAL VOC datasets. To do this, we carefully designed a flexible grammar model that can detect people under a wide range of partial occlusion, pose, and appearance variability. Automatically learning the structure of grammar models remains a significant challenge for future work. We hope that our empirical success will provide motivation for pursing this goal, and that the structure of our handcrafted grammar will yield insights into the properties that an automatically learned grammar might require. We also develop a structured training framework, weak-label structural SVM, that naturally handles learning a model with strong outputs, such as derivation trees, from data with weak labels, such as bounding boxes. Our training objective is nonconvex and we use a strong loss function to avoid bad local optima. We plan to explore making this loss softer, in an effort to make learning more robust to outliers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Shallow grammar model. This figure illustrates a shallow version of our grammar model (Section 2.1). This model has six person parts and an occlusion model ("occluder"), each of which comes in one of two subtypes. A detection places one subtype of each visible part at a location and scale in the image. If the derivation does not place all parts it must place the occluder. Parts are allowed to move relative to each other, but their displacements are constrained by deformation penalties.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example detections. Parts are blue. The occlusion part, if used, is dashed cyan. (a) Detections of fully visible people. (b) Examples where the occlusion part detects an occlusion boundary. (c) Detections where there is no occlusion, but a partial person is appropriate. (d) Mistakes where the model did not detect occlusion properly.</figDesc><graphic coords="8,330.88,158.05,51.86,70.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>PASCAL 2010 results. UoC-TTI and our method compete in comp3. Poselets competes comp4 due to its use of detailed pose and visibility annotations and non-PASCAL images.</figDesc><table><row><cell></cell><cell cols="7">Grammar +bbox +context UoC-TTI [9] +bbox +context Poselets [9]</cell></row><row><cell>AP</cell><cell>47.5</cell><cell>47.6</cell><cell>49.5</cell><cell>44.4</cell><cell>45.2</cell><cell>47.5</cell><cell>48.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Training objective and model structure evaluation on PASCAL 2007.After computing s i (w t ) and Φ(x i , s i (w t )) for all examples (implicitly for background examples), the weight vector is updated by minimizing a convex upper bound on the objective E(w):</figDesc><table><row><cell></cell><cell cols="4">Grammar LSVM Grammar WL-SSVM Mixture LSVM Mixture WL-SSVM</cell></row><row><cell>AP</cell><cell>45.3</cell><cell>46.7</cell><cell>42.6</cell><cell>43.2</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p><ref type="bibr" target="#b1">[2]</ref> claims the ranking framework overcomes a loss in performance when the number of background examples is increased. In contrast, we don't use a ranking framework but always observed a performance improvement when increasing the number of background examples.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments This research has been supported by NSF grant IIS-0746569.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to localize objects with structured output regression</title>
		<author>
			<persName><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Simultaneous object detection and ranking with weak supervision</title>
		<author>
			<persName><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Detecting people using mutually consistent poselet activations</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Trading convexity for scalability</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tighter bounds for structured estimation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Teo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-cue pedestrian classification with partial occlusion handling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Eigenstetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html" />
		<title level="m">The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2010/workshop/index.html" />
		<title level="m">The PASCAL Visual Object Classes Challenge 2010 (VOC2010) Results</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part based models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Object detection grammars. Univerity of Chicago</title>
		<author>
			<persName><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CS Dept</title>
		<imprint>
			<biblScope unit="page" from="2010" to="2012" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Discriminatively trained deformable part models</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<ptr target="http://people.cs.uchicago.edu/˜pff/latent-release4/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Context and hierarchy in a probabilistic image model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generalization bounds and consistency for latent structural probit and ramp loss</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Keshet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An analysis system for scenes containing objects with substructures</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sakai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Max-margin markov networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Large margin methods for structured and interdependent output variables</title>
		<author>
			<persName><forename type="first">I</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An hog-lbp human detector with partial occlusion handling</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning structural svms with latent variables</title>
		<author>
			<persName><forename type="first">C.-N</forename><forename type="middle">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The concave-convex procedure</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rangarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Part and appearance sharing: Recursive compositional models for multi-view multi-object detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised learning of probabilistic grammar-markov models for object categories</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Latent hierarchical structural learning for object detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A stochastic grammar of images. Foundations and Trends in Computer Graphics and Vision</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mumford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
