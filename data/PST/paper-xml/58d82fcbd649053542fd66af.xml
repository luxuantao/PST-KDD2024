<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Under review as a conference paper at ICLR 2017 IDENTITY MATTERS IN DEEP LEARNING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Brain</orgName>
								<address>
									<addrLine>1600 Amphitheatre Parkway</addrLine>
									<postCode>94043</postCode>
									<settlement>Mountain View</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
							<email>tengyu@cs.princeton.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Sciene</orgName>
								<orgName type="institution">Princeton University</orgName>
								<address>
									<addrLine>35 Olden Street</addrLine>
									<postCode>08540</postCode>
									<settlement>Princeton</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Under review as a conference paper at ICLR 2017 IDENTITY MATTERS IN DEEP LEARNING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>An emerging design principle in deep learning is that each layer of a deep artificial neural network should be able to easily express the identity transformation. This idea not only motivated various normalization techniques, such as batch normalization, but was also key to the immense success of residual networks.</p><p>In this work, we put the principle of identity parameterization on a more solid theoretical footing alongside further empirical progress. We first give a strikingly simple proof that arbitrarily deep linear residual networks have no spurious local optima. The same result for feed-forward networks in their standard parameterization is substantially more delicate. Second, we show that residual networks with ReLu activations have universal finite-sample expressivity in the sense that the network can represent any function of its sample provided that the model has more parameters than the sample size. Directly inspired by our theory, we experiment with a radically simple residual architecture consisting of only residual convolutional layers and ReLu activations, but no batch normalization, dropout, or max pool. Our model improves significantly on previous all-convolutional networks on the CIFAR10, CIFAR100, and ImageNet classification benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Traditional convolutional neural networks for image classification, such as AlexNet <ref type="bibr" target="#b12">(Krizhevsky et al. (2012)</ref>), are parameterized in such a way that when all trainable weights are 0, a convolutional layer represents the 0-mapping. Moreover, the weights are initialized symmetrically around 0. This standard parameterization makes it non-trivial for a convolutional layer trained with stochastic gradient methods to preserve features that were already good. Put differently, such convolutional layers cannot easily converge to the identity transformation at training time. This shortcoming was observed and partially addressed by <ref type="bibr" target="#b8">Ioffe &amp; Szegedy (2015)</ref> through batch normalization, i.e., layer-wise whitening of the input with a learned mean and covariance. But the idea remained somewhat implicit until residual networks <ref type="bibr" target="#b5">(He et al. (2015)</ref>; <ref type="bibr" target="#b6">He et al. (2016)</ref>) explicitly introduced a reparameterization of the convolutional layers such that when all trainable weights are 0, the layer represents the identity function. Formally, for an input x, each residual layer has the form x + h(x), rather than h(x). This simple reparameterization allows for much deeper architectures largely avoiding the problem of vanishing (or exploding) gradients. Residual networks, and subsequent architectures that use the same parameterization, have since then consistently achieved state-of-the-art results on various computer vision benchmarks such as CIFAR10 and ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">OUR CONTRIBUTIONS</head><p>In this work, we consider identity parameterizations from a theoretical perspective, while translating some of our theoretical insight back into experiments. Loosely speaking, our first result underlines how identity parameterizations make optimization easier, while our second result shows the same is true for representation.</p><p>Linear residual networks. Since general non-linear neural networks, are beyond the reach of current theoretical methods in optimization, we consider the case of deep linear networks as a simplified model. A linear network represents an arbitrary linear map as a sequence of matrices A</p><formula xml:id="formula_0">• • • A 2 A 1 . The objective function is E y − A • • • A 1 x 2</formula><p>, where y = Rx for some unknown linear transformation R and x is drawn from a distribution. Such linear networks have been studied actively in recent years as a stepping stone toward the general non-linear case (see Section 1.2). Even though A • • • A 1 is just a linear map, the optimization problem over the factored variables (A , . . . , A 1 ) is non-convex.</p><p>In analogy with residual networks, we will instead parameterize the objective function as min A1,...,A E y − (I + A ) • • • (I + A 1 )x 2 .</p><p>(1.1)</p><p>To give some intuition, when the depth is large enough, we can hope that the target function R has a factored representation in which each matrix A i has small norm. Any symmetric positive semidefinite matrix O can, for example, be written as a product O = O • • • O 1 , where each O i = O 1/ is very close to the identity for large so that A i = O i − I has small spectral norm. We first prove that an analogous claim is true for all linear transformations R. Specifically, we prove that for every linear transformation R, there exists a global optimizer (A 1 , . . . , A ) of (1.1) such that for large enough depth , max</p><formula xml:id="formula_1">1≤i≤ A i ≤ O(1/ ). (1.2)</formula><p>Here, A denotes the spectral norm of A. The constant factor depends on the conditioning of R.</p><p>We give the formal statement in Theorem 2.1. The theorem has the interesting consequence that as the depth increases, smaller norm solutions exist and hence regularization may offset the increase in parameters.</p><p>Having established the existence of small norm solutions, our main result on linear residual networks shows that the objective function (1.1) is, in fact, easy to optimize when all matrices have sufficiently small norm. More formally, letting A = (A 1 , . . . , A ) and f (A) denote the objective function in (1.1), we can show that the gradients of vanish only when f (A) = 0 provided that max i A i ≤ O(1/ ). See Theorem 2.2. This result implies that linear residual networks have no critical points other than the global optimum. In contrast, for standard linear neural networks we only know, by work of <ref type="bibr" target="#b11">Kawaguchi (2016)</ref> that these networks don't have local optima except the global optimum, but it doesn't rule out other critical points. In fact, setting A i = 0 will always lead to a bad critical point in the standard parameterization.</p><p>Universal finite sample expressivity. Going back to non-linear residual networks with ReLU activations, we can ask: How expressive are deep neural networks that are solely based on residual layers with ReLU activations? To answer this question, we give a very simple construction showing that such residual networks have perfect finite sample expressivity. In other words, a residual network with ReLU activations can easily express any functions of a sample of size n, provided that it has sufficiently more than n parameters. Note that this requirement is easily met in practice. On CIFAR 10 (n = 50000), for example, successful residual networks often have more than 10 6 parameters. More formally, for a data set of size n with r classes, our construction requires O(n log n+r 2 ) parameters. Theorem 3.2 gives the formal statement.</p><p>Each residual layer in our construction is of the form x + V ReLU(U x), where U and V are linear transformations. These layers are significantly simpler than standard residual layers, which typically have two ReLU activations as well as two instances of batch normalization.</p><p>The power of all-convolutional residual networks. Directly inspired by the simplicity of our expressivity result, we experiment with a very similar architecture on the CIFAR10, CIFAR100, and ImageNet data sets. Our architecture is merely a chain of convolutional residual layers each with a single ReLU activation, but without batch normalization, dropout, or max pooling as are common in standard architectures. The last layer is a fixed random projection that is not trained. In line with our theory, the convolutional weights are initialized near 0, using Gaussian noise mainly as a symmetry breaker. The only regularizer is standard weight decay ( 2 -regularization) and there is no need for dropout. Despite its simplicity, our architecture reaches 6.38% top-1 classification error on the CIFAR10 benchmark (with standard data augmentation). This is competitive with the best residual network reported in <ref type="bibr" target="#b5">He et al. (2015)</ref>, which achieved 6.43%. Moreover, it improves upon the performance of the previous best all-convolutional network, 7.25%, achieved by <ref type="bibr" target="#b14">Springenberg et al. (2014)</ref>. Unlike ours, this previous all-convolutional architecture additionally required dropout and a non-standard preprocessing (ZCA) of the entire data set. Our architecture also improves significantly upon <ref type="bibr" target="#b14">Springenberg et al. (2014)</ref> on both Cifar100 and ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">RELATED WORK</head><p>Since the advent of residual networks <ref type="bibr" target="#b5">(He et al. (2015)</ref>; <ref type="bibr" target="#b6">He et al. (2016)</ref>), most state-of-the-art networks for image classification have adopted a residual parameterization of the convolutional layers. Further impressive improvements were reported by <ref type="bibr" target="#b7">Huang et al. (2016)</ref> with a variant of residual networks, called dense nets. Rather than adding the original input to the output of a convolutional layer, these networks preserve the original features directly by concatenation. In doing so, dense nets are also able to easily encode an identity embedding in a higher-dimensional space. It would be interesting to see if our theoretical results also apply to this variant of residual networks.</p><p>There has been recent progress on understanding the optimization landscape of neural networks, though a comprehensive answer remains elusive. Experiments in <ref type="bibr" target="#b4">Goodfellow et al. (2014)</ref> and <ref type="bibr" target="#b3">Dauphin et al. (2014)</ref> suggest that the training objectives have a limited number of bad local minima with large function values. Work by <ref type="bibr" target="#b2">Choromanska et al. (2015)</ref> draws an analogy between the optimization landscape of neural nets and that of the spin glass model in physics <ref type="bibr" target="#b0">(Auffinger et al. (2013)</ref>). <ref type="bibr" target="#b13">Soudry &amp; Carmon (2016)</ref> showed that 2-layer neural networks have no bad differentiable local minima, but they didn't prove that a good differentiable local minimum does exist. <ref type="bibr" target="#b1">Baldi &amp; Hornik (1989)</ref> and <ref type="bibr" target="#b11">Kawaguchi (2016)</ref> show that linear neural networks have no bad local minima.</p><p>In contrast, we show that the optimization landscape of deep linear residual networks has no bad critical point, which is a stronger and more desirable property. Our proof is also notably simpler illustrating the power of re-parametrization for optimization. Our results also indicate that deeper networks may have more desirable optimization landscapes compared with shallower ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">OPTIMIZATION LANDSCAPE OF LINEAR RESIDUAL NETWORKS</head><p>Consider the problem of learning a linear transformation R : R d → R d from noisy measurements y = Rx + ξ, where ξ ∈ N (0, Id d ) is a d-dimensional spherical Gaussian vector. Denoting by D the distribution of the input data x, let Σ = Ex∼D[xx ] be its covariance matrix.</p><p>There are, of course, many ways to solve this classical problem, but our goal is to gain insights into the optimization landscape of neural nets, and in particular, residual networks. We therefore parameterize our learned model by a sequence of weight matrices A 1 , . . . , A ∈ R d×d ,</p><formula xml:id="formula_2">h 0 = x , h j = h j−1 + A j h j−1 , ŷ = h . (2.1)</formula><p>Here h 1 , . . . , h −1 are the − 1 hidden layers and ŷ = h are the predictions of the learned model on input x. More succinctly, we have</p><formula xml:id="formula_3">ŷ = (Id d + A ) . . . (Id + A 1 )x .</formula><p>It is easy to see that this model can express any linear transformation R. We will use A as a shorthand for all of the weight matrices, that is, the × d × d-dimensional tensor the contains A 1 , . . . , A as slices. Our objective function is the maximum likelihood estimator,</p><formula xml:id="formula_4">f (A, (x, y)) = ŷ − y 2 = (Id + A ) . . . (Id + A 1 )x − Rx − ξ 2 . (2.2)</formula><p>We will analyze the landscape of the population risk, defined as,</p><formula xml:id="formula_5">f (A) := E [f (A, (x, y))] .</formula><p>Recall that A i is the spectral norm of A i . We define the norm |||•||| for the tensor A as the maximum of the spectral norms of its slices,</p><formula xml:id="formula_6">|||A||| := max 1≤i≤ A i .</formula><p>The first theorem of this section states that the objective function f has an optimal solution with small |||•|||-norm, which is inversely proportional to the number of layers . Thus, when the architecture is deep, we can shoot for fairly small norm solutions. We define γ := max{| log σ max (R)|, | log σ min (R)|}. Here σ min (•), σ max (•) denote the least and largest singular values of R respectively. Theorem 2.1. Suppose ≥ 3γ. Then, there exists a global optimum solution A of the population risk</p><formula xml:id="formula_7">f (•) with norm |||A ||| ≤ 2( √ π + 3γ) 2 / .</formula><p>Here γ should be thought of as a constant since if R is too large (or too small), we can scale the data properly so that σ min (R) ≤ 1 ≤ σ max (R). Concretely, if σ max (R)/σ min (R) = κ, then we can scaling for the outputs properly so that σ min (R) = 1/ √ κ and σ max (R) = √ κ. In this case, we have γ = log √ κ, which will remain a small constant for fairly large condition number κ. We also point out that we made no attempt to optimize the constant factors here in the analysis. The proof of Theorem 2.1 is rather involved and is deferred to Section A.</p><p>Given the observation of Theorem 2.1, we restrict our attention to analyzing the landscape of f (•) in the set of A with |||•|||-norm less than τ ,</p><formula xml:id="formula_8">B τ = {A ∈ R ×d×d : |||A||| ≤ τ } .</formula><p>Here using Theorem 2.1, the radius τ should be thought of as on the order of 1/ . Our main theorem in this section claims that there is no bad critical point in the domain B τ for any τ &lt; 1. Recall that a critical point has vanishing gradient. Theorem 2.2. For any τ &lt; 1, we have that any critical point A of the objective function f (•) inside the domain B τ must also be a global minimum.</p><p>Theorem 2.2 suggests that it is sufficient for the optimizer to converge to critical points of the population risk, since all the critical points are also global minima.</p><p>Moreover, in addition to Theorem 2.2, we also have that any A inside the domain B τ satisfies that</p><formula xml:id="formula_9">∇f (A) 2 F ≥ 4 (1 − τ ) −1 σ min (Σ) 2 (f (A) − C opt ) .</formula><p>(2.3)</p><p>Here C opt is the global minimal value of f (•) and ∇f (A) F denotes the euclidean norm<ref type="foot" target="#foot_0">1</ref> of the × d × d-dimensional tensor ∇f (A). Note that σ min (Σ) denote the minimum singular value of Σ.</p><p>Equation (2.3) says that the gradient has fairly large norm compared to the error, which guarantees convergence of the gradient descent to a global minimum <ref type="bibr" target="#b10">(Karimi et al. (2016)</ref>) if the iterates stay inside the domain B τ , which is not guaranteed by Theorem 2.2 by itself.</p><p>Towards proving Theorem 2.2, we start off with a simple claim that simplifies the population risk.</p><p>We also use • F to denote the Frobenius norm of a matrix. Claim 2.3. In the setting of this section, we have,</p><formula xml:id="formula_10">f (A) = ((Id + A ) . . . (Id + A 1 ) − R)Σ 1/2 2 F + C . (2.4)</formula><p>Here C is a constant that doesn't depend on A, and Σ 1/<ref type="foot" target="#foot_1">2</ref> denote the square root of Σ, that is, the unique symmetric matrix B that satisfies B 2 = Σ.</p><p>Proof of Claim 2.3. Let tr(A) denotes the trace of the matrix A.</p><formula xml:id="formula_11">Let E = (Id+A ) . . . (Id+A 1 )−R.</formula><p>Recalling the definition of f (A) and using equation (2.2), we have</p><formula xml:id="formula_12">f (A) = E Ex − ξ 2 (by equation (2.2)) = E Ex 2 + ξ 2 − 2 Ex, ξ = E tr(Exx E ) + E ξ 2 (since E [ Ex, ξ ] = E [ Ex, E [ξ|x] ] = 0) = tr E E xx E + C (where C = E[xx ]) = tr(EΣE ) + C = EΣ 1/2 2 F + C . (since E xx = Σ)</formula><p>Next we compute the gradients of the objective function f (•) from straightforward matrix calculus.</p><p>We defer the full proof to Section A. Lemma 2.4. The gradients of f (•) can be written as,</p><formula xml:id="formula_13">∂f ∂A i = 2(Id + A ) . . . (Id + A i+1 )EΣ(Id + A i−1 ) . . . (Id + A 1 ) ,<label>(2.5</label></formula><p>)</p><formula xml:id="formula_14">where E = (Id + A ) . . . (Id + A 1 ) − R .</formula><p>Now we are ready to prove Theorem 2.2. The key observation is that each matric A j has small norm and cannot cancel the identity matrix. Therefore, the gradients in equation (2.5) is a product of non-zero matrices, except for the error matrix E. Therefore, if the gradient vanishes, then the only possibility is that the matrix E vanishes, which in turns implies A is an optimal solution.</p><p>Proof of Theorem 2.2. Using Lemma 2.4, we have,</p><formula xml:id="formula_15">∂f ∂A i F = 2 (Id + A ) . . . (Id + A i+1 )EΣ(Id + A i−1 ) . . . (Id + A 1 ) F (by Lemma 2.4) ≥ 2 j =i σ min (Id + A i ) • σ min (Σ) E F (by Claim C.2) ≥ 2(1 − τ ) −1 σ min (Σ) E . (since σ min (Id + A) ≥ 1 − A ) It follows that ∇f (A) 2 F = i=1 ∂f ∂A i 2 F ≥ 4 (1 − τ ) −1 σ min (Σ) 2 E 2 ≥ 4 (1 − τ ) −1 σ min (Σ) 2 (f (A) − C)</formula><p>(by the definition of E and Claim 2.3)</p><formula xml:id="formula_16">≥ 4 (1 − τ ) −1 σ min (Σ) 2 (f (A) − C opt ) . (since C opt = min A f (A) ≥ C by Claim 2.3)</formula><p>Therefore we complete the proof of equation (2.3). Finally, if A is a critical point, namely, ∇f (A) = 0, then by equation ( <ref type="formula" target="#formula_13">2</ref>.3) we have that f (A) = C opt . That is, A is a global minimum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">REPRESENTATIONAL POWER OF THE RESIDUAL NETWORKS</head><p>In this section we characterize the finite-sample expressivity of residual networks. We consider a residual layers with a single ReLU activation and no batch normalization. The basic residual building block is a function</p><formula xml:id="formula_17">T U,V,s (•) : R k → R k that is parameterized by two weight matrices U ∈ R ×k , V ∈ R k×k and a bias vector s ∈ R k , T U,V,s (h) = V ReLu(U h + s) . (3.1)</formula><p>A residual network is composed of a sequence of such residual blocks. In comparison with the full pre-activation architecture in <ref type="bibr" target="#b6">He et al. (2016)</ref>, we remove two batch normalization layers and one ReLU layer in each building block.</p><p>We assume the data has r labels, encoded as r standard basis vectors in R r , denoted by e 1 , . . . , e r . We have n training examples (x (1) , y (1) ), . . . , (x (n) , y (n) ), where x (i) ∈ R d denotes the i-th data and y (i) ∈ {e 1 , . . . , e r } denotes the i-th label. Without loss of generality we assume the data are normalized so that x (i) = 1. We also make the mild assumption that no two data points are very close to each other. Assumption 3.1. We assume that for every 1 ≤ i &lt; j ≤ n, we have x (i) − x (j) 2 ≥ ρ for some absolute constant ρ &gt; 0.</p><p>Images, for example, can always be imperceptibly perturbed in pixel space so as to satisfy this assumption for a small but constant ρ.</p><p>Under this mild assumption, we prove that residual networks have the power to express any possible labeling of the data as long as the number of parameters is a logarithmic factor larger than n.</p><p>Theorem 3.2. Suppose the training examples satisfy Assumption 3.1. Then, there exists a residual network N (specified below) with O(n log n + r<ref type="foot" target="#foot_2">2</ref> ) parameters that perfectly expresses the training data, i.e., for all i ∈ {1, . . . , n}, the network N maps x (i) to y (i) .</p><p>It is common in practice that n &gt; r 2 , as is for example the case for the Imagenet data set where n &gt; 10 6 and r = 1000.</p><p>We construct the following residual net using the building blocks of the form T U,V,s as defined in equation (3.1). The network consists of + 1 hidden layers h 0 , . . . , h , and the output is denoted by ŷ ∈ R r . The first layer of weights matrices A 0 maps the d-dimensional input to a k-dimensional hidden variable h 0 . Then we apply layers of building block T with weight matrices A j , B j ∈ R k×k . Finally, we apply another layer to map the hidden variable h to the label ŷ in R k . Mathematically, we have</p><formula xml:id="formula_18">h 0 = A 0 x , h j = h j−1 + T Aj ,Bj ,bj (h j−1 ), ∀j ∈ {1, . . . , } ŷ = h + T A +1 ,B +1 ,s +1 (h ) .</formula><p>We note that here A +1 ∈ R k×r and B +1 ∈ R r×r so that the dimension is compatible. We assume the number of labels r and the input dimension d are both smaller than n, which is safely true in practical applications. 2 The hyperparameter k will be chosen to be O(log n) and the number of layers is chosen to be = n/k . Thus, the first layer has dk parameters, and each of the middle building blocks contains 2k 2 parameters and the final building block has kr + r 2 parameters. Hence, the total number of parameters is O(kd</p><formula xml:id="formula_19">+ k 2 + rk + r 2 ) = O(n log n + r 2 ).</formula><p>Towards constructing the network N of the form above that fits the data, we first take a random matrix A 0 ∈ R k×d that maps all the data points x (i) to vectors h i) . Here we will use h</p><formula xml:id="formula_20">(i) 0 := A 0 x (</formula><formula xml:id="formula_21">(i) j</formula><p>to denote the j-th layer of hidden variable of the i-th example. By Johnson-Lindenstrauss Theorem <ref type="bibr" target="#b9">(Johnson &amp; Lindenstrauss (1984)</ref>, or see Wikipedia ( <ref type="formula">2016</ref>)), with good probability, the resulting vectors h</p><p>(i) 0 's remain to satisfy Assumption 3.1 (with slightly different scaling and larger constant ρ), that is, any two vectors h Then we construct middle layers that maps h (i) 0 to h (i) for every i ∈ {1, . . . , n}. These vectors h (i) will clustered into r groups according to the labels, though they are in the R k instead of in R r as desired. Concretely, we design this cluster centers by picking r random unit vectors q 1 , . . . , q r in R k . We view them as the surrogate label vectors in dimension k (note that k is potentially much smaller than r). In high dimensions (technically, if k &gt; 4 log r) random unit vectors q 1 , . . . , q r are pair-wise uncorrelated with inner product less than &lt; 0.5. We associate the i-th example with the target surrogate label vector v (i) defined as follows, if y (i) = e j , then v (i) = q j .</p><p>(3.2)</p><p>Then we will construct the matrices (A 1 , B 1 ), . . . , (A , B ) such that the first layers of the network maps vector h (i) 0 to the surrogate label vector v (i) . Mathematically, we will construct</p><formula xml:id="formula_22">(A 1 , B 1 ), . . . , (A , B ) such that ∀i ∈ {1, . . . , n}, h (i) = v (i) . (3.3)</formula><p>Finally we will construct the last layer T A +1 ,B +1 ,b +1 so that it maps the vectors q 1 , . . . , q r ∈ R k to e 1 , . . . , e r ∈ R r , ∀j ∈ {1, . . . , r}, q j + T A +1 ,B +1 ,b +1 (q j ) = e j .</p><p>(3.4) Putting these together, we have that by the definition (3.2) and equation (3.3), for every i, if the label is y (i) is e j , then h (i) will be q j . Then by equation (3.4), we have that ŷ(i) = q j + T A +1 ,B +1 ,b +1 (q j ) = e j . Hence we obtain that ŷ(i) = y (i) .</p><p>The key part of this plan is the construction of the middle layers of weight matrices so that h (i) = v (i) . We encapsulate this into the following informal lemma. The formal statement and the full proof is deferred to Section B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 3.3 (Informal version of Lemma B.2). In the setting above, for (almost) arbitrary vectors h</head><p>(1) 0 , . . . , h</p><p>(n) 0</p><p>and v (1) , . . . , v (n) ∈ {q 1 , . . . , q r }, there exists weights matrices (A 1 , B 1 ), . . . , (A , B ), such that, ∀i ∈ {1, . . . , n}, h</p><formula xml:id="formula_23">(i) = v (i) .</formula><p>We briefly sketch the proof of the Lemma to provide intuitions, and defer the full proof to Section B. The operation that each residual block applies to the hidden variable can be abstractly written as, ĥ → h + T U,V,s (h) .</p><p>(3.5)</p><p>where h corresponds to the hidden variable before the block and ĥ corresponds to that after. We claim that for an (almost) arbitrary sequence of vectors h (1) , . . . , h (n) , there exist T U,V,s (•) such that operation (3.5) transforms k vectors of h (i) 's to an arbitrary set of other k vectors that we can freely choose, and maintain the value of the rest of n − k vectors. Concretely, for any subset S of size k, and any desired vector v (i) (i ∈ S), there exist U, V, s such that</p><formula xml:id="formula_24">v (i) = h (i) + T U,V,s (h (i) ) ∀i ∈ S h (i) = h (i) + T U,V,s (h (i) ) ∀i ∈ S (3.6)</formula><p>This claim is formalized in Lemma B.1. We can use it repeatedly to construct layers of building blocks, each of which transforms a subset of k vectors in {h</p><p>(1) 0 , . . . , h</p><p>(n) 0 } to the corresponding vectors in {v (1) , . . . , v (n) }, and maintains the values of the others. Recall that we have = n/k layers and therefore after layers, all the vectors h (i) 0 's are transformed to v (i) 's, which complete the proof sketch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">POWER OF ALL-CONVOLUTIONAL RESIDUAL NETWORKS</head><p>Inspired by our theory, we experimented with all-convolutional residual networks on standard image classification benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">CIFAR10 AND CIFAR100</head><p>Our architectures for CIFAR10 and CIFAR100 are identical except for the final dimension corresponding to the number of classes 10 and 100, respectively. In Table <ref type="table" target="#tab_0">1</ref>, we outline our architecture. Each residual block has the form x + C 2 (ReLU(C 1 x)), where C 1 , C 2 are convolutions of the specified dimension (kernel width, kernel height, number of input channels, number of output channels). The second convolution in each block always has stride 1, while the first may have stride 2 where indicated. In cases where transformation is not dimensionality-preserving, the original input x is adjusted using averaging pooling and padding as is standard in residual layers.</p><p>We trained our models with the Tensorflow framework, using a momentum optimizer with momentum 0.9, and batch size is 128. All convolutional weights are trained with weight decay 0.0001. The initial learning rate is 0.05, which drops by a factor 10 and 30000 and 50000 steps. The model reaches peak performance at around 50k steps, which takes about 24h on a single NVIDIA Tesla K40 GPU. Our code can be easily derived from an open source implementation<ref type="foot" target="#foot_4">3</ref> by removing batch normalization, adjusting the residual components and model architecture. An important departure from the code is that we initialize a residual convolutional layer of kernel size k × k and c output channels using a random normal initializer of standard deviation σ = 1/k 2 c, rather than 1/k √ c used for standard convolutional layers. This substantially smaller weight initialization helped training, while not affecting representation.</p><p>A notable difference from standard models is that the last layer is not trained, but simply a fixed random projection. On the one hand, this slightly improved test error (perhaps due to a regularizing effect). On the other hand, it means that the only trainable weights in our model are those of the convolutions, making our architecture "all-convolutional".  An interesting aspect of our model is that despite its massive size of 13.59 million trainable parameters, the model does not seem to overfit too quickly even though the data set size is 50000. In contrast, we found it difficult to train a model with batch normalization of this size without significant overfitting on CIFAR10.</p><p>Table <ref type="table" target="#tab_1">2</ref> summarizes the top-1 classification error of our models compared with a non-exhaustive list of previous works, restricted to the best previous all-convolutional result by <ref type="bibr" target="#b14">Springenberg et al. (2014)</ref>, the first residual results <ref type="bibr" target="#b5">He et al. (2015)</ref>, and state-of-the-art results on CIFAR by <ref type="bibr" target="#b7">Huang et al. (2016)</ref>. All results are with standard data augmentation. The ImageNet ILSVRC 2012 data set has 1, 281, 167 data points with 1000 classes. Each image is resized to 224 × 224 pixels with 3 channels. We experimented with an all-convolutional variant of the 34-layer network in <ref type="bibr" target="#b5">He et al. (2015)</ref>. The original model achieved 25.03% classification error. Our derived model has 35.7M trainable parameters. We trained the model with a momentum optimizer (with momentum 0.9) and a learning rate schedule that decays by a factor of 0.94 every two epochs, starting from the initial learning rate 0.1. Training was distributed across 6 machines updating asynchronously. Each machine was equipped with 8 GPUs (NVIDIA Tesla K40) and used batch size 256 split across the 8 GPUs so that each GPU updated with batches of size 32.</p><p>In contrast to the situation with CIFAR10 and CIFAR100, on ImageNet our all-convolutional model performed significantly worse than its original counterpart. Specifically, we experienced a significant amount of underfitting suggesting that a larger model would likely perform better.</p><p>Despite this issue, our model still reached 35.29% top-1 classification error on the test set (50000 data points), and 14.17% top-5 test error after 700, 000 steps (about one week of training). While no longer state-of-the-art, this performance is significantly better than the 40.7% reported by <ref type="bibr" target="#b12">Krizhevsky et al. (2012)</ref>, as well as the best all-convolutional architecture by <ref type="bibr" target="#b14">Springenberg et al. (2014)</ref>. We believe it is quite likely that a better learning rate schedule and hyperparameter settings of our model could substantially improve on the preliminary performance reported here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>Our theory underlines the importance of identity parameterizations when training deep artificial neural networks. An outstanding open problem is to extend our optimization result to the non-linear case where each residual has a single ReLU activiation as in our expressivity result. We conjecture that a result analogous to Theorem 2.2 is true for the general non-linear case. Unlike with the standard parameterization, we see no fundamental obstacle for such a result.</p><p>We hope our theory and experiments together help simplify the state of deep learning by aiming to explain its success with a few fundamental principles, rather than a multitude of tricks that need to be delicately combined. We believe that much of the advances in image recognition can be achieved with residual convolutional layers and ReLU activations alone. This could lead to extremely simple (albeit deep) architectures that match the state-of-the-art on all image classification benchmarks.</p><p>A.2 PROOF OF LEMMA 2.4</p><p>We compute the partial gradients by definition. Let ∆ j ∈ R d×d be an infinitesimal change to A j . Using Claim 2.3, consider the Taylor expansion of f (A 1 , . . . , A + ∆ j , . . . , A )</p><formula xml:id="formula_25">f (A 1 , . . . , A + ∆ j , . . . , A ) = ((Id + A ) • • • (Id + A j + ∆ j ) . . . (Id + A 1 ) − R)Σ 1/2 2 F = ((Id + A ) • • • (Id + A 1 ) − R)Σ 1/2 + (Id + A ) • • • ∆ j . . . (Id + A 1 )Σ 1/2 2 F = (Id + A ) • • • (Id + A 1 ) − R)Σ 1/2 2 F + 2 ((Id + A ) • • • (Id + A 1 ) − R)Σ 1/2 , (Id + A ) • • • ∆ j . . . (Id + A 1 )Σ 1/2 + O( ∆ j 2 F ) = f (A) + 2 (Id + A ) . . . (Id + A j+1 )EΣ(Id + A j−1 ) . . . (Id + A 1 ), ∆ j + O( ∆ j 2 F ) .</formula><p>By definition, this means that the ∂f ∂Aj = 2(Id + A ) . . . (Id + A j+1 )EΣ(Id + A j−1 ) . . . (Id + A 1 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B MISSING PROOFS IN SECTION 3</head><p>In this section, we provide the full proof of Theorem 3.2. We start with the following Lemma that constructs a building block T that transform k vectors of an arbitrary sequence of n vectors to any arbitrary set of vectors, and main the value of the others. For better abstraction we use α (i) ,β (i) to denote the sequence of vectors.</p><p>Lemma B.1. Let S ⊂ [n] be of size k. Suppose α (1) , . . . , α (n) is a sequences of n vectors satisfying a) for every 1 ≤ i ≤ n, we have 1 − ρ ≤ α i 2 ≤ 1 + ρ , and b) if i = j and S contains at least one of i, j, then α (i) − β (j) ≥ 3ρ . Let β (1) , . . . , β (n) be an arbitrary sequence of vectors. Then, there exists U, V ∈ R k×k , s such that for every i ∈ S, we have T U,V,s (α (i) ) = β (i) − α (i) , and moreover, for every i ∈ [n]\S we have T U,V,s (α (i) ) = 0.</p><p>We can see that the conclusion implies</p><formula xml:id="formula_26">β (i) = α (i) + T U,V,s (α (i) ) ∀i ∈ S α (i) = α (i) + T U,V,s (α (i) ) ∀i ∈ S</formula><p>which is a different way of writing equation (3.6).</p><p>Proof of Lemma B.1. Without loss of generality, suppose S = {1, . . . , k}. We construct U, V, s as follows. Let the i-th row of U be α (i) for i ∈ [k], and let s = −(1 − 2ρ ) • 1 where 1 denotes the all 1's vector. Let the i-column of V be</p><formula xml:id="formula_27">1 α (i) 2 −(1−2ρ ) (β (i) − α (i) ) for i ∈ [k].</formula><p>Next we verify that the correctness of the construction. We first consider 1 ≤ i ≤ k. We have that U α (i) is a a vector with i-th coordinate equal to α (i) 2 ≥ 1 − ρ . The j-th coordinate of U α (i) is equal to α (j) , α (i) , which can be upperbounded using the assumption of the Lemma by</p><formula xml:id="formula_28">α (j) , α (i) = 1 2 α (i) 2 + α (j) 2 − α (i) − α (j) 2 ≤ 1 + ρ − 3ρ ≤ 1 − 2ρ . (B.1)</formula><p>Therefore, this means U α (i) − (1 − 2ρ ) • 1contains a single positive entry (with value at least α (i) 2 − (1 − 2ρ ) ≥ ρ ), and all other entries being non-positive. This means that ReLu(U α (i) + b) = α (i) 2 − (1 − 2ρ ) e i where e i is the i-th natural basis vector. It follows that</p><formula xml:id="formula_29">V ReLu(U α (i) + b) = ( α (i) 2 − (1 − 2ρ ))V e i = β (i) − α (i) .</formula><p>Finally, consider n ≥ i &gt; k. Then similarly to the computation in equation (B.1), U α (i) is a vector with all coordinates less than 1 − 2ρ . Therefore U α (i) + b is a vector with negative entries. Hence we have ReLu(U</p><formula xml:id="formula_30">α (i) + b) = 0, which implies V ReLu(U α (i) + b) = 0.</formula><p>Now we are ready to state the formal version of Lemma 3.3. Lemma B.2. Suppose a sequence of n vectors z (1) , . . . , z (n) satisfies a relaxed version of Assumption 3.1: a) for every i, 1 − ρ ≤ z (i) 2 ≤ 1 + ρ b) for every i = j, we have z (i) − z (j) 2 ≥ ρ ;. Let v (1) , . . . , v (n) be defined above. Then there exists weigh matrices (A 1 , B 1 ), . . . , (A , B ), such that given ∀i, h</p><formula xml:id="formula_31">i) 0 = z (i) , we have, ∀i ∈ {1, . . . , n}, h (i) = v (i) .<label>(</label></formula><p>We will use Lemma B.1 repeatedly to construct building blocks T Aj ,B k ,sj (•), and thus prove Lemma B.2. Each building block T Aj ,B k ,sj (•) takes a subset of k vectors among {z (1) , . . . , z (n) } and convert them to v (i) 's, while maintaining all other vectors as fixed. Since they are totally n/k layers, we finally maps all the z (i) 's to the target vectors v (i) 's.  i) , and for i ≥ k, it holds that h</p><formula xml:id="formula_32">(i) = z (i) and β (i) = v (i) for i ∈ [n], we obtain that there exists A 1 , B 1 , b 1 such that for i ≤ k, it holds that h (i) 1 = z (i) + T A1,B1,b1 (z (i) ) = v (</formula><formula xml:id="formula_33">(i) 1 = z (i) + T A1,B1,b1 (z (i) ) = z (i) .</formula><p>Now we construct the other layers inductively. We will construct the layers such that the hidden variable at layer j satisfies h (i) j = v (i) for every 1 ≤ i ≤ jk, and h (i) j = z (i) for every n ≥ i &gt; jk. Assume that we have constructed the first j layer and next we use Lemma B.1 to construct the j + 1 layer. Then we argue that the choice of α n) , and S = {jk + 1, . . . , (j + 1)k} satisfies the assumption of Lemma B.1. Indeed, because q i 's are chosen uniformly randomly, we have w.h.p for every s and i, q s , z (i) ≤ 1 − ρ . Thus, since v (i) ∈ {q 1 , . . . , q r }, we have that v (i) also doesn't correlate with any of the z (i) . Then we apply Lemma B.1 and conclude that there exists i) for jk &lt; i ≤ (j + 1)k, and T Aj+1,bj+1,bj+1 (z (i) ) = 0 for n ≥ i &gt; (j + 1)k. These imply that</p><formula xml:id="formula_34">(1) = v (1) , . . . , α (jk) = v (jk) , α (jk+1) = z (jk+1) , . . . , α (n) = z (</formula><formula xml:id="formula_35">A j+1 = U, B j+1 = V, b j+1 = s such that T Aj+1,bj+1,bj+1 (v (i) ) = 0 for i ≤ jk, T Aj+1,bj+1,bj+1 (z (i) ) = v (i) − z (</formula><formula xml:id="formula_36">h (i) j+1 = h (i) j + T Aj+1,bj+1,bj+1 (v (i) ) = v (i) ∀1 ≤ i ≤ jk h (i) j+1 = h (i) j + T Aj+1,bj+1,bj+1 (z (i) ) = v (i) ∀jk + 1 ≤ i ≤ (j + 1)k h (i) j+1 = h (i) j + T Aj+1,bj+1,bj+1 (z (i) ) = z (i) ∀(j + 1)k &lt; i ≤ n</formula><p>Therefore we constructed the j + 1 layers that meets the inductive hypothesis for layer j + 1. Therefore, by induction we get all the layers, and the last layer satisfies that h (i) = v (i) for every example i. Now we ready to prove Theorem 3.2, following the general plan sketched in Section 3.</p><p>Proof of Theorem 3.2. We use formalize the intuition discussed below Theorem 3.2. First, take k = c(log n)/ρ 2 for sufficiently large absolute constant c (for example, c = 10 works), by Johnson-Lindenstrauss Theorem <ref type="bibr" target="#b9">(Johnson &amp; Lindenstrauss (1984)</ref>, or see Wikipedia ( <ref type="formula">2016</ref>)) we have that when A 0 is a random matrix with standard normal entires, with high probability, all the pairwise distance between the the set of vectors {0, x (1) , . . . , x (n) } are preserved up to 1 ± ρ/3 factor. That is, we have that for every i, 1−ρ/3 ≤ A 0 x (i) ≤ 1+ρ/3, and for every i = j, A 0 x (i) −A 0 x (j) ≥ ρ(1 − ρ/3) ≥ 2ρ/3. Let z (i) = A 0 x (i) and ρ = ρ/3. Then we have z (i) 's satisfy the condition of Lemam B.2. We pick r random vectors q 1 , . . . , q r in R k . Let v (1) , . . . , v (n) be defined as in equation (3.2). Then by Lemma B.2, we can construct matrices (A 1 , B 1 ), . . . , (A , B ) such that</p><formula xml:id="formula_37">h (i) = v (i) . (B.2)</formula><p>Note that v (i) ∈ {q 1 , . . . , q r }, and q i 's are random unit vector. Therefore, the choice of α (1) = q 1 , . . . , α (r) = q r , β (1) = e 1 , . . . , β (r) = e r , and satisfies the condition of Lemma B.1, and using Lemma B.1 we conclude that there exists A +1 , B +1 , s +1 such that e j = v j + T A +1 ,B +1 ,b +1 (v j ), for every j ∈ {1, . . . , r} ..</p><formula xml:id="formula_38">(B.3)</formula><p>By the definition of v (i) in equation (3.2) and equation (B.2), we conclude that ŷ(i) = h (i) + T A +1 ,B +1 ,b +1 (h (i) ) = y (i) ., which complete the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C TOOLBOX</head><p>In this section, we state two folklore linear algebra statements. The following Claim should be known, but we can't find it in the literature. We provide the proof here for completeness. Claim C.1. Let U ∈ R d×d be a real normal matrix (that is, it satisfies U U = U U ). Then, there exists an orthonormal matrix S ∈ R d×d such that</p><formula xml:id="formula_39">U = SDS ,</formula><p>where D is a real block diagonal matrix that consists of blocks with size at most 2 × 2. Moreover, if d is even, then D consists of blocks with size exactly 2 × 2.</p><p>Proof. Since U is a normal matrix, it is unitarily diagonalizable (see <ref type="bibr" target="#b15">Weisstein (2016)</ref> for backgrounds). Therefore, there exists unitary matrix V in C d×d and diagonal matrix in C d×d such that U has eigen-decomposition U = V ΛV * . Since U itself is a real matrix, we have that the eigenvalues (the diagonal entries of Λ) come as conjugate pairs, and so do the eigenvectors (which are the columns of V ). That is, we can group the columns of V into pairs (v 1 , v1 ), . . . , (v s , vs ), v s+1 , . . . , v t , and let the corresponding eigenvalues be λ 1 , λ1 , . . . , λ λs , λs , λ s+1 , . . . , λ t . Here λ s+1 , . . . , λ t ∈ R. Then we get that U =</p><formula xml:id="formula_40">s i=1 2 (v i λ i v * i ) + t i=s+1 v i λ i v i . Let Q i = (v i λ i v * i )</formula><p>, then we have that Q i is a real matrix of rank-2. Let S i ∈ R d×2 be a orthonormal basis of the column span of Q i and then we have that Q i can be written as Q i = S i D i S i where D i is a 2 × 2 matrix. Finally, let S = [S 1 , . . . , S s , v s+1 , . . . , v t ], and D = diag(D 1 , . . . , D s , λ s+1 , . . . , λ t ) we complete the proof.</p><p>The following Claim is used in the proof of Theorem 2.2. We provide a proof here for completeness. Claim C.2 (folklore). For any two matrices A, B ∈ R d×d , we have that AB F ≥ σ min (A) B F .</p><p>Proof. Since σ min (A) 2 is the smallest eigenvalue of A A, we have that </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Convergence plots of best model for CIFAR10 (left) and CIFAR (100) right. One step is a gradient update with batch size 128.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>B A AB B • σ min (A) 2 Id • B .Therefore, it follows thatAB 2 F = tr(B A AB) ≥ tr(B • σ min (A) 2 Id • B) = σ min (A) 2 tr(B B) = σ min (A) 2 B 2 F .Taking square root of both sides completes the proof.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Architecture for CIFAR10/100 (55 convolutions, 13.5M parameters)</figDesc><table><row><cell>variable dimensions</cell><cell cols="2">initial stride description</cell></row><row><cell>3 × 3 × 3 × 16</cell><cell>1</cell><cell>1 standard conv</cell></row><row><cell>3 × 3 × 16 × 64</cell><cell>1</cell><cell>9 residual blocks</cell></row><row><cell>3 × 3 × 64 × 128</cell><cell>2</cell><cell>9 residual blocks</cell></row><row><cell>3 × 3 × 128 × 256</cell><cell>2</cell><cell>9 residual blocks</cell></row><row><cell>-</cell><cell>-</cell><cell>8 × 8 global average pool</cell></row><row><cell>256 × num classes</cell><cell>-</cell><cell>random projection (not trained)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">: Comparison of top-1 classification error on different benchmarks</cell></row><row><cell>Method</cell><cell cols="4">CIFAR10 CIFAR100 ImageNet remarks</cell></row><row><cell>All-CNN</cell><cell>7.25</cell><cell>32.39</cell><cell>41.2</cell><cell>all-convolutional, dropout, extra data processing</cell></row><row><cell>Ours</cell><cell>6.38</cell><cell>24.64</cell><cell>35.29</cell><cell>all-convolutional</cell></row><row><cell>ResNet</cell><cell>6.43</cell><cell>25.16</cell><cell>19.38</cell><cell></cell></row><row><cell>DenseNet</cell><cell>3.74</cell><cell>19.25</cell><cell>N/A</cell><cell></cell></row><row><cell cols="2">4.2 IMAGENET</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Proof of Lemma B.2. We use Lemma B.1 repeatedly. Let S 1 = [1, . . . , k]. Then using Lemma B.1 with α</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">That is, T F := ijk T</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">ijk .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2">In computer vision, typically r is less than 10</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3">and d is less than 10 5 while n is larger than 10 6</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_4">https://github.com/tensorflow/models/tree/master/resnet</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_5">Here for notational convenience, p, q are not chosen to be integers. But rounding them to closest integer will change final bound of the norm by small constant factor.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A MISSING PROOFS IN SECTION 2</head><p>In this section, we give the complete proofs for Theorem 2.1 and Lemma 2.4, which are omitted in Section 2.</p><p>A.1 PROOF OF THEOREM 2.1 It turns out the proof will be significantly easier if R is assumed to be a symmetric positive semidefinite (PSD) matrix, or if we allow the variables to be complex matrices. Here we first give a proof sketch for the first special case. The readers can skip it and jumps to the full proof below. We will also prove stronger results, namely, |||A ||| ≤ 3γ/ , for the special case.</p><p>When R is PSD, it can be diagonalized by orthonormal matrix U in the sense that R = U ZU , where Z = diag(z 1 , . . . , z d ) is a diagonal matrix with non-negative diagonal entries z 1 , . . . , z d . Let</p><p>We see that the network defined by A reconstruct the transformation R, and therefore it's a global minimum of the population risk (formally see Claim 2.3 below). Next, we verify that each of the A j has small spectral norm:</p><p>Then using equation (A.1) and the equation above, we have that |||A||| ≤ max j A j ≤ 3γ/ , which completes the proof for the special case.</p><p>Next we give the formal full proof of Theorem 2.1.</p><p>Proof of Theorem 2.1. We assume the dimension d is an even number. The odd case has very similar proof and is left to the readers. Let R = U KV be its singular value decomposition, where U ,V are two orthonormal matrices and K is a diagonal matrix. Since U is a normal matrix (that is, U satisfies that U U = U U ), by Claim C.1, we have that U can be block-diagnolaized by orthonormal matrix</p><p>) is a real block diagonal matrix with each block D i being of size 2 × 2.</p><p>Since U is orthonormal, U has all its eigenvalues lying on the unit circle (in complex plane). Since D and U are unitarily similar to each other, D also has eigenvalues lying on the unit circle, and so does each of the block D i . This means that each D i is a 2 × 2 dimensional rotation matrix. Each rotation matrix can be written as</p><p>we have that D i = T (θ i /q) q for any integer q (that is chosen later). Let W = diag(T (θ i /q)).</p><p>Therefore, it follows that D = diag(D i ) = W q . Moreover, we have</p><p>We verify the spectral norm of these matrices are indeed small,</p><p>Similarly, we can choose B 1 , . . . , B q with C j ≤ π/q so that V = (Id + B q ) . . . (Id + B 1 ).</p><p>Last, we deal with the diagonal matrix K. Let K = diag(k i ). We have min k i = σ min (R), max k i = σ max (R). Then, we can write K = (K ) p where K = diag(k 1/p i ) and p is an integer to be chosen later. We have that</p><p>3γ , 4 and let A 2p+q = B q , • • • = A p+q+1 = B 1 , A p+q = B p , . . . , A q+1 = B 1 , A q = B q , . . . , A 1 = B 1 . We have that 2q + = 1 and R = U KV = (Id + A ) . . . (Id + A 1 ) .</p><p>Moreover, we have |||A||| ≤ max{ B j , B j . B j } ≤ π/q + 3γ/p ≤ 2( √ π + √ 3γ) 2 / , as desired.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Random matrices and complexity of spin glasses</title>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Auffinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gérard</forename><surname>Ben Arous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiří</forename><surname>Černỳ</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications on Pure and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="165" to="201" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural networks and principal component analysis: Learning from examples without local minima</title>
		<author>
			<persName><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
		<idno type="DOI">10.1016/0893-6080(89)90014-2</idno>
		<ptr target="http://dx.doi.org/10.1016/0893-6080(89)90014-2" />
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<idno type="ISSN">0893-6080</idno>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="58" />
			<date type="published" when="1989-01">January 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The loss surfaces of multilayer networks</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gérard</forename><surname>Ben Arous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Identifying and attacking the saddle point problem in high-dimensional non-convex optimization</title>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2933" to="2941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Qualitatively characterizing neural network optimization problems</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Saxe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-12">December 2014</date>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXivprepringarXiv:1506.01497</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46493-038</idno>
		<ptr target="http://dx.doi.org/10.1007/978-3-319-46493-0_" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 -14th European Conference</title>
				<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">October 11-14, 2016. 2016</date>
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
	<note>Proceedings, Part IV</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>CoRR, abs/1608.06993</idno>
		<ptr target="http://arxiv.org/abs/1608.06993" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<ptr target="http://jmlr.org/proceedings/papers/v37/ioffe15.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning, ICML 2015</title>
				<meeting>the 32nd International Conference on Machine Learning, ICML 2015<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07-11">6-11 July 2015. 2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Extensions of lipschitz mappings into a hilbert space</title>
		<author>
			<persName><forename type="first">Johnson</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joram</forename><surname>Lindenstrauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Contemporary mathematics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Linear Convergence of Gradient and Proximal-Gradient Methods Under the Polyak-\L{}ojasiewicz Condition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nutini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-08">August 2016</date>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Kawaguchi</surname></persName>
		</author>
		<title level="m">Deep Learning without Poor Local Minima</title>
				<imprint>
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">No bad local minima: Data independent training error guarantees for multilayer neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Carmon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Striving for Simplicity: The All Convolutional Net</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-12">December 2014</date>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Normal matrix, from mathworld-a wolfram web resource</title>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">W</forename><surname>Weisstein</surname></persName>
		</author>
		<ptr target="http://mathworld.wolfram.com/NormalMatrix.html" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Johnsonlindenstrauss lemma -wikipedia, the free encyclopedia</title>
		<author>
			<persName><surname>Wikipedia</surname></persName>
		</author>
		<ptr target="https://en.wikipedia.org/w/index.php?title=Johnson%E2%80%93Lindenstrauss_lemma&amp;oldid=743553642" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
