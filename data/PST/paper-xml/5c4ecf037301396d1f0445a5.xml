<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reliability Engineering and System Safety</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-11-26">26 November 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">André</forename><surname>Listou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Ocean Operations and Civil Engineering</orgName>
								<orgName type="institution">Norwegian University of Science and Technology</orgName>
								<address>
									<postCode>6009</postCode>
									<settlement>Aalesund</settlement>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Emil</forename><surname>Bjørlykhaug</surname></persName>
							<email>emil.bjorlykhaug@ntnu.no</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Ocean Operations and Civil Engineering</orgName>
								<orgName type="institution">Norwegian University of Science and Technology</orgName>
								<address>
									<postCode>6009</postCode>
									<settlement>Aalesund</settlement>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vilmar</forename><surname>Aesøy</surname></persName>
							<email>vilmar.aesoy@ntnu.no</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Ocean Operations and Civil Engineering</orgName>
								<orgName type="institution">Norwegian University of Science and Technology</orgName>
								<address>
									<postCode>6009</postCode>
									<settlement>Aalesund</settlement>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sergey</forename><surname>Ushakov</surname></persName>
							<email>sergey.ushakov@ntnu.no</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Marine Technology</orgName>
								<orgName type="institution">Norwegian University of Science and Technology</orgName>
								<address>
									<postCode>7491</postCode>
									<settlement>Trondheim</settlement>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Houxiang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Ocean Operations and Civil Engineering</orgName>
								<orgName type="institution">Norwegian University of Science and Technology</orgName>
								<address>
									<postCode>6009</postCode>
									<settlement>Aalesund</settlement>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Reliability Engineering and System Safety</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-11-26">26 November 2018</date>
						</imprint>
					</monogr>
					<idno type="MD5">F59851703A7B41B5ECB7560235E892E9</idno>
					<idno type="DOI">10.1016/j.ress.2018.11.027</idno>
					<note type="submission">Received 18 June 2018; Received in revised form 16 November 2018; Accepted 24 November 2018 ⁎ Corresponding author.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>C-MAPSS Deep learning Genetic algorithm Prognostics and health management Remaining useful life Semi-supervised learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, research has proposed several deep learning (DL) approaches to providing reliable remaining useful life (RUL) predictions in Prognostics and Health Management (PHM) applications. Although supervised DL techniques, such as Convolutional Neural Network and Long-Short Term Memory, have outperformed traditional prognosis algorithms, they are still dependent on large labeled training datasets. With respect to real-life PHM applications, high-quality labeled training data might be both challenging and time-consuming to acquire. Alternatively, unsupervised DL techniques introduce an initial pre-training stage to extract degradation related features from raw unlabeled training data automatically. Thus, the combination of unsupervised and supervised (semi-supervised) learning has the potential to provide high RUL prediction accuracy even with reduced amounts of labeled training data. This paper investigates the effect of unsupervised pre-training in RUL predictions utilizing a semi-supervised setup. Additionally, a Genetic Algorithm (GA) approach is applied in order to tune the diverse amount of hyper-parameters in the training procedure. The advantages of the proposed semisupervised setup have been verified on the popular C-MAPSS dataset. The experimental study, compares this approach to purely supervised training, both when the training data is completely labeled and when the labeled training data is reduced, and to the most robust results in the literature. The results suggest that unsupervised pre-training is a promising feature in RUL predictions subjected to multiple operating conditions and fault modes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The remaining useful life (RUL) is a technical term used to describe the progression of faults in Prognostics and Health Management (PHM) applications <ref type="bibr" target="#b0">[1]</ref>. Prognosis algorithms tend ideally to achieve the ideal maintenance policy through predictions of the available time before a failure occurs within a component or sub-component, that is RUL <ref type="bibr" target="#b1">[2]</ref>. In this way, RUL predictions have the potential to prevent critical failures, and hence, becomes an important measurement to achieve the ultimate goal of zero-downtime performance in industrial systems. However, traditional prognosis algorithms suffer from a decreased capacity to process the increased complexity in today's sequential data with accuracy.</p><p>Recently, deep learning (DL) has emerged as a potent area to process highly non-linear and varying sequential data with minimal human input within the PHM domain <ref type="bibr" target="#b2">[3]</ref>. Today, DL is an extremely active sub-field of machine learning. With increased processing power and continuous developments in graphics processors, DL has the potential to improve prediction tasks as the computational burden reduces significantly <ref type="bibr" target="#b3">[4]</ref>. However, deep architectures introduce many diverse hyper-parameters, which are challenging to optimize in the training process. Thus, this study proposes a Genetic Algorithm (GA) approach in order to optimize the hyper-parameters in an efficient manner.</p><p>DL techniques, such as Convolutional Neural Network (CNN) and Long-Short Term Memory (LSTM), have shown rapid developments and outperformed traditional prognosis algorithms in RUL predictions for turbofan engine degradation <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>. DL techniques predict the RUL without any prior knowledge of engine degradation mechanics. Thus, data analysts today apply their knowledge about the RUL prediction problem to the selection and design of DL techniques, rather than to feature engineering. However, both CNN and LSTM depend on purely supervised learning. In other words, they require large labeled training datasets in the training procedure. Thus, the RUL prediction accuracy strongly depends on the quality of the constructed run-to-failure training data labels.</p><p>In contrast, unsupervised DL techniques introduce an initial pretraining stage to extract high-level abstract features from raw unlabeled training data automatically. Thus, the combination of unsupervised and supervised (semi-supervised) learning has the potential for even higher RUL prediction accuracy since the weights are initialized in a region near a good local minimum before supervised fine-tuning is conducted to minimize the global training objective <ref type="bibr" target="#b7">[8]</ref>.</p><p>More advanced and recent activation functions <ref type="bibr" target="#b8">[9]</ref>, learning rate methods <ref type="bibr" target="#b9">[10]</ref>, regularization techniques <ref type="bibr" target="#b10">[11]</ref>, and weight initializations <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> have indeed reduced the need for unsupervised pretraining in a variety of domains when the training data is completely labeled. Nevertheless, in real-life PHM applications, high-quality runto-failure labeled training data is not easily obtained, especially from new equipment. However, unsupervised pre-training in semi-supervised setups has the potential to perform with high RUL prediction accuracy even with reduced amounts of labeled training data in the fine-tuning procedure. Additionally, most data collected in real-life PHM applications is subjected to several operating conditions and fault modes. This increases the inherent degradation complexity, which makes it more difficult for the prognosis algorithm to discover clear trends in the input data directly. To cope with this issue, the initial unsupervised pre-training stage can be utilized. Unsupervised pretraining extracts more degradation related features before supervised fine-tuning, and hence, has the potential to support the whole architecture to better understand the underlying degradation phenomena.</p><p>The aim of this paper is to show the effect of unsupervised pretraining in RUL predictions utilizing a semi-supervised setup. The results are verified on the four different simulated turbofan engine degradation datasets in the publicly available Commercial Modular Aero-Propulsion System Simulation (C-MAPSS) dataset, produced and provided by NASA <ref type="bibr" target="#b13">[14]</ref>. This study's main contributions are as follows:</p><p>• The GA approach effectively tunes hyper-parameters in deep ar- chitectures.</p><p>• Semi-supervised learning improves the RUL prediction accuracy compared to supervised learning in multivariate time series data with several operating conditions and fault modes when the training data is completely labeled.</p><p>• Semi-supervised learning performs higher RUL prediction accuracy compared to supervised learning when the labeled training data in the fine-tuning procedure is reduced.</p><p>The overall organization of the paper is as follows. Section 2 introduces recent and related work on the C-MAPSS dataset. Section 3 introduces the necessary background on GAs and the proposed semisupervised setup. The experimental approach, results, and discussions are considered in Section 4. Finally, Section 5 concludes and closes the paper and provides directions for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>The C-MAPSS dataset has been extensively used to evaluate several DL approaches to RUL predictions. This section reviews the most recent studies applied on the C-MAPSS dataset. The selected studies either utilize a Convolutional Neural Network (CNN), a Deep Belief Network (DBN) or Long-Short Term Memory (LSTM) in the proposed deep architecture.</p><p>In most PHM applications, sequential data is a standard format of the input data, for example pressure and temperature time series data. LSTM is a well-established DL technique to process sequential data. The original LSTM <ref type="bibr" target="#b14">[15]</ref> was developed after the early 1990s, when researchers discovered a vanishing and exploding gradient issue in traditional Recurrent Neural Networks (RNNs) <ref type="bibr" target="#b15">[16]</ref>. This issue confirmed that traditional RNNs had difficulty learning long-term dependencies. To cope with this issue, the LSTM introduces a memory cell that regulates the information flow in and out of the cell. Consequently, the memory cell is able to preserve its state over long durations, that is learning long-term dependencies that may influence future predictions. Yuan et al. proposed an LSTM approach for several different faults <ref type="bibr" target="#b16">[17]</ref>. The proposed approach was compared with traditional RNN, Gated Recurrent Unit LSTM (GRU-LSTM) and AdaBoost-LSTM. It showed improved performance in all cases. Another LSTM approach was provided by Zheng et al. <ref type="bibr" target="#b5">[6]</ref>. The proposed approach provides RUL predictions using two LSTM layers, two Feed-forward Neural Network (FNN) layers, and an output layer. The LSTM layers were able to reveal hidden patterns in the C-MAPSS dataset and achieved higher accuracy compared to the Hidden Markov Model or traditional RNN. A similar study was provided by Wu et al. <ref type="bibr" target="#b17">[18]</ref>. In this study, an LSTM was combined with a dynamic difference method in order to extract new features from several operating conditions before the training procedure. These features contain important degradation information, which improves the LSTM to better control the underlying physical process. The proposed approach showed enhanced performance compared to traditional RNN and GRU-LSTM.</p><p>Although CNNs have performed excellently on 2D and 3D gridstructured topology data, such as object recognition <ref type="bibr" target="#b19">[20]</ref> and face recognition <ref type="bibr" target="#b20">[21]</ref>, respectively, CNNs can also be applied to 1D gridstructured topology sequential data in PHM applications. Babu et al. proposed a novel CNN approach for RUL predictions <ref type="bibr" target="#b4">[5]</ref>. This CNN approach includes two layers with convolution and average-pooling steps, and a final FNN layer to perform RUL predictions. The proposed approach indicated improved accuracy compared to the Multilayer Perceptron (MLP), Support Vector Machine (SVM), and Relevance Vector Machine. More recently, <ref type="bibr" target="#b6">[7]</ref> takes a CNN approach. In this study, Li et al. achieved even higher accuracy on the C-MAPSS dataset compared to both the LSTM approach in <ref type="bibr" target="#b5">[6]</ref> and the CNN approach in <ref type="bibr" target="#b4">[5]</ref>. They employed the recently developed, proven regularization technique "dropout" <ref type="bibr" target="#b10">[11]</ref> and the adaptive learning rate method "adam" <ref type="bibr" target="#b9">[10]</ref>.</p><p>Hinton et al. introduced the greedy layer-wise unsupervised learning algorithm in 2006, designing it for DBNs <ref type="bibr" target="#b21">[22]</ref>. A DBN consists of stacked Restricted Boltzmann Machines (RBMs) where the hidden layer in the previous RBM will serve as the input layer for the current RBM. The algorithm performs an initial unsupervised pre-training stage to learn internal representations from the input data automatically. Next, supervised fine-tuning is performed to minimize the training objective. Zhang et al. have proposed a multiobjective DBN ensemble approach <ref type="bibr" target="#b18">[19]</ref>. This approach combines a multiobjective evolutionary ensemble learning framework with the DBN training process. Accordingly, the proposed approach creates multiple DBNs of varying accuracy and diversity before the evolved DBNs are combined to perform RUL predictions. The combined DBNs are optimized through differential evolution where the average training error is the single objective. The proposed approach outperformed several traditional machine learning algorithms, such as SVM and MLP. The recent studies are summarized in Table <ref type="table">1</ref>.</p><p>These studies all utilize a completely labeled run-to-failure training dataset in the training procedure. However, in real-life PHM scenarios, most data accumulated is unstructured and unlabeled from the start.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>Recent DL approaches proposed for RUL predictions on the C-MAPSS dataset <ref type="bibr" target="#b13">[14]</ref> (the years between 2016 and 2018).</p><p>Author &amp; Refs.</p><p>Year Approach Li et al. <ref type="bibr" target="#b6">[7]</ref> 2018 CNN + FNN Wu et al. <ref type="bibr" target="#b17">[18]</ref> 2018 LSTM Zheng et al. <ref type="bibr" target="#b5">[6]</ref> 2017 LSTM + FNN Yuan et al. <ref type="bibr" target="#b16">[17]</ref> 2016 LSTM Zhang et al. <ref type="bibr" target="#b18">[19]</ref> 2016 MODBNE Babu et al. <ref type="bibr" target="#b4">[5]</ref> 2016 CNN + FNN Valuable domain knowledge is required to construct run-to-failure data labels. This is both a time-consuming and challenging process. Thus, this study will investigate the effect of unsupervised pre-training in a semi-supervised setup both with reduced and completely labeled training datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed semi-supervised setup</head><p>This section will introduce the necessary background on the proposed semi-supervised setup. First, the main DL techniques included, RBM and LSTM, are defined. Next, the proposed deep architecture structure as well as the GA approach for hyper-parameter tuning are elaborated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Restricted Boltzmann machine</head><p>RBMs were originally developed using binary stochastic visible units, v, in the input layer and binary stochastic hidden units, h, in the hidden layer <ref type="bibr" target="#b22">[23]</ref>. However, in real-value data applications, like the C-MAPSS dataset, linear Gaussian units replace the binary visible units and rectified linear units replace the binary hidden units <ref type="bibr" target="#b23">[24]</ref>. RBMs are symmetrical bipartite graphs since the visible and hidden units are fully connected and units in the same layer have zero connections.</p><p>RBMs are energy-based models with the joint probability distribution specified by their energy function <ref type="bibr" target="#b24">[25]</ref>:</p><formula xml:id="formula_0">= P v h Z e ( , ) 1 E v h ( , )<label>(1)</label></formula><p>where Z is the partition function that ensures that the distribution is normalized:</p><formula xml:id="formula_1">= Z e v h E v h ( , )<label>(2)</label></formula><p>The energy function for RBMs with Gaussian visible units is given by:</p><formula xml:id="formula_2">= = = = = E v h v b c h v h w ( , ) ( ) 2 i V i i i j H j j i V j H i i j ij 1 2 2 1 1 1<label>(3)</label></formula><p>where w ij denotes the weight between the visible unit v i and hidden unit h j , b i and c j represents the bias terms, V and H expresses the numbers of visible and hidden units, respectively, and γ i is the standard deviation of v i . As recommended by Hinton <ref type="bibr" target="#b24">[25]</ref>, zero mean and unit variance normalization should be applied to the input data. Contrastive divergence is used to train RBMs:</p><formula xml:id="formula_3">= w v h v h ( ) ij i j data i j recon (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>where ϵ is the learning rate. First, the data distribution samples visible units based on hidden units. Then, the input data is reconstructed, generated by Gibbs sampling, which samples hidden units based on visible units. This process continues until the parameters converge, that is, the hidden layer approximates the input layer. In this way, RBMs are able to model data distributions without any label knowledge. Typically, after the pre-training stage, the reconstruction part of the RBM is omitted and the pre-trained weights facilitate a subsequent supervised fine-tuning procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Long-Short term memory</head><p>Modifications by Gers et al. <ref type="bibr" target="#b25">[26]</ref> have been included in the original LSTM, and researchers generally refer to this LSTM setup as the "vanilla LSTM." Although several variants of the vanilla LSTM have been proposed, Greff et al. have shown that none of the variants can improve the vanilla LSTM significantly <ref type="bibr" target="#b26">[27]</ref>. Thus, the proposed semi-supervised setup uses the vanilla LSTM.</p><p>The memory cell, as illustrated in Fig. <ref type="figure">1</ref>, consists of three non-linear gating units that protect and regulate the cell state, S t <ref type="bibr" target="#b27">[28]</ref>:</p><formula xml:id="formula_5">= + + f W x R h b ( ) t f t f t f 1 (5) = + + i W x R h b ( ) t i t i t i 1 (6) = + + o W x R h b ( ) t o t o t o 1 (7)</formula><p>where σ is the sigmoid gate activation function in order to obtain a scaled value between 0 and 1, W is the input weight, R is the recurrent weight, and b is the bias weight.</p><p>The new candidate state values, S ˜, t are created by the tanh layer:</p><formula xml:id="formula_6">= + + S tanh W x R h b ˜( ) t s t s t s 1 (8)</formula><p>The previous cell state, S , t 1 is updated into the new cell state, S t , by:</p><formula xml:id="formula_7">= + S f S i S t t t t t 1 (9)</formula><p>where ⊗ denotes element-wise multiplication of two vectors. First, the forget layer, f t , determines which historical information the memory cell removes from S t . Then, the input layer, i t , decides what new Fig. <ref type="figure">1</ref>. Vanilla LSTM, adopted from Olah <ref type="bibr" target="#b27">[28]</ref>. The blue rectangle represents the memory cell. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) information in S ˜t the memory cell will update and store in S t .</p><p>The output layer, o t , determines which parts of S t the memory cell will output. S t is filtered in order to push the values between -1 and 1:</p><formula xml:id="formula_8">= h o tanh S ( ) t t t (<label>10</label></formula><formula xml:id="formula_9">)</formula><p>Through these steps, the vanilla LSTM has the ability to remove or add information to S t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">The proposed deep architecture structure and the genetic algorithm approach</head><p>The proposed semi-supervised deep architecture structure is shown in Fig. <ref type="figure" target="#fig_0">2</ref>. In the first layer (L1), a RBM will be utilized as an unsupervised pre-training stage in order to learn abstract features from raw unlabeled input data automatically. These features might contain important degradation information, and hence, initialize the weights in a region near a good local minimum before supervised fine-tuning of the whole architecture is conducted. In both the second and the third layer (L2 and L3), an LSTM layer is used to reveal hidden information and learn long-term dependencies in sequential data with multiple operating and fault conditions <ref type="bibr" target="#b5">[6]</ref>. Next, an FNN layer is used in the fourth layer (L4) in order to map all extracted features. In the final layer (L5), a time distributed fully connected output layer is attached to handle error calculations and perform RUL predictions.</p><p>The GA is a metaheuristic inspired by the natural selection found in nature <ref type="bibr" target="#b28">[29]</ref>. It is a powerful tool for finding a near-optimal solution in a big search space. In this work, a GA approach is proposed to tune hyperparameters. First, the GA approach selects random hyper-parameters for the proposed semi-supervised deep architecture within a given search space. One such set of random hyper-parameters is called an individual and a set of individuals is called a population. Next, the accuracy of each of the individuals in the population are evaluated by training networks with the individuals hyper-parameters. The best results from the training are then kept and used as parents for the next generation of hyper-parameters. Additionally, some random mutation is performed after the crossover for increasing the exploration of the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental study</head><p>In the following experimental study, the proposed semi-supervised deep architecture will be compared to recent studies in the literature as well as purely supervised training. The latter comparison will be performed with and without the initial pre-training stage utilizing the proposed semi-supervised deep architecture when the labeled training data in the fine-tuning procedure is reduced. Experiments are performed on the four subsets provided in the benchmark C-MAPSS dataset: FD001, FD002, FD003, and FD004. All experiments are run on NIVIDIA GeForce GTX 1060 6 GB and the Microsoft Windows 10 operating system. The programming language is Java 8 with deep learning library "deeplearning4j" (DL4J) version 0.9.1 <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">The benchmark C-MAPSS dataset and performance evaluation</head><p>The C-MAPSS dataset is divided into four subsets, as shown in Table <ref type="table">2</ref>, and each subset is further divided into training and test sets of multiple multivariate time series. Each time series is from a different aircraft gas turbine engine and starts with different degrees of initial wear and manufacturing variation, which is unknown to the data analyzer. All engines operate in normal condition at the start, then begin to degrade at some point during the time series. The degradation in the training sets grows in magnitude until failure, while the degradation in the test sets ends sometime prior to failure, that is the RUL. That is, the last time step for each engine in the test sets provides the true RUL targets. Thus, the main objective is to predict the correct RUL value for each engine in the test sets. The four subsets vary in operating and fault conditions and the data is contaminated with sensor noise. Each subset includes 26 columns: engine number, time step, three operational sensor settings, and 21 sensor measurements. See <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b30">31]</ref> for a deeper understanding of the C-MAPSS dataset.</p><p>The scoring function (S) provided in Saxena et al. <ref type="bibr" target="#b30">[31]</ref> and the root mean square error (RMSE) are used in this study to evaluate the performance of the proposed semi-supervised setup:</p><formula xml:id="formula_10">= &lt; = = S e d e d</formula><p>1, for 0 1, for 0</p><formula xml:id="formula_11">i n i i n i 1 ( )<label>1</label></formula><formula xml:id="formula_12">( ) d i d i 13 10<label>(11)</label></formula><formula xml:id="formula_13">= = RMSE n d 1 i n i 1 2 (12)</formula><p>where n is the total number of true RUL targets in the respective test set and = d RUL RUL i predicted true . As shown in Fig. <ref type="figure">3</ref>, the RMSE gives equal penalty to early and late predictions. In the asymmetric scoring function, however, the penalty for late predictions is larger. Late predictions could cause serious system failures in real-life PHM applications as the maintenance procedure will be scheduled too late. On the other hand, early predictions pose less risk since the maintenance procedure will be scheduled too early, and hence, there is still time to perform maintenance. Nevertheless, the main objective is to achieve the smallest value possible for both S and RMSE, that is, when = d 0 i . Only evaluating performance at the last time step for each engine in the test sets has both advantages and disadvantages. High and reliable RUL prediction accuracy at the very end of components and sub-components lifetime have of course great industrial significance, as this period is critical for PHM applications. However, this evaluation approach could hide the true overall prognostics accuracy as the prognostics horizon of the algorithm is not considered. The prognostics horizon is critical in order to achieve trustworthy confidence intervals for the corresponding RUL prediction. These confidence intervals are important due to both inherent uncertainties with the degradation process and potential flaws in the prognosis algorithm <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>The C-MAPSS dataset <ref type="bibr" target="#b13">[14]</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Data preparation 4.2.1. Masking and padding</head><p>The DL4J library provides a "CSVSequenceRecordReader" to handle time series data. It reads time series data, where each time series is defined in its own file. Each line in the files represents one time step. Consequently, each time series (engine) in the four training sets are split into their own file. The input training data has the following shape: [miniBatchSize, inputSize, timeSeriesLength], where miniBatchSize is the number of engines in the mini batch, input size is the number of columns, and timeSeriesLength is the total number of time steps in the mini batch. The engines have variable time step lengths, and hence, the shorter engines in a mini batch are padded with zeros such that the time step lengths are equal to the longest among them. Accordingly, mask arrays are used during training. These additional arrays record whether a time step is actually present, or whether it is just padding. In all performance evaluations, mask arrays are considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Feature selection</head><p>Sensor 1, 5, 6, 10, 16, 18, and 19 in subset FD001 and FD003 exhibit constant sensor measurements throughout the engineâ;;s lifetime. Constant sensor measurements does not provide any useful degradation  information regarding RUL predictions <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b32">33]</ref>. In addition, subset FD001 and FD003 are subjected to a single operating condition <ref type="bibr" target="#b4">[5]</ref>.</p><p>Hence, the three operational settings are excluded. Accordingly, sensor 2, 3, 4, 7, 8, 9, 11, 12, 13, 14, 15, 17, 20, and 21 are used as the input features for subset FD001 and FD003. Subset FD002 and FD004 are more complex due to six operating conditions <ref type="bibr" target="#b17">[18]</ref>. Six operating conditions make it challenging for the prognosis algorithm to detect clear degradation patterns in the input data directly. However, two LSTM layers were able to find hidden patterns in Zheng et al. <ref type="bibr" target="#b5">[6]</ref>. Additionally, the initial unsupervised pretraining stage is able to capture hierarchically statistical patterns before the supervised fine-tuning procedure. Consequently, these patterns will enable the whole architecture to cope with the complexity inherent in degradation. Thus, all three operational sensor settings and all sensor measurements are used as the input features for subset FD002 and FD004.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">RUL targets</head><p>True RUL targets are only provided at the last time step for each engine in the test sets. In order to construct labels for every time step for each engine in the training sets, Heimes et al. <ref type="bibr" target="#b32">[33]</ref> used an MLP function estimator to show that it is reasonable to estimate RUL as a constant value when the engines operate in normal condition. Based on their experiments, a degradation model was proposed with a constant RUL value (R c ) of 130 and a minimum value of 0. This piece-wise linear RUL target function is still the most common approach in the literature <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>. However, R c varies among the different studies. For this study, the GA approach is used to test different R c since it has a notable impact on the experimental performance for the different subsets in the C-MAPSS dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4.">Data normalization</head><p>All input features and labels are normalized with zero mean unit variance (z-score) normalization:</p><formula xml:id="formula_14">= z x µ (<label>13</label></formula><formula xml:id="formula_15">)</formula><p>where μ is the mean and σ is the corresponding standard deviation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Deep architecture configuration and training</head><p>In the initial RBM layer, a rectified linear unit (ReLU) is used as the activation function as ReLUs improve the performance of RBMs compared to the tanh activation function <ref type="bibr" target="#b8">[9]</ref>. Stochastic gradient descent is the selected optimization algorithm and adaptive moment estimation (Adam) <ref type="bibr" target="#b9">[10]</ref> is the learning rate method applied to the deep architecture. Recently, Adam has shown great results on the C-MAPSS dataset <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18]</ref>. To better preserve the information in the pre-trained weights, the learning rate in the initial RBM layer is one order of magnitude higher than the learning rate in the remaining layers. ReLU weight initialization <ref type="bibr" target="#b12">[13]</ref> is applied to the RBM layer while Xavier weight initialization <ref type="bibr" target="#b11">[12]</ref> is applied to the remaining layers in the proposed semi-supervised deep architecture.</p><p>Truncated backpropagation through time (TBPTT) is used in this study due to a large amount of time steps in the training sets. TBPTT performs more frequent parameter updates compared to standard backpropagation through time. This both reduces computational complexity and improves learning of temporal dependencies <ref type="bibr" target="#b33">[34]</ref>. The forward and backward passes are set to 100 time steps, as the shortest time series in the C-MAPSS dataset contains 128 time steps.</p><p>In the training procedure, each complete training subset is split into a training set and a cross-validation set. In subset FD001 and FD003, Table <ref type="table" target="#tab_0">3shows</ref> all the hyper-parameters which the GA approach needs to optimize for each subset. The recent and well-proven regularization technique dropout <ref type="bibr" target="#b10">[11]</ref> is applied to the deep architecture. Dropout introduces the hyper-parameter, p, which randomly drops units during training. In this way, dropout approximately combines an exponential number of different architectures. Thus, the deep architecture learns to make generalized representations of the input data, which enhances the feature extraction ability. In Table <ref type="table" target="#tab_0">3</ref>, n and p refer to the number of hidden units and the probability of retaining each hidden unit in the coupled hidden layer L, respectively. A p value of 1.0 is functionally equivalent to zero dropout, i.e. 100% probability of retaining each hidden unit. A typical value for p used in the literature is 0.5 <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18]</ref>. However, p depends on n. In this study, the GA approach is able to test different values of n in both L1, L2, L3, and L4, and hence, it is also able to test different values of p in the range from 0.5 to 0.9. As Patterson and Gibson <ref type="bibr" target="#b34">[35]</ref> recommend, to preserve important features in the input data, dropout is disabled in the first layer, L1. Additionally, dropout is not used in the output layer, L5. It should be noted that dropout is only applied to the non-recurrent connections in the LSTM layers.</p><p>The GA approach is run once for each subset. It trains a diverse number of individuals on the training sets and evaluates the RMSE, Eq. 12, on the cross-validation set as its objective function. In this way, the GA approach optimizes the hyper-parameters for each subset. To limit the time consumed during the optimization process, the population size is restricted to 20 individuals and the population is evolved three times with the selected GA parameters as shown in Table <ref type="table" target="#tab_1">4</ref>. This results in an average training time of 60 hours for each subset. However, the training time will reduce significantly along with future developments in GPUs. Additionally, to prevent overfitting, early stopping (ES) is applied to monitor the performance during the training process of each individual. In the unsupervised pre-training stage, ES is used to monitor the reconstruction error on the training set. If the number of epochs with no improvement exceeds nine, the unsupervised pretraining procedure is terminated. In the fine-tuning procedure, ES is used to monitor the RMSE accuracy on the cross-validation set. If the number of epochs with no improvement exceeds four, the fine-tuning procedure is terminated. Finally, the top five GA individuals for each subset are evaluated on the test sets where both RMSE and S are calculated. A complete flowchart of the GA approach is shown in Fig. <ref type="figure" target="#fig_1">4</ref> and the best GA individuals for each subset are shown in Table <ref type="table" target="#tab_2">5</ref>. In Table <ref type="table" target="#tab_2">5</ref>, nIn and nOut represents the number of input and output (hidden) units for each layer, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Experimental results and discussions</head><p>The aim of this paper is to show increased RUL prediction accuracy in multivariate time series data subjected to several operating conditions and fault modes utilizing a semi-supervised setup. The experiments conducted in this study shows the effect of unsupervised pretraining both when the training data is completely labeled and when the labeled training data in the fine-tuning procedure is reduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1.">The effect of unsupervised pre-training in RUL predictions</head><p>Subset FD004 is chosen for this experiment due to the complexity inherent in its six operating conditions and two fault modes. As shown in Table <ref type="table" target="#tab_3">6</ref>, semi-supervised learning provides higher RUL prediction accuracy compared to supervised learning when the training data is 100% labeled. This indicates that the unsupervised pre-training stage initializes the weights using a more suitable local minimum than In real-life PHM scenarios, high-quality labeled training data is hard to acquire. To address this problem, this study has performed an experiment where only reduced parts of the training data in subset FD004 contains labels. The labels in the training set are randomly reduced into fractions of 20%, 40%, 60%, 80%, and 90%, respectively. To minimize any selection bias, the random selection process is repeated five times for each fraction. Each random selection is then trained on the training set and evaluated on the test set where RMSE and S are calculated. Finally, the top three performance results are averaged as shown in Table <ref type="table" target="#tab_3">6</ref>. It should be noted that a similar experiment, which has made interesting and valuable results using a variational autoencoder (VAE), is conducted on subset FD001 in <ref type="bibr" target="#b35">[36]</ref>.</p><p>To show the effect of unsupervised pre-training, the proposed deep architecture is trained with and without the initial pre-training stage. In the initial pre-training stage, the proposed deep architecture is trained with 100% training features. The ES procedure is used to monitor the performance. As shown in Figs. <ref type="figure" target="#fig_2">5</ref> and<ref type="figure" target="#fig_3">6</ref>, the proposed semi-supervised deep architecture provides the overall highest RUL prediction accuracy when trained with the initial unsupervised pre-training stage. It should be noted that the proposed deep architecture, when trained in a purely supervised manner, also provides satisfactory RUL prediction accuracy, especially when more than 60% of the training labels are included. This proves that recent weight initializations and regularization techniques, such as Xavier and dropout, have indeed reduced the need for unsupervised pre-training. Dropout in particular improves the feature extraction ability by approximately combining several different architectures in the fine-tuning procedure. However, the improvement of utilizing semi-supervised learning is noticeable when more than 40% of the training labels are removed, as shown in Table <ref type="table" target="#tab_3">6</ref>.</p><p>Additionally, as shown in Fig. <ref type="figure" target="#fig_4">7</ref>, the average training time per epoch will almost linearly decrease with decreasing training labels, e.g. 15.2 s training time at 40% labels, which is = 15.2 s/34.14 s 44.5% training time per epoch compared to 100% labels. Also, as seen in Figs. <ref type="figure" target="#fig_2">5</ref> and<ref type="figure" target="#fig_3">6</ref>, the RUL prediction accuracy is satisfactory when more than 60% training labels are included. Depending on the reliability and safety requirements of the application, the trade-off of reduced RUL prediction accuracy might be acceptable if the training time is critical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2.">Comparison with the literature</head><p>Studies that have reported results on all four subsets in the C-MAPSS dataset have been selected for comparison. Although the initial Rc values are somewhat different, the results are still comparable. As shown in Tables <ref type="table" target="#tab_4">7</ref> and<ref type="table" target="#tab_5">8</ref>, the proposed semi-supervised deep architecture has achieved promising results compared to the recent studies when the training data is completely labeled. The CNN approach in Li et al. <ref type="bibr" target="#b6">[7]</ref> achieved slightly higher RMSE prediction accuracy on subset FD002. However, the proposed semi-supervised deep architecture indicates substantially improved S prediction accuracy on all subsets. Consequently, the proposed semi-supervised deep architecture reduces the  average number of late predictions across the test sets considerably. This is because the unsupervised pre-training stage extracts more degradation related features before supervised fine-tuning. Thus, this stage supports the whole architecture to better understand the underlying degradation trends. Late predictions impose a serious threat to reliability and safety in real-life PHM applications as the maintenance procedure will be scheduled too late. Therefore, semi-supervised learning is a promising approach in RUL predictions tasks both subjected to a single and multiple operating conditions and fault modes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and future work</head><p>This paper has investigated the effect of unsupervised pre-training in RUL predictions utilizing a semi-supervised setup. The experiments are performed on the publicly available C-MAPSS dataset. Additionally, a GA approach was proposed to tune the number of diverse hyperparameters in deep architectures. Combining all the hyper-parameters in Table <ref type="table" target="#tab_0">3</ref> results in a total of 8 748 000 combinations. Although, the GA approach only used 20 individuals and three evolutions, it was able to optimize hyper-parameters for each subset in the C-MAPSS dataset effectively. This is a promising approach compared to using a time consuming, exhaustive search. However, the average training time of 60 hours for each subset will be further optimized in future work.</p><p>In the experimental study, the proposed semi-supervised setup is compared to purely supervised training as well as recent studies in the literature. The proposed semi-supervised setup achieved promising RUL prediction accuracy with both completely and reduced amounts of labeled training data. Hence, unsupervised pre-training is indeed a promising feature in real-life PHM applications subjected to multiple operating conditions and fault modes, as large amounts of high-quality labeled training data might be both challenging and time-consuming to acquire. Unsupervised pre-training supports the deep architecture to improve our understanding of the inherent complexity by extracting more features that contain important degradation information.</p><p>In this study, an RBM was utilized as the initial unsupervised pretraining stage. However, RBM is a rather old, unsupervised DL technique. Today, more powerful unsupervised DL techniques are available. For instance, the VAE <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref> seems promising. The VAE models the underlying probability distribution of the training data using variational inference. It is possible to extend to a wide range of model architectures, and this is one of its key advantages compared to RBM, which requires careful model design to maintain tractability <ref type="bibr" target="#b37">[38]</ref>.</p><p>In RUL predictions based on data-driven approaches, such as DL, the accuracy strongly depends on the quality of the constructed run-tofailure training data labels. This study confirms that R c has a notable impact on the RUL prediction accuracy for each subset. Nevertheless, the piece-wise linear degradation model used in this study is considered a major limitation as each engine in each subset has, in fact, an individual degradation pattern. Recently, the VAE has been used for unsupervised reconstruction based anomaly detection by applying a reconstruction error as an anomaly score <ref type="bibr" target="#b38">[39]</ref>. Thus, in future work, the VAE will also be used in order to create an unsupervised fault detector to optimize R c for each engine in each subset in the C-MAPSS dataset.</p><p>Normally, tanh is used as the input and output (I/O) activation function in LSTMs. However, in this study it was discovered that sigmoid performed better than tanh as the LSTM I/O activation function in combination with the initial RBM layer with ReLU as the activation function. A novel rectified LSTM I/O activation function would be a positive contribution to be included in future work. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The proposed semi-supervised deep architecture structure.</figDesc><graphic coords="4,139.47,57.01,316.39,335.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Flowchart of the GA approach.</figDesc><graphic coords="6,243.78,588.89,172.34,106.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. RMSE comparison on subset FD004 when the labeled training data is reduced from 100% to 10%.</figDesc><graphic coords="8,39.69,57.00,515.93,333.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. S comparison on subset FD004 when the labeled training data is reduced from 100% to 10%.</figDesc><graphic coords="9,49.78,56.98,495.77,315.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Average training time in seconds per epoch in the fine-tuning procedure when the labeled training data is reduced from 100% to 10%.</figDesc><graphic coords="10,46.26,57.01,502.77,335.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="3,110.95,57.03,373.37,206.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="5,122.68,475.44,349.92,235.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 3</head><label>3</label><figDesc>Genes in the GA approach.</figDesc><table><row><cell>Dataset</cell><cell>FD001</cell><cell>FD002</cell><cell>FD003</cell><cell>FD004</cell><cell>Gene</cell><cell>Hyper-parameter</cell><cell>Values</cell></row><row><cell>Time series training set</cell><cell>100</cell><cell>260</cell><cell>100</cell><cell>249</cell><cell>1</cell><cell>R c</cell><cell>115, 120, 125, 130, 135, 140</cell></row><row><cell>Time series test set</cell><cell>100</cell><cell>259</cell><cell>100</cell><cell>248</cell><cell>2</cell><cell>Learning rate RBM layer</cell><cell>10 , 1 10 , 2 10 3</cell></row><row><cell>Operating conditions Fault conditions</cell><cell>1 1</cell><cell>6 1</cell><cell>1 2</cell><cell>6 2</cell><cell>3 4 5 6 7 8 9 10</cell><cell>Learning rate remaining layers L2 Regularization miniBatch n L1 n L2 n L3 n L4 p L2</cell><cell>2 10 , 3 10 4 10 , 4 10 , 5 10 6 5, 10 32, 64, 128 32, 64, 128 32, 64, 128 8, 16 10 , 0.5, 0.6, 0.7, 0.8, 0.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>11</cell><cell>p L3</cell><cell>0.5, 0.6, 0.7, 0.8, 0.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>12</cell><cell>p L4</cell><cell>0.5, 0.6, 0.7, 0.8, 0.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>13</cell><cell>I/O activation function LSTM</cell><cell>sigmoid, tanh</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>14</cell><cell>Activation function FNN</cell><cell>sigmoid, tanh</cell></row></table><note><p>Fig. 3. Simple illustration of the scoring function vs. RMSE, where = d RUL RUL i predicted true .</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4</head><label>4</label><figDesc>Parameters of the GA approach.</figDesc><table><row><cell>Parameter</cell><cell>Value</cell></row><row><cell>Population size</cell><cell>20</cell></row><row><cell>Nr of elite</cell><cell>3</cell></row><row><cell>Mutation rate</cell><cell>0.5</cell></row><row><cell>Mutation gain</cell><cell>0.3</cell></row><row><cell>Evolution iterations</cell><cell>3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5</head><label>5</label><figDesc>GA individuals.</figDesc><table><row><cell>FD001</cell><cell>Layer index</cell><cell>DL technique</cell><cell>nIn</cell><cell>nOut</cell><cell>Dropout</cell><cell>Activation function</cell></row><row><cell></cell><cell>1</cell><cell>RBM</cell><cell>14</cell><cell>64</cell><cell>1.0</cell><cell>ReLU</cell></row><row><cell></cell><cell>2</cell><cell>LSTM</cell><cell>64</cell><cell>64</cell><cell>0.9</cell><cell>Sigmoid</cell></row><row><cell></cell><cell>3</cell><cell>LSTM</cell><cell>64</cell><cell>64</cell><cell>0.6</cell><cell>Sigmoid</cell></row><row><cell></cell><cell>4</cell><cell>FNN</cell><cell>64</cell><cell>8</cell><cell>0.6</cell><cell>Sigmoid</cell></row><row><cell></cell><cell>5</cell><cell>Output</cell><cell>8</cell><cell>1</cell><cell>1.0</cell><cell>Identity</cell></row><row><cell></cell><cell>R c</cell><cell>Learning rate RBM layer</cell><cell>Learning rate remaining layers</cell><cell>L2 regularization</cell><cell>mini batch size</cell><cell>RMSE cross-validation set</cell></row><row><cell></cell><cell>115</cell><cell>10 2</cell><cell>10 3</cell><cell>10 6</cell><cell>5</cell><cell>8.49</cell></row><row><cell></cell><cell>Layer index</cell><cell>DL technique</cell><cell>nIn</cell><cell>nOut</cell><cell>Dropout</cell><cell>Activation function</cell></row><row><cell></cell><cell>1</cell><cell>RBM</cell><cell>24</cell><cell>64</cell><cell>1.0</cell><cell>ReLU</cell></row><row><cell></cell><cell>2</cell><cell>LSTM</cell><cell>64</cell><cell>128</cell><cell>0.7</cell><cell>Sigmoid</cell></row><row><cell></cell><cell>3</cell><cell>LSTM</cell><cell>128</cell><cell>32</cell><cell>0.8</cell><cell>Sigmoid</cell></row><row><cell></cell><cell>4</cell><cell>FNN</cell><cell>32</cell><cell>8</cell><cell>0.6</cell><cell>Sigmoid</cell></row><row><cell></cell><cell>5</cell><cell>Output</cell><cell>8</cell><cell>1</cell><cell>1.0</cell><cell>Identity</cell></row><row><cell></cell><cell>R c</cell><cell>Learning rate RBM layer</cell><cell>Learning rate remaining layers</cell><cell>L2 regularization</cell><cell>mini batch size</cell><cell>RMSE cross-validation set</cell></row><row><cell></cell><cell>135</cell><cell>10 2</cell><cell>10 3</cell><cell>10 5</cell><cell>10</cell><cell>9.60</cell></row><row><cell>FD003</cell><cell>Layer index</cell><cell>DL technique</cell><cell>nIn</cell><cell>nOut</cell><cell>Dropout</cell><cell>Activation function</cell></row><row><cell></cell><cell>1</cell><cell>RBM</cell><cell>14</cell><cell>32</cell><cell>1.0</cell><cell>ReLU</cell></row><row><cell></cell><cell>2</cell><cell>LSTM</cell><cell>32</cell><cell>128</cell><cell>0.9</cell><cell>Sigmoid</cell></row><row><cell></cell><cell>3 4 5 R c 125</cell><cell>LSTM FNN Output Learning rate RBM layer 10 2</cell><cell>128 64 8 Learning rate remaining layers 10 3</cell><cell>64 8 1 L2 regularization 10 6</cell><cell>0.9 0.9 1.0 mini batch size 5</cell><cell>Sigmoid Sigmoid Identity RMSE cross-validation set 8.59</cell></row><row><cell>FD004</cell><cell>Layer index</cell><cell>DL technique</cell><cell>nIn</cell><cell>nOut</cell><cell>Dropout</cell><cell>Activation function</cell></row><row><cell></cell><cell>1</cell><cell>RBM</cell><cell>24</cell><cell>64</cell><cell>1.0</cell><cell>ReLU</cell></row><row><cell></cell><cell>2</cell><cell>LSTM</cell><cell>64</cell><cell>128</cell><cell>0.8</cell><cell>Sigmoid</cell></row><row><cell></cell><cell>3</cell><cell>LSTM</cell><cell>128</cell><cell>32</cell><cell>0.7</cell><cell>Sigmoid</cell></row><row><cell></cell><cell>4</cell><cell>FNN</cell><cell>32</cell><cell>8</cell><cell>0.6</cell><cell>Sigmoid</cell></row><row><cell></cell><cell>5</cell><cell>Output</cell><cell>8</cell><cell>1</cell><cell>1.0</cell><cell>Identity</cell></row><row><cell></cell><cell>R c</cell><cell>Learning rate RBM layer</cell><cell>Learning rate remaining layers</cell><cell>L2 regularization</cell><cell>mini batch size</cell><cell>RMSE cross-validation set</cell></row><row><cell></cell><cell>135</cell><cell>10 2</cell><cell>10 3</cell><cell>10 5</cell><cell>10</cell><cell>10.45</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6</head><label>6</label><figDesc>The proposed semi-supervised deep architecture with and without unsupervised pre-training on subset FD004 when the labeled training data is reduced from 100% to 10%. Improvement = (1 )</figDesc><table><row><cell>Semi supervised Supervised</cell><cell>.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RMSE</cell><cell></cell><cell>100%</cell><cell>80%</cell><cell>60%</cell><cell>40%</cell><cell>20%</cell><cell>10%</cell></row><row><cell cols="2">Semi-supervised with 100% training features in the pre-training stage</cell><cell>22.66</cell><cell>23.04</cell><cell>24.07</cell><cell>25.46</cell><cell>30.26</cell><cell>34.19</cell></row><row><cell>Supervised</cell><cell></cell><cell>23.62</cell><cell>23.45</cell><cell>24.14</cell><cell>26.40</cell><cell>30.27</cell><cell>34.90</cell></row><row><cell>Improvement</cell><cell></cell><cell>4.06%</cell><cell>1.75%</cell><cell>0.29%</cell><cell>3.56%</cell><cell>0.03%</cell><cell>2.03%</cell></row><row><cell>S</cell><cell></cell><cell>100%</cell><cell>80%</cell><cell>60%</cell><cell>40%</cell><cell>20%</cell><cell>10%</cell></row><row><cell cols="2">Semi-supervised with 100% training features in the pre-training stage</cell><cell>2840</cell><cell>3175</cell><cell>3576</cell><cell>5522</cell><cell>9562</cell><cell>22,476</cell></row><row><cell>Supervised</cell><cell></cell><cell>3234</cell><cell>3427</cell><cell>3650</cell><cell>6536</cell><cell>15,612</cell><cell>27,138</cell></row><row><cell>Improvement</cell><cell></cell><cell>12.18%</cell><cell>7.35%</cell><cell>2.03%</cell><cell>15.51%</cell><cell>38.75%</cell><cell>17.18%</cell></row><row><cell>Average training time per epoch (s)</cell><cell></cell><cell>100%</cell><cell>80%</cell><cell>60%</cell><cell>40%</cell><cell>20%</cell><cell>10%</cell></row><row><cell>Pre-training stage</cell><cell></cell><cell>7.08</cell><cell>7.08</cell><cell>7.08</cell><cell>7.08</cell><cell>7.08</cell><cell>7.08</cell></row><row><cell>Fine-tuning procedure</cell><cell></cell><cell>34.14</cell><cell>28.97</cell><cell>22.39</cell><cell>15.2</cell><cell>9.74</cell><cell>5.93</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7</head><label>7</label><figDesc>RMSE comparison with the literature on the C-MAPSS dataset.</figDesc><table><row><cell>DL approach &amp; refs.</cell><cell>FD001</cell><cell>FD002</cell><cell>FD003</cell><cell>FD004</cell></row><row><cell>CNN + FNN [5]</cell><cell>18.45</cell><cell>30.29</cell><cell>19.82</cell><cell>29.16</cell></row><row><cell>LSTM + FNN [6]</cell><cell>16.14</cell><cell>24.49</cell><cell>16.18</cell><cell>28.17</cell></row><row><cell>MODBNE [19]</cell><cell>15.04</cell><cell>25.05</cell><cell>12.51</cell><cell>28.66</cell></row><row><cell>CNN + FNN [7]</cell><cell>12.61</cell><cell>22.36</cell><cell>12.64</cell><cell>23.31</cell></row><row><cell>Proposed semi-supervised setup</cell><cell>12.56</cell><cell>22.73</cell><cell>12.10</cell><cell>22.66</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 8</head><label>8</label><figDesc>Score function comparison with the literature on the C-MAPSS dataset.</figDesc><table><row><cell>DL approach &amp; Refs.</cell><cell>FD001</cell><cell>FD002</cell><cell>FD003</cell><cell>FD004</cell></row><row><cell>CNN + FNN [5]</cell><cell>1287</cell><cell>13,570</cell><cell>1596</cell><cell>7886</cell></row><row><cell>LSTM + FNN [6]</cell><cell>338</cell><cell>4450</cell><cell>852</cell><cell>5550</cell></row><row><cell>MODBNE [19]</cell><cell>334</cell><cell>5585</cell><cell>422</cell><cell>6558</cell></row><row><cell>CNN + FNN [7]</cell><cell>274</cell><cell>10,412</cell><cell>284</cell><cell>12,466</cell></row><row><cell>Proposed semi-supervised setup</cell><cell>231</cell><cell>3366</cell><cell>251</cell><cell>2840</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work was supported by the Norwegian University of Science and Technology within the Department of Ocean Operations and Civil Engineering under project no. 90329106 and funded by the Research Council of Norway, grant no. 245613/O30. The authors would like to thank Digital Twins For Vessel Life Cycle Service (DigiTwin) NFR 280703.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary material</head><p>Supplementary material associated with this article can be found, in the online version, at 10.1016/j.ress.2018.11.027</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Defining phm, a lexical evolution of maintenance and logistics</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Kalgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Byington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Roemer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Watson</surname></persName>
		</author>
		<idno type="DOI">10.1109/AUTEST.2006.283685</idno>
		<ptr target="https://doi.org/10.1109/AUTEST.2006.283685" />
	</analytic>
	<monogr>
		<title level="j">IEEE Autotestcon</title>
		<imprint>
			<biblScope unit="page" from="353" to="358" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Switching state-space degradation model with recursive filter/smoother for prognostics of remaining useful life</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zi</surname></persName>
		</author>
		<idno type="DOI">10.1109/TII.2018.2810284</idno>
		<ptr target="https://doi.org/10.1109/TII.2018.2810284" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Ind Inf</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Research advances in fault diagnosis and prognostic based on deep learning. Prognostics and System Health Management Conference</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/PHM.2016.7819786</idno>
		<ptr target="https://doi.org/10.1109/PHM.2016.7819786" />
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Big data deep learning: challenges and perspectives</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2014.2325029</idno>
		<ptr target="https://doi.org/10.1109/ACCESS.2014.2325029" />
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="514" to="525" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deep convolutional neural network based regression approach for estimation of remaining useful life</title>
		<author>
			<persName><forename type="first">Sateesh</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X-L</forename></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-32025-0_14</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-32025-0_14" />
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="214" to="228" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory network for remaining useful life estimation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ristovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farahat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Prognostics and Health Management (ICPHM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="88" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Remaining useful life estimation in prognostics using deep convolution neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J-Q</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reliab Eng Syst Saf</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The difficulty of training deep architectures and the effect of unsupervised pre-training</title>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P-A</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="page" from="153" to="160" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Gordon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Dunson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Dudãk</surname></persName>
		</editor>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics<address><addrLine>Fort Lauderdale, FL, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
	<note>15 of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>arXiv:141269802014</idno>
		<title level="m">Adam: a method for stochastic optimization</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Mach Learn Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the 13th International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Turbofan engine degradation simulation data set</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Goebel</surname></persName>
		</author>
		<ptr target="https://tiarcnasagov/tech/dash/groups/pcoe/prognostic-data-repository/" />
	</analytic>
	<monogr>
		<title level="m">NASA Ames Prognostics Data Repository</title>
		<meeting><address><addrLine>Moffett Field, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
		<respStmt>
			<orgName>NASA Ames Research Center</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
		<ptr target="https://doi.org/10.1162/neco.1997.9.8.1735" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<idno type="DOI">10.1109/72.279181</idno>
		<ptr target="https://doi.org/10.1109/72.279181" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Netw</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fault diagnosis and remaining useful life estimation of aero engine using lstm neural network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Aircraft Utility Systems (AUS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="135" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Remaining useful life estimation of engineered systems using vanilla lstm neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">275</biblScope>
			<biblScope unit="page" from="167" to="179" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multiobjective deep belief networks ensemble for remaining useful life estimation in prognostics</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Tan</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2016.2582798</idno>
		<ptr target="https://doi.org/10.1109/TNNLS.2016.2582798" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Netw Learn Syst</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2306" to="2318" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Cjc</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deepface: closing the gap to human-level performance in face verification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2014.220</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2014.220" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y-W</forename><surname>Teh</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.2006.18.7.1527</idno>
		<ptr target="https://doi.org/10.1162/neco.2006.18.7.1527" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised learning of distributions on binary vectors using two layer networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Haussler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Moody</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hanson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Lippmann</surname></persName>
		</editor>
		<imprint>
			<publisher>Morgan-Kaufmann</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted Boltzmann machines</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning (ICML-10)</title>
		<meeting>the 27th international conference on machine learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A practical guide to training restricted Boltzmann machines</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks: Tricks of the Trade</title>
		<imprint>
			<biblScope unit="page" from="599" to="619" />
			<date type="published" when="2012">2012</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to forget: continual prediction with lstm</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Cummins</surname></persName>
		</author>
		<idno type="DOI">10.1162/089976600300015015</idno>
		<ptr target="https://doi.org/10.1162/089976600300015015" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2451" to="2471" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Lstm: a search space odyssey</title>
		<author>
			<persName><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Koutník</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Steunebrink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2016.2582924</idno>
		<ptr target="https://doi.org/10.1109/TNNLS.2016.2582924" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Netw Learn Syst</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2222" to="2232" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Understanding lstm networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<ptr target="http://colahgithubio/posts/2015-08-Understanding-LSTMs/img/LSTM3-chainpng" />
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Comparison of parallel genetic algorithm and particle swarm optimization for real-time uav path planning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Roberge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tarbouchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Labonté</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Ind Inf</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="132" to="141" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Eclipse deeplearning4j development team, deeplearning4j: open-source distributed deep learning for the jvm</title>
	</analytic>
	<monogr>
		<title level="j">Apache Software Foundation License</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Damage propagation modeling for aircraft engine run-to-failure simulation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Goebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Eklund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Prognostics and Health Management</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Prognostic modelling options for remaining useful life estimation by industry</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Sikorska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hodkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ymssp.2010.11.018</idno>
		<ptr target="https://doi.org/10.1016/j.ymssp.2010.11.018" />
	</analytic>
	<monogr>
		<title level="j">Mech Syst Signal Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1803" to="1836" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for remaining useful life estimation</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">O</forename><surname>Heimes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Prognostics and Health Management, PHM 2008</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Training recurrent neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<pubPlace>Toronto, Ont, Canada</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Deep learning: a practitioner&apos;s approach</title>
		<author>
			<persName><forename type="first">J</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gibson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Reilly Media, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Semi-supervised learning with deep generative models for asset failure prediction</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<idno>2017. abs/1709.00845</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>arXiv:131261142013</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="http://www.deeplearningbook.org" />
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A multimodal anomaly detector for robot-assisted feeding using an lstm-based variational autoencoder</title>
		<author>
			<persName><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hoshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Kemp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Rob Autom Lett</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1544" to="1551" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
