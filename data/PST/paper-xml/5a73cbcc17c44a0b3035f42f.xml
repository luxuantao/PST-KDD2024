<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Finding Faster Configurations using FLASH</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Vivek</forename><surname>Nair</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">North Carolina State University</orgName>
								<address>
									<settlement>Raleigh</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhe</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">North Carolina State University</orgName>
								<address>
									<settlement>Raleigh</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tim</forename><surname>Menzies</surname></persName>
							<email>tim.menzies@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">North Carolina State University</orgName>
								<address>
									<settlement>Raleigh</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Norbert</forename><surname>Siegmund</surname></persName>
							<email>norbert.siegmund@uni-weimar.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">North Carolina State University</orgName>
								<address>
									<settlement>Raleigh</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Bauhaus-University Weimar</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sven</forename><surname>Apel</surname></persName>
							<email>apel@uni-passau.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">North Carolina State University</orgName>
								<address>
									<settlement>Raleigh</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Informatics and Mathematics</orgName>
								<orgName type="institution">University of Passau</orgName>
								<address>
									<addrLine>Innstr. 33</addrLine>
									<postCode>94032</postCode>
									<settlement>Passau</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Finding Faster Configurations using FLASH</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6A7932B6D4E3AC99D0CDF7531F9BF269</idno>
					<idno type="DOI">10.1109/TSE.2018.2870895</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TSE.2018.2870895, IEEE Transactions on Software Engineering IEEE TRANS SE. SUBMITTED NOV&apos;17 1 This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TSE.2018.2870895, IEEE Transactions on Software Engineering IEEE TRANS SE. SUBMITTED NOV&apos;17 2</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Performance prediction</term>
					<term>Search-based SE</term>
					<term>Configuration</term>
					<term>Multi-objective optimization</term>
					<term>Sequential Model-based Methods</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Finding good configurations of a software system is often challenging since the number of configuration options can be large. Software engineers often make poor choices about configuration or, even worse, they usually use a sub-optimal configuration in production, which leads to inadequate performance. To assist engineers in finding the better configuration, this article introduces FLASH, a sequential model-based method that sequentially explores the configuration space by reflecting on the configurations evaluated so far to determine the next best configuration to explore. FLASH scales up to software systems that defeat the prior state-of-the-art model-based methods in this area. FLASH runs much faster than existing methods and can solve both single-objective and multi-objective optimization problems. The central insight of this article is to use the prior knowledge of the configuration space (gained from prior runs) to choose the next promising configuration. This strategy reduces the effort (i.e., number of measurements) required to find the better configuration. We evaluate FLASH using 30 scenarios based on 7 software systems to demonstrate that FLASH saves effort in 100% and 80% of cases in single-objective and multi-objective problems respectively by up to several orders of magnitude compared to state-of-the-art techniques.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Most software systems available today are configurable; that is, they can be easily adjusted to achieve a wide range of functional and non-functional (e.g., energy or performance) properties. Once a configuration space becomes large, it becomes difficult for humans to keep track of the interactions between the configuration options. Section 2 of this article offers more details on many of the problems seen with software configuration. In summary:</p><p>• Many software systems have poorly chosen defaults <ref type="bibr" target="#b1">[1]</ref>, <ref type="bibr" target="#b2">[2]</ref>. Hence, it is useful to seek better configurations. • Understanding the configuration space of software systems with large configuration spaces is challenging <ref type="bibr" target="#b3">[3]</ref>. • Exploring more than just a handful of configurations is usually infeasible due to long benchmarking time <ref type="bibr" target="#b4">[4]</ref>. This article describes FLASH, a novel way to find better configurations for a software system (for a given workload). FLASH is a sequential model-based method (SMBO) <ref type="bibr" target="#b5">[5]</ref>, <ref type="bibr" target="#b6">[6]</ref>, <ref type="bibr" target="#b7">[7]</ref> that reflects on the evidence (configurations) retrieved at some point to select the estimated best configuration to measure next. This way, FLASH uses fewer evaluations to find better configurations compared to more expensive prior work <ref type="bibr" target="#b8">[8]</ref>, <ref type="bibr" target="#b9">[9]</ref>, <ref type="bibr" target="#b10">[10]</ref>, <ref type="bibr" target="#b11">[11]</ref>, <ref type="bibr" target="#b12">[12]</ref>.</p><p>Prior work in this area primarily used two strategies. Firstly, researchers used machine learning to model the configuration space. The model is built sequentially, where new configurations are sampled randomly, and the quality or accuracy of the model is measured using a holdout set. The size of the holdout set in some cases could be up to 20% of the configuration space <ref type="bibr" target="#b11">[11]</ref> and needs to be evaluated (i.e., measured) before even the model is fully built. This strategy makes these methods not suitable in a practical setting since Manuscript received November XX, 2017.</p><p>the generated holdout set can be (very) expensive. Secondly, the sequential model-based techniques used in prior work relied on Gaussian Process Models (GPM) to reflect on the configurations explored (or evaluated) so far <ref type="bibr" target="#b13">[13]</ref>. However, GPMs do not scale well for software systems with more than a dozen configuration options <ref type="bibr" target="#b14">[14]</ref>.</p><p>The key idea of FLASH is to build a performance model that is just accurate enough for differentiating better configurations from the rest of the configuration space. Tolerating the inaccuracy of the model is useful to reduce the cost (measured in terms of the number of configurations evaluated) and the time required to find the better configuration. To increase the scalability of methods using GPM, FLASH replaces the GPMs with a fast and scalable decision tree learner. The novel contributions of the article are:</p><p>• We show that FLASH can solve single-objective performance configuration optimization problems using an order of magnitude fewer measurements than the state-ofthe-art (Section 7.1). This is a critical feature because, as discussed in Section 2, it can be very slow to sample multiple properties of modern software systems, when each such sample requires (say) to compile and benchmark the corresponding software system. • We empower FLASH to multi-objective performance configuration optimization problems. • We show that FLASH overcomes the shortcomings of prior work and achieves similar performance and scalability, with greatly reduced runtimes (Section 7.2). • Background material, a replication package, all measurement data, and the open-source version of FLASH are available at supplementary website (http://tiny.cc/ flashrepo/). The rest of the article is structured as follows: Section 2 motivates this work. Section 3 describes the problem formulation and the theory behind SMBO. Section 4 describes prior work in software performance configuration optimization, followed by the core algorithm of FLASH in Section 5. In Section 6, we present our research questions along with experimental settings used to answer them. Prior work in this area addresses a single-objective problem with the only 3 Not specified</p><p>The design space consists of 206 different hardware implementations of a sorting network for 256 inputs SS-B1 206 <ref type="bibr" target="#b13">[13]</ref> Throughput SS-B2  The abbreviations of the systems (Abbr) are sorted in the order of the number of configuration options of the system. The column #Config Options represent the number of configuration options of the software system and #Configurations represents the total number of configurations of the system. See http://tiny.cc/flash systems/ for more details.</p><p>exception of ePAL <ref type="bibr" target="#b13">[13]</ref>. Hence, we evaluate FLASH separately for single-objective and multi-objective performance configuration optimization problems. In Section 7, we apply FLASH on single-objective performance configuration optimization and multi-objective performance configuration optimization. The article ends with a discussion on various aspects of FLASH, and finally, we conclude along with a discussion of future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PERFORMANCE CONFIGURATION OPTIMIZATION FOR SOFTWARE</head><p>This section motivates our research by reviewing the numerous problems associated with software configuration. Many researchers report that modern software systems come with a daunting number of configuration options. For example, the number of configuration options in Apache (a popular web server) increased from 150 to more than 550 configuration options within 16 years <ref type="bibr" target="#b3">[3]</ref>. Van Aken et al. <ref type="bibr" target="#b1">[1]</ref> also reports a similar trend. They indicate that, in over 15 years, the number of configuration options of POSTGRES and MYSQL increased by a factor of three and six, respectively. This is troubling since Xu et al. <ref type="bibr" target="#b3">[3]</ref> report that developers tend to ignore over 80% of configuration options, which leaves considerable optimization potential untapped and induces major economic cost <ref type="bibr" target="#b3">[3]</ref>. 1 For illustration, Figure <ref type="figure" target="#fig_0">1</ref> offer examples of the kinds of configuration options seen in software systems.</p><p>Another problem with configurable systems is the issue of poorly chosen default configurations. Often, it is assumed that software architects provide useful default configurations of their systems. This assumption can be very misleading. Van Aken et al. report that the default MySQL configurations in 2016 assume that it will be installed on a machine that has 160MB of RAM (which, at that time, was incorrect by, at least, an order of magnitude) <ref type="bibr" target="#b1">[1]</ref>. Herodotou et al. <ref type="bibr" target="#b2">[2]</ref> show how standard settings for text mining applications in Hadoop result in worst-case execution times. In the same vein, Jamshidi et al. <ref type="bibr" target="#b15">[15]</ref> reports for text mining applications on Apache Storm, the throughput achieved using the worst configuration is 480 times slower than the throughput achieved by the best configuration.</p><p>Yet another problem is that exploring benchmark sets for different configurations is very slow. Wang et al. <ref type="bibr" target="#b17">[17]</ref> comments on the problems of evolving a test suite for software if every candidate solution requires a time-consuming execution of the entire system: such test suite generation can take weeks of execution time. Zuluaga et al. <ref type="bibr" target="#b18">[18]</ref> report on the cost of analysis for software/hardware co-design: "synthesis of only one design can take hours or even days". The challenges of having numerous configuration options are just not limited to software systems. The problem to find an good set of configuration options is pervasive and faced in numerous other sub-domains of computer science and beyond. In software engineering, software product lines-where the objective is to find a product which (say) reduces cost and defects <ref type="bibr" target="#b19">[19]</ref>, <ref type="bibr" target="#b20">[20]</ref>-have been widely studied. The problem of configuration optimization is present in domains, such as machine learning, cloud computing, and software security.</p><p>The area of hyper-parameter optimization (a.k.a. parameter tuning) is very similar to the performance configuration optimization problem studied in this article. Instead of optimizing the performance of a software system, the hyperparameter method tries to optimize the performance of a machine learner. Hyper-parameter optimization is an active area of research in various flavors of machine learning. For example, Bergstra and Bengiol <ref type="bibr" target="#b21">[21]</ref> showed how random search could be used for hyper-parameter optimization of high dimensional spaces. Recently, there has been much interest in hyper-parameter optimization applied to the area of software analytics <ref type="bibr" target="#b22">[22]</ref>, <ref type="bibr" target="#b23">[23]</ref>, <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b25">[25]</ref>, <ref type="bibr" target="#b26">[26]</ref>.</p><p>Another area of application for performance configuration optimization is cloud computing. With the advent of big data, long-running analytics jobs are commonplace. Since different analytic jobs have diverse behaviors and resource requirements, choosing the correct virtual machine type in a cloud environment has become critical. This problem has received considerable interest, and we argue, this is another useful application of performance configuration optimization -that is, optimize the performance of a system while minimizing cost <ref type="bibr" target="#b5">[5]</ref>, <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b28">[28]</ref>, <ref type="bibr" target="#b29">[29]</ref>, <ref type="bibr" target="#b30">[30]</ref>.</p><p>As a sideeffect of the wide-spread adoption of cloud computing, the security of the instances or virtual machines (VMs) has become a daunting task. In particular, optimized security settings are not identical in every setup. They depend on characteristics of the setup, on the ways an application is used or on other applications running on the same system. The problem of finding security setting for a VM is similar to performance configuration optimization <ref type="bibr" target="#b31">[31]</ref>, <ref type="bibr" target="#b32">[32]</ref>, <ref type="bibr" target="#b33">[33]</ref>, <ref type="bibr" target="#b34">[34]</ref>, <ref type="bibr">[35]</ref>. Among numerous other problems which are similar to performance configuration optimization, the problem of how to maximize conversions on landing pages or click-through rates on search-engine result pages <ref type="bibr" target="#b4">[4]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref> has gathered interest.</p><p>The rest of this article discusses how FLASH addresses configuration problems (using the case studies of Figure <ref type="figure" target="#fig_0">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THEORY</head><p>The following theoretical notes define the framework used throughout the article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">What are Configurable Software Systems?</head><p>A configurable software system has a set X of configurations x ∈ X. Let x i represent the ith configuration of a software system. x i,j represent the jth configuration option of the configuration x i . In general, x i,j indicates either an (i) integer variable or a (ii) Boolean variable. The configuration space (X) represents all the valid configurations of a software system. The configurations are also referred to as independent variables. Each configuration (x i ), where 1 ≤ i ≤ |X|, has one (single-objective) or many (multiobjective) corresponding performance measures y i,k ∈ Y , where y i,k indicates the 1 ≤ kth ≤ m objective associated with a configuration x i . The performance measure is also referred to as dependent variable. For multi-objective problems, there are multiple dependent variables. We denote the performance measures (y ∈ Y ) associated with a given configuration by (y i,1 , ..y i,m ) = f (x i ), in multi-objective setting y i is a vector, where: f : X → Y is a function which maps X ∈ R n to Y ∈ R m . In a practical setting, whenever we try to obtain the performance measure corresponding to a certain configuration, it requires actually executing a benchmark run with that configuration. In our setting, evaluation of a configuration (or using f ) is expensive and is referred to as a measurement. The cost or measurement is defined as the number of times f is used to map a configuration x i ∈ X to Y . In our setting, the cost of an optimization technique is the total number of measurements required to find the better solution.</p><p>In the following, we will explore two different kinds of configuration optimization: single-objective and multiple objective. In single-objective performance configuration optimization, we consider the problem of finding a good configuration (x * ) such that f (x * ) is less than other configurations in X. Our objective is to find x * while minimizing the number of measurements.</p><formula xml:id="formula_0">f (x * ) ≤ f (x), ∀x ∈ X \ x *<label>(1)</label></formula><p>That is, our goal is to find the better configuration of a system with least cost or measurements as possible when compared to prior work.</p><p>In multi-objective performance configuration optimization, we consider the problem of finding a configuration (x * ) that is better than other configurations in the configuration space of X while minimizing the number of measurements. Unlike, the single-objective configuration optimization problem, where one solution can be the best (optimal) solution (except multiple configurations have the same performance measure), in multi-objective configuration optimization there may be no best solution (best in all objectives). Rather there may be a set of solutions that are equivalent to each other. Hence, to declare that one solution is better than another, all objectives must be polled separately. Given two vectors of configurations x 1 , x 2 with associated objectives y 1 , y 2 , then x 1 is binary dominant ( ) over x 2 when: y 1,p ≤ y 2,p ∀p ∈ {1, 2, ..., m} and y 1,q &lt; y 2,q for at least one index q ∈ {1, 2, ..., m}</p><p>where y ∈ Y are the performance measures. We refer to binary dominant configurations as better configurations. For the multi-objective configuration optimization problem, our goal is to find a set of better configurations of a given software system using fewer measurements compared to prior work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sequential Model-based Optimization</head><p>Sequential Model-based Optimization (SMBO) is a useful strategy to find extremes of an unknown objective (or per- formance) function which is expensive (both in terms of cost and time) to evaluate. In literature, a certain variant of SMBO is also called Bayesian optimization. SMBO is efficient because of its ability to incorporate prior belief as already measured solutions (or configurations), to help direct further sampling. Here, the prior represents the already known areas of the search (or performance optimization) problem. The prior can be used to estimate the rest of the points (or unevaluated configurations). Once we have evaluated one (or many) points based on the prior, we can define the posterior. The posterior captures our updated belief in the objective function. This step is performed by using a machine learning model, also called surrogate model. The concept of SMBO is simple stated:</p><p>• Given what we know about the problem... This can also be explained as follows. Firstly, few points (or configurations) are (say) randomly selected and measured. These points along with their performance measurements are used to build a model (prior). Secondly, this model is then used to estimate or predict the performance measurements of other unevaluated points (or configurations).This can be used by an acquisition function to select the configurations to measure next. This process continues till a predefined stopping criterion (budget) is reached.</p><p>Much of the prior research in configuration optimization of software systems can be characterized as an exploration of different acquisition functions. These acquisition function (or sampling heuristics) were used to satisfy two requirements: (i) use a 'reasonable' number of configurations (along with corresponding measurements) and (ii) the selected configurations should incorporate the relevant interactions-how different configuration options influence the performance measure <ref type="bibr" target="#b37">[38]</ref>. In a nutshell, the intuition behind such functions is that it is not necessary to try all configuration options-for pragmatic reasons. Rather, it is only necessary to try a small representative configurationswhich incorporates the influences of various configuration options. Randomized functions select random items <ref type="bibr" target="#b8">[8]</ref>, <ref type="bibr" target="#b9">[9]</ref>. Other, more intricate, acquisition functions first cluster the data then sample only a subset of examples within each cluster <ref type="bibr" target="#b10">[10]</ref>. But the more intricate the acquisition function, the longer it takes to execute-particularly for software with very many configurations. For example, recent studies with a new state-of-the-art acquisition function show that such approaches are limited to models with less than a dozen decisions (i.e. configuration options) <ref type="bibr" target="#b13">[13]</ref>.</p><p>As an example of an acquisition function, consider Figure <ref type="figure" target="#fig_0">1</ref>. It illustrates a time series of a typical run of SMBO. The bold black line represents the actual performance function (f -which is unknown in our setting) and the dotted black line represents the estimated objective function (in the language of SMBO, this is the prior). The purple regions represent the configuration or uncertainty of estimation in a region-the thicker that region, the higher the uncertainty.</p><p>The green line in that figure represents the acquisition function. The optimization starts with two points (t=2). At each iteration, the acquisition function is maximized to determine where to sample next. The acquisition function is a user-defined strategy, which takes into account the estimated performance measures (mean and variance) associated with each configuration.. The chosen sample (or configuration) maximizes the acquisition function (argmax). This process terminates when a predefined stopping condition is reached which is related to the budget associated with the optimization process.</p><p>Gaussian Process Models (GPM) is often the surrogate model of choice in the machine learning literature. GPM is a probabilistic regression model which instead of returning a scalar (f (x)) returns the mean and variance associated with x. There are various acquisition functions to choose from: (1) Maximum Mean, (2) Maximum Upper Interval, (3) Maximum Probability of Improvement, (4) Maximum Variance, and (5) Maximum Expected Improvement. Building GPMs can be very challenging since:</p><p>• GPMs can be very fragile, that is, very sensitive to the parameters of GPMs; • GPMs do not scale to high dimensional data as well as a large dataset (software system with large configuration space) <ref type="bibr" target="#b38">[39]</ref>. For example, in SE, the state-of-the-art in this area using GPMs for optimization was limited to models with around ten decisions <ref type="bibr" target="#b14">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PERFORMANCE OPTIMIZATION OF CONFIG-URABLE SOFTWARE SYSTEMS</head><p>In this section, we discuss the model-based methods used in the prior work to find the better configurations of software systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Residual-based: "Build an Accurate Model"</head><p>In this section, we discuss the residual-based method for building performance models for software systems, which, in SMBO terminology, is an optimizer with a flat acquisition function, that is, all the points are equally likely to be selected (random sampling).</p><p>When the cost of collecting data (benchmarking time) is higher than the cost of building a performance model (surrogate model), it is imperative to minimize the number of measurements required for model building. A learning curve shows the relationship between the size of the training set and the accuracy of the model. In Figure <ref type="figure">2</ref>, the horizontal axis represents the number of samples used to create the performance model, whereas the vertical axis represents the accuracy (measured in terms of MMRE-Mean Magnitude of Relative Error) of the model learned. Learning curves typically have a steep sloping portion early in the curve followed by a plateau late in the curve. The plateau occurs when adding data does not improve the accuracy of the model. As engineers, we would like to stop sampling as soon as the learning curve starts to flatten. Two types of residual-based methods have been introduced in Sarkar et al. namely progressive and projective sampling.</p><p>(1) Progressive Sampling uses an iterative sampling strategy to inform the process of building the performance model. It starts by sampling a small set of configurations and their corresponding performance measures to build a model and validating the model using a holdout set. Configurations are iteratively sampled and used to construct the performance model until the performance model achieves a specified accuracy (measured in terms of MMRE). The sampling process terminates when a predefined threshold is reached. One of the shortcomings of progressive sampling is that the resulting performance model achieves an acceptable accuracy only after a large number of iterations, which implies high modeling cost. There is no way to determine the cost of modeling until the performance model is already built, which defeats its purpose, as there is a risk of overshooting the modeling budget and still not obtaining an accurate model.</p><p>(2) Projective sampling addresses this problem by approximating the learning curve using a minimal set of initial configurations, thus providing the stakeholders with an estimate of the modeling cost.</p><p>We use progressive sampling as a representative because projective sampling adds only a sample estimation technique to the progressive sampling and does not add anything to the sampling itself.</p><p>The residual-based method discussed here considers only performance configuration optimization scenarios with a single-objective. In the residual-based method, the correctness of the performance model built is measured using error measures such as MMRE:</p><formula xml:id="formula_2">MMRE = | f (x ) -y | y • 100<label>(3)</label></formula><p>For further details, please refer to Sarkar et. al <ref type="bibr" target="#b9">[9]</ref>. pool) and used for testing the quality of the surrogate model (in terms of residual-based measures) called holdout set. The training pool is the set from which the configurations would be selected (randomly, in this case) and then tested against the holdout set. At each iteration, a (set of) data instance(s) of the training pool is added to the training set (Line 9). Once the data instances are selected from the training pool, they are evaluated, which in our setting means measuring the performance of the selected configuration (Line 12). The configurations and the associated performance scores are used to build the performance model (Line 14). The model is validated using the testing set<ref type="foot" target="#foot_0">2</ref> , then the accuracy is computed. In our setting, we assume that the measure is accuracy (higher is better). Once the accuracy score is calculated, it is compared with the accuracy score obtained before adding the new set of configurations to the training set. If the accuracy of the model (with more data) does not improve the accuracy when compared to the previous iteration (lesser data), then life is lost. This termination criterion is widely used in the field of Evolutionary Algorithms to determine the degree of convergence <ref type="bibr" target="#b39">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Rank-based: "Build a Rank-preserving Model"</head><p>As an alternative to the residual-based method, a rankbased method has recently been proposed <ref type="bibr" target="#b11">[11]</ref>. The rankbased method is similar to residual-based method in that it has a flat acquisition function, which resembles random sampling. Like the residual-based method, the rank-based method discussed here also considers only performance configuration optimizations with a single-objective. For further details, please refer to Nair et. al <ref type="bibr" target="#b11">[11]</ref>.</p><p>In a nutshell, instead of using residual measures of errors, as described in Equation <ref type="formula" target="#formula_2">3</ref>, which depend on residuals (r = y -f (x)) <ref type="foot" target="#foot_1">3</ref> , it uses a rank-based measure. While training the performance model (f (x)), the configuration space is  iteratively sampled (from the training pool) to train the performance model. Once the model is trained, the accuracy of the model is measured by sorting the values of y = f (x) from 'small' to 'large', that is:</p><formula xml:id="formula_3">f (x 1 ) ≤ f (x 2 ) ≤ f (x 3 ) ≤ ... ≤ f (x n ).<label>(4)</label></formula><p>The predicted rank order is then compared to the actual rank order. The accuracy is calculated using the mean rank difference (µRD):</p><formula xml:id="formula_4">µRD = 1 n • n i=1 rank(y i ) -rank(f (x i ))<label>(5)</label></formula><p>This measure simply counts how many of the pairs in the test data have been ordered incorrectly by the performance model f (x) and measures the average of magnitude of the ranking difference.</p><p>In Figure <ref type="figure">4</ref>, we list a generic algorithm for the rank-based method. Sampling starts by selecting samples randomly  from the training pool and by adding them to the training set (Line 8). Then, the collected sample configurations are evaluated (Line 11). The configurations and the associated performance measure are used to build a performance model (Line 13). The generated model (CART, in our case) is used to predict the performance measure of the configurations in the testing pool (Line 16). Since the performance value of the holdout set is already measured, hence known, the ranks of the actual performance measures, and predicted performance measure are calculated. (Lines 18-19). The actual and predicted performance measure is then used to calculate the rank difference using Equation <ref type="formula" target="#formula_4">5</ref>. If the rank difference (µRD) of the model (with more data) does not decrease when compared to the previous generation (lesser data), then a life is lost (Lines 23 <ref type="bibr">-24)</ref>. When all lives are expired, sampling terminates (Line 27).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">ePAL: "Traditional SMBO"</head><p>Unlike the residual-based and rank-based methods, epsilon Pareto Active Learning (ePAL) reflects on the evaluated configurations (and corresponding performance measures) to decide the next best configuration to measure using Maximum Variance (predictive uncertainty) as an acquisition function. ePAL incrementally updates a model (GPM) representing a generalization of all samples (or configurations) seen so far. The model can be used to decide the next most promising configuration to evaluate. This ability to avoid unnecessary measurement (by just exploring a model) is very useful in the cases where each measurement can take days to weeks.</p><p>In Figure <ref type="figure" target="#fig_4">5</ref>, we list a generic algorithm for ePAL. ePAL starts by selecting samples randomly from the configuration space (all configs) and by adding them to the training set (Line 5). The collected sample configurations are then evaluated (Line 7). The configurations and the associated performance values are used to build a performance model (Line 13). The generated model (GPM, in this case) is used to predict the performance values of the configurations in the testing pool (Line 16). Note that the model returns both the value (µ) as well as the associated confidence interval (σ). These predicted values are used to discard configurations, which have a high probability of being dominated by another point (Line 18). Domination is defined in Equation 2 4 . After configurations (which have a high probability of being dominated) have been discarded, a new configuration (new point) is selected and measured (Line 20). The selected configuration new point is the most uncertain in all configs. Then, new point is added to train, which is then used to build the model in the subsequent iteration (Line 22). When all the configuration in all configs have been discarded (or evaluated and moved to train) the process terminates.</p><p>Note again, since ePAL is a traditional SMBO, it shares its shortcomings (refer to Section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">FLASH: A FAST SEQUENTIAL MODEL-BASED METHOD</head><p>To overcome the shortcomings of the traditional SMBO, FLASH makes certain design choices: • FLASH's acquisition function uses Maximum Mean. Maximum Mean returns the sample (configuration) with highest expected (performance) measure; • GPM is replaced with CART <ref type="bibr" target="#b40">[41]</ref>, a fixed-point regression model. This is possible because the acquisition function requires only a single point value rather than the mean and the associated variance. When used in a multi-objective optimization setting, FLASH models each objective as a separate performance (CART) model. This is because the CART model can be trained for one performance measure or dependent value.</p><p>The basic idea of CART is as follows: CART recursively partitions the set of configurations (based on a configuration option) into smaller clusters until the performance of the configurations in the clusters are similar. Each split of the set of configurations is driven by a decision on the configuration option that would minimize the entropy or prediction error. These recursive clustering is represented as a binary decision tree. So, when we need to predict the performance of a new configuration not measured so far, we use the decision tree to find the cluster which is most similar to the new configuration.</p><p>FLASH replaces the actual evaluation of all configurations (which can be a very slow process) with a surrogate evaluation, where the CART decision trees are used to guess the objective scores (which is a very fast process). Once guesses are made, then some select operator must be applied to remove less-than-satisfactory configurations. Inspired by the decomposition approach of MOEA/D <ref type="bibr" target="#b41">[42]</ref>, FLASH uses the following stochastic Maximum Mean method, which we call Bazza 5 .</p><p>For problems with o objectives that we seek to maximize, Bazza returns the configuration that has outstandingly maximum objective values across N random projections. Using the predictions for the o objectives from the learned CART models, then Bazza executes as follows. Note that the first step (randomly assigning weights to goals) is a technique we burrow and adapt from the MOEA/D algorithm <ref type="bibr" target="#b41">[42]</ref>. <ref type="bibr" target="#b4">4</ref>. ePAL then removes all ε-dominated points: a is discarded due to b if µ b + σ b ε-dominates µa -σa, where x ε-dominates y if x + ε y and " " is binary domination-see Equation <ref type="formula" target="#formula_1">2</ref>5. Short for "bazzinga". Also, "Bazza" is Australian for "Barry" which the name of Barry Allen of the Flash T.V. series; and the childhood nickname of the 44th United States President Barack Obama. -Guess its performance scores y i,j using the predictions from the CART models. -Compute its mean weight as follows:</p><formula xml:id="formula_5">mean i = 1 N N n m j (V n,j • x i,j )<label>(6)</label></formula><p>-If mean &gt; max , then max := mean and best := x i . • Return best.</p><p>In summary, given a set of V weight vectors of length m, Bazza finds the vector that scores best across N different weighted sums, each of which is computed with random weight vectors.</p><p>The resulting algorithm is shown in Figure <ref type="figure" target="#fig_5">6</ref>. Before initializing FLASH, a user needs to define three parameters N, size, and budget (refer to Section 8.2 for a sensitivity analysis). FLASH starts by randomly sampling a predefined number (size) of configurations from the configuration space and evaluate them (Line 4). The evaluated configurations are removed from the unevaluated pool of configurations (Line 6). The evaluated configurations and the corresponding performance measure/s are then used to build CART model/s (Line 10). This model (or models, in case of multi-objective problems) is then used by the acquisition function to determine the next point to measure (Line 13). The acquisition function accepts the model (or models) generated in Line 10 and the pool of unevaluated configurations (uneval configs) to choose the next configuration to measure. The model is used to generate a prediction for the unevaluated configurations. For single-objective optimization problems, the next configuration to measure is the configuration with the highest predicted performance measure (Line 26). For multiobjective optimization problems, Bazza is applied. The configuration chosen by the acquisition function is evaluated and added to the evaluated pool of configurations (Line 12-13) and removed from the unevaluated pool (Line 14). FLASH terminates once it runs out of budget (Line 8).</p><p>Note the advantages of this approach: SMBO is a widely used method <ref type="bibr" target="#b42">[43]</ref> for many important tasks (cloud configuration <ref type="bibr" target="#b5">[5]</ref>, hyperparameter optimization <ref type="bibr" target="#b43">[44]</ref>) so even a small improvement in this method would be significant for a large of number of domains Truly FLASH includes many novel innovations. The following list shows the significant innovations of this work and the delta to prior research. 1. Evolutionary algorithms (EAs) have been used for optimizing black-box optimization <ref type="bibr" target="#b44">[45]</ref>. Using Evolutionary Algorithms is relatively straightforward since it requires no domain knowledge to solve a problem. Challenge: Evolutionary algorithms suffer from two problems. Firstly, there is the issue of the number of evaluations required for an EA. A standard EA experiment is 100 individuals mutated for 100+ generations <ref type="bibr" target="#b45">[46]</ref>. This renders Evolutionary Algorithms unsuitable for our domain since individual evaluation can be very slow (requires re-running a benchmark suite). A second problem with EAs is the problem of slow convergence (i.e., the performance delta across these generations may be very slow and take a long time to stabilize <ref type="bibr" target="#b46">[47]</ref>). For this reason, research in this area in the last decade has explored non-EA methods for software configuration <ref type="bibr" target="#b8">[8]</ref>, <ref type="bibr" target="#b9">[9]</ref>, <ref type="bibr" target="#b10">[10]</ref>, <ref type="bibr" target="#b11">[11]</ref>, <ref type="bibr" target="#b16">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>New approach:</head><p>• Here we explore SMBO for software configuration optimization.</p><p>• While SMBO is gaining some popularity in other domains <ref type="bibr" target="#b5">[5]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>, this article is the first reporting a successful application of SMBO to software configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Prior work in this area used some surrogate model learned by data mining (e.g., with CART <ref type="bibr" target="#b8">[8]</ref>, <ref type="bibr" target="#b9">[9]</ref>, <ref type="bibr" target="#b10">[10]</ref>), possibly combined with random sampling. Such surrogates are useful for guiding the construction of better configuration models since they can be much faster to execute than (say) re-running a benchmark. Hence, an optimizer that uses such surrogates can terminate relatively quickly. Challenge: One drawback with surrogate models is that they require a holdout set, against which the surrogate model (built iteratively) is evaluated. Interestingly, prior work does not discuss the cost of populating, which may require exploring up to 20% of the total configuration space <ref type="bibr" target="#b11">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>New approach:</head><p>• We found that applying SMBO removes the need for this hold out. We build the model incrementally, thus the configurations (and their performance) sampled at a given point in time used to intelligently select the next data point to collect. • This work is the first successful application of such incremental model construction (with no holdout sets) for software configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>Standard SMBO algorithms are a widely used method for finding good samples (as used in hyper-parameter optimization). Standard SMBO builds its models using a method called Gaussian Process Models. Due to internal complexities of some of its matrix operations, GPM can handle only a dozen configuration options (or less), while modern software may required many more configuration options. Challenge: How can we scale SMBO to much larger configuration options? New approach:</p><p>• One of the core innovations of this article is the use of CART (one CART per goal) for surrogate modeling.</p><p>That is, we replace GPM with CART. GPM takes time O(M 3 ) <ref type="bibr" target="#b49">[50]</ref> while recursive bifurcating algorithms like CART are much faster (takes time O(M N 2 ) to build its trees where M is the size of the training dataset and N is the number of attributes <ref type="bibr" target="#b50">[51]</ref>). Furthermore, methods such as CART have been extensively studied, and very fast incremental versions are simple to implement-see Domingos et al. <ref type="bibr" target="#b51">[52]</ref>, which is to say that if CART ever gets slow, there are many alternatives, we could try that would readily speed it up. • Theoretical complexity results aside, we demonstrate empirically that that our CART-based method scales much better than GPM. As of 2016, published stateof-the-art results <ref type="bibr" target="#b14">[14]</ref> report that they were unable to use more than 10 attributes within a GPM. As of 2018, our own experiments confirm that GPM cannot scale to more than a dozen configuration options. As shown in Figures <ref type="figure" target="#fig_10">10(a</ref>) and (b), FLASH scales linearly in number of attributes to models that defeats SMBO. We regard FLASH's ability to scale linearly in number of attributes to be a major contribution of this article. 4. One advantages of SMBO algorithms such as GPM is that they identify region(s) of most variance within a model. Such regions represent zones of most uncertainty (and sampling there has greatest chance of most improving a model). Challenge: If we are not using GPM, how can we find the best regions for future sampling? New approach:</p><p>• Another core innovation is the Bazza algorithm. Bazza assumes that the greatest mean might contain the values that most extend to the desired maximal (or minimal) goals. Bazza finds that region in linear time (since it only has to track the most extreme values seen so far). • Bazza is an important innovation since standard methods for finding the best candidates within a population of size M require time O(M 2 ) <ref type="bibr" target="#b52">[53]</ref>. But as shown in this article, our methods require only O(M). 5. The kernels used in Gaussian Process Model assume "smoothness" <ref type="bibr" target="#b49">[50]</ref>, or in other words, the configurations which are closer to each other have similar performance. In the case of software configuration, this assumption is highly unlikely since we know of many software options where a small change can lead to radically different software performance (e.g. switching from link lists to B-trees is a single change to one value of one configuration option-but that change can lead to dramatic speed ups in the software). Challenge: How to avoid GPM's smoothness assumptions? New approach:</p><p>• We use CART, a learner that recursively bifurcates training data into different regions. The important point here is that CART makes no assumption that neighboring regions have the same properties.</p><p>• Unlike prior work, our use of CART makes no limiting assumptions about the smoothness of the space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Research Questions</head><p>In prior work, performance configuration optimization was conducted by sequentially sampling configurations to build models, both accurate (residual-based method) and inaccurate (rank-based method). Both methods divide the configuration space into (i) training pool, (ii) validation set, and (iii) holdout set. They sequentially select a configuration from the training pool and add it to the training set (which is a subset of the training pool). The configuration (along with the corresponding performance measure) is used to build a model. Both methods use a validation set to evaluate the quality of the model. The size of the validation set is based on an engineering judgment and expected to be a representative of the whole configuration space. Prior work <ref type="bibr" target="#b11">[11]</ref> used 20% of the configuration space as holdout set, but did not consider the cost of using the validation set.</p><p>Our research questions are geared towards assessing the performance of FLASH based on two aspects: (i) Effectiveness of the solution or the rank-difference between the best configuration found by FLASH to the actual best configuration, and (ii) Effort (number of measurements) required to find the better configuration. The above considerations lead to two research questions: RQ1: Can FLASH find the better configuration? Here, the better configurations found using FLASH are compared to the ones identified in prior work, using the residual-based and rank-based method. The effectiveness of the methods is compared using rank-difference (Equation <ref type="formula" target="#formula_6">7</ref>). RQ2: How expensive is FLASH (in terms of how many configurations must be measured)? It is expensive to build (residual-based or rank-based) models since they require using a holdout set. Our goal is to demonstrate that FLASH can find better configurations of a software system using fewer measurements.</p><p>To the best of knowledge, SMBO has never been used for multi-objective performance configuration optimization in software engineering. However, similar work has been done by Zuluaga et al. <ref type="bibr" target="#b13">[13]</ref> in the machine learning community, where they introduced ePAL. We use ePAL as a state-of-theart method to compare FLASH.</p><p>We do not consider the work by Oh et al. <ref type="bibr" target="#b53">[54]</ref>, which uses true random sampling to find the better configurations. We do not compare FLASH with Oh et al.'s method mainly for the following reason: Oh et al.'s work supports only Boolean configuration options, which limits its practical applicability. FLASH, and the prior work considered in this article, do not have this limitation. Moreover, Oh's work is limited to single-objective problems. Since it does not build a performance model during the search process, it cannot be easily adapted to multi-objective problems. One may argue that running Oh et al.'s approach alternatively on different objectives (of a multi-objective problem) could lead to a set of solutions on the Pareto Front. However, this is not a proper alternative since these runs (on separate objectives) are independent of each other (i.e., they are run separately and cannot inform each other).</p><p>Since ePAL suffers from the shortcomings of traditional SMBO, our research questions are geared towards finding the estimated Pareto-optimal solutions (predicted Pareto Frontier 6 ), which is closest to the true Pareto Frontier (which requires measuring all configurations) with least effort. We assess the performance of FLASH by considering three aspects: (i) Effectiveness of the configurations between the Pareto Frontier and the ones approximated by an optimizer, and Effort evaluated in terms of (ii) number of measurements, and (iii) time to approximate the Pareto Frontier. The above considerations lead to three research questions: RQ3: How effective is FLASH for multi-objective performance configuration optimization? The effectiveness of the solution or the difference between the predicted Pareto Frontier found by optimizers to the true Pareto Frontier, RQ4: Can FLASH be used to reduce the effort of multi-objective performance configuration optimization compared to ePAL? Effort (number of measurements) required to estimate the Pareto Frontier which is closest to the true Pareto Frontier, and RQ5 Does FLASH save time for multi-objective performance configuration optimization compared to ePAL? Since ePAL may take substantial time to find the approximate the Pareto Frontier, it is imperative to show that FLASH can approximate the Pareto Frontier and converge faster.</p><p>Our goal is to minimize the effort (time and number of measurements) required to find an approximate Pareto Frontier as close to the actual Pareto Frontier as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Case Studies</head><p>We evaluated FLASH in two different types of problems namely: (1) single-objective optimization problems and (2) multi-objective optimization problems using 30 scenarios (15 scenarios in multi-objective settings) from 6 software systems. These systems are summarized in Table <ref type="table" target="#tab_1">1</ref>. More details about the software systems are available at http: //tiny.cc/flash systems/.</p><p>We selected these software systems since they are widely used in the configuration and search-based SE literature <ref type="bibr" target="#b8">[8]</ref>, <ref type="bibr" target="#b9">[9]</ref>, <ref type="bibr" target="#b10">[10]</ref>, <ref type="bibr" target="#b11">[11]</ref>, <ref type="bibr" target="#b13">[13]</ref>, <ref type="bibr" target="#b15">[15]</ref>, <ref type="bibr" target="#b16">[16]</ref>, <ref type="bibr" target="#b53">[54]</ref> as benchmark problems for this kind of optimization work. Furthermore, extensive documentation is available at the supplementary Web site for all these models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Experimental Rig</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Exploring RQ1, RQ2</head><p>For each subject system, we build a table of data, one row per valid configuration. We then run all configurations of all systems (that is, that are invoked by a benchmark) and recorded the performance scores. Note that, while answering the research questions, we ensure that we never test any prediction model on the data that we used to learn the model.</p><p>To answer our research questions, we split the datasets into training pool (40%), holdout set (20%), and validation pool (40%). The size of the holdout set is taken from prior 6. Pareto Frontier is a set of solutions which are non-dominated by any other solution.</p><p>work <ref type="bibr" target="#b11">[11]</ref>. It is worth to note that this is a hyper-parameter and is set based on an engineering judgment. To perform a fair comparison while comparing FLASH with prior work, the training pool and validation pool are merged for FLASH experiments.</p><p>The experiment to find better configuration using the residual-based and rank-based methods is conducted in the following way:</p><p>• Randomize the order of rows in the training data • Do -Select one configuration (by sampling with replacement) and add it to the training set -Determine the performance scores associated with the configuration. This corresponds to a table look up but would entail compiling or configuring and executing a system configuration in a practical setting. -Using the training set and the accuracy, build a performance model using CART. -Using the data from the testing pool, assess the accuracy either using MMRE (as described in Equation <ref type="formula" target="#formula_2">3</ref>) or rank difference (as described in Equation <ref type="formula" target="#formula_4">5</ref>).</p><p>• While the accuracy is greater or equal to the threshold determined by the practitioner (rank difference in the case of rank-based method and MMRE in the case of residualbased method). Once the model is trained, it is tested on the data in the validation pool. Please note, the learner has not been trained on the validation pool. The experiment to find better configuration by FLASH is conducted in the following way:</p><p>• Choose 80% of the data (at random) Once FLASH has terminated, the configuration with the best performance is selected as the better configuration. Please note that unlike the methods proposed in prior work, there is no training and validation pool in FLASH. It uses the whole space and returns the configuration with the best performance.</p><p>RQ1 relates the results found by FLASH to ones of residual-based and rank-based methods. We use the absolute difference between the ranks of the configurations predicted to be the best configuration and the actual optimal 7. We use 80% because other method find the better configuration sampling from a training set of 40% and test is against a testing pool of 40%. To make sure, we make a fair comparison, we use FLASH to find the best configuration among 80% of the configuration space.</p><p>configuration. We call this measure rank difference.</p><formula xml:id="formula_6">RD = |rank(actual best ) -rank(predicted best )|<label>(7)</label></formula><p>Ranks are calculated by sorting the configurations based on their performance scores. The configuration with the minimum performance score, rank(actual best ), is ranked 1 and the one with the highest score is ranked as N , where N is the number of configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">Exploring RQ3, RQ4, and RQ5</head><p>Similar to RQ1 and RQ2, for each subject system, we build a table of data, one row per valid configuration. We then run all configurations of all systems and record the performance scores. To this table, we add two columns of measurements (one for each objective) obtained from measurements.</p><p>To measure effectiveness, we use quality indicators as advised by Wang et al. <ref type="bibr" target="#b54">[55]</ref>. The quality indicators are:</p><p>• The Generational Distance (GD) <ref type="bibr" target="#b55">[56]</ref> measures the closeness of the solutions from by the optimizers to the Pareto frontier that is, the actual set of non-dominated solutions. • The Inverted Generational Distance (IGD) <ref type="bibr" target="#b56">[57]</ref> is the mean distance from points on the true Pareto-optimal solutions to its nearest point in the predicted Pareto-optimal solutions returned by the optimizer. Note that, for both measures, smaller values are better. Also, according to Coello et al. <ref type="bibr" target="#b56">[57]</ref>, IGD is a better measure of how well solutions of a method are spread across the space of all known solutions. A lower value of GD indicates that the predicted Pareto-optimal solutions have converged (or are near) to the actual Pareto-optimal solutions. However, it does not comment on the diversity (or spread) of the solutions. GD is useful while interpreting the results of RQ3 and RQ6, where we would notice that FLASH has low GD values but relatively high IGD values.</p><p>To answer our research questions, we initialize ePAL and FLASH with randomly selected configurations along with their corresponding performance scores. Since, ePAL does not have an explicit stopping criterion, we allow ePAL to run until completion. For FLASH, we allowed a budget of 50 configurations. The value 50 was assigned by parameter tuning (from Section 8.2). The configurations evaluated during the execution of the three methods are then used to measure the quality measures (to compare methods). Note that we use two versions of ePAL: ePAL with = 0.01 (ePAL 0.01), and ePAL with = 0.3 (ePAL 0.3) 8 . These ePAL versions represents two extremes of ePAL from the most cautious ( = 0.01)-maximizing quality to most careless ( = 0.3)minimizing measurements. 9  Other aspects of our experimental setting were designed in response to the specific features of the experiments. For example, all the residual-based, rank-based and FLASH methods are implemented in Python. We use Zuluaga et al.'s implementation of ePAL, which was implemented in Matlab. Since we are comparing methods implemented in different languages, we measure "speed" in terms of the number of measurements (a language-independent feature) along with runtimes.</p><p>8. Refer to Section 4.3 for definition of 9. We have measured other values of epsilon between 0.01 and 0.3, but due to space constraints we show results from two variants of ePAL IEEE TRANS SE. SUBMITTED NOV' <ref type="bibr" target="#b17">17</ref> 11 Residual-based Rank-based </p><formula xml:id="formula_7">SS-A1 SS-A2 SS-B1 SS-B2 SS-C1 SS-C2 SS-D1 SS-D2 SS-E1 SS-E2 SS-F1 SS-F2 SS-G1 SS-G2 SS-H1 SS-H2 SS-I1 SS-I2 SS-J1 SS-J2 SS-K1 SS-K2 SS-L1 SS-L2 SS-M1 SS-M2 SS-N1 SS-N2 SS-O1 SS-O2<label>0</label></formula><formula xml:id="formula_8">SS-A1 SS-A2 SS-B1 SS-B2 SS-C1 SS-C2 SS-D1 SS-D2 SS-E1 SS-E2 SS-F1 SS-F2 SS-G1 SS-G2 SS-H1 SS-H2 SS-I1 SS-I2 SS-J1 SS-J2 SS-K1 SS-K2 SS-L1 SS-L2 SS-M1 SS-M2 SS-N1 SS-N2 SS-O1 SS-O2<label>0</label></formula><formula xml:id="formula_9">SS-A1 SS-A2 SS-B1 SS-B2 SS-C1 SS-C2 SS-D1 SS-D2 SS-E1 SS-E2 SS-F1 SS-F2 SS-G1 SS-G2 SS-H1 SS-H2 SS-I1 SS-I2 SS-J1 SS-J2 SS-K1 SS-K2 SS-L1 SS-L2 SS-M1 SS-M2 SS-N1 SS-N2 SS-O1 SS-O2<label>0</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Single-objective Problems</head><p>RQ1: Can FLASH find the better configuration? Figure <ref type="figure">7</ref> shows the Rank Difference of the predictions using FLASH, the rank-based method and the residual based method. The horizontal axis shows subject systems. The vertical axis shows the rank difference (Equation <ref type="formula" target="#formula_6">7</ref>).</p><p>• The ideal result would be when all the points lie on the line y=0 or the horizontal axis, which means the method was able to find the better configurations for all subject systems or the rank difference between the predicted optimal solution and the actual optimal solution is 0. • The sub-figures (left to right) represent the residual-based method, rank-based method, and FLASH. Overall, in Figure <ref type="figure">7</ref>, we find that:</p><p>• All methods can find better configurations. For example, FLASH for SS-J1 predicted the configuration whose performance score is ranked 20th in configuration space. That is good enough since FLASH finds the 20th most performant configuration among 3072 configurations. • The mean rank difference of the predicted optimal configuration is 6.082, 5.81, and 5.58 10 for residual-based, rankbased, and FLASH. So, the rank of the better configuration found by all the three methods is practically the same. To verify the similarity is statistically significant, we further studied the results using non-parametric tests Scott-Knott test recommended by Mittas and Angelis <ref type="bibr" target="#b57">[58]</ref> and Arcuri &amp; Briand <ref type="bibr" target="#b57">[58]</ref>.</p><p>Scott-Knott is a top-down clustering approach used to rank different treatments. If the approach finds an interesting division of the data, then some statistical test is applied to the two divisions to check if they are statistically significantly different. If so, Scott-Knott recurses into both halves. To apply Scott-Knott, we sorted a list of l = 20 values of performance of different method found by different methods. Then, we split l into sub-lists m, n to maximize the expected value of differences in the observed performances before and after division. For example, for lists l, m, n of size ls, ms, ns where l = m ∪ n:</p><formula xml:id="formula_10">E(∆) = ms ls |m.µ -l.µ| 2 + ns ls |n.µ -l.µ| 2</formula><p>We then apply a statistical hypothesis test H to check if m, n are significantly different (in our case, the conjunction of A12 and bootstrapping). If so, Scott-Knott recurses on the splits. In other words, we divide the data if both bootstrap sampling and effect size test agree that a division is statistically significant (with a confidence level of 99%) and not a small effect (A12 ≥ 0.6).  For a justification of the use of non-parametric bootstrapping, see <ref type="bibr">Efron &amp; Tibshirani [59,</ref>. For a justification of the use of effect size tests, see Shepperd and MacDonell <ref type="bibr" target="#b59">[60]</ref>; Kampenes <ref type="bibr" target="#b60">[61]</ref>; and Kocaguenli et al. <ref type="bibr" target="#b61">[62]</ref>. These researchers warn that, even if a hypothesis test declares two populations to be "significantly" different, then that result is misleading if the "effect size" is very small. Hence, to assess the performance differences we first must rule out small effects using A12.</p><p>In Figure <ref type="figure">8</ref>, we show the Scott-Knott ranks for the three methods. The quartile charts show the Scott-Knott results for the subject systems, where FLASH did not do as well as the other two methods. For example, the statistic test for SS-C2 shows that the rank difference of configurations found by FLASH is statistically larger from the other methods. This is reasonably close, since the median rank of the configurations found by FLASH is 7 of 1512 configurations, where for the other methods found configurations have a median rank of 2. As engineers, we feel that this is close because we can find the 7th best configuration using 34 measurements compared to 339 and 346 measurements used by other methods. Overall, our results indicate that: FLASH can find better configurations, similar to the residual-based and the rank-based method, of a software system without using a holdout set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RQ2: How expensive is FLASH (in terms of how many configurations must be executed)?</head><p>To recommend FLASH as a cheap method for performance optimization, it is important to demonstrate that it requires fewer measurements to find the better configurations. In our setting, the cost of finding the better configuration is quantified by number of measurements required (i.e., table lookup). Figure <ref type="figure" target="#fig_9">9</ref> shows our results. The vertical axis represents the ratio of the measurements of different methods are represented as the percentage of number of measurements required by residual-based method since it uses the most measurements in 66% scenarios.</p><p>Overall, we see that FLASH requires the least number of measurements to find better configurations. For example, in SS-E1, FLASH requires 9% of the measurements when compared with the residual-based method and the rankbased method. There are few cases (SS-M1 to SS-O2) where FLASH requires less than 1% of the residual-based method, which is because these systems have a large configuration space and the holdout set required by the residual-based method and the rank-based method (except FLASH) uses 20% of the measurements.</p><p>For performance configuration optimization, FLASH is cheaper than the state-of-the-art method. In 57% of the software systems, FLASH requires an order of magnitude fewer measurement compared to the residualbased method and rank-based method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Multi-objective Optimization</head><p>RQ3: How effective is FLASH for multi-objective performance configuration optimization? Table <ref type="table" target="#tab_8">2</ref> shows the results of a statistical analysis that compares the quality measures of the approximated Paretooptimal solutions generated by FLASH to those generated by ePAL.</p><p>• The rows of the table shows median numbers of 20 repeated runs over 15 different scenarios. • The columns report the quality measures, generational distance (GD), and inverted generation distance (IGD).</p><p>Recall smaller values of GD and IGD are better. • 'X' denotes cases where a method did not terminate within a reasonable amount of time (10 hours). • Bold-typed results are statistically better than the rest.</p><p>• The last row of the table (Win (%)) reports the percentage of times a method is significantly better than other methods overall software systems. One way to get a quick summary of this table is to read the last row (Win(%)). This row is the percentage number of times a method was marked statistically better than the other methods. From Table <ref type="table" target="#tab_8">2</ref>, we can observe that FLASH outperforms variants of ePAL since FLASH has the highest Win(%) in both quality measures. This is particularly true for scenarios with more than 10 configuration options, where ePAL failed to terminate while FLASH always did.</p><p>We further notice that ePAL-0.01 has a higher win percentage than ePAL-0.3. This is not surprising since (as discussed in Section 6.3.2) ePAL-0.01 (optimized for quality) is more cautious than ePAL-0.3 (which is optimized for speed measured in terms of number of measurements). This can be regarded as a sanity check. It is interesting to note that FLASH has impressive convergence score (lower GD scores)-it converges better for 93% of the systems, but not so remarkable in terms of the spread (lower IGD scores). However, the performance of FLASH is similar to ePAL. It is also interesting that, for software systems where FLASH was not statistically better, these are cases where the statistically better method always converged to the actual Pareto Frontier (with few exceptions).</p><p>FLASH is effective for multi-objective performance configuration optimization. It also works in software systems with more than 10 configuration options whereas ePAL does not terminate in reasonable time.</p><p>RQ4: Can FLASH be used to reduce the effort of multiobjective performance configuration optimization compared to ePAL? In the RQ4 section (right-hand side) of Table <ref type="table" target="#tab_8">2</ref>, the number of measurements required by methods, ePAL, and FLASH are shown. Rows show different software systems and columns shows the number of measurements associated with each method. The numbers highlighted in bold mark methods that are statistically better than the other. For example in SS-K, FLASH uses statistically fewer samples than (variants of) ePAL.</p><p>From the table we observe:</p><p>• FLASH uses fewer samples than ePAL 0.01. In 9 of 15 cases, ePAL 0.01 is, at least, two times better than FLASH. • (Variants of) ePAL does not terminate for SS-M, SS-N, and SS-O even after ten hours of execution-a pragmatic choice. The reason for this can be seen in Table <ref type="table" target="#tab_1">1</ref>: these software systems have more than 10 configuration options and the GPMs used by ePAL does not scale beyond that number. FLASH saves time and is faster than (variants of) ePAL in 13 of 15 cases. Furthermore, FLASH is an order of magnitudes faster than ePAL in 5 of 15 software systems. In other 2 out of 15 cases, the FLASH 's runtimes are similar to (variants of) ePAL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Why CART is used as the surrogate model?</head><p>Decision Trees are a very simple way to learn rules from a set of examples and can be viewed as a tool for the analysis of a large dataset. The reason why we chose CART is two-fold. Firstly, it is shown to be scalable and there is a growing interest to find new ways to speed up the decision tree learning process <ref type="bibr" target="#b50">[51]</ref>. Secondly, a decision tree can describe with the tree structure the dependence between the decisions and the objectives, which is useful for induction and comprehensibility. These are the primary reasons for choosing decision-trees to replace Gaussian Process as the surrogate model for FLASH.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">What is the trade-off between the starting size and budget of FLASH?</head><p>There are two main parameters of FLASH which require being set very carefully. In our setting, the parameters are size and budget. The parameter size controls the exploration capabilities of FLASH whereas parameter budget controls the exploitation capabilities. In Figure <ref type="figure" target="#fig_11">11</ref>, we show the tradeoff between generational distance and inverted generational distance by varying parameters size and budget. The markers in Figure <ref type="figure" target="#fig_11">11</ref> are annotated with the starting size of FLASH. The trade-off characterizes the relationship between two  conflicting objectives, for example in Figure <ref type="figure" target="#fig_11">11</ref>, point <ref type="bibr" target="#b10">(10)</ref> achieves high convergence (low GD value) but low diversity (high IGD value). Note, that the curves are an aggregate of the trade-off curves for all the software systems. From the figure <ref type="figure" target="#fig_11">11</ref> we observe that: The number of initial samples (size in Figure <ref type="figure" target="#fig_5">6</ref>) determines the diversity of the solution.</p><formula xml:id="formula_11">M X X 0 X X 0 X X 50 SS-N X X 0.065 X X 0.015 X X 50 SS-O X X 3.01E-07 X X 3.20E-06 X X<label>50</label></formula><p>With ten initial samples the algorithm converges (lowest GD values) but lowest diversity (high IGD values). However, with 50 initial samples (random sampling) FLASH achieves highest diversity (low IGD values) but lowest convergence (high GD values). We choose the starting size of 30 because it achieves a good trade-off between convergence and diversity. These values were used in the experiments in section 7.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Can rules learned by CART guide the search?</head><p>Currently, FLASH does not explicitly reflect on the Decision Tree to select the next sample (or configuration). But, rules learned by Decision Tree can be used to guide the process of search. Though we have not tested this approach, a Decision Tree can be used to learn about importance of various configuration options which can be then used to recursively prune the configuration space, similar to the approach of Oh et al. <ref type="bibr" target="#b53">[54]</ref>. We hypothesize that this would make FLASH more scalable and be used to much larger models. We leave this for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4">What are the shortcomings of FLASH?</head><p>FLASH suffers from the following shortcomings.</p><p>• Parallelization: FLASH like all sequential model-based approaches completes evaluating a configuration before evaluating a new one. However, in practice, this feature can lead to really long runtimes. A possible extension might be to evaluate multiple configurations in parallel. This has not been considered in this version of FLASH and is something we leave for the future work. • Non-stationary: FLASH assumes that the benchmark or the load in the system is stationary. Hence, there is no inherent mechanism in FLASH which would adapt itself based on the change in workload. This non-stationary nature of the problem is a significant assumption and currently not addressed in this paper. Addressing this aspect may include an ensemble-based approach where a new model is built at a specified time interval. The importance of the model is defined by a time-dependent weight decay of the model, that is, older the model, the lower its significance (weight). • Cost Sensitivity: FLASH also assumes that the cost of evaluating all the configurations are same. However, in practice, this is not true. For example, the wall clock time of running a specific benchmark on a software system with and without caching can be substantially different.</p><p>In practice, stakeholders may demand to find a good configuration within a specified time limit (instead of the number of configurations measured). We leave this for future work. • Cold Start: FLASH randomly selects the initial configurations to evaluate, which can affect its effectiveness. One of the way to reduce the impact of randomness its to select the initial points based on domain knowledge or use transfer learning from similar software systems that have been optimized in the past, to select the initial configurations of FLASH.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">THREATS TO VALIDITY</head><p>Reliability refers to the consistency of the results obtained from the research. For example, how well can independent researchers reproduce the study? To increase external reliability, we took care to either define our algorithms or use implementations from the public domain (SciKitLearn) <ref type="bibr">[63]</ref>. All code used in this work are available online 11 .</p><p>11. http://tiny.cc/flashrepo/</p><p>Validity refers to the extent to which a piece of research investigates what the researcher purports to investigate <ref type="bibr">[64]</ref>. Internal validity concerns with whether the differences found in the treatments can be ascribed to the treatments under study.</p><p>For the case-studies relating to configuration control, we cannot measure all possible configurations in a reasonable time. Hence, we sampled only a few hundred configurations to compare the prediction to actual values. We are aware that this evaluation leaves room for outliers and that measurement bias can cause false interpretations [65]. We also limit our attention to predicting PF for a given workload; we did not vary benchmarks.</p><p>Internal bias originates from the stochastic nature of multi-objective optimization algorithms. The evolutionary process required many random operations, same as the FLASH was introduced in this article. To mitigate these threats, we repeated our experiments for 20 runs and reported the median of the indicators. We also employed statistical tests to check the significance of the achieved results.</p><p>It is challenging to find the representatives sample test cases to covers all kinds of software systems. We just selected six most common types of software system to discuss the FLASH basing on them. In the future, we also need to explore more types of SBSE problems for other domains such as process planning, next release planning. We aimed to increase external validity by choosing case-studies from different domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">CONCLUSION</head><p>This article proposes a sequential model-based method called FLASH, an approach for finding better configurations while minimizing the number of measurements. To the best of our knowledge, this is the first time a sequential model-based method is used to solve the problem of performance configuration optimization. FLASH sequentially gains knowledge about the configuration space like traditional SMBO. FLASH is different from the traditional SMBOs because of the choice of the surrogate model (CART) and the acquisition function (the stochastic Maximum-Mean Bazza function). We have demonstrated the effectiveness of FLASH on single-objective and multi-objective problems using 30 scenarios from 7 software systems.</p><p>For a single-objective setting, we experimentally demonstrate that FLASH can locate the better configuration of 30 different scenarios for seven software systems, accurately compared to the state-of-the-art approaches while removing the need for a holdout dataset, hence saving measurement costs. In 57% of the scenarios, FLASH can find the better configuration by using an order of magnitude fewer solutions than other state-of-the-art approaches.</p><p>For multi-objective setting, we show how FLASH can overcome the shortcomings of traditional SMBO (ePAL) while being as effective as ePAL as well as being scalable to software systems with higher number (greater than 10) of configuration options (where ePAL does not terminate in a reasonable time-frame).</p><p>Regarding future work, the two directions for this research are i) test on different case studies and ii) further 0098-5589 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TSE.2018.2870895, IEEE Transactions on Software Engineering IEEE TRANS SE. SUBMITTED NOV' <ref type="bibr">17 16</ref> improve the scalability of FLASH. To conclude, we urge the SE community to learn from communities which tackle similar problems. This article experiments with ideas from fields of machine learning, SBSE, and software analytics to create FLASH, which is a fast, scalable and effective optimizer. We hope this article inspires other researchers to look further afield than their home discipline.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: An example of Sequential Model-based method's working process from [7]. The figures show a Gaussian process model (GPM) of an objective function over four iterations of sampled values. Green shaded plots represent acquisition function. The value of the acquisition function is high where the GPM predicts larger objective and where the prediction uncertainty (confidence) is high such points (configurations in our case) is sampled first. Note that the area on the far left is never sampled even when it has high uncertainty (low confidence) associated.</figDesc><graphic coords="4,223.44,43.70,165.12,72.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>• ... what should we do next? The "given what we know about the problem" part is achieved by using a machine learning model whereas "what should we do next" is performed by an acquisition function. Such acquisition function automatically adjusts the exploration ("should we sample in uncertain parts of the search space) and exploitation ("should we stick to what is already known") behavior of the method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 Fig. 2 :</head><label>32</label><figDesc>Fig. 2: The relationship between the accuracy and the number of samples used to train the performance model of the running Word Count application on Apache Storm. Note that the accuracy does not improve substantially after 20 sample configurations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :Fig. 4 :</head><label>34</label><figDesc>Fig. 3: Python code of progressive sampling, a residual-based method. # rank-based method def rank based(training, holdout, lives=3): last score = -1 independent vals = list() dependent vals = list() for count in range(1, len(training)): # Add one configuration to the training set independent vals += training[count] # Measure the performance value for the newly # added configuration dependent vals += measure(training set[count]) # Build model model = build model(independent vals, dependent vals) # Predicted performance values predicted performance = model(holdout) # Compare the ranks of the actual performance # scores to ranks of predicted performance scores actual ranks = ranks(measure(holdout)) predicted ranks = ranks(predicted performance) mean RD = RD(actual ranks, predicted ranks) # If current rank difference is not better than # the previous rank difference, then loose life if mean rank difference &lt;= last rank difference: lives -= 1 last rank difference = mean RD # If all lives are lost, exit loop if lives == 0: break return model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Python code of ePAL, a multi-objective SMBO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Python code of FLASH • N vectors V of length m are generated and filled with random numbers of the range 0..1. This represents the various weight vectors. The idea is to decompose one problem into a set of N sub-problems (uniformly spread N weight vectors. • Set max = 0 and best = nil . • For each configuration x i-Guess its performance scores y i,j using the predictions from the CART models. -Compute its mean weight as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>FlashFig. 7 :Fig. 8 :</head><label>78</label><figDesc>Fig. 7: The rank difference of the prediction made by model built using the residual-based method, the rank-based methods, and FLASH. Note that the y-axis of this chart rises to large values; e.g., SS-M has 239,260 possible configurations. Hence, the above charts could be summarized as follows: "the FLASH is surprisingly accurate since the rank difference is usually close to 0% of the total number of possible configurations." SS-A2 1 Rank-based</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: Measurements required to find better configurations with the residual-based method as the baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>WinFig. 10 :</head><label>10</label><figDesc>Fig. 10: The time required to find better solutions using ePAL and Flash (sum of 20 repeats). Note that the axis's of the first two figures (left, and center) are in log scale. The time required for FLASH compared to (variants of) ePAL is much lower (with an exception on 2 of 15 software systems). The dashed line in the figure (left and middle) represents cases where ePAL did not terminate within a reasonable time (10 hours). In the right-hand figure, we show the performance gain (wrt. to time) achieved by using FLASH. All the bars above the dashed line (y=1) performs worse than FLASH.</figDesc><graphic coords="14,48.00,263.83,520.23,198.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 :</head><label>11</label><figDesc>Fig.11:The trade-off between the number of starting samples (exploration) and number of steps to converge (exploitation). The ideal point in these trade-off curves is (0,0), which mean the algorithm has perfect convergence (GD = 0) and perfect diversity (IGD = 0). The tradeoff curve for multi-objective performance configuration optimization is shown with budget of 50 evaluations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 : Configuration problems explored in this article.</head><label>1</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TSE.2018.2870895, IEEE Transactions on Software Engineering</figDesc><table /><note><p><p><p>0098-5589 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p>IEEE TRANS SE. SUBMITTED NOV</p>'17  3    </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>7  • Randomize the order of rows in the training data While the stopping criterion (budget) is not met, continue adding configurations to the training set.</figDesc><table><row><cell>• Do</cell></row><row><cell>-Select 30 configurations (by sampling with replacement)</cell></row><row><cell>and add them to the training set</cell></row><row><cell>-Determine the performance scores associated with the</cell></row><row><cell>configurations. This corresponds to a table look up, but</cell></row><row><cell>would entail compiling or configuring and executing a</cell></row><row><cell>system configuration in a practical setting.</cell></row><row><cell>-Using the training set, build a performance model using</cell></row><row><cell>CART.</cell></row><row><cell>-Using the CART model, find the configuration with best</cell></row><row><cell>predicted performance.</cell></row><row><cell>-Add the configuration with best predicted performance</cell></row><row><cell>to the training set.</cell></row></table><note><p>•</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TSE.2018.2870895, IEEE Transactions on Software Engineering</figDesc><table><row><cell cols="6">IEEE TRANS SE. SUBMITTED NOV'17</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>12</cell></row><row><cell></cell><cell>140</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Residual-based</cell><cell></cell><cell></cell><cell cols="4">Rank-based</cell><cell></cell><cell></cell><cell cols="2">Flash</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Evaluations as % of Residual-based method</cell><cell>20 40 60 80 100 120</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>SS-A1</cell><cell>SS-A2</cell><cell>SS-B1</cell><cell>SS-B2</cell><cell>SS-C1</cell><cell>SS-C2</cell><cell>SS-D1</cell><cell>SS-D2</cell><cell>SS-E1</cell><cell>SS-E2</cell><cell>SS-F1</cell><cell>SS-F2</cell><cell>SS-G1</cell><cell>SS-G2</cell><cell>SS-H1</cell><cell>SS-H2</cell><cell>SS-I1</cell><cell>SS-I2</cell><cell>SS-J1</cell><cell>SS-J2</cell><cell>SS-K1</cell><cell>SS-K2</cell><cell>SS-L1</cell><cell>SS-L2</cell><cell>SS-M1</cell><cell>SS-M2</cell><cell>SS-N1</cell><cell>SS-N2</cell><cell>SS-O1</cell><cell>SS-O2</cell></row></table><note><p><p>10. The median rank difference is 1.61, 2.583, and 1.28.</p>0098-5589 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>•</head><label></label><figDesc>The obvious feature of Table2is that FLASH used fewer measurements in 12 of 15 software systems. in log scale), while the y-axis represents the time taken to perform 20 repeats in seconds (in log scale), which means lower the better. The dotted lines in the figure, shows the cases where a method (in this case, ePAL) did not terminate. The sub-figure in the middle represents how the run-time varies with the number of configuration options. The x-axis represents the number of configuration options, and the y-axis represents the time taken for 20 repeats in seconds (in log scale), which means lower the better. The sub-figure to the right represent the performance gain achieved by FLASH over (variants of) ePAL. The x-axis shows the software systems, and the Y-axis represents the gain ratio. Any bar higher than the line (y=1) represent cases where FLASH is better than ePAL. From the figure, we observe: • From sub-figures left and middle, FLASH is much faster than (variants of) ePAL except in 2 of 15 cases. • The run times of ePAL increase exponentially with the number of configurations and configuration options, similar to the trend reported in the literature. • (Variants of) ePAL does not terminate for cases with large numbers of configurations and configuration options, whereas FLASH always terminates an order of magnitude faster than ePAL. This effect is magnified in case of a scenarios with large configuration space. Overall our results indicate:</figDesc><table><row><cell>FLASH requires fewer measurements than ePAL to</cell></row><row><cell>approximate Pareto-optimal solutions. The number of</cell></row><row><cell>evaluations used by FLASH is less than (more careful)</cell></row><row><cell>ePAL-0.01 for all the software systems and 12 of 15</cell></row><row><cell>software systems for (more careless) ePAL-0.3.</cell></row><row><cell>RQ5:Does FLASH save time for multi-objective perfor-</cell></row><row><cell>mance configuration optimization compared to ePAL?</cell></row><row><cell>Figure 10 compares the run times of ePAL with FLASH.</cell></row></table><note><p>Please note that we use the author's version of ePAL in our experiments, which is implemented in Matlab. However, FLASH was implemented in Python. Even though this may not be a fair comparison, for the sake of completeness, we report the run-times of the test. The sub-figure to the left shows how the run times vary with the number of configurations of the system. The x-axis represents the number of configurations (</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 2 : Statistical comparisons of FLASH and ePAL regarding the Performance measures are GD (Generational Distance), IGD (Inverted Generational Distance) and a number of measurements. For all measures, less is better; X denotes cases where methods did not terminate within a reasonable amount of time (10hrs). The numbers in bold represent statistically better runs than the rest. For example, for SS-G, GD of FLASH is statistically better than of ePAL.</head><label>2</label><figDesc></figDesc><table><row><cell>Software</cell><cell></cell><cell>GD</cell><cell></cell><cell></cell><cell>IGD</cell><cell></cell><cell></cell><cell>Evals</cell><cell></cell></row><row><cell></cell><cell>epal 0.01</cell><cell>epal 0.3</cell><cell>FLASH</cell><cell>epal 0.01</cell><cell>epal 0.3</cell><cell>FLASH</cell><cell>epal 0.01</cell><cell>epal 0.3</cell><cell>FLASH</cell></row><row><cell>SS-A</cell><cell>0.002</cell><cell>0.002</cell><cell>0</cell><cell>0.002</cell><cell>0.002</cell><cell>0</cell><cell>109.5</cell><cell>73.5</cell><cell>50</cell></row><row><cell>SS-B</cell><cell>0</cell><cell>0</cell><cell>0.005</cell><cell>0</cell><cell>0.003</cell><cell>0.001</cell><cell>84.5</cell><cell>20</cell><cell>50</cell></row><row><cell>SS-C</cell><cell>0.001</cell><cell>0.001</cell><cell>0.003</cell><cell>0.004</cell><cell>0.004</cell><cell>0</cell><cell>247</cell><cell>101</cell><cell>50</cell></row><row><cell>SS-D</cell><cell>0</cell><cell>0.004</cell><cell>0.014</cell><cell>0.002</cell><cell>0.007</cell><cell>0.009</cell><cell>119.5</cell><cell>67</cell><cell>50</cell></row><row><cell>SS-E</cell><cell>0.001</cell><cell>0.001</cell><cell>0.012</cell><cell>0.004</cell><cell>0.008</cell><cell>0.002</cell><cell>208</cell><cell>54.5</cell><cell>50</cell></row><row><cell>SS-F</cell><cell>0</cell><cell>0.016</cell><cell>0.008</cell><cell>0</cell><cell>0.006</cell><cell>0.016</cell><cell>138</cell><cell>71</cell><cell>50</cell></row><row><cell>SS-G</cell><cell>0</cell><cell>0</cell><cell>0.023</cell><cell>0.003</cell><cell>0.006</cell><cell>0.004</cell><cell>131</cell><cell>69</cell><cell>50</cell></row><row><cell>SS-H</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>52</cell><cell>28</cell><cell>50</cell></row><row><cell>SS-I</cell><cell>0.008</cell><cell>0.018</cell><cell>0</cell><cell>0.008</cell><cell>0.018</cell><cell>0</cell><cell>48</cell><cell>30</cell><cell>50</cell></row><row><cell>SS-J</cell><cell>0</cell><cell>0</cell><cell>0.002</cell><cell>0.002</cell><cell>0.002</cell><cell>0</cell><cell>186</cell><cell>30</cell><cell>50</cell></row><row><cell>SS-K</cell><cell>0.001</cell><cell>0.001</cell><cell>0.003</cell><cell>0.001</cell><cell>0.002</cell><cell>0.001</cell><cell>209</cell><cell>140</cell><cell>50</cell></row><row><cell>SS-L</cell><cell>0.01</cell><cell>0.028</cell><cell>0.006</cell><cell>0.007</cell><cell>0.008</cell><cell>0.009</cell><cell>68.5</cell><cell>35</cell><cell>50</cell></row><row><cell>SS-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>The testing data consist of the configurations as well as the corresponding performance scores.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>Refer to Section 3.1 for definitions.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">ACKNOWLEDGEMENT</head><p>Apel's work has been supported by the German Research Foundation (AP 206/6, AP 206/7, and AP 206/11). Siegmund's work is supported by the DFG under the contracts SI 2171/2 and SI 2171/3-1.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Vivek Nair is a fifth year Ph.D. student in department of Computer Science at North Carolina State University. He received his bachelor and master degree in West Bengal University of Technology and National Institute of Technology, Durgapur respectively. His primary interest lies in the exploring possibilities of using multiobjective optimization to solve problems in Software Engineering. He is currently working on performance prediction models of highly configurable systems. He received his master degree and worked in the mobile industry for a period of 2 years before returning to graduate school.</p><p>Zhe Yu is a third year Ph.D. student in omputer Science at North Carolina State University. He received his masters degree in Shanghai Jiao Tong University, China. His primary interest lies in the collaboration of human and machine learning algorithms that leads to better performance and higher efficiency. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">SUBMITTED NOV&apos;17 13 REFERENCES</title>
		<author>
			<persName><surname>Ieee</surname></persName>
		</author>
		<author>
			<persName><surname>Se</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic database management system tuning through large-scale machine learning</title>
		<author>
			<persName><forename type="first">Dana</forename><surname>Van Aken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Pavlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bohan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Management of Data</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Starfish: A self-tuning system for big data analytics</title>
		<author>
			<persName><forename type="first">Herodotos</forename><surname>Herodotou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harold</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nedyalko</forename><surname>Borisov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Innovative Data Systems Research</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hey, you have given me too many knobs!: understanding and dealing with over-designed configuration in system software</title>
		<author>
			<persName><forename type="first">Tianyin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuepeng</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shankar</forename><surname>Pasupathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rukma</forename><surname>Talwadker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations of Software Engineering</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Optimized cost per click in taobao display advertising</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junqi</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cherrypick: Adaptively unearthing the best cloud configurations for big data analytics</title>
		<author>
			<persName><forename type="first">Omid</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harry</forename><surname>Hongqiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianshu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivaram</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlan</forename><surname>Venkataraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Networked Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Practical bayesian optimization of machine learning algorithms</title>
		<author>
			<persName><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A tutorial on bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Brochu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nando</forename><forename type="middle">De</forename><surname>Vlad M Cora</surname></persName>
		</author>
		<author>
			<persName><surname>Freitas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Variability-aware performance prediction: A statistical learning approach</title>
		<author>
			<persName><forename type="first">Jianmei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Apel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norbert</forename><surname>Siegmund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrzej</forename><surname>Wasowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automated Software Engineering</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Cost-efficient sampling for performance prediction of configurable systems (t)</title>
		<author>
			<persName><forename type="first">Atri</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norbert</forename><surname>Siegmund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Apel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Czarnecki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>In Automated Software Engineering</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Faster discovery of faster system configurations with spectral learning</title>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Menzies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norbert</forename><surname>Siegmund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Apel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Automated Software Engineering</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Using bad learners to find good configurations</title>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Menzies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norbert</forename><surname>Siegmund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Apel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations of Software Engineering</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dataefficient performance learning for configurable systems</title>
		<author>
			<persName><forename type="first">Jianmei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norbert</forename><surname>Siegmund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Apel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atrisha</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Valov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrzej</forename><surname>Wasowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiqun</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Empirical Software Engineering</title>
		<imprint>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">and Markus P üschel. ε-pal: an active learning approach to the multi-objective optimization problem</title>
		<author>
			<persName><forename type="first">Marcela</forename><surname>Zuluaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bayesian optimization in a billion dimensions via random embeddings</title>
		<author>
			<persName><forename type="first">Ziyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masrour</forename><surname>Zoghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Matheson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nando</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feitas</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An uncertainty-aware approach to optimal configuration of stream processing systems</title>
		<author>
			<persName><forename type="first">Pooyan</forename><surname>Jamshidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuliano</forename><surname>Casale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Modeling, Analysis and Simulation of Computer and Telecommunication Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Predicting performance via automated feature-interaction detection</title>
		<author>
			<persName><forename type="first">Norbert</forename><surname>Siegmund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sergiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Kästner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Don</forename><surname>Apel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marko</forename><surname>Batory</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunter</forename><surname>Rosenm</surname></persName>
		</author>
		<author>
			<persName><surname>Saake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Software Engineering</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Searching for better configurations: a rigorous approach to clone evaluation</title>
		<author>
			<persName><forename type="first">Tiantian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Harman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Krinke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations of Software Engineering</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Active learning for multi-objective optimization</title>
		<author>
			<persName><forename type="first">Marcela</forename><surname>Zuluaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Sergent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><forename type="middle">P</forename><surname>Üschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference of Machine Learning</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Is sampling better than evolution for search-based software engineering</title>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Menzies</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Combining multi-objective search and constraint solving for configuring large software product lines</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Henard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Papadakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Harman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yves</forename><surname>Le Traon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Software Engineering</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures</title>
		<author>
			<persName><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Wei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Menzies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Shen</surname></persName>
		</author>
		<title level="m">Tuning for software analytics: Is it really necessary? Information and Software Technology</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Easy over hard: A case study on deep learning</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Menzies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations of Software Engineering</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Why is differential evolution better than grid search for tuning defect predictors</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Menzies</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02613</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Automated parameter optimization of classification techniques for defect prediction models</title>
		<author>
			<persName><forename type="first">Chakkrit</forename><surname>Tantithamthavorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Mcintosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><forename type="middle">E</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenichi</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Software Engineering</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">What is wrong with topic modeling?(and how to fix it using search-based se)</title>
		<author>
			<persName><forename type="first">Amritanshu</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Menzies</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ernest: Efficient performance prediction for largescale advanced analytics</title>
		<author>
			<persName><forename type="first">Shivaram</forename><surname>Venkataraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Networked Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Selecting the best vm across multiple public clouds: A data-driven performance modeling approach</title>
		<author>
			<persName><forename type="first">J</forename><surname>Neeraja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Yadwadkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Burton</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Randy</forename><forename type="middle">H</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Cloud Computing</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bestconfig: Tapping the performance potential of systems via automatic configuration tuning</title>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengying</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yungang</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenlong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoyue</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunpeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingchun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Cloud Computing</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Boat: Building auto-tuners with structured bayesian optimization</title>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Dalibard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schaarschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eiko</forename><surname>Yoneki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee</title>
		<meeting>the 26th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hothardening: getting more out of your security settings</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Biedermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Katzenbeisser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Szefer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Security Applications Conference</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Leveraging virtual machine introspection for hot-hardening of arbitrary cloud-user applications</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Biedermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Katzenbeisser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Szefer</surname></persName>
		</author>
		<editor>HotCloud</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Method and apparatus for automatic configuration and management of a virtual private network</title>
		<author>
			<persName><forename type="first">John</forename><surname>Drabik</surname></persName>
		</author>
		<idno>App. 10/460</idno>
		<imprint>
			<date type="published" when="2003-10-08">October 8 2003</date>
			<biblScope unit="page">518</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">US Patent</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<ptr target="http://files.asset.microfocus.com/" />
		<title level="m">Hpe security research</title>
		<imprint>
			<date type="published" when="2015">4aa5-0858/en/ 4aa5-0858.pdf, 2015. Nov-2017</date>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An efficient bandit algorithm for realtime multivariate optimization</title>
		<author>
			<persName><forename type="first">Houssam</forename><surname>Daniel N Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Nassif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anand</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Beyond ranking: Optimizing whole-page presentation</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Makoto</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Web Search and Data Mining</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Performance-influence models for highly configurable systems</title>
		<author>
			<persName><forename type="first">Norbert</forename><surname>Siegmund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Grebhahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Apel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Kästner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations of Software Engineering</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fast gaussian process regression using kd-trees</title>
		<author>
			<persName><forename type="first">Yirong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Seeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Gale: Geometric active learning for search-based software engineering</title>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Krall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Menzies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Misty</forename><surname>Davies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Classification and regression trees</title>
		<author>
			<persName><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerome</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">A</forename><surname>Olshen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>Wadsworth &amp; Brooks/Cole Advanced Books &amp; Software</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Moea/d: A multiobjective evolutionary algorithm based on decomposition</title>
		<author>
			<persName><forename type="first">Qingfu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Modular mechanisms for bayesian optimization</title>
		<author>
			<persName><forename type="first">W</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bobak</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><surname>Shahriari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Bayesian Optimization</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Algorithms for hyper-parameter optimization</title>
		<author>
			<persName><forename type="first">Rémi</forename><surname>James S Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bardenet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balázs</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Kégl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Search based software engineering: A comprehensive analysis and review of trends techniques and applications</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Harman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afshin</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanyuan</forename><surname>Zhang</surname></persName>
		</author>
		<idno>TR-09-03</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
			<pubPlace>Kings College London</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multi-objective software effort estimation</title>
		<author>
			<persName><forename type="first">Federica</forename><surname>Sarro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessio</forename><surname>Petrozziello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Software Engineering</title>
		<meeting>the 38th International Conference on Software Engineering</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="619" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Beyond evolutionary algorithms for search-based software engineering</title>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Menzies</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Google vizier: A service for black-box optimization</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Daniel Golovin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhodeep</forename><surname>Solnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Moitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Kochanski</surname></persName>
		</author>
		<author>
			<persName><surname>Karro</surname></persName>
		</author>
		<author>
			<persName><surname>Sculley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Taking the human out of the loop: A review of bayesian optimization</title>
		<author>
			<persName><forename type="first">Bobak</forename><surname>Shahriari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nando</forename><forename type="middle">De</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="148" to="175" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Gaussian processes in machine learning</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rasmussen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced lectures on machine learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="63" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A fast decision tree learning algorithm</title>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harry</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Mining high-speed data streams</title>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Hulten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the sixth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="71" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">A fast and elitist multiobjective genetic algorithm: Nsga-ii</title>
		<author>
			<persName><forename type="first">Kalyanmoy</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amrit</forename><surname>Pratap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><surname>Meyarivan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Finding nearoptimal configurations in product lines by random sampling</title>
		<author>
			<persName><forename type="first">Jeho</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Don</forename><surname>Batory</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norbert</forename><surname>Siegmund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations of Software Engineering</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A practical guide to select quality indicators for assessing pareto-based search algorithms in search-based software engineering</title>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaukat</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marius</forename><surname>Liaaen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Software Engineering</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Multiobjective evolutionary algorithms: Classifications, analyses, and new innovations</title>
		<author>
			<persName><forename type="first">David</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Van</forename><surname>Veldhuizen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A study of the parallelization of a coevolutionary multi-objective evolutionary algorithm</title>
		<author>
			<persName><forename type="first">Carlos</forename><forename type="middle">A</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Coello</forename><surname>Coello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margarita</forename><forename type="middle">Reyes</forename><surname>Sierra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mexican International Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Ranking and clustering software cost estimation models through a multiple comparisons algorithm</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mittas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Angelis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">An Introduction to the Bootstrap</title>
		<author>
			<persName><forename type="first">B</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>CRC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Evaluating prediction systems in software project estimation</title>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">J</forename><surname>Shepperd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">G</forename><surname>Macdonell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information &amp; Software Technology</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="820" to="827" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A systematic review of effect size in software engineering experiments</title>
		<author>
			<persName><forename type="first">By</forename><surname>Vigdis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tore</forename><surname>Kampenes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jo</forename><forename type="middle">Erskine</forename><surname>Dybå</surname></persName>
		</author>
		<author>
			<persName><surname>Hannay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">K</forename><surname>Dag</surname></persName>
		</author>
		<author>
			<persName><surname>Sjøberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information &amp; Software Technology</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">11-12</biblScope>
			<biblScope unit="page" from="1073" to="1086" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ekrem</forename><surname>Kocaguneli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bird</surname></persName>
		</author>
		<imprint>
			<publisher>Nachiappan Na</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
