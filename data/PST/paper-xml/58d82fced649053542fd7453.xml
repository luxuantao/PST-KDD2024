<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-12-06">6 Dec 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wasserstein</forename><surname>Gan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Courant Institute of Mathematical Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Courant Institute of Mathematical Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-12-06">6 Dec 2017</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1701.07875v3[stat.ML]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The problem this paper is concerned with is that of unsupervised learning. Mainly, what does it mean to learn a probability distribution? The classical answer to this is to learn a probability density. This is often done by defining a parametric family of densities (P θ ) θ∈R d and finding the one that maximized the likelihood on our data: if we have real data examples {x (i) } m i=1 , we would solve the problem max</p><formula xml:id="formula_0">θ∈R d 1 m m i=1 log P θ (x (i) )</formula><p>If the real data distribution P r admits a density and P θ is the distribution of the parametrized density P θ , then, asymptotically, this amounts to minimizing the Kullback-Leibler divergence KL(P r P θ ).</p><p>For this to make sense, we need the model density P θ to exist. This is not the case in the rather common situation where we are dealing with distributions supported by low dimensional manifolds. It is then unlikely that the model manifold and the true distribution's support have a non-negligible intersection (see <ref type="bibr" target="#b0">[1]</ref>), and this means that the KL distance is not defined (or simply infinite).</p><p>The typical remedy is to add a noise term to the model distribution. This is why virtually all generative models described in the classical machine learning literature include a noise component. In the simplest case, one assumes a Gaussian noise with relatively high bandwidth in order to cover all the examples. It is well known, for instance, that in the case of image generation models, this noise degrades the quality of the samples and makes them blurry. For example, we can see in the recent paper <ref type="bibr" target="#b22">[23]</ref> that the optimal standard deviation of the noise added to the model when maximizing likelihood is around 0.1 to each pixel in a generated image, when the pixels were already normalized to be in the range <ref type="bibr">[0,</ref><ref type="bibr" target="#b0">1]</ref>. This is a very high amount of noise, so much that when papers report the samples of their models, they don't add the noise term on which they report likelihood numbers. In other words, the added noise term is clearly incorrect for the problem, but is needed to make the maximum likelihood approach work.</p><p>Rather than estimating the density of P r which may not exist, we can define a random variable Z with a fixed distribution p(z) and pass it through a parametric function g θ : Z → X (typically a neural network of some kind) that directly generates samples following a certain distribution P θ . By varying θ, we can change this distribution and make it close to the real data distribution P r . This is useful in two ways. First of all, unlike densities, this approach can represent distributions confined to a low dimensional manifold. Second, the ability to easily generate samples is often more useful than knowing the numerical value of the density (for example in image superresolution or semantic segmentation when considering the conditional distribution of the output image given the input image). In general, it is computationally difficult to generate samples given an arbitrary high dimensional density <ref type="bibr" target="#b15">[16]</ref>.</p><p>Variational Auto-Encoders (VAEs) <ref type="bibr" target="#b8">[9]</ref> and Generative Adversarial Networks (GANs) <ref type="bibr" target="#b3">[4]</ref> are well known examples of this approach. Because VAEs focus on the approximate likelihood of the examples, they share the limitation of the standard models and need to fiddle with additional noise terms. GANs offer much more flexibility in the definition of the objective function, including Jensen-Shannon <ref type="bibr" target="#b3">[4]</ref>, and all f -divergences <ref type="bibr" target="#b16">[17]</ref> as well as some exotic combinations <ref type="bibr" target="#b5">[6]</ref>. On the other hand, training GANs is well known for being delicate and unstable, for reasons theoretically investigated in <ref type="bibr" target="#b0">[1]</ref>.</p><p>In this paper, we direct our attention on the various ways to measure how close the model distribution and the real distribution are, or equivalently, on the various ways to define a distance or divergence ρ(P θ , P r ). The most fundamental difference between such distances is their impact on the convergence of sequences of probability distributions. A sequence of distributions (P t ) t∈N converges if and only if there is a distribution P ∞ such that ρ(P t , P ∞ ) tends to zero, something that depends on how exactly the distance ρ is defined. Informally, a distance ρ induces a weaker topology when it makes it easier for a sequence of distribution to converge. <ref type="foot" target="#foot_0">1</ref>Section 2 clarifies how popular probability distances differ in that respect.</p><p>In order to optimize the parameter θ, it is of course desirable to define our model distribution P θ in a manner that makes the mapping θ → P θ continuous. Continuity means that when a sequence of parameters θ t converges to θ, the distributions P θt also converge to P θ . However, it is essential to remember that the notion of the convergence of the distributions P θt depends on the way we compute the distance between distributions. The weaker this distance, the easier it is to define a continuous mapping from θ-space to P θ -space, since it's easier for the distributions to converge. The main reason we care about the mapping θ → P θ to be continuous is as follows. If ρ is our notion of distance between two distributions, we would like to have a loss function θ → ρ(P θ , P r ) that is continuous, and this is equivalent to having the mapping θ → P θ be continuous when using the distance between distributions ρ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The contributions of this paper are:</head><p>• In Section 2, we provide a comprehensive theoretical analysis of how the Earth Mover (EM) distance behaves in comparison to popular probability distances and divergences used in the context of learning distributions.</p><p>• In Section 3, we define a form of GAN called Wasserstein-GAN that minimizes a reasonable and efficient approximation of the EM distance, and we theoretically show that the corresponding optimization problem is sound.</p><p>• In Section 4, we empirically show that WGANs cure the main training problems of GANs. In particular, training WGANs does not require maintaining a careful balance in training of the discriminator and the generator, and does not require a careful design of the network architecture either. The mode dropping phenomenon that is typical in GANs is also drastically reduced.</p><p>One of the most compelling practical benefits of WGANs is the ability to continuously estimate the EM distance by training the discriminator to optimality. Plotting these learning curves is not only useful for debugging and hyperparameter searches, but also correlate remarkably well with the observed sample quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Different Distances</head><p>We now introduce our notation. Let X be a compact metric set (such as the space of images [0, 1] d ) and let Σ denote the set of all the Borel subsets of X . Let Prob(X ) denote the space of probability measures defined on X . We can now define elementary distances and divergences between two distributions P r , P g ∈ Prob(X ):</p><p>• The Total Variation (TV) distance</p><formula xml:id="formula_1">δ(P r , P g ) = sup A∈Σ |P r (A) − P g (A)| .</formula><p>• The Kullback-Leibler (KL) divergence</p><formula xml:id="formula_2">KL(P r P g ) = log P r (x) P g (x) P r (x)dµ(x) ,</formula><p>where both P r and P g are assumed to be absolutely continuous, and therefore admit densities, with respect to a same measure µ defined on X . <ref type="foot" target="#foot_1">2</ref> The KL divergence is famously assymetric and possibly infinite when there are points such that P g (x) = 0 and P r (x) &gt; 0.</p><p>• The Jensen-Shannon (JS) divergence JS(P r , P g ) = KL(P r P m ) + KL(P g P m ) ,</p><p>where P m is the mixture (P r + P g )/2. This divergence is symmetrical and always defined because we can choose µ = P m .</p><p>• The Earth-Mover (EM) distance or Wasserstein-1</p><formula xml:id="formula_3">W (P r , P g ) = inf γ∈Π(Pr,Pg) E (x,y)∼γ x − y ,<label>(1)</label></formula><p>where Π(P r , P g ) denotes the set of all joint distributions γ(x, y) whose marginals are respectively P r and P g . Intuitively, γ(x, y) indicates how much "mass" must be transported from x to y in order to transform the distributions P r into the distribution P g . The EM distance then is the "cost" of the optimal transport plan.</p><p>The following example illustrates how apparently simple sequences of probability distributions converge under the EM distance but do not converge under the other distances and divergences defined above.</p><p>Example 1 (Learning parallel lines). Let Z ∼ U [0, 1] the uniform distribution on the unit interval. Let P 0 be the distribution of (0, Z) ∈ R 2 (a 0 on the x-axis and the random variable Z on the y-axis), uniform on a straight vertical line passing through the origin. Now let g θ (z) = (θ, z) with θ a single real parameter. It is easy to see that in this case, if θ = 0 . When θ t → 0, the sequence (P θt ) t∈N converges to P 0 under the EM distance, but does not converge at all under either the JS, KL, reverse KL, or TV divergences. Figure <ref type="figure">1</ref> illustrates this for the case of the EM and JS distances.</p><p>Example 1 gives us a case where we can learn a probability distribution over a low dimensional manifold by doing gradient descent on the EM distance. This cannot be done with the other distances and divergences because the resulting loss function is not even continuous. Although this simple example features distributions with disjoint supports, the same conclusion holds when the supports have a non empty Figure <ref type="figure">1</ref>: These plots show ρ(P θ , P0) as a function of θ when ρ is the EM distance (left plot) or the JS divergence (right plot). The EM plot is continuous and provides a usable gradient everywhere. The JS plot is not continuous and does not provide a usable gradient. intersection contained in a set of measure zero. This happens to be the case when two low dimensional manifolds intersect in general position <ref type="bibr" target="#b0">[1]</ref>.</p><p>Since the Wasserstein distance is much weaker than the JS distance<ref type="foot" target="#foot_2">3</ref> , we can now ask whether W (P r , P θ ) is a continuous loss function on θ under mild assumptions. This, and more, is true, as we now state and prove.</p><p>Theorem 1. Let P r be a fixed distribution over X . Let Z be a random variable (e.g Gaussian) over another space Z. Let g : Z × R d → X be a function, that will be denoted g θ (z) with z the first coordinate and θ the second. Let P θ denote the distribution of g θ (Z). Then, 1. If g is continuous in θ, so is W (P r , P θ ).</p><p>2. If g is locally Lipschitz and satisfies regularity assumption 1, then W (P r , P θ ) is continuous everywhere, and differentiable almost everywhere.</p><p>3. Statements 1-2 are false for the Jensen-Shannon divergence JS(P r , P θ ) and all the KLs.</p><p>Proof. See Appendix C</p><p>The following corollary tells us that learning by minimizing the EM distance makes sense (at least in theory) with neural networks.</p><p>Corollary 1. Let g θ be any feedforward neural network<ref type="foot" target="#foot_3">4</ref> parameterized by θ, and p(z) a prior over z such that E z∼p(z) [ z ] &lt; ∞ (e.g. Gaussian, uniform, etc.).</p><p>Then assumption 1 is satisfied and therefore W (P r , P θ ) is continuous everywhere and differentiable almost everywhere.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof. See Appendix C</head><p>All this shows that EM is a much more sensible cost function for our problem than at least the Jensen-Shannon divergence. The following theorem describes the relative strength of the topologies induced by these distances and divergences, with KL the strongest, followed by JS and TV, and EM the weakest.</p><p>Theorem 2. Let P be a distribution on a compact space X and (P n ) n∈N be a sequence of distributions on X . Then, considering all limits as n → ∞, 1. The following statements are equivalent • δ(P n , P) → 0 with δ the total variation distance.</p><p>• JS(P n , P) → 0 with JS the Jensen-Shannon divergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The following statements are equivalent</head><p>• W (P n , P) → 0. 3. KL(P n P) → 0 or KL(P P n ) → 0 imply the statements in (1).</p><p>4. The statements in (1) imply the statements in (2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof. See Appendix C</head><p>This highlights the fact that the KL, JS, and TV distances are not sensible cost functions when learning distributions supported by low dimensional manifolds. However the EM distance is sensible in that setup. This obviously leads us to the next section where we introduce a practical approximation of optimizing the EM distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Wasserstein GAN</head><p>Again, Theorem 2 points to the fact that W (P r , P θ ) might have nicer properties when optimized than JS(P r , P θ ). However, the infimum in (1) is highly intractable. On the other hand, the Kantorovich-Rubinstein duality <ref type="bibr" target="#b21">[22]</ref> tells us that</p><formula xml:id="formula_4">W (P r , P θ ) = sup f L ≤1 E x∼Pr [f (x)] − E x∼P θ [f (x)]<label>(2)</label></formula><p>where the supremum is over all the 1-Lipschitz functions f : X → R. Note that if we replace f L ≤ 1 for f L ≤ K (consider K-Lipschitz for some constant K), then we end up with K • W (P r , P g ). Therefore, if we have a parameterized family of functions {f w } w∈W that are all K-Lipschitz for some K, we could consider solving the problem max</p><formula xml:id="formula_5">w∈W E x∼Pr [f w (x)] − E z∼p(z) [f w (g θ (z)]<label>(3)</label></formula><p>and if the supremum in ( <ref type="formula" target="#formula_4">2</ref>) is attained for some w ∈ W (a pretty strong assumption akin to what's assumed when proving consistency of an estimator), this process would yield a calculation of W (P r , P θ ) up to a multiplicative constant. Furthermore, we could consider differentiating W (P r , P θ ) (again, up to a constant) by back-proping through equation ( <ref type="formula" target="#formula_4">2</ref>) via estimating</p><formula xml:id="formula_6">E z∼p(z) [∇ θ f w (g θ (z))].</formula><p>While this is all intuition, we now prove that this process is principled under the optimality assumption.</p><p>Theorem 3. Let P r be any distribution. Let P θ be the distribution of g θ (Z) with Z a random variable with density p and g θ a function satisfying assumption 1. Then, there is a solution f : X → R to the problem</p><formula xml:id="formula_7">max f L ≤1 E x∼Pr [f (x)] − E x∼P θ [f (x)]</formula><p>and we have</p><formula xml:id="formula_8">∇ θ W (P r , P θ ) = −E z∼p(z) [∇ θ f (g θ (z))]</formula><p>when both terms are well-defined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof. See Appendix C</head><p>Now comes the question of finding the function f that solves the maximization problem in equation <ref type="bibr" target="#b1">(2)</ref>. To roughly approximate this, something that we can do is train a neural network parameterized with weights w lying in a compact space W and then backprop through E z∼p(z) [∇ θ f w (g θ (z))], as we would do with a typical GAN. Note that the fact that W is compact implies that all the functions f w will be K-Lipschitz for some K that only depends on W and not the individual weights, therefore approximating (2) up to an irrelevant scaling factor and the capacity of the 'critic' f w . In order to have parameters w lie in a compact space, something simple we can do is clamp the weights to a fixed box (say W = [−0.01, 0.01] l ) after each gradient update. The Wasserstein Generative Adversarial Network (WGAN) procedure is described in Algorithm 1.</p><p>Weight clipping is a clearly terrible way to enforce a Lipschitz constraint. If the clipping parameter is large, then it can take a long time for any weights to reach their limit, thereby making it harder to train the critic till optimality. If the clipping is small, this can easily lead to vanishing gradients when the number of layers is big, or batch normalization is not used (such as in RNNs). We experimented with simple variants (such as projecting the weights to a sphere) with little difference, and we stuck with weight clipping due to its simplicity and already good performance. However, we do leave the topic of enforcing Lipschitz constraints in a neural network setting for further investigation, and we actively encourage interested researchers to improve on this method.</p><p>Algorithm 1 WGAN, our proposed algorithm. All experiments in the paper used the default values α = 0.00005, c = 0.01, m = 64, n critic = 5.</p><p>Require: : α, the learning rate. c, the clipping parameter. m, the batch size.</p><p>n critic , the number of iterations of the critic per generator iteration. Require: : w 0 , initial critic parameters. θ 0 , initial generator's parameters.</p><p>1: while θ has not converged do 2:</p><p>for t = 0, ..., n critic do 3:</p><p>Sample {x (i) } m i=1 ∼ P r a batch from the real data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Sample {z (i) } m i=1 ∼ p(z) a batch of prior samples.</p><p>5:</p><formula xml:id="formula_9">g w ← ∇ w 1 m m i=1 f w (x (i) ) − 1 m m i=1 f w (g θ (z (i) )) 6: w ← w + α • RMSProp(w, g w ) 7: w ← clip(w, −c, c) 8:</formula><p>end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>Sample {z (i) } m i=1 ∼ p(z) a batch of prior samples.</p><p>10:</p><formula xml:id="formula_10">g θ ← −∇ θ 1 m m i=1 f w (g θ (z (i) )) 11: θ ← θ − α • RMSProp(θ, g θ ) 12: end while</formula><p>The fact that the EM distance is continuous and differentiable a.e. means that we can (and should) train the critic till optimality. The argument is simple, the more we train the critic, the more reliable gradient of the Wasserstein we get, which is actually useful by the fact that Wasserstein is differentiable almost everywhere. For the JS, as the discriminator gets better the gradients get more reliable but the true gradient is 0 since the JS is locally saturated and we get vanishing gradients, as can be seen in Figure <ref type="figure">1</ref> of this paper and Theorem 2.4 of <ref type="bibr" target="#b0">[1]</ref>. In Figure <ref type="figure" target="#fig_1">2</ref> we show a proof of concept of this, where we train a GAN discriminator and a WGAN critic till optimality. The discriminator learns very quickly to distinguish between fake and real, and as expected provides no reliable gradient information. The critic, however, can't saturate, and converges to a linear function that gives remarkably clean gradients everywhere. The fact that we constrain the weights limits the possible growth of the function to be at most linear in different parts of the space, forcing the optimal critic to have this behaviour.</p><p>Perhaps more importantly, the fact that we can train the critic till optimality makes it impossible to collapse modes when we do. This is due to the fact that mode collapse comes from the fact that the optimal generator for a fixed discriminator is a sum of deltas on the points the discriminator assigns the highest values, as observed by <ref type="bibr" target="#b3">[4]</ref> and highlighted in <ref type="bibr" target="#b10">[11]</ref>.</p><p>In the following section we display the practical benefits of our new algorithm, and we provide an in-depth comparison of its behaviour and that of traditional GANs. As we can see, the discriminator of a minimax GAN saturates and results in vanishing gradients. Our WGAN critic provides very clean gradients on all parts of the space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Empirical Results</head><p>We run experiments on image generation using our Wasserstein-GAN algorithm and show that there are significant practical benefits to using it over the formulation used in standard GANs.</p><p>We claim two main benefits:</p><p>• a meaningful loss metric that correlates with the generator's convergence and sample quality</p><p>• improved stability of the optimization process</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Procedure</head><p>We run experiments on image generation. The target distribution to learn is the LSUN-Bedrooms dataset <ref type="bibr" target="#b23">[24]</ref> -a collection of natural images of indoor bedrooms. Our baseline comparison is DCGAN <ref type="bibr" target="#b17">[18]</ref>, a GAN with a convolutional architecture trained with the standard GAN procedure using the − log D trick <ref type="bibr" target="#b3">[4]</ref>. The generated samples are 3-channel images of 64x64 pixels in size. We use the hyper-parameters specified in Algorithm 1 for all of our experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Meaningful loss metric</head><p>Because the WGAN algorithm attempts to train the critic f (lines 2-8 in Algorithm 1) relatively well before each generator update (line 10 in Algorithm 1), the loss function at this point is an estimate of the EM distance, up to constant factors related to the way we constrain the Lipschitz constant of f . Our first experiment illustrates how this estimate correlates well with the quality of the generated samples. Besides the convolutional DCGAN architecture, we also ran experiments where we replace the generator or both the generator and the critic by 4-layer ReLU-MLP with 512 hidden units.</p><p>Figure <ref type="figure" target="#fig_2">3</ref> plots the evolution of the WGAN estimate (3) of the EM distance during WGAN training for all three architectures. The plots clearly show that these curves correlate well with the visual quality of the generated samples.</p><p>To our knowledge, this is the first time in GAN literature that such a property is shown, where the loss of the GAN shows properties of convergence. This property is extremely useful when doing research in adversarial networks as one does not need to stare at the generated samples to figure out failure modes and to gain information on which models are doing better over others.</p><p>However, we do not claim that this is a new method to quantitatively evaluate generative models yet. The constant scaling factor that depends on the critic's architecture means it's hard to compare models with different critics. Even more, in practice the fact that the critic doesn't have infinite capacity makes it hard to know just how close to the EM distance our estimate really is. This being said, we have succesfully used the loss metric to validate our experiments repeatedly and without failure, and we see this as a huge improvement in training GANs which previously had no such facility.</p><p>In contrast, Figure <ref type="figure" target="#fig_3">4</ref> plots the evolution of the GAN estimate of the JS distance during GAN training. More precisely, during GAN training, the discriminator is trained to maximize</p><formula xml:id="formula_11">L(D, g θ ) = E x∼Pr [log D(x)] + E x∼P θ [log(1 − D(x))]</formula><p>which is is a lower bound of 2JS(P r , P θ )−2 log 2. In the figure, we plot the quantity 1 2 L(D, g θ ) + log 2, which is a lower bound of the JS distance.</p><p>This quantity clearly correlates poorly the sample quality. Note also that the JS estimate usually stays constant or goes up instead of going down. In fact it often remains very close to log 2 ≈ 0.69 which is the highest value taken by the JS distance. In other words, the JS distance saturates, the discriminator has zero loss, and the generated samples are in some cases meaningful (DCGAN generator, top right plot) and in other cases collapse to a single nonsensical image <ref type="bibr" target="#b3">[4]</ref>. This last phenomenon has been theoretically explained in <ref type="bibr" target="#b0">[1]</ref> and highlighted in <ref type="bibr" target="#b10">[11]</ref>. When using the − log D trick <ref type="bibr" target="#b3">[4]</ref>, the discriminator loss and the generator loss are different. Figure <ref type="figure" target="#fig_9">8</ref> in Appendix E reports the same plots for GAN training, but using the generator loss instead of the discriminator loss. This does not change the conclusions.</p><p>Finally, as a negative result, we report that WGAN training becomes unstable at times when one uses a momentum based optimizer such as Adam <ref type="bibr" target="#b7">[8]</ref> (with β 1 &gt; 0) on the critic, or when one uses high learning rates. Since the loss for the critic is nonstationary, momentum based methods seemed to perform worse. We identified momentum as a potential cause because, as the loss blew up and samples got worse, the cosine between the Adam step and the gradient usually turned negative. The only places where this cosine was negative was in these situations of instability. We therefore switched to RMSProp <ref type="bibr" target="#b20">[21]</ref> which is known to perform well even on very nonstationary problems <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Improved stability</head><p>One of the benefits of WGAN is that it allows us to train the critic till optimality. When the critic is trained to completion, it simply provides a loss to the generator that we can train as any other neural network. This tells us that we no longer need to balance generator and discriminator's capacity properly. The better the critic, the higher quality the gradients we use to train the generator.</p><p>We observe that WGANs are much more robust than GANs when one varies the architectural choices for the generator. We illustrate this by running experiments on three generator architectures: (1) a convolutional DCGAN generator, (2) a convolutional DCGAN generator without batch normalization and with a constant number of filters, and (3) a 4-layer ReLU-MLP with 512 hidden units. The last two are known to perform very poorly with GANs. We keep the convolutional DCGAN architecture for the WGAN critic or the GAN discriminator.</p><p>Figures <ref type="figure" target="#fig_4">5, 6</ref>, and 7 show samples generated for these three architectures using both the WGAN and GAN algorithms. We refer the reader to Appendix F for full sheets of generated samples. Samples were not cherry-picked.</p><p>In no experiment did we see evidence of mode collapse for the WGAN algorithm. Figure <ref type="figure">6</ref>: Algorithms trained with a generator without batch normalization and constant number of filters at every layer (as opposed to duplicating them every time as in <ref type="bibr" target="#b17">[18]</ref>). Aside from taking out batch normalization, the number of parameters is therefore reduced by a bit more than an order of magnitude. Left: WGAN algorithm. Right: standard GAN formulation. As we can see the standard GAN failed to learn while the WGAN still was able to produce samples.</p><p>Figure <ref type="figure">7</ref>: Algorithms trained with an MLP generator with 4 layers and 512 units with ReLU nonlinearities. The number of parameters is similar to that of a DCGAN, but it lacks a strong inductive bias for image generation. Left: WGAN algorithm. Right: standard GAN formulation. The WGAN method still was able to produce samples, lower quality than the DCGAN, and of higher quality than the MLP of the standard GAN. Note the significant degree of mode collapse in the GAN MLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>There's been a number of works on the so called Integral Probability Metrics (IPMs) <ref type="bibr" target="#b14">[15]</ref>. Given F a set of functions from X to R, we can define</p><formula xml:id="formula_12">d F (P r , P θ ) = sup f ∈F E x∼Pr [f (x)] − E x∼P θ [f (x)]<label>(4)</label></formula><p>as an integral probability metric associated with the function class F. It is easily verified that if for every f ∈ F we have −f ∈ F (such as all examples we'll consider), then d F is nonnegative, satisfies the triangular inequality, and is symmetric. Thus, d F is a pseudometric over Prob(X ). While IPMs might seem to share a similar formula, as we will see different classes of functions can yeald to radically different metrics.</p><p>• By the Kantorovich-Rubinstein duality <ref type="bibr" target="#b21">[22]</ref>, we know that W (P r , P θ ) = d F (P r , P θ ) when F is the set of 1-Lipschitz functions. Furthermore, if F is the set of K-Lipschitz functions, we get K • W (P r , P θ ) = d F (P r , P θ ).</p><p>• When F is the set of all measurable functions bounded between -1 and 1 (or all continuous functions between -1 and 1), we retrieve d F (P r , P θ ) = δ(P r , P θ ) the total variation distance <ref type="bibr" target="#b14">[15]</ref>. This already tells us that going from 1-Lipschitz to 1-Bounded functions drastically changes the topology of the space, and the regularity of d F (P r , P θ ) as a loss function (as by Theorems 1 and 2).</p><p>• Energy-based GANs (EBGANs) <ref type="bibr" target="#b24">[25]</ref> can be thought of as the generative approach to the total variation distance. This connection is stated and proven in depth in Appendix D. At the core of the connection is that the discriminator will play the role of f maximizing equation ( <ref type="formula" target="#formula_12">4</ref>) while its only restriction is being between 0 and m for some constant m. This will yeald the same behaviour as being restricted to be between −1 and 1 up to a constant scaling factor irrelevant to optimization. Thus, when the discriminator approaches optimality the cost for the generator will aproximate the total variation distance δ(P r , P θ ).</p><p>Since the total variation distance displays the same regularity as the JS, it can be seen that EBGANs will suffer from the same problems of classical GANs regarding not being able to train the discriminator till optimality and thus limiting itself to very imperfect gradients.</p><p>• Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b4">[5]</ref> is a specific case of integral probability metrics when F = {f ∈ H : f ∞ ≤ 1} for H some Reproducing Kernel Hilbert Space (RKHS) associated with a given kernel k : X × X → R. As proved on <ref type="bibr" target="#b4">[5]</ref> we know that MMD is a proper metric and not only a pseudometric when the kernel is universal. In the specific case where H = L 2 (X , m) for m the normalized Lebesgue measure on X , we know that {f ∈ C b (X ), f ∞ ≤ 1} will be contained in F, and therefore d F (P r , P θ ) ≤ δ(P r , P θ ) so the regularity of the MMD distance as a loss function will be at least as bad as the one of the total variation. Nevertheless this is a very extreme case, since we would need a very powerful kernel to approximate the whole L 2 . However, even Gaussian kernels are able to detect tiny noise patterns as recently evidenced by <ref type="bibr" target="#b19">[20]</ref>. This points to the fact that especially with low bandwidth kernels, the distance might be close to a saturating regime similar as with total variation or the JS. This obviously doesn't need to be the case for every kernel, and figuring out how and which different MMDs are closer to Wasserstein or total variation distances is an interesting topic of research.</p><p>The great aspect of MMD is that via the kernel trick there is no need to train a separate network to maximize equation ( <ref type="formula" target="#formula_12">4</ref>) for the ball of a RKHS. However, this has the disadvantage that evaluating the MMD distance has computational cost that grows quadratically with the amount of samples used to estimate the expectations in (4). This last point makes MMD have limited scalability, and is sometimes inapplicable to many real life applications because of it. There are estimates with linear computational cost for the MMD <ref type="bibr" target="#b4">[5]</ref> which in a lot of cases makes MMD very useful, but they also have worse sample complexity.</p><p>• Generative Moment Matching Networks (GMMNs) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b1">2]</ref> are the generative counterpart of MMD. By backproping through the kernelized formula for equation ( <ref type="formula" target="#formula_12">4</ref>), they directly optimize d M M D (P r , P θ ) (the IPM when F is as in the previous item). As mentioned, this has the advantage of not requiring a separate network to approximately maximize equation ( <ref type="formula" target="#formula_12">4</ref>). However, GMMNs have enjoyed limited applicability. Partial explanations for their unsuccess are the quadratic cost as a function of the number of samples and vanishing gradients for low-bandwidth kernels. Furthermore, it may be possible that some kernels used in practice are unsuitable for capturing very complex distances in high dimensional sample spaces such as natural images. This is properly justified by the fact that <ref type="bibr" target="#b18">[19]</ref> shows that for the typical Gaussian MMD test to be reliable (as in it's power as a statistical test approaching 1), we need the number of samples to grow linearly with the number of dimensions. Since the MMD computational cost grows quadratically with the number of samples in the batch used to estimate equation ( <ref type="formula" target="#formula_12">4</ref>), this makes the cost of having a reliable estimator grow quadratically with the number of dimensions, which makes it very inapplicable for high dimensional problems. Indeed, for something as standard as 64x64 images, we would need minibatches of size at least 4096 (without taking into account the constants in the bounds of <ref type="bibr" target="#b18">[19]</ref> which would make this number substantially larger) and a total cost per iteration of 4096 2 , over 5 orders of magnitude more than a GAN iteration when using the standard batch size of 64.</p><p>That being said, these numbers can be a bit unfair to the MMD, in the sense that we are comparing empirical sample complexity of GANs with the theoretical sample complexity of MMDs, which tends to be worse. However, in the original GMMN paper <ref type="bibr" target="#b9">[10]</ref> they indeed used a minibatch of size 1000, much larger than the standard 32 or 64 (even when this incurred in quadratic computational cost). While estimates that have linear computational cost as a function of the number of samples exist <ref type="bibr" target="#b4">[5]</ref>, they have worse sample complexity, and to the best of our knowledge they haven't been yet applied in a generative context such as in GMMNs.</p><p>On another great line of research, the recent work of <ref type="bibr" target="#b13">[14]</ref> has explored the use of Wasserstein distances in the context of learning for Restricted Boltzmann Machines for discrete spaces. The motivations at a first glance might seem quite different, since the manifold setting is restricted to continuous spaces and in finite discrete spaces the weak and strong topologies (the ones of W and JS respectively) coincide. However, in the end there is more in commmon than not about our motivations. We both want to compare distributions in a way that leverages the geometry of the underlying space, and Wasserstein allows us to do exactly that.</p><p>Finally, the work of <ref type="bibr" target="#b2">[3]</ref> shows new algorithms for calculating Wasserstein distances between different distributions. We believe this direction is quite important, and perhaps could lead to new ways of evaluating generative models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We introduced an algorithm that we deemed WGAN, an alternative to traditional GAN training. In this new model, we showed that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we showed that the corresponding optimization problem is sound, and provided extensive theoretical work highlighting the deep connections to other distances between distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Why Wasserstein is indeed weak</head><p>We now introduce our notation. Let X ⊆ R d be a compact set (such as [0, 1] d the space of images). We define Prob(X ) to be the space of probability measures over X . We note</p><formula xml:id="formula_13">C b (X ) = {f : X → R, f is continuous and bounded} Note that if f ∈ C b (X ), we can define f ∞ = max x∈X |f (x)|, since f is bounded.</formula><p>With this norm, the space (C b (X ), • ∞ ) is a normed vector space. As for any normed vector space, we can define its dual</p><formula xml:id="formula_14">C b (X ) * = {φ : C b (X ) → R, φ is linear and continuous} and give it the dual norm φ = sup f ∈C b (X ), f ∞≤1 |φ(f )|.</formula><p>With this definitions, (C b (X ) * , • ) is another normed space. Now let µ be a signed measure over X , and let us define the total variation distance</p><formula xml:id="formula_15">µ T V = sup A⊆X |µ(A)|</formula><p>where the supremum is taken all Borel sets in X . Since the total variation is a norm, then if we have P r and P θ two probability distributions over X , δ(P r , P θ ) := P r − P θ T V is a distance in Prob(X ) (called the total variation distance).</p><p>We can consider Φ : (Prob(X ), δ) → (C b (X ) * , • )</p><p>where Φ(P)(f ) := E x∼P [f (x)] is a linear function over C b (X ). The Riesz Representation theorem ( <ref type="bibr" target="#b6">[7]</ref>, Theorem 10) tells us that Φ is an isometric immersion. This tells us that we can effectively consider Prob(X ) with the total variation distance as a subset of C b (X ) * with the norm distance. Thus, just to accentuate it one more time, the total variation over Prob(X ) is exactly the norm distance over C b (X ) * . Let us stop for a second and analyze what all this technicality meant. The main thing to carry is that we introduced a distance δ over probability distributions. When looked as a distance over a subset of C b (X ) * , this distance gives the norm topology. The norm topology is very strong. Therefore, we can expect that not many functions θ → P θ will be continuous when measuring distances between distributions with δ. As we will show later in Theorem 2, δ gives the same topology as the Jensen-Shannon divergence, pointing to the fact that the JS is a very strong distance, and is thus more propense to give a discontinuous loss function. Now, all dual spaces (such as C b (X ) * and thus Prob(X )) have a strong topology (induced by the norm), and a weak* topology. As the name suggests, the weak* topology is much weaker than the strong topology. In the case of Prob(X ), the strong topology is given by the total variation distance, and the weak* topology is given by the Wasserstein distance (among others) <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Assumption definitions</head><p>Assumption 1. Let g : Z × R d → X be locally Lipschitz between finite dimensional vector spaces. We will denote g θ (z) it's evaluation on coordinates (z, θ). We say that g satisfies assumption 1 for a certain probability distribution p over Z if there are local Lipschitz constants L(θ, z) such that</p><formula xml:id="formula_16">E z∼p [L(θ, z)] &lt; +∞</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Proofs of things</head><p>Proof of Theorem 1. Let θ and θ be two parameter vectors in R d . Then, we will first attempt to bound W (P θ , P θ ), from where the theorem will come easily. The main element of the proof is the use of the coupling γ, the distribution of the joint (g θ (Z), g θ (Z)), which clearly has γ ∈ Π(P θ , P θ ).</p><p>By the definition of the Wasserstein distance, we have</p><formula xml:id="formula_17">W (P θ , P θ ) ≤ X ×X x − y dγ = E (x,y)∼γ [ x − y ] = E z [ g θ (z) − g θ (z) ]</formula><p>If g is continuous in θ, then g θ (z) → θ→θ g θ (z), so g θ − g θ → 0 pointwise as functions of z. Since X is compact, the distance of any two elements in it has to be uniformly bounded by some constant M , and therefore g θ (z) − g θ (z) ≤ M for all θ and z uniformly. By the bounded convergence theorem, we therefore have</p><formula xml:id="formula_18">W (P θ , P θ ) ≤ E z [ g θ (z) − g θ (z) ] → θ→θ 0</formula><p>Finally, we have that |W (P r , P θ ) − W (P r , P θ )| ≤ W (P θ , P θ ) → θ→θ 0 proving the continuity of W (P r , P θ ). Now let g be locally Lipschitz. Then, for a given pair (θ, z) there is a constant L(θ, z) and an open set U such that (θ, z) ∈ U , such that for every (θ , z</p><formula xml:id="formula_19">) ∈ U we have g θ (z) − g θ (z ) ≤ L(θ, z)( θ − θ + z − z )</formula><p>By taking expectations and z = z we</p><formula xml:id="formula_20">E z [ g θ (z) − g θ (z) ] ≤ θ − θ E z [L(θ, z)]</formula><p>whenever (θ , z) ∈ U . Therefore, we can define U θ = {θ |(θ , z) ∈ U }. It's easy to see that since U was open, U θ is as well. Furthermore, by assumption 1, we can define L(θ) = E z [L(θ, z)] and achieve</p><formula xml:id="formula_21">|W (P r , P θ ) − W (P r , P θ )| ≤ W (P θ , P θ ) ≤ L(θ) θ − θ</formula><p>for all θ ∈ U θ , meaning that W (P r , P θ ) is locally Lipschitz. This obviously implies that W (P r , P θ ) is everywhere continuous, and by Radamacher's theorem we know it has to be differentiable almost everywhere. The counterexample for item 3 of the Theorem is indeed Example 1.</p><p>Proof of Corollary 1. We begin with the case of smooth nonlinearities. Since g is C 1 as a function of (θ, z) then for any fixed (θ, z) we have L(θ, Z) ≤ ∇ θ,x g θ (z) + is an acceptable local Lipschitz constant for all &gt; 0. Therefore, it suffices to prove</p><formula xml:id="formula_22">E z∼p(z) [ ∇ θ,z g θ (z) ] &lt; +∞</formula><p>If H is the number of layers we know that ∇ z g θ (z) = H k=1 W k D k where W k are the weight matrices and D k is are the diagonal Jacobians of the nonlinearities. Let f i:j be the application of layers i to j inclusively (e.g. g θ = f 1:H ). Then,</p><formula xml:id="formula_23">∇ W k g θ (z) = H i=k+1 W i D i D k f 1:k−1 (z). We recall that if L is the Lipschitz constant of the nonlinearity, then D i ≤ L and f 1:k−1 (z) ≤ z L k−1 k−1 i=1 W i . Putting this together, ∇ z,θ g θ (z) ≤ H i=1 W i D i + H k=1 H i=k+1 W i D i D k f 1:k−1 (z) ≤ L H K i=H W i + H k=1 z L H k−1 i=1 W i H i=k+1 W i If C 1 (θ) = L H H i=1 W i and C 2 (θ) = H k=1 L H k−1 i=1 W i H i=k+1 W i then E z∼p(z) [ ∇ θ,z g θ (z) ] ≤ C 1 (θ) + C 2 (θ)E z∼p(z) [ z ] &lt; +∞ finishing the proof Proof of Theorem 2.</formula><p>1.</p><p>• (δ(P n , P) → 0 ⇒ JS(P n , P) → 0) -Let P m be the mixture distribution P m = 1 2 P n + 1 2 P (note that P m depends on n). It is easily verified that δ(P m , P n ) ≤ δ(P n , P), and in particular this tends to 0 (as does δ(P m , P)). We now show this for completeness. Let µ be a signed measure, we define µ T V = sup A⊆X |µ(A)|. for all Borel sets A. In this case,</p><formula xml:id="formula_24">δ(P m , P n ) = P m − P n T V = 1 2 P + 1 2 P n − P n T V = 1 2 P − P n T V = 1 2</formula><p>δ(P n , P) ≤ δ(P n , P)</p><p>Let f n = dPn dPm be the Radon-Nykodim derivative between P n and the mixture. Note that by construction for every Borel set A we have P n (A) ≤ 2P m (A). If A = {f n &gt; 3} then we get</p><formula xml:id="formula_25">P n (A) = A f n dP m ≥ 3P m (A)</formula><p>which implies P m (A) = 0. This means that f n is bounded by 3 P m (and therefore P n and P)-almost everywhere. We could have done this for any constant larger than 2 but for our purposes 3 will sufice. Let &gt; 0 fixed, and A n = {f n &gt; 1 + }. Then,</p><formula xml:id="formula_26">P n (A n ) = An f n dP m ≥ (1 + )P m (A n ) Therefore, P m (A n ) ≤ P n (A n ) − P m (A n ) ≤ |P n (A n ) − P m (A n )| ≤ δ(P n , P m )</formula><p>≤ δ(P n , P).</p><p>Which implies P m (A m ) ≤ 1 δ(P n , P). Furthermore, We now can see that</p><formula xml:id="formula_27">P n (A n ) ≤ P m (A n ) + |P n (A n ) − P m (A n )| ≤<label>1</label></formula><formula xml:id="formula_28">KL(P n P m ) = log(f n ) dP n ≤ log(1 + ) + An log(f n ) dP n ≤ log(1 + ) + log(3)P n (A n ) ≤ log(1 + ) + log(3) 1 + 1 δ(P n , P)</formula><p>Taking limsup we get 0 ≤ lim sup KL(P n P m ) ≤ log(1 + ) for all &gt; 0, which means KL(P n P m ) → 0.</p><p>In the same way, we can define g n = dP dPm , and 2P m ({g n &gt; 3}) ≥ P({g n &gt; 3}) ≥ 3P m ({g n &gt; 3}) meaning that P m ({g n &gt; 3}) = 0 and therefore g n is bounded by 3 almost everywhere for P n , P m and P. With the same calculation, B n = {g n &gt; 1 + } and ≤ 2 JS(P n , P) → 0 2. This is a long known fact that W metrizes the weak* topology of (C(X ), • ∞ ) on Prob(X ), and by definition this is the topology of convergence in distribution. A proof of this can be found (for example) in <ref type="bibr" target="#b21">[22]</ref>.</p><formula xml:id="formula_29">P(B n ) = Bn g n dP m ≥ (1 + )P m (B n ) so P m (B n ) ≤ 1 δ(P, P m ) → 0,</formula><p>3. This is a straightforward application of Pinsker's inequality</p><formula xml:id="formula_30">δ(P n , P) ≤ 1 2 KL(P n P) → 0 δ(P, P n ) ≤ 1 2 KL(P P n ) →<label>0</label></formula><p>4. This is trivial by recalling the fact that δ and W give the strong and weak* topologies on the dual of (C(X ), • ∞ ) when restricted to Prob(X ).</p><p>Proof of Theorem 3. Let us define</p><formula xml:id="formula_31">V ( f , θ) = E x∼Pr [ f (x)] − E x∼P θ [ f (x)] = E x∼Pr [ f (x)] − E z∼p(z) [ f (g θ (z))] where f lies in F = { f : X → R , f ∈ C b (X ), f L ≤ 1} and θ ∈ R d .</formula><p>Since X is compact, we know by the Kantorovich-Rubenstein duality <ref type="bibr" target="#b21">[22]</ref> that there is an f ∈ F that attains the value</p><formula xml:id="formula_32">W (P r , P θ ) = sup f ∈F V ( f , θ) = V (f, θ)</formula><p>Let us define X * (θ) = {f ∈ F : V (f, θ) = W (P r , P θ )}. By the above point we know then that X * (θ) is non-empty. We know that by a simple envelope theorem ( <ref type="bibr" target="#b11">[12]</ref>, Theorem 1) that</p><formula xml:id="formula_33">∇ θ W (P r , P θ ) = ∇ θ V (f, θ)</formula><p>for any f ∈ X * (θ) when both terms are well-defined. Let f ∈ X * (θ), which we knows exists since X * (θ) is non-empty for all θ. Then, we get</p><formula xml:id="formula_34">∇ θ W (P r , P θ ) = ∇ θ V (f, θ) = ∇ θ [E x∼Pr [f (x)] − E z∼p(z) [f (g θ (z))] = −∇ θ E z∼p(z) [f (g θ (z))]</formula><p>under the condition that the first and last terms are well-defined. The rest of the proof will be dedicated to show that</p><formula xml:id="formula_35">−∇ θ E z∼p(z) [f (g θ (z))] = −E z∼p(z) [∇ θ f (g θ (z))]<label>(5)</label></formula><p>when the right hand side is defined. For the reader who is not interested in such technicalities, he or she can skip the rest of the proof. Since f ∈ F, we know that it is 1-Lipschitz. Furthermore, g θ (z) is locally Lipschitz as a function of (θ, z). Therefore, f (g θ (z)) is locally Lipschitz on (θ, z) with constants L(θ, z) (the same ones as g). By Radamacher's Theorem, f (g θ (z)) has to be differentiable almost everywhere for (θ, z) jointly. Rewriting this, the set A = {(θ, z) : f • g is not differentiable} has measure 0. By Fubini's Theorem, this implies that for almost every θ the section A θ = {z : (θ, z) ∈ A} has measure 0. Let's now fix a θ 0 such that the measure of A θ0 is null (such as when the right hand side of equation ( <ref type="formula" target="#formula_35">5</ref>) is well defined). For this θ 0 we have ∇ θ f (g θ (z))| θ0 is well-defined for almost any z, and since p(z) has a density, it is defined p(z)-a.e.</p><p>By assumption 1 we know that and since E z∼p(z) [2L(θ 0 , z)] &lt; +∞ by assumption 1, we get by dominated convergence that Equation 6 converges to 0 as θ → θ 0 so</p><formula xml:id="formula_36">∇ θ E z∼p(z) [f (g θ (z))] = E z∼p(z) [∇ θ f (g θ (z))]</formula><p>for almost every θ, and in particular when the right hand side is well defined. Note that the mere existance of the left hand side (meaning the differentiability a.e. of E z∼p(z) [f (g θ (z))]) had to be proven, which we just did.</p><p>Therefore, we know that inf This is a long known fact, found for example in <ref type="bibr" target="#b21">[22]</ref>, but we prove it later for completeness. In that case, we define D * (x) = m 2 f * (x) + m 2 . We then have 0 ≤ D(x) ≤ m and For completeness, we now show a proof for equation <ref type="bibr" target="#b6">(7)</ref> and the existence of said f * that attains the value of the infimum. Take µ = P r − P θ , which is a signed measure, and (P, Q) its Hahn decomposition. Then, we can define f * := 1 Q − 1 P .        </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>By construction, then</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Generator's cost during normal GAN training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Sheets of samples</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>convergence in distribution for random variables.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Optimal discriminator and critic when learning to differentiate two Gaussians.As we can see, the discriminator of a minimax GAN saturates and results in vanishing gradients. Our WGAN critic provides very clean gradients on all parts of the space.</figDesc><graphic url="image-3.png" coords="9,162.00,126.00,288.00,216.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Training curves and samples at different stages of training. We can see a clear correlation between lower error and better sample quality. Upper left: the generator is an MLP with 4 hidden layers and 512 units at each layer. The loss decreases constistently as training progresses and sample quality increases. Upper right: the generator is a standard DCGAN. The loss decreases quickly and sample quality increases as well. In both upper plots the critic is a DCGAN without the sigmoid so losses can be subjected to comparison. Lower half: both the generator and the discriminator are MLPs with substantially high learning rates (so training failed). Loss is constant and samples are constant as well. The training curves were passed through a median filter for visualization purposes.</figDesc><graphic url="image-6.png" coords="10,222.75,238.90,166.50,111.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: JS estimates for an MLP generator (upper left) and a DCGAN generator (upper right) trained with the standard GAN procedure. Both had a DCGAN discriminator. Both curves have increasing error. Samples get better for the DCGAN but the JS estimate increases or stays constant, pointing towards no significant correlation between sample quality and loss. Bottom: M LP with both generator and discriminator. The curve goes up and down regardless of sample quality. All training curves were passed through the same median filter as in Figure 3.</figDesc><graphic url="image-9.png" coords="11,221.85,238.00,168.30,104.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Algorithms trained with a DCGAN generator. Left: WGAN algorithm. Right: standard GAN formulation. Both algorithms produce high quality samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>E</head><label></label><figDesc>z∼p(z) [ ∇ θ f (g θ (z))| θ0 ] ≤ E z∼p(z) [L(θ 0 , z)] &lt; +∞ so E z∼p(z) [∇ θ f (g θ (z))| θ0] is well-defined for almost every θ 0 . Now, we can seeE z∼p(z) [f (g θ (z))] − E z∼p(z) [f (g θ0 (z))] − (θ − θ 0 ), E z∼p(z) [∇ θ f (g θ (z))| θ0 ] θ − θ 0 (6) = E z∼p(z) f (g θ (z)) − f (g θ0 (z)) − (θ − θ 0 ), ∇ θ f (g θ (z))| θ0 θ − θ 0By differentiability, the term inside the integral converges p(z)-a.e. to 0 as θ → θ 0 . Furthermore,f (g θ (z)) − f (g θ0 (z)) − (θ − θ 0 ), ∇ θ f (g θ (z))| θ0 θ − θ 0 ≤ θ − θ 0 L(θ 0 , z) + θ − θ 0 ∇ θ f (g θ (z))| θ0 θ − θ 0 ≤ 2L(θ 0 , z)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>0≤D(x) ≤m L 2 EE</head><label>≤m2</label><figDesc>D (D, g θ ) = m + inf 0≤D(x)≤m E x∼Pr [D(x)] − E x∼P θ [D(x)] = m + inf − m 2 ≤D(x)≤ m x∼Pr [D(x)] − E x∼P θ [D(x)] x∼Pr [f (x)] − E x∼P θ [f (x)]The interesting part is that inf−1≤f (x)≤1 E x∼Pr [f (x)] − E x∼P θ [f (x)] = −δ(P r , P θ )(7)and there is anf * : X → [−1, 1] such that E x∼Pr [f * (x)]−E x∼P θ [f * (x)] = −δ(P r , P θ ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>L 2 E</head><label>2</label><figDesc>D (D * , g θ ) = m + E x∼Pr [D * (x)] − E x∼P θ [D * (x)] = m + m x∼Pr [D * (x)] − E x∼P θ [f * (x)] = m − m 2 δ(P r , P θ ) = inf 0≤D(x)≤m L D (D, g θ )This shows that D * is optimal and L D (D * , g θ ) = m − m 2 δ(P r , P θ ). Furthermore,L G (D * , g θ ) = E z∼p(z) [D * (g θ (z))] − E x∼Pr [D * (x)] = −L D (D * , g θ ) + m = m2δ(P r , P g ) concluding the proof.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>EE x∼Pr [f * (x)] − E x∼P θ [f * (x)] = f * dµ = µ(Q) − µ(P ) = −(µ(P ) − µ(Q)) = − µ T V = − P r − P θ T V = −δ(P r , P θ )Furthermore, if f is bounded between -1 and 1, we get|E x∼Pr [f (x)] − E x∼P θ [f (x)]| = | f dP r − f dP θ | = | f dµ| ≤ |f | d|µ| ≤ 1 d|µ| = |µ|(X ) = µ T V = δ(P r , P θ ) Since δ is positive, we can conclude E x∼Pr [f (x)] − E x∼P θ [f (x)] ≥ −δ(P r , P θ ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Cost of the generator during normal GAN training, for an MLP generator (upper left) and a DCGAN generator (upper right). Both had a DCGAN discriminator. Both curves have increasing error. Samples get better for the DCGAN but the cost of the generator increases, pointing towards no significant correlation between sample quality and loss. Bottom: M LP with both generator and discriminator. The curve goes up and down regardless of sample quality. All training curves were passed through the same median filter as in Figure 3.</figDesc><graphic url="image-18.png" coords="29,223.95,272.57,164.10,104.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: WGAN algorithm: generator and critic are DCGANs.</figDesc><graphic url="image-19.png" coords="30,156.06,68.72,299.88,299.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Standard GAN procedure: generator and discriminator are DCGANs.</figDesc><graphic url="image-20.png" coords="30,156.06,402.47,299.88,299.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: WGAN algorithm: generator is a DCGAN without batchnorm and constant filter size. Critic is a DCGAN.</figDesc><graphic url="image-21.png" coords="31,156.06,57.76,299.89,299.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Standard GAN procedure: generator is a DCGAN without batchnorm and constant filter size. Discriminator is a DCGAN.</figDesc><graphic url="image-22.png" coords="31,156.06,402.48,299.88,299.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: WGAN algorithm: generator is an MLP with 4 hidden layers of 512 units, critic is a DCGAN.</figDesc><graphic url="image-23.png" coords="32,156.06,63.24,299.88,299.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: GAN procedure: generator is an MLP with 4 hidden layers of 512 units, discriminator is a DCGAN.</figDesc><graphic url="image-24.png" coords="32,156.06,396.99,299.88,299.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>and therefore P(B n ) → 0. We can now showKL(P P m ) = log(g n ) dP</figDesc><table><row><cell>≤ log(1 + ) +</cell><cell>log(g n ) dP</cell></row><row><cell>Bn</cell><cell></cell></row><row><cell cols="2">≤ log(1 + ) + log(3)P(B n )</cell></row><row><cell cols="2">so we achieve 0 ≤ lim sup KL(P P m ) ≤ log(1+ ) and then KL(P P m ) →</cell></row><row><cell>0. Finally, we conclude</cell><cell></cell></row><row><cell cols="2">JS(P n , P) = KL(P P ≤ 1 2 KL(P n P m ) + 1 2 1 2 KL(P n P m ) + 1 2 KL(P P m )</cell></row></table><note>m ) → 0• (JS(P n , P) → 0 ⇒ δ(P n , P) → 0) -by a simple application of the triangular and Pinsker's inequalities we get δ(P n , P) ≤ δ(P n , P m ) + δ(P, P m )</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">More exactly, the topology induced by ρ is weaker than that induced by ρ when the set of convergent sequences under ρ is a superset of that under ρ .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">Recall that a probability distribution Pr ∈ Prob(X ) admits a density pr(x) with respect to µ, that is, ∀A ∈ Σ, Pr(A) = A Pr(x)dµ(x), if and only it is absolutely continuous with respect to µ, that is, ∀A ∈ Σ, µ(A) = 0 ⇒ Pr(A) = 0 .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">The argument for why this happens, and indeed how we arrived to the idea that Wasserstein is what we should really be optimizing is displayed in Appendix A. We strongly encourage the interested reader who is not afraid of the mathematics to go through it.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">By a feedforward neural network we mean a function composed by affine transformations and pointwise nonlinearities which are smooth Lipschitz functions (such as the sigmoid, tanh, elu, softplus, etc). Note: the statement is also true for rectifier nonlinearities but the proof is more technical (even though very similar) so we omit it.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Mohamed Ishmael Belghazi, Emily Denton, Ian Goodfellow, Ishaan Gulrajani, Alex Lamb, David Lopez-Paz, Eric Martin, Maxime Oquab, Aditya Ramesh, Ronan Riochet, Uri Shalit, Pablo Sprechmann, Arthur Szlam, Ruohan Wang, for helpful comments and advice.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Energy-based GANs optimize total variation</head><p>In this appendix we show that under an optimal discriminator, energy-based GANs (EBGANs) <ref type="bibr" target="#b24">[25]</ref> optimize the total variation distance between the real and generated distributions.</p><p>Energy-based GANs are trained in a similar fashion to GANs, only under a different loss function. They have a discriminator D who tries to minimize</p><p>for some m &gt; 0 and [x] + = max(0, x) and a generator network g θ that's trained to minimize</p><p>Very importantly, D is constrained to be non-negative, since otherwise the trivial solution for D would be to set everything to arbitrarily low values. The original EBGAN paper used only E z∼p(z) [D(g θ (z))] for the loss of the generator, but this is obviously equivalent to our definition since the term E x∼Pr [D(x)] does not dependent on θ for a fixed discriminator (such as when backproping to the generator in EBGAN training) and thus minimizing one or the other is equivalent.</p><p>We say that a measurable function D * : X → [0, +∞) is optimal for g θ (or</p><p>for all other measurable functions D. We show that such a discriminator always exists for any two distributions P r and P θ , and that under such a discriminator, L G (D * , g θ ) is proportional to δ(P r , P θ ). As a simple corollary, we get the fact that L G (D * , g θ ) attains its minimum value if and only if δ(P r , P θ ) is at its minimum value, which is 0, and P r = P θ (Theorems 1-2 of <ref type="bibr" target="#b24">[25]</ref>).</p><p>Theorem 4. Let P r be a the real data distribution over a compact space X . Let g θ : Z → X be a measurable function (such as any neural network). Then, an optimal discriminator D * exists for P r and P θ , and</p><p>Proof. First, we prove that there exists an optimal discriminator. Let D : X → [0, +∞) be a measurable function, then D (x) := min(D(x), m) is also a measurable function, and</p><p>We are then interested to see if there's an optimal discriminator for the problem min 0≤D(x)≤m L D (D, g θ ).</p><p>Note now that if 0 ≤ D(x) ≤ m we have </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards principled methods for training generative adversarial networks</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Under review</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Training generative neural networks via maximum mean discrepancy optimization</title>
		<author>
			<persName><forename type="first">Gintare</forename><surname>Karolina Dziugaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno>CoRR, abs/1505.03906</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Stochastic optimization for large-scale optimal transport</title>
		<author>
			<persName><forename type="first">Aude</forename><surname>Genevay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Peyré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3440" to="3448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A kernel two-sample test</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malte</forename><forename type="middle">J</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="723" to="773" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">How (not) to train your generative model: Scheduled sampling, likelihood</title>
		<author>
			<persName><forename type="first">Ferenc</forename><surname>Huszar</surname></persName>
		</author>
		<idno>CoRR, abs/1511.05101</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Concrete representation of abstract (m)-spaces (a characterization of the space of continuous functions)</title>
		<author>
			<persName><forename type="first">Shizuo</forename><surname>Kakutani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematics</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="994" to="1024" />
			<date type="published" when="1941">1941</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno>CoRR, abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno>CoRR, abs/1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative moment matching networks</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML-15)</title>
				<meeting>the 32nd International Conference on Machine Learning (ICML-15)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1718" to="1727" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Unrolled generative adversarial networks</title>
		<author>
			<persName><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<idno>Corr, abs/1611.02163</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Envelope theorems for arbitrary choice sets</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Milgrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Segal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="583" to="601" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrià</forename><surname>Puigdomènech Badia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning, ICML 2016</title>
				<meeting>the 33nd International Conference on Machine Learning, ICML 2016<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">June 19-24, 2016. 2016</date>
			<biblScope unit="page" from="1928" to="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Wasserstein training of restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3718" to="3726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Integral probability metrics and their generating classes of functions</title>
		<author>
			<persName><forename type="first">Alfred</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Applied Probability</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="429" to="443" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Annealed importance sampling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="125" to="139" />
			<date type="published" when="2001-04">April 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">f-gan: Training generative neural samplers using variational divergence minimization</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Botond</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryota</forename><surname>Tomioka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="271" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno>CoRR, abs/1511.06434</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">On the high-dimensional power of linear-time kernel two-sample testing under mean-difference alternatives</title>
		<author>
			<persName><forename type="first">Aaditya</forename><surname>Ramdas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sashank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barnabas</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aarti</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><surname>Wasserman</surname></persName>
		</author>
		<idno>Corr, abs/1411.6314</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generative models and model criticism via optimized maximum mean discrepancy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dougal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiao-Yu</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiko</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumyajit</forename><surname>Strathmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaditya</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ramdas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><surname>Gretton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Under review</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Lecture 6.5-RmsProp: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COURSERA: Neural Networks for Machine Learning</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Cédric</forename><surname>Villani</surname></persName>
		</author>
		<title level="m">Optimal Transport: Old and New. Grundlehren der mathematischen Wissenschaften</title>
				<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">On the quantitative analysis of decoder-based generative models</title>
		<author>
			<persName><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
		<idno>CoRR, abs/1611.04273</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">LSUN: Construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<author>
			<persName><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<idno>Corr, abs/1506.03365</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Energy-based generative adversarial network</title>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno>Corr, abs/1609.03126</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
