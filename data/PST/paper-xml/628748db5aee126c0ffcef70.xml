<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Selecting Stickers in Open-Domain Dialogue through Multitask Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-09-16">16 Sep 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhexin</forename><surname>Zhang</surname></persName>
							<email>zhang18@mails.tsinghua.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Yeshuang</forename><surname>Zhu</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Pattern Recognition Center</orgName>
								<orgName type="institution">Tencent Inc</orgName>
								<address>
									<addrLine>WeChat AI</addrLine>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhengcong</forename><surname>Fei</surname></persName>
							<email>feizhengcong@ict.ac.cn</email>
							<affiliation key="aff4">
								<orgName type="department">Pattern Recognition Center</orgName>
								<orgName type="institution">Tencent Inc</orgName>
								<address>
									<addrLine>WeChat AI</addrLine>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jinchao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Pattern Recognition Center</orgName>
								<orgName type="institution">Tencent Inc</orgName>
								<address>
									<addrLine>WeChat AI</addrLine>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Pattern Recognition Center</orgName>
								<orgName type="institution">Tencent Inc</orgName>
								<address>
									<addrLine>WeChat AI</addrLine>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">The CoAI group</orgName>
								<orgName type="department" key="dep2">DCST</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">State Key Lab of Intelligent Technology and Systems</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Selecting Stickers in Open-Domain Dialogue through Multitask Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-09-16">16 Sep 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2209.07697v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the increasing popularity of online chatting, stickers are becoming important in our online communication. Selecting appropriate stickers in open-domain dialogue requires a comprehensive understanding of both dialogues and stickers, as well as the relationship between the two types of modalities. To tackle these challenges, we propose a multitask learning method comprised of three auxiliary tasks to enhance the understanding of dialogue history, emotion and semantic meaning of stickers. Extensive experiments conducted on a recent challenging dataset show that our model can better combine the multimodal information and achieve significantly higher accuracy over strong baselines. Ablation study further verifies the effectiveness of each auxiliary task. Our code is available at https://github.com/ nonstopfor/Sticker-Selection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the development of mobile messaging apps (e.g., WhatsApp and Messenger), visual content is getting more and more frequently used in our daily conversation, such as emojis and stickers. Compared with emojis, stickers are larger images consisting of drawing characters, symbolic icons, and text titles, and are hence more expressive and versatile <ref type="bibr" target="#b8">(Konrad et al., 2020)</ref>. Users send stickers along with text to show intimacy, express strong emotion, and experience the enjoyment of creativity <ref type="bibr" target="#b12">(Tang and Hew, 2019)</ref>.</p><p>Despite the importance of stickers in daily communication, selecting stickers in open-domain dialogue hasn't been widely explored. In this paper, we address the task of selecting an appropriate sticker from a candidate set for an open-domain multi-turn dialogue. This task is a typical setting for various applications, e.g., automatically recommending stickers in messaging apps and building * Work done during internship at WeChat AI.  Given a dialogue history, the model needs to add a sticker to the last textual message which is the most appropriate one among a collection of candidate stickers (the one marked in the red rectangle). The words below in red denote the emotion or meaning of each sticker. more interesting and human-like chatbots which could respond with stickers. As shown in Figure <ref type="figure" target="#fig_1">1</ref>, this task requires an understanding of dialogue context, emotion and semantic meaning of stickers, and a jointly modeling ability for the multimodal information. Only a few previous works have explored this task <ref type="bibr" target="#b7">(Gao et al., 2020;</ref><ref type="bibr" target="#b13">Wang and Jurgens, 2021)</ref>. However, existing models are only trained on an end-to-end matching objective and lacks finer-grained supervision signals which could guide models to understand multimodal information better.</p><p>Considering the challenges of this task and the shortcomings of previous work, we propose a novel multitask learning method to improve sticker selection in open-domain multi-turn dialogue. We design three auxiliary tasks: 1) masked context prediction, which uses multimodal context to predict masked tokens in the dialogue history, aiming to understand the dialogue in the presence of the sticker;</p><p>2) sticker emotion classification, which utilizes the sticker's contextualized representation to predict its emotion, aiming to improve the model's understanding of sticker emotion; 3) sticker semantic prediction, which explicitly instills semantic understanding of stickers by training the model to reconstruct a sticker's semantic label based on the multimodal inputs. Moreover, all these tasks help improve our model's joint modeling capability, as both our model architecture and task design require multimodal inputs and deep interactions between them. We evaluate the performance of our method on a recently proposed and challenging dataset. Extensive experiments show that our multitask method achieves state-of-the-art performance.</p><p>There are two contributions of this paper:</p><p>? We propose a multitask learning method to help select appropriate stickers in opendomain multi-turn dialogue.</p><p>? Experiment results on a challenging dataset demonstrate the effectiveness of each auxiliary task and combining all the tasks achieves state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Sticker selection. Previous works proposed to recommend emojis in dialogue systems based on textual or multimodal context <ref type="bibr" target="#b1">(Barbieri et al., 2018;</ref><ref type="bibr" target="#b15">Xie et al., 2016;</ref><ref type="bibr" target="#b2">Barbieri et al., 2017)</ref>. However, emojis are limited in variety and are much less expressive than stickers. <ref type="bibr" target="#b9">Laddha et al. (2020)</ref> retrieved stickers for generated text utterances by simply matching the text tags of stickers. Several works have proposed improved matching methods for stickers. <ref type="bibr" target="#b7">Gao et al. (2020)</ref> utilized co-attention to capture the interaction between a sticker and each utterance, and used a fusion network to combine the features. <ref type="bibr" target="#b13">Wang and Jurgens (2021)</ref> followed the matching framework of CLIP <ref type="bibr" target="#b11">(Radford et al., 2021)</ref> and designed a multimodal encoder for animated GIFs. <ref type="bibr" target="#b6">Fei et al. (2021)</ref> proposed to generate special sticker tokens along with text utterances using one single GPT <ref type="bibr" target="#b14">(Wang et al., 2020)</ref> for emotion prediction and retrieval of stickers. However, existing models are only trained on an end-to-end matching objective that implicitly guides the models to understand multimodal information. In our work, we design finer-grained auxiliary tasks that instill knowledge of stickers and their contextualized usage in a more efficient way. Visual Dialogue. Visual dialogue is a task to answer questions about the factual content of the real-world image <ref type="bibr" target="#b10">(Liang et al., 2021;</ref><ref type="bibr">Das et al., 2017a,b)</ref>. In contrast, selecting appropriate stickers in open-domain dialogue requires understanding sentiment and semantic expression of usergenerated, artistic style images.</p><p>3 Method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task Definition</head><p>We assume there is a multi-turn dialogue context C = {u 1 , ..., u N }, and a candidate sticker set S = {s 1 , ..., s M }, where u i represents the i-th utterance in the dialogue, and s i represents the i-th candidate sticker. N is the number of utterances in the dialogue and M is the number of candidate stickers. In this work, we suppose that there is only one appropriate sticker s * ? S, and s * and u N belong to the same speaker. The goal is to train a model that can select the right sticker s * among all candidates S given the dialogue history C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Method Overview</head><p>An overview of the design of our training tasks is shown in Figure <ref type="figure" target="#fig_2">2</ref>. Our main task is to decide whether the candidate sticker is appropriate given the dialogue context. To accomplish this task, we concatenate the embedded dialogue context and the sticker embedding as inputs to BERT. Then we apply a binary classification layer on top of the hidden state of the [CLS] token. In order to enhance the model's ability to understand the multimodal input, we design three auxiliary tasks: 1) masked context prediction, which improves the model's understanding of dialogue context; 2) sticker emotion classification, which aims to make the model better understand sticker's emotion; 3) sticker semantic prediction, which instills semantic information of stickers to the model. Next, we will introduce our three auxiliary tasks in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Task 1: Masked Context Prediction</head><p>The masked context prediction task follows the masked language modeling (MLM) task in BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref>. One difference is that we additionally append the embedding of the appropriate sticker to the input embeddings. In this way, the model can learn to utilize stickers for dialogue reconstruction, and thus the interaction between the two modalities is enhanced. The loss for this task is denoted as L ctx , and takes the same form of cross-entropy loss as in the original MLM task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Task 2: Sticker Emotion Classification</head><p>In the dataset we used, stickers are annotated with one context-dependent emotion, which means one sticker could have different emotions in different dialogue contexts. Therefore, we design a sticker emotion classification task to enable the model to utilize the text and sticker information simultaneously for understanding sticker emotion. Specifically, we take the hidden state corresponding to the sticker and apply a softmax layer with crossentropy loss on top of it for emotion classification. The loss for this task is denoted as L emo .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Task 3: Sticker Semantic Prediction</head><p>Task 1 and Task 2 emphasize learning the implicit meaning of stickers and their correlation with dialogue text. However, many stickers express a clear intention that indicates their proper usage context, e.g., greetings and declines. We believe empowering our model to predict and utilize the semantic meaning of stickers is beneficial for our task. Hence, we further design a semantic label prediction task. We modify our model's inputs by inserting a fixed-length sequence of [MASK] tokens after the dialogue. The model is trained to recover the label text from the hidden states of the [MASK] tokens. The loss is formulated as the sum of cross-entropy loss for each token in label and is denoted as L sem . Since the dataset we used has no ground truth semantic labels for stickers, we take the textual information recognized by an OCR tool as semantic labels for stickers. Note that L sem is only applied for those stickers with text recognized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Total Loss</head><p>Besides the above three auxiliary tasks, our main task is a binary classification of whether a candidate sticker is appropriate given the dialogue context. We take all dialogue-sticker pairs in the dataset as positive samples and randomly sample stickers to create an equal number of negative samples. The cross-entropy loss is denoted as L main .</p><p>Our final loss is a combination of the four loss:</p><formula xml:id="formula_0">L = L main + ?L ctx + ?L emo + ?L sem (1)</formula><p>where ?, ?, ? are manually tuned hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We use the Chinese version of the MOD dataset from DSTC10-Track1<ref type="foot" target="#foot_0">1</ref> . The dataset is grounded in a dialogue scenario and contains various stickers with contextualized emotion annotation. We split each dialogue into several samples, each containing a text sequence of dialogue history and an accompanying sticker. Note that this dataset is revised from the unpublished one used in <ref type="bibr" target="#b6">Fei et al. (2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>We compare our model with the following baselines from recent related work: 1) SRS <ref type="bibr" target="#b7">(Gao et al., 2020)</ref>, which encodes dialogue history and candidate sticker separately, and then employs a deep interaction network and a fusion network to score each candidate sticker; 2) MOD-GPT <ref type="bibr" target="#b6">(Fei et al., 2021)</ref>, which uses one single GPT to generate response text and match sticker; 3) CLIP, which finetunes pretrained CLIP <ref type="bibr" target="#b11">(Radford et al., 2021)</ref> for sticker selection using the same contrastive loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results and Analysis</head><p>The result is shown in Table <ref type="table">1</ref>. Our full model (MMBERT+ctx+emo+sem) outperforms all baselines on two test sets, and achieves the best performance in almost all settings. As expected, all the results get worse on the hard test set and when selecting one amongst all stickers. As only one  out of the numerous and various online stickers is considered correct, the task is inherently challenging. We find that CLIP is a strong baseline due to its better generalization ability on the hard set, compared with our base model which has no auxiliary task (MMBERT). This may be because CLIP is pretrained on a large number of imagetext pairs. However, with multitask learning, our full model outperforms CLIP, although BERT has never seen images during pretraining. Thus, we conclude that our multitask learning method can improve sticker selection by explicitly guiding the model to understand multimodal information. We also perform an ablation study to verify the effect of each auxiliary task. A clear trend emerges that the performance improves as each auxiliary training task is added to MMBERT, verifying the efficacy of our task design. One exception is that MMBERT+ctx+emo performs slightly better than our full model in terms of R 10 @2, R 10 @5, and MRR 10 . However, the inconsistency disappears when considering all stickers as candidates. Furthermore, our full model performs significantly better on the hard test set which contains unseen stickers. Hence, we conclude that introducing semantic information improves the model's generalization ability. We also find that our full model achieves 60% accuracy on the validation set for the auxiliary Figure <ref type="figure">6</ref>: A case in which our model's prediction is not the same as the answer but also appropriate. The leftmost sticker is selected by our model among the four candidate stickers. The appropriate sticker is marked with a red rectangle. The red words explain the stickers' emotions or meanings.</p><p>sticker emotion classification task with 52 emotion labels in total, which is reasonable and confirms our model can learn from the auxiliary tasks.</p><p>We visualize the saliency of different words in the dialogue history in Figure <ref type="figure" target="#fig_4">3</ref>, which shows that the more relevant words (e.g., guessed, good and modest) in the dialogue history contribute more to our model's prediction. Notably, our model could attend to some distant words (e.g., good), not just the words inside the previous utterance.</p><p>We also analyze the prediction diversity of our full model. As shown in Figure <ref type="figure" target="#fig_5">4</ref>, the predictions of our model are diverse in general, covering almost all stickers in the whole candidate set. We note that a few stickers are predicted significantly more times than other stickers, which is because they appear much more frequently than other stickers in the training set. We leave addressing the imbalance problem of the training set as our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Case Study</head><p>We present a successful case in Figure <ref type="figure" target="#fig_1">1</ref>, where the ground truth sticker has no OCR information, making it challenging for the model to understand its semantic meaning. Moreover, the model needs to understand that the dialogue is in a delighted context, and the stickers' emotions and meanings in order to distinguish the most appropriate sticker from the others. This case suggests our model has a good understanding of dialogue history and sticker emotion and semantic meaning with the help of auxiliary tasks.</p><p>We show a failing case of our model in Figure <ref type="figure">5</ref>. In this case, the appropriate sticker never appears in the training set. Considering the hard test set is more challenging than the easy test set, improving the generalization ability of our model is thus an important direction of future work. The same is true for baselines.</p><p>In the dataset we used, only one sticker is considered correct. However, we observe cases where the model's selection is not the same as the answer but is also appropriate. An example is shown in Figure <ref type="figure">6</ref>. Therefore, the results in Table <ref type="table">1</ref> indicate a lower bound performance and our model may perform better in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we address the challenging task of selecting appropriate stickers in open-domain multiturn dialogue. We propose a multitask learning method with three auxiliary tasks to enhance the understanding of dialogues and stickers. Experiments show that our model outperforms strong baselines, confirming the effectiveness of our multitask learning method for sticker selection. Although our experiments are conducted on a Chinese dataset, our methods are expected to work for other languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Dataset Details</head><p>Statistics of the dataset are shown in Table <ref type="table" target="#tab_1">3</ref>. There are 307 stickers in total and 228 out of them have textual information extracted by OCR. For stickers without emotion labels or semantic labels, we simply ignore the emotion classification loss or the semantic prediction loss. A better way to deal with the missing labels is left as future work.</p><p>For each dialogue sample, we ignore stickers in the middle of the dialogue history, as we found in preliminary experiments that removing them has no significant impact on the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Implementation Details</head><p>For all the models implemented by ourselves in our experiments, we set the batch size to 8 and use AdamW optimizer with cosine scheduler. For the CLIP baseline, as there is no available CLIP R10@1 R10@2 R10@5 MRR10 R ALL @1 R ALL @2 R ALL @5 MRR ALL  model especially pretrained in Chinese, we use a multilingual version adapted via knowledge distillation (Reimers and Gurevych, 2020)<ref type="foot" target="#foot_2">3</ref> . We cut the dialogue history to take only the last sentence in order to fit the length limit of CLIP's text encoder. We also tried to use the last two or more sentences, but found that the performance decreased.</p><p>All BERT-based models and MOD-GPT use the image encoder in the CLIP baseline. We set the CLIP image encoder's learning rate to 5e-7 and the text encoder's learning rate to 9e-6. For BERTbased models, we set the learning rate to 9e-6 and fix the image encoder following <ref type="bibr" target="#b6">(Fei et al., 2021)</ref>. The maximum epoch is set to 10. For the total loss, ? is set to 0.05, ? is set to 0.2 and ? is set to 0.1. All the hyperparameters are selected based on the validation set. The maximum training time for one epoch is about 5 hours on one single V100 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Effect of Semantic Information</head><p>In our full model, we also added semantic labels to other tasks' inputs, i.e., the main task of contextsticker matching, the masked context prediction task and the sticker emotion classification task. It raises an interesting question of how the performance will change if we remove this information.</p><p>The result is shown in Table <ref type="table" target="#tab_0">2</ref>. As we can see, the sticker semantic prediction task is more beneficial for the easy test set, while adding OCR information to other tasks is more beneficial for the hard test set. We conjecture that because of the relatively small number of stickers (less than 300), it could be easier for the model to memorize the meaning of all stickers in the dataset, which potentially damages the model's generalization ability on unseen stickers in the hard test set. Adding OCR information for other tasks greatly alleviates this phenomenon because it could offer semantic labels for unseen stickers and enhance the model's generalization ability.</p><p>We also tried to enhance the model's generalization ability by incorporating additional stickerdescription pairs from another source into the  <ref type="table">4</ref>: Performance of our full model and CLIP on the divided dataset. The three values divided by / correspond to the performance on the sub dataset where stickers have text, the performance on the sub dataset where stickers don't have text and their difference. All the numbers are scaled by 100. The easy test set contains only the same stickers seen during training, while the hard test set has unseen stickers. R@k is the recall rate of top-k predicted stickers and MRR the Mean Reciprocal Rank of ground truth stickers. The abbreviations ctx, emo and sem correspond to the auxiliary task 1, 2 and 3 respectively in Section 3. sticker semantic prediction task. As Table <ref type="table" target="#tab_0">2</ref> shows, this method could increase the performance on the hard test set as expected, but the performance on the easy test set drops significantly, which may be attributed to the distribution difference between the additional data and our original data.</p><formula xml:id="formula_1">R ALL @1 R ALL @2 R ALL @</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Analysis of Sensitivity to the Text in Stickers</head><p>To explore whether stickers have text or not could affect the model's performance, we split each test set into two parts, i.e., stickers with recognized text labels versus those without text labels. The number of samples in each part is 2164 and 1051 for the easy set, and 4429 and 2599 for the hard set. We compare the performance of our full model with that of CLIP on the divided test sets in Table <ref type="table">4</ref>. To avoid randomness in candidate set construction, we only compare the two models with the whole candidate set. In general, our full model and CLIP work better when the stickers have text, which suggests that the text in the sticker could help the model better understand the sticker.<ref type="foot" target="#foot_3">4</ref> However, our model is less sensitive to whether stickers have text or not according to the smaller difference value compared with CLIP. This implies our model is more robust to different kinds of candidate stickers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>the suitcase? When will you be home?! Ha ha let's hang out together when you arrive home?) (It depends. I may be a little busy this time) ????????????? ??????????? (What are you up to with so many days off) (Two VIPs have returned home) ??? ??????? (You don't get it) (What does VIP mean?) ???????? (I will if you explain to me) angry cute brokenhearted goodbye</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of the sticker selection task. Given a dialogue history, the model needs to add a sticker to the last textual message which is the most appropriate one among a collection of candidate stickers (the one marked in the red rectangle). The words below in red denote the emotion or meaning of each sticker.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An overview of our training task design. The base model architecture is a multimodal BERT that learns to predict whether the candidate sticker is appropriate given the dialogue context. Three auxiliary tasks are proposed to enhance the model's ability to understand multimodal input. c i and e i represent tokens of dialogue context and semantic label respectively.</figDesc><graphic url="image-13.png" coords="2,462.10,173.28,118.77,95.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Table 1 :</head><label>1</label><figDesc>Performance of the models on DSTC10 dataset. All the numbers are scaled by 100. The easy test set contains only the same stickers seen during training, while the hard test set has unseen stickers. The footnotes 10 and ALL indicate the numbers of candidate stickers considered for each train and test case, which are 10 (ground truth sticker plus 9 randomly sampled stickers) or all available stickers respectively. R@k is the recall rate of top-k predicted stickers and MRR the Mean Reciprocal Rank of ground truth stickers. The abbreviations ctx, emo and sem correspond to the auxiliary task 1, 2 and 3 respectively in Section 3. A paired t-test is conducted between the full model (MMBERT+ctx+emo+sem) and CLIP ( * : p &lt; 0.05, * * : p &lt; 0.01).Bad guy, definitely.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Examples of word saliency in the dialogue history. Word saliency is computed as Frobenius norm of its gradient with regard to the main task loss. Darker color indicates the word is more important. The words in red denote the text in the sticker.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The diversity of the predicted and ground truth stickers in the easy test set.</figDesc><graphic url="image-24.png" coords="4,306.14,365.46,218.26,145.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Figure 5: A failing case of our model. The leftmost sticker is selected by our model among the four candidate stickers. The appropriate sticker is marked with a red rectangle. The red words explain the stickers' emotions or meanings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Effect of incorporating semantic label prediction and OCR feature on DSTC10 dataset. All the numbers are scaled by 100. The easy test set only contains stickers ever seen in the training set, while the hard test set contains stickers unseen during training. R 10 @k and R ALL @k mean recall rate of ground truth stickers from top-k stickers chosen by the models, given a candidate set of 10 or all available stickers respectively. MRR 10 and MRR ALL represent Mean Reciprocal Rank of ground truth stickers among 10 or all available stickers. MM-BERT+ctx+emo+sem-OCR means not using OCR information for other tasks except sticker semantic prediction. MMBERT+ctx+emo+sem-OCR+data means not using OCR information for other tasks except sticker semantic prediction and adds extra sticker-description pairs for sticker semantic prediction task.</figDesc><table><row><cell>easy test</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">MMBERT+ctx+emo</cell><cell></cell><cell>48.80</cell><cell>70.67</cell><cell>92.29</cell><cell>66.88</cell><cell>6.07</cell><cell>11.26</cell><cell>22.02</cell><cell>15.22</cell></row><row><cell cols="2">MMBERT+ctx+emo+sem</cell><cell></cell><cell>49.14</cell><cell>69.46</cell><cell>91.76</cell><cell>66.67</cell><cell>7.40</cell><cell>12.07</cell><cell>22.08</cell><cell>15.99</cell></row><row><cell cols="3">MMBERT+ctx+emo+sem-OCR</cell><cell>49.95</cell><cell>70.89</cell><cell>92.13</cell><cell>67.39</cell><cell>6.63</cell><cell>12.07</cell><cell>22.18</cell><cell>15.80</cell></row><row><cell cols="4">MMBERT+ctx+emo+sem-OCR+data 47.06</cell><cell>67.50</cell><cell>91.07</cell><cell>65.12</cell><cell>6.03</cell><cell>9.74</cell><cell>19.75</cell><cell>14.23</cell></row><row><cell>hard test</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">MMBERT+ctx+emo</cell><cell></cell><cell>35.39</cell><cell>52.26</cell><cell>78.14</cell><cell>53.65</cell><cell>4.87</cell><cell>8.18</cell><cell>14.66</cell><cell>11.06</cell></row><row><cell cols="2">MMBERT+ctx+emo+sem</cell><cell></cell><cell>36.64</cell><cell>55.48</cell><cell>80.78</cell><cell>55.40</cell><cell>6.06</cell><cell>9.65</cell><cell>15.79</cell><cell>12.40</cell></row><row><cell cols="3">MMBERT+ctx+emo+sem-OCR</cell><cell>32.87</cell><cell>50.03</cell><cell>76.07</cell><cell>51.51</cell><cell>4.58</cell><cell>7.74</cell><cell>13.70</cell><cell>10.44</cell></row><row><cell cols="4">MMBERT+ctx+emo+sem-OCR+data 33.42</cell><cell>50.94</cell><cell>78.78</cell><cell>52.49</cell><cell>4.75</cell><cell>7.80</cell><cell>14.31</cell><cell>10.75</cell></row><row><cell></cell><cell>Train</cell><cell>Valid</cell><cell cols="3">Easy test Hard test</cell><cell></cell><cell></cell><cell></cell></row><row><cell># samples</cell><cell>211575</cell><cell>3542</cell><cell>3215</cell><cell>7028</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell># emo samples</cell><cell>209890</cell><cell>3495</cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell># utterances</cell><cell cols="2">1666208 26040</cell><cell>25447</cell><cell cols="2">59773</cell><cell></cell><cell></cell><cell></cell></row><row><cell># tokens</cell><cell>10400</cell><cell>2718</cell><cell>2780</cell><cell>3818</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell># stickers</cell><cell>283</cell><cell>249</cell><cell>239</cell><cell>278</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Avg. # utterances</cell><cell>7.88</cell><cell>7.35</cell><cell>7.92</cell><cell>8.50</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Avg. # tokens</cell><cell>18.42</cell><cell>12.47</cell><cell>12.91</cell><cell cols="2">14.54</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Dataset statistics. Easy test set's stickers all</cell></row><row><cell>appear in the train set, while the hard test set contains</cell></row><row><cell>stickers which don't appear in the train set. One orig-</cell></row><row><cell>inal dialogue could be split into several samples, each</cell></row><row><cell>containing one sticker response. The token num is com-</cell></row><row><cell>puted by the tokenizer of BERT. # emo samples means</cell></row><row><cell>the number of samples containing emotion annotation.</cell></row><row><cell>Avg. # tokens means the average number of tokens for</cell></row><row><cell>each utterance.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>.85/3.84 12.80/10.75/2.05 19.69/27.12/-7.43 15.89/16.28/-0.39 hard test CLIP 6.82/3.58/3.24 10.23/6.46/3.77 16.60/12.66/3.94 12.74/9.26/3.48 MMBERT+ctx+emo+sem 6.95/4.50/2.45 10.68/7.77/2.91 16.48/14.62/1.86 13.24/10.91/2.33</figDesc><table><row><cell></cell><cell>5</cell><cell>MRR ALL</cell></row><row><cell>easy test</cell><cell></cell></row><row><cell>CLIP</cell><cell cols="2">7.49/2.95/4.54 11.23/6.09/5.14 18.21/13.23/4.98 14.29/9.54/4.75</cell></row><row><cell cols="2">MMBERT+ctx+emo+sem 8.69/4Table</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://openai.weixin.qq.com/dstc/ DescriptionEN. See Appendix A for more details.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We only provide the results with 10 candidate stickers as their public code does.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://www.sbert.net/docs/ pretrained_models.html#image-text-models</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Strictly speaking, there may be other factors that make the two parts inherently different in difficulty, but a fair comparison is difficult to make. Intuitively, stickers without text labels are generally harder to understand for the models.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<idno>SRS 2 30.51 54.24 71.28 48.15 - - - - MOD-GPT</idno>
		<title level="m">R10@2 R10@5 MRR10 R ALL @1 R ALL @2 R ALL @5 MRR ALL easy test</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multimodal emoji prediction</title>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Barbieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Ronzano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horacio</forename><surname>Saggion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Short Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="679" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Are emojis predictable?</title>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Barbieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horacio</forename><surname>Saggion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Short Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="105" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">2017a. Visual dialog</title>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khushi</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avi</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deshraj</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Jos?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="page" from="326" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning cooperative visual dialog agents with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Jos?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2951" to="2960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Zhengcong</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zekang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinchao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.01839</idno>
		<title level="m">Towards expressive communication with internet memes: A new multimodal conversation dataset and benchmark</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to respond with stickers: A framework of unifying multi-modality in multi-turn dialog</title>
		<author>
			<persName><forename type="first">Shen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference 2020</title>
		<meeting>The Web Conference 2020</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1138" to="1148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sticker and emoji use in facebook messenger: Implications for graphicon change</title>
		<author>
			<persName><forename type="first">Artie</forename><surname>Konrad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><forename type="middle">C</forename><surname>Herring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer-Mediated Communication</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="217" to="235" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Understanding chat messages for sticker recommendation in messaging apps</title>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Laddha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Hanoosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debdoot</forename><surname>Mukherjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13156" to="13163" />
		</imprint>
	</monogr>
	<note>Parth Patwa, and Ankur Narang</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Maria: A visual experience powered conversational agent</title>
		<author>
			<persName><forename type="first">Zujie</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongyang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiubo</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yining</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.435</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5596" to="5611" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Making monolingual sentence embeddings multilingual using knowledge distillation</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020">2021. 2020</date>
			<biblScope unit="page" from="4512" to="4525" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Learning transferable visual models from natural language supervision</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Emoticon, emoji, and sticker use in computer-mediated communication: A review of theories and research findings</title>
		<author>
			<persName><forename type="first">Ying</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khe</forename><surname>Foon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hew</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Communication</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An animated picture says at least a thousand words: Selecting gif-based replies in multimodal dialog</title>
		<author>
			<persName><forename type="first">Xingyao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Jurgens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3228" to="3257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A large-scale chinese short-text conversation dataset</title>
		<author>
			<persName><forename type="first">Yida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhe</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaili</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CCF International Conference on Natural Language Processing and Chinese Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="91" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04609</idno>
		<title level="m">Neural emoji recommendation in dialogue systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
