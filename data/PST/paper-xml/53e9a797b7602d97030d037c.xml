<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Bayesian network classifiers for facial expression recognition using both labeled and unlabeled data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ira</forename><surname>Cohen</surname></persName>
							<email>iracohen@ifp.uiuc.edu</email>
						</author>
						<author>
							<persName><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Fabio</forename><forename type="middle">G</forename><surname>Cozman</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Marcelo</forename><forename type="middle">C</forename><surname>Cirelo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
							<email>huang¥@ifp.uiuc.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Beckman Institute for Advanced Science and Technology</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Leiden Institute of Advanced Computer Science</orgName>
								<orgName type="institution">Leiden University</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Escola Politécnica</orgName>
								<orgName type="institution">Universidade de São Paulo</orgName>
								<address>
									<settlement>São Paulo</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Bayesian network classifiers for facial expression recognition using both labeled and unlabeled data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E615457981DD4B16EA55D9B56399BF1C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Understanding human emotions is one of the necessary skills for the computer to interact intelligently with human users. The most expressive way humans display emotions is through facial expressions. In this paper, we report on several advances we have made in building a system for classification of facial expressions from continuous video input. We use Bayesian network classifiers for classifying expressions from video. One of the motivating factor in using the Bayesian network classifiers is their ability to handle missing data, both during inference and training. In particular, we are interested in the problem of learning with both labeled and unlabeled data. We show that when using unlabeled data to learn classifiers, using correct modeling assumptions is critical for achieving improved classification performance. Motivated by this, we introduce a classification driven stochastic structure search algorithm for learning the structure of Bayesian network classifiers. We show that with moderate size labeled training sets and large amount of unlabeled data, our method can utilize unlabeled data to improve classification performance. We also provide results using the Naive Bayes (NB) and the Tree-Augmented Naive Bayes (TAN) classifiers, showing that the two can achieve good performance with labeled training sets, but perform poorly when unlabeled data are added to the training set.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Since the early 1970s, Ekman has performed extensive studies of human facial expressions <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> and found evidence to support universality in facial expressions. These "universal facial expressions" are those representing happiness, sadness, anger, fear, surprise, and disgust. Ekman's work inspired many researchers to analyze facial expressions using image and video processing. By tracking facial features and measuring the amount of facial movement, they attempt to categorize different facial expressions. Recent work on facial expression analysis has used these "basic expressions" or a subset of them (see Pantic and Rothkrantz's <ref type="bibr" target="#b18">[19]</ref> detailed review of many of the research done in recent years). All these methods are similar in that they first extract some features from the images or video, then these features are used as inputs into a classification system, and the outcome is one of the preselected emotion categories. They differ mainly in the features extracted and in the classifiers used to distinguish between the different emotions.</p><p>We have developed a real time facial expression recognition system. The system uses a model based non-rigid face tracking algorithm to extract motion features that serve as input to a Bayesian network classifier used for recognizing facial expressions <ref type="bibr" target="#b4">[5]</ref>. In our system, as with all other past research in facial expression recognition, learning the classifiers was done using labeled data and supervised learning algorithms. One of the challenges facing researchers attempting to design facial expression recognition systems is the relatively small amount of available labeled data. Construction and labeling of a good database of images or videos of facial expressions requires expertise, time, and training of subjects. Only a few such databases are available, such as the Cohn-Kanade database <ref type="bibr" target="#b13">[14]</ref>. However, collecting, without labeling, data of humans displaying expressions is not as difficult. Such data is called unlabeled data. It is beneficial to use classifiers that are learnt with a combination of some labeled data and a large amount of unlabeled data. This paper is focused at describing how to learn to classify facial expressions with labeled and unlabeled data, also known as semi-supervised learning.</p><p>Bayesian networks, the classifiers used in our system, can be learned with labeled and unlabeled data using maximum likelihood estimation. One of the main questions is whether adding the unlabeled data to the training set improves the classifier's recognition performance on unseen data. In Section 3 we briefly discuss our recent results demonstrating that, counter to statistical intuition, when the assumed model of the classifier does not match the true data generating distribution, classification performance could degrade as more and more unlabeled data are added to the training set. Motivated by this, we propose in Section 4 a classification driven stochastic structure search (SSS) algorithm for learning the structure of Bayesian network classifiers. We demonstrate the algorithm's performance using commonly used databases from the UCI repository <ref type="bibr" target="#b1">[2]</ref>. In Section 5 we perform experiments with our facial expression recognition system using two databases and show the ability to use unlabeled data to enhance the classification performance, even with a small labeled training set.We have concluding remarks in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Facial Expression Recognition System</head><p>We start with a brief description of our real time facial expression recognition system. The system is composed of a face tracking algorithm which outputs a vector of motion features of certain regions of the face. The features are used as inputs to a Bayesian network classifier.</p><p>The face tracking we use in our system is based on a system developed by Tao and Huang <ref type="bibr" target="#b21">[22]</ref> called the Piecewise Bézier Volume Deformation (PBVD) tracker. This face tracker uses a model-based approach where an explicit 3D wireframe model of the face is constructed. Once the model is constructed and fitted, head motion and local deformations of the facial features such as the eyebrows, eyelids, and mouth can be tracked. The recovered motions are represented in terms of magnitudes of some predefined motion of various facial features. Each feature motion corresponds to a simple deformation on the face, defined in terms of the Bézier volume control parameters. We refer to these motions vectors as Motion-Units (MU's). The MU's used in the face tracker are shown in Figure <ref type="figure" target="#fig_0">1</ref>(a). The MU's are used as the basic features for the classification scheme described in the next sections. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Bayesian network classifiers</head><p>We start with a few conventions that are adopted throughout. The goal here is to label an incoming vector of features (MUs) . Each instantiation of is a record. We assume that there is a class variable ¡ ; the values of ¡ are the labels, one of the facial expressions. The classifier receives a record ¢ and generates a label £ ¤ ¦¥ ¢ ¨ § . An optimal classification rule can be obtained from the exact distribution © ¥ ¡ § . However, if the distribution is not known, we have to learn it from expert knowledge or data. For recognizing facial expression using the features extracted from the face tracking system, we consider probabilistic classifiers that represent the a-posteriori probability of the class given the features, © ¥ ¡ § , using Bayesian networks <ref type="bibr" target="#b19">[20]</ref>. A Bayesian network is composed of a directed acyclic graph in which every node is associated with a variable and with a conditional distribution © ¥ ! " # $ § , where " denotes the parents of in the graph. The directed acyclic graph is the structure, and the distributions © ¥ ! " § represent the parameters of the network. We say that the assumed structure for a network, % '&amp; , is correct when it is possible to find a distribution, © ¥ ¡ ( )! % 0&amp; 1 § , that matches the distribution that generates data; otherwise, the structure is incorrect. We use maximum likelihood estimation to learn the parameters of the network. When there are missing data in our training set, we use the EM algorithm <ref type="bibr" target="#b8">[9]</ref> to maximize the likelihood.</p><p>A Bayesian network having the correct structure and parameters is also optimal for classification because the aposteriori distribution of the class variable is accurately represented. A Bayesian network classifier is a generative classifier when the class variable is an ancestor (e.g., parent) of some or all features. A Bayesian network classifier is diagnostic, when the class variable has non of the features as descendants. As we are interested in using unlabeled data in learning the Bayesian network classifier, we restrict ourselves to generative classifiers and exclude structures that are diagnostic, which cannot be trained using maximum likelihood approaches with unlabeled data <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>Two examples of generative Bayesian network classifiers are the Naive Bayes (NB) classifier, in which the features are assumed independent given the class, and the Tree-Augmented Naive Bayes classifier (TAN). The NB classifier makes the assumption that all features are conditionally independent given the class label. Although this assumption is typically violated in practice, NB have been used successfully in many classification applications. One of the reasons for the NB success is attributed to the small number of parameters needed to be learnt.</p><p>In the structure of the TAN classifier, the class variable is the parent of all the features and each feature has at most one other feature as a parent, such that the resultant graph of the features forms a tree. Using the algorithm presented by Friedman et al. <ref type="bibr" target="#b11">[12]</ref>, the most likely TAN classifier can be estimated efficiently. When unlabeled data are available, estimating the parameters of the Naive Bayes classifier can be done using the EM algorithm. As for learning the TAN classifier, we learn the structure and parameters using the EM-TAN algorithm, derived from <ref type="bibr" target="#b15">[16]</ref>.</p><p>We have previously used both the NB and TAN classifiers to perform facial expression recognition <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b4">5]</ref> with good success. However, we used only labeled data for classification. With unlabeled data we show in our experiments that the limited expressive power of Naive Bayes and TAN causes the use of unlabeled data to degrade the performance of our recognition system. This statement will become clear as we describe the properties of learning with labeled and unlabeled data in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Learning a classifier from labeled and unlabeled training data</head><p>In this section we discuss properties of classifiers learned with labeled and unlabeled data. In particular, we discuss the possibility that unlabeled data degrade classification performance.</p><p>Early work proved that unlabeled data lead to improved classification performance, provided that the modeling assumptions of the classifier are correct <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b22">23]</ref>. These have advanced an optimistic view of the labeled-unlabeled problem, where unlabeled data can be profitably used whenever available. However, unlabeled data can also lead to significant degradation in classification performance. A few results in the literature illustrate this possibility. Nigam et al <ref type="bibr" target="#b17">[18]</ref> use Naive Bayes classifiers and a large number of features, and report that, when modeling assumptions "are not satisfied, EM may actually degrade rather than improve classifier accuracy" and suggest giving a smaller weight to the unlabeled data. Baluja <ref type="bibr" target="#b0">[1]</ref> use unlabeled data to help learn how to determine face orientation. He observed that with Naive Bayes classifiers, unlabeled data sometimes degraded the performance, and proceeded to model the dependencies among the features, finding that such models use better the unlabeled data.</p><p>We have conducted an investigation on the effect of unlabeled data and showed that unlabeled data can have deleterious effect when the modeling assumptions are incorrect <ref type="bibr" target="#b7">[8]</ref>; here we summarize the main points. We have observed that degradation is not just caused by numerical problems, such as local convergence of the EM algorithm; nor is it just caused by differences between the distribution of labeled data and the distribution of unlabeled data; nor is it just caused by outliers. These explanations do not suffice to clarify why is it that labeled records are routinely seen to improve classification, even in the presence of outliers or incorrect clusters of features, while the same modeling problems lead unlabeled data to degrade classification. This degradation occurs because the asymptotic classification performance of a classifier with incorrect structure can be different when this classifier is learned with fully labeled data and when the classifier is learned with labeled and un-labeled data. Moreover, we proved that there is a fundamental lack of robustness of maximum likelihood estimators when trained with labeled and unlabeled data under incorrect modeling assumptions. Consider Figure <ref type="figure" target="#fig_1">2</ref> which illustrates the differences in classification bias of classifiers learned from labeled and unlabeled data, where the bias is measured from the Bayes error rate. We simulate the asymptotic case of infinite data. <ref type="foot" target="#foot_0">1</ref>We generated 100 different binary classifiers, each with 4 Gaussian distributed features given the class, and not independent of each other. The parameters of each classifier are: the class prior, ¢¡ © ¥ ¡ £¡ ¥¤ § , the mean vectors, ¦ ¨ § ©¦ , and a common covariance matrix % !</p><p>. The Bayes error rate of the classifiers ranged from ¤ #" %$ '&amp; )( 10 32 , with most being around 4 !¤ 52 (the Bayes error was computed analyti- cally using the true parameters of the classifiers).</p><p>For each classifier we looked at different combinations of making incorrect independence assumptions, by assuming that features are independent of each other (from one to all features being independent of each other; overall 4 64 com- binations). For example, if we assume that 7 8 and 7 @9 are independent of the rest of the features, the covariance matrix we estimate under this assumption must have the form:</p><formula xml:id="formula_0">A B C D E E F G £ £ H H H H G ¦ ¦ H G ¦ PI H H G § § H H G I ¦ H G I QI R QS S T (1)</formula><p>thus some elements of the covariance matrix are incorrectly forced to be zero.</p><p>For each combination we computed the classification error of two classifiers (trained under the independence assumptions): one simulating training with infinite labeled data and a second trained with infinite unlabeled data. For the labeled data case, since the ML estimation is unbiased, the learned parameters are the true priors, the means, and the elements of the covariance matrix that were not forced to be zero. For unlabeled data, we approximated infinity with 4 !¤ 6¤ ¤ 1¤ 6¤ training records (which is very large compared to ¡ 0 , the largest number of parameters estimated in the ex- periments). We used EM to learn with unlabeled data, with the starting point being the parameter set of the labeled only classifier, therefore assuring that the difference in the results of the two estimated classifiers do not depend on the starting point of EM.</p><p>Over all, we computed 1100 classification errors for the completely labeled case and 1100 for the unlabeled case. From the errors we generated the classification error bias histograms in Figure <ref type="figure" target="#fig_1">2</ref>. The histograms show that the classification bias of the labeled based classifiers tends to be more highly concentrated closer to ¤ compared to the un- labeled based classifiers. We also observed that using unlabeled data always resulted in a higher error rate compared to using labeled data. The only exception was when we did not make any incorrect independence assumptions, in which the classifiers trained with unlabeled data achieved the Bayes error rate, as expected. What we understand from these histograms is that when training with labeled data, many classifiers will perform well (although never achieve the optimal Bayes rate). However, classifiers trained with unlabeled data need to be more accurate in their modeling assumptions to achieve good performance and they are a great deal more sensitive to such inaccuracies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Learning the structure of Bayesian network classifiers</head><p>The conclusion of the previous section indicates the importance of obtaining the correct structure when using unlabeled data in learning the classifier. If the correct structure is obtained, unlabeled data improve a classifier; otherwise, unlabeled data can actually degrade performance. Somewhat surprisingly, the option of searching for better structures was not proposed by researchers that previously witnessed the performance degradation. Apparently, performance degradation was attributed to unpredictable, stochastic disturbances in modeling assumptions, and not to mistakes in the underlying structure -something that can be detected and fixed. One attempt to overcome the performance degradation from unlabeled data could be to switch models as soon as degradation is detected. Suppose that we learn a classifier with labeled data only and we observe a degradation in performance when the classifier is learned with labeled and unlabeled data. We can switch to a more complex structure at that point. An interesting idea is to start with a Naive Bayes classifier and, if performance degrades with unlabeled data, switch to a different type of Bayesian network classifier, namely the TAN classifier. If the correct structure can be represented using a TAN structure, this approach will indeed work. However, even the TAN structure is only a small set of all possible structures. Moreover, as the experiments in the next sections show, switching from NB to TAN does not guarantee that the performance degradation will not occur.</p><p>A different approach to overcome performance degradation is to use some standard structure learning algorithm, as there are many such algorithms in the Bayesian network literature <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b6">7]</ref>. A common goal of many existing methods is to find a structure that best fits the joint distribution of all the variables given the data. Because learning is done with finite datasets, most methods penalize very complex structures that might overfit the data, using for example the minimum description length (MDL) score. The difficulty of structure search is the size of the space of possible structures. With finite amounts of data, algorithms that search through the space of structures maximizing the likelihood, can lead to poor classifiers because the a-posteriori probability of the class variable could have a small effect on the score <ref type="bibr" target="#b11">[12]</ref>. Therefore, a network with a higher score is not necessarily a better classifier. Friedman et al <ref type="bibr" target="#b11">[12]</ref> further suggest changing the scoring function to focus only on the posterior probability of the class variable, but show that it is not computationally feasible.</p><p>The drawbacks of likelihood based structure learning algorithms could be magnified when learning with unlabeled data; the posterior probability of the class has a smaller effect during the search, while the marginal of the features would dominate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Classification driven stochastic structure search</head><p>In this section we propose a method that can effectively search for better structures with an explicit focus on classification. We essentially need to find a search strategy that can efficiently search through the space of structures. As we have no simple closed-form expression that relates structure with classification error, it would be difficult to design a gradient descent algorithm or a similar iterative method. Even if we did that, a gradient search algorithm would be likely to find a local minimum because of the size of the search space.</p><p>First we define a measure over the space of structures which we want to maximize:</p><formula xml:id="formula_1">Definition 1 The inverse error measure for structure % '&amp; is ¢ ¤£ ¦¥ ¨ § © ¤ £ "! $# &amp;%' (# 0) 21 43 5 76 1 8 @9 £ # &amp;%' (# 0) 21 A3 5 B6 1 7C (2)</formula><p>where the summation is over the space of possible structures and © ED ¥ £ ¤ ¥ ) § GF¡ ¡ § is the probability of error of the best classifier learned with structure % .</p><p>We use Metropolis-Hastings sampling <ref type="bibr" target="#b16">[17]</ref> to generate samples from the inverse error measure, without having to ever compute it for all possible structures. For constructing the Metropolis-Hastings sampling, we define a neighborhood of a structure as the set of directed acyclic graphs to which we can transit in the next step. Transition is done using a predefined set of possible changes to the structure; at each transition a change consists of a single edge addition, removal, or reversal. We define the acceptance probability of a candidate structure, % ¡ £¢ ¥¤ , to replace a previous structure, % §¦ as follows:</p><formula xml:id="formula_2">© § "! § $# B "% § '&amp; )( "! § # B "0 ( 21 43 5 76 # B 0 98 B £% § '&amp; ¡( 6 # B % § &amp; 8 B "0 ( @ C © BA 0 § 'C ¥C ED FC A % § '&amp; § 'C ¥C ED FC 1 3 5 HG 0 G % § '&amp; @ (3)</formula><p>where I ¥ % &amp; $! % 0 § is the transition probability from % to % 0&amp; , P is a temperature factor, and Q ¦ and Q £¢ E¤ are the sizes of the neighborhoods of % ¦ and % £¢ E¤ respectively; this choice corresponds to equal probability of transition to each member in the neighborhood of a structure. This further creates a Markov chain which is aperiodic and irreducible, thus satisfying the Markov chain Monte Carlo (MCMC) conditions <ref type="bibr" target="#b14">[15]</ref>. We summarize our algorithm in Figure <ref type="figure" target="#fig_3">3</ref>.  Roughly speaking, P close to 4 would allow acceptance of more structures with higher probability of error than previous structures. P close to ¤ mostly allows acceptance of structures that improve probability of error. A fixed P amounts to changing the distribution being sampled by the MCMC, while a decreasing P is a simulated annealing run, aimed at finding the maximum of the inverse error distribution. The rate of decrease of the temperature determines the rate of convergence. Asymptotically in the number of data, a logarithmic decrease of P will guarantee convergence to a global maximum with probability that tends to one <ref type="bibr" target="#b12">[13]</ref>.</p><p>There are two caveats though; first, the logarithmic cooling schedule is very slow, second, we never have access to the true probability of error for each structure -we calculate the classification error from a limited pool of training data (denoted by £ © D ¢ E 9 9 F ). To avoid the problem of overfit- ting we can take several approaches. Cross-validation can be performed by splitting the labeled training set to smaller sets. However, this approach can significantly slow down the search, and is suitable only if the labeled training set is moderately large. Instead, we use the multiplicative penalty term derived from structural risk minimization to define a modified error term:</p><formula xml:id="formula_3">© T 9 § 'C ¥C ¥D UC ' D ¥ T 9 § 'C ¥C ED UC p e # ed D ¥f # ¦ % eg 1 s £ 1 ih Bd D ¥f # ej g I 1 % C<label>(4)</label></formula><p>where k D is the Vapnik-Chervonenkis (VC) dimension of the classifier with structure % , l is the number of training records, and ¤ are between ¤ and 4 . To approximate the VC dimension, we use k D nm Q D , with Q D the number of (free) parameters in the Markov blanket of the class variable in the network, assuming that all variables are discrete.</p><p>To illustrate the performance of SSS algorithm, we performed experiments with some of the UCI datasets and an artificially generated data set (a Bayesian network with TAN structure), using relatively small labeled sets and large unlabeled sets (Table <ref type="table" target="#tab_0">1</ref>). The results using the UCI datasets show, to varying degrees, the ability of SSS to utilize unlabeled data. The most dramatic improvement is seen with the Shuttle dataset. The results with the artificially generated data show that SSS was able to achieve almost the same performance as TAN, which had the advantage of a-priori knowledge of the correct structure. We also see that for both NB and TAN, using unlabeled data can cause performance degradation, therefore the idea of switching between these simple models is not guaranteed to work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Facial Expression Recognition Experiments</head><p>We test the algorithms for the facial expression recognition system. We initially consider experiments where all the data is labeled. Then we investigate the effect of using both labeled and unlabeled data. We use two different databases, one collected by Chen and Huang <ref type="bibr" target="#b3">[4]</ref> and the Cohn-Kanade database <ref type="bibr" target="#b13">[14]</ref>. The first consists of subjects that were instructed to display facial expressions corresponding to six types of emotions. In the Chen-Huang database there are five subjects. For each subjects there are six video sequences per expression, each sequence starting and ending in the Neutral expression. There are on average 60 frames per expression sequence. The Cohn-Kanade database <ref type="bibr" target="#b13">[14]</ref> consists of expression sequences of subjects, starting from a Neutral expression and ending in the peak of the facial expression. There are 104 subjects in the database. Because for some of the subjects, not all of the six facial expressions sequences were available to us, we used a subset of 53 subjects, for which at least four of the sequences were available. For each person there are on average 8 frames for each expression.</p><p>We measure the accuracy with respect to the classification result of each frame, where each frame in the video sequence was manually labeled to one of the expressions (including Neutral). This manual labeling can introduce some 'noise' in our classification because the boundary between Neutral and the expression of a sequence is not necessarily optimal, and frames near this boundary might cause confusion between the expression and the Neutral.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental results with labeled data</head><p>We start with a person-independent experiment using all the labeled data. For this test we use the sequences of some subjects as test sequences and the sequences of the remaining subjects as training sequences (we leave out one subject in the Chen-Huang database and 10 subjects for the Cohn-Kanade database). This test is repeated five times, each time leaving different subjects out (leave one out cross validation). Table <ref type="table" target="#tab_1">2</ref> shows the recognition rate of the test for all classifiers. We see that the Naive Bayes classifier performs poorly. However, a significant improvement for both the TAN and the SSS algorithm is obtained, with SSS being significantly better. It should be noted that with a smaller training set, SSS would not have been able to explore many structure and its performance would have probably be the same or worse than NB and TAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Experiments with labeled and unlabeled data</head><p>We consider now both labeled and unlabeled data in a person-independent experiment. We first partition the data to a training set and a test set and randomly choose a portion of the training set and remove the labels. This procedure ensures that the distribution of the labeled and the unlabeled sets are the same. We train Naive Bayes and TAN classifiers, using just the labeled part of the training data and the combination of labeled and unlabeled data. We use the SSS algorithm to train a classifier using both labeled and unlabeled data (we do not search for the structure with just the labeled part because it is too small for performing a full structure search).</p><p>We see in Table <ref type="table" target="#tab_2">3</ref> that with NB and TAN, even when using only 200 and 300 labeled samples, adding the unlabeled data degrades the performance of the classifiers, and we would have been better off not using the unlabeled data. Using the SSS algorithm, we are able to improve the results and use the unlabeled data to achieve performance which is higher than using just the labeled data with NB and TAN. The fact that the performance is lower than in the case when all the training set was labeled (see Table <ref type="table" target="#tab_1">2</ref>) implies that the relative value of labeled data is higher than of unlabeled data, as was shown by Castelli <ref type="bibr" target="#b2">[3]</ref>. However, had there been more unlabeled data, the performance would be expected to improve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Summary and Discussion</head><p>In this work, we presented several advances we made in building a real-time system for classification of facial expressions from continuous video input. The facial expression recognition was done using Bayesian networks classifiers. Collecting labeled data of humans displaying expressions is a difficult task and therefore, we were interested in learning the classifiers with both labeled and unlabeled data. One question we asked was whether adding the unlabeled data to the training set improves the classifier's recognition performance on unseen data. We showed that when incorrect modeling assumptions are used, the unlabeled data could have deleterious effect on the classification performance, while the same unlabeled data, under correct modeling assumptions, are theoretically guaranteed to improve the classification performance. With this result we proposed a classification driven stochastic structure search algorithm for learning the structure of the Bayesian network classifiers. We demonstrated the algorithm's performance using standard databases from the UCI repository. Using moderate size labeled training sets and large amount of unlabeled data, our method was able to utilize unlabeled data to improve classification performance.</p><p>We tested our classifiers for facial expression recognition using two databases. We compared the results with two other Bayesian network classifiers that have been used in our system: Naive Bayes and TAN networks and we showed that the two can achieve good performance with labeled training sets, but perform poorly when unlabeled data are added to the training set. We showed that by searching for the structure driven by the classification error enables us to use the unlabeled data to improve the classification performance.</p><p>In conclusion, our main contributions are as follows. We applied Bayesian network classifiers to the problem of facial expression recognition and we proposed a method that can effectively search for the correct Bayesian network structure focusing on classification. We also stressed the importance of obtaining such a structure when using unlabeled data in learning the classifier. If correct structure is used, the unlabeled data improve the classification, otherwise they can actually degrade the performance. Finally, we integrated the classifiers and the face tracking system to build a real time facial expression recognition system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The facial motion measurements</figDesc><graphic coords="2,142.94,502.04,67.79,84.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Histogram of classification error bias from the Bayes error rate under incorrect independence assumptions for training with labeled data (left) and training with unlabeled data (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 . 2 .</head><label>12</label><figDesc>Fix the network structure to some initial structure, SR . Estimate the parameters of the structure R and compute the probability of error T R § C UC ED UC . 3. Set V XW . 4. Repeat, until a maximum number of iterations is reached (Y 2ba dc eV 'f hg ) i Sample a new structure % § '&amp; , from the neighborhood of 0 uniformly, with probability p hq 9r 0 . i Learn the parameters of the new structure using maxi- mum likelihood estimation. Compute the probability of error of the new classifier, T % § '&amp; § 'C ¥C ¥D UC . i Accept % § &amp; with probability given in Eq. (3). i If % § '&amp; is accepted, set 0 ts £ % § '&amp; and T 0 ts £ § 'C ¥C ED UC T % § '&amp; § C UC ED UC and change u according to the temperature decrease schedule. Otherwise 0 ts £ @ 0 . i V V Sv Xp . 5. return the structure "w , such that x bg hy b ¢ £ R 9 bw h B 0 § C © T w § C UC ED UC .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Stochastic structure search algorithm (SSS)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Classification accuracy for Naive Bayes, TAN, and stochastic structure search: Naive Bayes classifier learned with labeled data only (NB-L), Naive Bayes classifier learned with labeled and unlabeled data (NB-LUL), TAN classifier learned with labeled data only (TAN-L), TAN classifier learned with labeled and unlabeled data (TAN-LUL), stochastic structure search with labeled and unlabeled data (SSS-LUL).</figDesc><table><row><cell>Dataset</cell><cell cols="3">Training records # labeled # unlabeled # Test</cell><cell>NB-L</cell><cell cols="4">NB-LUL TAN-L TAN-LUL SSS-LUL</cell></row><row><cell>TAN artificial</cell><cell>300</cell><cell>30000</cell><cell cols="2">50000 83.41%</cell><cell>59.21%</cell><cell>90.89%</cell><cell>91.94%</cell><cell>91.05%</cell></row><row><cell>Shuttle</cell><cell>500</cell><cell>43000</cell><cell cols="2">14500 82.44%</cell><cell>76.10%</cell><cell>81.19%</cell><cell>90.22%</cell><cell>96.26%</cell></row><row><cell>Satimage</cell><cell>600</cell><cell>3835</cell><cell>2000</cell><cell>81.65%</cell><cell>77.45%</cell><cell>83.54%</cell><cell>81.05%</cell><cell>83.35%</cell></row><row><cell>Adult</cell><cell>6000</cell><cell>24862</cell><cell cols="2">15060 83.86%</cell><cell>73.11%</cell><cell>84.72%</cell><cell>80.00%</cell><cell>85.04%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Recognition rates (%) for person-independent test</figDesc><table><row><cell></cell><cell>NB</cell><cell>TAN</cell><cell>SSS</cell></row><row><cell>Chen-Huang Database</cell><cell cols="3">71.78 80.31 83.62</cell></row><row><cell>Cohn-Kandade Database</cell><cell cols="3">77.70 80.40 81.80</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Classification results for facial expression recognition with labeled and unlabeled data.</figDesc><table><row><cell>Dataset</cell><cell cols="3">Training records # labeled # unlabeled # Test</cell><cell>NB-L</cell><cell cols="4">NB-LUL TAN-L TAN-LUL SSS-LUL</cell></row><row><cell>Cohn-Kanade</cell><cell>200</cell><cell>2980</cell><cell>1000</cell><cell>72.50%</cell><cell>69.10%</cell><cell>72.90%</cell><cell>69.30%</cell><cell>74.80%</cell></row><row><cell>Chen-Huang</cell><cell>300</cell><cell>11982</cell><cell>3555</cell><cell>71.25%</cell><cell>58.54%</cell><cell>72.45%</cell><cell>62.87%</cell><cell>74.99%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Care should be taken when using only unlabeled data in training. As noted by Castelli<ref type="bibr" target="#b2">[3]</ref>, with unlabeled data it is possible to recover all the parameters of the classifier (under some restrictions, such as identifiability), but a decision on the actual labeling is not possible since we do not know what are the class labels. In the following we assume that we are given this knowledge and therefore are able to perform classification.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Alex Bronstein and Marsha Duro at HP-Labs for proposing the research on labeled-unlabeled data and for many suggestions and comments during the course of the work on this topic, as their help was critical to the results described here. We coded our own classifiers in the Java language, using the libraries of the JavaBayes system (freely available at http://www.cs.cmu.edu/˜javabayes). This work has been supported in part by the National Science Foundation Grants CDA-96-24396 and IIS-00-85980. The work of Ira Cohen has been supported by a Hewlett Packard fellowship.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Probabilistic modelling for face orientation discrimination: Learning from labeled and unlabeled data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">UCI repository of machine learning databases</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Merz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The relative value of labeled and unlabeled samples in pattern recognition</title>
		<author>
			<persName><forename type="first">V</forename><surname>Castelli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<pubPlace>Stanford</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Joint processing of audio-visual information for the recognition of emotional expressions in human-computer interaction</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
		<respStmt>
			<orgName>University of Illinois at Urbana-Champaign, Dept. of Electrical Engineering</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Facial expression recognition from video sequences: Temporal and static modeling</title>
		<author>
			<persName><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note>to appear in CVIU special issue on face recognition</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Facial expression recognition from video sequences</title>
		<author>
			<persName><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Bayesian method for the induction of probabilistic networks from data</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Herskovits</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="308" to="347" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unlabeled data can degrade classification performance of generative classifiers</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">G</forename><surname>Cozman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FLAIRS</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Strong evidence for universals in facial expressions: A reply to Russell&apos;s mistaken critique</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="268" to="287" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
		<title level="m">Facial Action Coding System: Investigator&apos;s Guide</title>
		<imprint>
			<publisher>Consulting Psychologists Press</publisher>
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bayesian network classifiers</title>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goldszmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="131" to="163" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cooling schedules for optimal annealing</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hajek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematics of operational research</title>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="311" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Comprehensive database for facial expression analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bayesian graphical models for discrete data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Madigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>York</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Statistical Review</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="215" to="232" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning with mixture of trees</title>
		<author>
			<persName><forename type="first">M</forename><surname>Meila</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Equation of state calculation by fast computing machines</title>
		<author>
			<persName><forename type="first">N</forename><surname>Metropolis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Rosenbluth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Rosenbluth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Teller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Teller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Physics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1087" to="1092" />
			<date type="published" when="1953">1953</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Text classification from labeled and unlabeled documents using EM</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="103" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatic analysis of facial expressions: The state of the art</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J M</forename><surname>Rothkrantz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1424" to="1445" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Probabilistic Reasoning in Intelligent Systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning with labeled and unlabeled data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Seeger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>Edinburgh University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Connected vibrations: A modal analysis approach to non-rigid motion tracking</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="735" to="740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A probability analysis on the value of unlabeled data for classification problems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Oles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
