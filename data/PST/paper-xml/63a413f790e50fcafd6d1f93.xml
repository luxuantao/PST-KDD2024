<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NodeFormer: A Scalable Graph Structure Learning Transformer for Node Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wentao</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zenan</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Wipf</surname></persName>
							<email>davidwipf@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">Amazon Web Service</orgName>
								<address>
									<settlement>Shanghai AI Lab</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
							<email>yanjunchi@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Wentao Zhao and Zenan Li contribute equally</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">Shanghai AI Laboratory. 36th Conference on Neural Information Processing Systems</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">NodeFormer: A Scalable Graph Structure Learning Transformer for Node Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural networks have been extensively studied for learning with interconnected data. Despite this, recent evidence has revealed GNNs' deficiencies related to over-squashing, heterophily, handling long-range dependencies, edge incompleteness and particularly, the absence of graphs altogether. While a plausible solution is to learn new adaptive topology for message passing, issues concerning quadratic complexity hinder simultaneous guarantees for scalability and precision in large networks. In this paper, we introduce a novel all-pair message passing scheme for efficiently propagating node signals between arbitrary nodes, as an important building block for a pioneering Transformer-style network for node classification on large graphs, dubbed as NODEFORMER. Specifically, the efficient computation is enabled by a kernerlized Gumbel-Softmax operator that reduces the algorithmic complexity to linearity w.r.t. node numbers for learning latent graph structures from large, potentially fully-connected graphs in a differentiable manner. We also provide accompanying theory as justification for our design. Extensive experiments demonstrate the promising efficacy of the method in various tasks including node classification on graphs (with up to 2M nodes) and graph-enhanced applications (e.g., image classification) where input graphs are missing. The codes are available at https://github.com/qitianwu/NodeFormer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Relational structure inter-connecting instance nodes as a graph is ubiquitous from social domains (e.g., citation networks) to natural science (protein-protein interaction), where graph neural networks (GNNs) <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b35">36]</ref> have shown promising power for leveraging such data dependence as geometric priors. However, there arises increasing evidence challenging the core GNN hypothesis that propagating information along observed graph structures will necessarily produce better nodelevel representations for prediction on each individual instance node. Conflicts with this premise lead to commonly identified deficiencies with GNN message-passing rules w.r.t. heterophily <ref type="bibr" target="#b51">[52]</ref>, over-squashing <ref type="bibr" target="#b1">[2]</ref>, long-range dependencies <ref type="bibr" target="#b7">[8]</ref>, and graph incompleteness <ref type="bibr" target="#b10">[11]</ref>, etc. Moreover, in graph-enhanced applications, e.g., text classification <ref type="bibr" target="#b44">[45]</ref>, vision navigation <ref type="bibr" target="#b11">[12]</ref>, physics simulation <ref type="bibr" target="#b28">[29]</ref>, etc., graph structures are often unavailable though individual instances are strongly inter-correlated. A common practice is to artificially construct a graph via some predefined rules (e.g., k-NN), which is agnostic to downstream tasks and may presumably cause the misspecification of GNNs' inductive bias on input geometry (induced by the local feature propagation design).</p><p>Natural solutions resort to organically combining learning optimal graph topology with message passing. However, one critical difficulty is the scalability issue with O(N 2 ) (where N denotes • We further propose NODEFORMER, a new class of graph networks with layer-wise message passing as operated over latent graphs potentially connecting all nodes. The latter are optimized in an end-to-end differentiable fashion through a new objective that essentially pursues sampling optimal topology from a posterior conditioned on node features and labels. To our knowledge, NODEFORMER is the first Transformer model that scales all-pair message passing to large node classification graphs.</p><p>• We demonstrate the model's efficacy by extensive experiments over a diverse set of datasets, including node classification benchmarks and image/text classification, where significant improvement over strong GNN models and SOTA structure learning methods is shown. Besides, it successfully scales to large graph datasets with up to 2M nodes where prior arts failed, and reduces the time/space consumption of the competitors by up to 93.1%/80.6% on moderate sized datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Graph Neural Networks. Building expressive GNNs is a fundamental problem in learning over graph data. With Graph Attention Networks (GAT) <ref type="bibr" target="#b35">[36]</ref> as an early attempt, there are many follow-up works, e.g., <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b41">42]</ref>, considering weighting the edges in input graph for enhancing the expressiveness. Other studies, e.g., <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b50">51]</ref> focus on sparsifying input structures to promote robust representations. There are also quite a few approaches that propose scalable GNNs through, e.g., subgraph sampling <ref type="bibr" target="#b46">[47]</ref>, linear feature mapping <ref type="bibr" target="#b38">[39]</ref>, and channel-wise transformation <ref type="bibr" target="#b47">[48]</ref>, etc. However, these works cannot learn new edges out of the scope of input geometry, which may limit the model's receptive fields within local neighbors and neglect global information.</p><p>Graph Structure Learning. Going beyond observed topology, graph structure learning targets learning a new graph for message passing among all the instances <ref type="bibr" target="#b52">[53]</ref>. One line of work is similaritydriven where the confidence of edges are reflected by some similarity functions between node pairs, e.g., Gaussian kernels <ref type="bibr" target="#b42">[43]</ref>, cosine similarity <ref type="bibr" target="#b3">[4]</ref>, attention networks <ref type="bibr" target="#b16">[17]</ref>, non-linear MLP <ref type="bibr" target="#b6">[7]</ref> etc. Another line of work optimizes the adjacency matrix. Due to the increased optimization difficulties, some sophisticated training methods are introduced, such as bi-level optimization <ref type="bibr" target="#b10">[11]</ref>, variational Table <ref type="table">1</ref>: Comparison of popular graph structure learning approaches for node-level tasks where in particular, the graph connects all instance nodes and one's target is for prediction on each individual node. For parameterization, 'Function' means learning through functional mapping and 'Adjacency' means directly optimizing graph adjacency. For expressivty, 'Fixed' means learning one graph shared by all propagation layers and 'Layer-wise' means learning graph structures per layers. The largest demo means the largest # nodes of datasets used. † m denotes # anchors (i.e., a subset of nodes).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>Parameterization Expressivity Input Graphs Inductive Complexity Largest Demo LDS-GNN <ref type="bibr" target="#b10">[11]</ref> Adjacency Fixed Required No O(N 2 ) 0.01M ProGNN <ref type="bibr" target="#b17">[18]</ref> Adjacency Fixed Required No O(N 2 ) 0.02M VGCN <ref type="bibr" target="#b9">[10]</ref> Adjacency Fixed Required No O(N 2 ) 0.02M BGCN <ref type="bibr" target="#b49">[50]</ref> Adjacency Fixed Required No O(N 2 ) 0.02M GLCN <ref type="bibr" target="#b16">[17]</ref> Function Fixed Not necessary Yes O(N 2 ) 0.02M IDGL <ref type="bibr" target="#b3">[4]</ref> Function approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b33">34]</ref>, Bayesian inference <ref type="bibr" target="#b49">[50]</ref> and projected gradient descent <ref type="bibr" target="#b17">[18]</ref>. To push further the limits of structure learning, this paper proposes a new model NODEFORMER (for enabling scalable node-level Transformers) whose merits are highlighted via a high-level comparison in Table <ref type="table">1</ref>. In particular, NODEFORMER enables efficient structure learning in each layer, does not require input graphs and successfully scales to graphs with 2M nodes.</p><p>Node-Level v.s. Graph-Level Prediction. We emphasize upfront that our focus is on node-level prediction tasks involving a single large graph such that scalability is paramount, especially if we are to consider arbitrary relationships across all nodes (each node is an instance with label and one can treat all the nodes non-i.i.d. generated due to the inter-dependence) for structure-learning purposes. Critically though, this scenario is quite distinct from graph-level classification tasks whereby each i.i.d. instance is itself a small graph and fully connecting nodes within each graph is computationally inexpensive. While this latter scenario has been explored in the context of graph structure learning <ref type="bibr" target="#b37">[38]</ref> and all-pair message passing design, e.g., graph Transformers <ref type="bibr" target="#b8">[9]</ref>, existing efforts do not scale to the large graphs endemic to node-level prediction. Each node u ∈ N is assigned with node features x u ∈ R D and a label y u . We define an adjacency matrix A = {a uv } ∈ {0, 1} N ×N where a uv = 1 if edge (u, v) ∈ E and a uv = 0 otherwise. Without loss of generality, E could be an empty set in case of no input structure. There are two common settings: transductive learning, where testing nodes are within the graph used for training, and inductive learning which handles new unseen nodes out of the training graph. The target is to learn a function for node-level prediction, i.e., estimate labels for unlabeled or new nodes in the graph.</p><p>General Model and Key Challenges. We start with the observation that the input structures may not be the ideal one for propagating signals among nodes and instead there exist certain latent structures that could facilitate learning better node representations. We thus consider the updating rule</p><formula xml:id="formula_0">Ã(l) = g(A, Z (l) ; ω), Z (l+1) = h( Ã(l) , A, Z (l) ; θ),<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">Z (l) = {z (l) u } u∈N and Ã(l) = {ã (l)</formula><p>uv } u,v∈N denotes the node representations and the estimated latent graph of the l-th layer, respectively, and g, h are both differentiable functions aiming at 1) structure estimation for a layer-specific latent graph Ã(l) based on node representations and 2) feature propagation for updating node representations, respectively. The model defined by Eqn. 1 follows the spirit of Transformers <ref type="bibr" target="#b34">[35]</ref> (where in particular Ã(l) can be seen as an attentive graph) that potentially enables message passing between any node pair in each layer, which, however, poses two challenges:</p><p>• (Scalability): How to reduce the prohibitive quadratic complexity for learning new graphs?</p><p>• (Differentiability): How to enable end-to-end differentiable optimization for discrete structures? Notice that the first challenge is non-trivial in node-level prediction tasks (the focus of our paper), since the latent graphs could potentially connect all the instance nodes (e.g., from thousands to millions, depending on dataset sizes), which is fairly hard to guarantee both precision and scalability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Efficient Learning Discrete Structures</head><p>We describe our new message-passing scheme with an efficient kernelized Gumbel-Softmax operator to resolve the aforementioned challenges. We assume z (0) u = x u as the initial node representation. Kernelized Message Passing. We define a full-graph attentive network that estimates latent interactions among instance nodes and enables corresponding densely-connected message passing:</p><formula xml:id="formula_2">ã(l) uv = exp((W (l) Q z (l) u ) ⊤ (W (l) K z (l) v )) N w=1 exp((W (l) Q z (l) u ) ⊤ (W (l) K z (l) w )) , z (l+1) u = N v=1 ã(l) uv • (W (l) V z (l) v ),<label>(2)</label></formula><p>where</p><formula xml:id="formula_3">W (l) Q , W<label>(l)</label></formula><p>K and W (l)</p><p>V are learnable parameters in l-th layer. We omit non-linearity activation (after aggregation) for brevity. The updating for N nodes in one layer using Eqn. 2 requires prohibitive O(N 2 ) complexity. Also, given large N , the normalization in the denominator would shrink attention weights to zero and lead to gradient vanishing. We call this problem as over-normalizing.</p><p>To accelerate the full-graph model, we observe that the dot-then-exponentiate operation in Eqn. 2 can be converted into a pairwise similarity function:</p><formula xml:id="formula_4">z (l+1) u = N v=1 κ(W (l) Q z (l) u , W (l) K z (l) v ) N w=1 κ(W (l) Q z (l) u , W (l) K z (l) w ) • (W (l) V z (l) v ),<label>(3)</label></formula><p>where κ(•, •) : R d ×R d → R is a positive-definite kernel measuring the pairwise similarity. The kernel function can be further approximated by random features (RF) <ref type="bibr" target="#b25">[26]</ref>which serves as an unbiased</p><formula xml:id="formula_5">estimation via κ(a, b) = ⟨Φ(a), Φ(b)⟩ V ≈ ϕ(a) ⊤ ϕ(b),</formula><p>where the first equation is by Mercer's theorem with Φ : R d → V a basis function and V a high-dimensional vector space, and ϕ(•) : R d → R m is a low-dimensional feature map with random transformation. There are many potential choices for ϕ, e.g., Positive Random Features (PRF) <ref type="bibr" target="#b5">[6]</ref> ϕ</p><formula xml:id="formula_6">(x) = exp ( −∥x∥ 2 2 2 ) √ m [exp(w ⊤ 1 x), • • • , exp(w ⊤ m x)],<label>(4)</label></formula><p>where w k ∼ N (0, I d ) is i.i.d. sampled random transformation. The RF converts dot-thenexponentiate operation into inner-product in vector space, which enables us to re-write Eqn. 3 (assuming</p><formula xml:id="formula_7">q u = W (l) Q z (l) u , k u = W (l) K z (l) u and v u = W (l) V z (l)</formula><p>u for simplicity):</p><formula xml:id="formula_8">z (l+1) u = N v=1 ϕ(q u ) ⊤ ϕ(k v ) N w=1 ϕ(q u ) ⊤ ϕ(k w ) • v v = ϕ(q u ) ⊤ N v=1 ϕ(k v ) • v ⊤ v ϕ(q u ) ⊤ N w=1 ϕ(k w ) . (<label>5</label></formula><formula xml:id="formula_9">)</formula><p>The key advantage of Eqn. 5 is that the two summations are shared by each u, so that one only needs to compute them once and re-used for others. Such a property enables O(N ) computational complexity for full-graph message passing, which paves the way for learning graph structures among large-scale instances. Moreover, one can notice that Eqn. 5 avoids computing the N × N similarity matrix, i.e., {ã</p><p>uv } N ×N , required by Eqn. 2, thus also reducing the learning difficulties. Nevertheless, Eqn. 5 still suffers what we mentioned the over-normalizing issue. The crux is that the message passing is operated on a weighted fully-connected graph where, in fact, only partial edges are important. Also, such a deterministic way of feature aggregation over all the instances may increase the risk for over-fitting, especially when N is large. We next resolve the issues by distilling a sparse structure from the fully-connected graph.</p><p>Differentiable Stochastic Structure Learning. The difficultly lies in how to enable differentiable optimization for discrete graph structures. The weight ã(l) uv given by Eqn. 2 could be used to define a categorical distribution for generating latent edges from distribution Cat(π</p><formula xml:id="formula_11">(l) u ) where π (l) u = {π (l) uv } N v=1 and π (l) uv = p(v|u) = ã(l) uv .</formula><p>Then in principle, we can sample over the categorical distribution multiple times for each node to obtain its neighbors. However, the sampling process would introduce discontinuity and hinders back-propagation. Fortunately, we notice that the Eqn. 3 can be modified to incorporate the reparametrization trick <ref type="bibr" target="#b15">[16]</ref> to allow differentiable learning:</p><formula xml:id="formula_12">z (l+1) u = N v=1 exp((q ⊤ u k v + g v )/τ ) N w=1 exp((q ⊤ u k w + g w )/τ ) • v v = N v=1 κ(q u / √ τ , k v / √ τ )e gv/τ N w=1 κ(q u / √ τ , k w / √ τ )e gw/τ • v v ,<label>(6)</label></formula><p>where g u is i.i.d. sampled from Gumbel distribution and τ is a temperature coefficient. Eqn. 6 is a continuous relaxation of sampling one neighbored node for u over Cat(π</p><formula xml:id="formula_13">(l)</formula><p>u ) and τ controls the closeness to hard discrete samples <ref type="bibr" target="#b21">[22]</ref>. Following similar reasoning as Eqn. 3 and 5, we can yield</p><formula xml:id="formula_14">z (l+1) u ≈ N v=1 ϕ(q u / √ τ ) ⊤ ϕ(k v / √ τ )e gv/τ N w=1 ϕ(q u / √ τ ) ⊤ ϕ(k w / √ τ )e gw/τ •v v = ϕ(q u / √ τ ) ⊤ N v=1 e gv/τ ϕ(k v / √ τ ) • v ⊤ v ϕ(q u / √ τ ) ⊤ N w=1 e gw/τ ϕ(k w / √ τ ) .</formula><p>(7) Eqn. 7 achieves message passing over a sampled latent graph (where we only sample once for each node) and still guarantees linear complexity as Eqn. <ref type="bibr" target="#b4">5</ref>. In practice, we can sample K times (e.g., K = 5) for each node and take an average of the aggregated results. Due to space limit, we defer more details concerning the differentiable sampling-based message passing to Appendix A. Besides, in Fig. <ref type="figure" target="#fig_5">5</ref> and Alg. 1 of Appendix A, we present an illustration for node embedding updating in each layer, from a matrix view that is practically used for implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Well-posedness of the Kernelized Gumbel-Softmax Operator</head><p>One reasonable concern for Eqn. 7 is whether the RF approximation for kernel functions maintains the well-posedness of Gumbel approximation for the target discrete variables. As a justification for the new message-passing function, we next answer two theoretical questions: 1) How is the approximation capability of RF for the original dot-then-exponentiate operation with Gumbel variables in Eqn. 6? 2) Does Eqn. 7 still guarantee a continuous relaxation of the categorical distributions? We formulate the results as follows and defer proofs to Appendix B. Theorem 1 (Approximation Error for Softmax-Kernel). Assume ∥q u ∥ 2 and ∥k v ∥ 2 are bounded by r, then with probability at least 1 − ϵ, the gap</p><formula xml:id="formula_15">∆ = ϕ(q u / √ τ ) ⊤ ϕ(k v / √ τ ) − κ(q u / √ τ , k v / √ τ ) ),</formula><p>where ϕ is defined by Eqn. 4, will be bounded by O exp(6r/τ ) mϵ</p><p>.</p><p>We can see that the error bound of RF for approximating original softmax-kernel function depends on both the dimension of feature map ϕ and temperature τ . Notably, the error bound is independent of node number N , which implies that the approximation ability is insensitive to dataset sizes.</p><p>The second question is non-trivial since Eqn. 7 involves randomness of Gumbel variables and random transformation in ϕ, which cannot be decoupled apart. We define c uv = ϕ(qu/ √ τ ) ⊤ ϕ(kv/ √ τ )e gv /τ N w=1 ϕ(qu/ √ τ ) ⊤ ϕ(kw/ √ τ )e gw /τ as the result from the kernelized Gumbel-Softmax and c u = {c uv } N v=1 denotes the sampled edge vector for node u. We can arrive at the result as follows. Theorem 2 (Property of Kernelized Gumbel-Softmax Random Variables). Suppose m is sufficiently large, we have the convergence property for the kernelized Gumbel-Softmax operator</p><formula xml:id="formula_16">lim τ →0 P(c uv &gt; c uv ′ , ∀v ′ ̸ = v) = exp(q ⊤ u k v ) N w=1 exp(q ⊤ u k w ) , lim τ →0 P(c uv = 1) = exp(q ⊤ u k v ) N w=1 exp(q ⊤ u k w )</formula><p>.</p><p>It shows that when i) the dimension of feature map is large enough and ii) the temperature goes to zero, the distribution from which latent structures are sampled would converge to the original categorical distribution.</p><p>Remark. The two theorems imply a trade-off between RF approximation and Gumbel-Softmax approximation w.r.t. the choice of τ . A large τ would help to reduce the burden on kernel dimension m, and namely, small τ would require a very large m to guarantee enough RF approximation precision.</p><p>On the other hand, if τ is too large, the weight on each edge will converge to 1 N , i.e., the model nearly degrades to mean pooling, while a small τ would endow the kernelized Gumbel-Softmax with better approximation to the categorical distribution. Empirical studies on this are presented in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Input Structures as Relational Bias</head><p>Eqn. 7 does not leverage any information from observed geometry which, however, is often recognized important for modeling physically-structured data <ref type="bibr" target="#b2">[3]</ref>. We therefore accommodate input topology (if any) as relational bias via modifying the attention weight as where b (l) is a learnable scalar as relational bias for any adjacent node pairs (u, v) and σ is a certain (bounded) activation function like sigmoid. The relational bias aims at assigning adjacent nodes in G with proper weights, and the node representations could be accordingly updated by</p><formula xml:id="formula_17">ã(l) uv ← ã(l) uv + I[a uv = 1]σ(b (l) ),</formula><formula xml:id="formula_18">z (l+1) u ← z (l+1) u + v,auv=1 σ(b (l) ) • v v .<label>(8)</label></formula><p>Eqn. 8 increases the algorithmic complexity for message passing to O(N + E), albeit within the same order-of-magnitude as common GNNs operating on input graphs. Also, one can consider higher-order adjacency as relational bias for better expressiveness at some expense of efficiency, as similarly done by <ref type="bibr" target="#b0">[1]</ref>. We summarize the feed-forward computation of NODEFORMER in Alg. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Learning Objective</head><p>Given training labels Y tr = {y u } u∈Ntr , where N tr denotes the set of labeled nodes, the common practice is to maximize the observed data log-likelihood which yields a supervised loss (with C classes)</p><formula xml:id="formula_19">L s (Y tr , Ŷtr ) = − 1 N tr v∈Ntr C c=1 I[y u = c] log ŷu,c ,<label>(9)</label></formula><p>where I[•] is an indicator function. However, it may not suffice to generalize well due to that the graph topology learning increases the degrees of freedom and the number of training labels is not comparable to that. Therefore, we additionally introduce an edge-level regularization:</p><formula xml:id="formula_20">L e (A, Ã) = − 1 N L L l=1 (u,v)∈E 1 d u log π (l) uv ,<label>(10)</label></formula><p>where d u denotes the in-degree of node u and π</p><formula xml:id="formula_21">(l)</formula><p>uv is the predicted probability for edge (u, v) at the l-th layer. Eqn. 10 is a maximum likelihood estimation for edges in E, with data distribution defined</p><formula xml:id="formula_22">p 0 (v|u) = 1 du , a uv = 1 0, otherwise.<label>(11)</label></formula><p>We next show how to efficiently obtain π (l)</p><p>uv . Although the feed-forward NODEFORMER computation defined by Eqn. 7 does not explicitly produce the value for each π (l) uv , we can query their values by   where the summation term can be re-used from once computation, as is done by Eqn. 5 and Eqn. 7. Therefore, after once computation for the summation that requires O(N ), the computation for each</p><formula xml:id="formula_23">π (l) uv = ϕ(W (l) Q z (l) u ) ⊤ ϕ(W (l) K z (l) v ) ϕ(W (l) Q z (l) u ) ⊤ N w=1 ϕ(W (l) K z (l) w ) ,<label>(12)</label></formula><formula xml:id="formula_24">π (l)</formula><p>uv requires O(1) complexity, yielding the total complexity controlled within O(E) (since we only need to query the observed edges). The final objective can be the combination of two: L = L s + λL e , where λ controls how much emphasis is put on input topology. We depict the whole data flow of NODEFORMER's training in Fig. <ref type="figure" target="#fig_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>We consider a diverse set of datasets for experiments and present detailed dataset information in Appendix D. For implementation, we set σ as sigmoid function and τ as 0.25 for all datasets. The output prediction layer is a one-layer MLP. More implementation details are presented in Appendix C. All experiments are conducted on a NVIDIA V100 with 16 GB memory.</p><p>As baseline models, we basically consider GCN <ref type="bibr" target="#b18">[19]</ref> and GAT <ref type="bibr" target="#b35">[36]</ref>. Besides, we compare with some advanced GNN models, including JKNet <ref type="bibr" target="#b43">[44]</ref> and MixHop <ref type="bibr" target="#b0">[1]</ref>. These GNN models all rely on input graphs. We further consider DropEdge <ref type="bibr" target="#b26">[27]</ref> and two SOTA graph structure learning methods, LDS-GNN <ref type="bibr" target="#b10">[11]</ref> and IDGL <ref type="bibr" target="#b3">[4]</ref> for comparison. For large-scale datasets, we additionally compare with two scalable GNNs, a linear model SGC <ref type="bibr" target="#b38">[39]</ref> and a graph-sampling model GraphSAINT <ref type="bibr" target="#b46">[47]</ref>. More detailed information about these models are presented in Appendix C. All the experiments are repeated five times with different initializations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiments on Transductive Node Classification</head><p>We study supervised node classification in transductive setting on common graph datasets: Cora, Citeseer, Deezer and Actor. The first two have high homophily ratios and the last two are identified as heterophilic graphs <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b19">20]</ref>. These datasets are of small or medium sizes (with 2K∼20K nodes). We use random splits with train/valid/test ratios as 50%/25%/25% and accuracy on test nodes for evaluation. Results are plotted in Fig. <ref type="figure" target="#fig_2">2</ref> and NODEFORMER achieves the best mean accuracy across four datasets and in particular, outperforms other models by a large margin on two heterophilic graphs. The results indicate that NODEFORMER can handle both homophilious and nonhomophilious graphs. Compared with two structure learning models LDS and IDGL, NODEFORMER yields significantly better performance, which shows its superiority. Also, for deezer, LDS and IDGL suffers from out-of-memory (OOM). In fact, the major difficulty for deezer is the large  dimensions of input node features (nearly 30K), which causes OOM for IDGL even with the anchor approximation. In contrast, NODEFORMER manages to scale and produce desirable accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiments on Larger Graph Datasets</head><p>To further test the scalability, we consider two large-sized networks, OGB-Proteins and Amazon2M, with over 0.1 million and 2 million of nodes, respectively. OGB-Proteins is a multi-task dataset with 112 output dimensions, while Amazon2M is extracted from the Amazon Co-Purchasing network that entails long-range dependence <ref type="bibr" target="#b12">[13]</ref>. For OGB-Proteins, we use the protocol of <ref type="bibr" target="#b14">[15]</ref> and ROC-AUC for evaluation. For Amazon2M, we adopt random splitting with 50%/25%/25% nodes for training, validation and testing, respectively. Due to the large dataset size, we adopt mini-batch partition for training, in which case, for NODEFORMER we only consider structure learning among nodes in a random mini-batch. We use batch size 10000 and 100000 for Proteins and Amazon2M, respectively. While the mini-batch partition may sacrifice the exposure to all instances, we found using large batch size can yield decent performance, which is also allowable thanks to the O(N ) complexity of our model. For example, even setting the batch size as 100000, we found NODEFORMER costs only 4GB GPU memory for training on Amazon2M. Table <ref type="table" target="#tab_1">2</ref> presents the results on OGB-Proteins where for fair comparison mini-batch training is also used for other models except GraphSAINT. We found that NODEFORMER yields much better ROC-AUC and only requires comparable memory as simple GNN models. Table <ref type="table" target="#tab_2">3</ref> reports the results on Amazon2M which shows that NODEFORMER outperforms baselines by a large margin and the memory cost is even fewer than GCN. This shows its practical efficacy and scalability on large-scale datasets and also the capability for addressing long-range dependence with shallow layers (we use L = 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiments on Graph-Enhanced Applications</head><p>We apply our model to semi-supervised image and text classification on Mini-ImageNet and 20News-Groups datasets, without input graphs. The instances of Mini-ImageNet <ref type="bibr" target="#b36">[37]</ref>  Impact of Temperature and Feature Map Dimension. We study the effects of τ and m in Fig. <ref type="figure">6</ref> located in Appendix E and the variation trend accords with our theoretical analysis in Section 3.2. Specifically, the result shows that the test accuracy increases and then falls with the temperature changing from low to high values (usually achieves the peak accuracy with a temperature of 0.4). Besides, we can see that when the temperature is relatively small, the test accuracy goes high with the dimension of random features increasing. However, when the temperature is large, the accuracy would drop even with large feature dimension m. Such a phenomenon accords with the theoretical result presented in Section 3.2. For low temperature which enables desirable approximation performance for Gumbel-Softmax, then larger random feature dimension would help to produce better approximation to the original exponentiate-then-dot operator. In contrast, high temperature could not guarantee precise approximation for the original categorical distribution, which deteriorates the performance.</p><p>Visualization and Implications. Fig. <ref type="figure" target="#fig_4">4</ref> visualizes node embeddings and edge connections (filter out the edges with weights larger than a threshold) on 20News-Groups and Mini-Imagenet, which show that NODEFORMER tends to assign more weights for nodes with the same class and sparse edges for nodes with different classes. This helps to interpret why NODEFORMER improves the performance on downstream node-level prediction: the latent structures can propagate useful information to help the model learn better node representations that can be easily distinguished by the classifier. We also compare the learned structures with original graphs in Fig. <ref type="figure">7</ref> located in Appendix E. We can see that the latent structures learned by NODEFORMER show different patterns from the observed ones, especially for heterophilic graphs. Another interesting phenomenon is that there exist some dominant nodes which are assigned large weights by other nodes, forming some vertical 'lines' in the heatmap. This suggests that these nodes could contain critical information for the learning tasks and play as pivots that could improve the connectivity of the whole system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Why NODEFORMER Improves Downstream Prediction?</head><p>There remains a natural question concerning our learning process: how effective can the learned latent topology be for downstream tasks? We next dissect the rationale from a Bayesian perspective. In fact, our model induces a predictive distribution p(Y, Ã|X, A) = p( Ã|X, A)p(Y| Ã, X, A) where we can treat the estimated graph Ã as a latent variable. <ref type="foot" target="#foot_0">2</ref> Specifically, p( Ã|X, A) is instantiated with the structure estimation module and p(Y| Ã, X, A) is instantiated with the feature propagation module.</p><p>In principle, ideal latent graphs should account for downstream tasks and maximize the potentials of message passing for producing informative node representations. Thus, optimal latent graphs presumably come from the posterior p( Ã|Y, X, A) =</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>p(Y|X,A, Ã)p( Ã|X,A)</head><p>Y p(Y|X,A, Ã)p( Ã|X,A)dY which is given by Bayes theorem. Unfortunately, such a posterior is unknown and intractable for the integration. A Variational Perspective. An intriguing conclusion stems from another view into the learning process: we can treat the structure estimation as a variational distribution q( Ã|X, A) and our learning objective in Section 3.4 can be viewed as the embodiment of a minimization problem over the predictive and variational distributions via</p><formula xml:id="formula_25">p * , q * = arg min p,q −E q [log p(Y| Ã, X, A)] Ls + D(q( Ã|X, A)∥p 0 ( Ã|X, A)) Le ,<label>(13)</label></formula><p>where D denotes the Kullback-Leibler divergence. Specifically, the predictive term is equivalent to minimizing the supervised loss (with Gumbel-Softmax as a surrogate for sampling-based estimates over q( Ã|X, A)), and the KL regularization term is embodied with the edge-level MLE loss (Eqn. 10) (if we define the prior distribution p 0 ( Ã|X, A) following Eqn. 11). One may notice that Eqn. 13 is essentially the Evidence Lower Bound (ELBO) for the log-likelihood log p(Y|X, A).</p><p>Proposition 1. Assume q can exploit arbitrary distributions over Ã. When Eqn. 13 achieves the optimum, we have 1) D(q( Ã|X, A)∥p( Ã|Y, X, A)) = 0 and 2) log p(Y|X, A) is maximized.</p><p>The proposition indicates that our adopted learning objective intrinsically minimizes the divergence between latent graphs generated by the model and the samples from the posterior p( Ã|Y, X, A) that ideally helps to propagate useful adjacent information w.r.t. downstream tasks. Therefore, a well-trained network of NODEFORMER on labeled data could produce effective latent topology that contributes to boosting the downstream performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper proposes a scalable and efficient graph Transformer (especially for node level) that can propagate layer-wise node signals between arbitrary pairs beyond input topology. The key module, a kernelized Gumbel-Softmax operator, enables us to learn layer-specific latent graphs with linear algorithmic complexity without compromising the precision. The results on diverse graph datasets and situations verify the effectiveness, scalability, and stability. We provide more discussions on the limitations and potential impacts in Appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A More Details for NODEFORMER A.1 Differentiable Sampling-based Message Passing on Latent Structures</head><p>We provide more details concerning the differentiable sampling-based message passing through our kernelized Gumbel-Softmax operator, as complementary to the content of Sec. 3.1. As illustrated in Sec. 3.1, the l-th layer's feature propagation is defined over the l-th layer's latent graph composed of the sampled edges e (l) uv ∼ Cat(π u ) (l) . For each layer, we sample K times for each node, i.e., there will be K sampled neighbored nodes for each node u. We assume Ẽ(l) = {e (l) uv } as the set of sampled edges in the latent graph of the l-th layer. Then the updating rule for node embeddings at the l-th layer based on the latent graph can be written as</p><formula xml:id="formula_26">z (l+1) u = 1 K v,e<label>(l)</label></formula><p>uv ∈ Ẽ(l)</p><formula xml:id="formula_27">v u = 1 K v I[e (l) uv ∈ Ẽ(l) ]v u .<label>(14)</label></formula><p>The above equation introduces dis-continuity due to the sampling process that disables the end-to-end differentiable training. We thus adopt Gumbel-Softmax as a reparameterization trick to approximate the discrete sampled results via continuous relaxation:</p><formula xml:id="formula_28">z (l+1) u ≈ 1 K K k=1 N v=1 exp((q ⊤ u k u + g kv )/τ ) N w=1 exp((q ⊤ u k w + g kw )/τ ) • v u , g kw ∼ Gumbel(0, 1). (<label>15</label></formula><formula xml:id="formula_29">)</formula><p>The temperature τ controls the closeness to hard discrete samples <ref type="bibr" target="#b21">[22]</ref>. If τ is close to zero, then the Gumbel-Softmax term exp((q ⊤ u ku+g kv )/τ ) N w=1 exp((q ⊤ u kw+g kw )/τ ) for any v converges to a one-hot vector:</p><formula xml:id="formula_30">exp((q ⊤ u k v + g kv )/τ ) N w=1 exp((q ⊤ u k w + g kw )/τ ) = 1, if v satisfies q ⊤ u k v + g kv &gt; q ⊤ u k v ′ + g kv ′ ∀v ′ ̸ = v, 0, otherwise.</formula><p>(16) The Eqn. 15 requires O(N 2 ) for computing the embeddings for N nodes in one layer. To reduce the complexity to O(N ), we resort to the kernel approximation idea, following similar reasoning as Eqn. 3 and 5:</p><formula xml:id="formula_31">z (l+1) u ≈ 1 K K k=1 N v=1 exp((q ⊤ u k u + g kv )/τ ) N w=1 exp((q ⊤ u k w + g kw )/τ ) • v u = 1 K K k=1 N v=1 exp((q ⊤ u k u + g kv )/τ ) N w=1 exp((q ⊤ u k w + g kw )/τ ) • v u = 1 K K k=1 N v=1 κ(q u / √ τ , k v / √ τ )e g kv /τ N w=1 κ(q u / √ τ , k w / √ τ )e g kw /τ • v v ≈ 1 K K k=1 N v=1 ϕ(q u / √ τ ) ⊤ ϕ(k v / √ τ )e g kv /τ N w=1 ϕ(q u / √ τ ) ⊤ ϕ(k w / √ τ )e g kw /τ • v v = 1 K K k=1 ϕ(q u / √ τ ) ⊤ N v=1 e g kv /τ ϕ(k v / √ τ ) • v ⊤ v ϕ(q u / √ τ ) ⊤ N w=1 e g kw /τ ϕ(k w / √ τ ) . (<label>17</label></formula><formula xml:id="formula_32">)</formula><p>The above result yields the one-layer updating rule for NODEFORMER's feed-forwarding w.r.t. each node u. In terms of practical implementation, we adopt matrix multiplications for computing the node embeddings for all the nodes in the next layer, for which we present the details in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Model Implementation from the Matrix View</head><p>In practice, the implementation of NODEFORMER is based on matrix operations that simultanenously update all the nodes in one layer. We present the feed-forward process of NODEFORMER from a</p><p>Theorem 2 (Property of Kernelized Gumbel-Softmax Random Variables). Suppose m is sufficiently large, we have the convergence property for the kernelized Gumbel-Softmax operator</p><formula xml:id="formula_33">lim ⌧ !0 P(c uv &gt; c uv 0 , 8v 0 6 = v) = exp(q &gt; u k v ) P N w=1 exp(q &gt; u k w ) , lim ⌧ !0 P(c uv = 1) = exp(q &gt; u k v ) P N w=1 exp(q &gt; u k w )</formula><p>.</p><p>It shows that when i) the dimension of feature map is large enough and ii) the temperature goes to zero, the distribution from which latent structures are sampled would converge to the original categorical distribution. Algorithm 1: Scalable All-Pair Message Passing on Latent Graphs with Linear Complexity (O(N ) or O(N + E))</p><p>Input: Node features Z (0) = X, input adjacency A.</p><formula xml:id="formula_34">1 for l = 0 . . . , L 1 do 2 Q (l) W (l) Q Z (l) , K (l) W (l) K Z (l) , V (l) W (l) V Z (l) ; 3 for k = 1, 2, . . . , K do 4 Gk = {e g ku /⌧ } N u=1</formula><p>, gku ⇠ Gumbel(0, 1); 5 Gk = Gk.unsqueeze <ref type="bibr" target="#b0">(1)</ref>.repeat(1, m); l+1) ; % add relational bias</p><formula xml:id="formula_35">6 K(l) k = Gk (K (l) / p ⌧ ), Q(l) k = Gk (Q (l) / p ⌧ ); 7 U (l) k ( K(l) k ) &gt; V (l) , O (l) k ( K(l) k ) &gt; 1N⇥1; 8 Z (l+1) 1 K P K k=1 Q(l) k U (l) k Q(l) k O (l) k ; % average K samples 9 Z (l+1) Z (l+1) + (b (l) ) • AZ (</formula><formula xml:id="formula_36">Output: Predict node labels Ŷ = MLP({Z (l) } L l=0 ).</formula><p>Remark. The two theorems imply a trade-off between RF approximation and Gumbel-Softmax approximation w.r.t. the choice of ⌧ . A large ⌧ would help to reduce the burden on kernel dimension m, and namely, small ⌧ would require a very large m to guarantee enough RF approximation precision. On the other hand, if ⌧ is too large, the weight on each edge will converge to 1 N , i.e., the model nearly degrades to mean pooling, while a small ⌧ would endow the kernelized Gumbel-Softmax with better approximation to the categorical distribution. Empirical studies on this are presented in Appendix D.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proof for Technical Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Proof for Theorem 1</head><p>To prove our theorem, we first introduce the following lemma given by the Lemma 2 in <ref type="bibr" target="#b5">[6]</ref>. Proposition 1. Denote a softmax kernel as SM(x, y) = exp(x ⊤ y). The Positive Random Features defined by Eqn. 4 for softmax-kernel estimation, i.e., SM m (x, y)</p><formula xml:id="formula_37">= 1 m m i=1 [exp(w ⊤ i x − ∥x∥<label>2</label></formula><p>2 ) exp(w ⊤ i y − ∥y∥ 2</p><p>2 )], has the mean and variance over w ∼ N (0,</p><formula xml:id="formula_38">I d ) as E w ( SM m (x, y)) =SM(x, y) = exp(x ⊤ y), V w ( SM m (x, y)) = 1 m exp(∥x + y∥ 2 )SM 2 (x, y) (1 − exp(−∥x + y∥ 2 )).<label>(18)</label></formula><p>The lemma shows that the Positive Random Features can achieve unbiased approximation for the softmax kernel with a quantified variance.</p><p>Back to our main theorem, suppose the L2-norms of q u and k v are bounded by r, we can derive the probability using the Chebyshev's inequality:</p><formula xml:id="formula_39">P(∆ ≤ exp(6r/τ ) mϵ ) ≥ 1 − V w ( SM m (q u / √ τ , k v / √ τ )) exp(6r/τ )/mϵ (<label>19</label></formula><formula xml:id="formula_40">)</formula><p>where</p><formula xml:id="formula_41">∆ = SM m (q u / √ τ , k v / √ τ ) − SM(q u / √ τ , k v / √ τ</formula><p>) denotes the deviation of the kernel approximation. Using the result in Lemma 1, we can further obtain that the RHS of Eqn. 19 is no greater than</p><formula xml:id="formula_42">1 − ϵ exp(∥ q u + k v √ τ ∥ 2 + 2 q ⊤ u k v τ − 6 r τ ). (<label>20</label></formula><formula xml:id="formula_43">)</formula><p>Since ∥ qu+kv √ τ ∥ 2 ≤ 4r τ and 2 q ⊤ u kv τ ≤ 2r τ , we can achieve the stated result:</p><formula xml:id="formula_44">P(∆ ≤ exp(6r/τ ) mϵ ) ≥ 1 − ϵ.<label>(21)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Proof for Theorem 2</head><p>Before entering the proof for the theorem, we first introduce two basic technical lemmas. While such results are already mentioned in previous studies <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22]</ref>, their proofs will be useful for the subsequent reasoning. Therefore, we restate the proofs as building blocks for the following presentation. Proposition 2. Given real numbers x i , x j ∈ R and u i , u j i.i.d. sampled from uniform distribution within (0, 1). With Gumbel perturbation defined as g(u) = − log(− log(u)), we have the probability</p><formula xml:id="formula_45">P (x i + g(u i ) &gt; x j + g(u j )) = 1 1 + exp (−(x i − x j ))</formula><p>.</p><p>Proof. Due to g(u) = − log(− log(u)), the inequality of interests x i + g(u i ) &gt; x j + g(u j ) can be rearranged as</p><formula xml:id="formula_46">e xi−xj &gt; log(u i ) log(u j ) . (<label>22</label></formula><formula xml:id="formula_47">)</formula><p>Since log(u j ) &lt; 0, Eqn. 22 can be written as</p><formula xml:id="formula_48">u j &lt; u e x j −x i i . (<label>23</label></formula><formula xml:id="formula_49">)</formula><p>As u i , u j are i.i.d. sampled from a uniform distribution, the probability when the above formula can be calculated via:</p><formula xml:id="formula_50">1 0 u e x j −x i i 0 du j du i = 1 0 u e x j −x i i du i = 1 1 + exp(−(x i − x j )) .<label>(24)</label></formula><p>Thus, we conclude the proof with</p><formula xml:id="formula_51">P (x i + g(u i ) &gt; x j + g(u j )) = 1 1 + exp (−(x i − x j )) .<label>(25)</label></formula><p>Proposition 3. Let X ∼ Gumbel(α, τ ) (i.e. X k = exp((log α k +g k )/τ ) n i=1 exp((log αi+gi)/τ ) ) with location parameters α ∈ (0, ∞) n and temperature τ ∈ (0, ∞), then:</p><formula xml:id="formula_52">• P (X k &gt; X i , ∀i ̸ = k) = α k n i=1 αi , • P (lim τ →0 X k = 1) = α k n i=1 αi .</formula><p>Proof. This result can be similarly proved as Lemma 2. The event of interests</p><formula xml:id="formula_53">X k &gt; X i , ∀i ̸ = k is equivalent to log α k − log(− log u k ) &gt; log α 1 − log(− log u 1 ), log α k − log(− log u k ) &gt; log α 2 − log(− log u 2 ), ... log α k − log(− log u k ) &gt; log α n − log(− log u n ).<label>(26)</label></formula><p>Since all the above inequalities are independent given u k , we can rearrange the first inequality as</p><formula xml:id="formula_54">u 1 &lt; u α1/α k k ≤ 1.<label>(27)</label></formula><p>Since u 1 ∼ U [0, 1], the probability for the first inequality in Eqn. 26 being true would be u α1/α k k . Thus, the probability for Eqn. 26 being true can be calculated via</p><formula xml:id="formula_55">u α1/α k k u α2/α k k ...g αn/α k k = g (α1+α2+...+αn)/α k k = g (1/α k )−1 k . (<label>28</label></formula><formula xml:id="formula_56">)</formula><p>For simplicity, we assume n i=1 α i = 1. Then for any g k ∈ [0, 1], we obtain</p><formula xml:id="formula_57">P (X k &gt; X i , ∀i ̸ = k) = 1 0 g (1/α k )−1 k dg k = α k n i=1 α i ,<label>(29)</label></formula><p>and arrive at the result for the first bullet point. For the second bullet point, when τ → 0, we have</p><formula xml:id="formula_58">lim τ →0 exp((log α i + g i )/τ ) exp((log α j + g j )/τ ) = lim τ →0 exp((log α i + g i − log α j − g j )/τ ) = ∞, if α i &gt; α j 0, otherwise.<label>(30)</label></formula><p>Such a fact indicates that the output of a Concrete distribution with τ → 0 will be a one-hot vector (X arg maxi αi = 1). This yields the conclusion that</p><formula xml:id="formula_59">P ( lim τ →0 X k = 1) = P (X k &gt; X i , ∀i ̸ = k) = α k n i=1 α i .<label>(31)</label></formula><p>Now we turn to the proof of our theorem. We are to prove that the kernelized form in Eqn. 7 has the same property as the original Gumbel-Softmax in the limit sense (when τ goes to zero). We recall that we have defined</p><formula xml:id="formula_60">q u = W (l) Q z (l) u , k u = W (l) K z (l) u and v u = W (l) V z<label>(l)</label></formula><p>u for simplicity. First, by definition we have</p><formula xml:id="formula_61">ϕ( q u √ τ ) ⊤ ϕ( k v √ τ )e gv τ = 1 m exp(− || qu √ τ || 2 + || kv √ τ || 2 2 ) m i=1 exp(ω ⊤ i ( q u √ τ + k v √ τ ) + g v τ ).<label>(32)</label></formula><p>The property holds that for ∀w ̸ = v, we have</p><formula xml:id="formula_62">lim τ →0 ϕ( qu √ τ ) ⊤ ϕ( kv √ τ )e gv τ ϕ( qu √ τ ) ⊤ ϕ( kw √ τ )e gw τ</formula><p>equals to ∞ or 0, i.e. the output of the kernelized Gumbel-Softmax is still a one-hot vector when τ → 0. Let</p><formula xml:id="formula_63">Y v = ϕ( qu √ τ ) ⊤ ϕ( kv √ τ )e gv τ N w=1 ϕ( qu √ τ ) ⊤ ϕ( kw √ τ )e gw τ .<label>(33)</label></formula><p>Here Y v is defined in the same way as c uv in Section 3.2. We thus have τ ). To keep notation clean, we define</p><formula xml:id="formula_64">P (lim τ →0 Y v = 1) = P (Y v &gt; Y v ′ , ∀v ′ ̸ = v). To compute P (Y v &gt; Y v ′ , ∀v ′ ̸ = v), for simplicity, let us consider the probability P (Y v &gt; Y v ′ ) = P (ϕ( qu √ τ ) ⊤ ϕ( kv √ τ )e gv τ &gt; ϕ( qu √ τ ) ⊤ ϕ( k v ′ √ τ )e g v ′</formula><formula xml:id="formula_65">β v = ϕ( q u √ τ ) ⊤ ϕ( k v √ τ ), β v ′ = ϕ( q u √ τ ) ⊤ ϕ( k v ′ √ τ ).<label>(34)</label></formula><p>Then the above-mentioned probability can be rewritten as P (log</p><formula xml:id="formula_66">β v + gv τ &gt; log β v ′ + g v ′ τ ), where β v and β v ′ are two i.i.d. random variables. From Lemma 1, we have E(β v ) = exp(q ⊤ u k v /τ ) = α 1 τ v , E(β v ′ ) = exp(q ⊤ u k v ′ /τ ) = α 1 τ</formula><p>v ′ , where α v and α v ′ are two constant values. Then using Lemma 2, we have</p><formula xml:id="formula_67">P (log α 1/τ v + g v τ &gt; log α 1/τ v ′ + g v ′ τ ) =P (log α v + g v &gt; log α v ′ + g v ′ ) = 1 1 + exp(log α v ′ − log α v ) = α v ′ α v + α v ′ .<label>(35)</label></formula><p>According to the Chebyshev's inequality, we have</p><formula xml:id="formula_68">P (|β v − α 1 τ v | ≤ ϵ v ) ≥ 1 − σ 2 v ϵ 2 v . Here σ 2 v = V w ( SM m ( qu √ τ , kj √ τ ))</formula><p>, which can given by Lemma 1.</p><p>Due to the convexity of logarithmic function, we have</p><formula xml:id="formula_69">| log β v − 1 τ log α v | |β v − α 1 τ v | ≤ 1 α 1 τ v − ϵ v ,<label>(36)</label></formula><p>and subsequently,</p><formula xml:id="formula_70">| log β v − 1 τ log α v | ≤ |β v − α 1 τ v | α 1 τ v − ϵ v ≤ ϵ v α 1 τ v − ϵ v .<label>(37)</label></formula><p>Therefore we have</p><formula xml:id="formula_71">P (| log β v − 1 τ log α v | ≤ ϵv α 1 τ v −ϵv ) ≥ P (|β v − α 1 τ v | ≤ ϵ v ).</formula><p>Based on this, we can derive the result:</p><formula xml:id="formula_72">P (| log β v − 1 τ log α v | ≤ ϵ v α 1 τ v − ϵ v ) ≥ 1 − σ 2 v ϵ 2 v .<label>(38)</label></formula><p>Since β v and β v ′ are two i.i.d. random variables, we have</p><formula xml:id="formula_73">P (| log β v − 1 τ log α v | ≤ ϵ v α 1 τ v − ϵ v , | log β v ′ − 1 τ log α v ′ | ≤ ϵ v ′ α 1 τ v ′ − ϵ v ′ ) ≥ (1 − σ 2 v ϵ 2 v )(1 − σ 2 v ′ ϵ 2 v ′ ).<label>(39)</label></formula><p>For simplicity, we denote ϵ = ϵv α</p><formula xml:id="formula_74">1 τ v −ϵv + ϵ v ′ α 1 τ v ′ −ϵ v ′ and P ϵ = (1 − σ 2 v ϵ 2 v )(1 − σ 2 v ′ ϵ 2 v ′</formula><p>). We therefore have</p><formula xml:id="formula_75">P (| log β v − 1 τ log α v | + | log β v ′ − 1 τ log α v ′ | ≤ ϵ) ≥ P ϵ .<label>(40)</label></formula><p>Using the triangular inequality, we can yield</p><formula xml:id="formula_76">|(log β v − 1 τ log α v ) − (log β v ′ − 1 τ log α v ′ )| ≤ | log β v − 1 τ log α v | + | log β v ′ − 1 τ log α v ′ |.<label>(41)</label></formula><p>Combining Eqn. 40 and 41, we have</p><formula xml:id="formula_77">P (|(log β v − 1 τ log α v ) − (log β v ′ − 1 τ log α v ′ )| ≤ ϵ) ≥ P ϵ .<label>(42)</label></formula><p>Let c = log</p><formula xml:id="formula_78">β v − log β v ′ , so that E(c) = 1 τ (log α v − log α v ′ ).</formula><p>From Eqn. 42, we can obtain</p><formula xml:id="formula_79">P (c ≥ E(c) − ϵ) ≥ P ϵ .<label>(43)</label></formula><p>According to Lemma 2, the probability</p><formula xml:id="formula_80">P (E(c) − ϵ ≥ g v ′ −gv τ</formula><p>) can be written as</p><formula xml:id="formula_81">P (log α v − log α v ′ − τ ϵ ≥ g v ′ − g v ) = 1 1 + exp(log α v ′ − log α v + τ ϵ) = 1 1 + α v ′ αv e τ ϵ .<label>(44)</label></formula><p>Since c, g v , g v ′ are generated independently, combining Eqn. 43 and 44, we can yield</p><formula xml:id="formula_82">P (c ≥ g v ′ − g v τ ) ≥ P ϵ 1 + α v ′ αv e τ ϵ .<label>(45)</label></formula><p>Similarly, from Eq. 42 we have P (c ≤ E(c) + ϵ) ≥ P ϵ and subsequently,</p><formula xml:id="formula_83">P ( g v ′ − g v τ ≥ E(c) + ϵ) = 1 − P (c + ϵ ≥ g v ′ − g v τ ) = 1 − 1 1 + α v ′ αv e −τ ϵ .<label>(46)</label></formula><p>Thus we have P (c</p><formula xml:id="formula_84">≤ g v ′ −gv τ ) ≥ P ϵ (1 − 1 1+ α v ′ αv e −τ ϵ</formula><p>) and also</p><formula xml:id="formula_85">P (c ≥ g v ′ − g v τ ) ≤ 1 − P ϵ (1 − 1 1 + α v ′ αv e −τ ϵ ).<label>(47)</label></formula><p>Combining Eqn. 45 and 47, we conclude that</p><formula xml:id="formula_86">P ϵ 1 + α v ′ αv e τ ϵ ≤ P (c ≥ g v ′ − g v τ ) ≤ 1 − P ϵ (1 − 1 1 + α v ′ αv e −τ ϵ ).<label>(48)</label></formula><p>Based on this we consider the limitation for two sides and thus obtain lim</p><formula xml:id="formula_87">Pϵ→1 lim τ →0 P (c ≥ g v ′ − g v τ ) = 1 1 + α v ′ αv = α v α v + α v ′ . (<label>49</label></formula><formula xml:id="formula_88">)</formula><p>Then with similar reasoning as Lemma 3, we have</p><formula xml:id="formula_89">lim Pϵ→1 lim τ →0 P (Y v = 1) = lim Pϵ→1 lim τ →0 P (Y v &gt; Y v ′ , ∀v ′ ̸ = v) = α v /( N w=1 α w ).<label>(50)</label></formula><p>Recall that</p><formula xml:id="formula_90">P ϵ = (1 − σ 2 v ϵ 2 v )(1 − σ 2 v ′ ϵ 2 v ′ ) σ 2 = V w ( SM + m (x, y)) = 1 m exp(− ∥x∥ 2 + ∥y∥ 2 2 ) m i=1 exp(w ⊤ i (x + y)),<label>(51)</label></formula><p>where</p><formula xml:id="formula_91">x = qu √ τ , y = k v,v ′ √ τ .</formula><p>Therefore, P ϵ is dependent of the precision ϵ v , ϵ v ′ , the random feature dimension m, and the temperature τ . If m is sufficiently large, σ would converge to zero and P ϵ goes to 1. In such a case, Eqn. 50 holds once τ tends to zero. We thus conclude the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Proof for Proposition 1</head><p>According to our definitions in Section 5, we have</p><formula xml:id="formula_92">D KL (q ϕ ( Ã|X, A)∥p( Ã|Y, X, A)) = A * q ϕ ( Ã|X, A) log q ϕ ( Ã|X, A) p( Ã|Y, X, A) d Ã = A * q ϕ ( Ã|X, A) log q ϕ ( Ã|X, A)p θ (Y|X, A) p( Ã, Y|X, A) d Ã = A * q ϕ ( Ã|X, A) log q ϕ ( Ã|X, A)p θ (Y|X, A) p( Ã, Y|X, A) d Ã = A * q ϕ ( Ã|X, A) log q ϕ ( Ã|X, A)p θ (Y|X, A) p(Y| Ã, X, A)p( Ã|X, A) d Ã = − E q ϕ ( Ã|X,A) [log p(Y| Ã, X, A)] + log p θ (Y|X, A) + D KL (q ϕ ( Ã|X, A)∥p( Ã|X, A)) = − ELBO(θ, ϕ) + log p θ (Y|X, A)<label>(52)</label></formula><p>Since we assume q ϕ can exploit arbitrary distributions over Ã, we know that when the ELBO is optimized to the optimum, D KL (q ϕ ( Ã|X, A)∥p( Ã|Y, X, A)) = 0 holds. Otherwise, there exists ϕ * ̸ = ϕ such that ELBO(θ, ϕ * ) &gt; ELBO(θ, ϕ). Pushing further, when the optimum is achieved, log p θ (Y|X, A) would equal to ELBO and namely is maximized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Implementation Details</head><p>We present implementation details in our experiments for reproducibility. For more concrete details concerning architectures and hyper-parameter settings for NODEFORMER, one can directly refer to our public repository https://github.com/qitianwu/NodeFormer. We next present descriptions for baseline models' implementation. For baseline models MLP, GCN, GAT, MixHop and JKnet, we use the implementation provided by <ref type="bibr" target="#b19">[20]</ref> <ref type="foot" target="#foot_1">3</ref> . For DropEdge and two structure learning baseline models (LDS and IDGL), we also refer to their implementation provided by the original papers <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b3">4]</ref>. Concretely, we use GCN as the backbone for them.</p><p>C.1 Details for Node Classification Experiments in Sec. 4.1</p><p>Architectures. For experiments on the datasets Cora, Citeseer, Deezer and Actor, the baseline models (GCN, GAT, MixHop, JKNet) are implemented with the following settings:</p><p>• Two GNN layers with hidden size 32 by default (unless otherwise mentioned). GAT uses 8 attention heads followed by its original setting.</p><p>• The activation function is ReLU (except GAT using ELU).</p><p>The architecture of our NODEFORMER is specified as follows:</p><p>• Two message-passing layers with hidden size 32. We also consider multi-head designs for our all-pair attentive message passing, and for each head we use independent parameterization. The results for different heads are combined in an average manner in each layer.</p><p>• The activation function is ELU that is only used for input MLP, and we do not use any activation for the feature propagation layers. In terms of relational bias, we specify σ as sigmoid function and consider 2-order adjacency to strengthen the observed links of the input graph.</p><p>Training Details. In each epoch, we feed the whole data into the model, calculate the loss and conduct gradient descent accordingly. Concretely, we use BCE-Loss for two-class classification and NLL-Loss for multi-class classification, the Adam optimizer is used for gradient-based optimization.</p><p>The training procedure will repeat the above process until a given budget of 1000 epochs. Finally, we report the test accuracy achieved by the epoch that gives the highest accuracy on validation dataset.</p><p>Hyperparameters. For each model, we use grid search on validation set for hyper-parameter setting. The learning rate is searched within {0.01, 0.001, 0.0001, 0.00001}, weight decay within {0.05, 0.005, 0.0005, 0.00005}, and dropout probability within {0.0, 0.5}. The hyper-parameters for NODEFORMER is provided in our public codes. The hyperparameters for baseline models are set as follows (we use the same notation as the original papers).</p><p>• For GCN and GAT, the learning rate is 0.01, and weight decay is set to 0.05. No dropout is used.</p><p>• For MixHop, the hyperparameters are the same as above, except that we further use grid search for hidden channels within {8, 16, 32, 64, 128}. We adopt 2 hops for all the four datasets.</p><p>• For JKNet, GCN is used as the backbone. Learning rate is set to 0.01 for Deezer and 0.001 for all the other three datasets. We concatenate the features in the final stage for Deezer, while we use max-pooling for the three other datasets. The hidden size is set as default, except for Deezer as 64.</p><p>• For DropEdge, the hidden size is chosen from {32, 64, 96, 128, 160}, the learning rate is within {0.01, 0.001, 0.0001}, and the dropedge rate is chosen from {0.3, 0.4, 0.5}.</p><p>• For LDS, the sampling time S = 16, the patience window size ρ = 6, the hidden size ∈ {8, 16, 32, 64}, the inner learning rate γ ∈ {1e-4, 1e-3, 1e-2, 1e-1}, and the number of updates used to compute the truncated hypergradient τ ∈ {5, 10, 15}.</p><p>• For IDGL, we use its original version without anchor approximation on Cora, Citeseer and Actor. For Deezer, even using anchor approximation, it would also suffer from out-of-memory. Besides, we set: ϵ = 0.01, hidden size ∈ {16, 64, 96, 128}, λ ∈ {0. Architectures. For experiments on the two large datasets (OGB-Proteins and Amazon2M), the baseline models are implemented with the following settings:</p><p>• Three GNN layers with hidden size 64.</p><p>• The activation function is ReLU (except GAT using ELU).</p><p>The architecture of our NODEFORMER is specified as follows:</p><p>• Three message-passing layers with hidden size 64. The head number is set as 1.</p><p>• The activation function is ELU that is used for all the layers. In terms of relational bias, we specify σ as identity function and consider 1-order adjacency to strengthen the observed links of the input graph.</p><p>Training Details. In each epoch, we use random mini-batch partition to split the whole set of nodes and feed each mini-batch of nodes into the model for all-pair propagation, as we mentioned in Section 4. Hyperparameters. The hyperparameters for baseline models are listed as follows.</p><p>• For GCN, the learning rate is 0.01 and the weight decay is 5e-4 on both datasets. The size of hidden channel is set to 64. Dropout is not used during training. • For GAT, the learning rate is 0.001 and the weight decay is 5e-3 on both datasets. The size of hidden channel is 32 on Mini-ImageNet and 64 on 20News. No dropout is used during training. The number of attention heads is 8. • For LDS, its hyperparameters are determined by grid search in the same manner as in the transductive setting. • For IDGL, we use its anchor-based version that can scale to these two datasets. Besides, on 20News, we set: hidden size=64, learning rate=0.01 λ = 0.7, η = 0.1, α = 0.1, β = 0, γ = 0.1, ϵ = 0.01, m = 12. On Mini-ImageNet, we set: hidden size=96, learning rate=0.01 λ = 0.8, η = 0.2, α = 0, β = 0, γ = 0.1, ϵ = 0.01, m = 12.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Dataset Information</head><p>We present detailed information for our used datasets concerning the data collection, preprocessing and statistic information. Table <ref type="table" target="#tab_7">5</ref> provides an overview of the datasets we used in the experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Dataset Information</head><p>Node Classification Datasets. For experiments on transductive node classification, we evaluate our model on two homophilous datasets Cora and Citeseer <ref type="bibr" target="#b31">[32]</ref>, and other two non-homophilous datasets Actor <ref type="bibr" target="#b24">[25]</ref> and Deezer <ref type="bibr" target="#b27">[28]</ref>. The first two are citation network datasets that contain sparse bag-of-words feature vectors for each document and a list of citation links between documents. The citation links are treated as (undirected) edges and each document has a class label. Deezer is a social network of users on Deezer from European countries, where edges represent mutual follower relationships. The node features are based on artists liked by each user and nodes are labeled with reported gender. Actor is a graph representing actor co-occurrence in Wikipedia pages. Each node corresponds to an actor, and the edge between two nodes denotes co-occurrence on the same Wikipedia page. Node features correspond to some keywords in the Wikipedia pages and the nodes are classified into five categories w.r.t. words of actor's Wikipedia. These four datasets are relatively small with thousands of instances and edges (Deezer is the largest one with nearly 20K nodes).</p><p>To evaluate NODEFORMER's scalability, we further consider two large datasets: OGB-Proteins <ref type="bibr" target="#b14">[15]</ref> and Amazon2M <ref type="bibr" target="#b4">[5]</ref>. These two datasets have million-level nodes and edges and require the model for scalable training. The OGB-Proteins dataset is an undirected, and typed (according to species) graph in which nodes represent proteins and edges indicate different types of biologically meaningful associations between proteins. All edges come with 8-dimensional features, where each dimension represents the approximate confidence of a single association type and takes on values between 0 and 1. The proteins come from 8 species and our task is to predict the presence of 112 protein functions in a multi-label binary classification setup respectively. Amazon2M is extracted from Amazon Co-Purchasing network <ref type="bibr" target="#b22">[23]</ref>, where each node represents a product, and the graph link represents whether two products are purchased together, the node features are generated by extracting bag-of-word features from the product descriptions. The top-level categories are used as labels for the products.</p><p>Graph-enhanced Application Datasets. We evaluate our model on two datasets without graph structure: 20News-Groups <ref type="bibr" target="#b23">[24]</ref> and Mini-ImageNet <ref type="bibr" target="#b36">[37]</ref>. The 20News dataset is a collection of approximately 20,000 newsgroup documents (nodes), partitioned (nearly) evenly across 20 different newsgroups. We take 10 classes from 20 newsgroups and use words (TFIDF) with a frequency of more than 5% as features. The Mini-ImageNet dataset consists of 84×84 RGB images from 100 different classes with 600 samples per class. For our experiment use, we choose 30 classes from the dataset, each with 600 images (nodes) that have 128 features extracted by CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Dataset Preprocessing</head><p>All the datasets we used in the experiment are directly collected from the source, except Mini-ImageNet, whose features are extracted by ourselves. Following the setting of <ref type="bibr" target="#b29">[30]</ref>, we compute node embeddings via a CNN model with 4 convolutional layers followed by a fully-connected layer resulting in a 128 dimensional embedding. Finally, the 128 dimensional outputs are treated as the features of the nodes (images) for subsequent GNN-based downstream task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E More Experiment Results</head><p>We present extra ablation study results on the four transductive datasets for NODEFORMER w/ and w/o relational bias and edge-level regularization. Fig. <ref type="figure">6</ref> studies the impact of the temperature τ and the dimension of feature map m on Cora. Furthermore, we visualize the attention maps of two model layers and compare with original input graphs of Cora, Citeseer, Deezer and Actor in Fig. <ref type="figure">7</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Current Limitations, Outlooks and Potential Impacts</head><p>Current Limitations. In the present work, we focus on node classification for experiments, though NODEFORMER can be used as a flexible (graph) encoder for other graph-related problems such as graph classification, link prediction, etc. Beyond testing accuracy, social aspects like robustness and explainability can also be considered as the target for future works on top of NODEFORMER.</p><p>Potential Impact. Besides facilitating better node representations via message passing, graph structure learning also plays as key components in many other perpendicular problems in graph learning community, like explainability <ref type="bibr" target="#b45">[46]</ref>, adversarial robustness <ref type="bibr" target="#b48">[49]</ref>, training acceleration <ref type="bibr" target="#b32">[33]</ref>, handling feature extrapolation <ref type="bibr" target="#b39">[40]</ref> and cold-start users in recommendation <ref type="bibr" target="#b40">[41]</ref>. NODEFORMER can serve as a plug-in scalable structure learning encoder for uncovering underlying dependence, identifying novel structures and purifying noisy data in practical systems. Another promising direction is to leverage our kernelized Gumbel-Softmax operator as a plug-in module for designing efficient and expressive Transformers on graph data where the large graph size plays a critical performance bottleneck.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>3</head><label></label><figDesc>NODEFORMER: A Transformer Graph Network at Scale Let G = (N , E) denote a graph with N a node set (|N | = N ) and E ⊆ N × N an edge set (|E| = E).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Illustration for the data flow of NODEFORMER which takes node embedding matrix X and (optional) graph adjacency matrix A as input. There are three components in NODEFORMER. The first one is the all-pair message passing (MP) module (colored red) which adopts our proposed kernelized Gumbel-Softmax operator to update node embeddings in each layer with O(N ) complexity. The other two components are optional based on the availability of input graphs: 1) relational bias (colored green) that reinforces the propagation weight on observed edges; 2) edge regularization loss (colored blue) that aims to maximize the probability for observed edges. These two components require O(E) complexity. The final training loss L is the weighted sum of the standard supervised classification loss and the edge regularization loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Experimental results for node classification in transductive setting on four common datasets. The missing results on Deezer is caused by out-of-memory (OOM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparison of training/inference time and GPU memory cost w.r.t. different instance numbers (by removing a certain portion of nodes) on 20News-Groups.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: Visualization of node embeddings and edge connections produced by NODEFORMER on graph-enhanced application datasets. We mark the nodes with a particular class with one color. More comparison between the learned structures and original input graphs is presented in Appendix E.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>5</head><label>5</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Alg. 1 presents the details for NODEFORMER's feed-forward process from a matrix view that is practically used in our implementation. The figure illustrates the layer-wise node representation updating based on the kernelized Gumbel-Softmax operator, which reduces the algorithmic complexity from quadratic to O(N ) via avoiding explicit computation of the all-pair similarities. ⊙ in Alg. 1 denotes element-wise product. φ(•) in the figure represents the random feature map with Gumbel noise whose details are shown by the blue part of Alg. 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Figure 6: Impact of the temperature τ and the dimension of random feature map m on Cora.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Testing ROC-AUC and training memory cost on OGB-Proteins with batch size 10K.</figDesc><table><row><cell>Method</cell><cell cols="2">ROC-AUC (%) Train Mem</cell></row><row><cell>MLP GCN SGC GraphSAINT-GCN GraphSAINT-GAT NODEFORMER NODEFORMER-dt NODEFORMER-tp</cell><cell>72.04 ± 0.48 72.51 ± 0.35 70.31 ± 0.23 73.51 ± 1.31 74.63 ± 1.24 77.45 ± 1.15 75.50 ± 0.64 76.18 ± 0.09</cell><cell>2.0 GB 2.5 GB 1.2 GB 2.3 GB 5.2 GB 3.2 GB 3.1 GB 3.2 GB</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Testing Accuracy and training memory cost on Amazon2M with batch size 100K.</figDesc><table><row><cell>Method</cell><cell cols="2">Accuracy (%) Train Mem</cell></row><row><cell>MLP GCN SGC GraphSAINT-GCN GraphSAINT-GAT NODEFORMER NODEFORMER-dt NODEFORMER-tp</cell><cell>63.46 ± 0.10 83.90 ± 0.10 81.21 ± 0.12 83.84 ± 0.42 85.17 ± 0.32 87.85 ± 0.24 87.02 ± 0.75 87.55 ± 0.11</cell><cell>1.4 GB 5.7 GB 1.7 GB 2.1 GB 2.2 GB 4.0 GB 2.9 GB 4.0 GB</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Experimental results on semi-supervised classficiation on Mini-ImageNet and 20News-Groups where we use k-NN (with different k's) for artificially constructing an input graph. ± 0.42 85.61 ± 0.40 85.93 ± 0.59 85.96 ± 0.66 65.98 ± 0.68 64.13 ± 0.88 62.95 ± 0.70 62.59 ± 0.62 GAT 84.70 ± 0.48 85.24 ± 0.42 85.41 ± 0.43 85.37 ± 0.51 64.06 ± 0.44 62.51 ± 0.71 61.38 ± 0.88 60.80 ± 0.59 DropEdge 83.91 ± 0.24 85.35 ± 0.44 85.25 ± 0.63 85.81 ± 0.65 64.46 ± 0.43 64.01 ± 0.42 62.46 ± 0.51 62.68 ± 0.71 IDGL 83.63 ± 0.32 84.41 ± 0.35 85.50 ± 0.24 85.66 ± 0.42 65.09 ± 1.23 63.41 ± 1.26 61.57 ± 0.52 62.21 ± 0.79 LDS OOM OOM OOM OOM 66.15 ± 0.36 64.70 ± 1.07 63.51 ± 0.64 63.51 ± 1.75 NODEFORMER 86.77 ± 0.45 86.74 ± 0.23 86.87 ± 0.41 86.64 ± 0.42 66.01 ± 1.18 65.21 ± 1.14 64.69 ± 1.31 64.55 ± 0.97</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Method</cell><cell>k = 5</cell><cell></cell><cell cols="4">Mini-ImageNet k = 10 k = 15</cell><cell>k = 20</cell><cell cols="2">k = 5</cell><cell></cell><cell cols="2">20News-Group k = 10 k = 15</cell><cell>k = 20</cell></row><row><cell cols="5">GCN 84.86 NODEFORMER w/o graph</cell><cell></cell><cell></cell><cell cols="2">87.46 ± 0.36</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>64.71 ± 1.33</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>IDGL</cell><cell></cell><cell>LDS</cell><cell cols="2">NodeFormer</cell><cell></cell><cell></cell></row><row><cell>training time (s)</cell><cell>0 200 400 600</cell><cell>0</cell><cell cols="2">1000 # removed nodes 2000 3000</cell><cell>4000</cell><cell>testing time (s)</cell><cell>0.00 0.25 0.50 0.75 1.00 1.25</cell><cell>0</cell><cell cols="2">1000 # removed nodes 2000 3000</cell><cell>4000</cell><cell>GPU usage (MB)</cell><cell>2000 4000 6000 8000 10000</cell><cell>0</cell><cell>1000 # removed nodes 2000 3000</cell><cell>4000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>are 84×84 RGB images and we randomly choose 30 classes each of which contains 600 samples for experiments. 20News-Groups<ref type="bibr" target="#b23">[24]</ref> consists of nearly 10K texts whose features are extracted by TF-IDF. More details for preprocessing are presented in Appendix D. Also, for each dataset, we randomly split instances into 50%/25%/25% for train/valid/test. Since there is no input graph, we use k-NN (over input node features) for artificially constructing a graph for enabling GNN's message passing and the graph-based components (edge regularization and relational bias) of NODEFORMER. Table4presents the comparison results under different k's. We can see that NODEFORMER achieves the best performance in seven cases out of eight. The performance of GNN competitors varies significantly with different k values, and NODEFORMER is much less sensitive. Intriguingly, when we do not use the input graph, i.e., removing both the edge regularization and relational bias, NODEFORMER can still yield competitive even superior results on Mini-ImageNet. This suggests that the k-NN graphs are not necessarily informative and besides, our model learns useful latent graph structures from data.4.4 Further DiscussionsComparison of Time/Space Consumption. Fig.3plots training/inference time and GPU memory costs of NODEFORMER and two SOTA structure learning models. Compared with LDS, NODE-FORMER reduces the training time, inference time, memory cost by up to 93.1%, 97.9%, 75.6%, respectively; compared with IDGL (using anchor-based approximation for speedup), NODEFORMER reduces the training time, inference time, memory cost by up to 61.8%, 80.8%, 80.6%, respectively.Ablation on Stochastic Components. Table2and 3 also include two variants of NODEFORMER for ablation study. 1) NODEFORMER-dt: replace Gumbel-Softmax by original Softmax (with temperature 1.0) for deterministic propagation; 2) NODEFORMER-tp: use original Softmax with temperature set as 0.25 (the same as NODEFORMER). There is performance drop when removing the Gumbel components, which may be due to over-normalizing or over-fitting that are amplified in large datasets, as we discussed in Section 3.1 and the kernelized Gumbel-Softmax operator shows its effectiveness.Ablation on Edge Loss and Relational Bias. We study the effects of edge-level regularization and relation bias as ablation study shown in Table6located in Appendix E, where the results consistently show that both components contribute to some positive effects and suggest that our edge-level loss and relation bias can both help to leverage useful information from input graphs.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>.2 Details for Node Classification on Larger Graphs in Sec. 4.2</figDesc><table /><note>5, 0.6, 0.7, 0.8}, η ∈ {0, 0.1, 0.2}, α ∈ {0, 0.1, 0.2}, β ∈ {0, 0.1}, γ ∈ {0.1, 0.2}, m ∈ {6, 9, 12}.C</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>1. Similarly, we use BCE-Loss for two-class classification and NLL-Loss for multi-class classification, the Adam optimizer is used for gradient-based optimization. The training procedure will repeat the above process until a given budget of 1000 epochs. The evaluation on testing data is conducted on CPU which enables full-batch feature propagation. Finally, we report the test accuracy achieved by the epoch that gives the highest accuracy on validation dataset. C.3 Details for Graph-Enhanced Experiments in Sec. 4.3Architectures. The architectures of baselines (GCN, GAT, LDS and IDGL) and NODEFORMER model are the same as the transductive setting, except that we use grid search to adaptively tune the hidden size. Besides, we also adopt BatchNorm for baseline models.</figDesc><table /><note>Training Details. The input data have no graph structures in this setting. As mentioned in Section 4.3, we use k-NN for artificially constructing a graph to enable message passing. The training procedure is the same as the transductive setting.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Information for experiment datasets.</figDesc><table><row><cell>Dataset Cora Citeseer Deezer Actor OGB-Proteins Amazon2M Mini-ImageNet 20News-Groups</cell><cell>Context Citation network Citation network Social network Actors in movies Protein interaction Product co-occurrence long-range dependence Property homophilous homophilous non-homophilous non-homophilous multi-task classification Image classification no graph/k-NN graph Text classification no graph/k-NN graph</cell><cell># Task 1 1 1 1 112 1 1 1</cell><cell># Nodes 2,708 3,327 28,281 7,600 132,534 2,449,029 61,859,140 # Edges 5,429 4,732 92,752 29,926 39,561,252 18,000 0 9,607 0</cell><cell># Node Feat # Class 1,433 7 3,703 6 31,241 2 931 5 8 2 100 47 128 30 236 10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Ablation study results on transductive datasets, where "rb" denotes relational bias and "reg" represents edge-level regularization. Cora 88.69 ± 0.46 81.98 ± 0.46 88.06 ± 0.59 Citeseer 76.33 ± 0.59 70.60 ± 1.20 74.12 ± 0.64 Deezer 71.24 ± 0.32 71.22 ± 0.32 71.10 ± 0.36 Actor 35.31 ± 1.29 35.15 ± 1.32 34.60 ± 1.32</figDesc><table /><note>DatasetNODEFORMER NODEFORMER w/o reg NODEFORMER w/o rb</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">We assume one latent graph to simplify the illustration though we practically learn layer-specific graphs for each layer of NODEFORMER. The analysis can be trivially extended to such a case.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">https://github.com/CUAI/Non-Homophily-Benchmarks.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was partly supported by National Key Research and Development Program of China (2020AAA0107600), National Natural Science Foundation of China (61972250, 72061127003), and Shanghai Municipal Science and Technology (Major) Project (22511105100, 2021SHZDZX0102).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazanin</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrayr</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Ver Steeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the bottleneck of graph neural networks and its practical implications</title>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><surname>Vandergheynst</surname></persName>
		</author>
		<idno>CoRR, abs/1611.08097</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Iterative deep graph learning for graph neural networks: Better and robust node embeddings</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><forename type="middle">J</forename><surname>Zaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks</title>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rethinking attention with performers</title>
		<author>
			<persName><forename type="first">Valerii</forename><surname>Krzysztof Marcin Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyou</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreea</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamás</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Sarlós</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">Quincy</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afroz</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">Benjamin</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><forename type="middle">J</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Colwell</surname></persName>
		</author>
		<author>
			<persName><surname>Weller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Latent patient network learning for automatic diagnosis</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Cosmo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anees</forename><surname>Kazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seyed-Ahmad</forename><surname>Ahmadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno>CoRR, abs/2003.13620</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning steadystates of iterative algorithms over graphs</title>
		<author>
			<persName><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1114" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A generalization of transformer networks to graphs</title>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dwivedi</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<idno>CoRR, abs/2012.09699</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Variational inference for graph convolutional networks in the absence of graph data and adversarial settings</title>
		<author>
			<persName><forename type="first">Pantelis</forename><surname>Elinas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edwin</forename><forename type="middle">V</forename><surname>Bonilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><forename type="middle">C</forename><surname>Tiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning discrete structures for graph neural networks</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Franceschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="1972">1972-1982, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Room-and-object aware knowledge reasoning for remote embodied referring expression</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3064" to="3073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Implicit graph neural networks</title>
		<author>
			<persName><forename type="first">Fangda</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somayeh</forename><surname>Sojoudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><forename type="middle">El</forename><surname>Ghaoui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with graph learning-convolutional networks</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doudou</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11313" to="11320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Graph structure learning for robust graph neural networks</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaorui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianfeng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="66" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">New benchmarks for learning on non-homophilous graphs</title>
		<author>
			<persName><forename type="first">Derek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hohne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
		<idno>CoRR, abs/2104.01404</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to drop: Robust graph neural network via topological denoising</title>
		<author>
			<persName><forename type="first">Dongsheng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenchao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingchao</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Web Search and Data Mining</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="779" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Inferring networks of substitutable and complementary products</title>
		<author>
			<persName><forename type="first">Julian</forename><forename type="middle">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Scikitlearn: Machine learning in python</title>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaël</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bertrand</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Geom-gcn: Geometric graph convolutional networks</title>
		<author>
			<persName><forename type="first">Hongbin</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingzhe</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Chen-Chuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Random features for large-scale kernel machines</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1177" to="1184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dropedge: Towards deep graph convolutional networks on node classification</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Characteristic functions on graphs: Birds of a feather, from statistical descriptors to parametric models</title>
		<author>
			<persName><forename type="first">Benedek</forename><surname>Rozemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rik</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Information and Knowledge Management</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1325" to="1334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to simulate complex physics with graph networks</title>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Godwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Pfaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8459" to="8468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName><forename type="first">Garcia</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Satorras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Estrach</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename><surname>Ah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><surname>Monfardini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="61" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Mag</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="106" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Fast graph attention networks using effective resistance based graph sparsification</title>
		<author>
			<persName><forename type="first">Rakshith</forename><surname>Sharma Srinivasa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Romberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
		</author>
		<idno>CoRR, abs/2006.08796</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Graph structure learning with variational information bottleneck</title>
		<author>
			<persName><forename type="first">Qingyun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingcheng</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno>CoRR, abs/2112.08903</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
	<note>Attention is all you need</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dynamic graph CNN for learning on point clouds</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><forename type="middle">H</forename><surname>Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Towards open-world feature extrapolation: An inductive graph learning approach</title>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="19435" to="19447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Towards open-world recommendation: An inductive model-based collaborative filtering approach</title>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="11329" to="11339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Graph information bottleneck</title>
		<author>
			<persName><forename type="first">Tailin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A quest for structure: Jointly learning the graph structure and semi-supervised classification</title>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Information and Knowledge Management</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="87" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5449" to="5458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengsheng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7370" to="7377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">GNN explainer: A tool for post-hoc explanation of graph neural networks</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dylan</forename><surname>Bourgeois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Graphsaint: Graph sampling based inductive learning method</title>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Prasanna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Scalegcn: Efficient and effective graph convolution via channel-wise scale transformation</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Gnnguard: Defending graph neural networks against adversarial attacks</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Bayesian graph convolutional neural networks for semi-supervised classification</title>
		<author>
			<persName><forename type="first">Yingxue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumyasundar</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deniz</forename><surname>Üstebay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5829" to="5836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Robust graph representation learning via neural sparsification</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongjin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingchao</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenchao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="11458" to="11468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Beyond homophily in graph neural networks: Current limitations and effective designs</title>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Deep graph structure learning for robust representations: A survey</title>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinghao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<idno>CoRR, abs/2103.03036</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
